abstractive summarization of reddit posts with multi level memory networks byeongchang kim hyunwoo kim gunhee kim department of computer science and engineering center for superintelligence seoul national university seoul korea byeongchang
kim hyunwoo

snu
ac
kr
ac
kr
snu
ac
kr projects reddit tifu abstract we address the problem of abstractive rization in two directions proposing a novel dataset and a new model
first we collect reddit tifu dataset consisting of k posts from the online discussion forum reddit
we use such informal crowd generated posts as text source in contrast with existing datasets that mostly use formal documents as source such as news articles
thus our dataset could less suffer from some biases that key sentences usually locate at the beginning of the text and favorable summary candidates are already side the text in similar forms
second we pose a novel abstractive summarization model named multi level memory networks mmn equipped with multi level memory to store the information of text from different levels of abstraction
with quantitative evaluation and user studies via amazon mechanical turk we show the reddit tifu dataset is highly stractive and the mmn outperforms the of the art summarization models
introduction abstractive summarization methods have been der intensive study yet they often suffer from ferior performance compared to extractive ods allahyari et al
nallapati et al
see et al

admittedly by task tion abstractive summarization is more ing than extractive summarization
however we argue that such inferior performance is partly due to some biases of existing summarization datasets
the source text of most datasets over et al
hermann et al
cohan et al
grusky et al
narayan et al
originates from formal documents such as news articles which have some structural patterns of which extractive methods better take advantage
in formal documents there could be a strong tendency that key sentences locate at the ning of the text and favorable summary dates are already inside the text in similar forms
hence summarization methods could generate good summaries by simply memorizing keywords or phrases from particular locations of the text
moreover if abstractive methods are trained on these datasets they may not show much tion see et al
because they are implicitly forced to learn structural patterns kedzie et al

grusky et al
and narayan et al
recently report similar extractive bias in existing datasets
they alleviate this bias by lecting articles from diverse news publications or regarding intro sentences as gold summary
different from previous approaches we pose to alleviate such bias issue by changing the source of summarization dataset
we exploit generated posts from the online discussion forum reddit especially tifu subreddit which are more casual and conversational than news articles
we observe that the source text in reddit does not follow strict formatting and disallows models to simply rely on locational biases for tion
moreover the passages rarely contain tences that are nearly identical to the gold mary
our new large scale dataset for tive summarization named as reddit tifu tains pairs of an online post as source text and its corresponding long or short summary tence
these posts are written by many different users but each pair of post and summary is ated by the same user
another key contribution of this work is to pose a novel memory network model named level memory networks mmn
our model is equipped with multi level memory networks ing the information of source text from different levels of abstraction i
e
word level level paragraph level and document level
this design is motivated by that abstractive r a l c
s c v
v i x r a tion is highly challenging and requires not only to understand the whole document but also to nd salient words phrases and sentences
our model can sequentially read such multiple levels of mation to generate a good summary sentence
most abstractive summarization methods see et al
li et al
zhou et al
liu et al
cohan et al
paulus et al
employ sequence to sequence models sutskever et al
where an rnn encoder embeds an input document and another rnn decodes a summary sentence
our mmn has two major advantages over based els
first rnns accumulate information in a few xed length memories at every step regardless of the length of an input sequence and thus may fail to utilize far distant information due to ishing gradient
it is more critical in tion tasks since input text is usually very long words
on the other hand our tional memory explicitly captures long term mation
second rnns can not build tions of different ranges since hidden states are sequentially connected over the whole sequence
this still holds even with hierarchical rnns that can learn multiple levels of representation
in contrast our model exploits a set of convolution operations with different receptive elds hence it can build representations of not only multiple levels but also multiple ranges e

sentences paragraphs and the whole document
our perimental results show that the proposed mmn model improves abstractive summarization formance on both our new reddit tifu and isting newsroom abs grusky et al
and xsum narayan et al
datasets
it performs several state of the art abstractive els with architecture such as see et al
zhou et al
li et al

we uate with quantitative language metrics e

plexity and rouge lin and user studies via amazon mechanical turk amt
the contributions of this work are as follows

we newly collect a large scale abstractive summarization dataset named reddit tifu
as far as we know our work is the rst to use non formal text for abstractive rization

we propose a novel model named multi level memory networks mmn
to the best of our knowledge our model is the rst attempt to leverage memory networks for the tive summarization
we discuss the unique updates of the mmn over existing memory networks in section

with quantitative evaluation and user studies via amt we show that our model forms state of the art abstractive tion methods on both reddit tifu room abstractive subset and xsum dataset
related work our work can be uniquely positioned in the text of the following three topics
neural abstractive summarization
many deep neural network models have been proposed for abstractive summarization
one of the most dominant architectures is to employ rnn based models with attention mechanism such as rush et al
chopra et al
ati et al
cohan et al
hsu et al
gehrmann et al

in addition cent advances in deep network research have been promptly adopted for improving abstractive marization
some notable examples include the use of variational autoencoders vaes miao and blunsom li et al
graph based tention tan et al
pointer generator els see et al
self attention networks liu et al
reinforcement learning paulus et al
pasunuru and bansal contextual agent attention celikyilmaz et al
and tegration with extractive models hsu et al
gehrmann et al

compared to existing neural methods of stractive summarization our approach is novel to replace an rnn based encoder with explicit multi level convolutional memory
while based encoders always consider the whole quence to represent each hidden state our level memory network exploits convolutions to control the extent of representation in multiple els of sentences paragraphs and the whole text
summarization datasets
most existing marization datasets use formal documents as source text
news articles are exploited the most including in duc over et al
word napoles et al
cnn dailymail lapati et al
hermann et al
room grusky et al
and xsum narayan et al
datasets
cohan et al
troduce datasets of academic papers from arxiv dataset posts words post words summ rottentomatoes



idebate tifu short tifu long



table statistics of the reddit tifu dataset pared to existing opinion summarization corpora tentomatoes and idebate wang and ling
we show average and median in parentheses values
tioning
one of the closest works to ours may be singh et al
which use a memory network for text summarization
however they only deal with extractive summarization by storing dings of individual sentences into memory
compared to previous memory networks our i building a mmn has four novel features multi level memory network that better abstracts multi level representation of a long document employing a dilated convolutional memory write mechanism to correlate adjacent memory cells iii proposing normalized gated tanh units to avoid covariate shift within the network and generating an output sequence without rnns
reddit tifu dataset we introduce the reddit tifu dataset whose key statistics are outlined in table
we collect data from reddit which is a discussion forum platform with a large number of subreddits on diverse topics and interests
specically we crawl all the posts from jan to mar in the tifu dit where every post should strictly follow the posting rules otherwise they are removed
thanks to the following the posts in this subreddit can be an excellent corpus for abstractive rization rule posts and titles without context will be removed
your title must make an attempt to encapsulate the nature of your
rule all posts must end with a summary that is descriptive of your and its consequences
thus we regard the body text as source the tle as short summary and the summary as long summary
as a result we make two sets of datasets tifu short and tifu long
figure shows an example post of the tifu subreddit

preprocessing we build a vocabulary dictionary v by choosing the most frequent v k words in the dataset

com r tifu wiki rules
figure an example post of the tifu subreddit
and pubmed
hu et al
propose the sts dataset as a collection of chinese microblog s short text each paired with a summary
however it selects only formal text posted by veried ganizations such as news agencies or government institutions
compared to previous summarization datasets our dataset is novel in that it consists of posts from the online forum reddit
rotten tomatoes and idebate dataset wang and ling use online text as source but they are relatively small in scale
k posts of tomatoes compared to k posts of tifu short as shown in table
moreover rotten tomatoes use multiple movie reviews written by different users as single source text and one sentence consensus made by another professional editor as summary
thus each pair of this dataset could be less ent than that of our tifu which is written by the same user
the idebate dataset is collected from short arguments of debates on controversial ics and thus the text is rather formal
on the other hand our dataset contains the posts of interesting stories happened in daily life and thus the text is more unstructured and informal
neural memory networks
many ory network models have been proposed to prove memorization capability of neural networks kaiser et al
na et al
yoo et al

weston et al
propose one of early memory networks for language question ing qa since then many memory networks have been proposed for qa tasks sukhbaatar et al
kumar et al
miller et al

park et al
propose a convolutional read memory network for personalized image
iliveprettyfarfromwindsor
afivehourdrivelater ifinallygotbackhome
homeworketc
init andmychemistrytextbookbackinwindsor
ialsohaveamathandchemtestnextweekwhichiamnowsocompletelyscrewedfor
long summary text whichisfivehourdriveawayandiamnowscrewedfortherestofthesemester
short summary words figure relative locations of bigrams of gold summary in the source text across different datasets
dataset cnn dm nallapati et al
ny times sandhaus newsroom grusky et al
newsroom abs grusky et al




xsum narayan et al
tifu short tifu long pg lead ext oracle pg lead pg oracle r l r l r l ratio r l ratio r l








































































table comparison of rouge scores between different datasets row and methods column
pg is a of the art abstractive summarization method and lead and ext oracle are extractive ones
pg lead and pg oracle are the rouge l ratios of pg with lead and ext oracle respectively
we report the numbers for each dataset row from the corresponding cited papers
we exclude any urls unicodes and special acters
we lowercase words and normalize digits to
subreddit names and user ids are replaced with and token respectively
we use package to strip markdown format and to tokenize words
common prexes of summary sentences e

tifu by are trimmed
we do not take oov words into consideration since our vocabulary with size k covers about of word frequencies in our dataset
we set the maximum length of a ument as
we exclude the gold summaries whose lengths are more than and for short and tifu long respectively
they amount to about
k posts in both datasets i
e
less than and
we use these maximum lengths based on previous datasets e

words on average per summary in gigaword duc and cnn dailymail datasets respectively
we domly split the dataset into for training for test

abstractive properties of reddit tifu we discuss some abstractive characteristics found in reddit tifu dataset compared to existing marization datasets based on news articles
weak lead bias
formal documents including news articles tend to be structured to emphasize key information at the beginning of the text
on markdown
github


io
the other hand key information in informal online text data are more spread across the text
figure plots the density histogram of the relative tions of bigrams of gold summary in the source text
in the cnn dailymail and newsroom the bigrams are highly concentrated on the front parts of documents
contrarily our reddit tifu dataset shows rather uniform distribution across the text
this characteristic can be also seen from the rouge score comparison in table
the lead baseline simply creates a summary by selecting the rst few sentences or words in the document
thus a high score of the lead baseline implicates a strong lead bias
the lead scores are the lowest in our tifu dataset in which it is more difcult for models to simply take advantage of locational bias for the summary
strong abstractness
besides the locational bias news articles tend to contain wrap up tences that cover the whole article and they its ten have resemblance to its gold summary
existence can be measured by the score of the ext oracle baseline which creates a summary by selecting the sentences with the highest average score of l
thus it can be viewed as an upper bound for extractive models narayan et al
nallapati et al

in table the rouge scores of the ext oracle are the lowest in our tifu dataset
it means that the sentences that are similar to gold summary scarcely exist inside the source text in dailymailnewsroom absxsumreddit tifurelative














figure illustration of the proposed multi level memory network mmn model
word at a time by extracting relevant information from memory cells in response to previously erated words
the input of the model is a source text xi


xn and the output is a quence of summary words yt


yt each of which is a symbol from the dictionary v

text embedding online posts include lots of morphologically ilar words which should be closely embedded
thus we use the fasttext bojanowski et al
trained on the common crawl corpus to initialize the word embedding matrix wemb
we use the same embedding matrix wemb for both source text and output sentences
that is we represent a source text in a distributional space as i n i wembxi where xi is a one hot vector for i th word in the source text
likewise output words is embedded as
and by i and t t
construction of multi level memory s s as shown in figure the multi level memory i n network takes the source text embedding as an input and generates s number of memory tensors ma c as output where superscript a and c denote input and output memory sentation respectively
the multi level memory network is motivated by that when human stand a document she does not remember it as a single whole document but ties together several levels of abstraction e

word level level paragraph level and document level
that figure comparison between the gated linear unit gehring et al
and the proposed normalized gated tanh unit
our dataset
this property forces the model to be trained to focus on comprehending the entire text instead of simply nding wrap up sentences
finally pg lead and pg oracle in table are the rouge l ratios of pg with lead and ext oracle respectively
these metrics can quantify the dataset according to the degree of culty for extractive methods and the suitability for abstractive methods respectively
high scores of the tifu dataset in both metrics show that it is potentially an excellent benchmark for evaluation of abstractive summarization systems
multi level memory networks mmn figure shows the proposed multi level memory network mmn model
the mmn memorizes the source text with a proper representation in the memory and generates a summary sentence one multi level sequence



convweight normsigmoidconvweight normtanhlayer normconvweight
is we generate s sets of memory tensors each of which associates each cell with different ber of neighboring word embeddings based on the level of abstraction
to build memory slots of such multi level memory we exploit a multi layer cnn as the write network where each layer is chosen based on the size of its receptive eld
however one issue of convolution is that large receptive elds require many layers or large lter sizes
for example stacking layers with a lter size of results in a receptive eld size of i
e
each output depends on input words
in order to grow the receptive eld without increasing the computational cost we exploit the dilated lution yu and koltun oord et al
for the write network
memory writing with dilated convolution
in dilated convolution the lter is applied over an area larger than its length by skipping input values with a certain gap
formally for a d n length input and a lter w


k the dilated convolution operation f on s ements of a sequence is dened as k s where is the dilation rate k is the lter size s i accounts for the direction of dilation and w and are the parameters of the lter
with the dilated convolution reduces to a regular convolution
ing a larger dilation enables a single output at the top level to represent a wider range of input thus effectively expanding the receptive eld
to the embedding of a source text we recursively apply a series of dilated convolutions f rn
we denote the output of the l th convolution layer as n
normalized gated tanh units
each tion is followed by our new activation of ized gated tanh unit ngtu which is illustrated in figure and layer normalization ba et al

this mixed normalization improves earlier work of gehring et al
where only weight malization is applied to the glu
as in figure it tries to preserve the variance of activations throughout the whole network by scaling the

however we put of residual blocks by serve that this heuristic does not always preserve the variance and does not empirically work well in our dataset
contrarily the proposed ngtu not only guarantees preservation of activation ances but also signicantly improves the mance
multi level memory
instead of using only the last layer output of cnns we exploit the outputs of multiple layers of cnns to construct s sets of memories
for example memory constructed from the th layer whose receptive eld is may have sentence level embeddings while ory from the th layer whose receptive eld is may have document level embeddings
we obtain each s th level memory ma c by bling key value memory networks miller et al
s ma s mc s
s and mc s rn are input and recall that ma output memory matrix respectively
cates an index of convolutional layer used for the s th level memory
for example if we set s and m we make three level ries each of which uses the output of the rd and th convolution layer respectively
to output memory representation mc s we add the document embedding as a skip connection

state based sequence generation we discuss how to predict the next word at time step t based on the memory state and ously generated words t
figure visualizes the overall procedure of decoding
l dl l we rst apply max pooling to the output of the last layer of the encoder network to build a whole document embedding dwhole where is a sigmoid is the element wise plication and f l g denote the lter and gate for l th layer dilated convolution respectively
and f l the ngtu is an extension of the existing gated tanh units gtu oord et al
by ing weight normalization salimans and kingma dwhole


dl n
the decoder is designed based on wavenet oord et al
that uses a series of causal lated convolutions denoted by t
t as we globally condition dwhole to obtain dings of previously generated words ol f l hl t wl a hl g t hl t where hl are the lter and gate hidden state spectively and learnable parameters are wl and wl t wembyt
we set the level of the decoder network to l for tifu short and l for tifu long

we initialize a next we generate s number of query vectors s at time to our memory network as qol t qs q and bs each of these query vectors qs t bs q q
s where ws is fed into the attention function of each level of memory
as in vaswani et al
the attention function is ms ot softmax mc s t ma qs demb where we set demb for the embedding mension and ms ot
next we obtain the output word probability st ot


ms ot ol where wo
finally we lect the word with the highest probability argmaxsv st
unless is an eos token we repeat generating the next word by feeding into the output convolution layer of eq


training we use the softmax cross entropy loss from mated yt to its target ygt t
however it forces the model to predict extremes zero or one to tinguish among the ground truth and alternatives
the label smoothing alleviates this issue by acting as a regularizer that makes the model less dent in its prediction
we smooth the target bution with a uniform prior distribution u pereyra et al
edunov et al
vaswani et al

thus the loss over the training set d is experiments
experimental setting evaluation metrics
we evaluate the rization performance with two language metrics perplexity and standard rouge scores lin
we remind that lower perplexity and higher rouge scores indicate better performance
datasets
in addition to reddit tifu we also evaluate on two existing datasets abstractive set of newsroom grusky et al
and xsum narayan et al

these are suitable marks for evaluation of our model in two aspects
first they are specialized for abstractive rization which meets well the goal of this work
second they have larger vocabulary size k k than reddit tifu k and thus we can evaluate the learning capability of our model
baselines
we compare with three abstractive summarization methods one basic model two heuristic extractive methods and variants of our model
we choose pg see et al
seass zhou et al
drgd li et al
as the state of the art methods of abstractive marization
we test the attention based model denoted as att chopra et al

as heuristic extractive methods the uses the rst sentence in the text as summary and the ext oracle takes the sentence with the highest average score of l with the gold summary in the text
thus ext oracle can be viewed as an upper bound for extractive methods
we also test variants of our method
to validate the contribution of each component we exclude one of key components from our model as follows with tional convolutions instead with no multi level memory with ing gated linear units gehring et al

that is quanties the improvement by the dilated convolution assesses the effect of multi level memory and dates the normalized gated tanh unit
please refer to the appendix for implementation details of our method
l log

quantitative results we implement label smoothing by modifying the ground truth distribution for word ygt t to be t and for ygt t where is a smoothing parameter set to

ther details can be found in the appendix
table compares the summarization performance of different methods on the tifu short long dataset
our model outperforms the state of art abstractive methods in both rouge and plexity scores
pg utilizes a pointer network tifu short methods ext oracle att chopra et al
pg see et al
seass zhou et al
drgd li et al
mmn mmn nodilated mmn nomulti mmn nongtu ppl r l
n a
n a



































tifu long n a n a ext oracle

att chopra et al
















pg see et al
seass zhou et al
drgd li et al
mmn mmn nodilated mmn nomulti mmn nongtu



















table summarization results measured by perplexity and l on the tifu short long dataset
methods att pg t mmn ours xsum newsroom abs r l r l























table summarization results in terms of l on newsroom abs grusky et al
and xsum narayan et al

except mmn all scores are referred to the original papers
t is the topic aware convolutional model
to copy words from the source text but it may not be a good strategy in our dataset which is more abstractive as discussed in table
seass shows strong performance in duc and gigaword dataset in which the source text is a single long sentence and the gold summary is its shorter sion
yet it may not be sufcient to summarize much longer articles of our dataset even with its second level representation
drgd is based on the variational autoencoder with latent variables to capture the structural patterns of gold summaries
this idea can be useful for the similarly structured formal documents but may not go well with verse online text in the tifu dataset
these state of the art abstractive methods are not as good as our model but still perform better than extractive methods
although the ext oracle heuristic is an upper bound for tractive methods it is not successful in our highly tifu short tifu long vs
baselines win lose




att pg seass drgd gold




tie win lose














tie




table amt results on the tifu short long between our mmn and four baselines and gold summary
we show percentages of responses that turkers vote for our approach over baselines
abstractive dataset it is not effective to simply retrieve existing sentences from the source text
moreover the performance gaps between tive and extractive methods are much larger in our dataset than in other datasets see et al
paulus et al
cohan et al
which means too that our dataset is highly abstractive
table compares the performance of our mmn on newsroom abs and xsum dataset
we report the numbers from the original papers
our model outperforms not only the rnn based abstractive methods but also the convolutional based methods in all rouge scores
especially even trained on single end to end training procedure our model outperforms t which necessitates two training stages of lda and
these sults assure that even on formal documents with large vocabulary sizes our multi level memory is effective for abstractive datasets

qualitative results we perform two types of qualitative evaluation to complement the limitation of automatic language metrics as summarization evaluation
user preferences
we perform amazon chanical turk amt tests to observe general users preferences between the summarization of different algorithms
we randomly sample test examples
at test we show a source text and two summaries generated by our method and one baseline in a random order
we ask turkers to choose the more relevant one for the source text
we obtain answers from three different turkers for each test example
we compare with four tive baselines att pg seass and drgd and the gold summary gold
table summarizes the results of amt tests which validate that human annotators signicantly prefer our results to those of baselines
as pected the gold summary is voted the most
summary examples
figure shows selected explore the data in other online forums such as quora stackoverow and other subreddits
acknowledgments we thank chris dongjoo kim yunseok jang and the anonymous reviewers for their helpful ments
this work was supported by kakao and kakao brain corporations and iitp grant funded by the korea government msit no
development of qa systems for video story understanding to pass the video turing test
gunhee kim is the corresponding author
references mehdi allahyari seyedamin pouriyeh mehdi asse saeid safaei elizabeth d trippe juan b gutierrez and krys kochut

text summarization niques a brief survey
in

jimmy lei ba jamie ryan kiros and geoffrey e ton

layer normalization
in stat
piotr bojanowski edouard grave armand joulin and tomas mikolov

enriching word vectors with subword information
in tacl
asli celikyilmaz antoine bosselut xiaodong he and yejin choi

deep communicating agents for abstractive summarization
in naacl hlt
sumit chopra michael auli and alexander m rush

abstractive sentence summarization with in attentive recurrent neural networks
hlt
arman cohan franck dernoncourt doo soon kim trung bui seokhwan kim walter chang and zli goharian

a discourse aware attention model for abstractive summarization of long uments
in naacl hlt
sergey edunov myle ott michael auli david ier and marcaurelio ranzato

classical structured prediction losses for sequence to quence learning
in naacl hlt
jonas gehring michael auli david grangier denis yarats and yann n dauphin

convolutional sequence to sequence learning
in icml
sebastian gehrmann yuntian deng and alexander rush

bottom up abstractive summarization
in emnlp
xavier glorot and yoshua bengio

ing the difculty of training deep feedforward neural networks
in aistats
max grusky mor naaman and yoav artzi

newsroom a dataset of
million summaries with diverse extractive strategies
in naacl hlt
figure examples of abstractive summary generated by our model and baselines
in each set we too show the source text and gold summary
examples of abstractive summarization
baselines often generate the summary by mostly focusing on some keywords in the text while our model produces the summary considering both keywords and the whole context thanks to multi level ory
we present more examples in the appendix
conclusions we introduced a new dataset reddit tifu for stractive summarization on informal online text
we also proposed a novel summarization model named multi level memory networks mmn
periments showed that the reddit tifu dataset is uniquely abstractive and the mmn model is highly effective
there are several promising ture directions
first rouge metrics are ited to correctly capture paraphrased summaries for which a new automatic metric of abstractive summarization may be required
second we can
iknewmyparentswouldsaynosoisnuckoutofthehouse
ihadbeentalkingtomymomabouthowsadevenhearingthethemesongmademe
alsoshehadseenmewatchingabunchofsadanimethemesongsandtearingupalittlesoshemusthavethoughtiwasdepressed
whenigothometodaymymomwaspracticallyintears
source sneaking out of my friends house last sneaking out of my friends att sneaking out of not watching my accidentally spoiling my mom watching a


nochip
soigetreadytotakeherhomeanddefleaher

source
hadtogivedogbacktopossibleabusers
beingaccusedofstealingthefuckingdog
nogooddeedgoesunpunished
ours tried to help a dog got a bit and got accused of called a dog a unk might get charged with iwas a unk dog and i was nt playing attention and got arrested for being a unk unk karl moritz hermann tomas kocisky edward grefenstette lasse espeholt will kay mustafa leyman and phil blunsom

teaching chines to read and comprehend
in nips
wan ting hsu chieh kai lin ming ying lee kerui min jing tang and min sun

a unied model for extractive and abstractive tion using inconsistency loss
in acl
baotian hu qingcai chen and fangze zhu

sts a large scale chinese short text summarization dataset
in emnlp
ukasz kaiser or nachum aurko roy and samy bengio

learning to remember rare events
in iclr
chris kedzie kathleen mckeown and hal daume iii

content selection in deep learning models of summarization
in emnlp
diederik kingma and jimmy ba

adam a method for stochastic optimization
in iclr
ankit kumar ozan irsoy jonathan su james bury robert english brian pierce peter ondruska ishaan gulrajani and richard socher

ask me anything dynamic memory networks for ural language processing
in icml
piji li wai lam lidong bing and zihao wang

deep recurrent generative decoder for abstractive text summarization
in emnlp
chin yew lin

rouge a package for matic evaluation of summaries
in tsbo
peter j liu mohammad saleh etienne pot ben goodrich ryan sepassi lukasz kaiser and noam shazeer

generating wikipedia by ing long sequences
in iclr
yishu miao and phil blunsom

language as a latent variable discrete generative models for sentence compression
in emnlp
alexander miller adam fisch jesse dodge hossein karimi antoine bordes and jason weston

key value memory networks for directly reading documents
in emnlp
seil na sangho lee jisung kim and gunhee kim

a read write memory network for movie story understanding
in iccv
ramesh nallapati feifei zhai and bowen zhou

summarunner a recurrent neural work based sequence model for extractive rization of documents
in aaai
ramesh nallapati bowen zhou cicero dos santos caglar gulcehre and bing xiang

stractive text summarization using sequence sequence rnns and beyond
in conll
courtney napoles matthew gormley and benjamin in annotated gigaword
van durme

naacl hlt akbc wekex
shashi narayan shay b
cohen and mirella lapata

do nt give me the details just the mary topic aware convolutional neural networks for extreme summarization
in emnlp
shashi narayan shay b cohen and mirella lapata

ranking sentences for extractive rization with reinforcement learning
in hlt
aaron van den oord sander dieleman heiga zen karen simonyan oriol vinyals alex graves nal kalchbrenner andrew senior and koray kavukcuoglu

wavenet a generative model for raw audio
in ssw
aaron van den oord nal kalchbrenner lasse holt oriol vinyals alex graves al

ditional image generation with pixelcnn decoders
in nips
paul over hoa dang and donna harman

duc in context
in ipm
cesc chunseong park byeongchang kim and hee kim

attend to you personalized image captioning with context sequence memory works
in cvpr
ramakanth pasunuru and mohit bansal

reward reinforced summarization with saliency and entailment
in naacl hlt
romain paulus caiming xiong and richard socher

a deep reinforced model for abstractive summarization
in iclr
gabriel pereyra george tucker jan chorowski ukasz kaiser and geoffrey hinton

larizing neural networks by penalizing condent output distributions
in iclr
alexander m rush sumit chopra and jason weston

a neural attention model for abstractive sentence summarization
in emnlp
tim salimans and diederik p kingma

weight normalization a simple reparameterization to in celerate training of deep neural networks
nips
evan sandhaus

new york times annotated corpus
in ldc
abigail see peter j liu and christopher d manning

get to the point summarization with generator networks
in acl
abhishek kumar singh manish gupta and vasudeva varma

hybrid memnet for extractive marization
in cikm
sainbayar sukhbaatar jason weston rob fergus al

end to end memory networks
in nips
ilya sutskever oriol vinyals and quoc v le

sequence to sequence learning with neural works
in nips
jiwei tan xiaojun wan and jianguo xiao

stractive document summarization with a based attentional neural model
in acl
ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan n gomez ukasz kaiser and illia polosukhin

attention is all you need
in nips
lu wang and wang ling

neural based abstract generation for opinions and ments
in naacl hlt
jason weston sumit chopra and antoine bordes

memory networks
in iclr
seungjoo yoo hyojin bahng sunghyo chung junsoo lee jaehyuk chang and jaegul choo

oring with limited data few shot colorization via memory augmented networks
in cvpr
fisher yu and vladlen koltun

multi scale text aggregation by dilated convolutions
in iclr
qingyu zhou nan yang furu wei and ming zhou

selective encoding for abstractive sentence summarization
in acl
description initial learning rate embedding dimension demb kernel size k dilation rate d description grad clip of encoder layers of decoder layers layers used for memory m smoothing parameter description grad clip of encoder layers of decoder layers layers used for memory m smoothing parameter common congurations
tifu short tifu long



newsroom abs xsum



table model hyperparameters in experiments on tifu short long newsroom abstractive subset and xsum
novel n gram ratio gram gram gram gram dataset cnn dailymail ny times newsroom newsroom ext newsroom mix newsroom abs xsum tifu short tifu long



































table comparison of novel n gram ratios between reddit tifu and other summarization datasets
measure to nd extractive bias in the tion dataset
c more examples figure illustrates selected examples of summary generation
in each set we show a source text a reference summary and generated summaries by our method and baselines
in the examples while baselines generate summary by mostly focusing on some keywords our model produces summary considering both keywords and the whole context thanks to the multi level memory
a implementation details all the parameters are initialized with the xavier method glorot and bengio
we apply the adam optimizer kingma and ba with

and
we ply weight normalization salimans and kingma to all layers
we set learning rate to
and clip gradient at

at every epochs we divide learning rate by until it reaches

we train our models up to epochs for short and epochs for tifu long
table summarizes the setting of eters for our model in all experiments on short long dataset newsroom abstractive subset and xsum
b novel n gram ratios table compares the ratios of novel n grams in the reference summary between datasets
ing see et al
narayan et al
we compute this ratio as follows we rst count the number of n grams in the reference summary that do not appear in the source text and divide it with the total number of n grams
the higher the tio is the less the identical n grams are in the source text
the cnn dailymail new york times newsroom datasets all for example hibit low novel gram ratios as


respectively
this means that about of the words in reference summary already exist inside the source text
it is due to that the maries from formal documents e

news and demic papers tend to have same expressions with the source documents
therefore these datasets may be more suitable for extractive tion than abstractive one on the other hand our dataset is more abstractive
favorable for mixed methods we also compare the novel n gram ratio for xsum and three subsets of newsroom i newsroom ext a subset favorable for tractive methods newsroom mix a set and newsroom abs a subset favorable for tive methods
we summarize two interesting servations as follows
first as expected the more favorable for abstractive methods is the higher novel n gram ratio is
second novel n gram ratios of newsroom abs and xsum are higher than those of our dataset even though their data sources are news publications
thus we argue that novel n gram ratios are pretty good but not a sufcient figure examples of abstractive summary generated by our model and baselines
in each set we too show the source text and reference summary
weuseaninternalmessagingapplicationsoftwareatworkwhichhasbeengreatforcommunicatingwithotherteammates
alotofushavestartedusingittocomplainaboutthingswearenothappyaboutatwork
thisleadsmetotodaywherejustasiamabouttogohomemymanagercallsmeintoaprivatemeetinglookingreallyupset
thentheymentionedtheprogramnameandthattheyhadreceivedanemail andsuddenlyirealizedihadfuckeduponeofthequirksofthisprogramisthatwhensomeoneisofflineitemailsthemthemessage
arecentlyexco
theycameonlinesowestartedhavingaconversation thenanotherco workerwalkeduptomeforachatwhohasbeenhavingaroughweekandcomplainedaboutourboss
whentheyfinishedtheirrant ithenmessagedmyexco
theyhadgoneoffline soanemailwassenttotheiroldworkemail
pastemployeesemailsgetsenttotheboss incaseimportantemailsaresenttothem
soafterthemeetingistillhavemyjob

igearedupandwentonmywayformyfirstride
iwentallthewaybacktomyhouseandalas nophone

theseareallbackroadstomygirlfriendshousesoihadtoreallygetintheundergrowthtolookformyphone
itgotdarkandiheadedhomefeelingdejected

andasitturnsoutitfelloffmybikeandtumbledontothesideofhisdriveway
ichalkedthisupasawinandconsideredmyselflucky
untilyesterday
iwokeupwithpoisonivyallovermybody
andwhenisayallovermybody imeanallovermybody
ihavesomeonmyarms legs face andmostimportantlyallovermydick
andtheworstspotofthemallisonmydick
ihaveneverbeensouncomfortableinmylife
imusthavehadsomeoftheoilonmyhandsandscratchedanitchdownthere

f
source taking a ride on my new buying my new att trying to be a good wearing my cargo for a my phone therestofthefamilyproceededtoplaycardsandbecomequiteintoxicated
me beingthelittleshitthatiwas andprobablystillam tookthisopportunitytoraidtheliquorcooler andmadeoffwithabottleofwine
i alongwithafewotherofmyunderagecousins ranofftoconsumeourloot
now inmostsituations however
thathewillsmoothtalkitoverwiththem andthatsheshouldbringthemabottleoftheirfavoritewine
welljohnandjanearehavingdinneroutsidetomeettheparents shediscoversthewineisgone
janethenbeginstopanic andstartstearingupthesurroundingarealookingforit
orwhyjohnwouldbringcrazytothefamilyreunion
janenowslipsintocompletehysteria andrunsinsidetolockherselfinthebathroom
thedayafter johnconvinceshisparentstotryagain andallgoesverywell
source stealing a bottle of getting drunk and stealing a wine att getting drunk and making a family my future dinner stealing alcohol from my cousin s parent s a message program at work emailed a private message between a past co worker and myself to my boss saying how people where not happy with sent a message to my boss and now i m in a meeting with my att ilied about my boss to get my job and now i m in a job with a new a program that sacked from work and igot sent to a mettingby my
mycurfewforthenighthadbeenmidnight
thispromptedmymothertocallmeangrilyandgroggilyclaimingthatshewasmadatme


commentsquestions

ididso andproceededtoturnairplanemodeoff

shereluctantlyagreedandihurriedlyrushedtomyroom
source did some funky stuff with my accidentally sent an inappropriate text to a girl and then accidentally sent it to my mother s phone and now answering an airplane call and accidentally adding my mothers phone and her phone to find out she was sleeping on accidentally sent a text to my mom that i was sending her a text from the unk bomb
