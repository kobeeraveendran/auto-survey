understanding neural abstractive summarization models via uncertainty jiacheng xu shrey desai department of computer science the university of texas at austin greg durrett jcxu
utexas
edu
edu t c o l c
s c v
v i x r a abstract an advantage of abstractive rization models is that they generate text in a free form manner but this exibility makes it difcult to interpret model behavior
in this work we analyze summarization decoders in both blackbox and whitebox ways by studying on the entropy or uncertainty of the model s token level predictions
for two strong trained models pegasus zhang et al
and bart lewis et al
on two summarization datasets we nd a strong relation between low prediction entropy and where the model copies tokens rather than erating novel text
the decoder s uncertainty also connects to factors like sentence position and syntactic distance between adjacent pairs of tokens giving a sense of what factors make a context particularly selective for the model s next output token
finally we study the tionship of decoder uncertainty and attention behavior to understand how attention gives rise to these observed effects in the model
we show that uncertainty is a useful perspective for analyzing summarization and text tion models more broadly
introduction recent progress in abstractive summarization has been fueled by the advent of large scale ers pre trained on autoregressive language eling objectives hoang et al
khandelwal et al
lewis et al
zhang et al

despite their strong performance on automatic rics like rouge lin abstractive models are not as straightforward and interpretable as their extractive counterparts
free form generation in these models also leads to serious downstream rors such as factual inconsistencies with the input document cao et al
kryscinski et al
is available at
jiacheng xu text sum uncertainty wang et al
durmus et al
goyal and durrett
although the interpretability of nlu models has been extensively studied ribeiro et al
ghaeini et al
jain and wallace desai and durrett summarization models specically have not received similar tion with analysis efforts often focused on datasets and evaluation kryscinski et al

in this work we focus on interpreting and standing abstractive summarization models through the lens of decoder uncertainty or the entropy of decisions during generation
while uncertainty in generation has been studied from the perspective of data ott et al
sampling fan et al
holtzman et al
and training correia et al
kang and hashimoto it is lized as a technique for analysis and inspection of generation systems
we study two prominent summarization models pegasus zhang et al
and bart lewis et al
ne tuned on two english summarization datasets cnn daily mail hermann et al
and xsum narayan et al
to understand model behavior in each setting
first by comparing n grams between the input document and generated summaries we establish two coarse types for decoded tokens copy and erate see et al

we nd that the entropy of the generation decision correlates with whether the model is copying or generating as well as where in the sentence the token is
this paints a picture of certain contexts being more restrictive from the standpoint of generation particularly early in tences where a model has not decided what to copy yet and illustrates the interaction of content selection and lexical choice
second we extend this analysis by looking at how uncertainty relates to the syntax of the generated sentence whether uncertainty connects to syntactic notions of prisal roark et al
and how the entropy varies across certain syntactic productions
finally we derive a way to quantify decoder attention by aggregating distinct self attention heads ing the correlation between the attention entropy and prediction entropy and investigating the spondence between the prediction entropy and the fraction of the past and future decoded tokens
taking this analysis together we nd that the abstractiveness of reference summaries tally changes model behavior the extractive nature of cnn dm makes most of its decisions low tropy and copy oriented while the model maintains higher uncertainty on xsum yielding more stractive summaries
more broadly we show that uncertainty is a simple but effective tool to terize decoder behavior in text generation
model and experimental setup our experiments use pegasus zhang et al
and bart lewis et al
two state the art pre trained models
we use the large version of these two models which have and transformer layers respectively
both models have pre training objectives tailored what to this problem domain modeling for denoising bart or inlling of masked out tences pegasus
we directly use the pre trained models from wolf et al

as reported in the original papers and sured by l lin pegasus achieves


on cnn dm mann et al
and


on xsum narayan et al
and bart achieves


and



entropy
entropy is a standard measure of certainty in a probabilistic distribution
given a discrete random variable x with all possible comes xn the entropy of x is dened as p xi log p xi
for pre trained transformers the domain of the predictions the vocabulary is large and also fers between models
the vocabulary sizes for pegasus and bart are and and the prediction distribution is usually long tailed
google pegasus cnn dailymail google pegasus xsm facebook bart large cnn and facebook bart large xsum for pegasus and bart on these two datasets
that entropy generally increases as the variable s domain grows a uniform distribution over outcomes has entropy
while a uniform distribution over outcomes has entropy

to combat this nucleus sampling holtzman et al
is used to sample from only the top most probable outcomes the nucleus to avoid erating very unlikely tokens
to more fairly pare models with different vocabulary sizes and to better reect the actual sampling distribution we therefore compute all entropy values in this work over the nucleus distribution
that is we sort the prediction distribution p xi in ing order and get a minimal set of tokens where v min xiv min p xi
then we normalize the distribution as follows p p if xi v min otherwise
the probability cumulative where xiv min p xi
we use p
for all experiments
the entropy is computed based on the new distribution p
model uncertainty during generation in this section we analyze and compare the tion uncertainty from different models and different datasets by inspecting entropy values during ation allowing us to localize uncertainty to certain positions in a decoded sentence
a principle factor that past work has investigated is the amount of copying in abstractive summarization models see et al
paulus et al

we rst aim to derstand how decisions to copy document content or generate new text are reected in the model s uncertainty
one complicating factor is that while bart and pegasus both exhibit a mix of copying and novel generation they do not have an explicit copy ation like in past models and so these behaviors are more difcult to dene
we rst separate tion decisions by bigrams that appear in the input document existing bigrams or whether they are free form generations novel bigrams
figure shows a histogram of model entropies broken down by these two categories
most notably there is a strong correlation between copy like behavior and the entropy of the model s tion distribution
on cnn dm we see that low entropy decisions are largely those generating isting bigrams and conversely existing bigrams are usually generated with low entropy
new grams are generated with a broad range of high are dened based on tokens rather than pieces and so may consist of more than two generation steps
figure next token entropies computed on k eration steps from pegasuscnn dm pegasusxsum bartcnn dm and bartxsum respectively broken into two cases an existing bigram means the bigram just generated occurs in the input document while a novel bigram is an organic model generation
these cases are associated with low entropy and high entropy actions respectively
the axis shows the entropy truncated at and the y axis shows the count of bigram falling in each bin
the dashed lines indicate the median of each distribution
entropy values and are much more frequent on xsum
these results align with our manual ysis of these summaries pegasuscnn dm and bartcnn dm summaries largely consist of spans from the input document with minor compression while pegasusxsum and bartxsum summaries involve stitching together disparate concepts and paraphrasing key details
this reects a sponding divergence in the gold summaries where cnn dm summaries are far more extractive than those in xsum
critically though the entropy distributions are dissimilar across the two datasets we see ities among the approximate copy and generate operations on cnn dm and xsum the median entropy values of using existing bigrams are
and
respectively and for generating new grams
and

with this connection between entropy and ing behavior we make the following additional observations based on figures and entropy varies across token positions cially on cnn dm
in figure we depict a different view of entropy looking at the ing process as it progresses through each sentence
across both cnn dm and xsum models are most uncertain at the beginning of the sentence and least uncertain at the end of the sentence
however the rate at which entropy drops off is quite ferent on cnn dm the entropy after decoding of tokens falls below while the entropies figure prediction entropy values by relative sentence positions
for example
indicates the rst of tokens in a sentence and
is the last of tokens
pegasuscnn dm and bartcnn dm make highly certain decisions to start but then entropy decreases suggesting that these models may be copying based on a sentence prex
entropies on xsum are more stant across the sentence
on xsum only begin to considerably drop after decoding of tokens
our manual analysis gests the following characterization to generate each sentence on cnn dm the model makes some high entropy decisions to identify a tence and begin to copy its prex followed by a series of low entropy decisions to copy that sentence s content
on xsum which is highly abstractive and features single sentence summaries content planning and generation are less clearly decoupled
pegasus copies and generates more tokens with entropy
bart and pegasus port similar rouge results on cnn dm but these models do not place the same distributions over summaries
pegasus has more low entropy copying decisions and its start of sentence tropies are also signicantly lower figure
this suggests that it is more condent than bart in lecting content to discuss next
there are also more low entropy generation decisions particularly on xsum
entropies of syntactic productions having observed connections between sentence position and entropy we now esh out this analysis from the lens of syntax focusing in particular on uncertainty at constituent boundaries
from our pegasus generations on cnn dm and xsum dmpegasusbartexisting bigramsnovel



















positionentropy production rule example np np np arsenal vs

the game that changed the
np np sbar

who has not been

np cd nn nns np nnp cd




table examples of specic np productions with high entropy top and low entropy bottom
the tation implies the constituent y is generated with entropy
might be straightforward perhaps due to a direct copy from the document but generating a sitional phrase might be more challenging due to the large search space of possible constructions or the higher chance that the model might delete this constituent
low entropy spans are often short specic units of information
we also investigate the erage entropy of spans within a rule production to uncover what types of spans are likely to elicit certainty or uncertainty during generation
in ble we see qualitatively that productions with low average entropy productions are short extracts of document content such as felony counts
these are largely factual often containing cardinal ues and more likely to be copied
within these constituents the model is very certain about what to generate next supporting the connection with low syntactic distance
understanding decoder self attention while we have analyzed the model s predictions we have not yet determined how the different haviors we see emerge from the context
our goal is to explore what the encoder attention places its emphasis during generation and how it correlates with the prediction entropy
blocking low information tokens
analyzing the inner workings of attention in transformers is challenging clark et al
kovaleva et al
particularly because many heads are useless redundant or noisy and they frequently attend to pegasus and bart models the encoder and decoder attention during decoding are two separate distributions where the encoder attention looks at the encoding context and the decoder attention attends to the previously decoded tokens
in this paper we chiey examine the encoder attention to understand how the model references the input document
figure correlating syntactic distance between boring tokens with the entropy change in those tokens generation decisions for pegasus summaries
the median entropy change is depicted as a dashed black line
at points of high syntactic distance the model s behavior is less restricted by the context correlating with higher entropy
we obtain constituency parses for each summary sentence using the berkeley neural parser kitaev and klein and explore connections between syntax and uncertainty in more depth
low and high entropy decisions can be ized to constituent span boundaries
parsing has long been used to explain psycholinguistic tions of surprisal hale roark et al
inter alia which are in turn related to uncertainty under a language model
in our case uncertainty about generating a text is a different notion than uncertainty when a reader is processing it
hence rather than looking at an incremental parser s havior we instead look at a simpler notion of tactic distance shen et al
or the number of left and right parentheses between wt and in a linearized constituency tree
our hypothesis is that when these words exhibit high syntactic distance this word boundary is a choice point where the model may be less restricted in what it can choose to generate next
figure shows the correlation between syntactic distance and the percent change in entropy between the adjacent tokens
on both cnn dm and xsum we see two patterns emerge generating a token within the same immediate parent constituent i
e
zero syntactic distance is typically a certain cision while generating a token belonging to a new constituent is an increasingly uncertain sion
from these results we can draw a parallel to the copy vs
generate behavior established in section for example generating york after new figure correlation between attention entropy and prediction entropy of and bart on dm and
we compute the mean value of the attention entropy within each bucket of tion entropy
the uncertainty of attention strongly relates with the entropy of the model s prediction
low information tokens such as end of sentence markers or periods
inspired by tf idf joachims we propose a method to compute a set of tokens most meaningfully attended to by the model
if a token in the encoding document is attended to across many time steps like a word appearing in many documents in tf idf we want to disregard it in our analysis
let t denote the number of decoder timesteps and l be the length of the source document
we compute an aggregate attention matrix s rt by summing the attentions across all heads and all layers
we then compute a count of how often each token is attended to above a threshold q fl q and discard the attention values on tokens with the highest score
in practice we discard of tokens from the source document
attention entropy
one natural question we can ask is whether there is a connection between tropy of the attention distribution and entropy of the decoder s prediction
this relationship is shown in figure where each point represents the mean tention entropy within the corresponding prediction entropy bucket
the attention entropy is especially low where the prediction entropy ranges from to

for cases with prediction entropy greater than
the attention entropy saturates and no longer grows with the prediction entropy except the bartcnn dm
while attention entropy is ably not causing the low decoder entropy nevertheless decoder entropy provides a lens into the inner workings of the transformer model
projecting attention to vocabulary
we pothesize that low decoder entropies may arise if the model is heavily attending to certain relevant kens particularly the about to be predicted token yt of time step t and the input token of this time figure vocabulary projected attention attending to the last input current input current output yt and next output
when the prediction entropy is low the attention mostly focus a few tokens including the current input and current output yt
step xt equivalent to
for the predicted token yt we compute the vocabulary projected attention value where we late the attention of all of the occurrences of the specied token yt in the document
the higher the value the more attention put to the encoding which are predicted for this time step during decoding
we can dene the value for last time step input current time step input and the not yet decoded token for next time step
we show the relationship between the lary projected attention and the prediction entropy in figure
visualizations for both models and both datasets show that when the prediction tropy is low the attention focuses heavily on a few tokens including the current input token and the current token to predict
this suggests a tential mechanism where the model indexes into the source document by attending to then strongly identies and reads off as the next token to generate
conclusion this work analyzes pre trained summarization models via uncertainty or the entropy of ing decisions
we pursue several lines of inquiry uncertainty can help us understand copying ment spans vs
generating novel text the behavior of models in different syntactic environments and coarse properties of the model s attention tion
all of these give insight into what conditions most heavily restrict the model s generation ating an observed bigram copying low syntactic distance and attention which can easily identify decoder context in the source document
we lieve this approach can power future analyses of pre trained text generation systems




















entropyvocab projected attention acknowledgments this work was partially supported by nsf grant nsf grant a gift from salesforce inc and an equipment grant from nvidia
the authors acknowledge the texas vanced computing center tacc at the sity of texas at austin for providing hpc resources used to conduct this research
results presented in this paper were obtained using the chameleon testbed supported by the national science tion
thanks as well to the anonymous reviewers for their helpful comments
references ziqiang cao furu wei wenjie li and sujian li

faithful to the original fact aware neural in proceedings of the aaai tive summarization
conference on articial intelligence aaai
kevin clark urvashi khandelwal omer levy and christopher d
manning

what does bert look at an analysis of bert s attention
in ceedings of the acl workshop blackboxnlp lyzing and interpreting neural networks for nlp
goncalo m correia vlad niculae and andre ft tins

adaptively sparse transformers
in ceedings of the conference on empirical ods in natural language processing and the ternational joint conference on natural language processing emnlp ijcnlp pages
shrey desai and greg durrett

calibration of in proceedings of the pre trained transformers
conference on empirical methods in natural guage processing emnlp
esin durmus he he and mona diab

feqa a question answering evaluation framework for faithfulness assessment in abstractive tion
in proceedings of the annual conference of the association for computational linguistics acl
angela fan mike lewis and yann dauphin

hierarchical neural story generation
in ings of the annual conference of the association for computational linguistics acl
reza ghaeini xioali fern and prasad tadepalli

interpreting recurrent and attention based neural models a case study on natural language in proceedings of the conference on ence
pirical methods in natural language processing emnlp
john hale

a probabilistic earley parser as a cholinguistic model
in proceedings of the second meeting of the north american chapter of the ciation for computational linguistics
karl moritz hermann tomas kocisky edward stette lasse espeholt will kay mustafa suleyman and phil blunsom

teaching machines to read and comprehend
in proceedings of the ference on neural information processing systems neurips
andrew pau hoang antoine bosselut asli c elikyilmaz and yejin choi

efcient tation of pretrained transformers for abstractive summarization
arxiv preprint

ari holtzman jan buys li du maxwell forbes and yejin choi

the curious case of neural text degeneration
in proceedings of the conference on international conference on learning tions iclr
sarthak jain and byron c
wallace

attention is in proceedings of the not explanation
ference of the north american chapter of the ciation for computational linguistics human guage technologies naacl hlt
thorsten joachims

a probabilistic analysis of the rocchio algorithm with tfidf for text rization
in icml
daniel kang and tatsunori hashimoto

proved natural language generation via loss tion
arxiv preprint

urvashi khandelwal k
clark daniel jurafsky and lukasz kaiser

sample efcient text marization using a single pre trained transformer
arxiv preprint

nikita kitaev and dan klein

constituency ing with a self attentive encoder
in proceedings of the annual meeting of the association for putational linguistics acl
olga kovaleva alexey romanov anna rogers and anna rumshisky

revealing the dark crets of bert
in proceedings of the conference on empirical methods in natural language processing and the international joint conference on natural language processing emnlp ijcnlp
wojciech kryscinski nitish shirish keskar bryan cann caiming xiong and richard socher

neural text summarization a critical evaluation
in proceedings of the conference on cal methods in natural language processing and the international joint conference on natural language processing emnlp ijcnlp
tanya goyal and greg durrett

evaluating tuality in generation with dependency level ment
in findings of the conference on empirical methods in natural language processing findings of emnlp
wojciech kryscinski bryan mccann caiming xiong and richard socher

evaluating the factual consistency of abstractive text summarization
in proceedings of the conference on empirical ods in natural language processing emnlp
thomas wolf lysandre debut victor sanh julien chaumond clement delangue anthony moi ric cistac tim rault remi louf morgan icz and jamie brew

huggingface s formers state of the art natural language ing
arxiv preprint

jingqing zhang yao zhao mohammad saleh and ter j
liu

pegasus pre training with tracted gap sentences for abstractive tion
proceedings of machine learning research
pmlr
mike lewis yinhan liu naman goyal jan ghazvininejad abdelrahman mohamed omer levy veselin stoyanov and luke zettlemoyer

bart denoising sequence to sequence training for natural language generation translation and comprehension
in proceedings of the nual meeting of the association for computational linguistics pages online
association for computational linguistics
chin yew lin

rouge a package for matic evaluation of summaries
in proceedings of the annual meeting of the association for tional linguistics acl
shashi narayan shay b
cohen and mirella lapata

do nt give me the details just the mary topic aware convolutional neural networks for extreme summarization
in proceedings of the conference on empirical methods in natural guage processing emnlp
myle ott michael auli david grangier and marcaurelio ranzato

analyzing in tainty in neural machine translation
national conference on machine learning pages
romain paulus caiming xiong and richard socher

a deep reinforced model for abstractive summarization
in proceedings of the international conference on learning representations iclr
marco tulio ribeiro sameer singh and carlos guestrin

why should i trust you plaining the predictions of any classier
in proceedings of the acm sigkdd conference on knowledge discovery and data mining sigkdd
brian roark asaf bachrach carlos cardenas and christophe pallier

deriving lexical and tactic expectation based measures for tic modeling via incremental top down parsing
in proceedings of the conference on empirical methods in natural language processing pages singapore
association for computational linguistics
abigail see peter j
liiu and christopher d
ning

get to the point summarization in proceedings with pointer generator networks
of the annual meeting of the association for putational linguistics acl
yikang shen zhouhan lin chin wei huang and aaron courville

neural language modeling in by jointly learning syntax and lexicon
ceedings of the international conference on ing representations iclr
alex wang kyunghyun cho and mike lewis

asking and answering questions to evaluate the in factual consistency of summaries
ings of the annual conference of the association for computational linguistics acl

