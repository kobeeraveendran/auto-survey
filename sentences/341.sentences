enhancing extractive text summarization with topic aware graph neural networks peng cui le hu and yuanchao liu school of computer science and technology harbin institute of technology pcui lhu
hit
edu
abstract text summarization aims to compress a textual document to a short summary while keeping salient information
extractive approaches are widely used in text summarization because of their fluency and efficiency
however most of existing extractive models hardly capture sentence relationships particularly in long documents
they also often ignore the effect of topical information on capturing important contents
to address these issues this paper proposes a graph neural network extractive summarization model enabling to capture sentence relationships efficiently via graph structured document representation
moreover our model integrates a joint neural topic model ntm to discover latent topics which can provide document level features for sentence selection
the experimental results demonstrate that our model not only substantially achieves state of the art results on cnn dm and nyt datasets but also considerably outperforms existing approaches on scientific paper datasets consisting of much longer documents indicating its better robustness in document genres and lengths
further discussions show that topical information can help the model preselect salient contents from an entire document which interprets its effectiveness in long document summarization
introduction text summarization is an important task in natural language processing which can help people rapidly acquire important information from a large sum of documents
previous summarization approaches can be mainly classified into two categories which are abstractive and extractive
neural based abstractive models usually use a framework sutskever et al
to generate a word by word summary after encoding a full document
by contrast extractive models directly select important sentences from the original document and then aggregate them into a summary
abstractive models are generally more flexible but may produce disfluent or ungrammatical summary texts liu and lapata whereas extractive models have advantages in factuality and efficiency cao et al

despite their success modeling long range inter sentence relationships for summarization remains a challenge xu al

hierarchical networks are usually applied to this problem by modeling a document as a sequence of sequences cohan et al
zhang et al

however empirical observations liu and lapata showed that the use of such a paradigm to model sentence relationships does not provide much performance gain for summarization
hierarchical approaches are also slow to train and tend to overfit xiao and carenini
most recently graph neural networks gnns are widely explored to model cross sentence relationships for summarization task
the critical step of this framework is to build an effective document graph
several studies xu et al
yasunaga al
built document graphs based on discourse analysis
however this approach depends on external tools and may lead to other problems such as semantically fragmented output liu et al

wang and liu built a word sentence document graph based on word appearance but such statistical graph building approach hardly captures semantic level relationships
therefore how to model a document as a graph for summarization effectively remains an open question
corresponding author this work
org licenses

licensed under a creative commons attribution
international license
license details is another critical point of summarization is modeling global information which plays a key role in sentence selection xiao and carenini
pre trained language models can considerably boost the performance of summarization liu and lapata zhang et al
since they effectively capture context features
however they are poor at modeling document level information particularly for long documents because most of them are designed for sentences or a short paragraph xu et al

to the abovementioned weaknesses this paper proposes a novel graph based extractive summarization model
first we encode an entire document with a pre trained bert devlin et al
to learn contextual sentence representations and discover latent topics with a joint neural topic model ntm miao et al
srivastava and sutton
second we build a heterogeneous document graph consisting of sentence and topic nodes and simultaneously update their representations with a modified graph attention network gat velikovi et al

third the representations of sentence nodes are extracted to compute the final labels
intuitively our topic sentence document graph has the following advantages during the graph propagation sentence representations can be enriched by topical information which can be considered as a kind of document level feature and help our model important contents from an entire document
topic nodes can act as intermediary to bridge distance sentences hence our model can efficiently capture inter sentence relationships
we evaluate our model on four standard datasets including news articles and scientific papers
the experimental results show its effectiveness and superiority
to summarize our contributions are threefold
we conduct a quantitative exploration on the effect of latent topics on document summarization and provide an intuitive understanding of how topical information help summarize documents
we propose a novel graph based neural extractive summarization model which innovatively incorporates latent topics into graph propagation via a joint neural topic model
to the best of our knowledge we are the first to propose applying ntm to the extractive text summarization task
the experimental results demonstrate that our proposed model not only achieves competitive results compared with state of the art extractive models on news datasets but also considerably outperforms existing approaches on scientific paper datasets consisting of much longer documents indicating its better robustness in document genres and lengths
related work neural extractive summarization neural networks have achieved remarkable results in extractive summarization
existing works mainly regard extractive summarization as a sequence labeling task nallapati al
zhang et al
dong et al
or sentence ranking task narayan al

pre trained language models have provided substantial performance gain for summarization liu and lapata zhang et al
xu et al

in the current work we further model inter sentence relationships with a graph encoder and enrich sentence representations with topical information after a bert encoder
graph based summarization early works such as textrank mihalcea and tarau and lexrank erkan and radev built document graphs on the basis of inter sentence similarity and extracted summary sentences in an unsupervised manner
recently the application of gnns to document summarization has attracted considerable interests yasunaga et al
xu et al
fernandes et al
wang and liu et al

existing gnn based summarization models build document graphs on the basis of only words or sentences
on the contrary we explore the effects of high level semantic units i
e
latent topics
topic modeling for summarization topic modeling is a powerful approach to learning document features
however it has been rarely applied to document summarization
wei et al
proposed to build a document graph consisting of words sentences and topic nodes and learn the graph with markov chain
zheng al
proposed to summarize multiple documents by mining cross document subtopics
narayan et al
recommended enriching word representation with topical information
unlike them we discover latent topics with a neural topic model together with summarization
to the best of our knowledge ntm had never been applied to extractive summarization task
figure
overall architecture of our model topic graphsum
in the graph attention layer top right the square nodes denote the sentence representations output from the document encoder bottom right and the circular nodes denote the topic representations learned by ntm left
model this section describes our model namely topic aware graph neural network for document summarization topic graphsum
figure presents the overview architecture
given an arbitrary document that consists of sentences the objective of our model is to learn a sequence of binary labels where represents whether the sentence should be included in summary
our model generally consists of three parts which are the document encoder neural topic model and graph attention layer
given the input document the document encoder learns contextual representations of each sentence with a pre trained bert
the ntm aims to learn the document topic distribution and a group of topic representations
the graph attention layer builds a heterogeneous document graph with topics and sentences and then simultaneously update their node representations
after graph encoding sentence representations are further combined with topics and then sent to a sentence classifier to compute the final labels
we elucidate each part below

document encoder bert is a bidirectional transformer encoder pre trained with a large corpus
similar to previous works xu et al
liu and lapata we employ a modified version of bert to generates local context aware hidden representations of sentences
specifically we insert and tokens at the beginning and end of each sentence respectively
then we put all tokens into bert layer and learn their hidden states
where represents the word of the sentence
and represent the and tokens of the sentence and represents the hidden state of the corresponding token
after the bert encoding we regard the hidden states of as the corresponding sentence contextual representations which will be further enriched by topic information

neural topic model ntm is based on the variational autoencoder vae kingma and welling framework
it learns the latent topic via an encoding decoding process
let be the bag of words representation bert





document neural topic modelgraph attention encodersentence sentences of a given document where is the vocabulary
in the encoder we have where and are the prior parameters for parameterizing topic distribution in decoder networks
functions and are linear transformations with relu activation
the decoder can be regarded as a three step document generation process
first we employ gaussian softmax miao et al
to draw topic distribution i
e
where is the latent topic variable is the topic distribution and is the predefined topic number
second we learn the probability of predicted words throughout
is analogous to the topic word distribution matrix in lda style topic models and represents the relevance between the word and j th topic
finally we draw each word from to reconstruct input
we leave out the details and refer the readers to miao et al

considering the intermediate parameters and have encoded topical information we further use them to build topic representations as follows where represents a group of topic representations with a predefined dimension of and is a linear transformation with relu activation
is the weighted sum of each topic representation which can be regarded as the overall topic representation of document
and are used in the graph attention layer to enrich sentence representation
other summarization approaches zheng et al
narayan et al
with topical information learn topic as a fixed feature from an external model
in comparison with them the latent topic of our model is learned via a neural approach and can be dynamically updated with entire networks

graph attention layer graph building let represent an arbitrary graph where represents the node set and represents the edge set
formally our undirected graph can be defined as
where stands for sentence nodes and stands for topic nodes
represents the edge between the sentence and topic indicating that our document graph is bipartite
graph propagation we initialize the vectors of sentence nodes and that of topic nodes with learned from the document encoder and learned from ntm eq
respectively
then we update node representations with graph attention network which can be denoted as where is the node representation and represents its neighbor nodes
represents heads concatenation
and are model trainable parameters
the vanilla gat is designed for homogeneous graphs
however our document graph is heterogeneous because the sentence and topic should be considered different semantic units hence we need to make some adaptation
inspired by hu al
we consider a convenient approach to project the topic and sentence representations into an implicit common space in which we calculate the attention weight
let be the sentence node and be the topic node
we modify eq
by replacing shared matrix with different projection functions as shown as follows where and are the nonlinear transformation functions to project sentence and topic nodes to a common vector space respectively
the graph attention layer can build semantic relationships between sentences and topics
for example during graph propagation sentences can enrich their representation with topical information which can be regarded as a global feature
topics can capture their related sentences and distil salient contents from an entire document by their different topical relevance
meanwhile topic nodes can act as intermediary to help build inter sentence relationships because they are high level semantic units across sentences
after graph encoding we obtain topic sensitive sentence representations
we concatenate them with overall topic representation eq
to further capture their topical relevance to the document
then we choose a single feed forward layer as the sentence to predict the final labels i
e
where is the sigmoid function

joint training we jointly train ntm and sentence classifier
for the ntm the objective function is defined as the negative evidence lower bound as shown as follows where the first term indicates the kullback leibler divergence loss and the second term indicates the reconstruction loss
and represent the encoder and decoder networks respectively
the binary cross entropy loss of the sentence classifier is expressed as the final loss of our model is the linear combination of two parts of loss with hyperparameter to balance their weights i
e

experimental setup
datasets we conduct experiments on four datasets including two document types which are news article and scientific paper
the summarization of news articles has been widely explored but that of much longer scientific papers is more challenging since accurately encoding long texts for summarization is a known challenge vaswani et al
frermann and klementiev
therefore we conduct experiments on scientific paper datasets to verify the generalization capability of our model for long documents
the detailed statistics of four datasets is summarized in table
datasets source cnn daily mail nyt arxiv pubmed news news news scientific paper scientific paper train docs val test avg
tokens doc
sum
table statistics of four datasets split size average tokens of document and summary
we also tried adding more advanced classifiers e

cnn and rnn on top of gat layer
however the performance shows no substantial gain indicating that our model has already learned sufficient features
cnn dailymail hermann al
is the most widely used standard dataset for document summarization
we use standard splits and preprocess data in accordance with previous works see et al
liu and lapata wang and liu
nyt sandhaus is another popular summarization dataset
it is collected from new york times annotated corpus
we preprocess and divide this dataset according to durrett et al

arxiv and pubmed cohan al
are two newly constructed datasets for long document summarization which are collected from arxiv
org and pubmed
com respectively
xiao and carenini created oracle labels for the two datasets
we use the same split as that of cohan et al


models for comparison neusum zhou et al
is a neural extractive model based on framework with attention mechanism
banditsum dong al
regards sentence selection as a contextual bandit problem
policy gradient methods are used to train the model
jecs xu and durrett is a compression based summarization model that selects sentences and compresses them by pruning a dependency tree to reduce redundancy
bertsum liu and lapta inserts multiple segmentation tokens into document to obtain each sentence representation
it is the first bert based extractive summarization model
we employ its framework as the basic document encoder of our model
hibert zhang et al
modifies bert into a hierarchical structure and design an unsupervised method to pre train it
discobert xu al
is a state of the art bert based extractive model which encodes documents with bert and then updates sentence representations with a graph encoder
discobert builds a document graph with only sentence units based on discourse analysis whereas our model incorporates latent topics into a document graph and produce a heterogeneous bipartite graph

implementation details hyperparameters for the document encoder we use bert base uncased as our pre trained bert version and fine tune it for all experiments
we also implement a non bert version of our model by replacing the pre trained bert with a bi gru chung et al
layer and set its hidden size to to compare with baseline approaches without pre trained language models fairly
for ntm we set topic number
the dimension size of topic representation is set to
we implement gnns with dgl wang et al
and the number of gat layer is set to
we set the number of attention heads to for topic nodes and for sentence nodes with the same hidden size of to keep the dimension size of node representations unchanged
we train our model for epochs with nvidia cards and the batch size is set to
except for the pre trained bert encoder other parameters are randomly initialized and optimized using adam kingma and ba
eq
is set to
to balance the loss of topic modeling and sentence selection
all the hyperparameters are selected via grid search on the validation set with as metric
training strategy we consider some empirical training strategies similar with cui et al
to make our model efficiently converge
specifically we pre train ntm for epochs with a learning rate of considering its convergence speed is much slower than that of general neural networks
in joint training the ntm parameters are trained with a learning rate of while the learning rate of other parameters is set to because the ntm is relatively stable
result and analysis this section reports our experimental results
we evaluate our model on three criteria whether it can achieve state of the art results what benefits does the latent topic contribute to summarization to this end we first compare our model with state of the art approaches on two widely used benchmark datasets cnn dm and nyt
then we evaluate our model on two scientific paper datasets to verify whether discovering latent topics can help summarize long documents
lastly we present ablation and case studies for further analysis

overall performance table presents the rouge results of different models on cnn dm and nyt datasets
the first section reports the and oracle the second section reports the approaches without pre trained language models the third section reports bert based models and the last section reports our models
from the results we make the following observations
when removing pre trained language mode the bi gru version of our model outperforms all non bert baseline models and obtains competitive results compared with basic bert on both datasets
our model achieves state of the art results on nyt dataset and its performance on cnn dm dataset is on par with discobert which is a state the art bert based extractive summarization model
it needs to mention that discobert relies on external discourse analysis for modeling long range dependencies
our model achieves highly competitive results without external tools which proves its inherent superiority
model oracle neusum zhou et al
banditsum dong et al
jecs xu and durrett bert zhang et al
bertsum liu and lapata hibert zhang et al
discobert xu et al
topic graphsum bi gru topic graphsum cnn dm





















r l


















nyt







r l







table rouge results on the test set of cnn dm and nyt datasets
the results of comparison models are obtained from respective papers and represents that corresponding result is not reported

long document summarization long documents typically cover multiple topics xiao and carenini
we hypothesize that our model can capture important contents of an entire document by discovering latent topics thus enhancing model sumbasic lexrank lsa cheng lapata attn pntr gen discourse aware topic graphsum bi gru topic graphsum












arxiv












r l

























pubmed












r l












table rouge results on the test set of arxiv and pubmed datasets
results with are token from cohan et al
and results with are token from xiao and carenini
the summarization performance
to verify this hypothesis we conduct additional experiments on form documents
table presents the results of our model and state of the art public summarization systems on arxiv and pubmed datasets
the first section includes traditional approaches and oracle the second and third sections include abstractive and extractive models respectively
from table our model substantially outperforms baseline models by a large margin without pre trained bert and the gaps further increase when combined with bert
we note that discourse aware model cohan et al
slightly outperforms our model on r l of pubmed dataset a possible reason is that it explicitly leverages the section information e

introduction and conclusion of papers which may be strong clues in selecting summary sentences
our model achieves state of the art performance on scientific paper datasets without additional features indicating that discovering latent topics can indeed help summarize long document consistent with aforementioned analysis

ablation study to analyze the relative contributions of different modules in summarizing documents we compare our full model with three ablated variants ntm which removes the ntm module builds a document graph with fully connected sentence nodes and can be regarded as performing self attention calculation on the top of bert gat which removes the graph attention layer directly concatenates each of sentence representation with overall topic vector eq
and sends them to the sentence classifier and lda version which replaces ntm with standard lda and randomly initializes each topic representation
figure
and results of our full model and three ablated variants on four datasets
figure shows the results of different variants on four datasets from which we can make the following observations
our full model outperforms all variants on four datasets which proves that each module is necessary and combining them can help our model achieve the best performance
when ntm module is removed or using lda instead the performance on arxiv and pubmed datasets declines dramatically whereas on cnn dm and nyt datasets the results are competitive with our full model
a possible reason lies in that news documents are relatively short which leads to the data sparsity problem and thus reduces the effect of topic models
similarly when gat is removed the performance of scientific paper datasets has decreased more significantly than that of news datasets
this phenomenon indicates that inter sentence relationships are especially important for summarizing long documents
the lda topic model can also boost the performance but the gain of lda is much fewer than that of ntm for long documents a possible reason is that lda and neural networks are inevitably disconnected whereas ntm can be jointly optimized with the document encoder and graph networks which can mutually improve each module wang et al


analysis of latent topics in this subsection we conduct experiments to better understand how latent topics help summarize documents
to this end we define the topical weight of a sentence as the weighted summation of attention score between each topic and the sentence i
e
cnn









model model w model dmnytarxivpubmed figure
visualized results of sentence topical weight
the degree of highlighting represents the overall relevance of the sentence and all topics
underlined sentences are model selected summary
the left document is from pubmed dataset and the right document is from cnn dm dataset
where represents the topical weight of the sentence
is the topic distribution of the document learned by ntm described in section
and represents the weight of topic in document
eq
is the attention score from the j th topic node to the i th sentence node
figure shows two examples of visualized sentence topical weights
the ground truth summary sentences have relatively high topical weights and the final selected sentences highly overlap with these topical sentences
from such observation we can have an intuitive understanding of how our model works
first our model learns sentence representations and discovers latent topics individually
second the graph attention layer builds semantic relationships between sentences and topics and then roughly selects important contents on the basis of topical information
finally our model accurately selects summary sentences by integrating all features such as the topical relevance to the document context information and inter sentence relationships
this process may explain why our model is effective for long documents
latent topics can help our model preselect salient texts thus further selection can mainly focus on these fragments rather than entire document
conclusion and future work in this paper we systematically explore the effects of latent topics for document summarization and propose a novel graph based extractive summarization model which allows joint learning of latent topics and leverages them to enrich sentence representations via a heterogeneous graph neural network
the experimental results on four well studied datasets demonstrate that our model not only achieves results on par with state of the art summarization models on news article datasets but also significantly outperforms existing approaches on scientific paper datasets indicating its strong robustness in various document genres and lengths
further explorations on incorporating more types of semantic units e

keywords and entities into document graph for enhancing the performance of summarization will be addressed in our future work
acknowledgements this work is supported by grant from the national natural science foundation of china no

we thank anonymous reviewers for their helpful comments on various aspects of this work
reference alfred v
aho and jeffrey d
ullman

the theory of parsing translation and compiling volume
prentice hall englewood cliffs nj
benjamin borschinger and mark johnson

a particle filter algorithm for bayesian word segmentation
in proceedings of the australasian language technology association workshop pages canberra australia
peng cui yuanchao liu bingquan liu

a neural topic model based on variational auto encoder for aspect extraction from opinion texts
in natural language processing and chinese computing
nlpcc
lecture notes in computer science vol
arman cohan franck dernoncourt doo soon kim trung bui seokhwan kim walter chang and nazli goharian

a discourse aware attention model for abstractive summarization of long documents
in proceedings of naacl hlt pages new orleans louisiana
association for computational linguistics
ashok k
chandra dexter c
kozen and larry j
stockmeyer

alternation
journal of the association for computing machinery
ziqiang cao furu wei wenjie li and sujian li

faithful to the original fact aware neural abstractive summarization
in aaai conference on artificial intelligence
junyoung chung caglar gulcehre kyunghyun cho and yoshua bengio

empirical evaluation of gated recurrent neural networks on sequence modeling
in nips
greg durrett taylor berg kirkpatrick and dan klein

learning based single document summarization with compression and anaphoricity constraints
arxiv preprint

jacob devlin ming wei chang kenton lee and kristina toutanova

bert pre training of deep bidirectional transformers for language understanding
in proceedings of the conference of the north american chapter of the association for computational linguistics human language technologies volume long and short papers pages minneapolis minnesota
yue dong yikang shen eric crawford herke van hoof and jackie chi kit cheung

banditsum extractive summarization as a contextual bandit
in proceedings of the conference on empirical methods in natural language processing pages
association for computational linguistics
gunes erkan and dragomir r radev

lexrank graph based lexical centrality as salience in text summarization
journal of artificial intelligence research
patrick fernandes miltiadis allamanis and marc brockschmidt
structured neural summarization

arxiv preprint

lea frermann and alexandre klementiev

inducing document structure for aspect based summarization
in proceedings of the annual meeting of the association for computational linguistics pages florence italy
association for computational linguistics
dan gusfield

algorithms on strings trees and sequences
cambridge university press cambridge uk
karl moritz hermann tomas kocisky edward grefenstette lasse espeholt will kay mustafa suleyman and phil blunsom

teaching machines to read and comprehend
in c
cortes n
d
lawrence d
d
lee m
sugiyama and r
garnett editors advances in neural information processing systems pages
curran associates inc
linmei hu tianchi yang chuan shi houye ji and xiaoli li

heterogeneous graph attention networks for semi supervised short text classification
in proceedings of the conference on empirical methods in natural language processing and the international joint conference on natural language processing pages hong kong china
association for computational linguistics
diederik kingma and jimmy ba

adam a method for stochastic optimization
in proceedings of the international conference on learning representations
diederik p kingma and max welling

autoencoding variational bayes
arxiv preprint

zhengyuan liu nancy f
chen

exploiting discourse level segmentation for extractive summarization
in proceedings of the workshop on new frontiers in summarization pages hong kong china
association for computational linguistics
yang liu and mirella lapata

text summarization with pretrained encoders
in proceedings of the conference on empirical methods in natural language processing and the international joint conference on natural language processing emnlp ijcnlp pages hong kong china
association for computational linguistics
yang liu and mirella lapata

hierarchical transformers for multi document summarization
in proceedings of the annual meeting of the association for computational linguistics pages florence italy
association for computational linguistics yishu miao edward grefenstette and phil blunsom

discovering discrete latent topics with neural variational inference
in proceedings of the international conference on machine learning icml sydney nsw australia pages
rada mihalcea and paul tarau

textrank bringing order into text
in proceedings of the conference on empirical methods in natural language processing pages
shashi narayan shay b cohen mirella lapata

do nt give me the details just the summary topic aware convolutional neural networks for extreme summarization
in proceedings of the conference on empirical methods in natural language processing pages brussels belgium
association for computational linguistics
ramesh nallapati feifei zhai and bowen zhou

summarunner a recurrent neural network based sequence model for extractive summarization of documents
in aaai conference on artificial intelligence
mohammad sadegh rasooli and joel r
tetreault

yara parser a fast and accurate dependency parser
computing research repository arxiv preprint

version
abigail see peter j
liu and christopher d
manning

get to the point summarization with pointergenerator networks
in proceedings of the annual meeting of the association for computational linguistics volume long papers pages vancouver canada akash srivastava and charles sutton

autoencoding variational inference for topic models
arxiv preprint

ilya sutskever oriol vinyals and quoc v le

sequence to sequence learning with neural networks
in advances in neural information processing systems
pages
evan sandhaus

the new york times annotated corpus
linguistic data consortium philadelphia
petar velikovi guillem cucurull arantxa casanova adriana romero pietro lio and yoshua bengio

graph attention networks
arxiv preprint

ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan n gomez ukasz kaiser and illia polosukhin

attention is all you need
in i
guyon u
v
luxburg s
bengio h
wallach r
fergus s
vishwanathan and r
garnett editors advances in neural information processing systems pages
curran associates inc
yue wang jing li hou pong chan irwin king michael r
lyu and shuming shi

topic aware neural keyphrase generation for social media language
in proceedings of the annual meeting of the association for computational linguistics pages florence italy
association for computational linguistics
danqing wang pengfei liu yining zheng xipeng qiu and xuanjing huang

heterogeneous graph neural networks for extractive document summarization
arxiv preprint

minjie wang lingfan yu da zheng quan gan yu gai zihao ye mufei li jinjing zhou qi huang chao ma ziyue huang qipeng guo hao zhang haibin lin junbo zhao jinyang li alexander j smola and zheng zhang

deep graph library towards efficient and scalable deep learning on graphs
iclr workshop on representation learning on graphs and manifolds
yang wei

document summarization method based on heterogeneous graph
in international conference on fuzzy systems and knowledge discovery pages
ieee
wen xiao and giuseppe carenini

extractive summarization of long documents by combining global and local context
in proceedings of the conference on empirical methods in natural language processing and the international joint conference on natural language processing pages hong kong china
association for computational linguistics
jiacheng xu and greg durrett

neural extractive text summarization with syntactic compression
arxiv preprint

jiacheng xu zhe gan yu cheng and jingjing liu

discourse aware neural extractive model for text summarization
arxiv preprint

michihiro yasunaga rui zhang kshitijh meelu ayush pareek krishnan srinivasan and dragomir radev

graph based neural multi document summarization
in proceedings of the conference on computational natural language learning pages vancouver canada
association for computational linguistics
xingxing zhang mirella lapata furu wei and ming zhou

neural latent extractive document summarization
in proceedings of the conference on empirical methods in natural language processing pages
association for computational linguistics
xin zheng aixin sun jing li and karthik muthuswamy

subtopic driven multi document summarization
in proceedings of the conference on empirical methods in natural language processing and the international joint conference on natural language processing pages hong kong china
association for computational linguistics
xingxing zhang furu wei and ming zhou

hibert document level pre training of hierarchical bidirectional transformers for document summarization
in proceedings of the annual meeting of the association for computational linguistics pages florence italy
association for computational linguistics
qingyu zhou nan yang furu wei shaohan huang ming zhou and tiejun zhao

neural document summarization by jointly learning to score and select sentences
in proceedings of the annual meeting of the association for computational linguistics volume long papers pages
association for computational linguistics

