towards automatic extractive text summarization of single audit reports with machine learning vivian t
chou harvard medical school boston ma usa
com leanna kent elder research inc
arlington va usa leanna

com joel a
gngora elder research inc
arlington va usa joel

com sam ballerini elder research inc
arlington va usa sam

com carl d
hoover clarkson university potsdam ny usa
edu abstract the rapid growth of text data has motivated the development of machine learning based automatic text summarization strategies that concisely capture the essential ideas in a larger text
this study aimed to devise an extractive summarization method for single audits which assess if recipients of federal grants are compliant with program requirements for use of federal funding
currently these voluminous audits must be manually analyzed by officials for oversight risk management and prioritization purposes
automated summarization has the potential to streamline these processes
analysis focused on the findings section of single audits spanning
following text preprocessing and glove embedding sentence level k means clustering was performed to partition sentences by topic and to establish the importance of each sentence
for each audit key summary sentences were extracted by proximity to cluster centroids
summaries were judged by non expert human evaluation and compared to human generated summaries using the rouge metric
though the goal was to fully automate summarization of audits human input was required at various stages due to large variability in audit writing style content and context
examples of human inputs include the number of clusters the choice to keep or discard certain clusters based on their content relevance and the definition of a top sentence
overall this approach made progress towards automated extractive summaries of audits with future work to focus on full automation and improving summary consistency
this work highlights the inherent difficulty and subjective nature of automated summarization in a real world application
keywords extractive summarization unsupervised machine learning text mining clustering single audits introduction text summarization is the process of shortening long pieces of text into a coherent summary containing only the main points of the document
summarization is a subfield of natural language processing nlp that has garnered increasing attention over recent years due to the rapid growth of based data
while automated summarization methods have been utilized since the century to generate abstracts of domain specific technical papers human written largely sufficient for many summaries have remained applications for many decades
early nlp researchers foresaw that the continued growth in knowledge would necessitate efficient and consistent summarization on a scale that could not be achieved manually
indeed the information explosion of the century has stimulated broad use of automated summarization not just on technical documents but on a far greater variety of texts both in terms of subject and format
a summary can be defined as a text that conveys important information in the original and that is no longer than half of the original and usually significantly less than that
the abstract of this paper is a simple example of a summary
summarization can be achieved through two major approaches extractive and abstractive
extractive summarization selects important sentences phrases from the original text and combines them verbatim into a summary while abstractive summarization generates sentences novo
abstraction more closely resembles human cognition but is very challenging to implement and does not necessarily produce clearly superior summaries
despite the relative simplicity of extractive summarization it can produce highly effective summaries and is widely utilized
we will thus focus on extractive summarization and any references hereafter to summarization will refer to extractive methods although many ideas are also applicable to abstractive summarization
one readily applied method of summarization is latent dirichlet allocation lda which represents a corpus as a collection of the generative topics or words
while methodologies and assumptions of lda make it very flexible its ability to produce informative summaries may be limited by its definition of a topic therefore motivating the use of alternative strategies in certain contexts
here we describe one alternative that utilizes word embedding and clustering
the bulk of this work like many nlp workflows lies in the preprocessing methodology we present a method in which sentences phrases of the are embedded into single multi dimensional mean vector representations and grouped by cosine similarity
finally for each document in the corpus the sentences phrases nearest to the centroid of each group are defined as those which are most important to the document
i
extractive summarization methodology the fundamental for any endeavor the specific summarization strategy will vary depending on the source text the desired output and available resources among other variables
mechanistic details aside at level nearly all extractive summarization approaches are built upon a fairly universal set of principles initial conversion of text to an intermediate numerical representation followed by the ranking and selection of the most important units sentences paragraphs

in practice the workflow may not break down so cleanly or discretely
for instance a single technique may accomplish both numerical representation and ranking
on the other hand any given step might encompass multiple techniques e

a combination of algorithms may be used to rank and select sentences
steps may be further subdivided e

separate ranking and selection
small variations notwithstanding virtually all summarization is predicated on these major tasks
the utilized document summarization works typical first step numerical representation is necessary to convert text to a computer readable format and calculate quantitative features for downstream algorithms
pioneering statistical calculations such as word phrase frequency sentence position the presence of key phrases term frequency inverse
nowadays these older methods remain in use though more sophisticated methods may be preferred
one popular approach is to represent words as vectors i
e
word embeddings
while the concept of word embeddings dates back to the and the most significant breakthroughs arguably occurred in with the introduction of and glove these embeddings have since become mainstays of not only summarization but of nlp
very recently novel embedding methods such as elmo bert and xlnet have provided innovative alternatives
frequency tf idf following numerical representation sentences must be ranked on the basis of importance and the top sentences are selected
if the data are labeled supervised or semi supervised machine learning can be used to train a classifier that predicts the importance of a sentence
while supervised and supervised learning can achieve very desirable results many datasets are unlabeled largely precluding these methods
it is possible to hand label the data however the amount of labeled training data needed generally makes this approach very laborious even for semi supervised methods that do not require as much labeled data
thus extractive summarization often relies instead on unsupervised learning
such approaches include graph based algorithms such as the popular textrank which has its roots in google s pagerank algorithm as well as unsupervised deep learning
clustering methods such as k means are another broad category of unsupervised methods used in summarization
ii
summary evaluation approaches beyond the task of summarization per the problem of how to evaluate the quality of the eventual output warrants careful consideration
challenges arise not only from the concrete implementation of the evaluation method but also from the broader question of what quality entails
such questions include how does one resolve the trade off between sufficient brevity and content how important are factors such as readability and coherence alongside essential elements such as grammar naturally the answers to these questions are not only subjective but also highly circumstantial as they will be influenced by the intended purpose of the summaries among other variables
the actual utility of the summary to the end user might be the most decisive factor but this information may only become available on a longer timescale
evaluation is also complicated by the general requirement for significant human input
for instance the duc and tac summarization conferences have employed human judges to grade summaries however such labor intensive approaches are not broadly practical
alternatively automated evaluation methods are implemented by packages such as recall oriented understudy for gisting evaluation or rouge and its precursor bleu which calculate precision and recall scores that reflect the quality of a summary
however rouge and similar methods may still require considerable human effort as the system generated summary must be compared to a human written reference summary to calculate the required metrics
thus the utility of rouge is somewhat conditional i
e
it may only be suitable for a corpus where each document is accompanied by a pre existing abstract as with most academic journal articles abstract like text introductory blurbs for wikipedia articles or additional metadata
rouge is also contingent on the quality of the reference summary which is itself subject to all the questions and criteria outlined above about what constitutes a summary of good quality
in recognition of these limitations alternate evaluation techniques have been developed e

using latent semantic analysis lsa to measure the similarity of system generated summaries to the original source text
ultimately the problem of evaluation remains a many headed hydra and as with any other aspect of automatic summarization the right technique is likely to be highly context specific
iii
summarization of single audits a rich resource for federal grant agencies our current work focuses on summarization of a corpus of single audits from fy to capture the qualities and activities of grant recipients detailed in the audits
there are significant financial and economic motivations for better understanding grant recipients
grants are one of the largest categories of spending by the united states government which is projected to award over billion in grants in fy
to ensure proper use of funds single audits full database at harvester
census
gov all organizations that expend at least of federal grants in a year to assess program compliance by analysis of a recipient federal award financial transactions and expenditures internal control systems and the federal assistance received during the audit period
thus the single audit represents a valuable resource for decision making and risk management by grant agencies
for instance grants managers utilize the information within single audits to decide whether or not to award a grant based on an applicant past performance or if a grant has already been oversight
how awarded when statements conducted financial perform records and are on to manually extracting information from single audits is nontrivial as the length of the audits makes it cumbersome for a human reader to locate the desired information
it would be beneficial to identify and frame the most relevant information in a compressed easily digestible format
the volume of the data presents an additional barrier
while the used in this study spans only fy it already comprises over documents
million pages and will inevitably grow over time
thus automation is not only necessary to accomplish corpus wide analyses on a reasonable timescale but also to ensure consistent and unbiased outcomes
a
related work on single audits the current work builds upon an earlier project at elder research inc
which devised an automated method to quantify the severity of an audit and an associated numerical risk score
to this end single audits from were downloaded to form a corpus
while most audits are pages long and some are hundreds of pages the pertinent information is contained in the section titled schedule of findings and questioned costs which typically spans a few pages
a method was devised to extract only the relevant findings pages which comprise
of the pages in the corpus
this enables the analysis to focus solely on the most essential data
sentiment analysis of the extracted findings was performed to generate a risk score for each audit report
altogether this work makes it possible to quickly assess grantees through a straightforward intuitive metric and facilitates prioritization of the grantees of most interest e

those deemed most risky
b
motivation for and overview of current approach the previous work by elder research offers a powerful method to answer the question of who is high risk and raises the natural question of why these entities are high risk
here we attempt to address this latter question of why by extracting the text that provides explanations
the choice to answer why through automatic summarization as opposed to simply using manual summaries is driven by the volume of data
our corpus of selected audits already comprises documents and the problem only exacerbates when one considers the rest of the audits in existence to say nothing of the new documents that will be added year after year
even if one were only interested in a much smaller subset of documents for which manual summarization is reasonable it is unlikely that a human or group of humans would maintain consistent quality and objectivity over so many documents
for our current approach the desired output for each document is a summary consisting of the that best illustrate the broader context
below are examples of sentences that would be desirable to extract because they help elucidate the reasoning behind a negative assessment of a grantee grantee was unable to provide proper documentation
a student at the school was missed and never corrected
the cause stemmed from turnover within staff and ultimately from a lapse in procedures
once the relevant documents were accessed the data were prepared for modeling
preparation entailed both standard and specific text preprocessing steps followed by word and sentence embeddings using pre trained glove vectors
sentence vectors from the entire corpus were pooled and used to train a k means clustering model which was iteratively optimized
summaries for each document were generated by extracting important and subsequently evaluated using both human and system metrics
the sentences deemed most our initial task was to extract the findings sections and exclude all other pages in the audits to reduce noise
this a excluded a substantial number of entire reports that did not contain findings which typically pertained to high performing grantees that did not elicit critique the was thus reduced from full length reports to abbreviated documents containing only findings
any reference hereafter to document will refer to the abbreviated documents
while this filtering approach helps simplify the problem it may induce a selection bias in subsequent analysis that can only be evaluated with out of sample performance testing
iv
data exploration each document was initially represented as a single string which was tokenized i
e
divided or split on a or word level as needed
to ensure that all canonically equivalent strings have the same binary representation nfkd unicode normalization was performed on all text prior to preprocessing
to gain an intuition for the corpus simple univariate analysis of document lengths was performed
first each string for each document was divided into its constituent sentences the natural language toolkit nltk sentence using tokenizer note that tokenization at this stage was only for the purposes of measuring document length and that the tokenized string was used for eventual preprocessing
sentence counts were analyzed yielding some notable observations the mean document length was sentences
the corpus is skewed towards shorter documents the mode was sentences followed by and
of documents were sentences or shorter

of documents were extra long sentences
the longest document was sentences
we randomly selected and read a fixed number of documents from each length bracket short medium long extra long to gain a sense for organization and content as well as orthography and stylistic conventions
some observations most documents contained brief filler sentences such page headers footers e

state of new mexico denver housing center section headings e

condition cause and page numbers
many documents contained expository statements e

historical background not relevant to risk
such statements predominated in longer documents and dilute the relevant information information was more concentrated in short medium documents
the shortest documents corresponded to grantees that had satisfactory audits and might only contain sentences e

no findings or questioned costs
stage removal criteria roman numerals there was a lack of universal formatting which proved challenging during analysis
section headings could appear as cause or cause and inline or separately lists might appear as
or
a b parenthetical phrases extra whitespace non alphanumeric regular expression
za table ii
regular expressions for text cleaning v
text preprocessing we performed a combination of standard and specific preprocessing table i to ensure predictable and consistent results
standard preprocessing reduces noise associated with virtually any text data such as punctuation and frequently occurring stopwords
due to the specialized nature of single audits our also contained idiosyncrasies in diction syntax format and structure that warranted additional preprocessing modification of the standard steps
manual preprocessing occurred in two major stages fig

stage a removed clearly irrelevant sentences but preserved orthography to generate human readable intermediate text
stage b produced fully processed sentences for embedding
thus while ranking selection was performed on the processed sentences the corresponding intermediate sentences were used to produce a readable summary output
a
initial preprocessing to generate readable intermediates at the outset each document was represented by a single string
page headers and section headings were removed based on length we looked for all substrings that occurred between two newline breaks and removed substrings less than characters
to avoid removal of dangling lines we only removed short substrings that started with an uppercase letter or a non alphabetical character
each document was then divided using the nltk sentence tokenizer
to further isolate section headings each sentence that contained a colon or dash was further split at that character
additional elements were removed using regular expressions regex table ii
table i
standard vs
corpus specific preprocessing standard preprocessing no stemming lemmatization or gram modeling performed convert text to lower case remove extra whitespace remove stopwords ntlk list punctuation symbols corpus specific remove page headers and short sentences remove roman numerals remove parenthetical phrases keep negation stopwords no not fig

breakdown of preprocessing stages and subsequent steps
each document was stored as list of strings identifiable by index
the intermediate text still contained typical english orthography e

capitalization punctuation and stopwords
though these sentences were not suitable for embedding they remained easily readable and were retained for later use
b
final touches and sanity check we next the entire lower cased text and removed punctuation using regex table ii
the final step stopword removal was performed in tandem with glove embedding see next section
stopword removal can be accomplished using the predefined nltk stopword list
for this corpus one key modification was to retain the words no and not
while these words may not add value to certain corpora they were important for this corpus because audits often focus on the lack or failure to fulfill a certain requirement these words were also important to determining if no problems were detected
after preprocessing documents of the corpus had zero length i
e
became empty lists
to ensure that preprocessing was not removing key information the original vs
preprocessed text lengths were compared
the rationale for was that if the preprocessing were functioning properly empty lists should originate from documents that were short had little substantial content
as expected the documents that produced empty lists were sentences or shorter originally and were sentences long on average
even the longest of these documents contained only filler text such as headers
these results indicated that preprocessing was sensible and that empty lists were valid reflections of the original data rather than artifacts of over exuberant preprocessing
though necessary in this case all such filtering of information is ideal for an automated process and may induce selection bias
the remaining documents of the stayed non empty after preprocessing
interestingly of the corpus showed increased sentence counts likely from splitting of longer sentences
this was unlikely to be problematic since information was merely partitioned and not lost
regardless this length increase was uncommon and of the corpus showed reduced length following preprocessing with the cleaned text about the sentence count of the original text
vi
word sentence embedding with glove upon completion of preprocessing stages a and b the fully cleaned was ready for embedding
the glove method was chosen based on its recognized effectiveness across many nlp tasks as well as prior success in using glove with this corpus
the ease and speed of implementation of trained vectors was also a decisive factor due to time constraints
a natural extension would be to use methods such as bert or custom glove that require modifications
the glove

zip file containing vectors trained on the english language wikipedia gigaword was from nlp
stanford
edu projects
we downloaded obtained individual word tokens for word level embedding by splitting sentences at each whitespace
for each non stopword word the appropriate word vector was retrieved
a single vector for each sentence was then obtained by taking the arithmetic mean across each element of the constituent word vectors
a vector of all zeros was automatically assigned to the very rare sentences comprised solely of stopwords or an empty string this occurred in only
of the sentences out of
this entire embedding process and subsequent k means clustering see next section was performed using both and vectors
since vectors conferred no obvious advantage the less intensive vectors were used for the remainder of the work
vii
sentence ranking selection by k means both ranking and selection of the vectorized sentences were accomplished by k means clustering which was chosen for its intuitive interpretation in this context
the algorithm partitions n observations i
e
sentences n into k clusters i
e
topics by assigning each observation to the nearest centroid fig

the centroids and clusters are iteratively updated to minimize intra cluster variation i
e
the within cluster sum of squares
the importance of each sentence correlates to its proximity to its cluster centroid allowing identification of the top sentences per cluster topic fig
and overall fig

a
implementation details and hyperparameter tuning to train the k means model on the entire at once vectorized sentences were pooled and stored in a flattened list
each vector was mapped back to its text representation based on its unique index in the list
clustering was performed using both the scikit learn and nltk implementations of k means
selection of cosine similarity as the distance metric a key k means hyperparameter is the distance metric used in assigning centroids and clusters
while euclidean distance is a common default cosine similarity is preferred for text and therefore was our metric of choice for all modeling and calculations
the scikit learn k means implementation does not allow ready use of a metric besides euclidean distance
to circumvent this we trained our scikit learn k means model on vectors normalized to unit length based on the principle that squared euclidean distance and cosine similarity are for unit vectors
because the nltk k means implementation allows specification of the distance metric normalization was only necessary for the scikit learn model
denote squared euclidean distance and the cosine metric y by expansion of squared euclidean distance y yty yty y fig

simplified visualization of k means clustering to rank and select sentences
color coded clusters correlate to distinct topics
top sentences were selected by a importance within individual topic or b
selection of cluster number the other major hyperparameter was k representing the number of clusters topics
we first utilized the elbow method whereby k means is run for a range of k and the sum of squared errors for each model is plotted as a function of k
to reduce spatial temporal costs the scikit learn mini batch means model was run on
the resulting plot indicated an optimal k around
these values were tested empirically by running the normal scikit learn k means model on cluster sizes of and with performing the best
ranking and selection of sentences the k means algorithm outputs k centroids which represent the means of all observations in a given cluster
the distance of each observation to its respective centroid can then be calculated
we thus generated a matrix of observation centroid distances
for a given document top sentences were selected by cluster fig
or by overall proximity fig

these processes can be described algorithmically top sentences by cluster
consider all sentences in cluster
if no sentences belong in this cluster go on to the next cluster else rank sentences by proximity to cluster centroid and save the index of the top
repeat for clusters k
the result is a list of sentence indices of length or less
top sentences overall
begin with the matrix storing the proximity of each sentence in the document to its cluster centroid
sort sentences by distance
select top j sentences
the result is a list of sentence indices of length j or the document length whichever is smaller
j was roughly scaled to the original document length sentences j rounding up
sentences j
sentences j
sentences j rounding up
these values were chosen empirically to attempt to balance sufficient information with brevity and to obtain summaries roughly the same length as those obtained by the per cluster method
the output for each document was a list of indices and the output for the was a list of lists
summaries were generated by extracting the intermediate sentence see preprocessing stage a corresponding to each of the indices
b
preliminary results and filtering of clusters by relevance as desired a text summary was successfully generated for each document following these steps
preliminary evaluation via reading randomly sampled summaries revealed that despite preprocessing summaries still contained filler text such as page headers or section headings
the persistence of such text despite preprocessing was likely due to inconsistent formatting across documents preventing detection of filler text by established criteria
summaries also contained extraneous text that were valid sentences and therefore would not have been filtered out by preprocessing but that nonetheless added little value to the summaries
these sentences included but were not limited to logistical information such as lists of dates and the amounts of individual financial transactions while this information may have value in certain contexts they do not greatly improve understanding of grantee risk
it would be preferable to omit such sentences from the eventual summaries
to reduce the amount of extraneous information in the summaries the clusters were selectively discarded based on content
two criteria were used to filter the clusters cluster topic determined by examining the sentences closest to the centroid
uniformity of content determined by examining a random sample of sentences from each cluster
ultimately out of clusters produced with both learn and nltk models were discarded leaving useful clusters of the original
for certain clusters this decision was straightforward for instance both scikit learn and nltk models produced one cluster out of that contained exclusively page headers and section headings as well as a cluster that contained almost exclusively dates
other discarded clusters were characterized by excessive financial detail e

dollar amounts of transactions overly long sentences and sentences that are part of the standard verbiage of audits but that do not add information specific to individual grantees
the choice to keep or discard certain clusters was in some cases not straightforward
while some clusters were almost entirely composed of irrelevant text this was not universal the content of most of clusters was far from uniform
clusters which contained predominantly irrelevant information would also contain a non trivial quantity of relevant similarly clusters that were useful as a whole would still contain many sentences of little value
arguably discarding any but the most unambiguous clusters risks significant loss of information
this is further complicated by the question of what is relevant or irrelevant
for instance we decided that sentences stating exact dollar amounts of transactions were excessively detailed
however it could be argued that financial details do provide clues to a grantee s risk and should be retained
in short cluster filtering is a challenging step that is rife with ambiguities and trade offs and the current manual approach has much potential for improvement such as a more thorough analysis of and inter cluster distances and of cluster density via silhouette scores
given the major effect of cluster filtering on summary quality see next section greater investment on examining the different clusters and on methods to prioritize information would likely have significant benefit
fig

distinct summaries produced by different combinations of k means implementations and sentence selection methods
throughout the above steps the text output was evaluated by ad read throughs by a graduate level data analyst not a domain expert in audits
even from preliminary evaluation it was apparent that removal of irrelevant clusters significantly changed the summary content
despite concerns about information loss the summaries as a whole appeared to have a greater proportion of relevant content
thus this change was implemented despite potential trade offs
to mitigate possible loss of information from the per cluster method due to the reduction of available clusters from to two sentences rather than one were chosen per cluster the number of sentences selected overall still followed the same criteria
four sets of summaries were ultimately generated fig

these summaries were evaluated both in isolation for their overall quality as well as relative to each other to determine if any method conferred obvious advantages
to this end medium length documents sentences prior to summarization were chosen randomly
these documents were then evaluated both manually and using the rouge metric
in general the four summary sets appeared comparable with the per cluster methods performing slightly better fig
b there was no conspicuous advantage to either the learn fig
c or the nltk implementations fig
d
viii
human evaluation manual evaluation was performed with these criteria brevity
were the summaries of a reasonable length for high throughput reading while the selection criteria imposed a length limit summaries might still be longer than desired
for instance a sentence summary is not impressive if the original text was sentences
occasional very long sentences also inflated summary lengths in an unpredictable way
information
did the summary contain enough information to understand grantee risk did the summary successfully omit unnecessary information explicit wording
how transparent and accessible was the information besides these benchmark questions no formal rubric or scoring system was used thus human evaluation relied largely on an intuitive sense of good and poor
we found that the sampled summaries were of varying quality
best case summaries fulfilled all three above criteria
worst case summaries were composed entirely of irrelevant information such summaries would be functionally useless no matter how well they fulfilled any other criteria
most commonly our summaries were functional even if not optimal
three broad scenarios not mutually exclusive were apparent multiple findings
a single summary contained multiple distinct findings resulting in a logically confusing text more details in final section
dilution of information
the desired information was present but was hidden among sentences that were not desired thus requiring the reader to actively filter out the unnecessary information
oblique phrasing
ideally the desired information would be stated explicitly e

the grantee was compliant because some action was not performed
in practice summaries contained statements such as it is recommended that grantees perform some action to be compliant
it is possible to infer the former statement from the information in the latter but this would require additional effort on the part of the reader which may or may not be tenable
summaries pertaining to the latter two scenarios might be acceptable since they still contain the desired information with the caveat that they would likely yield a suboptimal user experience
the first scenario of multiple findings see final section for more detail is perhaps the most problematic because of the potential for missing or misleading information
due to the significant challenges of separating documents by findings the problem of multiple findings was not resolved in this work but would be a valuable future endeavor
ix
rouge evaluation the summaries evaluated manually were also evaluated using rouge
for each summary a human reference was written
overlaps between the system generated summaries and corresponding ideal human reference were then calculated
to reduce noise in the calculations the above mentioned preprocessing steps of stopword filtering punctuation removal and lower casing were performed on both the human and system summaries stemming was also done using the nltk porterstemmer
rouge produced two key metrics recall
how much of the human reference summary was captured by the system summary precision
how much of the system summary was actually needed or relevant despite the popularity and general effectiveness of the rouge method for this particular application human evaluation was considered more informative than rouge
some shortcomings of rouge could be attributed to the general pitfalls of the method although a few additional issues were noted
on the whole the recall and precision scores were poor compared to what is typically desired making it difficult to confidently assess summary quality
another shortcoming was that only one set of reference summaries was available which constricted the definition of ideal
in reality there is rarely a single ground truth and a text can have many valid summaries thus the use of multiple reference summaries written by different people would likely improve rouge evaluation
finally since rouge was only performed on
of the and moreover limited to documents of a certain length sentences it might be ill advised to extrapolate these results to any significant extent
we have provided potential future directions based on our current assessment of our method
it will also be invaluable to solicit expert user feedback to ensure that our method achieves not only technical soundness but also its desired functionality
x
short term modifications these comparatively minor modifications do not radically change the overarching logic of the approach
a
word embeddings we have noted several alternatives to glove see methods section
while it is difficult to precisely predict the results of any method and how much they would improve the output if at all we speculate that customized embeddings would better capture the specialized language of audits
the glove vectors currently in use were trained on wikipedia and gigaword a newswire corpus audit documents differ greatly from both these texts
thus it is likely that the words meanings and contexts captured by the pre trained glove vectors do not fully reflect the usage of the words in audits
custom vectors e

lda vector representations may alleviate this problem
b
tailor cluster number and filtering to document length we built and uniformly applied a single k means model to all documents in the corpus
however this is almost certainly not optimal
a k of corresponding to topics might be a decent approximation of the number of topics for an average length document of sentences it is unlikely to be the appropriate number for a document of nearly sentences
in some cases more clusters are likely needed to capture the different topics
furthermore the current method uses a narrow range of output lengths for a broad range of raw texts
while a sentence summary might be desired for a document of sentences it is barely a summary if the original was sentences and is too brief for a document that is hundreds or thousands of sentences
in the future we will train multiple models and also explore other clustering algorithms including density based and hierarchical approaches
xi
long term summarize by distinct findings as noted above one problematic scenario was when into a single multiple distinct findings were collapsed summary
in such a situation the summary was to the human reader seemingly illogical and disjointed
this occurs because summaries are currently generated per document and a single document may have multiple distinct findings
intellectually these findings are effectively separate despite originating from the same grant recipient as they concern independent largely non overlapping causes effects
some possible consequences of condensing multiple findings into one summary include misleading information
for example the cause of one finding may be attributed to the effect of a separate finding
oftentimes there is not enough context in the summary to detect when this happens
lack of context
summaries become spread too thin over multiple findings
for instance some findings require multiple sentences to be fully explained
a shortcoming of the current method is that only one of several relevant sentences may be selected
compared to other imperfect summary outcomes the issue the potential for is particularly noteworthy because of misinformation
a summary that reads awkwardly could still be considered successful if its information is relevant or accurate and a summary with no useful information is usually clearly identifiable as being non functional
however the misinformation that can arise from mixed findings is not always easy to detect
to address these issues a potential future direction could be to extract distinct findings from each audit document and to generate a single summary for a single class of findings
this approach would have the added advantage of addressing some of the concerns related to length outlined above
realistically extracting findings is a complex and difficult task that would require investment of significant time and effort but if successful would likely have a noticeable positive effect on summarization
xii
conclusion overall this approach made progress towards automated extractive summaries of audits using custom text preprocessing glove embedding k means clustering and several selection heuristics
this strategy successfully extracted summaries in a technically sound algorithmic manner from a large volume of federal grant audits
future aims include greater automation of the more exploratory steps that involved human in the loop criteria
other aims include improving the consistency of the results and further validation both technically and in a practical setting
this work highlights the inherent difficulty and subjectivity of machine learning based automated summarization in a real world application and demonstrates the value of reducing non automated steps reducing validation subjectivity and assessing true out sample results with expert input in order to improve output
acknowledgment the authors would like to thank robert han and ryan mcgibony for guidance and feedback
v
t
c
would also like to thank her ph
d
advisor david van vactor at harvard medical school for supporting her pursuit of this project separate from her dissertation
this work was funded by elder research as an internal research and development project
references h
p
luhn the automatic creation of literature abstracts ibm j
res
dev
vol
pp

h
p
edmundson new methods in automatic extracting j
acm vol
pp

p
b
baxendale machine made index for technical literature an experiment ibm j
res
dev
vol
pp

j

torres moreno automatic text summarization front matter in automatic text summarization pp
i xxiii
d
r
radev e
hovy and k
mckeown introduction to the special issue on summarization comput
linguist
vol
pp

u
hahn and i
mani the challenge of automatic summarization computer long
beach
calif
november pp

d
m
blei a
y
ng and m
i
jordan latent dirichlet allocation j
mach
learn
res
vol
pp

k
s
jones a statistical interpretation of term specificity and its application in retrieval j
doc
vol
pp

r
collobert and j
weston general deep architecture for nlp icml
pdf
y
bengio r
ducharme p
vincent and c
jauvin a neural probabilistic language model j
mach
learn
res
pp
aug

t
mikolov k
chen g
corrado and j
dean distributed representations of words and phrases and their compositionality neural inf
process
syst
vol
pp

t
mikolov k
chen g
corrado and j
dean efficient estimation of word representations in vector space pp

j
pennington r
socher and c
manning glove global vectors for word representation in proc
conf
empir
methods nat
lang
process
emnlp vol
pp

m
peters al
deep contextualized word representations in proc
conf
north american chapter assoc
comput
linguist human language technologies volume long papers pp

j
devlin m

chang k
lee and k
toutanova bert for language training of deep bidirectional transformers understanding arxiv prepr
oct

z
yang z
dai y
yang j
carbonell r
salakhutdinov and q
v
le xlnet generalized autoregressive pretraining for language understanding arxiv prepr
pp

k

wong m
wu and w
li extractive summarization using supervised and semi supervised learning proc
int
conf
comput
linguist
vol
pp

r
mihalcea and p
tarau textrank bringing order into texts proc
conf
empir
methods nat
lang
process
pp

l
page and s
brin the anatomy of a large scale hypertextual web search engine vol

s
verma and v
nidhi extractive summarization using deep learning arxiv prepr
aug

m
yousefi azar and l
hamey text summarization using unsupervised deep learning expert syst
appl
vol
pp
feb

j
macqueen some methods for classification and analysis of multivariate observations proc
berkeley symp
math
stat
probab
vol
pp

d
miller leveraging bert for extractive text summarization on lectures arxiv prepr

n
renu and kunal review on opinion data summarization using k means clustering and latent semantic analysis int
j
res
sci
technol
pp

s
twinandilla s
adhy b
surarso and r
kusumaningrum multi document summarization using k means and latent dirichlet allocation lda significance sentences procedia comput
sci
vol
pp

a
agrawal and u
gupta extraction based approach for text summarization using k means clustering int
j
sci
res
publ
vol
pp

m
r
prathima and h
r
divakar automatic extractive text summarization using k means clustering int
j
comput
sci
eng
vol
pp
jun

h
j
jain m
s
bewoor and s
h
patil context sensitive text summarization using k means clustering algorithm int
j
soft comput
eng
vol
pp

c

lin rouge a package for automatic evaluation of summaries assoc
comput
linguist
pp

k
papineni s
roukos t
ward and w

zhu bleu a method for automatic evaluation of machine translation in proc
annu
meeting assoc
comput
linguist
acl vol
pp

j
steinberger and k
jeek evaluation measures for text summarization comput
informatics vol
pp


