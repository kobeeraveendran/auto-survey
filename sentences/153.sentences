a m l c
s c v
v i x r a toward abstractive summarization using semantic representations fei liu jeffrey flanigan sam thomson norman sadeh noah a
smith school of computer science carnegie mellon university pittsburgh pa usa feiliu jflanigan sthomson sadeh
cmu
edu abstract we present a novel abstractive summarization framework that draws on the recent ment of a treebank for the abstract meaning representation amr
in this framework the source text is parsed to a set of amr graphs the graphs are transformed into a summary graph and then text is generated from the summary graph
we focus on the graph graph transformation that reduces the source semantic graph into a summary graph ing use of an existing amr parser and ing the eventual availability of an amr text generator
the framework is data driven trainable and not specically designed for a particular domain
experiments on standard amr annotations and system parses show promising results
code is available at
com summarization introduction abstractive summarization is an elusive ical capability in which textual summaries of tent are generated
demand is on the rise for high quality summaries not just for lengthy texts e

books bamman and smith and texts known to be prohibitively difcult for people to derstand e

website privacy policies sadeh et al
but also for non textual media e

videos and image collections kim et al
kuznetsova et al
zhao and xing where extractive and compressive summarization techniques simply do not sufce
we believe that the challenge of stractive summarization deserves renewed attention and propose that recent developments in semantic analysis have an important role to play
we conduct the rst study exploring the bility of an abstractive summarization system based on transformations of semantic representations such as the abstract meaning representation amr narescu et al

example sentences and their amr graphs are shown in fig

amr has much in common with earlier formalisms kasper dorr et al
today an annotated corpus prised of over amr analyzed english tences knight et al
and an automatic amr parser jamr flanigan et al
are available
in our framework summarization consists of three steps illustrated in fig
parsing the put sentences to individual amr graphs bining and transforming those graphs into a single summary amr graph and generating text from the summary graph
this paper focuses on step treating it as a structured prediction problem
we assume text documents as and use jamr for step
we use a simple method to read a bag of words off the summary graph allowing evaluation with and leave full text generation from amr step to future work
the graph summarizer described in rst merges amr graphs for each input sentence through a concept merging step in which coreferent nodes of the graphs are merged a sentence conjunction step which connects the root of each sentence s amr graph to a dummy root node and an optional principle the framework could be applied to other puts such as image collections if amr parsers became able for them
concepts can be english words dog propbank event predicates or special keywords person
for example resents a propbank roleset that corresponds to the rst sense of chase
according to banarescu et al
amr uses approximately relations
the rolesets and core semantic relations e

to are adopted from the propbank annotations in ontonotes hovy et al

other semantic lations include location mode name time and topic
the amr provide more detailed descriptions
banarescu et al
scribe amr bank a sentence corpus tated with amr by experts
step of our framework converts input document sentences into amr graphs
we use a statistical mantic parser jamr flanigan et al
which was trained on amr bank
jamr s current mance on our test dataset is f
we will analyze the effect of amr parsing errors by paring jamr output with gold standard annotations of input sentences in the experiments
in addition to predicting amr graphs for each sentence jamr provides alignments between spans of words in the source sentence and fragments of its predicted graph
for example a graph fragment headed by date entity could be aligned to the kens april
we use these alignments in our simple text generation module step
dataset to build and evaluate our framework we require a dataset that includes inputs and summaries each with gold standard amr annotations
this allows us to use a statistical model for step graph rization and to separate its errors from those in step amr parsing which is important in determining whether this approach is worth further investment
fortunately the proxy report section of the amr bank knight et al
suits our needs
a
isi
amr amr guidelines
pdf parse quality is evaluated using smatch cai and knight which measures the accuracy of concept and lation predictions
jamr was trained on the in domain training portion of for our experiments
multi document summarization datasets such as the ones used in duc and tac competitions do not have gold standard amr annotations
figure a toy example
sentences are parsed into vidual amr graphs in step step conducts graph formation that produces a single summary amr graph text is generated from the summary graph in step
graph expansion step where additional edges are added to create a fully dense graph on the level
these steps result in a single connected source graph
a subset of the nodes and arcs from the source graph are then selected for inclusion in the summary graph
ideally this is a condensed sentation of the most salient semantic content from the source
we briey review amr and jamr then present the dataset used in this paper
the main algorithm is presented in and we discuss our ple generation step in
our experiments sure the intrinsic quality of the graph transformation algorithm as well as the quality of the terms selected for the summary using
we explore variations on the transformation and the learning gorithm and show oracle upper bounds of various kinds
background abstract meaning representation and jamr amr provides a whole sentence semantic sentation represented as a rooted directed acyclic graph fig

nodes of an amr graph are labeled with concepts and edges are labeled with relations
a i saw joe s dog which was running in the garden
sentence b the dog was chasing a cat
joe s dog was chasing a cat in the garden
docs
source graph ave
sents
summ
doc
nodes edges expand





train dev
test table statistics of our dataset
expand shows the number of edges after performing graph expansion
the numbers are averaged across all documents in the split
we use the ofcial split dropping one training document for which no summary sentences were annotated
proxy report is created by annotators based on a gle newswire article selected from the english gaword corpus
the report header contains metadata about date country topic and a short summary
the report body is generated by editing or rewriting the content of the newswire article to approximate the style of an analyst report
hence this is a single ument summarization task
all sentences are paired with gold standard amr annotations
table vides an overview of our dataset
graph summarization given amr graphs for all of the sentences in the put step graph summarization transforms them into a single summary amr graph step
this is accomplished in two stages source graph tion
and subgraph prediction


source graph construction the source graph is a single graph constructed ing the individual sentences amr graphs by ing identical concepts
in the amr formalism an entity or event is canonicalized and represented by a single graph fragment regardless of how many times it is referred to in the sentence
this ple can be extended to multiple sentences ideally resulting in a source graph with no redundancy
cause repeated mentions of a concept in the input can signal its importance we will later encode the frequency of mentions as a feature used in subgraph prediction
concept merging involves collapsing certain graph fragments into a single concept then merging all concepts that have the same label
we collapse the graph fragments that are headed by either a entity date entity or a named entity name if figure graph fragments are collapsed into a single concept and assigned a new concept label
the fragment is a at structure
a collapsed named entity is further combined with its parent e

son into one concept node if it is the only child of the parent
two such graph fragments are trated in fig

we choose named and date entity concepts since they appear frequently but most ten refer to different entities e

april vs
nov

no further collapsing is done
a collapsed graph fragment is assigned a new label by concatenating the consisting concept and edge bels
each fragment that is collapsed into a new cept node can then only be merged with other tical fragments
this process wo nt recognize erent concepts like barack obama obama and but future work may porate both entity coreference resolution and event coreference resolution as concept nodes can sent either
due to the concept merging step a pair of cepts may now have multiple labeled edges between them
we merge all such edges between a given pair of concepts into a single unlabeled edge
we ber the two most common labels in such a group which are used in the edge label feature table
to ensure that the source graph is connected we add a new root node and connect it to every cept that was originally the root of a sentence graph see fig

when we apply this procedure to the documents in our dataset source graphs contain nodes and edges on average
we investigated how well these automatically constructed source graphs cover the gold standard summary graphs produced by amr annotators
ally a source graph should cover all of the standard edges so that summarization can be complished by selecting a subgraph of the source dayyeardate pansion increases the average number of edges by a factor of to
fig
illustrates the vation
document level expansion covers the standard summary edge garden yet the expansion is computationally prohibitive sentence level expansion adds an edge dog garden which enables the prediction of a ture with similar semantic meaning joe s dog was in the garden chasing a cat

subgraph prediction we pose the selection of a summary subgraph from the source graph as a structured prediction lem that trades off among including important formation without altering its meaning ing brevity and producing uent language nenkova and mckeown
we incorporate these cerns in the form of features and constraints in the statistical model for subgraph selection
let g v e denote the merged source graph where each node v v represents a unique cept and each directed edge e e connects two concepts
g is a connected directed node labeled graph
edges in this graph are unlabeled and edge labels are not predicted during subgraph selection
we seek to maximize a score that factorizes over graph nodes and edges that are included in the mary graph
for subgraph v vv where v and are the feature representations of node v and edge e respectively
we describe node and edge features in table
and are vectors of empirically estimated coefcients in a linear model
we next formulate the selection of the subgraph using integer linear programming ilp

and describe supervised learning for the parameters efcients from a collection of source graphs paired with summary graphs




decoding we cast decoding as an ilp whose constraints sure that the output forms a connected nent of the source graph
we index source graph concept nodes by i and j giving the root node figure a source graph formed from two sentence amr graphs
concept collapsing merging and graph expansion are demonstrated
edges are unlabeled
a root node is added to ensure connectivity
and are among edges added through the optional sion step corresponding to and document level expansion respectively
concept nodes included in the summary graph are shaded
summary edge coverage expand sent
doc
labeled unlabeled train dev
test











table percentage of summary edges that can be ered by an automatically constructed source graph
graph

in table columns one and two port labeled and unlabeled edge coverage
beled counts edges as matching if both the source and destination concepts have identical labels but ignores the edge label
in order to improve edge coverage we explore expanding the source graph by adding every ble edge between every pair of concepts within the same sentence
we also explored adding every sible edge between every pair of concepts in the tire source graph
edges that are newly introduced during expansion receive a default label null
we report unlabeled edge coverage in table columns three and four respectively
subgraph prediction became infeasable with the document level sion so we conducted our experiments using only sentence level expansion
sentence level graph graph a i saw joe s dog which was running in the garden
sentence b the dog was chasing a cat
node features edge features concept freq depth position span entity bias identity feature for concept label concept freq in the input sentence set one binary feature dened for each frequency threshold average and smallest depth of node to the root of the sentence graph binarized using depth thresholds average and foremost position of sentences containing the concept binarized using position thresholds average and longest word span of concept binarized using length thresholds word spans obtained from jamr two binary features indicating whether the concept is a named entity date entity or not bias term for any node first and second most frequent edge labels between concepts relative freq of each label binarized by thresholds edge frequency label non expanded edges in the document sentences binarized using frequency thresholds average and foremost position of sentences containing the edge without label binarized using position thresholds node features extracted from the source and target nodes all above node features except the bias term label freq position nodes isexpanded a binary feature indicating the edge is due to graph expansion or not edge freq label all occurrences bias bias term for any edge table node and edge features all binarized
index
let n be the number of nodes in the graph
let vi and ei j be binary variables
vi is iff source node i is included ei j is iff the directed edge from node i to node j is included
the ilp objective to be maximized is equation rewritten here in the present notation n vi i node score i ei j j edge score note that this objective is linear in vi ei j and that features and coefcients can be folded into node and edge scores and treated as constants during coding
constraints are required to ensure that the selected nodes and edges form a valid graph
in particular if an edge i j is selected ei j takes value of then both its endpoints i j must be included vi ei j vj ei j i n connectivity is enforced using a set of commodity ow variables fi j each taking a negative integral value representing the ow from node i to j
the root node sends out up to n units of ow one to reach each included node equation
each included node consumes one unit of ow ected as the difference between incoming and going ow equation
flow may only be sent over an edge if the edge is included equation
i vi i i fi j fj k vj j n the amr representation allows graph cies concept nodes having multiple parents yet reentrancies are rare about of edges are entrancies in our dataset
in this preliminary study we force the summary graph to be tree structured requiring that there is at most one incoming edge for each node j ei j n
interestingly the formulation so far equates to an ilp for solving the prize collecting steiner tree problem pcst segev which is known to be np complete karp
our ilp tion is modied from that of ljubic et al

flow based constraints for tree structures have also previously been used in nlp for dependency ing martins et al
and sentence sion thadani and mckeown
in our iments we use an exact ilp though many approximate methods are available
finally an optional constraint can be used to x the size of the summary graph measured by the number of edges to l i j ei j l the performance of summarization systems depends strongly on their compression rate so systems are only directly comparable when their compression rates are similar napoles et al

l is supplied to the system to control summary graph size
n ei fi j i n

gurobi
com

parameter estimation experiments given a collection of input and output pairs here source graphs and summary graphs a natural ing place for learning the coefcients and is the structured perceptron collins which is easy to implement and often performs well
tively incorporating factored cost functions through a structured hinge loss leads to a structured support vector machine svm taskar et al
which can be learned with a very similar stochastic mization algorithm
in our scenario however the gold standard summary graph may not actually be a subset of the source graph
in machine tion ramp loss has been found to work well in ations where the gold standard output may not even be in the hypothesis space of the model gimpel and smith
the structured perceptron hinge and ramp losses are compared in table
we explore learning by minimizing each of the perceptron hinge and ramp losses each optimized using adagrad duchi et al
a stochastic timization procedure
let be one model parameter coefcient from or
let be the ent of the loss on the instance considered on the tth iteration with respect to
given an initial step size the update for on iteration t is generation generation from amr like representations has ceived some attention e

by langkilde and knight who described a statistical method
though we know of work in progress driven by the goal of machine translation using amr there is currently no system available
we therefore use a heuristic approach to ate a bag of words
given a predicted subgraph a system summary is created by nding the most quently aligned word span for each concept node
recall that the jamr parser provides these ments
the words in the resulting spans are generated in no particular order
while this is not a natural language summary it is suitable for unigram based summarization evaluation methods like
in table we report the performance of subgraph prediction and end to end summarization on the test set using gold standard and automatic amr parses for the input
gold standard amr annotations are used for model training in all conditions
during testing we apply the trained model to source graphs constructed using either gold standard or jamr parses
in all of these experiments we use the ber of edges in the gold standard summary graph to x the number of edges in the predicted subgraph allowing direct comparison across conditions
subgraph prediction is evaluated against the standard amr graphs on summaries
we report cision recall and for nodes and for edges
oracle results for the subgraph prediction stage are obtained using the ilp decoder to minimize the cost of the output graph given the gold standard
we assign wrong nodes and edges a score of correct nodes and edges a score of then decode with the same structural constraints as in subgraph prediction
the resulting graph is the best summary graph in the hypothesis space of our model and provides an upper bound on performance able within our framework
oracle performance on node prediction is in the range of when using gold standard amr annotations and when ing jamr output
edge prediction has lower mance yielding
for gold standard and
for jamr parses
when graph expansion was plied the numbers increased to and
spectively
the uncovered summary edge i
e
those not covered by source graph is a major source for low recall values on edge prediction see table graph expansion slightly alleviates this issue
summarization is evaluated by comparing tem summaries against reference summaries using scores lin
system summaries are generated using the heuristic approach presented in given a predicted subgraph the approach nds the most frequently aligned word span for each cept node and then puts them together as a bag of words
is particularly usefully for recall and are equal since the number of edges is xed
version

with options data a
structured perceptron loss structured hinge loss structured ramp loss max max g max g g g g g max g table loss functions minimized in parameter estimation
g denotes the gold standard summary graph
score is as dened in equation
g penalizes each vertex or edge in g g g g
since cost factors just like the scoring function each max operation can be accomplished using a variant of ilp decoding

in which the cost is incorporated into the linear objective while the constraints remain the same
standard parses jamr parses perceptron hinge ramp ramp expand oracle oracle expand perceptron hinge ramp ramp expand oracle oracle expand subgraph prediction p











nodes r











f











edges f











summarization r









f









p









table subgraph prediction and summarization to bag of words results on test set
gold standard amr annotations are used for model training in all conditions
expand means the result is obtained using source graph with expansion edge performance is measured ignoring labels
uating such less well formed summaries such as those generated from speech transcripts liu and liu
oracle summaries are produced by taking the gold standard amr parses of the reference mary obtaining the most frequently aligned word span for each unique concept node using the jamr aligner and then generating a bag of words summary
evaluation of oracle summaries is formed in the same manner as for system maries
the above process does not involve graph expansion so summarization performance is the same for the two conditions oracle and oracle expand
we nd that jamr parses are a large source of degradation of edge prediction performance and a smaller but still signicant source of degradation for concept prediction
surprisingly using jamr parses leads to slightly improved scores
keep in mind though that under our bag of words generator scores only depend on concept prediction and are unaffected by edge prediction
the oracle summarization results
and
scores for gold standard and jamr parses spectively further suggest that improved graph marization models step might benet from future improvements in amr parsing step
across all conditions and both evaluations we nd that incorporating a cost aware loss function hinge vs
has little effect but that ing ramp loss leads to substantial gains
in table we show detailed results with and without graph expansion
expand means the sults are obtained using the expanded source graph
we nd that graph expansion only marginally affects system performance
graph expansion slightly hurts the system performance on edge prediction
for ample using ramp loss with jamr parser as input we obtained
and
for node and edge prediction with graph expansion
and
without edge expansion
on the other hand it creases the oracle performance by a large margin
this suggests that with more training data or a more sophisticated model that is able to better nate among the enlarged output space graph sion still has promise to be helpful
related and future work according to dang and owczarzak the jority of competitive summarization systems are tractive selecting representative sentences from put documents and concatenating them to form a summary
this is often combined with sentence compression allowing more sentences to be ilps and approximations cluded within a budget
have been used to encode compression and tion mcdonald martins and smith gillick and favre berg kirkpatrick et al
almeida and martins li et al

other decoding approaches have included a greedy method exploiting submodularity lin and bilmes document reconstruction he et al
and graph cuts qian and liu among others
previous work on abstractive summarization has explored user studies that compare extractive with nlg based abstractive summarization carenini and cheung
ganesan et al
pose to construct summary sentences by repeatedly searching the highest scored graph paths
gerani al
generate abstractive summaries by fying discourse parse trees
our work is similar in spirit to cheung and penn which splices and recombines dependency parse trees to produce stractive summaries
in contrast our work operates on semantic graphs taking advantage of the recently developed amr bank
also related to our work are graph based rization methods vanderwende et al
erkan and radev mihalcea and tarau
derwende et al
transform input to cal forms score nodes using pagerank and grow the graph from high value nodes using heuristics
in erkan and radev and mihalcea and rau the graph connects surface terms that co occur
in both cases the graphs are constructed based on surface text it is not a representation of propositional semantics like amr
however future work might explore similar graph based calculations to contribute features for subgraph selection in our framework
our constructed source graph can easily reach ten times or more of the size of a sentence dency graph
thus more efcient graph decoding algorithms e

based on lagrangian relaxation or approximate algorithms may be explored in future work
other future directions may include jointly performing subgraph and edge label prediction ploring a pipeline that consists of an tomatic amr parser a graph to graph summarizer and a amr to text generator and devising an uation metric that is better suited to abstractive marization
many domains stand to eventually benet from summarization
these include books audio video segments and legal texts
conclusion we have introduced a statistical abstractive rization framework driven by the abstract meaning representation
the centerpiece of the approach is a structured prediction algorithm that transforms mantic graphs of the input into a single summary mantic graph
experiments show the approach to be promising and suggest directions for future research
acknowledgments the authors thank three anonymous reviewers for their insightful input
we are grateful to nathan schneider kevin gimpel sasha rush and the ark group for valuable discussions
the research was supported by nsf grant darpa grant funded under the deft program the u
s
army research laboratory and the u
s
army research ofce under contract grant number and by iarpa via doi nbc contract number
the views and conclusions contained herein are those of the thors and should not be interpreted as necessarily representing the ofcial policies or endorsements either expressed or implied of the sponsors
references miguel b
almeida and andre f
t
martins

fast and robust compressive summarization with dual composition and multi task learning
in proceedings of acl
david bamman and noah a
smith

new ment methods for discriminative book summarization
in

laura banarescu claire bonial shu cai madalina georgescu kira griftt ulf hermjakob kevin knight philipp koehn martha palmer and nathan schneider

abstract meaning representation for sembanking
in proceedings of linguistic annotation workshop
taylor berg kirkpatrick dan gillick and dan klein
in jointly learning to extract and compress

proceedings of acl
shu cai and kevin knight

smatch an evaluation metric for semantic feature structures
in proceedings of acl
giuseppe carenini and jackie chi kit cheung

extractive vs
nlg based abstractive summarization of evaluative text the effect of corpus ity
in proceedings of the fifth international natural language generation conference inlg
jackie chi kit cheung and gerald penn

pervised sentence enhancement for automatic rization
in proceedings of emnlp
michael collins

discriminative training ods for hidden markov models theory and ments with perceptron algorithms
in proceedings of emnlp
hoa trang dang and karolina owczarzak

overview of the tac update summarization task
in proceedings of text analysis conference tac
bonnie dorr nizar habash and david traum

a thematic hierarchy for efcient generation from lexical conceptual structure
in david farwell rie gerber and eduard hovy editors machine lation and the information soup proceedings of the third conference of the association for machine translation in the americas lecture notes in puter science
springer
john duchi elad hazan and yoram singer

adaptive subgradient methods for online learning and stochastic optimization
journal of machine learning research
gunes erkan and dragomir r
radev

lexrank graph based lexical centrality as salience in text journal of articial intelligence marization
search
jeffrey flanigan sam thomson jaime carbonell chris dyer and noah a
smith

a discriminative graph based parser for the abstract meaning tation
in proceedings of acl
kavita ganesan chengxiang zhai and jiawei han

opinosis a graph based approach to tive summarization of highly redundant opinions
in proceedings of coling
shima gerani yashar mehdad giuseppe carenini mond t
ng and bita nejat

abstractive marization of product reviews using discourse ture
in proceedings of emnlp
dan gillick and benoit favre

a scalable global in proceedings of the model for summarization
naacl workshop on integer linear programming for natural langauge processing
kevin gimpel and noah a
smith

structured ramp loss minimization for machine translation
in proceedings of naacl hlt
zhanying he chun chen jiajun bu can wang lijun zhang deng cai and xiaofei he

document summarization based on data reconstruction
in ceedings of aaai
eduard hovy mitchell marcus martha palmer lance ramshaw and ralph weischedel

ontonotes the solution
in proceedings of naacl
richard m
karp

reducibility among torial problems
in complexity of computer tations pages
springer us
robert t
kasper

a exible interface for linking applications to penman s sentence generator
in ceedings of the darpa speech and natural language workshop
gunhee kim leonid sigal and eric p
xing

joint summarization of large scale collections of web ages and videos for storyline reconstruction
in ceedings of cvpr
kevin knight laura baranescu claire bonial madalina georgescu kira griftt ulf hermjakob daniel marcu martha palmer and nathan schneider

abstract meaning representation amr annotation release

web download
phia linguistic data consortium
polina kuznetsova vicente ordonez tamara l
berg and yejin choi

treetalk composition and compression of trees for image descriptions
tions of acl
irene langkilde and kevin knight

generation that exploits based statistical knowledge
in proceedings of coling
chen li yang liu fei liu lin zhao and fuliang weng

improving multi documents summarization by sentence compression based on expanded constituent parse tree
in proceedings of emnlp
hui lin and jeff bilmes

multi document marization via budgeted maximization of submodular functions
in proceedings of naacl
chin yew lin

rouge a package for matic evaluation of summaries
in proceedings of acl workshop on text summarization branches out
lucy vanderwende michele banko and arul menezes

event centric summary generation
in ings of duc
bin zhao and eric p
xing

quasi real time in proceedings of marization for consumer videos
cvpr
fei liu and yang liu

towards abstractive speech summarization exploring unsupervised and vised approaches for spoken utterance compression
ieee transactions on audio speech and language processing
ivana ljubic rene weiskircher ulrich pferschy nar w
klau petra mutzel and matteo fischetti

an algorithmic framework for the exact solution of the prize collecting steiner tree problem
in matical progamming series b
andre f
t
martins and noah a
smith

rization with a joint model for sentence extraction and in proceedings of the acl workshop compression
on integer linear programming for natural language processing
andre f
t
martins noah a
smith and eric p
xing

concise integer linear programming tions for dependency parsing
in proceedings of acl
ryan mcdonald

a study of global inference in gorithms in multi document summarization
ceedings of ecir
rada mihalcea and paul tarau

textrank ing order into text
in proceedings of emnlp
courtney napoles benjamin van durme and chris callison burch

evaluating sentence in pression pitfalls and suggested remedies
ceedings of the workshop on monolingual text to text generation mttg pages stroudsburg pa usa
association for computational linguistics
ani nenkova and kathleen mckeown

automatic summarization
foundations and trends in tion retrieval
xian qian and yang liu

fast joint compression and summarization via graph cuts
in proceedings of emnlp
norman sadeh alessandro acquisti travis d
breaux lorrie faith cranor aleecia m
mcdonald joel r
reidenberg noah a
smith fei liu n
cameron russell florian schaub and shomir wilson

the usable privacy policy project
technical report cmu carnegie mellon university
arie segev

the node weighted steiner tree problem
networks
ben taskar carlos guestrin and daphne koller

max margin markov networks
in advances in neural information processing systems
kapil thadani and kathleen mckeown

sentence in compression with joint structural inference
ceedings of conll

