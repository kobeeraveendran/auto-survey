n a j l c
s c v
v i x r a extractive summarization using deep learning sukriti verma and vagisha nidhi delhi technological university shahbad daulatpur main bawana road india dce
sukriti vagisha

com
ac
abstract
this paper proposes a text summarization approach for tual reports using a deep learning model
this approach consists of three phases feature extraction feature enhancement and summary tion which work together to assimilate core information and generate a coherent understandable summary
we are exploring various features to improve the set of sentences selected for the summary and are using a restricted boltzmann machine to enhance and abstract those features to improve resultant accuracy without losing any important information
the sentences are scored based on those enhanced features and an tractive summary is constructed
experimentation carried out on several articles demonstrates the eectiveness of the proposed approach
keywords unsupervised single document deep learning extractive introduction a summary can be dened as a text produced from one or more texts containing a signicant portion of the information from the original and that is no longer than half of the original
according to text summarization is the process of distilling the most important information from a source or sources to produce an abridged version for a particular user and
when this is done by means of a computer i
e
automatically we call it automatic text summarization
this process can be seen as a form of compression and it sarily suers from information loss but it is essential to tackle the information overload due to abundance of textual material available on the internet
text summarization can be classied into extractive summarization and stractive summarization based on the summary generated
extractive rization is creating a summary based on strictly what you get in the original text
abstractive summarization mimics the process of paraphrasing a text
summarized using this technique looks more human like and produces condensed summaries
these techniques are much harder to implement than the extractive summarization techniques
in this paper we follow the extractive methodology to develop techniques for summarization of factual reports or descriptions
we have developed an approach for single document summarization using deep learning
so this paper intends to propose an approach by referencing the architecture of the human brain
it is broken down into three phases feature extraction feature enhancement and summary generation based on values of those features
since it can be very dicult to construct high level abstract features from raw data we use deep learning in the second phase to build complex features out of simpler features extracted in the rst phase
these extracted features depend highly on how factual the given document is
in the end we have run the proposed algorithm on several factual reports to evaluate and demonstrate the eectiveness of the proposed approach based on the measures such as recall precision and measure
related works most early work on text summarization was focused on technical documents and early studies on summarization aimed at summarizing from pre given documents without any other requirements which is usually known as generic tion
luhn proposed that the frequency of a particular word in an article provides a useful measure of its signicance
a number of key ideas such as stemming and stop word ltering were put forward in this paper that have now been understood as universal preprocessing steps to text analysis
baxendale examined paragraphs and found that in of the paragraphs the topic sentence came as the rst one and in of the time it was the last sentence
this positional feature has been used in many complex machine learning based systems since
edmundson focused his work around the importance of word frequency and positional importance as features
two other features were also used cue words and the skeleton structure of the document
weights were sociated with these features manually and nally sentences were scored
during evaluation it was found that around of the system generated summaries matched the target summaries written manually by humans
upcoming researchers in text summarization have approached it problem from many aspects such as natural language processing statistical modelling and machine learning
while initially most machine learning systems assumed feature independence and relied on naive bayes methods other recent ones have shifted focus to selection of appropriate features and learning algorithms that make no independence assumptions
other signicant approaches involved den markov models and log linear models to improve extractive summarization
more recent papers in contrast used neural networks towards this goal
text summarization can be done for one document known as single document summarization or for multiple documents known as multi document marization
on basis of the writing style of the nal summary generated text summarization techniques can be divided into extractive methodology and abstractive methodology
the objective of generating summaries via the extractive approach is choosing certain appropriate sentences as per the ment of a user
due to the idiosyncrasies of human invented languages and mar extractive approaches which select a subset of sentences from the input documents to form a summary instead of paraphrasing like a human are the mainstream in the area
almost all extractive summarization methods have three main obstacles
the rst obstacle is the ranking problem i
e
how you rank words phrases sentences
the second obstacle is the selection problem i
e
how to select a set of those ranked units
the third obstacle is the coherence problem i
e
how to ensure that the selected units form an understandable summary rather than being a set of disconnected words phrases sentences
algorithms that determine the relevance of a textual unit that is words phrases sentences with respect to the requirement of the user are used to solve the ranking problem
the selection and coherence problems are solved by ods that improve diversity minimize redundancy and pick up phrases sentences that are somewhat similar so that more relevant information can be covered by the summary in lesser words and the summary is coherent
our proach solves the ranking problem by learning a certain set of features for each sentence
on the basis of these features a score is calculated for each sentence and sentences are arranged in decreasing order of their scores
even with a list of ranked sentences it is not a trivial problem to select a subset of sentences for a coherent summary which includes diverse information minimizes dancy and is within a word limit
our approach solves this problem as follows
the most relevant sentence is the rst sentence in this sorted list and is chosen as part of the subset of sentences which will form the summary
then the next sentence selected is a sentence having highest jaccard similarity with the rst sentence and is picked from the top half of the list
this process is recursively and incrementally repeated to select more sentences until limit is reached
proposed approach
preprocessing preprocessing is crucial when it comes to processing text
ambiguities can be caused by various verb forms of a single word dierent accepted spellings of a certain word plural and singular terms of the same things
moreover words like a an the is of
are known as stop words
these are certain high frequency words that do not carry any information and do nt serve any purpose towards our goal of summarization
in this phase we do
document segmentation the text is divided into paragraphs so as to keep a track of which paragraph each sentence belongs to and what is the position of a sentence in its respective paragraph

paragraph segmentation the paragraphs are further divided into tences

word normalization each sentence is broken down into words and the words are normalized
normalization involves lemmatization and results in all words being in one common verb form crudely stemmed down to their roots with all ambiguities removed
for this purpose we use porters rithm

stop word filtering each token is analyzed to remove high frequency stop words

pos tagging remaining tokens are part of speech tagged into verb noun adjective
using the pos tagging module supplied by nltk

feature extraction once the complexity has been reduced and ambiguities have been removed the document is structured into a sentence feature matrix
a feature vector is tracted for each sentence
these feature vectors make up the matrix
we have experimented with various features
the combination of the following tence features has turned out most suitable to summarize factual reports
these computations are done on the text obtained after the preprocessing phase
number of thematic words the most frequently occurring words of the text are found
these are thematic words
for each sentence the ratio of no
of thematic words to total words is calculated
sentence t hematic n o
of thematic words t otal words
sentence position this feature is calculated as follows
sentence p osition if its the rst or last sentence of the text os max min otherwise where senpos position of sentence in the text min n max n n is total number of sentences in document th is threshold calculated as
n by this we get a high feature value towards the beginning and ending of the document and a progressively decremented value towards the middle

sentence length this feature is used to exclude sentences that are too short as those sentences will not be able to convey much information
sentence length if number of words is less than n o
of words in the sentence otherwise
sentence position relative to paragraph this comes directly from the observation that at the start of each paragraph a new discussion is begun and at the end of each paragraph we have a conclusive closing
p osition in p ara otherwise if it is the rst or last sentence of a paragraph
number of proper nouns this feature is used to give importance to sentences having a substantial number of proper nouns
here we count the total number of words that have been pos tagged as proper nouns for each sentence

number of numerals since gures are always crucial to presenting facts this feature gives importance to sentences having certain gures
for each sentence we calculate the ratio of numerals to total number of words in the sentence
sentence n umerals n o
of numerals t otal words
number of named entities here we count the total number of named entities in each sentence
sentences having references to named entities like a company a group of people
are often quite important to make any sense of a factual report

term frequency inverse sentence frequency tf isf since we are working with a single document we have taken tf isf feature into account rather than tf idf
frequency of each word in a particular sentence is multiplied by the total number of occurrences of that word in all the other sentences
we calculate this product and add it over all words
t f isf all words t f isf t otal words
sentence to centroid similarity sentence having the highest tf isf score is considered as the centroid sentence
then we calculate cosine larity of each sentence with that centroid sentence
sentence similarity cosine centroid at the end of this phase we have a sentence feature matrix

feature enhancement the sentence feature matrix has been generated with each sentence having feature vector values
after this recalculation is done on this matrix to enhance and abstract the feature vectors so as to build complex features out of simple ones
this step improves the quality of the summary
to enhance and abstract the sentence feature matrix is given as input to a restricted boltzmann machine rbm which has one hidden layer and one visible layer
a single hidden layers will suce for the learning process based on fig

a restricted boltzmann machine the size of our training data
the rbm that we are using has perceptrons in each layer with a learning rate of

we use persistent contrastive divergence method to sample during the learning process
we have trained the rbm for epochs with a batch size of and parallel gibbs chains used for sampling using persistent cd method
each sentence feature vector is passed through the hidden layer in which feature vector values for each sentence are multiplied by learned weights and a bias value is added to all the feature vector values which is also learned by the rbm
at the end we have a rened and enhanced matrix
note that the rbm will have to be trained for each new document that has to be summarized
the idea is that no document can be summarized without going over it
since each document is unique in the features extracted in section
the rbm will have to be freshly trained for each new document

summary generation the enhanced feature vector values are summed to generate a score against each sentence
the sentences are then sorted according to decreasing score value
the most relevant sentence is the rst sentence in this sorted list and is chosen as part of the subset of sentences which will form the summary
then the next sentence we select is the sentence having highest jaccard similarity with the rst sentence selected strictly from the top half of the sorted list
this process is recursively and incrementally repeated to select more sentences until a specied summary limit is reached
the sentences are then re arranged in the order of appearance in the original text
this produces a coherent summary rather than a set of haywire sentences
results and performance evaluation several factual reports from various domains of health technology news sports
with varying number of sentences were used for experimentation and uation
the proposed algorithm was run on each of those and system generated summaries were compared to the summaries produced by humans
fig

comparison between feature vector sum and enhanced feature vector sum feature extraction and enhancement is carried out as proposed in sections
and
for all documents
the values of feature vector sum and enhanced feature vector sum for each sentence of one such document have been plotted in fig
the restricted boltzmann machine has extracted a hierarchical tation out of data that initially did not have much variation hence discovering the latent factors
the sentences have then been ranked on the basis of nal feature vector sum and summaries are generated as proposed in section

fig

precision values corresponding to summaries of various documents evaluation of the system generated summaries is done based on three basic measures precision recall and f measure
fig

recall values corresponding to summaries of various documents it can be seen that as the number of sentences in the original document cross a certain threshold the restricted boltzmann machine has ample data to be trained successfully and summaries with high precision and recall values are generated
see fig and
fig

f measure values corresponding to summaries of various documents f measure is dened as follows f m easure recall p recision recall p recision comparative analysis the existing approach was executed for the same set of articles with just one layer of rbm rather than two as it species and average values of precision recall and f measure were plotted for drawing a comparison between the existing approach and the proposed approach while keeping the amount of computation constant
fig

precison recall and f measure values for the proposed approach left bars and the existing approach right bars the proposed approach has an average precision value of
and average recall value of
which are both higher than those of the existing approach
hence the proposed approach responds better for summarization of factual ports
conclusion we have developed an algorithm to summarize single document factual reports
the algorithm runs separately for each input document instead of learning rules from a corpus as each document is unique in itself
this is an advantage that our approach provides
we extract features from the given document and enhance them to score each sentence
recent approaches have been using rbms stacked on top of each other for feature enhancement
our approach uses only one rbm and works eectively and eciently for factual reports
this has been demonstrated by hand picking factual descriptions from several domains and comparing the system generated summaries to those written by humans
this approach can further be developed by adapting the extracted features as per the user s requirements and further adjusting the hyperparameters of the rbm to minimize processing and error in encoded values
acknowledgments
we would like to extend our gratitude to dr
daya gupta professor department of computer science and engineering delhi ical university for providing insight and expertise that greatly assisted this search
references
hovy e
h
automated text summarization in ruslan mitkov ed the oxford handbook of computational linguistics
mani i
house d
klein g
hirschman l
firmin t
sundheim b
the ster summac text summarization evaluation
in proceedings of the ninth conference on european chapter of the association for computational linguistics pp

association for computational linguistics stroudsburg pa usa



chuang w
t
yang j
extracting sentence segments for text summarization a machine learning approach
in proceedings of the annual international acm sigir conference on research and development in information retrieval pp

acm new york ny usa



berger a
mittal v
query relevant summarization using faqs
in ings of the annual meeting on association for computational linguistics pp

association for computational linguistics stroudsburg pa usa



luhn h
p
the automatic creation of literature abstracts
ibm journal of search and development vol
issue

rd


baxendale p
machine made index for technical literature an experiment
ibm journal of research and development vol
issue
doi
rd


edmundson h
p
new methods in automatic extracting
journal of the acm vol
issue



zhang y
wang d
li t
idvs an interactive multi document visual rization system machine learning and knowledge discovery in databases lncs vol
pp

springer heidelberg


darling w
m
song f
probabilistic document modeling for syntax removal in text summarization
in proceedings of the annual meeting of the association for computational linguistics human language technologies pp

acm press stroudsburg pa usa

wan x
xiao j
single document keyphrase extraction using neighborhood knowledge
in proceedings of the twenty third aaai conference on articial intelligence

shen d
sun j
t
li h
yang q
chen z
document summarization using conditional random fields
in proceedings of the international joint ence on artical intelligence pp

morgan kaufmann publishers inc
san francisco ca usa

wong k
f
wu m
j
li w
j
extractive summarization using supervised and semi supervised learning
in proceedings of the international conference on computational linguistics volume pp

association for computational linguistics stroudsburg pa usa

chen e
k
yang x
k
zha h
y
zhang r
and zhang w
j

learning object classes from image thumbnails through deep neural networks
in ieee international conference on acoustics speech and signal processing
ieee

icassp


jin f
huang m
l
and zhu x
y
a comparative study on ranking and lection strategies for multi document summarization
in proceedings of the international conference on computational linguistics posters pp

ciation for computational linguistics stroudsburg pa usa

singh s
p
kumar a
mangal a
singhal s
bilingual automatic text marization using unsupervised deep learning
in international ence on electrical electronics and optimization techniques
ieee
doi
iceeot


natural language toolkit for python
nltk

deep learning tutorials
net
performance evaluation of information retrieval systems web
stanford
class handouts evaluation
ppt
padmapriya g
duraiswamy k
an approach for text summarization using deep learning algorithm
journal of computer science vol
issue

jcssp



