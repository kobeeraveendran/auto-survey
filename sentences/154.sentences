entity commonsense representation for neural abstractive summarization reinald kim amplayo and seonjae lim and seung won hwang yonsei university seoul south korea rktamplayo n u
j l c
s
c
v
v
i x r a abstract a major proportion of a text summary includes important entities found in the original text

these entities build up the topic of the mary
moreover they hold commonsense
formation once they are linked to a knowledge base
based on these observations this per investigates the usage of linked entities to guide the decoder of a neural text summarizer to generate concise and better summaries
to this end we leverage on an off the shelf entity linking system els to extract linked entities and propose t a module easily attachable to a sequence to sequence model that transforms a list of entities into a vector representation of the topic of the mary
current available els s are still not ciently effective possibly introducing solved ambiguities and irrelevant entities
we resolve the imperfections of the els by
coding entities with selective disambiguation and pooling entity vectors using rm tion
by applying t to a simple to sequence model with attention mechanism as base model we see signicant ments of the performance in the gigaword sentence to title and cnn long document to multi sentence highlights summarization datasets by at least rouge points
introduction text summarization is a task to generate a shorter and concise version of a text while preserving the meaning of the original text
the task can be vided into two subtask based on the approach
tractive and abstractive summarization
tive summarization is a task to create summaries by pulling out snippets of text form the nal text and combining them to form a summary

abstractive summarization asks to generate maries from scratch without the restriction to use amplayo and lim are authors with equal tribution
names are arranged alphabetically
figure observations on linked entities in summaries

summaries are mainly composed of entities

entities can be used to represent the topic of the mary

entity commonsense learned from a large corpus can be used
the available words from the original text
due to the limitations of extractive summarization on incoherent texts and unnatural methodology yao al the research trend has shifted towards abstractive summarization
sequence to sequence models sutskever et al with attention mechanism bahdanau et al have found great success in generating stractive summaries both from a single sentence chopra et al and from a long document with multiple sentences chen et al

ever when generating summaries it is necessary to determine the main topic and to sift out essary information that can be omitted
to sequence models have the tendency to include all the information relevant or not that are found in the original text
this may result to cise summaries that concentrates wrongly on relevant topics
the problem is especially severe when summarizing longer texts
in this paper we propose to use entities found in the original text to infer the summary topic
the los angeles dodgersacquired south koreanright hander jae seofrom the new york metson wednesdayin a four player swap
input textkoreasseoheaded to dodgersfrom metssummarytopic entitydistributionlos angeles dodgers korean
woongnew york mets gating the aforementioned problem
specically we leverage on linked entities extracted by ploying a readily available entity linking system

the importance of using linked entities in rization is intuitive and can be explained by
ing at figure as an example
first in the ure aside from auxiliary words to construct a tence a summary is mainly composed of linked entities extracted from the original text
second we can depict the main topic of the mary as a probability distribution of relevant ties from the list of entities
finally we can leverage on entity commonsense learned from a separate large knowledge base such as wikipedia
to this end we present a method to fectively apply linked entities in sequence sequence models called t
t is a module that can be easily attached to any sequence to sequence based summarization model
the module encodes the entities extracted from the original text by an entity linking system els constructs a vector representing the topic of the summary to be generated and informs the decoder about the constructed topic vector
due to the imperfections of current els the extracted linked entities may be too ambiguous and coarse to be considered relevant to the summary
we solve this issue by using entity encoders with lective disambiguation and by constructing topic vectors using rm attention
we experiment on two datasets gigaword and cnn with varying lengths
we show that
ing our module to a sequence to sequence model with attention mechanism signicantly increases its performance on both datasets
moreover when compared with the state of the art models for each dataset the model obtains a comparable mance on the gigaword dataset where the texts are short and outperforms all competing models on the cnn dataset where the texts are longer

thermore we provide analysis on how our model effectively uses the extracted linked entities to duce concise and better summaries
usefulness of linked entities in summarization
in the next subsections we present detailed guments with empirical and previously examined evidences on the observations and possible issues when using linked entities extracted by an entity linking system els for generating abstractive summaries
for this purpose we use the opment sets of the gigaword dataset provided in rush al and of the cnn dataset vided in hermann et al as the mental data for quantitative evidence and refer the readers to figure as the running example
observations as discussed in section we nd three tions that show the usefulness of linked entities for abstractive summarization
first summaries are mainly composed of linked entities extracted from the original text
in the
ample it can be seen that the summary contains four words that refer to different entities
in fact all noun phrases in the summary mention at least one linked entity
in our experimental data we tract linked entities from the original text and pare them to the noun phrases found in the mary
we report that and of the noun phrases on the gigaword and cnn datasets spectively contain at least one linked entity which conrms our observation
second linked entities can be used to represent the topic of the summary dened as a multinomial distribution over entities as graphically shown in the example where the probabilities refer to the relevance of the entities
entities have been viously used to represent topics newman et al as they can be utilized as a controlled cabulary of the main topics in a document hulpus et al
in the example we see that the tity jae seo is the most relevant because it is the subject of the summary while the entity south korean is less relevant because it is less tant when constructing the summary

third we can make use of the entity
sense that can be learned as a continuous vector representation from a separate larger corpus ni al yamada et al

in the
ample if we know that the entities los les dodgers and new york mets are american baseball teams and jae seo is a baseball player associated with the teams then we can use this formation to generate more coherent summaries

we nd that of the extracted linked ties are covered by the pre trained in our experimental data proving our third observation
possible issues despite its usefulness linked entities extracted from els s have issues because of low precision rates hasibi et al and design challenges in training datasets ling et al
these issues can be summarized into two parts ambiguity and coarseness

first the extracted entities may be ambiguous

in the example the entity south korean is biguous because it can refer to both the south korean person and the south korean language among
in our experimental data we
tract the top entities based on frequency and the entities extracted from randomly selected texts and check whether they have biguation pages in wikipedia or not
we discover that of the top entities and of the entities picked at random have disambiguation pages which shows that most entities are prone to ambiguity problems
second the linked entities may also be too
mon to be considered an entity
this may duce errors and irrelevance to the summary

in the example wednesday is erroneous because it is wrongly linked to the entity wednesday night baseball
also swap is irrelevant because
though it is linked correctly to the entity trade sports it is too common and irrelevant when generating the summaries

in our experimental data we randomly select data instances and tag the correctness and relevance of extracted tities into one of four labels a correct and evant b correct and somewhat relevant c rect but irrelevant and d incorrect
results show that and are tagged with a b c and d respectively which shows that there is a large amount of incorrect and vant entities

our model to solve the issues described above we present
t a module that can be easily attached to any sequence to sequence based stractive summarization model
t encodes the linked entities extracted from the text and forms them into a single topic vector
this vector is ultimately concatenated to the decoder hidden state vectors
the module contains two ules specically for the issues presented by the tity linking systems the entity encoding ule with selective disambiguation and the pooling submodule with rm attention
overall our full architecture can be illustrated as in figure which consists of an entity
ing system els a sequence to sequence with tention mechanism model and the t module

we note that our proposed module can be ily attached to more sophisticated abstractive marization models zhou et al tan et al that are based on the traditional decoder framework and consequently can produce better results
the code of the base model and the t are available
base model
as our base model we employ a basic decoder rnn used in most neural machine
lation bahdanau et al and text
tion nallapati al tasks
we employ a two layer bidirectional gru bigru as the current unit of the encoder
the bigru consists of a forward and backward gru which results to sequences of forward and backward hidden states

h n
h h tively h n and h
h h

gru xi
h

gru xi h

h
h
i the forward and backward hidden states are concatenated to get the hidden state vectors of the tokens hi
h i
the nal states of the forward and backward gru are also nated to create the nal text representation vector of the encoder s
h
these values are h n calculated per layer where xt of the second layer is ht of the rst layer
the nal text representation vectors are projected by a fully connected layer and are passed to the decoder as the initial hidden
states
for the decoder we use a two layer directional gru with attention
at each time step t the previous token the previous hidden state and the previous context vector are passed to a gru to calculate the new hidden state st as shown in the equation below
st gru korean figure full architecture of our proposed sequence to sequence model with t module
the context vector ct is computed using the additive attention mechanism bahdanau et al which matches the current decoder state st and each encoder state hi to get an importance score
the scores are then passed to a softmax and are used to pool the encoder states using weighted sum
the nal pooled vector is the context vector as shown in the equations below
globally disambiguating encoder one way to disambiguate an entity is by using all the other entities putting more importance to entities that are nearer
for this purpose we employ an based model to globally disambiguate the entities

specically we use bigru and concatenate the forward and backward hidden state vectors as the new entity vector gt a uahi
i at ihi at i ct i finally the previous token the current context vector ct and the current decoder state st are used to generate the current word yt with a softmax layer over the decoder vocabulary as shown below
ot

wcct wsst t sof
entity encoding submodule after performing entity linking to the input text

ing the els we receive a sequential list of linked entities arranged based on their location in the text
we embed these entities to dimensional vectors e em where rd

since these entities may still contain ambiguity it is necessary to resolve them before applying them to the base model
based on the idea that an ambiguous entity can be disambiguated using its neighboring entities we introduce two kinds of disambiguating encoders below
h

gru ei

h

gru ei
h i

h
i h

h

locally disambiguating encoder another way to disambiguate an entity is by using only the rect neighbors of the entity putting no importance value to entities that are far
to do this we ploy a cnn based model to locally disambiguate the entities
specically we do the convolution operation using lter matrices
wf rhd with lter size to a window of h words
we do this for different sizes of
this produces new ture vectors ci as shown below where is a non linear function ci h bf
the convolution operation reduces the number of entities differently depending on the lter size
to prevent loss of information and to produce the same amount of feature vectors ci h we pad the entity list dynamically such that when the lter size is h the number of paddings on each side is h
the lter size h therefore refers to the number of entities used to disambiguate a middle entity
finally we concatenate all feature vectors the los angeles dodgers acquired south korean right hander jae seofrom the new york mets on wednesday in a four player swap
input textentity linking systemthelosangelesdodgersacquiredsouth sequence to sequence with attention start koreasseokoreasseoheaded attention mechanismbi encoder with selective modulepooling with firm attention mation of ei and
i i

wxei
d

i bd i by
the full entity encoding submodule is trated in figure
ultimately the submodule outputs the disambiguated entity vectors em
pooling submodule
the entity vectors e are pooled to create a gle topic vector t that represents the topic of the summary
one possible pooling technique is to use soft attention xu al on the vectors to determine the importance value of each vector which can be done by matching each entity vector with the text vector s from the text encoder as the context vector
the entity vectors are then pooled using weighted sum
one problem with soft tention is that it considers all entity vectors when constructing the topic vector
however not all tities are important and necessary when ing summaries
moreover a number of these tities may be erroneous and irrelevant as reported in section
soft attention gives non negligible important scores to these entities thus adds essary noise to the construction of the topic vector

our pooling submodule instead uses rm tention mechanism to consider only top k entities when constructing the topic vector
this is done in a differentiable way as follows a e uas g
k top
p sparse

i gi
pi


i

aiei ai t i where the functions k top gets the indices of the top
k vectors in g and p sparse creates a sparse vector where the values of k is and

the sparse vector p is added to the original tance score vector g to create a new importance use to represent figure entity encoding submodule with selective disambiguation applied to the entity
the left ure represents the full submodule while the right gure represents the two choices of disambiguating encoders
of different h s for each i as the new entity vector

i
ci
the question on which disambiguating encoder is better has been a debate some argued that using only the local context is appropriate lau et al while some claimed that additionally using global context also helps wang et al
the rnn based encoder is good as it smartly makes use of all entities however it may perform bad when there are many entities as it introduces noise when using a far entity during disambiguation

the cnn based encoder is good as it minimizes the noise by totally ignoring far entities when ambiguating however determining the ate lter sizes h needs engineering
overall we argue that when the input text is short a tence both encoders perform comparably wise when the input text is long a document the cnn based encoder performs better

selective disambiguation it is obvious that not all entities need to be disambiguated
when a correctly linked and already adequately biguated entity is disambiguated again it would make the entity very context specic and might not be suitable for the summarization task
our tity encoding submodule therefore uses a selective mechanism that decides whether to use the biguating encoder or not
this is done by ducing a selective disambiguation gate
the nal entity vector ei is calculated as the linear
globally disambiguating encoder disambiguating encoder score vector
in this new vector important scores of non top k entities are when softmax is
plied this gives very small negligible and to zero values to non top k entities
the value depends on the lengths of the input text and mary
moreover when k increases towards ity rm attention becomes soft attention
we
cide k empirically see section

extending from the base model module extends the base model as follows
the nal text representation vector s is used as a context vector when constructing the topic vector t in the pooling submodule
the topic vector t is then concatenated to the decoder hidden state vectors
i

the concatenated vector is nally used to create the output vector oi wcci
i related work due to its recent success neural network els have been used with competitive results on stractive summarization
a neural attention model was rst applied to the task easily achieving of the art performance on multiple datasets rush al
the model has been extended to instead use recurrent neural network as decoder
chopra et al
the model was further
tended to use a full rnn encoder decoder work and further enhancements through lexical and statistical features nallapati al
the current state of the art performance is achieved by selectively encoding words as a process of ing salient information zhou et al
neural abstractive summarization models have also been explored to summarize longer ments
word extraction models have been ously explored performing worse than sentence extraction models cheng and lapata

erarchical attention based recurrent neural works have also been applied to the task owing to the idea that there are multiple sentences in a ument nallapati al
finally based models were proposed to enable models to traverse the text content and grasp the overall meaning chen et al
the current state
the art performance is achieved by a graph based attentional neural model considering the key tors of document summarization such as saliency uency and novelty tan et al
dataset

gigaword cnn k m table dataset statistics
previous studies on the summarization tasks have only used entities in the preprocessing stage to anonymize the dataset nallapati al and to mitigate out of vocabulary problems tan al
linked entities for summarization are still not properly explored and we are the rst to use linked entities to improve the performance of the summarizer
experimental settings datasets we use two widely used tion datasets with different text lengths
first we use the annotated english gigaword dataset as used in rush al
this dataset receives the rst sentence of a news article as input and use the headline title as the gold standard mary
since the development dataset is large we randomly selected pairs as our development dataset
we use the same held out test dataset used in rush al for comparison
second we use the cnn dataset released in hermann et al
this dataset receives the full news
cle as input and use the human generated multiple sentence highlight as the gold standard summary

the original dataset has been modied and processed specically for the document rization task nallapati al
in addition to the previously provided datasets we extract linked entities using
ceccarelli et al an open source els that links text snippets found in a given text to entities contained in wikipedia
we use the default recommended parameters stated in the website
we summarize the statistics of both datasets in table
implementation
for both datasets we further reduce the size of the input output and entity cabularies to at most k as suggested in see et al and replace less frequent words to unk
we use pennington et al and pre trained tors to initialize our word and entity vectors
for grus we set the state size to
for cnn we set h with feature maps respectively
for rm attention k is tuned by culating the perplexity of the model starting with smaller values k and stopping when the perplexity of the model comes worse than the previous model
our liminary tuning showed that for gigaword dataset and k for cnn dataset are the best choices
we use dropout srivastava et al on all non linear connections with a dropout rate of
we set the batch sizes of gigaword and cnn datasets to and respectively
training is done via stochastic gradient descent over ed mini batches with the adadelta update rule with constraint hinton et al of
we perform early stopping using a subset of the given development dataset
we use beam search of size to generate the summary

baselines for the gigaword dataset we pare our models with the following abstractive baselines rush
al is a ne tuned version of abs which uses an attentive cnn coder and an nnlm decoder ati al is an rnn sequence to sequence model with lexical and statistical features in the encoder luong nmt luong al is a two layer lstm encoder decoder model
elman chopra et al uses an attentive cnn encoder and an elman rnn decoder and
seass
zhou et al uses bigru encoders and gru decoders with selective encoding
for the cnn dataset we compare our models with the following extractive and abstractive baselines is a strong baseline that extracts the rst three sentences of the document as summary lexrank extracts texts using lexrank erkan and radev bi gru is a non hierarchical one layer sequence to sequence abstractive line distraction chen et al uses a sequence to sequence abstractive model with distraction based networks and gba tan et al is a graph based attentional neural tive model
all baseline results used beam search and are gathered from previous papers
also model base

luong nmt ras elman seass rg l table results on the gigaword dataset using the length variants of rouge
model base



lexrank
bi gru distraction gba rg l
table results on the cnn dataset using the length rouge metric
we compare our nal model t with the base model base and some variants of our model without selective disambiguation using soft tention
results
we report the rouge scores for both datasets of all the competing models using rouge scores lin
we report the results on the gigaword and the cnn dataset in table and ble respectively
in gigaword dataset where the texts are short our best model achieves a ble performance with the current state of the art

in cnn dataset where the texts are longer our best model outperforms all the previous models
we emphasize that t module is easily attachable to better models and we expect t to improve model gold base


mean table human evaluations on the gigaword dataset

bold faced values are the best while red colored values are the worst among the values in the evaluation metric
their performance as well
overall t achieves a signicant improvement over the baseline model
base with at least points increase in the gigaword dataset and points increase in the cnn dataset
in fact all variants of t gain improvements over the baseline plying that leveraging on linked entities improves the performance of the summarizer
among the model variants the cnn based encoder with lective disambiguation and rm attention performs the best
automatic evaluation on the gigaword dataset shows that the cnn and rnn variants of t have similar performance
to break the tie between both models we also conduct man evaluation on the gigaword dataset
we
struct two annotators to read the input sentence and rank the competing summaries from rst to last according to their relevance and uency
the original summary gold and from models
base c and

we then compute i the proportion of every ing of each model and the mean rank of each model
the results are reported in table
the model with the best mean rank is then by and followed by gold base respectively
we also perform anova and post hoc tukey tests to show that the cnn ant is signicantly p better than the rnn variant and the base model
the rnn variant does not perform as well as the cnn variant contrary to the automatic rouge evaluation above


terestingly the cnn variant produces better but with no signicant difference summaries than the gold summaries
we posit that this is due to the fact that the article title does not correspond to the summary of the rst sentence
selective disambiguation of entities we show the effectiveness of the selective disambiguation gate d in selecting which entities to disambiguate or not
table shows a total of four different
amples of two entities with the highest lowest values
in the rst example sentence tains the entity united states and is linked with the country entity of the same name however the correct linked entity should be united states davis cup team and therefore is given a high d value
on the other hand sentence is linked correctly to the country united states and thus is given a low d value
the second example vides a similar scenario where sentence is linked to the entity gold but should be linked to the entity gold medal
sentence is linked correctly to the chemical element
hence the mer case received a high value d while the latter case received a low d value
entities as summary topic finally we provide one sample for each dataset in table for case study comparing our nal model that uses rm attention a variant that uses soft attention and the baseline model base
we also show the attention weights of the rm and soft models

first in the gigaword example we nd three
the base model generated a servations
less informative summary not mentioning ico state and rst edition
second the soft model produced a factually wrong summary ing that guadalajara is a mexican state while
actually it is a city
third the rm model is able to solve the problem by focusing only on the ve most important entities eliminating possible noise such as unk and less crucial entities such as country club
we can also see the ness of the selective disambiguation in this ple where the entity state is corrected to mean the entity mexican state which becomes relevant and is therefore selected
in the cnn example we also nd that the line model generated a very erroneous summary

we argue that this is because the length of the put text is long and the decoder is not guided as to which topics it should focus on
the soft model generated a much better summary however it cuses on the wrong topics specically on iran s nuclear program making the summary less eral
a quick read of the original article tells us that the main topic of the article is all about the two political parties arguing over the deal with iran
however the entity nuclear appeared a lot in the article which makes the soft model wrongly focus on the nuclear entity
the rm model produced the more relevant summary focusing on the original gold
baseline entities soft firm
original gold baseline soft
firm western mexico will host the rst edition of the dollar ochoa invitation tournament on nov
in club the ochoa foundation said in a statement on wednesday
mexico to host lorena ochoa golf tournament in guadalajara to host ochoa tournament tournament gigaword dataset example guadalajara country club
lorena ochoa state

unk mexico state guadalajara to host ochoa ochoa invitation jalisco mexican state to host rst edition of ochoa invitation lorena ochoa golf
cnn dataset example url netanyahu says third option is standing rm to get a better deal
political sparring continues in
over the deal with iran

netanyahu says he is a country of unk cheating and that it is a country of unk cheating
netanyahu says he is a country of unk cheating and that is a very bad deal
he says he says he says the plan is a country of unk cheating and that it is a country of unk cheating
he says the is a country of unk cheating and that is a country of unk cheating benjamin netanyahu i think there s a third alternative and that is standing rm netanyahu tells cnn

he says he does not roll back iran s nuclear ambitions
it does not roll back iran s nuclear program
new netanyahu i think there s a third alternative and that is standing rm netanyahu says
obama s comments come as democrats and republicans spar over the framework announced last week to lift western sanctions on iran
table examples from gigaword and cnn datasets and corresponding summaries generated by competing models
the tagged part of text is marked bold and preceded with at sign
the red color ll represents the attention scores given to each entity
we only report the attention scores of entities in the gigaword example for conciseness since there are linked entities in the cnn example
text
linked entity andy roddick got the better of dmitry tursunov in straight sets on friday assuring the states a lead over defending champions russia in the davis cup nal

sir alex ferguson revealed friday that david beckham s move to the states had not surprised him because he knew the midelder would not return to england if he could not come back to manchester united
linked entity following is the medal standing at the olympic winter games tabulated under team silver and bronze
unk opened lower here on monday at
us dollars an ounce against friday s closing rate of


d table examples with highest lowest disambiguation gate d values of two example entities united states and gold
the tagged part of text is marked bold and preceded with at sign
litical entities republicans democrats

this is due to the fact that only the most important elements are attended to create the mary topic vector
conclusion we proposed to leverage on linked entities to prove the performance of sequence to sequence models on neural abstractive summarization task

linked entities are used to guide the decoding cess based on the summary topic and sense learned from a knowledge base
we duced t a module that is easily attachable to any model using an encoder decoder framework
t applies linked entities into the summarizer by encoding the entities with tive disambiguation and pooling them into one summary topic vector with rm attention nism
we showed that by applying t to a basic sequence to sequence model we achieve ca nt improvements over the base model and sequently achieve a comparable performance with more complex summarization models
acknowledgement
we would like to thank the three anonymous viewers for their valuable feedback
this work was supported by microsoft research and stitute for information communications ogy promotion iitp grant funded by the korea government msit ment of explainable humanlevel deep machine learning inference framework
hwang is a corresponding author
references dzmitry bahdanau kyunghyun cho and yoshua gio

neural machine translation by jointly arxiv preprint learning to align and translate


diego ceccarelli claudio lucchese salvatore lando raffaele perego and salvatore trani


dexter an open source framework for entity linking

in proceedings of the sixth international workshop on exploiting semantic annotations in information retrieval
acm pages
qian chen xiaodan zhu zhenhua ling si wei and hui jiang

distraction based neural works for document summarization
arxiv preprint

jianpeng cheng and mirella lapata

neural summarization by extracting sentences and words
arxiv preprint
sumit chopra michael auli and alexander m rush


abstractive sentence summarization with tentive recurrent neural networks
in proceedings of the conference of the north american
ter of the association for computational linguistics human language technologies pages
gunes erkan and dragomir r radev

lexrank graph based lexical centrality as salience in text
journal of articial intelligence summarization

research
faegheh hasibi krisztian balog and svein erik berg

on the reproducibility of the tagme tity linking system
in european conference on formation retrieval
springer pages
karl moritz hermann tomas kocisky edward grefenstette lasse espeholt will kay mustafa leyman and phil blunsom

teaching chines to read and comprehend
in advances in
ral information processing systems
pages
geoffrey e hinton nitish srivastava alex krizhevsky ilya sutskever and ruslan r salakhutdinov


improving neural networks by preventing arxiv preprint adaptation of feature detectors


ioana hulpus conor hayes marcel karnstedt and derek greene

unsupervised graph based topic labelling using dbpedia
in proceedings of the sixth acm international conference on web search and data mining
acm pages
jey han lau paul cook and timothy baldwin

unimelb topic modelling based word sense

in duction for web snippet clustering

naacl hlt
pages
chin yew lin

rouge
a package for matic evaluation of summaries
in text tion branches out proceedings of the shop
barcelona spain volume xiao ling sameer singh and daniel s weld


design challenges for entity linking
transactions of the association for computational linguistics

minh thang luong hieu pham and christopher d manning
effective approaches to based neural machine translation
arxiv preprint
ramesh nallapati bowen zhou caglar gulcehre bing xiang al

abstractive text rization using sequence to sequence rnns and yond
arxiv preprint
david newman chaitanya chemudugunta padhraic smyth and mark steyvers

analyzing ties and topics in news articles using statistical topic models
in isi
springer pages
yuan ni qiong kai xu feng cao yosi mass dafna
sheinwald hui jia zhu and shao sheng cao


semantic documents relatedness using
in proceedings of the cept graph representation

ninth acm international conference on web search and data mining acm pages
jeffrey pennington richard socher and christopher manning

glove
global vectors for word representation
in proceedings of the ence on empirical methods in natural language cessing emnlp
pages
alexander m rush sumit chopra and
jason
a neural attention model for arxiv preprint ston
stractive sentence summarization


abigail see peter j liu and
christopher d to the point summarization arxiv preprint ning
get with pointer generator networks

nitish srivastava geoffrey e hinton alex krizhevsky ilya sutskever and ruslan salakhutdinov


dropout a simple way to prevent neural networks from overtting
journal of machine learning search
ilya sutskever oriol vinyals and quoc v le

sequence to sequence learning with neural works
in advances in neural information ing systems
pages
jiwei tan xiaojun wan and jianguo xiao


abstractive document summarization with a
in proceedings of based attentional neural model
the annual meeting of the association for putational linguistics volume long papers
volume pages
jing wang mohit bansal kevin gimpel brian d
ziebart and t yu clement

a sense topic model for word sense induction with unsupervised data enrichment
transactions of the association for computational linguistics
kelvin xu jimmy ba ryan kiros kyunghyun cho aaron courville ruslan salakhudinov rich zemel and yoshua bengio

show attend and tell neural image caption generation with visual
in international conference on machine tention

learning
pages
ikuya yamada hiroyuki shindo hideaki takeda and yoshiyasu takefuji
learning distributed resentations of texts and entities from knowledge base
arxiv preprint
jin ge yao xiaojun wan and jianguo xiao

cent advances in document summarization
edge and information systems pages
qingyu zhou nan yang furu wei and ming zhou


selective encoding for abstractive sentence summarization
arxiv preprint

