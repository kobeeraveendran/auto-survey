r a m r i
s
c
v
v
i x r a automatic text summarization approaches to speed up topic model learning process mohamed juan manuel torres richard javier ramrez and georges universite davignon pays de vaucluse france ecole polytechnique de montreal quebec canada
autonoma
metropolitana azcapotzalco mexico abstract
the number of documents available into internet moves each day up
for this reason processing this amount of information eectively and expressibly becomes a major concern for companies and scientists

methods that represent a textual document by a topic representation are widely used in information retrieval ir to process big data such as wikipedia articles
one of the main diculty in using topic model on huge data collection is related to the material resources cpu time and memory required for model estimate
to deal with this issue we propose to build topic spaces from summarized documents
in this per we present a study of topic space representation in the context of big data
the topic space representation behavior is analyzed on ent languages
experiments show that topic spaces estimated from text summaries are as relevant as those estimated from the complete ments
the real advantage of such an approach is the processing time
gain we showed that the processing time can be drastically reduced using summarized documents more than in general
this study nally points out the dierences between thematic representations of documents depending on the targeted languages such as english or latin introduction the number of documents available into internet moves each day up in an ponential way
for this reason processing this amount of information eectively and expressibly becomes a major concern for companies and scientists
an portant part of the information is conveyed through textual documents such as blogs or micro blogs general or advertise websites and encyclopedic documents
this last type of textual data increases each day with new articles which vey large and heterogenous information
the most famous and used collaborative internet encyclopedia is wikipedia enriched by worldwide volunteers
it is the most visited website in the usa with around million users visiting preprint of international journal of computational linguistics and applications
the site daily and a total reaching millions of the estimated million internet users in the

the massive number of documents provided by wikipedia is mainly exploited by natural language processing nlp scientists in various tasks such as word extraction document clustering automatic text summarization
dierent classical representations of a document such as
term frequency based

tation
have been proposed to extract word level information from this large amount of data in a limited time
nonetheless these straightforward tations obtain poor results in many nlp tasks with respect to more abstract and complex representations
indeed the classical term frequency tion reveals little in way of or inter document statistical structure and does not allow us to capture possible and unpredictable context dependencies

for these reasons more abstract representations based on latent topics have been proposed
the most known and used one is the latent dirichlet allocation
lda
approach which outperforms classical methods in many nlp tasks

the main drawback of this topic based representation is the time needed to learn lda latent variables
this massive waste of time that occurs during the lda learning process is mainly due to the documents size along with the number of documents which is highly visible in the context of big data such as wikipedia

the solution proposed in this article is to summarize documents contained into a big data corpus here wikipedia and then learn a lda topic space
this should answer the these three raised diculties reducing the processing time during the lda learning process retaining the intelligibility of documents maintaining the quality of lda models

with this summarization approach the size of documents will be drastically reduced the intelligibility of documents will be preserved and we make the assumption that the lda model quality will be conserved
moreover for all these reasons the classical term frequency document reduction is not considered in this paper
indeed this extraction of a subset of words to represent the document content allows us to reduce the document size but does not keep the document structure and then the intelligibility of each document
the main objective of the paper is to compare topic space representations using complete documents and summarized ones
the idea behind is to show the eectiveness of this document representation in terms of performance and processing reduction when summarized documents are used
the topic space representation behavior is analyzed on dierent languages english french and spanish
in the series of proposed experiments the topic models built from complete and summarized documents are evaluated using the jensen shannon
j s divergence measure as well as the perplexity measure
to the best of our knowledge this is the most extensive set of experiments interpreting the ation of topic spaces built from complete and summarized documents without human models


the rest of the paper is organized in the following way section duces related work in the areas of topic modeling and automatic text rization evaluations
then section describes the proposed approach including the topic representation adopted in our work and the dierent summarization systems employed
section presents the topic space quality measures used for the evaluation
experiments carried out along with with the results presented in section
a discussion is nally proposed in section before concluding in section
related work
several methods were proposed by information retrieval ir researchers to process large corpus of documents such as wikipedia encyclopedia
all these methods consider documents as a bag of words where the word order is not taken into account

among the rst methods proposed in ir propose to reduce each ment from a discrete space words and documents to a vector of numeral values represented by the word counts number of occurrences in the document named
tf idf
this approach showed its eectiveness in dierent tasks and more precisely in the basic identication of discriminative words for a document

however this method has many weaknesses such as the small amount of tion in description length or the weak of or intra statistical structure of documents in the text corpus
to substantiate the claims regarding tf idf method ir researchers have proposed several other dimensionality reductions such as latent semantic ysis lsa
which uses a singular value decomposition svd to reduce the space dimension
this method was improved by which proposed a probabilistic lsa plsa
plsa models each word in a document as a sample from a mixture model where the mixture components are multinomial random variables that can be viewed as representations of topics
this method demonstrated its performance on various tasks such as sentence or keyword extraction
in spite of the eectiveness of the plsa approach this method has two main drawbacks
the distribution of topics in plsa is indexed by training documents
thus the number of its parameters grows with the training document set size and then the model is prone to overtting which is a main issue in an ir task such as documents tering
however to address this shortcoming a tempering heuristic is used to smooth the parameter of plsa models for acceptable predictive performance the authors in showed that overtting can occur even if tempering process is used
to overcome these two issues the latent dirichlet allocation lda
method was proposed
thus the number of lda parameters does not grow with the size of the training corpus and lda is not candidate for overtting
next section
scribes more precisely the lda approach that will be used in our experimental study

the authors in evaluated the eectiveness of the jensen shannon j s theoretic measure in predicting systems ranks in two summarization tasks query focused and update summarization
they have shown that ranks produced by pyramids and those produced by j s measure correlate
however they did not investigate the eect of the measure in summarization tasks such as generic multi document summarization duc task biographical summarization
duc task opinion summarization tac os and summarization in languages other than english next section describes the proposed approach followed in this article
ing the topic space representation with the lda approach and its evaluation with the perplexity and the jensen shannon metrics
overview of the proposed approach figure describes the approach proposed in this paper to evaluate the quality of a topic model representation with and without automatic text summarization systems
the latent dirichlet allocation lda approach described in details in the next section is used for topic representation in conjunction with dierent state of the art summarization systems presented in section
fig

overview of the proposed approach

wikipediaenglish spanish or frenchtraintestsummarysystemlatent dirichlet allocationfull textartex baseline first baseline randomtopic spaces from documents not summarizedtopic spaces from documents summarizedperplexitykl topic representation latent dirichlet allocation
lda is a generative model which considers a document seen as a bag of words as a mixture of latent topics
in opposition to a multinomial mixture model lda considers that a theme is associated to each occurrence of a word composing the document rather than associate a topic with the complete document
thereby a document can change of topics from a word to another
however the word occurrences are connected by a latent variable which controls the global respect of the distribution of the topics in the document
these latent topics are terized by a distribution of word probabilities associated with them
plsa and lda models have been shown to generally outperform lsa on ir tasks

moreover lda provides a direct estimate of the relevance of a topic knowing a word set
figure shows the lda formalism
for every document d of a corpus d a rst parameter is drawn according to a dirichlet law of parameter
a second parameter is drawn according to the same dirichlet law of parameter
then to generate every word w of the document c a latent topic z is drawn from a multinomial distribution on
knowing this topic z the distribution of the words is a multinomial of parameters
the parameter is drawn for all the documents from the same prior parameter
this allows to obtain a parameter binding all the documents together
fig

lda formalism
several techniques have been proposed to estimate lda parameters such as variational methods expectation propagation or gibbs sampling

gibbs sampling is a special case of markov chain monte carlo mcmc
and gives a simple algorithm to approximate inference in high dimensional models such as lda
this overcomes the diculty to directly and exactly estimate parameters that maximize the likelihood of the whole data collection dened as knowing the dirichlet parameters and for the whole data collection w w

the rst use of gibbs sampling for estimating
lda is reported in and a more comprehensive description of this method can be found in
zwwordtopicndtopicdistributionworddistribution the next section describes the income of the lda technique
the input of the lda method is an automatic summary of each document of the train corpus

these summaries are built with dierent systems
automatic text summarization systems various text summarization systems have been proposed over the years

two baseline systems as well as the artex summarization system that reaches state of the art performance are presented in this section
baseline rst bf
the baseline or leadbase selects the n rst sentences of the documents where n is determined by a compression rate
although very simple this method is a strong baseline for the performance of any automatic summarization system
this very old and very simple sentence weighting heuristic does not involve any terms at all it assigns highest weight to the rst sentences of the text
texts of some genres such as news reports or scientic papers are specically designed for this heuristic any scientic paper tains a ready summary at the beginning
this gives a baseline that proves to be very hard to beat on such texts
it is worth noting that in document standing conference duc competitions only ve systems performed above this baseline which does not demerit the other systems because this baseline is genre specic
baseline random br
the baseline random randomly selects n tences of the documents where n is also determined by a compression rate
this method is the classic baseline for measuring the performance of automatic text summarization systems
artex another text artex algorithm
is another simple extractive algorithm
the main idea is to represent the text in a suitable space model
vsm
then an average document vector that represents the average the global topic of all sentence vectors is constructed
at the same time the lexical weight for each sentence the number of words in the sentence is obtained
after that the angle between the average document and each sentence is calculated
narrow angles indicate that the sentences near the global topic should be important and are therefore extracted
see figure for the vsm of words p vector sentences and the average global topic are represented in a n dimensional space of words
the angle between the sentence s and the global topic b is processed as follow
b s next a weight for each sentence is calculated using their proximity with the global topic and their lexical weight
in figure the lexical weight is fig

the global topic in a vector space model of n words
represented in a vsm of p sentences
narrow angles indicate that words closest to the lexical weight should be important
finally the summary is generated concatenating the sentences with the highest scores following their order in the original document
formally artex algorithm computes the score of each tence by calculating the inner product between a sentence vector an average pseudo sentence vector the global topic and an average pseudo word weight
once the pre processing is complete a matrix n words and p sentences is created
let s be a vector of the sentence

the average pseudo word vector a was dened as the average number of occurrences of n words used in the sentence s n j fig

the lexical weight in a vector space model of p sentences
topicssentencevsm of of and the average pseudo sentence vector occurrences of each word
j used through the p sentences
bj as the average number of bj p the weight of a sentence s is calculated as follows b
a
s
np n bj a


p the computed by equation must be normalized between the interval

the calculation of s b indicates the proximity between the sentence s and the average pseudo sentence b
a weight this proximity using the average pseudo word a
if a sentence s is near b and their corresponding element a has a high value therefore s will have a high score

moreover a sentence s far from a main topic s b is near or their corresponding element amu has a low value amu are near therefore s will have a low score
b
the product s
it is not really necessary to divide the scalar product by the constant because the angle between element a is only a scale factor that does not modify b and s is the same if

np
the
n bj a



p
the term is a constant value and then equation and equation are both equivalent
this summarization system outperforms the cortex one with
the fresa
measure
artex is evaluated with several corpus such as the medecina clinica

artex performance is then better than cortex on english spanish or french which are the targeted languages in this study
evaluation of lda model quality
the previous section described dierent summarization systems to reduce the size of train corpus and to retain only relevant information contained into the train documents
this section proposes a set of metrics to evaluate the quality of topic spaces generated from summaries of the train documents
the rst one is the perplexity
this score is the most popular one
we also propose to study another measure to evaluate the dispersion of each word into a given topic space

this measure is called the jensen shannon j s divergence

perplexity perplexity is a standard measure to evaluate topic spaces and more generally a probabilistic model
a topic model z is eective if it can correctly predict an unseen document from the test collection
the perplexity used in language modeling is monotonically decreasing in the likelihood of the test data and is algebraically equivalent to the inverse of the geometric mean per word likelihood

a lower perplexity score indicates better generalization performance
exp log p w
nb m with nb nd m where nb is the combined length of all m testing terms and nd is the number of words in the document d p w is the likelihood that the generative model will be assigned to an unseen word w of a document d in the test collection

the quantity inside the exponent is called the entropy of the test collection
the logarithm enables to interpret the entropy in terms of bits of information
jensen shannon j s divergence

the perplexity evaluates the performance of a topic space
another important information is the distribution of words in each topic
the kullback leibler divergence kl estimates how much a topic is dierent from the n topics contained in the topic model
this distribution is dened hereafter pi log wa pi where pi p and pj p are the probabilities that the word w is generated by the topic zi or
thus the symmetric kl divergence is named jensen shannon j s
divergence metric
it is the mid point measure between and
j s is then dened with equation as the mean of the divergences between zi and zi as j
pi log pj log pi pj
pj
pi
wa
the j s divergence for the entire topic space is then dened as the divergence between each pair of topics composing the topic model z dened in equation as j j zj z ziz ziz zj z wa pi log pj log pi pj pj pi
if i

j log pj pi

after dening the metrics to evaluate the quality of the model the next section describes the experiment data sets and the experimental protocol
experiments these summarization systems are used to compress and retain only relevant information into train text collection in each language
this section presents the experiments processed to evaluate the relevance and the eectiveness of the proposed system of fast and robust topic space building
first of all the experimental protocol is presented and then a qualitative analysis of obtained results is performed using evaluation metrics described in section
experimental protocol
in order to train topic spaces a large corpus of documents is required
three corpus was used
each c is in a particular language english spanish and french and is composed of a training set a and a testing set the corpus are composed of articles from wikipedia
thus for each of the three languages a set of documents is collected
of the corpus is summarized and used to build topic spaces while is used to evaluate each model no need to be summarized
table shows that the latin languages french and spanish have a similar size a dierence of less than is observed while the english one is bigger than the others english text corpus is times bigger than french or spanish corpus
in spite of the size dierence of corpus both of them have more or less the same number of words and sentences in an article
we can also note that the english vocabulary size is roughly the same than the latin languages

same observations can be made in table that presents statistics at document level mean on the whole corpus
in next section the outcome of this fact is seen during the perplexity evaluation of topic spaces built from english train text collection

as set of topic spaces is trained to evaluate the perplexity and the
shannon j s scores for each language as well as the processing time to marize and compress documents from the train corpus
following a classical study of lda topic spaces quality the number of topics by model is xed to
these topic spaces are built with the mallet toolkit
table
dataset statistics of the wikipedia corpus

language words unique words sentences english
spanish
french
table
dataset statistics per document of the wikipedia corpus

language words unique words sentences english
spanish
french
results the experiments conducted in this paper are topic based concern
thus each metric proposed in section perplexity and j s are applied for each language
english spanish and french for each topic space size and nally for each compression rate during the summarization process to of the original size of the documents
figures and present results obtained by varying the number of topics figure a to c and the percentage of summary figure respectively for perplexity and jensen shannon j s measures
results are computed with a mean among the various topic spaces size and a mean among the dierent reduced summaries size
moreover each language was study separately to point out dierences of topic spaces quality depending on the language
fig

perplexity by varying the number of topics for each corpus
fig

perplexity by varying the summary for each corpus
discussions
the results reported in figures and allow us to point out a rst general remark already observed in section the two latin languages have more or less the same tendencies
this should be explained by the root of these languages which are both latins
figure
shows that the spanish and french corpus obtain a perplexity between and when the number of classes in the topic space varies
another observation is that for these two languages topic spaces obtained with summarized documents outperform the ones obtained with complete documents when at least topics are considered figures b and
the best system for all languages is ordered in the same way
systems are ordered from the best to the worst in this manner artex bf this fact is explained in the next part and is noted into j s measure curves in figures and and then br
if we considerer a number of topics up to we can note that the topic spaces from full text documents not summarized with an english text corpus obtain a better perplexity smaller than documents processed with a summarization system that is particularly visible into figures

to address the shortcoming due to the size of the english corpus times bigger than latin languages the number of topics contained into the thematic space has to be increased to eectively disconnect words into topics
in spite of moving up the number of topics move down the perplexity of topic spaces for all summarization systems except random baseline rb the perplexity obtained with the english corpus being higher than those obtained from the spanish and french corpus
among all summarization systems used to reduce the documents from the train corpus the baseline bf obtains good results for all languages
this performance is due to the fact that bf selects the rst paragraph of the ment as a summary when a wikipedia content provider writes a new article he exposes the main idea of the article in the rst sentences
furthermore the rest of the document relates dierent aspects of the article subject such as historical or economical details which are not useful to compose a relevant summary
thus this baseline is quite hard to outperform when the documents to summarize are from encyclopedia such as wikipedia
fig

jensen shannon measure by varying the number of topics for each corpus
fig
jensen shannon measure by varying the summary for each corpus

the random baseline rb composes its summary by randomly selecting a set of sentences in an article
this kind of system is particularly relevant when the main ideas are disseminated in the document such as a blog or a website

this is the main reason why this baseline did not obtain good results except for j s divergence measure see figures and
this can be explained by the fact that this system selects sentences at dierent places and then selects a variable set of words
thus topic spaces from these documents contain a variable vocabulary
the j s divergence evaluates how much a word contained in a topic is discriminative and allows to distinguish this topic from the others that compose the thematic representation
figures and also show that jensen shannon j s divergence scores tween topics obtained a similar performance order of summarization systems for all languages
moreover full text documents always outperform all topic spaces representation for all languages and all summary rates
the reason is that full text documents contain a larger vocabulary and j s divergence is sensitive to the vocabulary size especially when the number of topics is equal for summarized and full text documents
this observation is pointed out by ures b and where the means among topic spaces for each summary rate of full text documents are beyond all summarization systems
last points of the curves show that topic spaces with a high number of topics and estimated from summaries do not outperform those estimated from full text documents but become more and more closer to these ones this conrms the original idea that have motivated this work
tables and nally present the processing time in seconds by varying the number of topics for each language corpus respectively with the full text and the summarized documents
we can see that processing time is saved when topic spaces are learned from summarized documents
moreover tables show that the processing times follow an exponential curve especially for the full text context

for this reason we can easily imagine the processing time that can be saved using summaries instead of the complete documents which inevitably contain non informative and irrelevant terms
a general remark is that the best summarization system is artex but if we take into account the processing time during the topic space learning the baseline bf is the best agreement
indeed if one want to nd a common ground between a low perplexity a high j s divergence between topics and a fast learning process the bf method should be chosen
table
processing time in seconds by varying the number of topics for each corpus
system
full text english spanish french language
table
processing time in seconds by varying the number of topics for each corpus
system
artex english spanish french br english spanish
french bf english spanish french language language language system system





conclusions in this paper a qualitative study of the impact of documents summarization in topic space learning is proposed
the basic idea that learning topic spaces from compressed documents is less time consuming than learning topic spaces from the full documents is noted
the main advantage to use the full text document in text corpus to build topic space is to move up the semantic variability into each topic and then increase the divergence between these ones
experiments show that topic spaces with enough topics size have more or less roughly the same divergence

thus topic spaces with a large number of topics suitable knowing the size of the corpus more than topics in our case have a lower perplexity a better divergence between topics and are less time consuming during the lda learning process
the only drawback of topic spaces learned from text corpus of summarized documents disappear when the number of topics comes up suitable for the size of the corpus whatever the language considered
references
salton automatic text processing the transformation
analysis and retrieval
blei ng jordan latent dirichlet allocation
the journal of machine
baeza yates ribeiro neto et al

modern information retrieval
volume of information by computer learning research
acm press new york
salton mcgill introduction to modern information retrieval


salton yang
on the specication of term values in automatic indexing
journal of documentation

deerwester dumais furnas landauer harshman indexing by latent semantic analysis
journal of the american society for information science
bellegarda
a latent semantic analysis framework for large span language eling
in fifth european conference on speech communication and technology

hofmann probabilistic latent semantic analysis
in proc of uncertainty in articial intelligence uai citeseer
bellegarda exploiting latent semantic information in statistical language eling
proceedings of the ieee
suzuki fukumoto sekiguchi keyword extraction using term domain interdependence for dictation of radio news
in international conference on computational linguistics
volume acl
popescul pennock lawrence probabilistic models for unied laborative and content based recommendation in sparse data environments

in proceedings of the seventeenth conference on uncertainty in articial intelligence morgan kaufmann publishers

louis nenkova
automatically evaluating content selection in
in empirical methods in natural language rization without human models

processing singapore

lin divergence measures based on the shannon entropy
ieee transactions
hofmann unsupervised learning by probabilistic latent semantic analysis
on information theory machine learning

minka laerty expectation propagation for the generative aspect model

in proceedings of the eighteenth conference on uncertainty in articial gence morgan kaufmann publishers
griths steyvers
finding scientic topics
proceedings of the national academy of sciences of the united states of america
geman geman stochastic relaxation gibbs distributions and the bayesian restoration of images
ieee transactions on pattern analysis and machine ligence

heinrich
parameter estimation for text analysis
web arbylon
net publications text est pdf
torres moreno automatic text summarization
wiley and sons
torres moreno
artex is another text summarizer

ledeneva gelbukh garca hernandez terms derived from frequent sequences for extractive text summarization
in computational linguistics and intelligent text processing
springer
manning schutze foundations of statistical natural language ing
the mit press cambridge massachusetts
duc document understanding conference


torres moreno velazquez morales meunier
cortex un algorithme
in
volume lyon pour la condensation automatique textes

france

torres moreno saggion cunha sanjuan velazquez morales summary evaluation with and without references
polibits
rosen zvi griths steyvers smyth
the author topic model for authors and documents
in proceedings of the conference on uncertainty in articial intelligence auai press
mccallum mallet a machine learning for language toolkit


