a m l c
s
c
v
v
i x r a controllableabstractivesummarizationangelafandavidgrangierfacebookairesearchmenlopark california usamichaelauliabstractcurrentmodelsfordocumentsummarizationdisregarduserpreferencessuchasthedesiredlength style theentitiesthattheusermightbeinterestedin orhowmuchofthedocu menttheuserhasalreadyread
wepresentaneuralsummarizationmodelwithasimplebuteffectivemechanismtoenableuserstospecifythesehighlevelattributesinordertocontroltheshapeofthefinalsummariestobettersuittheirneeds
withuserinput oursystemcanproducehighqualitysummariesthatfollowuserpreferences
withoutuserin put wesetthecontrolvariablesautomatically onthefulltextcnn dailymaildataset
summarizationalgorithmsareeitherextractiveorabstractive
extractivealgorithmsformsummariesbypastingtogetherrelevantportionsoftheinput
incontrasttopreviouswork describesmech anismsthatenablethereadertocontrolimportantaspectsofthegeneratedsummary
thereadercanselectthedesiredlengthofthesummarydependingonhowdetailedtheywouldlikethesummarytobe
thereadercanrequirethetexttofocusonentitiestheyhaveaparticularinterestin
weletthereaderchoosethestyleofthesummarybasedontheirfavoritesourceofinformation
finally weallowthereadertospecifythattheyonlywanttosummarizeaportionofthearticle forexampletheremainingparagraphstheyhaventread
ourworkbuildsuponsequence to







alizedgenerationandfullyleveragethatautomaticsummariesaregeneratedatthereadersrequest

ourcomparisonwithstate of the artmodelsonthestandardcnn
sentencesummarizationnewscorpus highlightstheadvantageofourapproach
onboththeentity weoutperformpreviouspointer basedmodelstrainedwithmaximumlikelihooddespitetherelativesimplicityofourmodel
further to



theweightsarepredictedfromthecurrentdecoderstates allowingthedecodertoemphasizethepartsoftheinputdocumentwhicharethemostrelevantforgeneratingthenexttoken
weusemulti hopattention
danauetal
attentioninthedecodertoenablethemodeltoreferbacktopreviouslygeneratedwords
thisallowsthedecodertokeeptrackofitsprogressandreducesthegenerationofre

wealternatebetweeneachtypeofattentionateverylayer
muchpriorworkonthecnn dailymailbench

instead werelyonsub wordtokenizationandweightsharing
weshowthissimpleapproachisveryeffective
specifically weusebyte pair
constrainedsummarizationsummarizationallowsareaderwithlimitedtimetoquicklycomprehendtheessenceofadocument
controllingsummarylengthenablesreadingwithdifferenttimebudgets adocumentmightbesummarizedasafive wordheadline asinglesentenceoraparagraph eachprovidingmoreandmoredetail
toenabletheusertocontrollength wefirstquan tizesummarylengthintodiscretebins eachrepresent ingasizerange
lengthbinsarechosensothattheyeachcontainroughlyanequalnumberoftrainingdoc uments
wethenexpandtheinputvocabularywithspecialwordtypestoindicatethelengthbinofthedesiredsummary whichallowsgenerationtobecondi tioneduponthisdiscretelengthvariable
fortraining weprependtheinputofoursummarizerwithamarkerthatindicatesthelengthoftheground truthsummary
attesttime wecontrolthelengthofgeneratedtextbyprependingaparticularlengthmarkertoken
outputlengthiseasilycontrolledbychangingthelengthmarkerandsupplyinggroundtruthmarkersdrasticallyimprovessummaryquality
centricsummarizationthereadermightbeinterestedinadocumenttolearnaboutspecificentities suchaspeopleorlocations
forexample asportsfanreadingaboutarecentgamemightwanttofocusthesummaryontheperformanceoftheirfavoriteplayer
toenableentity centricsummaries wefirstanonymizeentitiesbyreplacingalloccurrencesofagivenentityinadocumentbythesametoken
fortraining wealsoanonymizethecorrespondingreferencesummary
allowingourapproachtoscaletomanyentitiesandgeneralizetounseenones
wethenexpressthatanentityshouldbepresentinthegeneratedsummarybyprependingtheentitytokentotheinput
ineffect thisinstructsthemodeltofocusonsentencesthatmentionthemarkedentities
attrainingtime weprependeachdocumentwithmarkersreferringtoanentityfromtheground truthsummary
toensuretheentityrequestisinformative weprovideanentitythatispresentintheground truthbutnotpresentinthesummarygeneratedbythebase linemodel
attesttime wemayspecifyanyentitymarkerthatwewishthesummarytocontain

weshowthathigheraccuracyisachievedwhenwespecifyentitiesfromthefirstfewsentencesofadocumentorifwesupplymarkerstakenfromthereferencesummarytoillustratespecificuserpreferences
weextendthisapproachtomultipleentitymarkersandexperimentwithappend ingallground
weshowthatpro specificsummarizationtextsourcessuchasnewspapersandmagazinesoftenhavespecificstyleguidelinestoprovideaconsistent experience
readersareaccustomedtothestylesoftheirfavoritesources
therefore weenableuserstospecifyapreferredsourcestyleforasummary
similartolengthandentities

fortraining wepreprendtheinputwiththemarkercorrespondingtotheground truthsource
atinference wecontrolthestyleofgeneratedsummarybyprependingdifferentmarkers
styleproducessummariesthatareclosertothereferencesummary
weadditionallyprovideexamplesofdistinctsummariesresultingfromchangingsource readersmaywanttheflexibilityofonlysummarizingcertainportionsofadocument
forexample areaderwhohasreadthefirstfewparagraphswouldwantasum maryoftheremainingtexttocoverwhattheymissed
trainingandevaluatingremaindersummarizationrequiresspecificdata namelyadatasetoffulldocumentswithpositionmarkersseparatingthealreadyreadportionfromtheremainderpartalongwiththecorrespondingsummaries
suchadatasetisnotreadilyavailableandwouldbechallengingtocollect
toenableremaindersummarizationwithoutsuchdata wealignsummariestofulldocuments
ourprocedurematcheseachreferencesummarysentencetoitsbestmatchingdocumentsentencebasedonrouge weremovesentencesalignedbeforethispointfromthefullsummaryandconsiderthisshortersummaryasthesummaryoftheremainder
inourexperiment weconsiderasreadportionsallarticlepositionslocatedatthemiddleoftwoalignmentpoints
thebaselinemodelpredictsafullsummary inferencealignment afullsummaryisgeneratedfromthebaselinemodelandthesummaryisshortenedwithouralignmentprocedure
thede themodelistrainedtomapthedocumentremainderstotheremaindersummariesonpre alignedtrainingdata

themodelreceivesbothreadportionofthearticleandtheremainderseparatedbyaspecialtoken
itistrainedtopredicttheremaindersummary
wedistinguishthereadandremainderpartofthearticlebyusingdistinctsetsofpositionembeddings to tivemethodshavebenefitedfromadvancesinnaturallanguageprocessing patternrecognition
sequence to
lapatietal












summarizationalsohasdistinctchal lenges
thegenerationofmulti sentencesummariesdiffersfromsinglesentencetranslation left to rightdecodersneedtobeawareoftheirpreviousgener ationatalargertimescale otherwisemodelstendtoproducerepeatedtext
toaddressthisimpedi

decoderattention

themostcommonautomaticmetrictoassesssummarization
combiningbothstrategiesisfoundtoperformbestinhumanevaluations astrainingwithrlaloneoftenproducesnon grammaticaltext
ourworkbuildsuponpriorresearch whichenablefastertraining
thiscontrastswithprior


hopintra attentioninspiredbymulti
wesharethewordrepresen






ingsummariesgiventhesourcedocument
butthisaspectislargelyorthogonaltoourmaingoal suchastheintroductionofvariationalauto




con trollablegenerationisanemergingresearchfield
researchincomputervisionincludesstyletrans


suchaslength
othershaveworkedonstyle turestotranslatetextindifferentdomains

timerestrictionsandtraining timelengthtokenembeddings
motivatedbysimplicity ourworkreliesonconditionallanguagemodelinganddoesnotrequireadversarialtraining latentvariablemodelssuchasvariationalauto encoders orpointernetworks

weusethecnn

sentencesummaries
onaverage







fornon bpemodels
further sentencesumma rizationtask

architecture training andgeneration dailymail


facebookresearch fairseq wepreventthedecoderfromgeneratingthesametrigrammorethanonce onthecnn dailymailbenchmark lapatietal



notethat althoughsimple thisbaselineisnotoutperformedbyallmodels
forhumanevaluation


weana
tions
bothofwhicharedifficulttomodelinword basedvocabular ies

wefindtuningthemin baselinewithoutcontrolvariables
eachrowaddafeatureontopofthepreviousrowfeatures
summarizationwithoraclecontroltosimulateuserpreference

fixedcontrolvariablesonentity anonymizedtext
evenwithfixedvariables entitiesonwhichitfocuseson
thissimulatesausersettingtheirpreferencestospecificvalues
wethenassesstheeffectofprovidingnon referencecontrolvariables

allcontrolvariablesimprovethesummaryquality butlengthcontrolhasthemost
summarizationwithfixedcontrolvariablesonoriginaltext
evenwithafixedsetting followedbyentitycontrolandsourcestyle
further theadvantagesofeachcontrolvariablecumu lativelyproduceanevenstrongersummary
thebaselinestrugglesatpredictingcorrectlengths
thelatterisduetolargeuncertaintyinsummarylength

themodelisshowntorespectlengthmarkers

entitycontrolhaslessimpactonrougecom truthwithouttheneedforadditionalinstruction


wereporthowoftentheentity centricmodelgeneratesasummarythatactuallycon tainstherequestedentity
whileforallentitiesfromtheinput themodelmen
inbothset tings theseratesaremuchhigherthanthebaseline
themodelhasdifficultygeneratingsummarieswithentitieswhichareunlikelytoappearinthehumanreferences ginningofthearticle
source stylecontrolistheleastimpactfulcontrolintermsofrouge weobservethatgeneratedsummariesinthedailymail stylearemorerepetitiveandslightlylongerthanthecnn stylesummaries
thismatchesthedifferencesbetweenthetwosourcesinthereferencetext
theimpactofstylerequestsmightbegreaterwitharichersetofstyles infuturework
however wecanalsosetthecontrolvariablesautomaticallyinabsenceofreaderdesiderata
forlengthandsource style wesetthevariabletoaconstantvaluethatmaximizesrougeonthevalidationset
forentitycontrol
anonymizedver


ourmethodisadvantageousoveralternatives
further
ontheoriginaltext
anonymizedtext

trainingobjectivesareorthogonaltoourworkoncontrolvariablesandweexpectreinforcementlearningtoequallybenefitourmodel

tion
notably rougeimprovesmoreforshortertextevaluation likelybecauserequestingashorterdocu mentallowsthemodeltoplanitsgeneration
compar

incontrast wesimplyprovidethedesiredlengthasaspecialtokenandshowthissimpleapproachiseffective
lastly wenotethatlength

lengthcontrolvssummarylength

baselineentity fractionofrequestedentityactuallyoccurringindecodedsummaries
dailymailsincetruncatedrecall rougeeval uationdoesnotpenalizelengthmismatchstrongly
overall theimprovementsfromautomaticcontrolshowthatabettermodelcanbeobtainedbyprovidingadditionalinformationduringtraining
whenthemodelisnotrequiredtopredictthesummarylengthortheentitiesofinterest itcanassignmorecapacitytogeneratingtextconditionedonthesevariables
thisisparticularlyusefulforvariableswhicharehardtopredictfromtheinputduetointrinsicuncertaintylikelength
insubsequentwork weplantoexplorearchitecturestoexplicitlydividethepredictionofcontrolvariablesandsequence to sequencemapping

thistaskismoredifficultthansummarizingtheentirearticle
first thelengthofbothreadportionsandsummariesvariesgreatly
itisdifficultforthemodeltodistinguishinformationspe cifictotheremainingportionofthedocumentfromthegeneralpointofthearticle
despitethis whenmodelstrainedonsummarizingtheremainderaretaskedwithsummarizingonlyfulldocuments theper re gardlessoftheportionofthearticlepresentedasinput
formingtheremaindersummariespost inferenceperformspoorlyasitde pendslargelyonalignmentquality
thenewsarticlesarerepetitive soonesummarysentencecanaligntomultiplelocationsinthesource
trainingthemodeltoperformremaindersummarizationsignificantlyimprovesourresults
modelsthatreceiveonlythere
anexplicitmethodforeliminatingredundancybetweenthereadandtheremainderisrelevantfuturework
remaindersummarylengthisparticularlydifficulttopredict
wethereforerelyonlengthcontrol
wedecodethetestdatawiththissettingwhichpro partitioningisnotanaccuratelengthmodelandwehypothesizethatlengthcontrolcouldprovideagreaterimprovementwithabettermodel



thisyear becauseofreallyhighsummerrainfall whichledtogreatfoodavailability
thisyear becauseofreallyhighsummerrainfall whichledtogreatfoodavailability


summarywithsource stylecontrolbluehighlightsdifferenttextrequestingcnn style
requestingdailymail style


hellbeabletogambleinacasino



summarieswithvarioussettingsforusercontrolvariablesandremaindersummarization



ourmodelcanthereforeimprovesummaryqualityinadiscernibleway
asanaside wefindthatrougeandratingsagreeintwo thirdsofthecases levelattributesofgeneratedsummaries suchaslength source style entitiesofinterest andsummarizingonlyremainingportionsofadocument
wesimulateuserpreferencesforthesevariablesbysettingthemtooraclevaluesandshowlargerougegains
thecontrolvariablesareeffectivewithoutuserinputwhichwedemonstratebyassigningthemfixedvaluestunedonaheld outset
thisout performscomparablestateoftheartsummarizationmodelsforbothrougeandhumanevaluation
acknowledgmentswethankyanndauphinforhelp fuldiscussions

wethankjonasgehringanddenisyaratsforwritingthefairseqtoolkitusedfortheseexperiments
referencesdzmitrybahdanau kyunghyuncho
lukevilnis oriolvinyals an rafaljozefowicz
michaelauli

angelafan michaelauli

tionforquestionanswering

leonagatys michaelauli davidgrangier denisyarats
jeanpouget abadie mehdimirza bingxu davidwarde farley sherjilozair
tomaskocisky edwardgrefen stette lasseespeholt willkay mustafasuleyman
zichaoyang xiaodanliang ruslansalakhutdinov
grahamneubig ryoheisasano hiroyatakamura encodingvariationalbayes josepcrego
inproceedingsoftheinternationalconferencerecentadvancesinnaturallanguageprocessing bulgaria neilzeghidour nicolasusunier antoinebordes ludovicdenoyer manipulatingimagesbyslidingattributes
bernhardeboser johnsdenker donniehenderson richardehoward wayneehubbard propagationnetwork
apackageforautomaticevaluationofsummaries
inworkshopontextsummarizationbranchesout

thangluong ilyasutskever oriolvinyals
multi tasksequencetosequencelearning
hieupham
effectiveapproachestoattention basedneuralmachinetranslation

cambridgeuniversitypress
rameshnallapati feifeizhai arecurrentneuralnetworkbasedsequencemodelforextractivesummarizationofdocuments
bowenzhou caglargulcehre bingxiang to sequencernnsandbeyond
con kathleenmckeown maticsummarization
tomasmikolov
caimingxiong sandeepsubramanian francisdutil christopherjosephpal
workshoponrepresentationlearningfornlp
alexandermrush seasharvard sumitchopra
peterjliu summarizationwithpointer generatornetworks
barryhaddow
controllingpolitenessinneuralmachinetranslationviasideconstraints
humanlanguagetechnologies
associationforcomputa tionallinguistics sandiego california barryhaddow
neuralmachinetranslationofrarewordswithsubwordunits
taolei reginabarzilay paralleltextbycross alignment
jamesmartens tializationandmomentumindeeplearning
oriolvinyals
offredundantrepeatinggenerationsforneuralabstractivesummarization
masaakinagata
taipei taiwan noamshazeer nikiparmar jakobuszkoreit llionjones aidanngomez lukaszkaiser meirefortunato
pointernetworks alexandertoshev samybengio
showandtell aneuralimagecaptiongenerator
weinanzhang junwang
sequencegenerativeadversarialnetswithpolicygradient



