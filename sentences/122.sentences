a m l c
s c v
v i x r a controllableabstractivesummarizationangelafandavidgrangierfacebookairesearchmenlopark california usamichaelauliabstractcurrentmodelsfordocumentsummarizationdisregarduserpreferencessuchasthedesiredlength style theentitiesthattheusermightbeinterestedin orhowmuchofthedocu menttheuserhasalreadyread
wepresentaneuralsummarizationmodelwithasimplebuteffectivemechanismtoenableuserstospecifythesehighlevelattributesinordertocontroltheshapeofthefinalsummariestobettersuittheirneeds
withuserinput oursystemcanproducehighqualitysummariesthatfollowuserpreferences
withoutuserin put wesetthecontrolvariablesautomatically onthefulltextcnn dailymaildataset


rougeandhumanevaluation

summarizationalgorithmsareeitherextractiveorabstractive
extractivealgorithmsformsummariesbypastingtogetherrelevantportionsoftheinput

thisworkfocusesonabstractivesummarizationand incontrasttopreviouswork describesmech anismsthatenablethereadertocontrolimportantaspectsofthegeneratedsummary
thereadercanselectthedesiredlengthofthesummarydependingonhowdetailedtheywouldlikethesummarytobe
thereadercanrequirethetexttofocusonentitiestheyhaveaparticularinterestin
weletthereaderchoosethestyleofthesummarybasedontheirfavoritesourceofinformation e

aparticularnewssource
finally weallowthereadertospecifythattheyonlywanttosummarizeaportionofthearticle forexampletheremainingparagraphstheyhaventread
ourworkbuildsuponsequence to









weintroduceastraightforwardandextensiblecontrollablesummarizationmodeltoenableperson alizedgenerationandfullyleveragethatautomaticsummariesaregeneratedatthereadersrequest

ourcomparisonwithstate of the artmodelsonthestandardcnn
sentencesummarizationnewscorpus highlightstheadvantageofourapproach
onboththeentity

weoutperformpreviouspointer basedmodelstrainedwithmaximumlikelihooddespitetherelativesimplicityofourmodel
further wedemonstrateinablindhumanevaluationstudythatourmodelgeneratessummariespreferredbyhumanreaders


to sequenceourapproachbuildsupontheconvolutionalmodelofgehringetal

theencoderanddecoder





theweightsarepredictedfromthecurrentdecoderstates allowingthedecodertoemphasizethepartsoftheinputdocumentwhicharethemostrelevantforgeneratingthenexttoken
weusemulti hopattention i
e
attentionisappliedateachlayerofthedecoder
danauetal
attentioninthedecodertoenablethemodeltoreferbacktopreviouslygeneratedwords
thisallowsthedecodertokeeptrackofitsprogressandreducesthegenerationofre


tocombineencoderanddecoderattention wealternatebetweeneachtypeofattentionateverylayer
muchpriorworkonthecnn dailymailbench

instead werelyonsub wordtokenizationandweightsharing
weshowthissimpleapproachisveryeffective
specifically weusebyte pair

wesharetherepresentationofthetokensintheencoderanddecoderembeddingsandinthelastdecoderlayer

constrainedsummarizationsummarizationallowsareaderwithlimitedtimetoquicklycomprehendtheessenceofadocument
controllingsummarylengthenablesreadingwithdifferenttimebudgets adocumentmightbesummarizedasafive wordheadline asinglesentenceoraparagraph eachprovidingmoreandmoredetail
toenabletheusertocontrollength wefirstquan tizesummarylengthintodiscretebins eachrepresent ingasizerange
lengthbinsarechosensothattheyeachcontainroughlyanequalnumberoftrainingdoc uments
wethenexpandtheinputvocabularywithspecialwordtypestoindicatethelengthbinofthedesiredsummary whichallowsgenerationtobecondi tioneduponthisdiscretelengthvariable
fortraining weprependtheinputofoursummarizerwithamarkerthatindicatesthelengthoftheground truthsummary
attesttime wecontrolthelengthofgeneratedtextbyprependingaparticularlengthmarkertoken

outputlengthiseasilycontrolledbychangingthelengthmarkerandsupplyinggroundtruthmarkersdrasticallyimprovessummaryquality
wecompareourmethodtokikuchietal


centricsummarizationthereadermightbeinterestedinadocumenttolearnaboutspecificentities suchaspeopleorlocations
forexample asportsfanreadingaboutarecentgamemightwanttofocusthesummaryontheperformanceoftheirfavoriteplayer
toenableentity centricsummaries wefirstanonymizeentitiesbyreplacingalloccurrencesofagivenentityinadocumentbythesametoken
fortraining wealsoanonymizethecorrespondingreferencesummary




thisabstractsawaythesurfaceform allowingourapproachtoscaletomanyentitiesandgeneralizetounseenones
wethenexpressthatanentityshouldbepresentinthegeneratedsummarybyprependingtheentitytokentotheinput
ineffect thisinstructsthemodeltofocusonsentencesthatmentionthemarkedentities
attrainingtime weprependeachdocumentwithmarkersreferringtoanentityfromtheground truthsummary
toensuretheentityrequestisinformative weprovideanentitythatispresentintheground truthbutnotpresentinthesummarygeneratedbythebase linemodel
attesttime wemayspecifyanyentitymarkerthatwewishthesummarytocontain


weshowthathigheraccuracyisachievedwhenwespecifyentitiesfromthefirstfewsentencesofadocumentorifwesupplymarkerstakenfromthereferencesummarytoillustratespecificuserpreferences
weextendthisapproachtomultipleentitymarkersandexperimentwithappend ingallground
weshowthatpro vidingmoreentitiesimprovessummarizationquality

specificsummarizationtextsourcessuchasnewspapersandmagazinesoftenhavespecificstyleguidelinestoprovideaconsistent experience
readersareaccustomedtothestylesoftheirfavoritesources
therefore weenableuserstospecifyapreferredsourcestyleforasummary
similartolengthandentities



fortraining wepreprendtheinputwiththemarkercorrespondingtotheground truthsource
atinference wecontrolthestyleofgeneratedsummarybyprependingdifferentmarkers
styleproducessummariesthatareclosertothereferencesummary
weadditionallyprovideexamplesofdistinctsummariesresultingfromchangingsource styleconditioning

readersmaywanttheflexibilityofonlysummarizingcertainportionsofadocument
forexample areaderwhohasreadthefirstfewparagraphswouldwantasum maryoftheremainingtexttocoverwhattheymissed
trainingandevaluatingremaindersummarizationrequiresspecificdata namelyadatasetoffulldocumentswithpositionmarkersseparatingthealreadyreadportionfromtheremainderpartalongwiththecorrespondingsummaries
suchadatasetisnotreadilyavailableandwouldbechallengingtocollect
toenableremaindersummarizationwithoutsuchdata wealignsummariestofulldocuments
ourprocedurematcheseachreferencesummarysentencetoitsbestmatchingdocumentsentencebasedonrouge l
foranypositioninthedocument
inourexperiment weconsiderasreadportionsallarticlepositionslocatedatthemiddleoftwoalignmentpoints
thebaselinemodelpredictsafullsummary disregardingtheseparationofthereadportionfromtheremainder
inferencealignment afullsummaryisgeneratedfromthebaselinemodelandthesummaryisshortenedwithouralignmentprocedure
thede codedsummarysentencesthataligntotheremainderportioncomposethesummaryoftheremainder
themodelistrainedtomapthedocumentremainderstotheremaindersummariesonpre alignedtrainingdata
thismodelisnotgiventhereadportionofthearticle
themodelreceivesbothreadportionofthearticleandtheremainderseparatedbyaspecialtoken
itistrainedtopredicttheremaindersummary
wedistinguishthereadandremainderpartofthearticlebyusingdistinctsetsofpositionembeddings
readportionandtheremainderofthedocument

to
extractiveandabstrac tivemethodshavebenefitedfromadvancesinnaturallanguageprocessing patternrecognition

recently sequence to
lapatietal







neuralabstractivesummarizationhasbuiltuponadvancesfrommachinetranslationandrelatedfields





however summarizationalsohasdistinctchal lenges
thegenerationofmulti sentencesummariesdiffersfromsinglesentencetranslation left to rightdecodersneedtobeawareoftheirpreviousgener ationatalargertimescale otherwisemodelstendtoproducerepeatedtext
toaddressthisimpedi

decoderattention

themostcommonautomaticmetrictoassesssummarization
combiningbothstrategiesisfoundtoperformbestinhumanevaluations astrainingwithrlaloneoftenproducesnon grammaticaltext
ourworkbuildsuponpriorresearch
whichenablefastertraining
thiscontrastswithprior



weborrowintra
hopintra attentioninspiredbymulti

tofacilitatecopyinginputentities wesharethewordrepresen






un
ingsummariesgiventhesourcedocument
ourmodelisamenabletorl butthisaspectislargelyorthogonaltoourmaingoal i
e
controllablesummarization


thefieldfollowsrecentadvancesingenerativemodels suchastheintroductionofvariationalauto






buildinguponunconditionedgeneration con trollablegenerationisanemergingresearchfield
researchincomputervisionincludesstyletrans


textgenerationworkfocusesoncontrollingtenseorsentimentwithvariationalauto

shenetal


takenoetal
suchaslength
othershaveworkedonstyle
turestotranslatetextindifferentdomains

kikuchietal
timerestrictionsandtraining timelengthtokenembeddings
motivatedbysimplicity ourworkreliesonconditionallanguagemodelinganddoesnotrequireadversarialtraining latentvariablemodelssuchasvariationalauto encoders orpointernetworks

weleavetheassessmentofhowadditionallatentvariablesmightimproveuponourresultstofuturework



itconsistsofnewsarticlesalongwithmulti sentencesummaries
onaverage


weevaluateontwoversionsofthedata






fornon bpemodels inputandoutputvocabularieshaveresp

further
sentencesumma rizationtask
wetrainonenglishgigawordfollowingtheprotocolofrushetal


headlineofnewsarticles



architecture training andgeneration
forcnn dailymail


forduc
similartogehringetal






wereducethelearningratebyanorderofmagnitudewhenthevalidationperplexityceasestoimprove

com facebookresearch fairseq
toavoidrepetition wepreventthedecoderfromgeneratingthesametrigrammorethanonce followingpaulusetal

evaluation onthecnn dailymailbenchmark
lapatietal




notethat althoughsimple thisbaselineisnotoutperformedbyallmodels
forhumanevaluation weconductahumanevaluationstudyusingamazonmechanicalturkandthetestsetgenerationoutputofseeetal






weana lyzetheperformanceoftheremaindersummarizationtaskanddemonstratetheadvantageofmodelingboththereadandremainderportionsofthedocument




addingintra

themodestimprovementislikelybecausethetwofeaturesaddressasimilarproblemofavoidingrepeatedgenera tions


bpeimprovestheabilitytocopypropernounsandrareinflections bothofwhicharedifficulttomodelinword basedvocabular ies


lastly wefindtuningthemin
















baselinewithoutcontrolvariables
eachrowaddafeatureontopofthepreviousrowfeatures















summarizationwithoraclecontroltosimulateuserpreference













intra















fixedcontrolvariablesonentity anonymizedtext
evenwithfixedvariables thecontrollablemodelimprovesrougecomparedtomlalternatives

entitiesonwhichitfocuseson
wefirstevaluatetheeffectofprovidingtheoraclereferencevariablesatdecodingtime
thissimulatesausersettingtheirpreferencestospecificvalues
wethenassesstheeffectofprovidingnon referencecontrolvariables

allcontrolvariablesimprovethesummaryquality butlengthcontrolhasthemost















summarizationwithfixedcontrolvariablesonoriginaltext
evenwithafixedsetting thecontrolledsummarizationmodelimprovesrouge
impact followedbyentitycontrolandsourcestyle
further theadvantagesofeachcontrolvariablecumu lativelyproduceanevenstrongersummary



thisimprovementisduetotwoeffects rouge
moreover thebaselinestrugglesatpredictingcorrectlengths
thelatterisduetolargeuncertaintyinsummarylength i
e
evenhumanshavedifficultypredictingthecorrectlength

themodelisshowntorespectlengthmarkers

entitycontrolhaslessimpactonrougecom



thisismainlybecauseoursummariesoftenalreadycontainmostentitiesfromtheground truthwithouttheneedforadditionalinstruction


wethenrepeattheexperimentwitheachentityfromthefullarticle
wereporthowoftentheentity centricmodelgeneratesasummarythatactuallycon tainstherequestedentity
whileforallentitiesfromtheinput themodelmen
inbothset tings theseratesaremuchhigherthanthebaseline
themodelhasdifficultygeneratingsummarieswithentitieswhichareunlikelytoappearinthehumanreferences e

unimportantentitiesfarfromthebe ginningofthearticle
source stylecontrolistheleastimpactfulcontrolintermsofrouge


generally weobservethatgeneratedsummariesinthedailymail stylearemorerepetitiveandslightlylongerthanthecnn stylesummaries
thismatchesthedifferencesbetweenthetwosourcesinthereferencetext
theimpactofstylerequestsmightbegreaterwitharichersetofstyles infuturework weplantoevaluateondatasetswherevariedstylesareavailable


however wecanalsosetthecontrolvariablesautomaticallyinabsenceofreaderdesiderata
forlengthandsource style wesetthevariabletoaconstantvaluethatmaximizesrougeonthevalidationset
forentitycontrol
anonymizedver



inbothcases ourmethodisadvantageousoveralternatives
further
ontheoriginaltext



ontheentity anonymizedtext






however trainingobjectivesareorthogonaltoourworkoncontrolvariablesandweexpectreinforcementlearningtoequallybenefitourmodel


tion
notably rougeimprovesmoreforshortertextevaluation likelybecauserequestingashorterdocu mentallowsthemodeltoplanitsgeneration
compar ingtokikuchietal


incontrast wesimplyprovidethedesiredlengthasaspecialtokenandshowthissimpleapproachiseffective
lastly wenotethatlength




























lengthcontrolvssummarylength

baselineentity



fractionofrequestedentityactuallyoccurringindecodedsummaries

tocnn dailymailsincetruncatedrecall rougeeval uationdoesnotpenalizelengthmismatchstrongly
overall theimprovementsfromautomaticcontrolshowthatabettermodelcanbeobtainedbyprovidingadditionalinformationduringtraining
whenthemodelisnotrequiredtopredictthesummarylengthortheentitiesofinterest itcanassignmorecapacitytogeneratingtextconditionedonthesevariables
thisisparticularlyusefulforvariableswhicharehardtopredictfromtheinputduetointrinsicuncertaintylikelength
insubsequentwork weplantoexplorearchitecturestoexplicitlydividethepredictionofcontrolvariablesandsequence to sequencemapping



inferencealign













remaindersummarization




generally thistaskismoredifficultthansummarizingtheentirearticle
first thelengthofbothreadportionsandsummariesvariesgreatly
itisdifficultforthemodeltodistinguishinformationspe cifictotheremainingportionofthedocumentfromthegeneralpointofthearticle
despitethis whenmodelstrainedonsummarizingtheremainderaretaskedwithsummarizingonlyfulldocuments theper


ourbaselinealwayspresentsthefullsummary re gardlessoftheportionofthearticlepresentedasinput


amongourthreeproposedmethods formingtheremaindersummariespost inferenceperformspoorlyasitde pendslargelyonalignmentquality
thenewsarticlesarerepetitive soonesummarysentencecanaligntomultiplelocationsinthesource
trainingthemodeltoperformremaindersummarizationsignificantlyimprovesourresults
modelsthatreceiveonlythere


wehypothesizethatpresentingthereadportionofthearticleimprovesthequalityasthemodelcanfocusonthenewinformationintheremainder
anexplicitmethodforeliminatingredundancybetweenthereadandtheremainderisrelevantfuturework
remaindersummarylengthisparticularlydifficulttopredict
wethereforerelyonlengthcontrol
wedecodethetestdatawiththissettingwhichpro

however partitioningisnotanaccuratelengthmodelandwehypothesizethatlengthcontrolcouldprovideagreaterimprovementwithabettermodel
a



thisyear becauseofreallyhighsummerrainfall whichledtogreatfoodavailability

thisyear becauseofreallyhighsummerrainfall whichledtogreatfoodavailability







alcoholcontent

summarywithsource stylecontrolbluehighlightsdifferenttextrequestingcnn style
hewaswearingbulletproofvest butroundenteredinhisarmandwentthroughhischest

requestingdailymail style




drinkandcelebrityparties

hellbeabletogambleinacasino buyadrinkinapuborseethehorrorfilm


summarieswithvarioussettingsforusercontrolvariablesandremaindersummarization

seeetal











ourmodelcanthereforeimprovesummaryqualityinadiscernibleway
asanaside wefindthatrougeandratingsagreeintwo thirdsofthecases whereatleastfouroutoffivehumansagree
levelattributesofgeneratedsummaries suchaslength source style entitiesofinterest andsummarizingonlyremainingportionsofadocument
wesimulateuserpreferencesforthesevariablesbysettingthemtooraclevaluesandshowlargerougegains
thecontrolvariablesareeffectivewithoutuserinputwhichwedemonstratebyassigningthemfixedvaluestunedonaheld outset
thisout performscomparablestateoftheartsummarizationmodelsforbothrougeandhumanevaluation
acknowledgmentswethankyanndauphinforhelp fuldiscussions

wethankjonasgehringanddenisyaratsforwritingthefairseqtoolkitusedfortheseexperiments
referencesdzmitrybahdanau kyunghyuncho andyoshuabengio

neuralmachinetranslationbyjointlylearningtoalignandtranslate


bowman lukevilnis oriolvinyals an drewm
dai rafaljozefowicz andsamybengio

generatingsentencesfromacontinuousspace
inconll
sumitchopra michaelauli andalexandermrush

abstractivesentencesummarizationwithattentiverecurrentneuralnetworks

dipanjandasandandreftmartins

asurveyonautomatictextsummarization

yannn
dauphin angelafan michaelauli anddavidgrangier

languagemodelingwithgatedconvolutionalnetworks
jessicaficlerandyoavgoldberg

controllinglinguisticstyleaspectsinneurallanguagegeneration


katjafilippova

sentenceandpassagesummariza tionforquestionanswering

andmatthiasbethge

aneuralalgorithmofartisticstyle


jonasgehring michaelauli davidgrangier denisyarats

convolutionalsequencetosequencelearning
ianj
goodfellow jeanpouget abadie mehdimirza bingxu davidwarde farley sherjilozair
courville andyoshuabengio

generativeadversarialnets
innips
karlmoritzhermann tomaskocisky edwardgrefen stette lasseespeholt willkay mustafasuleyman andphilblunsom

teachingmachinestoreadandcomprehend

zhitinghu zichaoyang xiaodanliang ruslansalakhutdinov andericp
xing

towardcontrolledgenerationoftext
inicml
yutakikuchi grahamneubig ryoheisasano hiroyatakamura andmanabuokumura

controllingoutputlengthinneuralencoder decoders


diederikp
kingmaandmaxwelling

auto encodingvariationalbayes


catherinekobus josepcrego andjeansenellart

domaincontrolforneuralmachinetranslation
inproceedingsoftheinternationalconferencerecentadvancesinnaturallanguageprocessing
incomaltd
varna bulgaria
guillaumelample neilzeghidour nicolasusunier antoinebordes ludovicdenoyer andmarcaurelioranzato

fadernetworks manipulatingimagesbyslidingattributes


yannlecun bernhardeboser johnsdenker donniehenderson richardehoward wayneehubbard andlawrencedjackel

handwrittendigitrecognitionwithaback propagationnetwork


chin yewlin

rouge apackageforautomaticevaluationofsummaries
inworkshopontextsummarizationbranchesout
h
p
luhn

theautomaticcreationofliteratureabstracts

minh thangluong quocv
le ilyasutskever oriolvinyals andlukaszkaiser

multi tasksequencetosequencelearning


thangluong hieupham
manning

effectiveapproachestoattention basedneuralmachinetranslation

kathleenmckeown

textgeneration
cambridgeuniversitypress
rameshnallapati feifeizhai andbowenzhou

summarunner arecurrentneuralnetworkbasedsequencemodelforextractivesummarizationofdocuments

rameshnallapati bowenzhou caglargulcehre bingxiang etal

abstractivetextsummarizationusingsequence to sequencernnsandbeyond
con
aninenkova kathleenmckeown etal

auto maticsummarization

razvanpascanu tomasmikolov andyoshuabengio

onthedifficultyoftrainingrecurrentneuralnetworks
inicml
romainpaulus caimingxiong andrichardsocher

adeepreinforcedmodelforabstractivesummarization


sairajeswar sandeepsubramanian francisdutil christopherjosephpal andaaronc
courville

adversarialgenerationofnaturallanguage
workshoponrepresentationlearningfornlp
alexandermrush seasharvard sumitchopra andjasonweston

aneuralattentionmodelforsentencesummarization

abigailsee peterjliu andchristopherdmanning

gettothepoint summarizationwithpointer generatornetworks

ricosennrich barryhaddow andalexandrabirch

controllingpolitenessinneuralmachinetranslationviasideconstraints
humanlanguagetechnologies
associationforcomputa tionallinguistics sandiego california
ricosennrich barryhaddow andalexandrabirch

neuralmachinetranslationofrarewordswithsubwordunits

tianxiaoshen taolei reginabarzilay
jaakkola

styletransferfromnon paralleltextbycross alignment


ilyasutskever jamesmartens georgee
dahl
hinton

ontheimportanceofini tializationandmomentumindeeplearning
inicml
ilyasutskever oriolvinyals andquocvle

sequencetosequencelearningwithneuralnetworks

junsuzukiandmasaakinagata

cutting offredundantrepeatinggenerationsforneuralabstractivesummarization

shunsuketakeno masaakinagata andkazuhideyamamoto

controllingtargetfeaturesinneuralmachinetranslationviaprefixconstraints

asianfederationofnaturallanguageprocessing taipei taiwan
ashishvaswani noamshazeer nikiparmar jakobuszkoreit llionjones aidanngomez lukaszkaiser andilliapolosukhin

attentionisallyouneed


oriolvinyals meirefortunato andnavdeepjaitly

pointernetworks

oriolvinyals alexandertoshev samybengio anddumitruerhan

showandtell aneuralimagecaptiongenerator

lantaoyu weinanzhang junwang andyongyu

seqgan sequencegenerativeadversarialnetswithpolicygradient
inaaai
junbozhao y
kim k
zhang a
m
rush andy
le cun

adversariallyregularizedautoencodersforgeneratingdiscretestructures



