r a m l c
s c v
v i x r a published as a conference paper at iclr bert fine tuning for arabic text summarization khalid n
elmadani mukhtar elgezouli anas showk department of electrical and electronics engineering p
o
box khartoum sudan khalidnabigh
com anas

edu abstract fine tuning a pretrained bert model is the state of the art method for tive abstractive text summarization in this paper we showcase how this tuning method can be applied to the arabic language to both construct the rst documented model for abstractive arabic text summarization and show its mance in arabic extractive summarization
our model works with multilingual bert as arabic language does not have a pretrained bert of its own
we show its performance in english rst before applying it to arabic corpora in both extractive and abstractive tasks
introduction arabic one of six ofcial languages of the united nations is the mother tongue of million ple and the ofcial language for countries nine of those are in africa
hence arabic had a huge inuence in mother africa forming the culture and religious values in west africa consequently its safe to say arabic is the latin of africa
the arabic script is an alphabet written from right to left
there are two types of symbols in the arabic script letters and diacritics habash
it has letters and each letters shape changes based on its position each character holds possible diacritics and the syntax of each word in the sentence depends on its last letters diacritic
unfortunately the diacritics are usually absent in the texts of news articles and any online content
the main challenge in arabic text summarization is in the ambiguity of the arabic language itself the meaning of a text depends heavily on the context
text summarization is to extract and generate the key information in a brief expression from long documents
generally there are two approaches for text summarization either extractive involves extracting the relevant phrases from the document then organizing them to form the summary or involves going through the hole document then try to write summary in your own abstractive words
arabic text summarization works are few related to other languages and the research is focused on extractive approaches which are based on sentences scoring then selecting best ones as a summary
three approaches are used for sentence scoring and selection al qassem et al
symbolic based systems model the discourse structures of text numerical based systems assign numerical scores to words of sentences which reects their signicance and hybrid systems combine both symbolic based and numerical based methods
recently jaafar bouzoubaa suggested hybrid approach to produce abstractive summaries based on extractive ones
data sets english is the golden standard for text summarization strongly because of the vast number of well proposed benchmark data sets containing a huge capacity of summarized articles both in tive and abstractive schemes like cnn daily mail news highlights data set hermann et al
contains k news articles and associated highlights another important data set in english is xsum narayan et al
contains news articles accompanied with a one sentence these authors contributed equally
code is available at
com mukhtar algezoli arabic presumm
published as a conference paper at iclr table rouge results on the cnn test set model rl
bert m bert




summary answering the questionwhat is this article about this type of rich corpus is what arabic language lacks in automatic text summarization
the lack of arabic benchmark corpora makes evaluation for arabic summarization more difcult
without unied benchmark corpus the results reported from existing model can only be a hint for overall performance comparison al qassem et al

but recently there is a turnout to use some corpora like easc containing arabic articles and human generated extractive summaries of those articles and kalimat a multipurpose arabic corpus containing articles with their extractive summaries
methodology we used the pretrained bert devlin et al
for both abstractive and extractive tion
the encoder bertsum liu lapata is pretrained bert expanded by adding several cls symbols for learning sentence representations and using interval segmentation embeddings to distinguish multiple sentences
for abstractive summarization task the decoder is layered formers vaswani et al
initialized randomly
this mismatching between encoder and decoder encoder was pretrained while decoder is may lead to unstable training so liu lapata proposed a new ne tuning schedule which adopts different optimizers for the encoder and the decoder bertsumabs
and for extractive summarization task a sigmoid classier was serted on top of each cls token in the encoder indicating whether the sentence should be included in the summary bertsumext
this method of using pretrained bert is perfect for our condition because the pretrained model will compensate for the relatively small data set we are using
but how could this model be applicable for arabic language since bert was trained on english documents the answer is multilingual bert m bert pires et al

its similar to the normal bert but trained on languages
we trained bertsumabs one time using bert and m bert another time on the cnn data set for steps to compare the impact of using m bert instead of bert sense m bert supports arabic
finally we included a non pretrained transformer baseline for both extractive and abstractive tasks in order to measure the effect of using pretrained m bert
both transformerabs and formerext encoders are layered transformers the rest of their architecture is the same as sumabs and bertsumext respectively
results we ve automatically evaluated the quality of the summary using rouge lin och
gram and bi gram overlap and are reported as a means of evaluating mativeness and the longest common subsequence rouge l as a means of evaluating uency
table demonstrates the rst step towards arabic text summarization switching from monolingual bert to multilingual bert
the results show very similar performance as compared to bert and m bert
table presents our results on kalimat data set
we conclude that pre trained m bert leads to huge improvements in performance for relatively small data sets in both extractive and abstractive summarization
it also reveals that extractive models would have higher performance for extractive data sets than their corresponding abstractive ones
published as a conference paper at iclr table rouge results on kalimat test set model rl bertsumext transformerext bertsumabs transformerabs











in this paper we showed how multilingual bert could be applied to arabic text summarization and how effective it could be in low resource situations
research in arabic nlp is still in its infancy compared to english abstractive text summarization was not attempted before at the time of this submission so there is no metrics output that we can evaluate against
conclusion references lamees mahmoud al qassem di wang zaid al mahmoud hassan barada ahmad al rubaie and nawaf i almoosa
automatic arabic summarization a survey of methodologies and systems
procedia computer science
jacob devlin ming wei chang kenton lee and kristina toutanova
bert pre training of deep bidirectional transformers for language understanding
arxiv preprint

nizar y habash
introduction to arabic natural language processing
synthesis lectures on human language technologies
karl moritz hermann tomas kocisky edward grefenstette lasse espeholt will kay mustafa in advances in suleyman and phil blunsom
teaching machines to read and comprehend
neural information processing systems pp

younes jaafar and karim bouzoubaa
towards a new hybrid approach for abstractive tion
procedia computer science
chin yew lin and fj och
looking for a few good metrics rouge and its evaluation
in ntcir yang liu and mirella lapata
text summarization with pretrained encoders
arxiv preprint shashi narayan shay b cohen and mirella lapata
do nt give me the details just the topic aware convolutional neural networks for extreme summarization
arxiv preprint telmo pires eva schlinger and dan garrette
how multilingual is multilingual bert arxiv preprint ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan n gomez ukasz kaiser and illia polosukhin
attention is all you need
in advances in neural information processing systems pp

workshop


mary



a data pre processing kalimat is a multipurpose arabic corpus used mainly for extractive summarization in this section we will show how we prepared it for bert
in the raw data set each category culture economy local news international news religion and sport has its own le containing articles of each month in a txt le the text on those txt les are published as a conference paper at iclr in ironically does not support arabic so we converted it to and put it conveniently in a csv le we then made a standalone
story le for each article and its summary with the summary formulated as highlights at the end of the le
at this stage we used the preprocessing suggested by liu lapata but with some changes to make it work with arabic
in the standford corenlp le we replaced the stanford

with stanford models and used its path as the path for stanford corenlp tokenizer

we then used the sentence splitting and tokenization as in the paper which split the articles and summaries into sentences list of vectors put into a json le

lastly we tokenized the vectors using bert vocabulary multilingual bert model and formatted it to pytorch les

at the end we got pytorch les each with entries each entry containing
src txt and src those the original articles and their tokenized counterparts tokenized using
tgt txt and tgt those the original summaries and their tokenized counterparts tokenized multilingual bert tokenizer
using multilingual bert tokenizer

