v o n l c
s c v
v i x r a towards supervised extractive text summarization via rnn based sequence classication eduardo brito max lubbering david biesner lars patrick hillebrand and christian bauckhage fraunhofer iais sankt augustin germany fraunhofer center for machine learning germany b it university of bonn bonn germany abstract
this article briey explains our submitted approach to the competition on extractive summarization
we implemented a recurrent neural network based model that learns to classify whether an article s sentence belongs to the corresponding extractive summary or not
we bypass the lack of large annotated news corpora for tive summarization by generating extractive summaries from abstractive ones which are available from the cnn corpus
keywords neural networks extractive text summarization introduction the doceng competition focused on automatic extractive text tion
participants were provided with a corpus of news articles from the cnn corpus
these articles contained corresponding extractive and tive summaries aimed to train and test a system to perform the summarization task
the gold standard summaries contained around of the original text with a minimum of sentences
after submission the methods were tested on a larger test set consisting of articles randomly chosen from the cnn corpus
the limited available training data was one of the major challenges of this petition which prevented any deep learning approach from being successful if no external corpus was incorporated to the training set
approach our work is based on the summarunner model
it consists of a two layer bi directional gated recurrent unit gru recurrent neural network rnn which treats the summarization problem as a binary sequence classication lem where each sentence is classied sequentially as sentence to be included or not in the summary
however we introduced two modications to the original summarunner architecture leading to better results while reducing ity e
brito et al
fig

our rnn based sequence classier based on
all word embeddings from each sentence are averaged to generate a sentence embedding
sentence embeddings are then used for the bidirectional rnn at sentence level
at the top the sigmoid activation based classication layer decides whether a sentence is included in the summary based on the content richness of the sentence its salience with respect to the document and its novelty respect to the accumulated summary representation

our model operates directly on a sentence level instead of at word level within each sentence
we compute sentence vector representations by means of the the flair library

these sentence embeddings substitute the tom layer of the summarunner architecture

we do not consider the position of each sentence absolute or relative for the logistic layer
the resulting architecture is displayed on figure
our code to generate tive summaries according to the instructions established for the competition is publicly
data in contrast to we trained our model only on cnn articles from the cnn daily mail corpus
due to the limited number of provided news articles we matically annotated a large corpus of cnn articles from which an abstractive summary was available
in a similar approach to we calculated the score between each sentence and its article s abstractive summary
finally for each article we sorted the sentences having the highest score and picked the top n
sentences

com zalandoresearch flair
iais
fraunhofer
stash users dbiesner repos fraunhofer towards supervised extractive text summarization via rnn classication table
evaluation on the labeled news articles provided by the competition organizers score precision recall sentence matching gold standard








we evaluated our model on the provided labeled cnn news articles with three dierent metrics sentences from the generated summary matching the gold dard summary and
the achieved scores with our trained model after epochs are displayed on table
evaluation conclusion our approach achieved the second best performance among the compared ods in the competition although the score dierence between both approaches is not statistically signicant
additionally the performance of these proaches is hardly better than some of the traditional algorithms that were presented as baselines which are much simpler than ours
moreover the real value of the dierent approaches on the various use cases of automatic text summarization can not be covered with the current evaluation since the valuable properties of the summaries vary depending on the use case
for instance ence is important if the summary will be read by a nal user while it is not if the summary is just a preprocessing step within an indexing pipeline
therefore it would be interesting to assess the dierent techniques on several downstream tasks to obtain a better overview about which algorithms are most suitable
references
akbik a
blythe d
vollgraf r
contextual string embeddings for sequence ing
in coling international conference on computational linguistics
pp

hermann k
m
kocisky t
grefenstette e
espeholt l
kay w
suleyman m
blunsom p
teaching machines to read and comprehend
in advances in neural formation processing systems nips
org

lins r
d
mello r
f
simske s
competition on extractive text summarization
in proceedings of the acm symposium on document engineering
pp

doceng acm new york ny usa




acm



lins r
d
oliveira h
cabral l
batista j
tenorio b
ferreira r
lima r
franca pereira e silva g
simske s
j
the cnn corpus a large textual corpus e
brito et al
for single document extractive summarization
in proceedings of the acm sium on document engineering
pp

doceng acm new york ny usa




acm



nallapati r
zhai f
zhou b
summarunner a recurrent neural network based sequence model for extractive summarization of documents
in thirty first aaai conference on articial intelligence
