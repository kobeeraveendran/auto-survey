extractive summarization as text matching ming zhong pengfei liu yiran chen danqing wang xipeng qiu xuanjing huang shanghai key laboratory of intelligent information processing fudan university school of computer science fudan university zhangheng road shanghai china
edu
cn r a l c
s c v
v i x r a abstract this paper creates a paradigm shift with regard to the way we build neural extractive rization systems
instead of following the monly used framework of extracting sentences individually and modeling the relationship tween sentences we formulate the extractive summarization task as a semantic text ing problem in which a source document and candidate summaries will be extracted from the original text matched in a semantic space
notably this paradigm shift to tic matching framework is well grounded in our comprehensive analysis of the inherent gap between sentence level and summary level tractors based on the property of the dataset
besides even instantiating the framework with a simple form of a matching model we have driven the state of the art extractive sult on cnn dailymail to a new level
in
experiments on the other ve datasets also show the effectiveness of the matching framework
we believe the power of this matching based summarization work has not been fully exploited
to age more instantiations in the future we have released our codes processed dataset as well as generated summaries in
com maszhongming matchsum
introduction the task of automatic text summarization aims to compress a textual document to a shorter highlight while keeping salient information on the original text
in this paper we focus on extractive rization since it usually generates semantically and grammatically correct sentences dong et al
nallapati et al
and computes faster
currently most of the neural extractive rization systems score and extract sentences or smaller semantic unit xu et al
one by these two authors contributed equally
corresponding author
figure matchsum framework
we match the textual representations of the document with gold mary and candidate summaries extracted from the ument
intuitively better candidate summaries should be semantically closer to the document while the gold summary should be the closest
one from the original text model the relationship between the sentences and then select several tences to form a summary
cheng and lapata nallapati et al
formulate the tractive summarization task as a sequence ing problem and solve it with an encoder decoder framework
these models make independent nary decisions for each sentence resulting in high redundancy
a natural way to address the above problem is to introduce an auto regressive decoder chen and bansal jadhav and rajan zhou et al
allowing the scoring operations of different sentences to inuence on each other
trigram blocking paulus et al
liu and pata as a more popular method recently has the same motivation
at the stage of selecting tences to form a summary it will skip the sentence that has trigram overlapping with the previously lected sentences
surprisingly this simple method of removing duplication brings a remarkable formance improvement on cnn dailymail
the above systems of modeling the relationship between sentences are essentially sentence level extractors rather than considering the semantics documentcandidate summarygold summaryextractsemantic spacebertbertbert of the entire summary
this makes them more inclined to select highly generalized sentences while ignoring the coupling of multiple sentences
narayan et al
bae et al
utilize reinforcement learning rl to achieve level scoring but still limited to the architecture of sentence level summarizers
to better understand the advantages and tations of sentence level and summary level proaches we conduct an analysis on six benchmark datasets in section to explore the characteristics of these two methods
we nd that there is indeed an inherent gap between the two approaches across these datasets which motivates us to propose the following summary level method
in this paper we propose a novel summary level framework matchsum figure and alize extractive summarization as a semantic text matching problem
the principle idea is that a good summary should be more semantically similar as a whole to the source document than the unqualied summaries
semantic text matching is an important research problem to estimate semantic similarity between a source and a target text fragment which has been applied in many elds such as tion retrieval mitra et al
question ing yih et al
severyn and moschitti natural language inference wang and jiang wang et al
and so on
one of the most ventional approaches to semantic text matching is to learn a vector representation for each text ment and then apply typical similarity metrics to compute the matching scores
specic to extractive summarization we pose a siamese bert architecture to compute the similarity between the source document and the candidate summary
siamese bert leverages the pre trained bert devlin et al
in a siamese network structure bromley et al
hoffer and ailon reimers and gurevych to rive semantically meaningful text embeddings that can be compared using cosine similarity
a good summary has the highest similarity among a set of candidate summaries
we evaluate the proposed matching framework and perform signicance testing on a range of benchmark datasets
our model outperforms strong baselines signicantly in all cases and improve the state of the art extractive result on cnn dailymail
besides we design experiments to observe the gains brought by our framework
we summarize our contributions as follows instead of scoring and extracting sentences one by one to form a summary we formulate tractive summarization as a semantic text ing problem and propose a novel summary level framework
our approach bypasses the difculty of summary level optimization by contrastive ing that is a good summary should be more mantically similar to the source document than the unqualied summaries
we conduct an analysis to investigate whether extractive models must do summary level tion based on the property of dataset and attempt to quantify the inherent gap between sentence level and summary level methods
our proposed framework has achieved rior performance compared with strong baselines on six benchmark datasets
notably we obtain a state of the art extractive result on cnn dailymail
in by only using the base version of bert
moreover we seek to observe where the performance gain of our model comes from
related work
extractive summarization recent research work on extractive summarization spans a large range of approaches
these work ally instantiate their encoder decoder framework by choosing rnn zhou et al
transformer zhong et al
wang et al
or gnn wang et al
as encoder non auto regressive narayan et al
arumae and liu or auto regressive decoders jadhav and rajan liu and lapata
despite the effectiveness these models are essentially sentence level tors with individual scoring process favor the est scoring sentence which probably is not the optimal one to form
the application of rl provides a means of summary level scoring and brings improvement narayan et al
bae et al

however these efforts are still limited to auto regressive or non auto regressive architectures
besides in the non neural approaches the integer linear ming ilp method can also be used for level scoring wan et al

in addition there is some work to solve tive summarization from a semantic perspective fore this paper such as concept coverage gillick will quantify this phenomenon in section
and favre reconstruction miao and som and maximize semantic volume gatama et al


two stage summarization recent studies alyguliyev galanis and droutsopoulos zhang et al
have attempted to build two stage document tion systems
specic to extractive summarization the rst stage is usually to extract some fragments of the original text and the second stage is to select or modify on the basis of these fragments
chen and bansal and bae et al
follow a hybrid extract then rewrite architecture with policy based rl to bridge the two networks together
lebanoff et al
xu and durrett mendes et al
focus on the then compress learning paradigm namely sive summarization which will rst train an tor for content selection
our model can be viewed as an extract then match framework which also employs a sentence extractor to prune unnecessary information
sentence level or summary level a dataset dependent analysis although previous work has pointed out the ness of sentence level extractors there is no tematic analysis towards the following questions for extractive summarization is the level extractor better than the sentence level tor given a dataset which extractor should we choose based on the characteristics of the data and what is the inherent gap between these two extractors in this section we investigate the gap between sentence level and summary level methods on six benchmark datasets which can instruct us to search for an effective learning framework
it is worth ing that the sentence level extractor we use here does nt include a redundancy removal process so that we can estimate the effect of the level extractor on redundancy elimination
notably the analysis method to estimate the theoretical fectiveness presented in this section is generalized and can be applicable to any summary level proach

denition we refer to d sn as a single document consisting of n sentences and c sk d as a candidate summary cluding k n sentences extracted from a ment
given a document d with its gold summary c we measure a candidate summary c by culating the rouge lin and hovy value between c and c in two levels sentence level score c sc where s is the sentence in c and represents the number of sentences
r denotes the average rouge
thus indicates the age overlaps between each sentence in c and the gold summary c
summary level score c where considers sentences in c as a whole and then calculates the rouge score with the gold summary c
pearl summary we dene the pearl summary to be the summary that has a lower sentence level score but a higher summary level score
denition a candidate summary c is dened as a pearl summary if there exists another didate summary that satises the inequality while
clearly if a candidate summary is a pearl summary it is challenging for sentence level summarizers to extract it
best summary the best summary refers to a summary has highest summary level score among all the candidate summaries
denition a summary c is dened as the summary when it satises c argmax where c denotes all the candidate summaries of the document
cc
ranking of best summary for each document we sort all candidate in descending order based on the level score and then dene z as the rank index of the best summary c
we use mean of and rouge l
use an approximate method here take ext see table of ten highest scoring sentences to form candidate maries
datasets source reddit xsum cnn dm wikihow pubmed multi news social media news news knowledge base scientic paper news type sds sds sds sds sds mds train pairs valid tokens ext test doc






sum






table datasets overview
sds represents single document summarization and mds represents multi document summarization
the data in doc
and sum
indicates the average length of document and summary in the test set respectively
ext denotes the number of sentences should extract in different datasets
since the appearance of the pearl summary will bring challenges to sentence level extractors we attempt to investigate the proportion of summary in different datasets on six benchmark datasets
a detailed description of these datasets is displayed in table
as demonstrated in figure we can observe that for all datasets most of the best summaries are not made up of the highest scoring sentences
cally for cnn dm only
of best summaries are not pearl summary indicating sentence level extractors will easily fall into a local optimization missing better candidate summaries
different from cnn dm pubmed is most able for sentence level summarizers because most of best summary sets are not pearl summary
ditionally it is challenging to achieve good mance on wikihow and multi news without a summary level learning process as these two datasets are most evenly distributed that is the appearance of pearl summary makes the selection of the best summary more complicated
in conclusion the proportion of the summaries in all the best summaries is a erty to characterize a dataset which will affect our choices of summarization extractors

inherent gap between sentence level and summary level extractors above analysis has explicated that the level method is better than the sentence level method because it can pick out pearl summaries but how much improvement can it bring given a specic dataset based on the denition of eq
and we can characterize the upper bound of the level and summary level summarization systems for a document d as reddit xsum cnn dm wikihow e pubmed multi news figure distribution of on six datasets
because the number of candidate summaries for each document is different short text may have relatively few dates we use z number of candidate summaries as the x axis
the y axis represents the proportion of the best summaries with this rank in the test set
intuitively if c comes rst it means that the best summary is composed of sentences with the highest score if z then the summary is a pearl summary
and as z increases c gets lower rankings we could nd more didate summaries whose sentence level score is higher than best summary which leads to the ing difculty for sentence level extractors
inherently unaware of pearl summary so ing the best summary is difcult
to better utilize the above characteristics of the data we propose a summary level framework which could score and extract a summary directly
specically we formulate the extractive rization task as a semantic text matching problem in which a source document and candidate maries will be extracted from the original text matched in a semantic space
the following section will detail how we instantiate our proposed ing summarization framework by using a simple siamese based architecture

siamese bert inspired by siamese network structure bromley et al
we construct a siamese bert tecture to match the document d and the candidate summary c
our siamese bert consists of two berts with tied weights and a cosine similarity layer during the inference phase
unlike the modied bert used in liu bae et al
we directly use the original bert to derive the semantically meaningful embeddings from document d and candidate summary c since we need not obtain the sentence level tion
thus we use the vector of the cls token from the top bert layer as the representation of a document or summary
let rd and rc the embeddings of the document d and candidate summary c
their similarity score is measured by d c rc
in order to ne tune siamese bert we use a margin based triplet loss to update the weights
tuitively the gold summary c should be cally closest to the source document which is the rst principle our loss should follow d c d c where c is the candidate summary in d and is a margin value
besides we also design a pairwise margin loss for all the candidate summaries
we sort all candidate summaries in descending order of rouge scores with the gold summary
naturally the candidate pair with a larger ranking gap should have a larger margin which is the second principle to design our loss function figure for different datasets
max ccd max ccd where cd is the set of candidate summaries tracted from d
then we quantify the potential gain for a ument d by calculating the difference between and
finally a dataset level potential gain can be tained as dd where d represents a specic dataset and is the number of documents in this dataset
we can see from figure the performance gain of the summary level method varies with the dataset and has an improvement at a imum
on cnn dm
from figure and ble we can nd the performance gain is lated to the length of reference summary for ferent datasets
in the case of short summaries reddit and xsum the perfect identication of pearl summaries does not lead to much ment
similarly multiple sentences in a long mary pubmed and multi news already have a large degree of semantic overlap making the improvement of the summary level method tively small
but for a medium length summary cnn dm and wikihow about words the summary level learning process is rewarding
we will discuss this performance gain with specic models in section

summarization as matching the above quantitative analysis suggests that for most of the datasets sentence level extractors are d cj d ci i j i redditxsumcnn dmwikihowpubmedmulti where ci represents the candidate summary ranked i and is a hyperparameter used to distinguish tween good and bad candidate summaries
finally our margin based triplet loss can be written as reddit xsum cnn dm wiki pubmed m news ext sel size l
the basic idea is to let the gold summary have the highest matching score and at the same time a ter candidate summary should obtain a higher score compared with the unqualied candidate summary
figure illustrate this idea
in the inference phase we formulate extractive summarization as a task to search for the best mary among all the candidates c extracted from the document d
c arg max d c
cc
candidates pruning curse of combination the matching idea is more intuitive while it suffers from combinatorial explosion problems
for example how could we determine the size of the candidate summary set or should we score all possible candidates to ate these difculties we propose a simple candidate pruning strategy
i d
concretely we introduce a content selection module to pre select salient sentences
the ule learns to assign each sentence a salience score and prunes sentences irrelevant with the current document resulting in a pruned document similar to much previous work on two stage summarization our content selection module is a parameterized neural network
in this paper we use bertsum liu and lapata without gram blocking we call it bertext to score each sentence
then we use a simple rule to obtain the candidates generating all combinations of sel sentences subject to the pruned document and organize the order of sentences according to the original position in the document to form candidate summaries
therefore we have a total of sel candidate sets
experiment
datasets in order to verify the effectiveness of our work and obtain more convicing explanations we perform experiments on six divergent mainstream datasets as follows
table details about the candidate summary for ferent datasets
ext denotes the number of sentences after we prune the original document sel denotes the number of sentences to form a candidate summary and size is the number of nal candidate summaries
cnn dailymail hermann et al
is a commonly used summarization dataset modied by nallapati et al
which contains news ticles and associated highlights as summaries
in this paper we use the non anonymized version
pubmed cohan et al
is collected from scientic papers and thus consists of long ments
we modify this dataset by using the duction section as the document and the abstract section as the corresponding summary
wikihow koupaee and wang is a verse dataset extracted from an online knowledge base
articles in it span a wide range of topics
xsum narayan et al
is a one sentence summary dataset to answer the question what is the article about
all summaries are ally written typically by the authors of documents in this dataset
multi news fabbri et al
is a document news summarization dataset with a tively long summary we use the truncated version and concatenate the source documents as a single input in all experiments
reddit kim et al
is a highly abstractive dataset collected from social media platform
we only use the tifu long version of reddit which regards the body text of a post as the document and the as the summary

implementation details we use the base version of bert to implement our models in all experiments
adam optimizer kingma and ba with warming up is used and our learning rate schedule follows vaswani et al
as lr
step
where each step is a batch size of and wm denotes warmup steps of
we choose and

when
and r l model r l model lead oracle match oracle








banditsum dong et al
neusum zhou et al
jecs xu and durrett hibert zhang et al
pnbert zhong et al
pnbert rl bertext bertext bertext liu bertext tri blocking bertsum





























liu and lapata


bae et al
rl bertext ours bertext tri blocking ours matchsum bert base matchsum roberta base











table results on cnn dm test set
the model with indicates that the large version of bert is used
bertext add an additional pointer network pared to other bertext in this table


they have little effect on mance otherwise they will cause performance degradation
we use the validation set to save three best checkpoints during training and record the performance of the best checkpoints on the test set
importantly all the experimental results listed in this paper are the average of three runs
to obtain a siamese bert model on cnn dm we use g gpus for about hours of training
for datasets we remove samples with empty document or summary and truncate the document to tokens therefore oracle in this paper is calculated on the truncated datasets
details of candidate summary for the different datasets can be found in table

experimental results results on cnn dm as shown in table we list strong baselines with different learning proaches
the rst section contains lead acle and match
because we prune documents before matching match oracle is relatively low
and oracle are common baselines in the marization task
the former means extracting the rst eral sentences of a document as a summary the latter is the groundtruth used in extractive models training
oracle is the groundtruth used to train matchsum
bertext num bertext num matchsum sel matchsum sel matchsum sel bertext num bertext num matchsum sel matchsum sel matchsum sel reddit xsum





























table results on test sets of reddit and xsum
n um indicates how many sentences bertext tracts as a summary and sel indicates the number of sentences we choose to form a candidate summary
we can see from the second section although rl can score the entire summary it does not lead to much performance improvement
this is ably because it still relies on the sentence level summarizers such as pointer network or sequence labeling models which select sentences one by one rather than distinguishing the semantics of ent summaries as a whole
trigram blocking is a simple yet effective heuristic on cnn dm even better than all redundancy removal methods based on neural models
compared with these models our proposed matchsum has outperformed all competitors by a large margin
for example it beats bertext by
score when using bert base as the encoder
additionally even compared with the baseline with bert large pre trained encoder our model matchsum bert base still perform better
furthermore when we change the encoder to base liu et al
the mance can be further improved
we think the provement here is because roberta introduced million english news articles during pretraining
the superior performance on this dataset strates the effectiveness of our proposed matching framework
results on datasets with short summaries reddit and xsum have been heavily evaluated by abstractive summarizer due to their short maries
here we evaluate our model on these two datasets to investigate whether matchsum could achieve improvement when dealing with model lead oracle match oracle bertext blocking blocking matchsum bert base wikihow






r l













pubmed













r l






multi news













r l






table results on test sets of wikihow pubmed and multi news
matchsum beats the state of the art bert model with ngram blocking on all different domain datasets
summaries containing fewer sentences compared with other typical extractive models
when taking just one sentence to match the inal document matchsum degenerates into a re ranking of sentences
table illustrates that this degradation can still bring a small ment compared to bertext num
on reddit
on xsum
ever when the number of sentences increases to two and summary level semantics need to be taken into account matchsum can obtain a more markable improvement compared to bertext num
on reddit
on xsum
in addition our model maps candidate summary as a whole into semantic space so it can exibly choose any number of sentences while most other methods can only extract a xed number of tences
from table we can see this advantage leads to further performance improvement
results on datasets with long summaries when the summary is relatively long level matching becomes more complicated and is harder to learn
we aim to compare the difference between trigram blocking and our model when dealing with long summaries
table presents that although trigram blocking works well on cnn dm it does not always tain a stable improvement
ngram blocking has little effect on wikihow and multi news and it causes a large performance drop on pubmed
we think the reason is that ngram blocking not really understand the semantics of sentences or summaries just restricts the presence of entities with many words to only once which is obviously not suitable for the scientic domain where entities may often appear multiple times
on the contrary our proposed method does not have these strong constraints but aligns the original document with the summary from semantic space
experiment results display that our model is robust on all domains especially on wikihow sum beats the state of the art bert model by
score

analysis in the following our analysis is driven by two tions whether the benets of matchsum are sistent with the property of the dataset analyzed in section why have our model achieved different formance gains on diverse datasets dataset splitting testing typically we choose three datasets xsum cnn dm and wikihow with the largest performance gain for this iment
we split each test set into roughly equal numbers of ve parts according to z described in section
and then experiment with each subset
figure shows that the performance gap tween matchsum and bertext is always the smallest when the best summary is not a summary
the phenomenon is in line with our understanding in these samples the ability of the summary level extractor to discover summaries does not bring advantages
as z increases the performance gap ally tends to increase
specically the benet of matchsum on cnn dm is highly consistent with the appearance of pearl summary
it can only bring an improvement of
in the subset with the smallest z but it rises sharply to
when z reaches its maximum value
wikihow is similar to cnn dm when best summary consists entirely of highest scoring sentences the performance gap is obviously smaller than in other samples
xsum xsum cnn dm wikihow figure datasets splitting experiment
we split test sets into ve parts according to z described in section

the x axis from left to right indicates the subsets of the test set with the value of z from small to large and the y axis represents the rouge improvement of matchsum over bertext on this subset
bertext on dataset d
moreover compared with the inherent gap between sentence level and summary level extractors we dene the ratio that matchsum can learn on dataset d as where is the inherent gap between level and summary level extractos
it is clear from figure the value of pends on z see figure and the length of the gold summary see table
as the gold summaries get longer the upper bound of summary level proaches becomes more difcult for our model to reach
matchsum can achieve
on xsum
words summary however is less than
in pubmed and multi news whose summary length exceeds
from another spective when the summary length are similar our model performs better on datasets with more summaries
for instance z is evenly distributed in multi news see figure so higher
can be obtained than pubmed
which has the least pearl summaries
a better understanding of the dataset allows us to get a clear awareness of the strengths and itations of our framework and we also hope that the above analysis could provide useful clues for future research on extractive summarization
conclusion we formulate the extractive summarization task as a semantic text matching problem and propose a novel summary level framework to match the source document and candidate summaries in the semantic space
we conduct an analysis to show how our model could better t the characteristic of the data
experimental results show matchsum figure of different datasets
reddit is excluded because it has too few samples in the test set
is slightly different although the trend remains the same our model does not perform well in the samples with the largest z which needs further improvement and exploration
from the above comparison we can see that the performance improvement of matchsum is concentrated in the samples with more summaries which illustrates our semantic based summary level model can capture sentences that are not particularly good when viewed individually thereby forming a better summary
intuitively comparison across datasets provements brought by matchsum framework should be associated with inherent gaps presented in section

to better understand their relation we introduce as follows s dd where cm s and cbe represent the candidate mary selected by matchsum and bertext in the document d respectively
therefore can indicate the improvement by matchsum over













dmwikihowpubmedmulti






outperforms the current state of the art extractive model on six benchmark datasets which strates the effectiveness of our method
we believe the power of this matching based summarization in the framework has not been fully exploited
future more forms of matching models can be plored to instantiated the proposed framework
acknowledgment we would like to thank the anonymous reviewers for their valuable comments
this work is ported by the national key research and ment program of china no
national natural science foundation of china no
and shanghai nicipal science and technology major project no
and zjlab
references rm alyguliyev

the two stage unsupervised proach to multidocument summarization
automatic control and computer sciences
kristjan arumae and fei liu

reinforced tive summarization with question focused rewards
in proceedings of acl student research workshop pages
sanghwan bae taeuk kim jihoon kim and goo lee

summary level training of sentence rewriting for abstractive summarization
in ings of the workshop on new frontiers in marization pages
jane bromley isabelle guyon yann lecun eduard sackinger and roopak shah

signature cation using a siamese time delay neural network
in advances in neural information processing tems pages
yen chun chen and mohit bansal

fast tive summarization with reinforce selected sentence rewriting
in proceedings of the annual ing of the association for computational linguistics volume long papers volume pages
jianpeng cheng and mirella lapata

neural in marization by extracting sentences and words
proceedings of the annual meeting of the sociation for computational linguistics volume long papers volume pages
arman cohan franck dernoncourt doo soon kim trung bui seokhwan kim walter chang and nazli goharian

a discourse aware attention model for abstractive summarization of long documents
in proceedings of the conference of the north american chapter of the association for tional linguistics human language technologies volume short papers volume pages
jacob devlin ming wei chang kenton lee and kristina toutanova

bert pre training of deep bidirectional transformers for language standing
in proceedings of the conference of the north american chapter of the association for computational linguistics human language nologies volume long and short papers pages
yue dong yikang shen eric crawford herke van hoof and jackie chi kit cheung

sum extractive summarization as a contextual dit
in proceedings of the conference on pirical methods in natural language processing pages
alexander richard fabbri irene li tianwei she suyi li and dragomir r
radev

multi news a large scale multi document summarization dataset in acl and abstractive hierarchical model
pages
association for computational linguistics
dimitrios galanis and ion androutsopoulos

an extractive supervised two stage method for sentence in human language technologies compression
the annual conference of the north american chapter of the association for computational guistics pages
association for tional linguistics
dan gillick and benoit favre

a scalable global in proceedings of the model for summarization
workshop on integer linear programming for ural language processing pages
karl moritz hermann tomas kocisky edward stette lasse espeholt will kay mustafa suleyman and phil blunsom

teaching machines to read in advances in neural and comprehend
tion processing systems pages
elad hoffer and nir ailon

deep metric learning using triplet network
in international workshop on similarity based pattern recognition pages
springer
aishwarya jadhav and vaibhav rajan

tive summarization with swap net sentences and in words from alternating pointer networks
ceedings of the annual meeting of the tion for computational linguistics volume long papers volume pages
byeongchang kim hyunwoo kim and gunhee kim

abstractive summarization of reddit posts in with multi level memory networks
ings of the conference of the north american chapter of the association for computational guistics human language technologies volume long and short papers pages
diederik kingma and jimmy ba

adam a method for stochastic optimization
arxiv preprint

mahnaz koupaee and william yang wang

ihow a large scale text summarization dataset
arxiv preprint

logan lebanoff kaiqiang song franck dernoncourt doo soon kim seokhwan kim walter chang and scoring sentence singletons and fei liu

pairs for abstractive summarization
arxiv preprint

chin yew lin and eduard hovy

matic evaluation of summaries using n gram occurrence statistics
in proceedings of the man language technology conference of the north american chapter of the association for tional linguistics pages
yang liu

fine tune bert for extractive rization
arxiv preprint

yang liu and mirella lapata

text in proceedings of tion with pretrained encoders
the conference on empirical methods in ural language processing and the international joint conference on natural language processing emnlp ijcnlp pages
yinhan liu myle ott naman goyal jingfei du dar joshi danqi chen omer levy mike lewis luke zettlemoyer and veselin stoyanov

roberta a robustly optimized bert pretraining proach
arxiv preprint

alfonso mendes shashi narayan sebastiao miranda zita marinho andre ft martins and shay b hen

jointly extracting and compressing uments with summary state representations
in ceedings of the conference of the north ican chapter of the association for computational linguistics human language technologies ume long and short papers pages
yishu miao and phil blunsom

language as a latent variable discrete generative models for tence compression
in proceedings of the ference on empirical methods in natural language processing pages
bhaskar mitra fernando diaz and nick craswell

learning to match using local and distributed representations of text for web search
in ings of the international conference on world wide web pages
international world wide web conferences steering committee
ramesh nallapati feifei zhai and bowen zhou

summarunner a recurrent neural network based quence model for extractive summarization of ments
in thirty first aaai conference on articial intelligence
ramesh nallapati bowen zhou cicero dos santos c a glar and bing xiang

tive text summarization using sequence to sequence rnns and beyond
conll page
shashi narayan shay b cohen and mirella lapata

do nt give me the details just the summary topic aware convolutional neural networks for in proceedings of the treme summarization
conference on empirical methods in natural guage processing pages
shashi narayan shay b cohen and mirella lapata

ranking sentences for extractive tion with reinforcement learning
in proceedings of the conference of the north american ter of the association for computational linguistics human language technologies volume long pers volume pages
romain paulus caiming xiong and richard socher

a deep reinforced model for abstractive marization
arxiv preprint

nils reimers and iryna gurevych

bert sentence embeddings using siamese networks
in proceedings of the conference on empirical methods in natural language processing and the international joint conference on ral language processing emnlp ijcnlp pages
aliaksei severyn and alessandro moschitti

learning to rank short text pairs with convolutional deep neural networks
in proceedings of the ternational acm sigir conference on research and development in information retrieval pages
acm
ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan n gomez ukasz kaiser and illia polosukhin

attention is all you need
in advances in neural information cessing systems pages
xiaojun wan ziqiang cao furu wei sujian li and ming zhou

multi document tion via discriminative summary reranking
arxiv preprint

danqing wang pengfei liu yining zheng xipeng qiu and xuan jing huang

heterogeneous graph neural networks for extractive document marization
in proceedings of the conference of the association for computational linguistics
danqing wang pengfei liu ming zhong jie fu xipeng qiu and xuanjing huang

exploring domain shift in extractive text summarization
arxiv preprint

shuohang wang and jing jiang

learning ral language inference with lstm
in proceedings of the conference of the north american ter of the association for computational linguistics human language technologies pages
zhiguo wang wael hamza and radu florian

bilateral multi perspective matching for natural guage sentences
in proceedings of the national joint conference on articial intelligence pages
aaai press
jiacheng xu and greg durrett

neural tive text summarization with syntactic compression
in proceedings of the conference on cal methods in natural language processing hong kong china
association for computational guistics
jiacheng xu zhe gan yu cheng and jingjing discourse aware neural extractive arxiv preprint liu

model for text summarization


wen tau yih ming wei chang christopher meek and andrzej pastusiak

question answering using enhanced lexical semantic models
in proceedings of the annual meeting of the association for computational linguistics volume long papers pages
dani yogatama fei liu and noah a smith

tractive summarization by maximizing semantic in proceedings of the conference on ume
empirical methods in natural language processing pages
haoyu zhang yeyun gong yu yan nan duan jun xu ji wang ming gong and ming zhou
language
arxiv preprint eration for text summarization


pretraining based natural xingxing zhang furu wei and ming zhou

hibert document level pre training of hierarchical bidirectional transformers for document tion
in acl
ming zhong pengfei liu danqing wang xipeng qiu and xuan jing huang

searching for tive neural extractive summarization what works and what s next
in proceedings of the ence of the association for computational tics pages
ming zhong danqing wang pengfei liu xipeng qiu and xuanjing huang

a closer look at data bias in neural extractive summarization models
emnlp ijcnlp page
qingyu zhou nan yang furu wei shaohan huang ming zhou and tiejun zhao

neural ment summarization by jointly learning to score and select sentences
in proceedings of the annual meeting of the association for computational guistics volume long papers volume pages

