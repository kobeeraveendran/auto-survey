a survey on neural network based summarization methods yue dong april r a m l c
s c v
v i x r a introduction every day enormous amounts of text are published online and quick access to the major points of these documents is critical for decision making
however manually producing summaries for such large amounts of documents in a timely manner is no longer feasible
automatic text summarization the automated process of shortening a text while reserving the main ideas of the has consequently became popular
up until recently text summarization was dominated by unsupervised information retrieval models
in kageback et al
demonstrated that the neural based continuous vector models are promising for text summarization
this marked the beginning of the widespread use of neural network based text summarization models because of their superior performance compared to the traditional techniques
the aim of this literature review is to survey the recent work on neural based models in automatic text summarization
this survey starts with the general background on document summarization section including the factors by which summarization tasks may be ed evaluation issues and a brief history of the traditional summarization techniques
section examines in detail ten neural based summarizers
section discusses the related techniques and presents promising paths for future research and section concludes the paper
background
summarization factors according to jones et al
text summarization tasks can be dened and classied by the following factors input purpose and output


input factors single document vs
multi document jones et al
denes this factor as the unit input parameter which simply is the number of input documents that the summarization system takes
monolingual multilingual vs
cross lingual the monolingual summarizers produce summaries that are in the same languages as the inputs while the multilingual systems can handle the input output pairs in the same language across several dierent languages
on the contrary the cross lingual summarization systems operate on input output pairs that are not necessarily in the same language


purpose factors informative vs
indicative an indicative summary serves as a road map to convey the relevant contents of the original documents so the readers can select documents that align with their interests to read further
an indicative summary itself is not supposed to be a substitute for the source documents
on the other hand the purpose of an informative summary is to replace the original documents as far as the important contents is concerned
generic vs
user oriented this factor concerns the coverage of the original documents conditioned on the potential readers of the summary
generic systems create summaries which consider all the information found in the documents
in contrast user oriented systems produce personalized summaries that focus on certain information from the source that are consistent with a user query
general purpose vs
domain specic general purpose summarizers can be used across any with little or no modication
on the other hand domain specic systems are designed for processing documents in a specic domain


output factors extractive vs
abstractive in relation to the source a summary can either be extractive or abstractive
there is no clear agreement on the denition of the two
in this literature review the denition of see et al
is adopted where an extractive summarizer explicitly selects text snippets words phrases sentences
from the source while an abstractive summarizer generates novel text snippets to convey the most salient cepts prevalent in the source

evaluation of summarization systems evaluation is critical for developing summarization systems
however what evaluation criteria should be used for assessing summarization systems still remains unclear due to the subjective aspect of what makes for a good summary
in general existing evaluation techniques can be split into either intrinsic or extrinsic jones et al

intrinsic methods directly evaluate the outcome of a summarization system and extrinsic methods evaluate summaries based on the performance of the down stream tasks that the system summaries are used for
the most prevalent intrinsic evaluation is to compare system generated summaries system summaries with human created gold summaries reference summaries
this allows the use of quantitative measures such as the precisions and recalls in rouge lin
however the problem with rouge is that people usually disagree on what a gold summary should be
evaluation methods such as pyramid nenkova and passonneau address this problem and assume that no single best gold summary exists
however pyramid is very expensive in terms of the human involvement
up to this day no single best summarization evaluation method exists and researchers usually adopt the cheap automated evaluation metric rouge coupled with human ratings


rouge lin recall oriented understudy for gisting evaluation rouge are a set of evaluation methods that automatically determine the quality of a system summary by comparing it to created summaries
rouge n rouge l and rouge su are commonly used in the marization literatures
rouge n computes the percentage of n gram overlapping of system and reference maries
it requires the consecutive matches of words in n grams n needs to be dened and xed that is often not the best assumption
rouge l computes the sum of the longest in sequence matches of each reference sentence to the system summary
it considers the sentence level word orders and automatically identify the longest in sequence word overlapping without a pre dened n
rouge su measures the percentage of skip bigrams and unigrams overlapping
bigram consists two words from the sentence with arbitrary gaps in their sentence order
plying skip bigrams without any constraint on the distance between the words usually produce spurious bigram matchings lin
therefore rouge su is usually used with a limited maximum skip distance such as rouge with maximum skip distance of


pyramid nenkova and passonneau instead of matching the exact phrase units as in lin pyramid tries to score summaries based on semantic matchings of the content units
it works under the assumption that there s no single best summary and therefore multiple reference summaries are necessary for this system
given a document with n human created reference summaries


rn pyramid score of a random summary s is roughly computed as follows
human annotation are rst required to identify all summarization content units scu in


rn and in s where scus are the smallest content unit for some semantic meaning nenkova and passonneau

each scu is then associated with a weight by counting how many reference summaries in the cluster contain this scu

suppose summary s is annotated with k scus with weights


wk the pyramid wi soptimal where soptimal is the sum of the largest score of s is computed as scus weights
p
summarization techniques

scope of this review in this literature review we primarily consider neural based extractive and abstractive rization techniques with the following factors single document english informative generic and general purpose
as far as i know related surveys either investigate the traditional models afantenos et al
das and martins nenkova et al
or give little details for neural based summarizers gambhir and gupta


brief history of pre neural networks era extractive models most early works on single document extractive summarization employ statistical techniques based on the edmundsonian paradigm afantenos et al

such algorithms rank each sentence based on its relation to the other sentences by using pre dened formulas such as the sum of frequencies of signicant words luhn the overlapping rate with the document title the correlation with salient cepts topics latent semantic and liu and sum of weighted similarities to other sentences textrank mihalcea and tarau
formulas usually do nt contain hyper parameters and therefore training is not required
later works on text summarization address the problem by creating sentence representations of the documents and utilizing machine learning algorithms
these models manually select the appropriate features and train supervised models to classify whether to include the sentence in the summary
for example wong et al
extracted surface content event and relevance features for the sentence representation and used support vector machines svm and nave in addition sequential models such as hidden markov bayes models for the classication
chains hmms conroy and oleary were proposed to improve the results by considering the sentence orders in the documents
abstractive models the core of abstractive summarization techniques is to identify the main ideas in the documents and encode them into feature representations
these encoded tures are then passed to natural language generation nlg systems such as the one proposed in reiter and dale for summary generation
most of the early work on abstractive summarization uses semi manual process of identifying the main ideas of the
prior knowledge such as scripts and templates are usually used to produce summaries
thus the abstractive summary is produced through slot llings and simple smoothing techniques such as in dejong radev and mckeown
neural based summarization techniques since the bloom of deep learning neural based summarizers have attracted considerable tion for automatic summarization
compared to the traditional models neural based models achieve better performance with less human involvement if the training data is abundant
in this section ve extractive and ve abstractive neural based models are examined in details
most neural based summarizers use the following pipeline words are transformed to continuous vectors called word embeddings by a look up table sentences documents are encoded as continuous vectors using the word embeddings sentence document tations sometimes also word embeddings are then fed to a model for selection extractive summarization or generation abstractive summarization
neural networks can be used in any of the above three steps
in step we can use neural networks to obtain pre learned look up tables such as cw vectors and glove
in step neural networks such as convolutional neural networks cnns or recurrent neural can be used as encoders for extracting sentence document features
in step neural network models can be used as regressors for ranking selection extraction or decoders for generation abstraction
cnns and rnns cnns and rnns are commonly used in neural based summarizers
both cnns and rnns serve the same purpose transform a sequence of word embeddings


xt rd to a vector sentence representation s rh
cnns achieve this purpose by using h lters and sliding them over the input sequence
each lter performs local on the sub sequences of the input to obtain a set of feature maps scalars then a global max pooling over time is performed to tain a scalar
these scalars from the h lters are then concatenated into the sequence representation vector s rh
convolution operation used here is basically element wise matrix multiplication followed by a tion
rnns achieve this purpose by introducing time dependent neural networks
at the time step t an rnn computes a hidden state vector ht which is obtained by a non linear transformation with two inputs the previous hidden state and the current word input xt the most basic rnn is called the elman rnn ht xt
ht
two other popular rnns which address the problem of long term dependencies by adding extra parameters are as follows gated recurrent unit gru long short term memory lstm zt tanh xt zt zt it ft ot c t tanh xt ct ft it t ht ot where denotes element wise matrix multiplication and wi are matrices with the responding dimensions
the last hidden state ht is usually used as the sequence sentation s rh

extractive models extractive summarizers which are selection based methods need to solve the following two ical challenges how to represent sentences how to select the most appropriate sentences taking into account of the coverage and the redundancy
in this section we review ve extractive neural based summarizers in chronological order
each summarization system is presented based on its sentence representation model and its sentence selection model
at the end of this section the techniques used in the extractive neural based models are summarized and the models performance are compared


continuous vector space models kageback et al
sentence representation kageback et al
proposes to represent sentences as tinuous vectors that are obtained by either adding the word embeddings or using an unfolding recursive auto encoder rae on word embeddings
the rae basically combines two text units into one in a recursive manner until only one vector the sentence representation left
the rae is trained in an unsupervised manner by the backpropagation method with the reconstruction errors
the pre computed word embeddings from collobert and weston s model cw vectors or mikolov et al
s model vectors are directly used without ne tuning
sentence selection kageback et al
formulates the task of choosing summary s as an optimization problem that maximizes the linear combination of the diversity of the sentences r and the coverage of the input text l where is the tread o between the converge and the diversity
according to kageback et al
this optimization problem is np hard
however there exists fast scalable approximation algorithms with theoretical guarantees if the objective tion is submodular
the authors choose two submodular functions which are computed based on sentence similarities as the diversity function and the converge function respectively
the objective function is therefore submodular and an approximation optimization algorithm described in kageback et al
is used for selecting the sentences


cnnlm yin and pei sentence representation yin and pei uses convolutional neural networks cnns similar to the basic cnn model we described previously on pre trained word embeddings to obtain the sentence representation
the learnable parameters including the word embeddings in the cnn are trained by unsupervised learning
the noise contrastive estimation nce mnih and teh is used as the cost function
with this cost function the model is basically trained as a language it learns to discriminate between true next words and noise words
sentence selection similar as in kageback et al
the authors frame the sentence selection as a direct optimization problem with the following objective function i pimi jpj
is x i js x here the matrix m is obtained by calculating the pairwise cosine similarities of the learned sentence representations
the prestige vector p is derived by using the pagerank algorithm on m
the goal is to nd a summary s as set of sentences that maximizes the above objective function
fortunately equation is also submodular proof in yin and pei
fore as stated in kageback et al
a near optimal solution exists and is presented in yin and pei


priorsum cao et al
sentence representation priorsum uses the cnn learned features concatenated with ument independent features as the sentence representation
three document independent tures are used sentence position averaged term frequency of words in the sentence based on the document averaged term frequency of words in the sentence based on the cluster multi document summarization
similar as in yin and pei cnns with multiple lters are used to capture sentence features
however priorsum employs a deeper and more complicated cnn
the cnn used in priorsum has multiple layers with alternating convolution and pooling operations
the lters in the convolution layers have dierent window sizes and two stage max over time pooling operations are performed in the pooling layers
the parameters in this cnn is updated by applying the diagonal variant of adagrad with mini batches as described in yin and pei
sentence selection unlike the previous two extractive neural based models priorsum is a supervised model that requires the gold standard summaries during training
priorsum follows the traditional supervised extractive framework it rst ranks each sentence and then selects the top k ranked non redundant sentences as the nal summary
function f is called submodular on the set s if s s a b implies s
this condition is also called as the diminishing return property
the authors frame the sentence ranking process as a regression problem
during training each sentence in the document is associated with the score stopwords removed with respect to the gold standard summary
then a linear regression model is trained to estimate these scores by updating the regression weights
during testing non redundant sentences are selected by a simple greedy algorithm
the greedy selection algorithm rst ranks all sentences with more than words in descending order based on the estimated informative scores
the top k sentences are then selected in order as long as the sentence is not redundant with respect to the current summary
a sentence is considered non redundant with respect to a summary if more than of its words do not appear in the summary


nn se cheng and lapata sentence representation in cheng and lapata sentence representations are tained by using a cnn followed by an rnn
the cnn extractor which is similar to the one in

has multiple feature maps with dierent window sizes
once sentence representations


st are obtained by using the cnn sentence extractor they are fed into an lstm encoder
the lstm s hidden states


ht are then used as the nal sentence sentations
comparing to


st the authors believe


ht capture the sentence dependency information and are therefore better suited as sentence representations
sentence selection similar to cao et al
s work nn se is a supervised model that rst scores the sentences and then selects them based on the estimated scores
instead of using a simple linear regressor as in cao et al
nn se utilizes an lstm decoder with a sigmoid layer equation for scoring sentences
during training the ground truth labels are given for sentences included in the reference summary and otherwise and the decoder is trained to label sentences sequentially by zeros and ones
given vectors


st obtained by the cnn and the lstm encoder s hidden states


ht the decoder s hidden states


ht are computed as lst where is the probability that the decoder believes the previous sentence should be included in the summary
the binary decision of whether to include sentence t are modeled by the following sigmoid layer ht where mlp is a multi layer neural network
joint training with a large scale dataset nn se is a sequence to sequence model with a encoder and a decoder
the encoder sentence representation model and the decoder sentence selection model can be jointly trained by the stochastic gradient descent sgd method with the objective of minimizing the negative log likelihood nll
training a sequence to sequence summarizer requires a large scale dataset with extractive bels i
e
documents with sentences labeled as summary worthy or not
the authors created a large scale dataset the dailymail dataset with about k training examples
each data instance contains an extractive reference summary that is obtained by labeling sentences based on a set of rules such as sentence positions and grams overlapping
m x

summarunner nallapati et al
sentence representation summarunner employs a two layer bi directional rnn for tences and document representations
the rst layer of the rnn is a bi directional gru that runs on words level it takes word embeddings in a sentence as the inputs and produces a set of hidden states
these hidden states are averaged into a vector which is used as the sentence representation
the second layer of the rnn is also a bi directional gru and it runs on the sentence level by taking the sentence representations obtained by the rst layer as inputs
the hidden states of the second layer are then combined into a vector document representation through a non linear transformation
sentence selection the authors frame the task of sentence selection as a sequentially tence labeling problem which is similar to the settings of cheng and lapata
dierent from cheng and lapata instead of using another rnn as the decoder summarunner uses the hidden states hm from the second layer of the encoder rnn directly for the binary decision modeled by a sigmoid function p yt st ht t where includes the information of the sentence s absolute and relative position as well as the bias
sj can be viewed as a soft summary representation that is computed as the running weighted sum of sentence representations until time t st hip yi
the sigmoid decision layer and the two layer encoder rnn are jointly trained by sgd with the objective function similar to in cheng and lapata
p comparison of the extractive models and their performance table compares and summarizes the ve extractive models mentioned previously
almost all these models are evaluated on dataset and we therefore compare their performance on dataset in table
models continuous tor space models sentence selection sentence tation adding word beddings or using rae table comparison of the techniques used in the extractive summarizers training of sentence representation for training no adding rae is trained in an supervise with res unsupervised ing with nce direct optimization on submodular jectives cnn no training training of sentence selection no training cnnlm priorsum cnn nn se unsupervised ing with diagonal variant of adagrad supervised co train with decoder supervised learning with scores supervised learning with sgd and nll summarunner rae recursive auto encoder res reconstruction errors nce noise contrastive estimation supervised co train with decoder ranking supervised learning with sgd and nll direct optimization on submodular jectives ranking sentence by linear regression from sentence ing sentence from sigmoid table rouge scores of the extractive summarizers on the dataset models rougel rougesu continuous vector space models cnnlm priorsum nn se summarunner extra data used for training in addition to pre trained and cw word embeddings pre trained word dings
pre trained cw word dings
gigaword for cnn dailymail
pre trained glove word beddings
dailymail











abstractive models abstractive summarizers focus on capturing the meaning representation of the whole document and then generate an abstractive summary based on this meaning representation
therefore neural based abstractive summarizers which are generation based methods need to make the following two decisions how to represent the whole document by an encoder how to generate the words sequence by a decoder
in this section we review ve abstractive neural based summarizers in chronological order
each summarization system is presented based on its encoder and its decoder
at the end of this section the techniques used in the abstractive neural based models are summarized and the models performance are compared


abs rush et al
encoder rush et al
proposes three encoder structures to capture the meaning resentation of a document
the common goal of these encoders is to transform a sequence of word embeddings


wt to a vector d which is used as the meaning representation of the document

bag of words encoder the rst encoder basically computes the summation of the word t xi
the word order is not preserved by embeddings appeared in the sequence t this bag of words encoder
p
convolutional encoder this encoder utilizes a cnn model with multiple alternating convolution and element max pooling layers
in each layer the convolution operations extract a sequence of feature vectors


ul and the number of these feature vectors are reduced by a factor of two with the element max pooling
after l layers of convolution and max pooling a max pooling over time is performed to obtain the document representation
ul
attention based encoder this encoder produces a document representation at each time step based on the previous c words context generated by the decoder
at time step t given the inputs word embeddings x


xm and the decoder s context c


the encoder produces a document representation for time step t as follows pt x where rm dt c
decoder for estimating the probability distribution that generates the word at each time step t rush et al
uses a feed forward neural network based language model nnlm c dt where ht c
training in rush et al
the encoder and decoder are trained jointly in mini batches
suppose


are j input summary pairs then the loss negative likelihood loss nll based on the parameters is computed as t
j x j t x x the training objective is to minimize the nll and it is achieved by using mini batch stochastic gradient descent


ras lstm and ras elman chopra et al
encoder the cnn based attentive encoder used in chopra et al
is similar to the attentive encoder proposed by rush et al
except the weights i is computed based on the aggregated vectors obtained by a cnn model
at time step t the attention weights are calculated by the aggregated vectors


zt and decoder s hidden state ht j t t ht
these attention weights are then combined with the inputs word embeddings to form the document representation dt dt t j
p p decoder chopra et al
replaces the nnlm model used in rush et al
to a recurrent neural network
instead of only using the previously generated c words for decoding as in nnlm the rnn decoder s hidden state ht can keep the information of all the words generated till time t
the authors propose two decoder models based on the elman rnn and the
in addition to the previous generated word and the previous hidden state the elman rnn and the lstm take encoder s context vector dt document representation at t as an additional input
for example the elman rnn s hidden state is computed as ht
once the decoder s hidden state ht is computed it is combined with the document sentation dt to decide which word to generate at the time step t
the decision is modeled by a softmax function which gives the probability distribution over all the words in the dictionary pt sof

hierarchical attentive rnns nallapati et al
encoder on the bidirectional gru to represent the document
nallapati et al
proposes a feature rich hierarchical attentive encoder based feature rich inputs the encoder takes the input vector obtained by concatenating the word embedding with additional linguistic features
the additional linguistic features used in their model are parts of speech pos tags named entity ner tags term frequency tf and inverse document frequency idf of the word
the continuous features tf and idf are rst discretized into a xed number of bins and then encoded into one hot vectors as other discrete features
all the one hot vectors are then transformed into continuous vectors by embedding matrices and these continuous vectors are concatenated into a single long vector which is then fed into the encoder
the document representation at time step t is also called the encoder s context at time step t which is commonly denoted as ct in literature
the elman rnn and lstm work to produce the hidden states are explained early in section cnns and rnns
words are generated from a dictionary
hierarchical attention the hierarchical encoder has two rnns with a similar structure as in nallapati et al
one runs on the word level and one runs on the sentence level
the hierarchical attention proposed by the authors basically re weigh the word attentions by the corresponding sentence level attention
the document representation dt is then obtained by the weighted sum of the feature rich input vectors
decoder nallapati et al
uses a rnn decoder based on uni directional gru which works similar to the decoder in chopra et al

in addition the following two mechanisms are used in nallapati et al
decoder
the large vocabulary trick lvt this trick reduces the computation time in the softmax layer by limiting the number of words the decoder can generate from during training
basically it denes a small dictionary in each mini batch during training
the dictionary only contains the words that are in the source documents of that batch and the most frequent k words in the global dictionary

decoder pointer switch using a pointer network which directly copy words from the source can improve the summaries quality by including the rare words from the source documents
a pointer network can simply be modeled based on the encoder s attention weights where the word with the largest weight is the word for copying
the decision of whether to copy or generate is controlled by a switch which is modeled by a sigmoid function p ht dt


pointer generator networks see et al
encoder the encoder of the pointer generator network is simply a single layer bidirectional lstm
it computes the document representation based on the attention weights and the encoder s hidden states which is exactly the same as the encoder in chopra et al

decoder the basic building block of see et al
decoder is a single layer uni directional lstm
in addition a decoder pointer switch similar to nallapati et al
is used for ing
moreover the authors propose a coverage mechanism for penalizing repeated attentions on already attended words
this is achieved by using a coverage vector ct which tracks the attentions that all the words in the dictionary has received till time t ct
the coverage vector is then used for the attention computation at time step t as well as in the objective function acted as a regularizer at p lt t i ct i i x t is the true label at the time step t and is a hyperparameter controlling the degree here w of the coverage regularizer


neural intra attention model paulus et al
encoder et al
also uses a bi directional lstm encoder for modeling the ument representation
the model is similar to the encoder in see et al
except the attention scores are computed by linear transformations and a softmax which is attention scores in all other models we reviewed are computed by sigmoid functions followed by a softmax function
called the intra attention mechanism by the authors
dt is then computed based on these intra attentions and the encoder s hidden states
decoder a uni directional lstm is used as the decoder in paulus et al

in addition the authors employ an intra attention mechanism on the decoder to prevent generating repeated phrases a decoder context vector ct is computed based on the intra attentions of the already generated sequence and then used as an additional input for the softmax layer of generating
a generator pointer switch similar to the ones in nallapati et al
and see et al
is also employed in the decoder
hybrid training objectives in terms of the encoder decoder model et al
and see et al
are very similar
however what novel in paulus et al
is how the parameters in their model are updated they use both stochastic gradient descent method and reinforcement learning method to update model parameters with a hybrid training objectives
stochastic gradient descent method sgd is used in abstractive summarization models to minimize the negative log likelihood of the ground truth values during the training as plained in the previous models rush et al
chopra et al
nallapati et al
see et al

we denote this nll objective as lml
using sgd to minimize lml has two shortcomings it creates a discrepancy during training and testing since there are no ground truth values during testing optimizing this objective does not always correlate to a high score on the discrete evaluation metric such as rouge scores
therefore the authors propose to use another objective based on the reinforcement learning method reinforce for training lrl


ys t x t where ys is obtained by sampling from the at each decoding time step t
y acts as the reinforce baseline which is obtained by performing a greedy selection rather than sampling at each decoding time step
is the reward score for an output sequence y which is usually obtained by an automated evaluation method such as rouge



ys the authors noticed that optimizing lrl directly would lead to sequences with high rouge scores that are ungrammatical
therefore a mixed training objective with hyperparameter is used for balancing the rouge score and the readability of the generated sequence lmixed lrl
comparison of the abstractive models and their performance table compares and summarizes the above ve abstractive models
two large scale datasets the gigaword dataset and the cnn dailymail dataset are commonly used as the abstractive summarization benchmarks
we therefore compare the ve abstractive models performance on these two datasets as in table
table comparison of the techniques used in the abstractive summarizers models abs ras lstm and elman hierarchical rnns attentive pointer generator works neural model intra attention encoder
bag of words encoder
cnn
attention based encoder cnn attention decoder nnlm elman rnn or lstm sgd training sgd feature rich bidirectional gru hierarchical attention bidirectional lstm attention bidirectional lstm intra attention gru lvt pointer switch sgd lstm pointer switch coverage mechanism lstm pointer switch intra attention sgd sgd reinforce table rouge scores of the abstractive summarizers on the datasets models rougesu abs ras lstm and ras elman hierarchical attentive rnns pointer generator networks neural intra attention model g


c


g


c


rougel g


c


g c
discussions and the promising paths for future search
other related tasks and techniques

reinforcement learning methods for sequence prediction bahdanau et al
et al
shows a promising path of applying the reinforcement learning rl method in abstractive summarization
paulus et al
applies reinforce which is an unbiased estimator with large variance for sequence prediction in summarization
in bahdanau et al
the authors apply the actor critic algorithm which is a biased estimator with smaller variable for machine translation
in addition to the policy network an encoder decoder model they introduce a critic network that is trained to predict the values of output tokens
this critic network is based on a bidirectional gru and is trained supervisely with the ground truth labels
the key dierence in the reinforce algorithm and the actor critic algorithm is what rewards the actor uses to update its parameters
reinforce uses the overall reward from the whole sequence and only performs the update after obtaining the whole trajectory
the actor critic algorithm uses the td errors bahdanau et al
calculated based on the critic network and can update the actor during the generating process
compared to the force algorithm the actor critic method has lower variance and faster convergence rate which makes it a promising algorithm to be used in summarization


text simplication xu et al
zhang and lapata the goal of text simplication is to rewrite complex documents into simpler ones that are easier to understand
this is usually achieved by three operations splitting deletion and paraphrasing xu et al

text simplication can help improve the performance of many natural language processing nlp tasks
for example text simplication techniques can transform long complex sentences into ones that are more easily processed by automatic text summarizers
one challenge of developing text simplication models is the lack of datasets with parallel complex simple sentence pairs
xu et al
created a good quality simplication dataset called the newsela dataset for the tasks of text simplication
from their analyses we could see that the words distribution are signicantly dierent in complex and simple texts
in addition the distribution of syntax patterns are also very dierent
these ndings indicate that a text simplication model need to consider both the semantic meaning of words and the syntactic patterns of sentences
zhang and lapata propose a sequence to sequence model with attentions based on lstms for text simplication
this encoder decoder model called deep reinforcement tence simplication dress is trained with the reinforcement learning method that optimizes a task specic discrete reward function
this discrete reward function encourages the outputs to be simple grammatical and semantically related to the inputs
experiments on three datasets demonstrate that their model is promising for text simplication tasks

discussions in summarization one critical issue is to represent the semantic meanings of the sentences and documents
neural based models display superior performance on automatically extracting these feature representations
however deep neural network models are neither transparent enough nor integrating with the prior knowledge well
more analysis and understanding of the neural based models are needed for further exploiting these models
in addition the current neural based models have the following limitations they are unable to deal with sequences longer than a few thousand words due to the large memory requirement of these models they are unable to work well on small scale datasets due to the large amount of parameters these models have they are very slow to train due to the complexity of the models
there are many very interesting and promising directions for future research on text marization
we proposed two directions in this review using the reinforcement learning approaches such as the actor critic algorithm to train the neural based models exploiting techniques in text simplication to transform documents into simpler ones for summarizers to process
conclusion this survey presented the potential of neural based techniques in automatic text tion based on the examination of the state of the art extractive and abstractive summarizers
neural based models are promising for text summarization in terms of the performance when large scale datasets are available for training
however many challenges with neural based models still remain unsolved
future research directions such as adding the reinforcement learning algorithms and text simplication methods to the current neural based models are provided to the researchers
references afantenos et al
afantenos s
karkaletsis v
and stamatopoulos p

summarization from medical documents a survey
articial intelligence in medicine
bahdanau et al
bahdanau d
brakel p
xu k
goyal a
lowe r
pineau j
courville a
and bengio y

an actor critic algorithm for sequence prediction
cao et al
cao z
wei f
li s
li w
zhou m
and wang h

learning summary prior representation for extractive summarization
in acl
cheng and lapata cheng j
and lapata m

neural summarization by extracting sentences and words
in proceedings of the annual meeting of the association for computational linguistics volume long papers pages berlin germany
association for computational linguistics
chopra et al
chopra s
auli m
and rush a
m

abstractive sentence summarization with attentive recurrent neural networks
in naacl hlt the conference of the north american chapter of the association for computational linguistics human language technologies san diego fornia usa june pages
conroy and oleary conroy j
m
and oleary d
p

text summarization via hidden markov models
in proceedings of the annual international acm sigir conference on research and development in information retrieval pages
acm
das and martins das d
and martins a
f

a survey on automatic text summarization
dejong dejong g
f

an overview of the frump system
in lehnert w
g
and ringle m
h
editors strategies for natural language processing pages
lawrence erlbaum
gambhir and gupta gambhir m
and gupta v

recent automatic text summarization niques a survey
articial intelligence review
gong and liu gong y
and liu x

generic text summarization using relevance measure and latent semantic analysis
in proceedings of the annual international acm sigir conference on research and development in information retrieval pages
acm
jones et al
jones k
s
al

automatic summarizing factors and directions
advances in automatic text summarization pages
kageback et al
kageback m
mogren o
tahmasebi n
and dubhashi d

extractive marization using continuous vector space models
in proceedings of the workshop on continuous vector space models and their compositionality eacl pages
lin lin c


rouge a package for automatic evaluation of summaries
in francine moens s
s
editor text summarization branches out proceedings of the workshop pages barcelona spain
association for computational linguistics
luhn luhn h
p

the automatic creation of literature abstracts
ibm journal of research and development
mihalcea and tarau mihalcea r
and tarau p

textrank bringing order into text
in ceedings of the conference on empirical methods in natural language processing
nallapati et al
nallapati r
zhai f
and zhou b

summarunner a recurrent neural network based sequence model for extractive summarization of documents
in proceedings of the thirty first aaai conference on articial intelligence february san francisco california usa
pages
nallapati et al
nallapati r
zhou b
dos santos c
n
gulcehre c
and xiang b

tive text summarization using sequence to sequence rnns and beyond
in proceedings of the signll conference on computational natural language learning conll berlin germany august pages
nenkova et al
nenkova a
mckeown k
al

automatic summarization
foundations and trends in information retrieval
nenkova and passonneau nenkova a
and passonneau r
j

evaluating content selection in summarization the pyramid method
in human language technology conference of the north american chapter of the association for computational linguistics hlt naacl boston massachusetts usa may pages
paulus et al
paulus r
xiong c
and socher r

a deep reinforced model for abstractive summarization
arxiv preprint

radev and mckeown radev d
r
and mckeown k
r

generating natural language maries from multiple on line sources
computational linguistics
reiter and dale reiter e
and dale r

building applied natural language generation systems
nat
lang
eng

rush et al
rush a
m
chopra s
and weston j

a neural attention model for abstractive sentence summarization
in proceedings of the conference on empirical methods in natural language processing emnlp lisbon portugal september pages
see et al
see a
liu p
j
and manning c
d

get to the point summarization with pointer generator networks
in proceedings of the annual meeting of the association for computational linguistics acl vancouver canada july august volume long papers pages
wong et al
wong k

wu m
and li w

extractive summarization using supervised and semi supervised learning
in proceedings of the international conference on computational volume pages
association for computational linguistics
xu et al
xu w
callison burch c
and napoles c

problems in current text simplication research new data can help
tacl
xu xu x

pyteaser
yin and pei yin w
and pei y

optimizing sentence modeling and selection for document summarization
in proceedings of the twenty fourth international joint conference on articial intelligence ijcai buenos aires argentina july pages
zhang and lapata zhang x
and lapata m

sentence simplication with deep reinforcement in proceedings of the conference on empirical methods in natural language processing learning
emnlp copenhagen denmark september pages

