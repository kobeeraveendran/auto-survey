leveraging word embeddings for spoken document summarization kuan yu shih hung liu hsin min wang berlin chen hsin hsi chen institute of information science academia sinica taiwan national taiwan normal university taiwan national taiwan university taiwan kychen journey
sinica
edu
tw
edu
tw
ntu
edu
tw abstract owing to the rapidly growing multimedia content available on the internet extractive spoken document summarization with the purpose of automatically selecting a set of representative sentences from a spoken document to concisely express the most important theme of the document has been an active area of research and experimentation
on the other hand word embedding has emerged as a newly favorite research subject because of its excellent performance in many natural language processing tasks
however as far as we are aware there are relatively few studies investigating its use in extractive text or speech summarization
a common thread of leveraging word embeddings in the summarization process is to represent the document or sentence by averaging the word embeddings of the words occurring in the document or sentence
then intuitively the cosine similarity measure can be employed to determine the relevance degree between a pair of representations
beyond the continued efforts made to improve the representation of words this paper focuses on building novel and efficient ranking models based on the general word embedding methods for extractive speech the summarization
experimental effectiveness of our proposed methods compared to existing state of the art methods
results demonstrate index terms spoken document summarization word embedding ranking model
introduction owing to the popularity of various internet applications rapidly growing content such as music video broadcast news programs and lecture recordings has been continuously filling our daily life
obviously speech is one of the most important sources of information about multimedia
by virtue of spoken document summarization sds one can efficiently content by listening to the associated speech summary
extractive sds manages to select a set of indicative sentences from a spoken document according to a target summarization ratio and concatenate them together to form a summary
the wide spectrum of extractive sds methods developed so far may be divided into three categories methods simply based on the sentence position or structure information methods based on unsupervised sentence ranking and methods based on supervised sentence classification
for the first category the important sentences are selected from some salient parts of a spoken document such as the introductory concluding parts
however such methods can be only applied to some specific domains with limited document structures
unsupervised sentence ranking methods attempt to select important sentences based on the statistical features of the sentences or of the words in the sentences without human annotations involved
popular methods include the vector space model vsm the latent semantic analysis lsa method the markov random walk mrw method the maximum marginal relevance mmr method the sentence significant score method the unigram language model based ulm method the lexrank method the submodularity based method and the integer linear programming ilp method
statistical features may include the term word frequency linguistic score recognition confidence measure and prosodic information
in contrast supervised sentence classification methods such as the gaussian mixture model gmm the bayesian classifier bc the support vector machine svm and the conditional random fields crfs usually formulate sentence selection as a binary classification problem i
e
a sentence can either be included in a summary or not
interested readers may refer to for comprehensive reviews and new insights into the major methods that have been developed and applied with good success to a wide range of text and speech summarization tasks
is to different from the above methods we explore in this paper various word embedding methods for use in extractive sds which have recently demonstrated excellent performance in many natural language processing tasks such as relational analogy prediction sentiment analysis and sentence completion
the central idea of these learn continuously distributed vector methods representations of words using neural networks which can probe latent semantic syntactic cues that can in turn be used to induce similarity measures among words sentences and documents
a common thread of leveraging word embedding methods to nlp related tasks is to represent the document or query and sentence by averaging the word embeddings corresponding to the words occurring in the document or query and sentence
then intuitively the cosine similarity measure can be applied to determine the relevance degree between a pair of representations
however such a framework ignores the inter dimensional correlation between the two vector representations
to mitigate this deficiency we further propose a novel use of the triplet learning model to enhance the estimation of the similarity degree between a pair of representations
in addition since most word embedding methods are founded on a probabilistic objective function a probabilistic similarity measure might be a more natural choice than non probabilistic ones
consequently we also propose a new language model based framework which incorporates the word embedding methods with the document likelihood measure
to recapitulate beyond the continued and tremendous efforts made to improve the representation of words this paper focuses on building novel and efficient ranking models on top of the general word embedding methods for extractive sds

review of word embedding methods perhaps one of the most known seminal studies on developing word embedding methods was presented in
it estimated a statistical n gram language model formalized as a forward neural network for predicting future words in context while inducing word embeddings or representations as a product
such an attempt has already motivated many up extensions to develop similar methods for probing latent semantic and syntactic regularities in the representation of words
representative methods include but are not limited to the continuous bag of words cbow model the gram sg model and the global vector glove model
as far as we are aware there is little work done to contextualize these methods for use in speech summarization


the continuous bag of words cbow model rather than seeking to learn a statistical language model the cbow model manages to obtain a dense vector representation embedding of each word directly
the structure of cbow is similar to a feed forward neural network with the exception that the non linear hidden layer in the former is removed
by getting around the heavy computational burden incurred by the non linear hidden layer the model can be trained on a large efficiently while still retains good performance
formally given a sequence of words the objective function of cbow is to maximize the log probability t t t wwp ct


w t w t


w ct where c is the window size of context words being considered for the central word wt t denotes the length of the training corpus and t wwp ct


w t w t


w ct t v exp v i w exp v v t w v w i w t denotes the vector representation of the word w at where position t v is the size of the vocabulary and denotes the weighted average of the vector representations of the context words of wt
the concept of cbow is motivated by the distributional hypothesis which states that words with similar meanings often occur in similar contexts and it is thus suggested to look for whose word representation can capture its context distributions well


the skip gram sg model in contrast to the cbow model the sg model employs an inverse training objective for learning word representations with a simplified feed forward neural network
given the word sequence the objective function of sg is to maximize the following log probability t t j jc log wp t j w t where c is the window size of the context words for the central word wt and the conditional probability is computed by v exp v t j t wp t j t w w exp v w i w v t w v i and denote the word representations of the where words at positions the implementations of cbow and sg the hierarchical soft max algorithm and the negative sampling algorithm can make the training process more efficient and effective
respectively
and in t

the global vector glove model the glove model suggests that an appropriate starting point for word representation learning should be associated with the ratios of co occurrence probabilities rather than the prediction probabilities
more precisely glove makes use of weighted least squares regression which aims at learning word representations by preserving the co occurrence frequencies between each pair of words v v j i xf ww i j v w i v w j w i w j log x ww i j denotes the number of times words wi and wj where occur in a pre defined sliding context window f is a monotonic smoothing function used to modulate the impact of each pair of words involved in the model training and vw and bw denote the word representation and the bias term of word w respectively


analytic comparisons there are several analytic comparisons can be made among the above three word embedding methods
first they have different model structures and learning strategies
cbow and sg adopt an on line learning strategy i
e
the parameters word representations are trained sequentially
therefore the order that the training samples are used may change the resulting models dramatically
in contrast glove uses a batch learning strategy i
e
it accumulates the statistics over the entire training and updates the model parameters at once
second it is worthy to note that sg trained with the negative an implicit explicit relation with the classic weighted matrix factorization approach while the major difference is that sg and glove concentrate on rendering the word by word occurrence matrix but weighted matrix factorization is usually concerned with decomposing the word by document matrix
and glove have algorithm sampling the observations made above on the relation between word embedding methods and matrix factorization bring us to the notion of leveraging the value decomposition svd method as an alternative mechanism to derive the word embeddings in this paper
given a training text corpus we have a word by word co occurrence matrix a
each element aij of a is the log frequency of times words wi and wj co occur in a pre defined sliding context window in the
subsequently svd decomposes a into three sub matrices vua t a where u and v are orthogonal matrices and is a diagonal matrix
finally each row vector of matrix u or the column vector of matrix vt u v since a is a symmetric matrix designates the word embedding of a specific word in the vocabulary
it is worthy to note that using svd to derive the word representations is similar in spirit to latent semantic analysis lsa but using the word word co occurrence matrix instead of the word by document co occurrence matrix

sentence ranking based on word embeddings

the triplet learning model inspired by the vector space model vsm a straightforward way to leverage the word embedding methods for extractive sds is to represent a sentence si and a document d to be summarized by averaging the vector representations of words occurring in the sentence si and the document d v s i sw i swn i s i v
w probability of word wj given another word wi can be calculated by by doing so the document d and each sentence si of d will have a respective fixed length dense vector representation and their relevance degree can be evaluated by the cosine similarity measure
however such an approach ignores the inter dimensional correlation between two vector representations
to mitigate the deficiency of the cosine similarity measure we employ a triplet learning model to enhance the estimation of the similarity degree between a pair of representations
without loss of generality our goal is to learn a similarity function r which assigns higher similarity scores to summary sentences than to non summary sentences i
e
r v v sd i r v v sd j denotes the sentence representation in the form of a where column vector for a summary sentence si while is the representation for a non summary sentence sj
the parametric ranking function has a bi linear form as follows v v sdr v t d wv s where and is the dimension of the vector representation
by applying the passive aggressive learning algorithm presented in we can derive the similarity function r such that all triplets obey r v v sd i r v v sd j
that is not only the similarity function will distinguish summary and non summary sentences but also there is a safety margin of a hinge loss between them
with function can be defined as loss v v sd i v s j r v v sd i r v v sd
then w can be obtained by applying an efficient sequential learning algorithm iteratively over the triplets
with w sentences can be ranked in descending order of similarity measure and the top sentences will be selected and sequenced to form a summary according to a target summarization ratio


the document likelihood measure a recent school of thought for extractive sds is to employ a language modeling lm approach for the selection of important sentences
a principal realization is to use a probabilistic generative paradigm for ranking each sentence s of a document d which can be expressed by
the simplest way is to estimate a unigram language model ulm based on the frequency of each distinct word w occurring in s with the maximum likelihood ml criterion swp swn s where s is the number of times that word w occurs in s and is the length of s
obviously one major challenge facing the lm approach is how to accurately estimate the model parameters for each sentence
stimulated by the document likelihood measure adopted by the ulm method for the various word representation methods studied in this paper we can construct a new word based language model for predicting the occurrence probability of any arbitrary word wj
taking cbow as an example the wwp j i exp v w j v exp v w l w i v vw l w i
as such we can linearly combine the associated word based language models of the words occurring in sentence s to form a composite sentence specific language model for s and express the document likelihood measure as sdp dw j w i wwp j i sw i dwn j where the weighting factor is set to be proportional to the frequency of word wi occurring in sentence s subject to
the sentences offering higher document likelihoods will be selected and sequenced to form the summary according to a target summarization ratio

experimental setup the dataset used in this study is the matbn broadcast news collected by the academia sinica and the public television service foundation of taiwan between november and april
the corpus has been segmented into separate stories and transcribed manually
each story contains the speech of one studio anchor as well as several field reporters and interviewees
a subset of broadcast news documents compiled between november and august was reserved for the summarization experiments
we chose documents as the test set while the remaining documents as the held out development set
the reference summaries were generated by ranking the sentences in the manual transcript of a spoken document by importance without assigning a score to each sentence
each document has three reference summaries annotated by three subjects
for the assessment of summarization performance we adopted the widely used rouge metrics
all the experimental results reported hereafter are obtained by calculating the scores of these rouge metrics
the summarization ratio was set to
a of text news documents compiled during the same period as the broadcast news documents was used to estimate related models compared in this paper which are cbow sg glove and svd
a subset of hour speech data from matbn compiled from november to december was used to bootstrap the acoustic training with the minimum phone error rate mpe criterion and a training data selection scheme
the vocabulary size is about thousand words
the average word error rate of automatic transcription is about

experimental results at the outset we assess the performance levels of several well practiced or and state of the art summarization methods for extractive sds which will serve as the baseline systems in this paper including the lm based summarization method i
e
ulm

eq
the vector space methods i
e
vsm lsa and mmr the graph based methods i
e
mrw and lexrank the submodularity method sm and the integer linear programming ilp method
the results are illustrated in table where td denotes the results obtained based on the manual transcripts of spoken documents and sd denotes the results using the speech recognition transcripts that may contain recognition errors
several noteworthy observations can be drawn from table
first the two graph based methods i
e
mrw and lexrank are quite competitive with each other and perform better than the vector space methods table
summarization results achieved by several well studied or and state of the art unsupervised methods
table
summarization results achieved by various word embedding methods in conjunction with the cosine similarity measure
method ulm vsm lsa mmr mrw




lexrank


sm ilp text documents td spoken documents sd rouge l rouge l







































i
e
vsm lsa and mmr for the td case
however for the sd case the situation is reversed
it reveals that imperfect speech recognition may affect the graph based methods more than the vector space methods a possible reason for such a phenomenon is that the speech recognition errors may lead to inaccurate similarity measures between each pair of sentences
the pagerank like procedure of the graph based methods in turn will be performed based on these problematic measures potentially leading to degraded results
second lsa which represents the sentences of a spoken document and the document itself in the latent semantic space instead of the index term word space performs slightly better than vsm in both the td and sd cases
third sm and ilp achieve the best results in the td case but only have comparable performance to other methods in the sd case
finally ulm shows competitive results compared to other state of the art methods confirming language modeling the applicability of approach for speech summarization
the we now turn to investigate the utilities of three state of art word embedding methods i
e
cbow sg and glove and the proposed svd method

section
working in conjunction with the cosine similarity measure for speech summarization
the results are shown in table
from the results several observations can be made
first the three of the art word embedding methods i
e
cbow sg and glove though with disparate model structures and learning strategies achieve comparable results to each other in both the td and sd cases
although these methods outperform the conventional vsm model they only achieve almost the same level of performance as lsa and mmr two improved versions of vsm and perform worse than mrw lexrank sm and ilp in the td case
to our surprise the proposed svd method outperforms other word embedding methods by a substantial margin in the td case and slightly in the sd case
it should be noted that the svd method outperforms not only cbow sg and glove but also lsa and mmr
it even outperforms all the methods compared in table in the sd case
learning model outperforms in the next set of experiments we evaluate the capability of the triplet learning model for improving the measurement of similarity when applying word embedding methods in speech summarization
the results are shown in table
from the table two observations can be drawn
first it is clear that the triplet the baseline cosine similarity measure

table in all cases
this indicates that triplet learning is able to improve the measurement of the similarity degree for sentence ranking and considering the inter dimensional correlation the similarity measure between two vector representations is indeed beneficial
second cbow with triplet learning outperforms all the methods compared in table in both the td and sd cases
however we have to note that learning w in eq
has to resort to a set of documents with reference summaries thus the comparison is unfair since all the methods in table are unsupervised ones
we used the development set c

section to learn w
so far we have not figured out systematic and in text documents td spoken documents sd rouge l rouge l























table
summarization results achieved by various word embedding methods in conjunction with the triplet learning model
text documents td spoken documents sd rouge l rouge l























table
summarization results achieved by various word embedding methods in conjunction with the document likelihood measure
text documents td spoken documents sd rouge l rouge l























method cbow sg glove svd method cbow sg glove svd method cbow sg glove svd effective ways to incorporate word embeddings into existing supervised speech summarization methods
we leave this as our future work
in the last set of experiments we pair the word embedding methods with the document likelihood measure for extractive sds
the deduced sentence based language models were linearly combined with ulm in computing the document likelihood using eq

the results are shown in table
comparing the results to that of the word embedding methods paired with the cosine similarity measure

table it is evident that the document likelihood measure works pretty well as a vehicle to leverage word embedding methods for speech summarization
we also notice that cbow outperforms the other three word embedding methods in both the td and sd cases just as it had done previously in table when combined with triplet learning whereas svd with document the superiority as svd with triplet learning c

table
moreover comparing the results with that of various state the art methods

table the word embedding methods with the document likelihood measure are quite competitive in most cases
likelihood measure does not preserve
conclusions future work in this paper both the triplet learning model and the document likelihood measure have been proposed to leverage the word embeddings learned by various word embedding methods for speech summarization
in addition a new svd based word embedding method has also been proposed and proven efficient and as effective as existing word embedding methods
experimental proposed summarization methods are comparable to several state of art methods thereby indicating the potential of the new word embedding based speech summarization framework
for future work we will explore other effective ways to enrich the representations of words and integrate extra cues such as speaker identities or prosodic emotional information into the proposed framework
we are also interested in investigating more represent spoken techniques documents in an elegant way
evidence indexing supports robust that the to
references s
furui al
fundamental technologies in modern speech recognition ieee signal processing magazine pp

m
ostendorf speech technology and information access ieee signal processing magazine pp

l
s
lee and b
chen spoken document understanding and organization ieee signal processing magazine vol
no
pp

y
liu and d
hakkani tur speech summarization chapter in spoken language understanding systems for extracting semantic information from speech g
tur and r
d
mori eds new york wiley
g
penn and x
zhu a critical reassessment of evaluation baselines for speech summarization in proc
of acl pp

a
nenkova and k
mckeown automatic summarization foundations and trends in information retrieval vol
no
pp

i
mani and m
t
maybury eds
advances in automatic text summarization cambridge ma mit press
p
b
baxendale machine made index for technical an experiment ibm journal october
y
gong and x
liu generic text summarization using relevance measure and latent semantic analysis in proc
of sigir pp

x
wan and j
yang multi document summarization using cluster based link analysis in proc
of sigir pp

j
carbonell and j
goldstein the use of mmr diversity based reranking for reordering documents and producing summaries in proc
of sigir pp

s
furui al
speech to text and speech to speech summarization of spontaneous speech ieee transactions on speech and audio processing vol
no
pp

t
mikolov et al
efficient estimation of word representations in vector space in proc
of iclr pp

j
pennington al
glove global vector for word representation in proc
of emnlp pp

d
tang et al
learning sentiment specific word embedding for twitter sentiment classification in proc
of acl pp

r
collobert and j
weston a unified architecture for natural language processing deep neural networks with multitask learning in proc
of icml pp
m
kageback et al
extractive summarization using continuous vector space models in proc
of cvsc pp

l
qiu al
learning word representation considering proximity and ambiguity in proc
of aaai pp

g
miller and w
charles contextual correlates of semantic similarity language and cognitive processes pp

t
mikolov al
distributed representations of words and phrases and their compositionality in proc
of iclr pp

f
morin and y
bengio hierarchical probabilistic neural network language model in proc
of aistats pp

a
mnih and k
kavukcuoglu learning word embeddings efficiently with noise contrastive estimation in proc
of nips pp

o
levy and y
goldberg neural word embedding as implicit matrix factorization in proc
of nips pp

k
y
chen al
weighted matrix factorization for spoken document retrieval in proc
of icassp pp

m
afify al
gaussian mixture language models for speech recognition in proc
of icassp pp

k
crammer al
online passive aggressive algorithms journal of machine learning research pp

g
erkan and d
r
radev lexrank graph based lexical centrality as salience in text summarization journal of artificial intelligent research vol
no
pp

g
chechik al
large scale online learning of image similarity through ranking journal of machine learning research pp

h
lin and j
bilmes multi document summarization via budgeted maximization of submodular functions in proc
of naacl hlt pp

k
riedhammer et al
long story short global unsupervised models for keyphrase based meeting summarization speech communication vol
no
pp

j
kupiec et al
a trainable document summarizer in proc
of sigir pp

j
zhang and p
fung speech summarization without lexical features for mandarin broadcast news in proc
of naacl hlt companion volume pp

m
galley skip chain conditional random field for ranking meeting utterances by importance in proc
of emnlp pp

y
bengio al
a neural probabilistic language model journal of machine learning research pp

a
mnih and g
hinton three new graphical models for statistical language modeling in proc
of icml pp

m
norouzi et al
hamming distance metric learning in proc
of nips pp

y
t
chen al
a probabilistic generative framework for extractive broadcast news speech summarization ieee transactions on audio speech and language processing vol
no
pp

c
zhai and j
lafferty a study of smoothing methods for language models applied to information retrieval in proc
of sigir pp

h
m
wang al
matbn a mandarin chinese broadcast news corpus international journal of computational linguistics and chinese language processing vol
no
pp

c
y
lin rouge recall oriented understudy for gisting available
evaluation

isi
edu
g
heigold et al
discriminative training for automatic speech recognition modeling criteria optimization implementation and performance ieee signal processing magazine vol
no
pp


