saliency maps generation for automatic text summarization david tuckey krysia broda alessandra russo department of computing imperial college london david

broda a

ac
uk l u j g l
s c v
v i x r a abstract saliency map generation techniques are at the front of explainable ai literature for a broad range of machine learning applications
our goal is to question the limits of these approaches on more in this paper we apply complex tasks
wise relevance propagation lrp to a to sequence attention model trained on a text marization dataset
we obtain unexpected saliency maps and discuss the rightfulness of these nations
we argue that we need a quantitative way of testing the counterfactual case to judge the fulness of the saliency maps
we suggest a protocol to check the validity of the importance attributed to the input and show that the saliency maps obtained sometimes capture the real use of the input features by the network and sometimes do not
we use this example to discuss how careful we need to be when accepting them as explanation
introduction ever since the lime algorithm ribeiro et al
nation techniques focusing on nding the importance of put features in regard of a specic prediction have soared and we now have many ways of nding saliency maps also called heat maps because of the way we like to visualize them
we are interested in this paper by the use of such a technique in an extreme task that highlights questions about the validity and evaluation of the approach
we would like to rst set the vocabulary we will use
we agree that saliency maps are not explanations in themselves and that they are more similar to attribution which is only one part of the human explanation process miller
we will prefer to call this importance mapping of the input an attribution rather than an tion
we will talk about the importance of the input relevance score in regard to the model s computation and not make lusion to any human understanding of the model as a result
there exist multiple ways to generate saliency maps over the input for non linear classiers bach et al
tavon et al
samek et al

we refer the reader to adadi and berrada for a survey of explainable ai in general
we use in this paper layer wise relevance gation lrp bach et al
which aims at redistributing the value of the classifying function on the input to obtain the importance attribution
it was rst created to explain the classication of neural networks on image recognition tasks
it was later successfully applied to text using convolutional neural networks cnn arras et al
and then short term memory lstm networks for sentiment analysis arras et al

our goal in this paper is to test the limits of the use of such a technique for more complex tasks where the notion of input importance might not be as simple as in topic classication or sentiment analysis
we changed from a classication task to a generative task and chose a more complex one than text translation in which we can easily nd a word to word spondence importance between input and output
we chose text summarization
we consider abstractive and informative text summarization meaning that we write a summary in our own words and retain the important information of the inal text
we refer the reader to radev et al
for more details on the task and the different variants that exist
since the success of deep sequence to sequence models for text translation bahdanau et al
the same approaches have been applied to text summarization tasks rush et al
see et al
nallapati et al
which use tures on which we can apply lrp
we obtain one saliency map for each word in the generated summaries supposed to represent the use of the input features for each element of the output sequence
we observe that all the saliency maps for a text are nearly identical and related with the attention distribution
we propose a way to check their validity by creating what could be seen as a terfactual experiment from a synthesis of the saliency maps using the same technique as in arras et al

we show that in some but not all cases they help identify the important input features and that we need to rigorously check tance attributions before trusting them regardless of whether or not the mapping makes sense to us
we nally argue that in the process of identifying the important input features verifying the saliency maps is as important as the generation step if not more
the task and the model we present in this section the baseline model from see et al
trained on the cnn daily mail dataset
we reproduce the results from see et al
to then apply lrp on it

dataset and training task the cnn daily mail dataset nallapati et al
is a text summarization dataset adapted from the deepmind answering dataset hermann et al

it contains around three hundred thousand news articles coupled with maries of about three sentences
these summaries are in fact highlights of the articles provided by the media themselves
articles have an average length of words and the maries of words
we had training pairs and test pairs
similarly to see et al
we limit ing training and prediction the input text to words and generate summaries of words
we pad the shorter texts using an unknown token and truncate the longer texts
we embed the texts and summaries using a vocabulary of size thus recreating the same parameters as see et al


the model the baseline model is a deep sequence to sequence coder decoder model with attention
the encoder is a rectional long short term cell hochreiter and schmidhuber and the decoder a single lstm cell with attention mechanism
the attention mechanism is puted as in bahdanau et al
and we use a greedy search for decoding
we train end to end including the words embeddings
the embedding size used is of and the den state size of the lstm cells is of

obtained summaries we train the parameters of the network for about epochs until we achieve results that are qualitatively alent to the results of see et al

we obtain summaries that are broadly relevant to the text but do not match the target summaries very well
we observe the same problems such as wrong reproduction of factual details replacing rare words with more common alternatives or repeating non sense after the third sentence
we can see in figure an example of summary obtained compared to the target one
target summary marseille prosecutor says so far no videos were used in the crash investigation despite media reports
journalists at bild and paris match are very dent the video clip is real an editor says
andreas lubitz had informed his lufthansa training school of an episode of severe depression airline says
generated summary s the unk was found in a crash on the board ight

the video was found by a source close to the investigation

the video was found by a source close to the investigation


truncated figure top example of target generated
bottom generated summary for the same text the summaries we generate are far from being valid summaries of the information in the texts but are sufcient to look at the attribution that lrp will give us
they pick up the general subject of the original text
layer wise relevance propagation we present in this section the layer wise relevance gation lrp bach et al
technique that we used to attribute importance to the input features together with how we adapted it to our model and how we generated the saliency maps
lrp redistributes the output of the model from the put layer to the input by transmitting information backwards through the layers
we call this propagated backwards portance the relevance
lrp has the particularity to attribute negative and positive relevance a positive relevance is posed to represent evidence that led to the classier s result while negative relevance represents evidence that participated negatively in the prediction

mathematical description we initialize the relevance of the output layer to the value of the predicted class before softmax and we then describe locally the propagation backwards of the relevance from layer to layer
for normal neural network layers we use the form of lrp with epsilon stabilizer bach et al

we write down the relevance received by the neuron i of layer l from the neuron j of layer l wl j dl ij zl i j j j j where wl is the network s weight parameter set during ij training i is the activation of neuron i on layer l is the stabilizing term set to
and dl is the dimension of the l th layer
is the bias for neuron j of layer l zl j the relevance of a neuron is then computed as the sum of the relevance he received from the above
for lstm cells we use the method from arras et al
to solve the problem posed by the element wise multiplications of vectors
arras et al
noted that when such computation happened inside an lstm cell it always volved a gate vector and another vector containing tion
the gate vector containing only value between and is essentially ltering the second vector to allow the passing of relevant information
considering this when we propagate relevance through an element wise multiplication operation we give all the upper layer s relevance to the information vector and none to the gate vector

generation of the saliency maps we use the same method to transmit relevance through the tention mechanism back to the encoder because bahdanau s attention bahdanau et al
uses element wise cations as well
we depict in figure the transmission to end from the output layer to the input through the decoder attention mechanism and then the bidirectional encoder
we then sum up the relevance on the word embedding to get the token s relevance as arras et al

the way we generate saliency maps differs a bit from the usual context in which lrp is used as we essentially do nt it passes through the decoder and attention figure representation of the propagation of the relevance from the output to the input
mechanism for each previous decoding time step then is passed onto the encoder which takes into account the relevance transiting in both direction due to the bidirectional nature of the encoding lstm cell
have one classication but one for each word in the summary
we generate a relevance attribution for the rst words of the generated summary as after this point they often repeat themselves
this means that for each text we obtain different saliency maps each one supposed to represent the relevance of the input for a specic generated word in the summary
experimental results in this section we present our results from extracting butions from the sequence to sequence model trained for stractive text summarization
we rst have to discuss the ference between the different saliency maps we obtain and then we propose a protocol to validate the mappings

first observations the rst observation that is made is that for one text the saliency maps are almost identical
indeed each mapping highlights mainly the same input words with only slight ations of importance
we can see in figure an example of two nearly identical attributions for two distant and unrelated words of the summary
the saliency map generated using lrp is also uncorrelated with the attention distribution that participated in the generation of the output word
the tion distribution changes drastically between the words in the generated summary while not impacting signicantly the tribution over the input text
we deleted in an experiment the relevance propagated through the attention mechanism to the encoder and did nt observe much changes in the saliency map
it can be seen as evidence that using the attention tion as an explanation of the prediction can be misleading
it is not the only information received by the decoder and the importance it allocates to this attention state might be very low
what seems to happen in this application is that most of the information used is transmitted from the encoder to the decoder and the attention mechanism at each decoding step figure left saliency map over the truncated input text for the second generated word the
right saliency map over the cated input text for the generated word investigation
we see that the difference between the mappings is marginal
just changes marginally how it is used
quantifying the ence between attention distribution and saliency map across multiple tasks is a possible future work
the second observation we can make is that the saliency map does nt seem to highlight the right things in the input for the summary it generates
the saliency maps on figure correspond to the summary from figure and we do nt see the word video highlighted in the input text which seems to be important for the output
this allows us to question how good the saliency maps are in the sense that we question how well they actually sent the network s use of the input features
we will call that truthfulness of the attribution in regard to the computation meaning that an attribution is truthful in regard to the putation if it actually highlights the important input features that the network attended to during prediction
we proceed to measure the truthfulness of the attributions by validating them quantitatively

validating the attributions we propose to validate the saliency maps in a similar way as arras et al
by incrementally deleting important words from the input text and observe the change in the sulting generated summaries
we rst dene what important and unimportant input words mean across the saliency maps per texts
relevance transmitted by lrp being positive or negative we average the absolute value of the relevance across the saliency maps to obtain one ranking of the most relevant words
the idea is that input words with negative relevance have an impact on the resulting generated word even if it is not ing positively while a word with a relevance close to zero should not be important at all
we did however also try with different methods like averaging the raw relevance or eraging a scaled absolute value where negative relevance is scaled down by a constant factor
the absolute value average seemed to deliver the best results
we delete incrementally the important words words with the highest average in the input and compared it to the control experiment that consists of deleting the least tant word and compare the degradation of the resulting maries
we obtain mitigated results for some texts we observe a quick degradation when deleting important words which are not observed when deleting unimportant words see figure but for other test examples we do nt observe a nicant difference between the two settings see figure
the unk deleting most important words unk unk unk unk unk unk unk unk unk unk unk

unk unk unk deleting least important words the unk was lmed by the magazine and the unk
the video was found by a source close to the investigation
the unk said the video was recovered from a phone at the wreckage site truncated figure summary from figure generated after deleting important and unimportant words from the input text
we observe a signicant difference in summary degradation between the two experiments where the decoder just repeats the unknown token over and over
one might argue that the second summary in figure is better than the rst one as it makes better sentences but as the model generates inaccurate summaries we do not wish to make such a statement
this however allows us to say that the attribution ated for the text at the origin of the summaries in figure are truthful in regard to the network s computation and we may use it for further studies of the example whereas for the text at the origin of figure we should nt draw any further conclusions from the attribution generated
one interesting point is that one saliency map did nt look better than the other meaning that there is no apparent way of determining their truthfulness in regard of the tion without doing a quantitative validation
this brings us to believe that even in simpler tasks the saliency maps might deleting most important words the unk mass index was carried out against the taliban in
the unk mass index was part of china s strike hard campaign against the notion that the was
deleting least important words the unk mass index was carried out in the wake of the horric attack on a school in peshawar

the government has issued a ban on executions in the figure summary from another test text generated after deleting important and unimportant words from the input text
we observe less signicant difference in summary degradation between the two experiments
make sense to us for example highlighting the animal in an image classication task without actually representing what the network really attended too or in what way
we dened without saying it the counterfactual case in our experiment would the important words in the input be deleted we would have a different summary
such factuals are however more difcult to dene for image cation for example where it could be applying a mask over an image or just ltering a colour or a pattern
we believe that dening a counterfactual and testing it allows us to sure and evaluate the truthfulness of the attributions and thus weight how much we can trust them
conclusion in this work we have implemented and applied lrp to a sequence to sequence model trained on a more complex task than usual text summarization
we used previous work to solve the difculties posed by lrp in lstm cells and adapted the same technique for bahdanau et al
tention mechanism
we observed a peculiar behaviour of the saliency maps for the words in the output summary they are almost all cal and seem uncorrelated with the attention distribution
we then proceeded to validate our attributions by averaging the absolute value of the relevance across the saliency maps
we obtain a ranking of the word from the most important to the least important and proceeded to delete one or another
we showed that in some cases the saliency maps are ful to the network s computation meaning that they do light the input features that the network focused on
but we also showed that in some cases the saliency maps seem to not capture the important input features
this brought us to cuss the fact that these attributions are not sufcient by selves and that we need to dene the counter factual case and test it to measure how truthful the saliency maps are
future work would look into the saliency maps generated by applying lrp to pointer generator networks and compare to our current results as well as mathematically justifying the average that we did when validating our saliency maps
some additional work is also needed on the validation of the saliency maps with counterfactual tests
the exploitation and evaluation of saliency map are a very important step and should not be overlooked
samek et al
wojciech samek alexander binder gregoire montavon and klaus robert muller
evaluating the visualization of ieee what a deep neural network has learned
actions on neural networks and learning systems
sebastian lapuschkin see et al
abigail see peter j liu and christopher d manning
get to the point summarization with arxiv preprint
generator networks

references adadi and berrada amina adadi and mohammed berrada
peeking inside the black box a survey on ieee access explainable articial intelligence xai

arras et al
leila arras franziska horn gregoire montavon klaus robert muller and wojciech samek
what is relevant in a text document an interpretable machine learning approach
plos one
arras et al
leila arras gregoire montavon klaus robert muller and wojciech samek
explaining recurrent neural network predictions in sentiment analysis

bach et al
sebastian bach alexander binder gregoire montavon frederick klauschen klaus robert muller and wojciech samek
on pixel wise explanations for non linear classier decisions by layer wise relevance propagation
plos one
bahdanau et al
dzmitry bahdanau kyunghyun cho and yoshua bengio
neural machine translation by jointly learning to align and translate
arxiv preprint

hermann et al
karl moritz hermann tomas kocisky edward grefenstette lasse espeholt will kay mustafa suleyman and phil blunsom
teaching machines to read and comprehend
pages
hochreiter and schmidhuber sepp hochreiter and jurgen schmidhuber
long short term memory
neural computation
miller tim miller
explanation in articial gence insights from the social sciences
articial gence
montavon et al
montavon wojciech samek and klaus robert muller
methods for preting and understanding deep neural networks
digital signal processing feb
nallapati et al
ramesh nallapati bowen zhou cicero nogueira dos santos caglar gulcehre and bing xiang
abstractive text summarization using sequence to sequence rnns and beyond
arxiv preprint

radev et al
dragomir r
radev eduard hovy and kathleen mckeown
introduction to the special issue on summarization
computational linguistics
ribeiro et al
marco tulio ribeiro sameer singh and carlos guestrin
why should i trust you plaining the predictions of any classier
proceedings of the acm sigkdd international conference on knowledge discovery and data mining pages
rush et al
alexander m rush sumit chopra and a neural attention model for arxiv preprint jason weston
stractive sentence summarization

sep

