distraction based neural networks for document summarization qian chen xiaodan zhu zhenhua ling si wei hui jiang university of science and technology of china hefei china national research council canada ottawa canada iflytek research hefei china york university toronto canada
ustc
edu
cn
com
edu
cn
com
yorku
ca t c o l c
s c v
v i x r a abstract distributed representation learned with neural works has recently shown to be effective in eling natural languages at ne granularities such as words phrases and even sentences
whether and how such an approach can be extended to help model larger spans of text e

documents is triguing and further investigation would still be sirable
this paper aims to enhance neural network models for such a purpose
a typical problem of document level modeling is automatic tion which aims to model documents in order to generate summaries
in this paper we propose ral models to train computers not just to pay tion to specic regions and content of input ments with attention models but also distract them to traverse between different content of a document so as to better grasp the overall meaning for marization
without engineering any features we train the models on two large datasets
the models achieve the state of the art performance and they signicantly benet from the distraction modeling particularly when input documents are long
introduction modeling the meaning of text lies in center of natural guage understanding
distributed representation learned with neural networks has recently shown to be effective in eling ne granularities of text including words collobert et al
mikolov et al
chen et al
phrases yin and schutze zhu et al
and guably sentences socher et al
irsoy and cardie kalchbrenner et al
tai et al
zhu et al
zhu et al
chen et al
zhu et al

whether and how such an approach can be extended to help model larger spans of text e

documents is intriguing and further investigation would still be desirable although there has been interesting research conducted recently along this line li et al
hu et al
wang and cho published in the international joint ence on articial intelligence
hermann et al

a typical problem of level modeling is automatic summarization mani das and martins nenkova and mckeown in which computers generate summaries for documents based on their shallow or deep understanding of the documents
if one regards the process of representing input documents generating summaries and the interaction between them to be a complicated function tting such a function could expect to have a large scale annotated dataset to estimate a large set of parameters while on the other hand coding summarization knowledge in different forms or iting the number of model parameters e

as in many tractive summarization models are often adopted when there are no enough training data
this work explores the former direction and utilizes relatively large datasets et al
hermann et al
to train neural summarization models
in general neural networks as universal approximators can very complicated functions and have shown to be very fective on many problems recently
understanding the input documents and generating maries are both challenging
on the understanding side much recent work seems to have suggested that distributed representations often vectors by themselves may not be adequate for representing sentences let along with longer documents
additional modeling such as soft or hard tention has been applied to retrospect subsequences or even words in input text to remedy the limits which has shown to improve performances of different tasks such as those discussed in bahdanau et al
luong et al
rush et al
among others
we regard this to be a mechanism that provides a connection between input ment modeling encoding and summary generating ing which could model a level of cognitive controls human summarizers themselves often move between the input ments and summaries when they summarize a document
we consider this control layer to be important and in this paper we focus on better designing this control layer for marization
we propose neural models to train computers not just to pay attention to specic regions and content of input documents with attention models but also distract them to traverse between different content of a document so as to ter grasp the overall meaning for summarization
without engineering any features we train the models with two large datasets
the models achieve the state of the art performance and they signicantly benet from the tion modeling particularly when input documents are long
we also explore several technologies that have been applied to sentence level tasks and extend them to document marization and we present in this paper the technologies that showed to help improve the summarization performance
even when it is applied onto the models that have leveraged these technologies the distraction models can further prove the performance signicantly
in general our models here aim to perform abstractive summarization
related work distributed representation distributed representation has shown to be effective in modeling ne granularities of text as discussed above
much recent work has also attempted to model longer spans of text with neural networks li et al
hu et al
lin et al
wang and cho hermann et al

this includes research that corporates document level information for language ing wang and cho lin et al
and that answers questions hermann et al
by comprehending input documents with attention based models
more relevant to li et al
learned distributed ours the work of resentation for short documents with the averaged length of about a hundred word tokens although the objective is not summarization
summarization typically faces documents longer than those and summarization may be more sary when documents are long
in this paper we propose neural models for summarizing typical news articles with up to thousands of word tokens
we nd it is necessary to enable computers not just to pay attention to specic content of put documents with attention models but also distract them to traverse between different content so as to better grasp the overall meaning for summarization particularly when ments are long
neural summarization models automatic summarization mani has been intensively studied for both text das and martins nenkova and mckeown and speech zhu and penn zhu et al

most of the art summarization models have focused on extractive summarization although some efforts have also been exerted on abstractive summarization
recent neural tion models include the recent efforts of rush et al
lopyrev hu et al

the research performed in rush et al
focuses on neural models for sentence compression and rewriting but not full document tion
the work of lopyrev leverages neural networks to generate news headline where input documents are ited to word tokens and the work of et al
also deals with short texts up to dozens of word tokens in which summarization problems such as content redundancy is less prominent and attention based models seem to be sufcient
however summarization typically faces documents longer than that and summarization is often more needed when uments are long
in this work we attempt to explore ral summarization technologies for news articles with up to thousands of word tokens in which we nd distraction based summarization models help improve performance
note that our improvement is achieved over the model that has already outperformed the attention based model reported in et al
on short documents
our approach
overview we base our model on the general encoder decoder work sutskever et al
sutskever et al
cho et al
that has shown to be effective recently on ferent tasks
this is a general sequence to sequence ing framework in which the encoding part can be devoted to model the input documents and the decoder to generate put
we believe the control layer that helps navigate the input documents to optimize the generation objectives would be of importance and we will focus on the control layer in this paper and enrich its expressiveness
specically for rization unlike much recent work that focuses more on tion in order to grasp local context or correspondence e
in machine translation and sentence compression we force our models to traverse between different content of a document to avoid focusing on a region or same content to better grasp the overall meaning for the summarization objective
we also explore several popular technologies that have been applied to sentence level tasks and extend them to ment summarization and we present those that help improve the summarization performance

gru based encoding and decoding encoding the general document modeling and ing framework takes in an input document xtx and write the summary of the document as the output y yty
the summarization process is modeled as ing the output text y that maximizes the conditional bility arg maxy given gold summary sequences
as discussed above such a model has been found to be very fective in modeling sentences or sub sentential text spans
we will address the challenges faced at the document level
on encoding we do not restrict the encoders architectures as if it is a recurrent neural network rnn
the recent ture shows long short term memory lstm hochreiter and schmidhuber sutskever et al
and gated rent units gru bahdanau et al
are both good chitectures
in developing our systems we empirically found gru achieved similar performance as lstm but it is fast to train we will therefore describe the gru implement of our neural summarization models
in the simplest uni directional setting when reading input symbols from left to right a gru learns the hidden tions hi at time i with hi where the hi rn encodes all content seen so far at time i which is computed from and where rm is the m dimensional embedding of the current word xi
the forward propagation of gru is computed as follows
hi ui ui hi hi u ri ui where wu wr w rnm and uu ur u rnn are weight matrices n is the number of hidden units and is element wise multiplication
in our work we actually applied bi directional grus grus which we found achieving better results than single directional grus consistently
as its name suggests in a bi gru unit the annotation vector ht encodes the sequence from two directions modeling both the left and right context
the bottom part of figure shows the encoder intuitively while for more details readers can refer to bahdanau et al
for further discussion
figure a high level view of the summarization model
generation when generating summaries the decoder dicts the next word yt given all annotations obtained in coding h htx as well as all previously predicted words
the objective is a probability over the summary y with decomposition into the ordered als y argmax h argmax st ty ty y y where equation depicts a high level abstraction of erating a summary word yt over previous output as well as input annotations h htx and yt is a legal output word at time t while y is the optimal summary found by the model
the conditional probability is further rewritten as tion to factorize the model according to the structures of neural networks
the function st ct is a nonlinear function that computes the probability vector for all legal put words at output time t and st takes the ement of the resulting vector corresponding to word yt i
e
the predicated probability for word yt
the vector st and ct are the control layers that connect put y and input h which we will discuss in details in tion

for completeness function g
is computed with st ct uost coct where is a softmax function wo rkn uo rnn vo rnm and co are weight matrices k is the vocabulary size rm is the m dimensional ding of the previously predicted word

the control layers the document modeling and summary generation are scribed above as two components input document ing and summary generation
a core problem is how these two components are associated
in sentence level modeling such as machine translation and speech recognition attention model is often applied to grasp local context and dence between input and output texts
for example in lation attention is shown to be very useful for aligning the words of the target language the language being translated to to the corresponding source words and their context
attention can be regarded as a type of cognitive controls
in modeling documents we take a general viewpoint on this control layer and propose distraction modeling to enable the model to traverse over different content of a long document and we will show it improves the summarization performance signicantly
in general the control layer allows a cated examination over the input
in this section we describe the controls that consider both attention and distraction to navigate input documents and to generate summaries
two layer hidden output we rst extended the recent level hidden output model luong et al
to our marization models
as presented later in the experiments the two level hidden output model consistently improves the summarization performance on different datasets
more specically the updating of st follows a two layer gru chitecture shown in the top part of figure
st t t ct the forward propagation of and are computed similar to equation above
and use untied parameter matrices
the two layer model allows for ing a direct interaction between t and ct with the former encoding the current and previous output information and the latter encoding the current input content that is primed with distraction and attention
we will discuss how these vectors are computed below
distraction in training we propose to enforce distraction from two perspectives adding the distraction constraints in training as well as in decoding
we rst discuss the tion in training
distraction over input content vectors in training we force the model not to pay attention to the same content or same part of the input documents too much
we accumulate the previously viewed content vector as a history content vector cj and incorporate it into the currently computed t
we refer to this model as
ct t uc cj where wc and ua are diagonal ces
and t is input content vectors that have not been rectly penalized with history yet t is directly computed with conventional equation as follows was found achieving a better performance than several natives e

and distances on the held out data
d t min i dc t max ci ds t max si the distraction score was then added into the output probability and the beam search in order to encourage the model to avoid redundant content
ty scoret t t t t t ihi where hi are annotation vectors that encode the current input word and its context with the input gru described above in equation
and t i is the attention weight put on hi at the current output time t
the distraction based ct computed in equation can then be incorporated in equation
distraction over attention weight vectors we also propose to add distraction directly on the attention weight vectors
ilarly as above we accumulate the past attention weights as a history attention weight j i and use it to prime the current attention weights
the model in tu et al
also uses history attention weights but we use history here to force distraction in order to avoid redundancy which is not a concern in the machine translation task
we refer to the model as
where scoret was used as follows in the beam search with distraction algorithm and parameter and were determined on the development set
we refer to this model as
algorithm beam search with distraction require vocabulary size k beam size b max output length n computed probabilities of all the words in vocabulary choose the b most likely words and initialize the b potheses for i n do for each hypothesis compute the next conditional probabilities then have b k candidates with the sponding probabilities use the distraction primed value score to choose b t i vt a t uahi ba j i most likely candidates end for where wa rln ua ba rl and rl are the weight matrices and l is the number of hidden units
note that t uahi in the equation computes the ventional attention without penalizing attention history
is often normalized with a softmax to generate attention weights t i below which is in turn used in equation
t t i t j distraction in decoding in the decoding process we also enforced different types of distraction one by computing the difference between the distribution of the current tion weight t and that of all previous attention weights
since can be seen as a proper probabilistic distribution normalized in equation we used leibler kl divergence to measure their difference with equation which was found to be consistently better than several other distance metrics we tried on the held out data
we also enforced distraction in a similar way on the attention primed input content vector ct as well as on the hidden output vector st
both ct and st are not normalized but are regular content vectors where the cosine similarity unknown word replacement for summarization we rowed the unknown word replacement jean et al
from machine translation to our summarization models and found it improved the performance when summarizing long documents
specically due to the time complexity in dling a larger vocabulary in the softmax layer in summary generation infrequent words were removed from the lary and were replaced with the symbol
the old of vocabulary size is data dependent and will be detailed later in the experiment set up section
after the rst round summary generated for a document a token labeled as will be replaced with a word in the input documents
more specically we obtained the ment using equation i
e
we used the largest element in t to nd the source location for the current
experiment set up
data we experiment with our summarization models on two licly available corpora with different document lengths and in different languages a cnn news collection hermann et al
and a chinese corpus made available more cently in et al

both are large datasets appropriate for training neural models which as discussed above ploy a large number of parameters to t the potentially plicated summarization process involving representing input documents generating summaries and interacting between them
hermann et al
have cnn data the cnn data a human generated real life summary for each news cle
the dataset collected in was made available at github
the data was preprocessed with the stanford corenlp tools manning et al
for tokenization and boundary detection all capital information is kept
to speed up training we removed the documents that are too long over word tokens from the training and validation set but kept all documents in the test set which does not change the difculty of the task
lcsts data the second corpus is lcsts which is a nese corpus made available more recently in et al

the data is constructed from the chinese microblogging site sina weibo
we used the original training testing split mentioned in et al
but additionally randomly sampled a small part of the training data as our validation set
table gives more details about the two datasets
we can see from the table that averaged document length of the cnn corpus is about seven time as long as the lcsts corpus and the summary is about times longer
lcsts cnn train valid test doc
l






sum
l
doc
train valid test





table the cnn and lcsts dataset
the rst two rows of the table are the averaged document length doc
l
and summary length sum
l
in terms of numbers of word kens
the bottom row lists the number of documents in the datasets

training details we used mini batch stochastic gradient descent sgd to timize log likelihood and adadelta zeiler to matically adapt the learning rate of parameters and

for the cnn dataset training was performed with shufed mini batches of size after sorting by length
we limit our vocabulary to include the top most frequent words
other words were replaced with the token as cussed earlier in the paper
based on the validation data we set embedding dimension to be the vector length in den layers to be for uni gru and for bi gru
an end of sentence token was inserted between every sentence and an end of document token was added at the end
the beam size of decoder was set to be
for the lcsts data a larger mini batch size was found to be better based the observation on the validation set

com deepmind rc data same as in et al
we used characters rather than words as our tokens
the vocabulary size is ding dimension is and the vector size of the hidden layer nodes is
beam search size is same as in the cnn dataset
we make our code publicly
our tion uses python and is based on the theano library bergstra et al

experimental results
results on the cnn dataset overall performance our results on the cnn dataset are presented in table
we used rouge scores lin to measure performance
since the summary lengths are not preset to be the same we report rouge
the upper part of the table includes the baseline results of a number of ical summarization algorithms which we listed in the ble as luhn luhn edmundson edmundson lsa steinberger and jezek lex rank erkan and radev text rank mihalcea and tarau basic vanderwende et al
and kl sum haghighi and vanderwende
these baseline results are implemented in the open source tool
the results at the lower half of the table show that the bi gru encoder achieves a better performance than the gru encoder
this is consistent with the results on the sts dataset reported later in table
we show that two level output model we discussed in the method section is cial which is also consistent with the results on the sts dataset
in addition the unknown replacement technique yields an additional improvement
over the strong model that has used these technologies the row marked as unk replace the model in the last row that incorporates all distraction modeling and nally achieves a score of
a score of
and a rouge l score of
signicantly improving the three rouge scores by

and
respectively
these are also the largest improvement presented in the table pared with the other techniques listed
the table also lists the details of how the model and improve the formance additively
again the neural models do not neer any features and use only content but not any additional formality features such as locations of input sentences which may bring additional improvement
performance on different lengths of documents to observe the effectiveness of the distraction model over different ument lengths we further selected all short documents from the cnn training dataset into a subset with age length at word tokens and a subset of data that have the same number of documents as the with averaged document length at word tokens
as shown in table on the data the distraction model improves the results more signicantly
the relative improvement is

and
compared with

and
on respectively
in general the best formance on both dataset is lower than that using all training code is available at
com nats
python
org pypi sumy system luhn edmundson lsa lex rank text rank sum basic kl sum uni gru bi gru two level out unk replace distraction distraction distraction rouge l









































table results on the cnn dataset
data suggesting using more training data can improve marization performance
rouge l

















w

distraction distraction relative impr
w

distraction distraction relative impr
table results on two subsets of the cnn datasets with different document lengths

results on the lcsts dataset we experiment with the proposed model on public lcsts corpus
the baseline is the best result reported in et al

our modied uni gru achieves a slight ment over the reported results
the bi gru attention based model achieves a better performance conrming the ness of bi directional models for summarization as well as that our implementation is the state of the art and serves as a very strong baseline in the cnn dataset discussed above
note that since the input text length of lcsts is far shorter than the cnn documents each containing about words and roughly sentences we show that distraction does not improve the performance but in contrast when documents are longer its benets are signicant achieving the biggest improvement as discussed earlier
this suggests the ness of distraction modeling in helping summarize the more thank the authors of et al
for generously sharing us the latest output of their models which achieves a better mance than the results reported in et al

we reported here the updated scores higher performance as our baseline
challenging longer documents where summarization is often more necessary than for short texts
system et al
uni gru bi gru two level att
unk replace distraction rouge l

















table results on the lcsts dataset
we also compare our models with the simple baseline that selects the rst n numbers of word tokens from the input documents which reaches its maximal rouge scores when the rst tokens were taken and achieves and rouge l at

and

and our models are signicantly better than that
for the cnn data set ing the rst three sentences achieves the best results which reach and rouge l at

and
respectively
since the cnn data is news data the baseline of selecting rst several sentences has known to be a very strong baseline
again the models we explore here are towards forming abstractive summarization
conclusions and future work we propose to train neural document summarization models not just to pay attention to specic regions of input documents with attention models but also distract the models to different content in order to better grasp the overall meaning of input documents
without engineering any features we train the models on two large datasets
the models achieve the of the art performance and they signicantly benet from the distraction modeling particularly when the input documents are long
we also explore several recent technologies for summarization and show that they help improve tion performance as well
even if applied onto the models that have already leveraged these technologies the distraction models can further improve the performance signicantly
from a more general viewpoint enriching the ness of the control layers that link the input encoding layer and the output decoding layer could be of importance to edy the shortcomings of the current models
we plan to form more work along this direction
acknowledgments the rst and the third author of this paper were supported in part by the science and technology development of hui province china grants no
the mental research funds for the central universities grant no
and the strategic priority research program of the chinese academy of sciences grant no

references bahdanau et al
dzmitry bahdanau kyunghyun cho and yoshua bengio
neural machine translation by jointly learning to align and translate
corr

bergstra et al
j bergstra o breuleux f bastien p blin r pascanu g desjardins j turian d warde farley and y bengio
theano a cpu and gpu math expression compiler
in scipy volume page
austin tx
chen et al
zhigang chen wei lin qian chen si wei hui jiang and xiaodan zhu
revisiting word embedding for ing meaning
in proceedings of acl
chen et al
qian chen xiaodan zhu zhenhua ling si wei and hui jiang
enhancing and combining sequential and tree lstm for natural language inference
in

cho et al
k
cho b
van merrienboer c
gulcehre d
bahdanau f
bougares h
schwenk and y
bengio
learning phrase representations using rnn encoder decoder for statistical machine translation
in emnlp
collobert et al
r
collobert j
weston l
bottou m
karlen k
kavukcuoglu and p
kuksa
natural language processing almost from scratch
jmlr
das and martins dipanjan das and andre martins
a vey on automatic text summarization

edmundson harold p edmundson
new methods in matic extracting
jacm
erkan and radev gunes erkan and dragomir r radev
lexrank graph based lexical centrality as salience in text marization
jair pages
haghighi and vanderwende aria haghighi and lucy derwende
exploring content models for multi document marization
in naacl
hermann et al
k
hermann t
kocisky e
grefenstette l
espeholt w
kay m
suleyman and p
blunsom
teaching machines to read and comprehend
in nips
hochreiter and schmidhuber sepp hochreiter and jurgen schmidhuber
long short term memory
neural computation
et al
baotian hu qingcai chen and fangze zhu
sts a large scale chinese short text summarization dataset
in emnlp
irsoy and cardie ozan irsoy and claire cardie
deep recursive neural networks for compositionality in language
in nips pages
jean et al
sebastien jean kyunghyun cho roland sevic and yoshua bengio
on using very large target vocabulary for neural machine translation
in acl
et al
nal kalchbrenner edward stette and phil blunsom
a convolutional neural network for modelling sentences
acl june
et al
jiwei li minh thang luong and dan jurafsky
a hierarchical neural autoencoder for paragraphs and documents
in acl
lin et al
r
lin s
liu m
yang m
li m
zhou and s
li
hierarchical recurrent neural network for document eling
in emnlp
lin chin yew lin
rouge a package for automatic ation of summaries

lopyrev konstantin lopyrev
generating news headlines with recurrent neural networks
corr

luhn hans peter luhn
the automatic creation of literature abstracts
ibm journal of research and development
luong et al
thang luong hieu pham and christopher d
manning
effective approaches to attention based neural chine translation
in emnlp
mani inderjeet mani
automatic summarization
j
jamins pub
co
amsterdam
manning et al
c manning m surdeanu j bauer j finkel s bethard and d mcclosky
the stanford corenlp natural guage processing toolkit
in acl
mihalcea and tarau rada mihalcea and paul tarau
trank bringing order into text
in emnlp
mikolov et al
t
mikolov i
sutskever k
chen g
rado and j
dean
distributed representations of words and phrases and their compositionality
in nips
nenkova and mckeown ani nenkova and kathleen eown
a survey of text summarization techniques
springer
rush et al
alexander m
rush sumit chopra and jason weston
a neural attention model for abstractive sentence marization
in emnlp
socher et al
r
socher b
huval c
manning and a
ng
semantic compositionality through recursive vector spaces
in emnlp
steinberger and jezek j
steinberger and k
jezek
using latent semantic analysis in text summarization and summary uation
in isim pages
sutskever et al
ilya sutskever james martens and frey hinton
generating text with recurrent neural networks
in icml pages
sutskever et al
ilya sutskever oriol vinyals and sequence to sequence learning with neural quoc vv le
networks
in nips pages
tai et al
kai sheng tai richard socher and pher manning
improved semantic representations from structured long short term memory networks
in acl
tu et al
zhaopeng tu zhengdong lu yang liu xiaohua liu and hang li
coverage based neural machine translation
corr

et al
l
vanderwende h
suzuki c
ett and a
nenkova
beyond sumbasic task focused marization with sentence simplication and lexical expansion

wang and cho tian wang and kyunghyun cho
context language modelling
corr

yin and schutze w
yin and h
schutze
an exploration in acl student of embeddings for generalized phrases
research workshop pages june
zeiler matthew d
zeiler
adadelta an adaptive learning rate method
corr

zhu and penn xiaodan zhu and gerald penn
comparing the roles of textual acoustic and spoken language features on spontaneous conversation summarization
in naacl
zhu et al
xiaodan zhu gerald penn and frank icz
summarizing multiple spoken documents finding evidence from untranscribed audio
in acl
zhu et al
xiaodan zhu hongyu guo saif mohammad and svetlana kiritchenko
an empirical study on the effect of negation words on sentiment
in proceedings of the annual meeting of the association for computational linguistics
zhu et al
xiaodan zhu hongyu guo and parinaz hani
neural networks for integrating compositional and compositional sentiment in sentiment composition
in ings of joint conference on lexical and computational tics june
zhu et al
xiaodan zhu parinaz sobhani and hongyu guo
long short term memory over recursive structures
in proceedings of international conference on machine learning
zhu et al
xiaodan zhu parinaz sobhani and hongyu guo
dag structured long short term memory for semantic positionality
in proceedings of the meeting of the north ican chapter of the association for computational linguistics naacl

