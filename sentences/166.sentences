p e s l c
s c v
v i x r a the rule of three abstractive text summarization in three bullet points tomonori kodaira
com graduate school of system design tokyo metropolitan university mamoru komachi
ac
jp graduate school of system design tokyo metropolitan university abstract neural network based approaches have come widespread for abstractive text rization
though previously proposed models for abstractive text summarization addressed the problem of repetition of the same tents in the summary they did not itly consider its information structure
one of the reasons these previous models failed to account for information structure in the erated summary is that standard datasets clude summaries of variable lengths ing in problems in analyzing information ow specically the manner in which the rst tence is related to the following sentences
therefore we use a dataset containing maries with only three bullet points and pose a neural network based abstractive marization model that considers the tion structures of the generated summaries
our experimental results show that the mation structure of a summary can be trolled thus improving the performance of the overall summarization
introduction summarization can be achieved using two proaches namely the extractive and abstractive proaches
the extractive approach involves ing some part of a document i
e
sentence phrase or word to construct a summary in contrast the stractive approach involves generating a document summary with words that are not necessarily present in the document itself
more grammatical summary than the abstractive proach because the former involves directly tracting output expressions from the source text
however as is obvious because words that are not present in the source text can not be selected in the case of the extractive approach abstractive proaches are becoming increasingly popular for tomatic summarization tasks
in previous work rush et al
proposed an abstractive sentence summarization method that volves generating novel words in a summary based on the sequence to sequence model proposed by sutskever et al

furthermore recently provements to the abstractive text summarization method were proposed by nallapati et al
see et al

although their proposed model generates uent summaries owing to the use of a large scale dataset it can not produce a structured summarization because it is trained using the cnn daily mail datasets which are not annotated with any structural information
therefore in this work we focus on generating a structured summary for a document in particular a summary in three sentences
because the cnn daily mail datasets include summaries with a ing number of sentences they can not be annotated with information structures directly
considering this we employ a japanese summarization dataset from livedoor news whose size is the same as the cnn daily mail datasets
because livedoor news broadcasts news with a summary in three sentences it is easy to analyze summaries using this dataset
thus the extractive approach nallapati et al
li et al
is considered to yield a to produce a summary in three bullet points we rst annotate the dataset with an information ture
then we train a binary classier using the formation structure of summaries to build two marization sub models on our dataset
finally the obtained summarization model selects the summary structure based on the input and generates the mary according to the desired structure
the contributions of our work are as follows cnn daily mail datasets
in particular their line model is based on the one proposed by pati et al
which is further improved by cluding a hybrid pointer generator network and erage mechanism
because our model is based on their model we describe their model in greater tail in this subsection
we annotated and analyzed the structure of summaries in a japanese news summarization dataset whose summaries are in the form of three sentences
our proposed model generates a summary in three bullet points
related works
dataset in the case of abstractive sentence summarization rush et al
proposed a new summarization method to generate an abstractive summary using a sequence to sequence model in particular they achieved state of the art performance on the and gigaword corpora
in contrast for abstractive text summarization ing the cnn daily mail datasets the objective is to output a summary of an article consisting of sentences
nallapati et al
proposed an proved summarization model for this task which is essentially an attention encoder decoder model cluding a trick to use a large vocabulary jean et al
switching pointer generator mechanism and hierarchical networks
they proposed a new dataset for multi sentence summarization and established a benchmark using this dataset
however these works did not address the lem of information structure in a generated mary
therefore in our work we attempt to sider the information structure of a summary to ther improve the summarization model

model currently the model proposed by see et al
is the state of the art summarization model on the
com attention encoder decoder
let the input quences be tokens of the article wi and the output sequences be tokens of the summary yi
a tional long short term memory lstm network is used as the encoder whereas a unidirectional lstm is used as the decoder
a sequence of encoder den states hi are produced by the encoder
at each step t the decoder receives the word embedding of the previous word and has decoder state st
the tention distribution at is calculated as in bahdanau et al
i vt wsst ba at where v wh ws and ba are learnable parameters
the attention distribution indicates the importance of the encoder hidden states as a probability bution at time step t
furthermore the context vector h t is computed as follows h t at ihi i the context vector is concatenated with the decoder state st and input through two linear layers to duce the vocabulary distribution pvocab pvocab st h where v v b and are learnable parameters
while training the loss at timestep t is the tive log likelihood of the target word w t for that timestep which is used to calculate the overall loss for the entire sequense losst log t loss t t losst training this is the previous word of the reference however during testing this is the previous word output by the decoder
hybrid pointer generator network
see et al
also proposed a hybrid pointer generator work which combines the attention and vocabulary distributions
in particular their pointer generator network is a hybrid between a sequence to sequence attention model section
and pointer network vinyals et al

thus this network ers the source as well as target word distributions and thereby addresses the problem of unknown word generation
in the pointer generator model the eration probability pgen at time step t is calculated from the context vector h t decoder state st and decoder input xt pgen hh t wt s st wt xt bg where vectors wh ws wx and scalar bg are able parameters and represents the sigmoid tion
pgen is used as a soft switch to select a word from the vocabulary distribution pvocab or a word from the attention distribution at
for each ment the authors developed an extended vocabulary which is the union of the vocabulary and all words in the source document
the probability of the tended vocabulary is calculated as follows p w pgen at i i wi w if w is an out of vocabulary oov word is in addition if w does not exist in the source words i wi w at i is
coverage mechanism
aside from the mentioned changes see et al
improved the coverage model tu et al
to address the etition problem
in their model a coverage vector ct which is the sum of attention distributions at all previous decoder timesteps is saved where vector wc is a learnable parameter
this vents the attention mechanism from repeatedly iting the same location in the document and thus avoids generating repetitive text
in addition they constructed a new loss function to incorporate the coverage loss to penalize repeated attention to the same location
losst log p w t i ct i annotation of the summary structure
dataset we crawled pairs of japanese articles and summaries from livedoor news in a manner similar to the vious work tanaka et al

these summaries are written by human editors
in particular these summaries consist of exactly three sentences which we will discuss in detail later
we crawled data from january to december which included pairs of articles and summaries
we divided these pairs into training pairs opment pairs and test pairs
the development and test pairs were extracted from the data of uary to december which included pairs of articles and summaries per month
each article was tagged with a category selected from the nine primary as well as a category selected from the several available gories
furthermore the articles also included some special tags such as keywords key phrases or more specic category information
in the crawled data each news item includes a title and article as well as a shorter title and abstractive summary
however in our experiments we only use the ticle and summary from the dataset
other useful information will be exploited in future work
the summaries of the livedoor news dataset sist of three sentences which enables us to lyze the structure of the output
we annotated maries with the structure of sentences for the opment and test data
summaries are typically single sentences whose lengths are less than characters
livedoor
world it business entertainment sports ct
annotation where ct is a distribution over the source document words that indicates the magnitude of attention each word in the source document receives until timestep t
the coverage vector is applied to the attention mechanism as follows vt wsst wcct et i ba movies foods lifestyle women and latest
parallel parallel enumeration sequence sequence segmented sents
dev test total table results of annotation of the summaries
mately of the summaries are tagged as allel while the remainder are tagged as sequence
in a sequence summary the second and third tences simply indicate examples related to the rst sentence but not in the form of a sentence
in addition annotation revealed that the rst tence is similar to the title and almost all the second sentences include additional information such as an example of the rst sentence
therefore the rst and second sentences can be successfully generated by existing models
however the third sentence plays various roles in this dataset
in particular in a sequence summary the third sentence is based on the second sentence whereas in a parallel summary the third sentence is based on the rst sentence
thus our proposed model uses this characteristic to generate the third sentence in a summary
structure aware summarization model we generate a summary considering its structure
first we predict the structure of the summary to be generated and then generate the summary according to the predicted structure
summary structure classication
first we need to train structure specic summarization models for the parallel and sequence types
however because the training dataset is not annotated with summary types we build a binary classier for summary types using summary information to label the summaries
using the binary classier the summaries are signed to either parallel or sequence types figure
let the input sequences be tokens of a summary xi and the output label be l
we use a bidirectional lstm as an encoder
a sequence of encoder hidden states hi is produced by the encoder
furthermore the nal hidden states of the forward and backward encoders are concatenated
finally a linear figure parallel and sequence summary structures
counted as full width
the average length of the summaries in the test set is

we did not form any pre processing for annotation and did not refer to the original article to assign labels
after a preliminary investigation we divide the summaries into four types parallel parallel with enumeration sequence and sequence with mented sentences
enumeration in a summary is when recommended items are introduced whereas segmented sentences in a summary are those that were originally part of a longer sentence in the ticle
however almost all summaries are divided into the following two types parallel and sequence
ure illustrates the difference between these types
in both these summary types the rst sentence scribes the primary incident and the second tence contains additional information about the mary incident
then in the case of the parallel type the third sentence explains the rst sentence ever its content is different from that of the second sentence
in contrast in the case of the sequence type the third sentence includes detailed tion about the second sentence
thus the parallel types have no particular order in terms of the ond and third sentences whereas the sequence types have these two sequences in order
the subject in addition is often omitted in japanese thus the third sentence s zero subject is almost the same as the second sentence s subject
when we annotate such a sentence the summary is marked as an instance that requires zero anaphora resolution to generate an appropriate output

analysis table lists the results of annotation of the maries of the livedoor news dataset
precision recall parallel sequence





table classication results based on summaries
precision recall parallel sequence





table classication results based on articles
can not be used as input during testing articles are used for training based on the classication results of this model our proposed summarization model selects a structure specic summarization model to output the nal summary
figure summary structure classication
mation is applied to the vector h for each type experiments hf orward hbackward n yparallel bp ysequence bs where wp ws bp and bs are learnable parameters
structure specic sub models
second we struct the structure specic summarization models using the automatically annotated dataset
in section
we describe the base tion model see et al

in particular we pre train the structure specic summarization els using all training data as in see et al
regardless of the summary structures and then form ne tuning for each type using the cally annotated dataset
structure aware summarization
finally we build another binary classier for summary types however in this case we use only articles because summaries are not available in the test data
then we generate a summary based on the type predicted by the second summary type classier using the structure specic summarization models
to decide which model should be used to generate a summary we construct another summary structure classication model in a manner similar to the one in this case because summaries specied above

experiment classication setup
the articles and summaries were mented using mecab
ipadic


the hidden states are represented as dimensional matrices while the word embeddings are dimensional vectors
the size of the vocabulary is which includes words that appear more than once
the model is trained using adagrad duchi et al
with a learning rate of

the annotated portion of the development data is divided into and training and test pairs spectively
because the summary structure is biased as indicated by the results in table we optimize our model by under sampling to achieve high cision
in particular we sample data for each label until its precision exceeds

results using summary as input
table lists the test results and number of classied summaries for training data
we obtained and instances for automatically labeled parallel and quence summaries respectively
results using article as input
table lists the sults of classication of the summary structures ing articles as input
the accuracy of the binary sication is

because the number of instances
com mecab coverage





r l


parallel model





r l


sequence model





r l


proposed





r l


all parallel sequence table evaluation results for three bullets summarization
coverage is baseline model which is proposed by see et al

parallel model and sequence model are sub models of the proposed method
pair of pairs

















r l

















r l

















r l





table evaluation of pairwise alignment
leftmost column indicates alignment between each sentence of the system summary and the oracle summary
the columns of and list the score of each sentence in the system summary
the bolded scores show the lowest scores
of sequence data is less than parallel data its recall is lower than that of the parallel data

experiment summarization similar to see et al
setup
in our experiment we use two models the baseline see et al
and proposed els
the hidden states of these two models are represented by dimensional matrices while the word embeddings are represented by dimensional vectors
the cabulary size is words in the case of both the source and target documents
furthermore we also followed see et al
to perform several data cleanup operations the ticles were truncated to words from the ning of the article for the training and test sets while the summaries shorter than words were excluded
the models were trained using adagrad duchi et al
with a learning rate of
and gradient clipping with a maximum gradient norm of
evaluation metric
we use the scores for and rouge l lin to evaluate the output this evaluation is applied to all test cases parallel summaries and sequence maries respectively
tem summary with a sentence of an oracle summary to evaluate consistency
the alignment is selected to maximize the average score of rouge l under the condition that there is no duplicate
result
tables and list the evaluation results for all parallel and sequence test data
as can be seen from tables and the proposed models perform the baseline model coverage
it is clear from table in a manner similar to the results in table the sequence model outperforms the ers on average
however as is evident from table each model behaves differently
we explore the reason for this in the next section
discussion
model comparison for the all values in table and ave values in table the scores of the sequence model are the it can be said that the sequence model highest
considers previous sentences more than the parallel model while generating the third sentence in a mary
the parallel model fails to incorporate the formation of the second sentence while generating the third sentence table
furthermore we align each sentence in the table shows an example of summarization
the ave ave ave coverage







r l



parallel model







r l



sequence model







r l



proposed







r l



a evaluation results in all test data
coverage







r l



parallel model







r l



sequence model







r l



proposed



r l







evaluation results in parallel test data
coverage







r l



parallel model







r l



sequence model







r l



proposed







r l



evaluation results in sequence test data
table evaluation results per sentence breakdown
the scores in the row of are computed between the rst sentence of the system summary and the sentence of the oracle summary
the rows of and are worked out in the same process
the row of ave is the average scores of and
example was annotated as a parallel type
the rst sentence in the reference mentions the result of a game
however it is difcult for all models to erate this information because it is not described in the article
among all the models only the quence model produces a sentence similar to the erence
this can be attributed to its ability to ate the second sentence based on previous tion more effectively than the others models

pairwise evaluation largest proportion of pair combinations their scores are almost as high as those of the pairs of
this is because the second and third sentences in a lel type summary are in no particular order
in contrast the score of the sentences in the verse order is low e

the scores shown in bold in the row of and the second column
thus based on the above discussion it is suggested that the rst sentence should be correctly generated in der to evaluate the summary structure appropriately
to analyze the system summary in greater detail we evaluate the performance of our proposed method for each pair combination
table lists the results of our pairwise evaluation
the pairs of occurred most frequently
it is considered that the summaries in the case of sequence or parallel types are suitably generated
these scores for the pairs of are higher than the others
the pairs of account for the second
evaluation methods in this work we evaluated the generated summaries using two methods of evaluation
we applied the ordinary evaluation which is discussed in section
as well as the evaluation of each pair based on the rouge l score which is discussed in section

the ordinary evaluation indicates summary mativeness
in contrast the evaluation of each pair shop on text summarization branches out pages
ramesh nallapati bing xiang and bowen zhou

abstractive text summarization using sequence in proceedings of sequence rnns and beyond
ceedings of the signll conference on tational natural language learning conll pages
ramesh nallapati feifei zhai and bowen zhou

summarunner a recurrent neural network based quence model for extractive summarization of ments
in proceedings of the thirty first aaai ference on articial intelligence aaai pages
alexander m
rush sumit chopra and jason weston

a neural attention model for abstractive tence summarization
in proceedings of the ference on empirical methods in natural language processing emnlp pages
abigail see peter j
liu and christopher d
manning

get to the point summarization with generator networks
in proceedings of the annual meeting of the association for computational tics acl pages
ilya sutskever oriol vinyals and quoc v le

quence to sequence learning with neural networks
in advances in neural information processing systems nips pages
shun tanaka ryohei sasano hiroya takamura and manabu okumura

news summarization with constraints of summary and sentence lengths and the number of sentences
in proceedings of the nual meeting of the association for natural language proacessing pages
zhaopeng tu zhengdong lu yang liu xiaohua liu and hang li

modeling coverage for neural chine translation
in proceedings of the annual meeting of the association for computational tics acl pages
oriol vinyals meire fortunato and navdeep jaitly
in advances in neural
pointer networks
formation processing systems nips pages
appendix a
example of generated summary shows how the proposed model generated summary sentences
it should be noted that the generated mary does not consist of the same order of sentence compared with the oracle summary
by performing pairwise evaluation using rouge l we were able to evaluate the order of the summary appropriately for the dataset
conclusion in this study we constructed a dataset focused on summaries with three sentences
we tated and analyzed the structure of the summaries in the considered dataset
in particular we posed a structure aware summarization model bining the summary structure classication model and summary specic summarization sub models
through our experiment we demonstrated that our proposed model improves summarization mance over the baseline model
in future work we will use category and gory tags to analyze the characteristics for each gory so that we can build specic models to improve the summarization system
references dzmitry bahdanau kyunghyun cho and yoshua
neural machine translation by jointly gio
in proceedings of learning to align and translate
the international conference on learning tations
john duchi elad hazan and yoram singer

adaptive subgradient methods for online learning and stochastic optimization
journal of machine learning research pages
sebastien jean kyunghyun cho roland memisevic and yoshua bengio

on using very large target in vocabulary for neural machine translation
ceedings of the annual meeting of the tion for computational linguistics and the national joint conference on natural language cessing acl ijcnlp pages
chen li xian qian and yang liu

using vised bigram based ilp for extractive summarization
in proceedings of the annual meeting of the
sociation for computational linguistics acl pages
chin yew lin

rouge a package for automatic evaluation of summaries
in proceedings of the source cologne s local paper express praised that yuya osako demonstrated its power in the game against burg the other day
it is reported that he demonstrated his power as a japan representative fw who showed up trying to toss the wolfsburg defense team
in the last season due to insufcient scoring ability he received erce criticism from passionate cologne fans so that cologne coach stoger dare to avoid appointment in a home game
nonetheless head coach stoger and manager joerg schmadtke is constantly defending osako as a ful football player
on the other hand cf modeste is showing off his team s top scorers i knew that his main job was cf said stoger but i really wanted to appoint him resulting in various positions by trial and error
but this season s cologne has adopted the system from the opening i am clear now
he will play as cf he asserted
osako got two goals in the rst round of pokal and also scored in the test match during international matches
and the rst opportunity of this season due to rudnevs injury osako showed off a success that the press paper praised as osako showed ball keeping ability speed breadth of vision and scoring ability through the game
in cologne s fw team he was even appreciated as the only player who achieves all the performance
as for the score this day he did not score at all because casteels played a blinder but he wants to maintain the good performance at cf as his professional career so as to secure a xed position
reference cologne to which yuya osako belongs drew in wolfsburg on the a local newspaper praised that osako tossed at the wolfsburg defense team in cologne s fw team he was described as having all the performance coverage rouge
cologne s local paper express reported that yuya osako demonstrated his power it defends osako as a wonderful football player in the wolfsburg game i understood that i am clear now he said parallel model rouge
cologne s local paper express recieved a erece criticism from the team top scorers fan i knew that cf was his main job said head coach stoger with a blinder last season yuya osako said i am clear now he will play as cf sequence model
cologne s local paper express reported that yuya osako demonstrated his power in the wolfsburg game japan representative fw who tried to toss at the wolfsburg defense team is praised as he demonstrated his performance i understood that i am clear now as he defended table example of generated three bullet points summarization in parallel structure
retrieved from
livedoor
com article on the january

