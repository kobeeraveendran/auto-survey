t c o v c
s c v
v i x r a a survey and taxonomy of adversarial neural networks for text to image synthesis jorge jonathan haicheng xingquan of computer electrical engineering and computer science florida atlantic university boca raton fl usa email
edu
edu
edu provincial key laboratory of e business nanjing university of finance and economics nanjing china email
ustc
edu
abstract text to image synthesis refers to computational methods which translate human written textual tions in the form of keywords or sentences into images with similar semantic meaning to the text
in earlier research image synthesis relied mainly on word to image correlation analysis combined with supervised methods to nd best alignment of the visual content matching to the text
recent progress in deep learning dl has brought a new set of unsupervised deep learning methods particularly deep erative models which are able to generate realistic visual images using suitably trained neural network models
the change of direction from the computer vision based approaches to articial intelligence ai driven methods ignited the intense interest in industry such as virtual reality recreational professional esports gaming and computer aided design
to automatically generate compelling images from text based natural language descriptions
in this paper we review the most recent development in the text to image synthesis research domain
our goal is to provide value by delivering a comparative review of the state of the art models in terms of their architecture and design
our survey rst introduces image synthesis and its challenges and then reviews key concepts such as generative adversarial networks gans and deep convolutional decoder neural networks dcnn
after that we propose a taxonomy to summarize gan based text image synthesis into four major categories semantic enhancement gans resolution enhancement gans diversity enhancement gans and motion enhancement gans
we elaborate the main tive of each group and further review typical gan architectures in each group
the taxonomy and the review outline the techniques and the evolution of different approaches and eventually provide a clear roadmap to summarize the list of contemporaneous solutions that utilize gans and dcnns to generate enthralling results in categories such as human faces birds owers room interiors object tion from edge maps games
the survey will conclude with a comparison of the proposed solutions challenges that remain unresolved and future developments in the text to image synthesis domain
text to image synthesis generative adversarial network gan deep learning machine keywords learning introduction gans and the variations that are now being proposed is the most interesting idea in the last years in ml in my opinion
yann lecun a picture is worth a thousand words while written text provide efcient effective and concise ways for communication visual content such as images is a more comprehensive accurate and wiley interdisciplinary reviews data mining and knowledge discovery
equally contributing authors
figure
early research on text to image synthesis zhu et al

the system uses correlation between keywords or keyphrase and images and identies informative and picturable text units then searches for the most likely image parts conditioned on the text and eventually optimizes the picture layout conditioned on both the text and image parts
ligible method of information sharing and understanding
generation of images from text descriptions i
e
text to image synthesis is a complex computer vision and machine learning problem that has seen great progress over recent years
automatic image generation from natural language may allow users to describe visual elements through visually rich text descriptions
the ability to do so effectively is highly desirable as it could be used in articial intelligence applications such as computer aided design image editing chen et al
yan et al
game engines for the development of the next generation of video et al
and pictorial art generation elgammal et al


traditional learning based text to image synthesis in the early stages of research text to image synthesis was mainly carried out through a search and in order to connect supervised learning combined process zhu et al
as shown in figure
text descriptions to images one could use correlation between keywords or keyphrase images that identies informative and picturable text units then these units would search for the most likely image parts conditioned on the text eventually optimizing the picture layout conditioned on both the text and the image parts
such methods often integrated multiple articial intelligence key components including natural language processing computer vision computer graphics and machine learning
the major limitation of the traditional learning based text to image synthesis approaches is that they lack the ability to generate new image content they can only change the characteristics of the given training images
alternatively research in generative models has advanced signicantly and delivers solutions to learn from training images and produce new visual content
for example yan et al
models each image as a composite of foreground and background
in addition a layered generative model with disentangled latent variables is learned using a variational encoder to generate visual content
because the learning is customized conditioned by given attributes the generative models of can generate images with respect to different attributes such as gender hair color age
as shown in figure

gan based text to image synthesis although generative model based text to image synthesis provides much more realistic image synthesis results the image generation is still conditioned by the limited attributes
in recent years several papers have been published on the subject of text to image synthesis
most of the contributions from these papers rely on multimodal learning approaches that include generative adversarial networks and deep figure
supervised learning based text to image synthesis yan et al

the supervised ing process aims to learn layered generative models to generate visual content
because the learning is customized conditioned by the given attributes the generative models of can generative images with respect to different attributes such as hair color age
figure
generative adversarial neural network gan based text to image synthesis huang et al

gan based text to image synthesis combines discriminative and generative learning to train neural networks resulting in the generated images semantically resemble to the training samples or lored to a subset of training images i
e
conditioned outputs
is a feature embedding function which converts text as feature vector
z is a latent vector following normal distributions with zero mean
denotes a synthetic image generated from the generator using latent vector z and the text features as the input
d denotes the prediction of the discriminator based on the input x the generated image and text information of the generated image
the explanations about the generators and discriminators are detailed in section

figure
a visual summary of gan based text to image synthesis process and the summary of gan based frameworks methods reviewed in the survey
convolutional decoder networks as their main drivers to generate entrancing images from text wu et al
reed et al
goodfellow et al
xu et al
odena et al

first introduced by ian goodfellow et al
goodfellow et al
generative adversarial networks gans consist of two neural networks paired with a discriminator and a generator
these two models compete with one another with the generator attempting to produce synthetic fake samples that will fool the discriminator and the discriminator attempting to differentiate between real genuine and synthetic samples
because gans adversarial training aims to cause generators to produce images similar to the real training images gans can naturally be used to generate synthetic images image synthesis and this process can even be customized further by using text descriptions to specify the types of images to generate as shown in figure
much like text to speech and speech to text conversion there exists a wide variety of problems that text to image synthesis could solve in the computer vision eld specically reed et al
haynes et al

nowadays researchers are attempting to solve a plethora of computer vision lems with the aid of deep convolutional networks generative adversarial networks and a combination of multiple methods often called multimodal learning methods reed et al

for simplicity multiple learning methods will be referred to as multimodal learning hereafter ngiam et al

searchers often describe multimodal learning as a method that incorporates characteristics from several methods algorithms and ideas
this can include ideas from two or more learning approaches in order to create a robust implementation to solve an uncommon problem or improve a solution reed et al
yang et al
li et al
dash et al
baltrusaitis et al

in this survey we focus primarily on reviewing recent works that aim to solve the challenge of text image synthesis using generative adversarial networks gans
in order to provide a clear roadmap we propose a taxonomy to summarize reviewed gans into four major categories
our review will elaborate the motivations of methods in each category analyze typical models their network architectures and possible drawbacks for further improvement
the visual abstract of the survey and the list of reviewed gan frameworks is shown in figure
the remainder of the survey is organized as follows
section presents a brief summary of existing works on subjects similar to that of this paper and highlights the key distinctions making ours unique
section gives a short introduction to gans and some preliminary concepts related to image generation as they are the engines that make text to image synthesis possible and are essential building blocks to achieve photo realistic images from text descriptions
section proposes a taxonomy to summarize gan based text to image synthesis discusses models and architectures of novel works focused solely on text to image synthesis
this section will also draw key contributions from these works in relation to their applications
section reviews gan based text to image synthesis benchmarks performance metrics and comparisons including a simple review of gans for other applications
in section we conclude with a brief summary and outline ideas for future interesting developments in the eld of to image synthesis
related work with the growth and success of gans deep convolutional decoder networks and multimodal ing methods these techniques were some of the rst procedures which aimed to solve the challenge of image synthesis
many engineers and scientists in computer vision and ai have contributed through tensive studies and experiments with numerous proposals and publications detailing their contributions
because gans introduced by goodfellow et al
are emerging research topics their practical applications to image synthesis are still in their infancy
recently many new gan architectures and signs have been proposed to use gans for different applications e

using gans to generate tal texts wang and wan or using gans to transform natural images into cartoons chen et al

although gans are becoming increasingly popular very few survey papers currently exist to marize and outline contemporaneous technical innovations and contributions of different gan tures hong et al
creswell et al

survey papers specically attuned to analyzing different contributions to text to image synthesis using gans are even more scarce
we have thus found two surveys huang et al
wu et al
on image synthesis using gans which are the two most closely related publications to our survey objective
in the following paragraphs we briey summarize each of these surveys and point out how our objectives differ from theirs
in huang et al
the authors provide an overview of image synthesis using gans
in this survey the authors discuss the motivations for research on image synthesis and introduce some ground information on the history of gans including a section dedicated to core concepts of gans namely generators discriminators and the min max game analogy and some enhancements to the inal gan model such as conditional gans addition of variational auto encoders

in this survey we will carry out a similar review of the background knowledge because the understanding of these preliminary concepts is paramount for the rest of the paper
three types of approaches for image ation are reviewed including direct methods single generator and discriminator hierarchical methods two or more generator discriminator pairs each with a different goal and iterative methods each generator discriminator pair generates a gradually higher resolution image
following the introduction huang et al
discusses methods for text to image and image to image synthesis respectively and also describes several evaluation metrics for synthetic images including inception scores and frechet inception distance fid and explains the signicance of the discriminators acting as learned loss tions as opposed to xed loss functions
different from the above survey which has a relatively broad scope in gans our objective is heavily focused on text to image synthesis
although this topic text to image synthesis has indeed been ered in huang et al
they did so in a much less detailed fashion mostly listing the many different works in a time sequential order
in comparison we will review several representative methods in the eld and outline their models and contributions in detail
similarly to huang et al
the second survey paper wu et al
begins with a standard introduction addressing the motivation of image synthesis and the challenges it presents followed by a section dedicated to core concepts of gans and enhancements to the original gan model
in addition the paper covers the review of two types of applications unconstrained applications of image thesis such as super resolution image inpainting
and constrained image synthesis applications namely image to image text to image and sketch to image and also discusses image and video editing using gans
again the scope of this paper is intrinsically comprehensive while we focus specically on text to image and go into more detail regarding the contributions of novel state of the art models
other surveys have been published on related matters mainly related to the advancements and plications of gans zhang et al
but we have not found any prior works which focus specically on text to image synthesis using gans
to our knowledge this is the rst paper to do so
preliminaries and frameworks in this section we rst introduce preliminary knowledge of gans and one of its commonly used variants conditional gan i
e
cgan which is the building block for many gan based text to image synthesis models
after that we briey separate gan based text to image synthesis into two types simple gan frameworks vs
advanced gan frameworks and discuss why advanced gan architecture for image synthesis
notice that the simple vs
advanced gan framework separation is rather too brief our taxonomy in the next section will propose a taxonomy to summarize advanced gan frameworks into four categories based on their objective and designs

generative adversarial neural network before moving on to a discussion and analysis of works applying gans for text to image synthesis there are some preliminary concepts enhancements of gans datasets and evaluation metrics that are present in some of the works described in the next section and are thus worth introducing
as stated previously gans were introduced by ian goodfellow et al
goodfellow et al
in and consist of two deep neural networks a generator and a discriminator which are trained pendently with conicting goals the generator aims to generate samples closely related to the original data distribution and fool the discriminator while the discriminator aims to distinguish between samples from the generator model and samples from the true data distribution by calculating the probability of the sample coming from either source
a conceptual view of the generative adversarial network gan architecture is shown in figure
the training of gans is an iterative process that with each iteration updates the generator and the discriminator with the goal of each defeating the other
leading each model to become increasingly adept at its specic task until a threshold is reached
this is analogous to a min max game between the two models according to the following equation min g max d v dd gg dd in eq
denotes a multi dimensional sample e

an image and z denotes a multi dimensional latent space vector e

a multidimensional data point following a predened distribution function such as that of normal distributions
dd denotes a discriminator function controlled by parameters d which aims to classify a sample into a binary space
gg denotes a generator function controlled by parameters g which aims to generate a sample from some latent space vector
for example means using a latent vector z to generate a synthetic fake image and dd means to classify an image as binary output i
e
true false or
in the gan setting the discriminator dd is learned to distinguish a genuine true image labeled as from fake images labeled as
therefore given a true image the ideal output from the discriminator dd would be
given a fake image generated from the generator the ideal prediction from the discriminator dd gg z would be indicating the sample is a fake image
following the above denition the min max objective function in eq
aims to learn parameters for the discriminator and generator g to reach an optimization goal the discriminator intends to differentiate true vs
fake images with maximum capability maxd whereas the generator intends to minimize the difference between a fake image vs
a true image ming
in other words the discriminator sets the characteristics and the generator produces elements often images iteratively until it meets the attributes set forth by the discriminator
gans are often used with images and other visual elements and are notoriously efcient in generating compelling and convincing photorealistic images
most recently gans were used to generate an original painting in an unsupervised fashion radford et al

the following sections go into further detail regarding how the generator and discriminator are trained in gans
generator in image synthesis the generator network can be thought of as a mapping from one representation space latent space to another actual data creswell et al

when it comes to image synthesis all of the images in the data space fall into some distribution in a very complex and dimensional feature space
sampling from such a complex space is very difcult so gans instead train a generator to create synthetic images from a much more simple feature space usually random noise called the latent space
the generator network performs up sampling of the latent space and is usually real data sample from real data sigmoid function real fake random noise vector z fake sample from generator figure
a conceptual view of the generative adversarial network gan architecture
the generator is trained to generate synthetic fake resemble to real samples from a random noise distribution
the fake samples are fed to the discriminator along with real samples
the discriminator is trained to differentiate fake samples from real samples
the iterative training of the generator and the discriminator helps gan deliver good generator generating samples very close to the underlying training samples
a deep neural network consisting of several convolutional fully connected layers creswell et al

the generator is trained using gradient descent to update the weights of the generator network with the aim of producing data in our case images that the discriminator classies as real
discriminator the discriminator network can be thought of as a mapping from image data to the probability of the image coming from the real data space and is also generally a deep neural network consisting of several convolution fully connected layers
however the discriminator performs down sampling as opposed to up sampling
like the generator it is trained using gradient descent but its goal is to update the weights so that it is more likely to correctly classify images as real or fake
in gans the ideal outcome is for both the generator s and discriminator s cost functions to converge so that the generator produces photo realistic images that are indistinguishable from real data and the discriminator at the same time becomes an expert at differentiating between real and synthetic data
this however is not possible since a reduction in cost of one model generally leads to an increase in cost of the other
this phenomenon makes training gans very difcult and training them simultaneously both models performing gradient descent in parallel often leads to a stable orbit where neither model is able to converge
to combat this the generator and discriminator are often trained independently
in this case the gan remains the same but there are different training stages
in one stage the weights of the generator are kept constant and gradient descent updates the weights of the discriminator and in the other stage the weights of the discriminator are kept constant while gradient descent updates the weights of the generator
this is repeated for some number of epochs until a desired low cost for each model is reached salimans al


cgan conditional gan conditional generative adversarial networks cgan are an enhancement of gans proposed by mirza and osindero shortly after the introduction of gans by goodfellow et al

the objective function of the cgan is dened in eq
which is very similar to the gan objective function in eq
except that the inputs to both discriminator and generator are conditioned by a class label
min g max d v dd gg e e dd the main technical innovation of cgan is that it introduces an additional input or inputs to the original gan model allowing the model to be trained on information such as class labels or other conditioning variables as well as the samples themselves concurrently
whereas the original gan was trained only with samples from the data distribution resulting in the generated sample reecting the general data distribution cgan enables directing the model to generate more tailored outputs
sample from real data real still bird images sigmoid function real fake random noise vector z fake sample from generator condition vector y still bird figure
a conceptual view of the conditional gan architecture
the generator generates samples from a random noise distribution and some condition vector in this case text
the fake samples are fed to the discriminator along with real samples and the same condition vector and the discriminator calculates the probability that the fake sample came from the real data distribution
in figure the condition vector is the class label text string red bird which is fed to both the generator and discriminator
it is important however that the condition vector is related to the real data
if the model in figure was trained with the same set of real data red birds but the condition text was yellow sh the generator would learn to create images of red birds when conditioned with the text yellow sh
note that the condition vector in cgan can come in many forms such as texts not just limited to the class label
such a unique design provides a direct solution to generate images conditioned by predened specications
as a result cgan has been used in text to image synthesis since the very rst day of its invention although modern approaches can deliver much better text to image synthesis results

simple gan frameworks for text to image synthesis in order to generate images from text one simple solution is to employ the conditional gan cgan designs and add conditions to the training samples such that the gan is trained with respect to the underlying conditions
several pioneer works have followed similar designs for text to image synthesis
an essential disadvantage of using cgan for text to image synthesis is that that it can not handle complicated textual descriptions for image generation because cgan uses labels as conditions to strict the gan inputs
if the text inputs have multiple keywords or long text descriptions they not be used simultaneously to restrict the input
instead of using text as conditions another two proaches reed et al
dash et al
use text as input features and concatenate such features with other features to train discriminator and generator as shown in figure and c
to ensure text being used as gan input a feature embedding or feature representation learning bengio et al
zhang et al
function is often introduced to convert input text as numeric features which are further concatenated with other features to train gans

advanced gan frameworks for text to image synthesis motivated by the gan and conditional gan cgan design many gan based frameworks have been proposed to generate images with different designs and architectures such as using multiple tors using progressively trained discriminators or using hierarchical discriminators
figure outlines several advanced gan frameworks in the literature
in addition to these frameworks many news figure
a simple architecture comparisons between ve gan networks for text to image synthesis
this gure also explains how texts are fed as input to train gan to generate images
conditional gan cgan mirza and osindero use labels to condition the input to the generator and the inator
the nal output is discriminator similar to generic gan manifold interpolation aware discriminator gan gan int cls reed et al
feeds text input to both generator and discriminator texts are preprocessed as embedding features using function and concatenated with other input before feeding to both generator and discriminator
the nal output is discriminator similar to generic gan auxiliary classier gan ac gan odena et al
uses an auxiliary sier layer to predict the class of the image to ensure that the output consists of images from different classes resulting in diversied synthesis images text conditioned auxiliary classier gan gan dash et al
share similar design as gan int cls whereas the output include both a discriminator and a classier which can be used for classication and e text conditioned semantic classier gan text segan cha et al
uses a regression layer to estimate the semantic vance between the image so the generated images are not limited to certain classes and are semantically matching to the text input
figure
a high level comparison of several advanced gans framework for text to image synthesis
all frameworks take text red triangle as input and generate output images
from left to right a uses multiple discriminators and one generator durugkar et al
nguyen et al
b uses multiple stage gans where the output from one gan is fed to the next gan as input zhang et al
denton et al
c progressively trains symmetric discriminators and generators huang et al
and d uses a single stream generator with a hierarchically nested discriminator trained from end to end zhang et al

signs are being proposed to advance the eld with rather sophisticated designs
for example a recent work gao et al
proposes to use a pyramid generator and three independent discriminators each focusing on a different aspect of the images to lead the generator towards creating images that are photo realistic on multiple levels
another recent publication cha et al
proposes to use criminator to measure semantic relevance between image and text instead of class prediction like most discriminator in gans does resulting a new gan structure outperforming text conditioned auxiliary classier tac gan dash et al
and generating diverse realistic and relevant to the input text regardless of class
in the following section we will rst propose a taxonomy that summarizes advanced gan works for text to image synthesis and review most recent proposed solutions to the challenge of ing photo realistic images conditioned on natural language text descriptions using gans
the solutions we discuss are selected based on relevance and quality of contributions
many publications exist on the subject of image generation using gans but in this paper we focus specically on models for text image synthesis with the review emphasizing on the model and contributions for text to image thesis
at the end of this section we also briey review methods using gans for other image synthesis applications
text to image synthesis taxonomy and categorization in this section we propose a taxonomy to summarize advanced gan based text to image synthesis frameworks as shown in figure
the taxonomy organizes gan frameworks into four categories cluding semantic enhancement gans resolution enhancement gans diversity enhancement gans and motion enhancement gags
following the proposed taxonomy each subsection will introduce several typical frameworks and address their techniques of using gans to solve certain aspects of the text to mage synthesis challenges

gan based text to image synthesis taxonomy although the ultimate goal of text to image synthesis is to generate images closely related to the textual descriptions the relevance of the images to the texts are often validated from different perspectives due to the inherent diversity of human perceptions
for example when generating images matching to the description rose owers some users many know the exact type of owers they like and intend to generate rose owers with similar colors
other users may seek to generate high quality rose owers with a nice background e

garden
the third group of users may be more interested in generating owers similar to rose but with different colors and visual appearance e

roses begonia and peony
the fourth group of users may want to not only generate ower images but also use them to form a meaningful action e

a video clip showing ower growth performing a magic show using those owers or telling a love story using the owers
from the text to image synthesis point of view the rst group of users intend to precisely control the semantic of the generated images and their goal is to match the texts and images at the semantic level
the second group of users are more focused on the resolutions and the qualify of the images in addition to the requirement that the images and texts are semantically related
for the third group of users their goal is to diversify the output images such that their images carry diversied visual appearances and are also semantically related
the fourth user group adds a new dimension in image synthesis and aims to generate sequences of images which are coherent in temporal order i
e
capture the motion information
based on the above descriptions we categorize gan based text to image synthesis into a taxonomy with four major categories as shown in fig

semantic enhancement gans semantic enhancement gans represent pioneer works of gan frameworks for text to image synthesis
the main focus of the gan frameworks is to ensure that the generated images are semantically related to the input texts
this objective is mainly achieved by using a neural network to encode texts as dense features which are further fed to a second network to generate images matching to the texts
resolution enhancement gans resolution enhancement gans mainly focus on generating high qualify images which are semantically matched to the texts
this is mainly achieved through a multi stage gan framework where the outputs from earlier stage gans are fed to the second or later stage gan to generate better qualify images
diversity enhancement gans diversity enhancement gans intend to diversify the output ages such that the generated images are not only semantically related but also have different types and visual appearance
this objective is mainly achieved through an additional component to timate semantic relevance between generated images and texts in order to maximize the output diversity
motion enhancement gans motion enhancement gans intend to add a temporal dimension to the output images such that they can form meaningful actions with respect to the text descriptions
this goal mainly achieved though a two step process which rst generates images matching to the actions of the texts followed by a mapping or alignment procedure to ensure that images are coherent in the temporal order
in the following we will introduce how these gan frameworks evolve for text to image synthesis and will also review some typical methods of each category

semantic enhancement gans semantic relevance is one the of most important criteria of the text to image synthesis
for most gnas discussed in this survey they are required to generate images semantically related to the text tions
however the semantic relevance is a rather subjective measure and images are inherently rich in terms of its semantics and interpretations
therefore many gans are further proposed to enhance the text to image synthesis from different perspectives
in this subsection we will review several classical approaches which are commonly served as text to image synthesis baseline


dc gan deep convolution generative adversarial network dc gan reed et al
represents the pioneer work for text to image synthesis using gans
its main goal is to train a deep convolutional generative adversarial network dc gan on text features
during this process these text features are encoded by another neural network
this neural network is a hybrid convolutional recurrent network at the character level
concurrently both neural networks have also feed forward inference in the way they condition text features
generating realistic images automatically from natural language text is the motivation of several of the works proposed in this computer vision eld
however actual articial intelligence ai figure
a taxonomy and categorization of advanced gan frameworks for text to image synthesis
we categorize advanced gan frameworks into four major categories semantic enhancement gans resolution enhancement gans diversity enhancement gans and motion enhancement gags
the relationship between relevant frameworks and their publication date are also outlined as a reference
systems are far from achieving this task reed et al
liu et al
yang et al
li et al
wang and gupta zhang et al
mirza and osindero
lately recurrent ral networks led the way to develop frameworks that learn discriminatively on text features
at the same time generative adversarial networks gans began recently to show some promise on generating compelling images of a whole host of elements including but not limited to faces birds owers and common images such as room et al

dc gan is a multimodal learning model that attempts to bridge together both of the above mentioned unsupervised machine learning algorithms the recurrent neural networks rnn and generative adversarial networks gans with the sole purpose of speeding the generation of text to image synthesis
deep learning shed some light to some of the most sophisticated advances in natural language resentation image synthesis wu et al
reed et al
wang et al
huang et al
and classication of generic data han et al

however a bulk of the latest breakthroughs in deep learning and computer vision were related to supervised learning reed et al

even though natural language and image synthesis were part of several contributions on the supervised side of deep learning unsupervised learning saw recently a tremendous rise in input from the research community specially on two subproblems text based natural language and image synthesis dong et al
yang et al
reed et al
cha et al
zhang et al

these subproblems are typically subdivided as focused research areas
dc gan s contributions are mainly driven by these two research areas
in order to generate plausible images from natural language dc gan contributions revolve around developing a straightforward yet effective gan architecture and training strategy that allows natural text to image synthesis
these contributions are primarily tested on the caltech ucsd birds and flowers datasets
each image in these datasets carry ve text descriptions
these text descriptions were created by the research team when setting up the evaluation environment
the gans model is subsequently trained on several subcategories
subcategories in this research represent the training and testing sub datasets
the performance shown by these experiments display a promising yet effective way to generate images from textual natural language descriptions reed et al



dc gan extensions following the pioneer dc gan framework reed et al
many researches propose revised work structures e

different discriminaotrs in order to improve images with better semantic relevance to the texts
based on the deep convolutional adversarial network dc gan network architecture gan cls with image text matching discriminator gan int learned with text manifold interpolation and gan int cls which combines both are proposed to nd semantic match between text and age
similar to the dc gan architecture an adaptive loss function i
e
perceptual loss johnson et al
is proposed for semantic image synthesis which can synthesize a realistic image that not only matches the target text description but also keep the irrelavant

background from source images dong et al

regarding to the perceptual losses three loss functions i
e
pixel struction loss activation reconstruction loss and texture reconstruction loss are proposed in cha et al
in which they construct the network architectures based on the dc gan i
e
gan int in dong et al
pixel gan int cls vgg and gan int cls gram with respect to three losses
a residual transformation unit is added in the network to retain similar structure of the source image
following the dong et al
and considering the features in early layers address background while foreground is obtained in latter layers in cnn a pair of discriminators with different architectures i
e
paired d gan is proposed to synthesize background and foreground from a source image ately vo and sugimoto
meanwhile the skip connection in the generator is employed to more precisely retain background information in the source image


mc gan when synthesising images most text to image synthesis methods consider each output image as one single unit to characterize its semantic relevance to the texts
this is likely problematic because most images naturally consist of two crucial components foreground and background
without properly separating these two components it s hard to characterize the semantics of an image if the whole image is treated as a single unit without proper separation
in order to enhance the semantic relevance of the images a multi conditional gan gan park et al
is proposed to synthesize a target image by combining the background of a source image and a text described foreground object which does not exist in the source image
a unique feature of mc gan is that it proposes a synthesis block in which the background feature is extracted from the given image without non linear function i
e
only using convolution and batch normalization and the foreground feature is the feature map from the previous layer
because mc gan is able to properly model the background and foreground of the generated images a unique strength of mc gan is that users are able to provide a base image and mc gan is able to preserve the background information of the base image to generate new images

resolution enhancement gans due to the fact that training gans will be much difcult when generating high resolution images a two stage gan i
e
is proposed in which rough
e
low resolution images are generated in stage i and rened in stage ii
to further improve the quality of generated images the second version of stackgan i
e
is proposed to use multi stage gans to generate multi scale images
a color consistency regularization term is also added into the loss to keep the consistency of images in different scales
while stackgan and are both built on the global sentence vector is posed to use attention mechanism i
e
deep attentional multimodal similarity model damsm to model the multi level information i
e
word level and sentence level into gans
in the following stackgan and will be explained in detail
recently dynamic memory generative adversarial network i
e
dm et al
which uses a dynamic memory component is proposed to focus on reningthe initial generated image which is the key to the success of generating high quality images


stackgan in zhang et al
proposed a model for generating photo realistic images from text descriptions called stackgan stacked generative adversarial network zhang et al

in their work they dene a two stage model that uses two cascaded gans each corresponding to one of the stages
the stage i gan takes a text description as input converts the text description to a text embedding containing several conditioning variables and generates a low quality image with rough shapes and colors based on the computed conditioning variables
the stage ii gan then takes this low quality stage i image as well as the same text embedding and uses the conditioning variables to correct and add more detail to the stage i result
the output of stage ii is a photorealistic image that resembles the text description with compelling accuracy
one major contribution of stackgan is the use of cascaded gans for text to image synthesis through a sketch renement process
by conditioning the stage ii gan on the image produced by the stage i gan and text description the stage ii gan is able to correct defects in the stage i output resulting in high quality images
prior works have utilized stacked gans to separate the age generation process into structure and style wang and gupta multiple stages each generating lower level representations from higher level representations of the previous stage huang et al
and multiple stages combined with a laplacian pyramid approach denton et al
which was troduced for image compression by p
burt and e
adelson in and uses the differences between consecutive down samples of an original image to reconstruct the original image from its down sampled version burt and adelson
however these works did not use text descriptions to condition their generator models
conditioning augmentation is the other major contribution of stackgan
prior works transformed the natural language text description into a xed text embedding containing static conditioning variables which were fed to the generator reed et al

stackgan does this and then creates a gaussian distribution from the text embedding and randomly selects variables from the gaussian distribution to add to the set of conditioning variables during training
this encourages robustness by introducing small variations to the original text embedding for a particular training image while keeping the training image that the generated output is compared to the same
the result is that the trained model produces more diverse images in the same distribution when using conditioning augmentation than the same model using a xed text embedding zhang et al



proposed by the same users as stackgan is also a stacked gan model but organizes the generators and discriminators in a tree like structure zhang et al
with multiple stages
the rst stage combines a noise vector and conditioning variables with conditional augmentation introduced in zhang et al
for input to the rst generator which generates a low resolution image by default this can be changed depending on the desired number of stages
each lowing stage uses the result from the previous stage and the conditioning variables to produce gradually higher resolution images
these stages do not use the noise vector again as the creators assume that the randomness it introduces is already preserved in the output of the rst stage
the nal stage produces a high quality image
introduces the joint conditional and unconditional approximation in their designs zhang et al

the discriminators are trained to calculate the loss between the image produced by the generator and the conditioning variables measuring how accurately the image represents the scription as well as the loss between the image and real images probability of the image being real or fake
the generators then aim to minimize the sum of these losses improving the nal result


attentional generative adversarial network xu et al
is very similar in terms of its structure to zhang et al
discussed in the previous section but some novel ponents are added
like previous works reed et al
zhang et al
a text encoder generates a text embedding with conditioning variables based on the overall sentence
additionally the text encoder generates a separate text embedding with conditioning variables based on individual words
this process is optimized to produce meaningful variables using a bidirectional recurrent neural work brnn more specically bidirectional long short term memory lstm schuster and paliwal which for each word in the description generates conditions based on the previous word as well as the next word bidirectional
the rst stage of generates a low resolution image based on the sentence level text embedding and random noise vector
the output is fed along with the level text embedding to an attention model which matches the word level conditioning variables to regions of the stage i image producing a word context matrix
this is then fed to the next stage of the model along with the raw previous stage output
each consecutive stage works in the same manner but produces gradually higher resolution images conditioned on the previous stage
two major contributions were introduced in attngan the attentional generative network and the deep attentional multimodal similarity model damsm zhang et al

the attentional ative network matches specic regions of each stage s output image to conditioning variables from the word level text embedding
this is a very worthy contribution allowing each consecutive stage to focus on specic regions of the image independently adding attentional details region by region as opposed to the whole image
the damsm is also a key feature introduced by which is used after the result of the nal stage to calculate the similarity between the generated image and the text embedding at both the sentence level and the more ne grained word level
table shows scores from different metrics for stackgan attngan and hdgan on the cub oxford and coco datasets
the table shows that outperforms the other models in terms of is on the cub dataset by a small amount and greatly outperforms them on the coco dataset


hdgan hierarchically nested adversarial network hdgan is a method proposed by zhang et al
and its main objective is to tackle the difcult problem of dealing with photographic images from semantic text descriptions
these semantic text descriptions are applied on images from diverse datasets
this method introduces adversarial objectives nested inside hierarchically oriented networks zhang et al

hierarchical networks helps regularize mid level manifestations
in addition to regularize level manifestations it assists the training of the generator in order to capture highly complex still media elements
these elements are captured in statistical order to train the generator based on settings tracted directly from the image
the latter is an ideal scenario
however this paper aims to incorporate a single stream architecture
this single stream architecture functions as the generator that will form an optimum adaptability towards the jointed discriminators
once jointed discriminators are setup in an optimum manner the single stream architecture will then advance generated images to achieve a much higher resolution zhang et al

the main contributions of the hdgans include the introduction of a visual semantic similarity sure zhang et al

this feature will aid in the evaluation of the consistency of generated images
in addition to checking the consistency of generated images one of the key objectives of this step is to test the logical consistency of the end product zhang et al

the end product in this case would be images that are semantically mapped from text based natural language descriptions to each area on the picture e

a wing on a bird or petal on a ower
deep learning has created a multitude of opportunities and challenges for researchers in the computer vision ai eld
coupled with gan and multimodal ing architectures this eld has seen tremendous growth reed et al
liu et al
yang et al
li et al
wang and gupta zhang et al
mirza and osindero
based on these advancements hdgans attempt to further extend some desirable and less common features when generating images from textual natural language zhang et al

in other words it takes sentences and treats them as a hierarchical structure
this has some positive and negative implications in most cases
for starters it makes it more complex to generate compelling images
however one of the key benets of this elaborate process is the realism obtained once all processes are completed
in addition one common feature added to this process is the ability to identify parts of sentences with bounding boxes
if a sentence includes common characteristics of a bird it will surround the attributes of such bird with bounding boxes
in practice this should happen if the desired image have other ements such as human faces e

eyes hair owers e

petal size color or any other inanimate object e

a table a mug
finally hdgans evaluated some of its claims on common ideal text to image datasets such as cub coco and reed et al
zhang et al
liu et al
yang et al
li et al
wang and gupta zhang et al
mirza and osindero
these datasets were rst utilized on earlier works reed et al
and most of them sport modied features such image annotations labels or descriptions
the qualitative and quantitative results reported by researchers in this study were far superior of earlier works in this same eld of computer vision ai

diversity enhancement gans in this subsection we introduce text to image synthesis methods which try to maximize the diversity of the output images based on the text descriptions


ac gan two issues arise in the traditional gans mirza and osindero for image synthesis bilirty problem traditional gans can not predict a large number of image categories and diversity problem images are often subject to one to many mapping so one image could be labeled as different tags or being described using different texts
to address these problems gan conditioned on additional information e

cgan is an alternative solution
however although cgan and many previously troduced approaches are able to generate images with respect to the text descriptions they often output images with similar types and visual appearance
slightly different from the cgan auxiliary classier gans ac gan odena et al
poses to improve the diversity of output images by using an auxiliary classier to control output images
the overall structure of ac gan is shown in fig

in ac gan every generated image is ated with a class label in addition to the true fake label which are commonly used in gan or cgan
the discriminator of ac gan not only outputs a probability distribution over sources i
e
whether the image is true or fake it also output a probability distribution over the class label i
e
predict which class the image belong to
by using an auxiliary classier layer to predict the class of the image ac gan is able to use the predicted class labels of the images to ensure that the output consists of images from different classes resulting in diversied synthesis images
the results show that ac gan can generate images with high diversity


tac gan building on the ac gan tac gan dash et al
is proposed to replace the class information with textual descriptions as the input to perform the task of text to image synthesis
the architecture of tac gan is shown in fig
which is similar to ac gan
overall the major difference between tac gan and ac gan is that tac gan conditions the generated images on text descriptions instead of on a class label
this design makes tac gan more generic for image synthesis
for tac gan it imposes restrictions on generated images in both texts and class labels
the input vector of tac gan s generative network is built based on a noise vector and embedded vector tation of textual descriptions
the discriminator of tac gan is similar to that of the ac gan which not only predicts whether the image is fake or not but also predicts the label of the images
a minor difference of tac gan s discriminator compared to that of the ac gan is that it also receives text information as input before performing its classication
the experiments and validations on the owers dataset show that the results produced by tac gan are slightly better that other approaches including gan int cls and stackgan


text segan in order to improve the diversity of the output images both ac gan and tac gan s discriminators predict class labels of the synthesised images
this process likely enforces the semantic diversity of the images but class labels are inherently restrictive in describing image semantics and images described by text can be matched to multiple labels
therefore instead of predicting images class labels an alternative solution is to directly quantify their semantic relevance
the architecture of text segan is shown in fig

in order to directly quantify semantic evance text segan cha et al
adds a regression layer to estimate the semantic relevance between the image and text instead of a classier layer of predicting labels
the estimated semantic reference is a fractional value ranging between and with a higher value reecting better semantic relevance between the image and text
due to this unique design an inherent advantage of text segan is that the generated images are not limited to certain classes and are semantically matching to the text input
experiments and validations on ower dataset show that text segan can generate diverse images that are semantically relevant to the input text
in addition the results of text segan show improved inception score compared to other approaches including gan int cls stackgan tac gan and hdgan


mirrorgan and scene graph gan due to the inherent complexity of the visual images and the diversity of text descriptions i
e
same words could imply different meanings it is difculty to precisely match the texts to the visual images at the semantic levels
for most methods we have discussed so far they employ a direct text to image generation process but there is no validation about how generated images comply with the text in a reverse fashion
to ensure the semantic consistency and diversity mirrorgan qiao et al
employs a ror structure which reversely learns from generated images to output texts an image to text process to further validate whether generated are indeed consistent to the input texts
mirrowgan includes three modules a semantic text embedding module stem a global local collaborative attentive ule for cascaded image generation glam and a semantic text regeneration and alignment module stream
the back to back text to image and image to text t are combined to sively enhance the diversity and semantic consistency of the generated images
in order to enhance the diversity of the output image scene graph gan johnson et al
poses to use visual scene graphs to describe the layout of the objects allowing users to precisely specic the relationships between objects in the images
in order to convert the visual scene graph as input for gan to generate images this method uses graph convolution to process input graphs
it computes a scene layout by predicting bounding boxes and segmentation masks for objects
after that it converts the computed layout to an image with a cascaded renement network

motion enhancement gans instead of focusing on generating static images another line of text to image synthesis research focuses on generating videos i
e
sequences of images from texts
in this context the synthesised videos are often useful resources for automated assistance or story telling


obamanet and one early interesting work of motion enhancement gans is to generate spoofed speech and lip sync videos or talking face of barack obama i
e
obamanet based on text input kumar et al

this framework is consisted of three parts i
e
text to speech using mouth shape representation synced to the audio using a time delayed lstm and video generation conditioned on the mouth shape using u net architecture
although the results seem promising obamanet only models the mouth region and the videos are not generated from noise which can be regarded as video prediction other than video generation
another meaningful trial of using synthesised videos for automated assistance is to translate spoken language e

text into sign language video sequences i
e
stoll et al

this is often achieved through a two step process converting texts as meaningful units to generate images followed by a learning component to arrange images into sequential order for best representation
more ically using rnn based machine translation methods texts are translated into sign language gloss quences
then glosses are mapped to skeletal pose sequences using a lookup table
to generate videos a conditional dcgan with the input of concatenation of latent representation of the image for a base pose and skeletal pose information is built


in li et al
a text to video model is proposed based on the cgan in which the input is the isometric gaussian noise with the text gist vector served as the generator
a key component of generating videos from text is to train a conditional generative model to extract both static and dynamic information from text followed by a hybrid framework combining a variational autoencoder vae and a generative adversarial network gan
more specically relies on two types of features static features and dynamic features to erate videos
static features called gist are used to sketch text conditioned background color and object layout structure
dynamic features on the other hand are considered by transforming input text into an image lter which eventually forms the video generator which consists of three entangled neural networks
the text gist vector is generated by a gist generator which maintains static information e

background and a which captures the dynamic information i
e
actions in the text to generate videos
as demonstrated in the paper li et al
the generated videos are semantically related to the texts but have a rather low quality e

only resolution


storygan different from which generates videos from a single text storygan aims to produce dynamic scenes consistent of specied texts i
e
story written in a multi sentence paragraph using a sequential gan model li et al

story encoder context encoder and discriminators are the main nents of this model
by using stochastic sampling the story encoder intends to learn an low dimensional embedding vector for the whole story to keep the continuity of the story
the context encoder is posed to capture contextual information during sequential image generation based on a deep rnn
two discriminators of storygan are image discriminator which evaluates the generated images and story discriminator which ensures the global consistency
the experiments and comparisons on clevr dataset and pororo cartoon dataset which are inally used for visual question answering show that storygan improves the generated video qualify in terms of structural similarity index ssim visual qualify consistence and relevance the last three measure are based on human evaluation
table
a summary of different gans and datasets used for validation
a x symbol indicates that the model was evaluated using the corresponding dataset method names cgan mirza and osindero ac gan odena et al
tac gan dash et al
text segan cha et al
gan int cls reed et al
stackgan zhang et al
zhang et al
xu et al
dc gan reed et al
hdgan zhang et al
mirrorgan qiao et al
evaluation datasets mnist coco cub x x x x x x x x x x x x x x x x x x x x x x x x gan based text to image synthesis applications mark and evaluation and comparisons
text to image synthesis applications computer vision applications have strong potential for industries including but not limited to the cal government military entertainment and online social media elds wu et al
nie et al
hong et al
mansimov et al
asmuth et al
fang et al

text to image sis is one such application in computer vision ai that has become the main focus in recent years due to its potential for providing benecial properties and opportunities for a wide range of applicable areas
text to image synthesis is an application byproduct of deep convolutional decoder networks in bination with gans wu et al
reed et al
xu et al

deep convolutional networks have contributed to several breakthroughs in image video speech and audio processing
this learning method intends among other possibilities to help translate sequential text descriptions to images mented by one or many additional methods
algorithms and methods developed in the computer vision eld have allowed researchers in recent years to create realistic images from plain sentences
advances in the computer vision deep convolutional nets and semantic units have shined light and redirected cus to this research area of text to image synthesis having as its prime directive to aid in the generation of compelling images with as much delity to text descriptions as possible
to date models for generating synthetic images from textual natural language in research ratories at universities and private companies have yielded compelling images of owers and birds reed et al

though owers and birds are the most common objects studied thus far research has been applied to other classes as well
for example there have been studies focused solely on human faces wu et al
reed et al
wang et al
yin et al

it s a fascinating time for computer vision ai and deep learning researchers and enthusiasts
the consistent advancement in hardware software and contemporaneous development of computer vision ai research disrupts multiple industries
these advances in technology allow for the extraction of several data types from a variety of sources
for example image data captured from a variety of photo ready devices such as smart phones and online social media services opened the door to the analysis of large amounts of media datasets fang et al

the availability of large media datasets allow new frameworks and algorithms to be proposed and tested on real world data

text to image synthesis benchmark datasets a summary of some reviewed methods and benchmark datasets used for validation is reported in table
in addition the performance of different gans with respect to the benchmark datasets and performance metrics is reported in table
in order to synthesize images from text descriptions many frameworks have taken a minimalistic proach by creating small and background less images mao et al

in most cases the experiments were conducted on simple datasets initially containing images of birds and owers
reed et al
contributed to these data sets by adding corresponding natural language text descriptions to subsets of the cub mscoco and datasets which facilitated the work on text to image synthesis for several papers released more recently
while most deep learning algorithms use mnist lecun and cortes dataset as the mark there are three main datasets that are commonly used for evaluation of proposed gan models for text to image synthesis cub wang et al
oxford nilsback and zisserman coco lin et al
and krizhevsky
cub wang et al
contains birds with matching text descriptions and oxford nilsback and zisserman contains categories of ers with images each and matching text descriptions
these datasets contain individual jects with the text description corresponding to that object making them relatively simple
coco lin et al
is much more complex containing images with different object types
krizhevsky dataset consists of colour images in classes with ages per class
in contrast to cub and oxford whose images each contain an individual object coco s images may contain multiple objects each with a label so there are many labels per image
the total number of labels over the images is
million lin et al


text to image synthesis benchmark evaluation metrics several evaluation metrics are used for judging the images produced by text to image gans
proposed by salimans et al
inception scores is calculates the entropy randomness of the conditional distribution obtained by applying the inception model introduced in szegedy et al
and marginal distribution of a large set of generated images which should be low and high respectively for ful images
low entropy of conditional distribution means that the evaluator is condent that the images came from the data distribution and high entropy of the marginal distribution means that the set of generated images is diverse which are both desired features
the is score is then computed as the divergence between the two entropies
fcn scores isola et al
are computed in a similar manner relying on the intuition that realistic images generated by a gan should be able to be classied correctly by a classier trained on real images of the same distribution
therefore if the fcn classier classies a set of synthetic images accurately the image is probably realistic and the corresponding gan gets a high fcn score
frechet inception distance fid heusel et al
is the other commonly used uation metric and takes a different approach actually comparing the generated images to real images in the distribution
a high fid means there is little relationship between statistics of the synthetic and real images and vice versa so lower fids are better
the performance of different gans with respect to the benchmark datasets and performance metrics is reported in table
in addition figure further lists the performance of gans with respect to their inception scores is
table
a summary of performance of different methods with respect to the three benchmark datasets and four performance metrics inception score is frechet inception distance fid human classier hc and ssim scores
the generative adversarial networks inlcude dcgan gan int cls gan paired d gan stackgan attngan objgan hdgan dm gan tac gan text segan scene graph gan and mirrorgan
the three benchmark datasets include cub oxford and coco datasets
a dash indicates that no data was found
methods is
dcgan
gan int cls dong gan paired d gan
stackgan

obj gan
hdgan
dm gan tac gan text segan scene graph gan mirrorgan
cub datasets metrics coco oxford fid





hc



ssim is











fid






hc



ssim is








fid



hc



ssim
gan based text to image synthesis results comparison while we gathered all the data we could nd on scores for each model on the cub oxford and coco datasets using is fid fcn and human classiers we unfortunately were unable to nd certain data for and hdgan missing in table
the best evaluation we can give for those with missing data is our own opinions by looking at examples of generated images provided in their papers
in this regard we observed that hdgan produced relatively better visual results on the cub and oxford datasets while attngan produced far more impressive results than the rest on the more complex coco dataset
this is evidence that the attentional model and damsm introduced by attngan are very effective in producing high quality images
examples of the best results of birds and plates of vegetables generated by each model are presented in figures and respectively
in terms of inception score is which is the metric that was applied to majority models except dc gan the results in table show that only showed slight improvement over its decessor stackgan for text to image synthesis
however did introduce a very worthy enhancement for unconditional image generation by organizing the generators and discriminators in a tree like structure
this indicates that revising the structures of the discriminators generators can bring a moderate level of improvement in text to image synthesis
in addition the results in table also show that dm gan zhu et al
has the best mance followed by obj gan li et al

notice that both dm gan and obj gan are most recently developed methods in the eld both published in indicating that research in text to image synthesis is continuously improving the results for better visual perception and interception
nical wise dm gan zhu et al
is a model using dynamic memory to rene fuzzy image contents initially generated from the gan networks
a memory writing gate is used for dm gan to select portant text information and generate images based on he selected text accordingly
on the other hand obj gan li et al
focuses on object centered text to image synthesis
the proposed framework of obj gan consists of a layout generation including a bounding box generator and a shape generator and an object driven attentive image generator
the designs and advancement of dm gan and gan indicate that research in text to image synthesis is advancing to put more emphasis on the image details and text semantics for better understanding and perception

notable mentions it is worth noting that although this survey mainly focuses on text to image synthesis there have been other applications of gans in broader image synthesis eld that we found fascinating and worth cub coco oxford s i g a n in t c l s d c g a n d ong g a n paired d g a n stack g a n stack g a n attn g a n o bj g a n h d g a n m a n t a c g a n d scene g raph g a n m irror g a n text se g a n figure
performance comparison between gans with respect to their inception scores is
figure
examples of best images of birds generated by gan int cls stackgan and hdgan
images reprinted from zhang et al
b xu et al
and zhang et al
respectively
ing a small section to
for example yin et al
used sem latent gans to generate images of faces based on facial attributes producing impressive results that at a glance could be mistaken for real faces
xu et al
fang et al
and karpathy and fei fei demonstrated great success in erating text descriptions from images image captioning with great accuracy with xu et al
using an attention based model that automatically learns to focus on salient objects and karpathy and fei fei using deep visual semantic alignments
finally there is a contribution made by that was not mentioned in the dedicated section due to its relation to unconditional image generation as opposed to conditional namely a color regularization term zhang et al

this additional term aims to keep the samples generated from the same input at different stages more consistent in color which resulted in signicantly better results for the unconditional model
conclusion the recent advancement in text to image synthesis research opens the door to several compelling ods and architectures
the main objective of text to image synthesis initially was to create images from figure
examples of best images of a plate of vegetables generated by gan int cls stackgan and hdgan
images reprinted from zhang et al
b xu et al
and zhang et al
respectively
simple labels and this objective later scaled to natural languages
in this paper we reviewed novel methods that generate in our opinion the most visually rich and photo realistic images from text based natural language
these generated images often rely on generative adversarial networks gans deep convolutional decoder networks and multimodal learning methods
in the paper we rst proposed a taxonomy to organize gan based text to image synthesis works into four major groups semantic enhancement gans resolution enhancement gans diversity enhancement gans and motion enhancement gans
the taxonomy provides a clear roadmap to show the motivations architectures and difference of different methods and also outlines their evolution timeline and relationships
following the proposed taxonomy we reviewed important features of each method and their architectures
we indicated the model denition and key contributions from some advanced gan framworks including stackgan attngan dc gan ac gan gan hdgan text segan storygan
many of the solutions surveyed in this paper tackled the highly complex challenge of generating photo realistic images beyond swatch size samples
in other words beyond the work of reed et al
in which images were generated from text in tiny swatches
lastly all methods were evaluated on datasets that included birds owers humans and other miscellaneous elements
we were also able to allocate some important papers that were as impressive as the papers we nally surveyed
though these notable papers have yet to contribute directly or indirectly to the expansion of the vast computer vision ai eld
looking into the future an excellent extension from the works surveyed in this paper would be to give more independence to the several learning ods e

less human intervention involved in the studies as well as increasing the size of the output images
acknowledgements conflict of interest references the authors declare that there is no conict of interest regarding the publication of this article
asmuth j
dixon d
hanna k
hsu s
c
kumar r
paragano v
pope a
samarasekera s
and sawhney h

multimedia applications of computer vision
in proceedings fourth ieee workshop on applications of computer vision
cat
no
princeton nj usa volume doi
acv

pages
baltrusaitis t
ahuja c
and morency l

multimodal machine learning a survey and omy
corr arxiv

bengio y
courville a
and vincent p

representation learning a review and new tives
ieee transactions on pattern analysis and machine intelligence
burt p
and adelson e

the laplacian pyramid as a compact image code
ieee transactions on cha m
gown y
l
and kung h
t

adversarial learning of semantic relevance in text to communications
image synthesis
aaai
cha m
gwon y
and kung h

adversarial nets with perceptual losses for text to image synthesis
in ieee international workshop on machine learning for signal processing mlsp pages
ieee
cha m
gwon y
and kung h
t

adversarial nets with perceptual losses for text to image synthesis
in ieee international workshop on machine learning for signal processing mlsp tokyo japan pages
cha m
gwon y
l
and kung h
t

adversarial learning of semantic relevance in text to image synthesis
in proceedings of the association for the advancement of articial intelligence aaai
chen j
shen y
gao j
liu j
and liu x

language based image editing with recurrent attentive models
in proc
of the eee cvf conference on computer vision and pattern recognition
chen y
lai y

and liu y


cartoongan generative adversarial networks for photo cartoonization
in ieee cvf conference on computer vision and pattern recognition cvpr lake salt city usa
creswell a
white t
dumoulin v
arulkumaran k
sengupta b
and bharath a

tive adversarial networks an overview
ieee signal processing magazine pages
dash a
gamboa j
c
b
ahmed s
afzal m
z
and liwicki m

tac gan text conditioned auxiliary classier generative adversarial network
corr arxiv

dash a
gamboa j
c
b
ahmed s
liwicki m
and afzal m
z

tac gan text conditioned auxiliary classier generative adversarial network
arxiv preprint

denton e
chintala s
szlam a
and fergus r

deep generative image models using a laplacian pyramid of adversarial networks
corr arxiv

denton e
l
chintala s
szlam a
and fergus r

deep generative image models using a laplacian pyramid of adversarial networks
in cortes c
lawrence n
d
lee d
d
sugiyama m
and garnett r
editors advances in neural information processing systems pages
curran associates inc
dong h
yu s
wu c
and guo y

semantic image synthesis via adversarial learning
in proceedings of the ieee international conference on computer vision pages
dong h
zhang j
mcilwraith d
and guo y

learning text to image synthesis with textual data augmentation
in ieee international conference on image processing icip beijing china pages
durugkar i
gemp i
and mahadevan s

generative multi adversarial networks
elgammal a
liu b
elhoseiny m
and mazzone m

can creative adversarial networks generating art by learning about styles and deviating from style norms
corr arxiv

in ieee international conference on image processing icip athens greece volume doi
icip

pages
image captioning with word level attention
fang f
wang h
and tang p

gao l
chen d
song j
xu x
zhang d
and shen h
t

perceptual pyramid ial networks for text to image synthesis
in proceedings of the association for the advancement of articial intelligence aaai
goodfellow i
j
pouget abadie j
mirza m
xu b
d
warde farley ozair s
courville a
and bengio y

generative adversarial networks
in proceedings of nips

han h
li y
and zhu x

convolutional neural network learning for generic data classication
information sciences
haynes m
norton a
mcparland a
and cooper r

speech to text for broadcasters from research to implementation
smpte motion imaging journal
heusel m
ramsauer h
unterthiner t
nessler b
and hochreiter s

gans trained by a two time scale update rule converge to a local nash equilibrium
corr arxiv

hong s
yang d
choi j
and lee h

inferring semantic layout for hierarchical text to image synthesis
corr arxiv

hong y
hwang u
yoo j
and yoon s

how generative adversarial networks and their variants work an overview
acm computing surveys csur
huang h
yu p
and wang c

an introduction to image synthesis with generative adversarial nets
corr arxiv

huang x
li y
poursaeed o
hopcroft j
and belongie s

stacked generative adversarial networks
in ieee conference on computer vision and pattern recognition pages
isola p
zhu j
zhou t
and efros a

image to image translation with conditional adversarial networks
corr arxiv

johnson j
alahi a
and fei fei l

perceptual losses for real time style transfer and resolution
in european conference on computer vision pages
springer
johnson j
gupta a
and fei fei l

image generation from scene graphs
in proceedings of the cvpr
karpathy a
and fei fei l

deep visual semantic alignments for generating image descriptions
ieee transactions on pattern analysis and machine intelligence
krizhevsky a

master s thesis dept
of computer science univ
of toronto
kumar r
sotelo j
kumar k
brebisson a
and bengio y

obamanet photo realistic lip sync from text
arxiv preprint

lecun y
and cortes c

mnist handwritten digit database
li c
su y
and liu w

text to text generative adversarial networks
in international joint conference on neural networks ijcnn rio de janeiro pages
li c
wang z
and qi h

fast converging conditional generative adversarial networks for in ieee international conference on image processing icip athens greece image synthesis
pages
li w
zhang p
zhang l
huang q
he x
lyu s
and gao j

object driven text image synthesis via adversarial training
corr

li y
gan z
shen y
liu j
cheng y
wu y
carin l
carlson d
and gao j

gan a sequential conditional gan for story visualization
in proceedings of the ieee conference on computer vision and pattern recognition pages
li y
min m
r
shen d
carlson d
and carin l

video generation from text
in second aaai conference on articial intelligence
lin t
maire m
belongie s
bourdev l
girshick r
hays j
perona p
ramanan d
zitnick c
and dollar p

microsoft coco common objects in context
corr arxiv

liu x
meng g
xiang s
and pan c

semantic image synthesis via conditional in international conference on pattern recognition icpr generative adversarial networks
beijing china pages
mansimov e
parisotto e
ba j
l
and salakhutdinov r

generating images from captions with attention
corr arxiv

mao x
li q
xie h
lau r
y
k
wang z
and smolley s
p

least squares generative adversarial networks
in ieee international conference on computer vision iccv venice pages
mirza m
and osindero s

conditional generative adversarial nets
corr arxiv

arxiv preprint mirza m
and osindero s

conditional generative adversarial nets
ngiam j
khosla a
kim m
nam j
lee h
and ng a
y

multimodal deep learning
in proceedings of the international conference on international conference on machine learning omnipress usa pages
nguyen t
d
le t
vu h
and phung d

dual discriminator generative adversarial nets
in

proc
of nips
nie d
trullo r
petitjean c
ruan s
and shen d

medical image synthesis with aware generative adversarial networks
corr arxiv

nilsback m
and zisserman a

automated ower classication over a large number of classes
in proceedings of the indian conference on computer vision graphics and image processing
odena a
olah c
and shlens j

conditional image synthesis with auxiliary classier gans
corr arxiv

odena a
olah c
and shlens j

conditional image synthesis with auxiliary classier gans
in proceedings of the international conference on machine learning volume pages
jmlr
org
park h
yoo y
and kwak n

mc gan multi conditional generative adversarial network for image synthesis
arxiv preprint

qiao t
zhang j
xu d
and tao d

mirrorgan learning text to image generation by redescription
corr

radford a
metz l
and chintala s

unsupervised representation learning with deep lutional generative adversarial networks
corr arxiv

reed s
akata z
mohan s
tenka s
schiele b
and lee h

learning what and where to draw
in proc
of nips international conference
reed s
akata z
yan x
logeswaran l
schiele b
and lee h

generative adversarial text to image synthesis
proceedings of the international conference on machine learning icml
salimans t
goodfellow i
zaremba w
cheung v
radford a
and chen x

improved techniques for training gans
corr arxiv

schuster m
and paliwal k

bidirectional recurrent neural networks
ieee transactions on signal processing
stoll s
camgoz n
c
hadeld s
and bowden r

sign language production using neural machine translation and generative adversarial networks
in bmvc page
szegedy c
vanhoucke v
ioffe s
and shlens j

rethinking the inception architecture for computer vision
in ieee conference on computer vision and pattern recognition pages
vo d
m
and sugimoto a

paired d gan for semantic image synthesis
in asian conference on computer vision pages
springer
wang k
gou c
duan y
lin y
zheng x
and wang f

the caltech ucsd dataset
computation and neural systems technical report cns
wang k
gou c
duan y
lin y
zheng x
and wang f

generative adversarial networks introduction and outlook
ieee caa journal of automatica sinica
wang k
and wan x

sentigan generating sentimental texts via mixture adversarial networks
in proceedings of the twenty seventh international joint conference on articial intelligence
wang x
and gupta a

generative image modeling using style and structure adversarial works
corr arxiv

wang y
chang l
cheng y
jin l
and cheng z

learning face sketch from facial attribute text
in ieee international conference on image processing icip athens greece pages
wu x
xu k
and hall p

a survey of image synthesis and editing with generative adversarial networks
tsinghua science and technology
xu k
ba j
kiros r
cho k
courville a
salakhutdinov r
zemel r
and bengio y

show attend and tell neural image caption generation with visual attention
corr arxiv

xu t
zhang p
huang q
zhang h
gan z
huang x
and he x

attngan fine grained text to image generation with attentional generative adversarial networks
corr arxiv

yan c
yang j
sohn k
and lee h

conditional image generation from visual attributes
in in leibe b
matas j
sebe n
welling m
eds computer vision eccv
eccv
lecture notes in computer science springer cham
volume
yan z
zhang h
wang b
paris s
and yu y

automatic photo adjustment using deep neural networks
acm transactions on graphics tog
yang j
kannan a
batra d
and parikh d

lr gan layered recursive generative adversarial networks for image generation
corr arxiv

yang m
zhao w
xu w
feng y
zhao z
chen x
and lei k

multitask learning for cross domain image captioning
ieee transactions on multimedia
yin w
fu y
sigaly l
and xue x

semi latent gan learning to generate and modify facial images from attributes
corr arxiv

zhang d
yin j
zhu x
and zhang c

network representation learning a survey
ieee transactions on big data doi
tbdata


zhang g
tu e
and cui d

stable and improved generative adversarial nets gans a in proceedings of the international conference on image processing pages constructive survey

zhang h
xu t
li h
zhang s
wang x
huang x
and metaxas d

realistic image synthesis with stacked generative adversarial networks
ieee transactions on pattern analysis and machine intelligence

zhang h
xu t
li h
zhang s
wang z
huang x
and metaxas d

stackgan text to photo realistic image synthesis with stacked generative adversarial networks
in ieee international conference on computer vision iccv venice pages
zhang s
zhai j
luo d
zhan y
and chen j

recent advance on generative adversarial networks
in proceedings of the international conference on machine learning and cybernetics pages
zhang z
xie y
and yang l

photographic text to image synthesis with a nested adversarial network
corr arxiv

zhu m
pan p
chen w
and yang y

dm gan dynamic memory generative adversarial networks for text to image synthesis
in proceedings of the ieee conference on computer vision and pattern recognition pages
zhu x
goldberg a
eldawy m
dyer c
and strock b

a text to picture synthesis system for augmenting communication
in prof
of aaai international conference pages

