automatic generation of chinese short product titles for mobile display yu xusheng kenny q
wenwu zhao lu algorithm team alibaba group jiao tong university intelligence department zhejiang cainiao supply chain management co
ltd
gongyu
gy inc
com
sjtu
edu
cn santong

com lizhao
inc
com duanlu

com a m l c
s c v
v i x r a abstract this paper studies the problem of automatically extracting a short title from a manually written longer description of commerce products for display on mobile devices
it is a new extractive summarization problem on short text inputs for which we propose a feature enriched network model bining three different categories of features in parallel
imental results show that our framework signicantly forms several baselines by a substantial gain of

over we produce an extractive summarization dataset for commerce short texts and will release it to the research munity
introduction mobile internet is fast becoming the primary venue for commerce
people have got used to browsing through tions of products and making transactions on the relatively small mobile phone screens
all major e commerce giants such as amazon ebay and taobao offer mobile apps that are poised to supersede the conventional websites
when a product is featured on an e commerce website or mobile app it is often associated with a textual title which describes the key characteristics of the product
these titles written by merchants often contain gory details so as to maximize the chances of being retrieved by user search queries
therefore such titles are often verbose over informative and hardly readable
while this is okay for display on a computer s web browser it becomes a problem when such longish titles are played on mobile apps
take figure as an example
the title for a red sweater on an e commerce mobile app is
due to the limited display space on mobile phones original long titles usually more than characters will be cut off leaving only the rst several characters one more summer woman


on the screen which is completely hensible unless the user clicks on the product and load the detailed product page
copyright association for the advancement of articial intelligence www
aaai
org
all rights reserved
equal contribution
figure a cut off long title on an e commerce mobile app vs
a corresponding short title
thus in order to properly display product listing on a bile screen one has to signicantly simplify e

to der characters the long titles while keeping the most important information
this way user only has to glance through the search result page to make quick decision whether they want to click into a particular product
figure also shows as comparison an alternate display of a ened title for the same product
the short title in the left which means snapshot is
in this paper we attempt to extract short titles from their longer more verbose counterparts for e commerce ucts
to the best of our knowledge this is the rst attempt that attacks the e commerce product short title extraction problem
this problem is related to text summarization which erates a summary by either extracting or abstracting words or sentences from the input text
existing summarization methods have primarily been applied to news or other long short
cut o long title one more long title one more m pq

documents which may contain irrelevant information
thus the goal of traditional summarization is to identify the most essential information in the input and condense it into thing as uent and readable as possible
we would attack our problem with an extractive rization approach rather than an abstractive one for these reasons
first our input title is relatively shorter and tains less noise average characters see table
some words in the long title may not be important but they are all relevant to the product
thus it is sufcient to decide if each word should or should not stay in the summary
second the number of words in the output is strictly constrained in our problem due to size of the display
generative abstractive approaches do not perform as well when there s such a straint
finally for e commerce it is better for the words in the summary to come from the original title
using ent words may lead to a change of original intention of the merchant
state of the art neural summarization models cheng and lapata narayan et al
are generally based on attentional rnn frameworks and have been applied on news or wiki like articles
however in e commerce customers are not so sensitive to the order of the words in a product title
besides using deep rnn with attention mechanism to encode word sequence we believe other single word level semantic features such as ner tags and tf idf scores will be as just as useful and should be given more weights in the model
in this paper we propose a feature enriched neural network model which is not only deep but also wide aiming to effectively shorten original long titles
the contributions of this paper are summarized below we collect and will open source a product title summary dataset section
we present a novel feature enriched network model bining three different types of word level features tion and the results show the model outperforms eral strong baseline methods with rou score of
section

by deploying the framework on an e commerce bile app we witnessed improved online sales and better turnover conversion rate in the popular shopping season section

data collection publicly available large scale summarization dataset is rare
existing document summarization datasets include and for english and for chinese
in this work we create a dataset on short title extraction for e commerce products
this dataset comes from a module in taobao named youhaohuo
youhaohuo is a collection of high quality products on taobao
if you click a product
nist
gov data
html
nist
gov
nist

hitsz
edu
cn article
html
m
taobao
com lanlan index
html in youhaohuo you will be redirected to the detailed product page including product title
what is different from nary taobao products is that online merchants are required to submit a short title for each youhaohuo product
this short title written by humans is readable and describes the key properties of the product
furthermore most of these short titles are directly extracted from the original product titles
thus we believe youhaohuo is a good data source of extractive summarization for product descriptions
figure procedure of data collection in youhaohuo
figure shows how we collected the data
on the left is a web page in youhaohuo displaying several products each of which contains an image and a short title below
when clicking on the bottom right dress we jump to the detailed page on the right
the title next to the picture in red box is the manually written short title which says miuco tight dress with knit vest
this short title is extracted from the long title below in the blue box
notice that all the characters in the short tile are directly extracted from the long title red boxes inside blue box
in addition to the characters in the short title the long title also contains extra information such as woman s wear brand new in winter
in this work we segment the original long titles and short titles into chinese words by
the dataset consists of pairs of original and short product titles which is the largest short text marization dataset to date
we call it large extractive mary dataset for e commerce whose statistics is shown in table
we believe this dataset will contribute to the future research of short text
no
of summaries no
of words per text no
of chars per text no
of words per summary no
of chars per summary table statistics of dataset

python pypi data can be found at
pangolulu product short title
notice all the original data can be crawled online
problem denition in this section we formally dene the problem of short title extraction
a char is a single chinese or english character
a segmented word or term is a sequence of several chars such as nike or jean
a product title denoted as x is a sequence of words


xn
let y be a quence of labels


over x where yi
the corresponding short title is a subsequence of x denoted as s xi where yi and n
we regard short title extraction task as a sequence sication problem
each word is sequentially visited in the original product title order and a binary decision is made
we do this by scoring each word xi within x and ing a label yi indicating whether the word should or should not be included in the short title s
as we apply supervised training the objective is to maximize the hood of all word labels y


given the input product title x and model parameters log log
n feature enriched neural extractive model in this section we describe our extractive model for product short title extraction
the overall architecture of our ral network based extractive model is shown in figure
basically we use a recurrent neural network rnn as the main building block of the sequential classier
ever unlike traditional rnn based sequence labeling els used in ner or pos tagging where all the word level features are fed into rnn cell we instead divide the tures into three parts namely content attention and tic respectively
finally we combine all three features in an ensemble
figure architecture of feature enriched neural tive model

content feature to encode the product title we rst look up an ding matrix ex rdv to get the word embeddings x


xn
here denotes the dimension of the beddings and v denotes the vocabulary size of natural guage words
then the embeddings are fed into a rectional lstm networks
to this end we get two hidden


hn from the forward network state sequences and hn from the backward network
we


catenate the forward hidden state of each word with sponding backward hidden state resulting in a tion hi
at this point we obtain the representation of the product title x
hi as the content feature of current word xi is then calculated hi hi bcont where wcont and bcont are model parameters
cont wt cont
attention feature in order to measure the importance of each word relevant to the whole product title we borrow the idea of attention mechanism bahdanau cho and bengio to calculate a relevance score between the hidden vector of current word and representation of the entire title sequence
the representation of the entire product title is modeled as a non linear transformation of the average pooling of the concatenated hidden states of the bilstm tanh wd n n hi hi bd
therefore the attention feature of current word xi is culated by a bilinear combination function as att dt watt hi where watt is a parameter matrix
hi batt
semantic feature apart from the two hidden features calculated using an rnn encoder we design another kind of feature including idf and ner tag to capture the deep semantics of each word in a product title
tf idf tf idf short for term frequencyinverse document frequency is a numerical statistic that is intended to reect how important a word is to a document in corpus or a tence in a document
a simple choice to calculate term frequency of current word xi is to use the number of its occurrences or count in the title in the case of inverse document frequency we calculate it as x cntxi n
n cxi where n is the number of product titles in the corpus and cxi is the number of titles containing the word xi
by combining the above two the tf idf score of word xi in a product title x denoted as tf x is then calculated as the product of x and
we design a feature vector containing three values tf score idf score and tf idf score and the calculate a third ture vtf idf x tf x tf idf wt tf idf vtf idf btf idf
bi lstmembeddingproduct idfner idfwner ner we use a specialized ner tool for e commerce to label entities in a product title
in total there are types of entities which are of common interest in e commerce scenario such as color style and size
for example in segmented product title nike red sweatpants with free shipping free shipping is labeled as marketing service nike is labeled as brand red is labeled as color and sweatpants is labeled as egory
we use one hot representation to encode ner feature vner of each word and then integrate it into the model by proposing a fourth feature ner wt nervner bner

ensemble we combine all the features above into one nal score of word xi scorei cont att idf ner bf where is the sigmoid or logistic function which restrain the score between and
based on that we set a threshold to decide whether we keep word xi in the short title or not
our model is very much like the wide deep model chitecture cheng et al

while the content and tion features are deep since they rely on deep rnn structure the semantic features are relatively wide and linear
experiments in this section we rst introduce the experimental setup and the previous state of the art systems as comparison to our own model known as feature enriched net
we then show the implementation details and the evaluation results before giving some discussions on the results

training and testing data we randomly select product titles as our training data and another for testing
each product title x is annotated with a sequence of binary label y i
e
each word xi is labeled with included in short title or not included in short title
readers may refer to section for the details about how we collect the product titles and their corresponding short titles

baseline systems since there are no previous work that directly solves the short title extraction for e commerce product we select our baselines from three categories
the rst one is tional methods
we choose a keyword extraction framework known as textrank mihalcea and tarau
it rst fers an importance score for each word within the long title by an algorithm similar to pagerank then decides whether each word should or should not be kept in the short title cording to the scores
the second category is standard sequence labeling tems
we choose the system mentioned by huang et al
in which a multi layer bilstm is used
pared to our system it does not exploit the attention anism and any side feature information
we substitute the conditional random field crf layer with logistic gression to make it compatible with our binary labeling problem
we call this system bilstm net
the last category of methods is attention based works which use encoder decoder architecture with tion mechanism
we choose pointer network vinyals tunato and jaitly as a comparison and call it net
during decoding it looks at the whole sentence culates the attentional distribution and then makes decisions based on the attentional probabilities
implementation details
we pre train word embeddings used in our model on the whole product titles data plus an extra corpus called commerce product recommended reason which is ten by online merchants and is also extracted from huo section
we use the mikolov et al
cbow model with context window size negative pling size iteration steps and hierarchical softmax
the size of pre trained word embeddings is set to
for of vocabulary oov words embeddings are initialized as zero
all embeddings are updated during training
for current neural network component in our system we used a two layers lstm network with unit size
all uct titles are padded to a maximum sentence length of
we perform a mini batch cross entropy loss training with a batch size of sentences for training epochs
we use adam optimizer and the learning rate is initialized with


ofine evaluation to evaluate the quality of automatically extracted short tles we used rouge lin and hovy to compare model generated short titles to manually written short titles
in this paper we only report mainly because guistic uency and word order are not of concern in this task
unlike previous works in which rouge is a oriented metric we jointly consider precision recall and score since recall only presents the ratio of the number of extracted words included in ground truth short title over the total number of words in ground truth short title
however due to the limited display space on mobile phones the ber of words or characters of extracted short title itself should be constrained as well
thus precision is also sured in our experiments
and rou is considered a comprehensive evaluation metric rou gep rou ger rou gef gep rou ger rou gep rou ger
where shuman is the manually written short title and is the number of overlapping words appearing in short titles generated by model and humans
final results on the test set are shown in table

we can nd that our method feature enriched net forms the baselines on both precision and recall
our method achieves the best
score which improves by a ative gain of

pointer net would achieve higher cision score for its attentive ability to the whole sentence and select the most important words
our method considers long short memories attention mechanism and other dant semantic features
that is to say our model has the ity to extract the most essential words from original long titles and make the short titles more accurate and hensive
we also tune the threshold used in bilstm net and our feature enriched net
this threshold indicates how large the predicted likelihood should be so that a word will be included in the nal short title
we reported the results in figure in which we set as

and

from the ures we can conclude that our model stably performs better than the other
models textrank bilstm net pointer net feature enriched net











table final results on the test set
we report precision recall and corresponding
we use the tuned threshold
see figure for bilstm net and feature enriched net
best rouge score in each column is highlighted in boldface
figure ofine results of bi lstm net and enriched net under different thresholds online a b ing of sales volume and turnover conversion rate
online a b testing this subsection presents the results of online evaluation in the search result page scenario of an e commerce mobile app with a standard a b testing conguration
due to the limited display space on mobile phones only a xed number of chars characters in this app can be shown out and excessive part will be cut off
therefore unlike previously mentioned inference approach with threshold in section
we regard it as classic knapsack problem
each word xi in the product title is an item with weight and value scorei where represents the char length of word xi and scorei represents the predicted likelihood of word by our model
the maximum weight capacity of our knapsack also known as char length limit is m
then the target is max scoreizi s
t
m zi n n where zi means xi should be reserved in the short title
similar to the standard solution to knapsack problem we use a dynamic programming dp algorithm
in our online a b testing evaluation of the users were randomly selected as testing group about
million user views uv in which we substituted the original cut off long title displayed to users with extracted short titles by our model with dp inference
we claim that after showing the short titles with most important keywords users have much better idea what the product is about on the search result page and thus nd the product they want more easily
we deployed a b testing for days from to and achieved on average
and
ments of sales volume and turnover conversion rate see ure for each day
this clearly shows that better short uct titles are more user friendly and hence improve the sales substantially

discussions in table
we show a real case of original long title along with short title annotated by human beings predicted by bilstm net and feature enriched net respectively
from the human annotated short title we nd that a proper short title should contain the most important elements of the uct such as category dress and description of ties boat neckline and off sleeves
while some other ments such as brand terms xunruo twin designer brand or service terms open for reservation should not be kept in the short title
our feature enriched net has the ability to generate a satisfying short title while baseline model tends to miss some essential information
leather clothing furthermore in order to visually see the performance of our feature enriched net model we also carried out a feature analysis
in the following example see ure leather jacket jacket are all the category words named entity type as category but the leather clothing and leather snake and jacket have higher attention scores
europe and america both belong to modiers of the product but the idf score of snake is much higher than europe and america
by introducing bilstm the quential model can learn that print form a bigram pattern and they should appear together so as motorcycle and leather clothing
snake and however there is still room for improvement
terms with similar meaning may co occur in the short title generated by our model when they all happen to be important terms both such as a category
for example mean jacket in a long title and the model tends to keep both of them
however only one of them is enough and the and












precisionbi lstm netfeature enriched net












recallbi lstm netfeature enriched net












lstm netfeature enriched net




in novemberonline a b testingsales volumeturnover conversion rate original human feature enriched net bilstm net bookable xunruo twin designer brand bubble series dress with boat neckline and off sleeves
dress with boat neckline and off sleeves
bubble series dress with boat neckline and off sleeves
bookable xunruo brand dress with off sleeves
table a real experimental case with chars length limit
pointer net is not included since encoder decoder architecture ca nt directly adapt to character length limit figure an example of product long title ground truth short title with red boxes and model predicted short title with blue boxes
means the named entity type
means a higher score
means a lower score
space saved can be used to display other useful information to customers
we will explore intra attention paulus xiong and socher as an extra feature in our future work
related work our work can be comfortably set in the area of short text summarization
this line of research is in fact essentially sentence compression working on short text inputs such as tweets microblogs or single sentence
recent advances ically contribute to improving seq to seq learning or tentional rnn encoder decoder structures nallapati et al

while these methods are mostly abstractive we use an extractive framework combining with deep attentional rnn network and explicit semantic features due to ent scenarios of summarization problem
the most related work is wang et al
since they also try to compress product title in e commerce
they use user search log as an external knowledge to guide the model and regard short title as some kind of query to serve the purpose of improving line business values
differently user search log is not essary in our model and our goal is to make the short title as real as if it is written by human
conclusion to the best of our knowledge this is the rst piece of work that focuses on extractive summarization for e commerce product title
we propose a deep and wide model ing attentional rnn framework with rich semantic features such as tf idf scores and ner tags
our model forms several popular summarization models and achieves score of

besides the result of line a b testing shows substantial benets of our model in real online shopping scenario
possible future works include handling similar terms that appear in the short titles ated by our model
references bahdanau cho and bengio bahdanau d
cho k
and bengio y

neural machine translation by jointly learning to align and translate
arxiv
cheng and lapata cheng j
and lapata m

neural summarization by extracting sentences and words
arxiv
cheng et al
cheng h

koc l
harmsen j
shaked t
chandra t
aradhye h
anderson g
rado g
chai w
ispir m
et al

wide deep in proceedings of the learning for recommender systems
workshop on deep learning for recommender systems
acm
huang xu and yu huang z
xu w
and yu k

bidirectional lstm crf models for sequence tagging
arxiv
lin and hovy lin c

and hovy e

matic evaluation of summaries using n gram co occurrence statistics
in naacl hlt volume
acl
mihalcea and tarau mihalcea r
and tarau p

textrank bringing order into text
in emnlp
mikolov et al
mikolov t
sutskever i
chen k
corrado g
s
and dean j

distributed tations of words and phrases and their compositionality
in nips
nallapati et al
nallapati r
zhou b
gulcehre c
xiang b
et al

abstractive text summarization using sequence to sequence rnns and beyond
arxiv
narayan et al
narayan s
papasarantopoulos n
lapata m
and cohen s
b

neural extractive marization with side information
arxiv
paulus xiong and socher paulus r
xiong c
and socher r

a deep reinforced model for abstractive summarization
arxiv
vinyals fortunato and jaitly vinyals o
fortunato
m
and jaitly n

pointer networks
in nips
wang et al
wang j
tian j
qiu l
li s
lang j
si l
and lan m

a multi task learning approach for improving product title compression with user search log data
in aaai

