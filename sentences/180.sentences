abstractive text summarization by incorporating reader comments shen xiuying piji zhaochun lidong dongyan rui of computer science and technology peking university beijing china for data science peking university beijing china ai lab shenzhen china
com beijing china center singapore machine intelligence technology alibaba damo academy shengao xy chen zhaody
edu
cn
com
com l
inc
com c e d l c
s c v
v i x r a abstract in neural abstractive summarization eld conventional sequence to sequence based models often suffer from marizing the wrong aspect of the document with respect to the main aspect
to tackle this problem we propose the task of reader aware abstractive summary generation which lizes the reader comments to help the model produce better summary about the main aspect
unlike traditional abstractive summarization task reader aware summarization confronts two main challenges comments are informal and noisy jointly modeling the news document and the reader ments is challenging
to tackle the above challenges we sign an adversarial learning model named reader aware mary generator rasg which consists of four components a sequence to sequence based summary generator a reader attention module capturing the reader focused aspects a supervisor modeling the semantic gap between the erated summary and reader focused aspects a goal tracker producing the goal for each generation step
the supervisor and the goal tacker are used to guide the training of our work in an adversarial manner
extensive experiments are conducted on our large scale real world text summarization dataset and the results show that rasg achieves the of the art performance in terms of both automatic metrics and human evaluations
the experimental results also strate the effectiveness of each module in our framework
we release our large scale dataset for further
introduction abstractive summarization can be regarded as a sequence mapping task that the source text is mapped to the target summary and has drawn much attention since the deep ral networks are widely applied in natural language ing eld
recently sequence to sequence work sutskever vinyals and le has been proved fective for the task of abstractive summarization chopra auli and rush see liu and manning and other text generation tasks tao et al
gao et al

in this paper we use aspect to denote the topic described in a specic paragraph or a sentence of a news document and use main aspect to denote the central corresponding author rui yan
edu
cn copyright association for the advancement of articial intelligence www
aaai
org
all rights reserved

cn table examples of the text summarization
the text in red denotes the focused aspect by the good summary while the text in blue is described by the bad summary
the text with underline is the focused aspect by reader comments
document comments good summary bad summary on august according to a person familiar with the matter toyota motor corporation will invest million u
s
lars into the uber a taxi service company with a valuation of up to billion u
s
dollars
the investment will focus on driverless car technology
however its development path is not smooth
in march of this year a uber driverless car hit a woman and caused her death
in last year softbank also invested into uber with a valuation of billion
toyota s investment in uber is a wise choice
million investment is really a lot of money toyota invests million into uber with a valuation of billion an uber driverless car hits a passerby to death topic which the author tends to convey to the readers
though a document may describe an event in many ent aspects the summary of this document should always focus on the main aspect
as shown in table the good summary describes the main aspect and the bad summary describes another trivial aspect that is not the main point of the document
to focus on the main aspect some marization methods sun et al
zhou et al
bansal and chen rst select several sentences about the main aspect and then generate the summary
however it is very challenging to discover which is the main aspect of the news document
nowadays a great number of news comments are erated by readers to express their opinions about the event
some comments may mention the main aspect of the ument for several times
take the case in table as an ample the focused aspect of the reader is investment of toyota which is also the main aspect of this document
to be specic we dene reader focused aspect to denote the focused aspect by a reader through the comments
itively these reader comments may help the summary erator capture the main aspect of document thereby ing the quality of the generated summary
therefore in this paper we investigate a new problem setting of the task of abstractive text summarization
we name such paradigm of extension as reader aware abstractive text summarization
the effect of comments or social contexts in ment summarization have been explored by several vious works hu sun and lim yang et al
li et al
li bing and lam
unlike these approaches that directly extract sentences from the nal document hu sun and lim yang et al
li et al
we aim to generate a natural sounding mary from scratch instead of extracting words from the ument
generally existing text summarization approaches front two challenges when addressing reader aware rization task
the rst challenge is that reader comments are very noisy and informative
not all the information provided by the comments is useful when modeling the reader focused aspects
therefore it is crucial to make the model own the ability of capturing main aspect and ltering noisy mation when incorporating reader comments
the second challenge is how to generate summaries by jointly modeling the main aspect of document and the reader focused aspect revealed by comments
meanwhile the model should not be sensitive to the diverse unimportant aspects introduced by some reader comments
thus simply absorbing all the reader aspect information to directly guide the model to erate summary is not feasible as it will make the generator lose the ability of modeling the main aspect
in this paper we propose a summarization framework named reader aware summary generator rasg that corporates reader comments to improve the summarization performance
specically a architecture with tion mechanism is employed as the basic summary ator
we rst calculate alignment between the reader ments words and document words and this alignment formation is regarded as reader attention representing the reader focused aspect
then we treat the decoder tion weights as the focused aspect of the generated summary a

a
decoder focused aspect
after each decoding step a supervisor is designed to measure the distance between the reader focused aspect and the decoder focused aspect
given this distance a goal tracker provides the goal to the decoder to induce it to reduce this distance
the training of our framework rasg is conducted in an adversarial way
to evaluate the performance of our model we collect a large amount of document summary pairs associated with several reader comments from social media website
extensive periments conducted on this dataset show that rasg icantly outperforms the state of the art baselines in terms of rouge metrics and human evaluations
to sum up our contributions can be summarized as lows we propose a reader aware abstractive text tion task
to solve this task we propose an end to end ing framework to conduct the reader attention modeling and reader aware summary generation
we design a supervisor as well as a goal tracker to guide the generator to focus on the main aspect of the document
to reduce the noisy information introduced by the reader comments we propose a denoising module to identify which comments are helpful for summary generation matically
we release a large scale abstractive text summarization dataset associated with reader comments
experimental sults on this dataset demonstrate the effectiveness of our posed framework
related work text summarization can be classied into extractive and stractive methods
extractive methods jadhav and rajan narayan cohen and lapata read the article and get the representations of the sentences and article to lect sentences
however summaries generated by extractive methods always suffer from redundancy problem
recently with the emergence of neural network models for text eration a vast majority of the literature on summarization is dedicated to abstractive summarization bansal and chen ma et al
zhou et al

on the text marization benchmark dataset cnn dailymail the state the art abstractive methods outperform the best extractive method in terms of rouge score
most methods for stractive text summarization are based on the sequence sequence model sutskever vinyals and le which encodes the source texts into the semantic representation with an encoder and generates the summaries from the resentation with a decoder
to tackle the out of vocabulary problem some researchers employ the copy mechanism to copy some words from the input document to summary gu et al
see liu and manning
to capture the main aspect of document chen et al
propose to lect salient sentences and then rewrite these sentences to a concise summary
this approach achieves the state of art of text summarization on cnn dailymail benchmark dataset
unlike document summarization that needs to code a long text social media summarization usually reads short and noisy text and has become a popular task these days
after hu et al
propose a short text tion dataset on social media and many researchers follow this task
lin et al
propose a based model which uses an cnn to rene the representation of source context
wang et al
use convolutional model to summarize text and use the policy gradient algorithm to directly optimize the rouge score
however these marization models do not utilize the reader s comments in generating summaries
to consider the reader s comments into text tion the reader aware summarization is proposed and it mainly takes the form of extractive approaches
graph based method has been used for comment oriented tion task such as hu sun and lim where they identify three relations topic quotation and tion by which comments can be linked to one another
recently nguyen et al
publish a small extractive sentence comment dataset which can not be used to train neural models due to its small size
li et al
pose an unsupervised compressive multi document rization model using sparse coding method
following vious work there are some models li bing and lam li et al
using variational auto encoder to model the latent semantic of original article and reader comments
ferent from our abstractive summarization task these related works are all based on extractive or compressive approaches
xd problem formulation before presenting our approach for the reader aware marization we rst introduce our notations and key cepts
xc


xd to begin with for a document x d xd t d we assume there is a comment set x c


ct c is the i th comment xd


xc where ci xc i denotes the i th word in document x d and xc i j denotes the j th word in i th comment sentence ci
given the ument x d the summary generator reads the comments x c then generates a summary y


yt y
finally we use the difference between generated summary y and ground truth summary y as the training signal to optimize the model parameters
i t c i the proposed rasg model overview in this section we propose our reader aware summary erator abbreviated as rasg
the overview of rasg is shown in figure which can be split into four main parts summary generator is a based architecture with attention and copy mechanisms
reader attention module learns a semantic alignment between each word in document and comments thus tures the reader focused aspect
supervisor measures the semantic gap between decoder focused aspect and reader focused aspect
there is also a criminator which uses convolutional neural network to tract features and then distinguishes how similar is decoder focused aspect to reader focused aspect
goal tracker utilizes the semantic gap learned by visor and the features extracted learned by the tor to set a goal which is further utilized as a more specic guidance for summary generator to produce better summary
summary generator at the beginning we use an embedding matrix e to map hot representation of each word in the document x and comments x c to a high dimensional vector space
we note as the embedding representation of word
from these embedding representations we employ a bi directional recurrent neural network bi rnn to model the temporal interactions between words bi hd t hd where hd t denotes the hidden state of t th step in bi rnn for document x d
we denote the nal hidden state hd t d of bi rnnd as the vector representation of the document x d
following see liu and manning ma et al
we choose the long short term memory lstm as the rnn cell
then we apply a linear transform layer on the input t d and use the output of this ument vector representation hd layer as the initial state of decoder lstm shown in tion
in order to reduce the burden of compressing ment information into initial state we use the attention mechanism bahdanau cho and bengio to rize the input document into context vector cally and we will show the detail of these in the following sections
we then concatenate the context vector with the embedding of previous step output and feed this into decoder lstm shown in equation
we use the notion as the concatenation of two vectors
wdhd st lstm
t d bd at t th decoding step we use the decoder state to attend to each the document states hd and resulting in the attention distribution t rt d shown in equation
then we use the attention distribution t to weighted sum the document states as the context vector
a tanh whhd i d exp t i t j t w t i exp d t ihd i
finally an output projection layer is applied to get the nal generating distribution pv over vocabulary as shown in equation
we concatenate goal vector gt gap content dt and the output of decoder lstm st as the input of the put projection layer
the goal vector gt represents the goal of current generation step the gap content dt denotes the mantic gap between generated summary and reader focused document and we will show the details of these variables in the following sections
pv softmax gt bv in order to handle the out of vocabulary oov problem we equip the pointer network gu et al
vinyals nato and jaitly see liu and manning with our decoder which makes our decoder capable to copy words from the source text
the design of the pointer network is the same as the model used in see liu and manning thus we omit this procedure in our paper due to the limited space
we use the negative log likelihood as the loss tion lg y log
denoising module due to the fact that reader comments are a kind of mal text they may consist of many noisy information and not all the comments are helpful for generating better maries
consequently we employ a denoising module to distinguish which comments are helpful
first we employ an bi rnnc to model the comment word embeddings i t bi hc i i t where i t denotes the hidden state of t th word in i ment ci
next we use average pooling operation over these figure overview of rasg
we divide our model into four parts summary generator generates a summary to describe the main aspect of document
reader attention module models the readers attention of document
supervisor models the gap of focused document aspect between generated summary and reader comments
goal tracker sets a goal of summary generator according to gap given by supervisor
hidden states to produce a vector representation ai of i th comment shown in equation
finally we apply a ear transform with sigmoid function to predict whether the comment is useful and the sigmoid output i also can be seen as a salience score of i th comment given the document representation hd ai i hd t d
hc


t bi i t c i to train the denoising module we use the cross entropy loss to supervise this procedure
ld c i
where i is the ground truth salience score of ments
denotes the i th comment ci is helpful for generating summary and vice versa
reader attention modeling to model the reader focused aspect we rst calculate the word alignment of reader comments towards the document
we use the embeddings of words in document and ments to calculate the semantic alignment score
precisely i j k is the alignment socre between the i th document word xd i and the k th word in the j th comment xc j as shown in equation i j k j


i j t c j k j in equation we use a max operation over the alignment i j to signify whether the i th word of document is focused by the j th comment
we regard the alignment score i as the reader attention weight for the j th reader comment to the i th document word
in order to reduce the interference caused by the noisy comments we employ the comment salience score j tained from the denoising module to weighted combine the j th reader attention i as shown in equation
it means that noisy comments will contribute less in the procedure of reader attention modeling
i c i j exp i j d exp j
finally we get the reader attention r for i th document word after a softmax function as shown in equation
supervisor to model the semantic gap between the generated summary and the reader focused aspects we design a supervisor ule
first for the decoder we need to know which aspect in document has been focused by our summary generator in the past decoding steps
we sum up the latest k attention butions t


and result in t rt d as the focus distribution of generated summary over t d ument words shown in equation
then we use t to weighted sum the document hidden states hd and result in mt t k mt d t ihd i ti where mt represents the focused aspect by the latest k coding steps a

a
decoder focused aspect
next we use the reader attention to weighted sum the document hidden states hd u d i reader supervisorcomment contentdiscriminatorgoal goal trackerreader focused aspectgenerated aspectgap contentdocumentattentiondocumentattentiondocumentattentioncontext summary generator where u represents the reader focused aspect
for encouraging the decoder focused aspect become ilar to the reader focused aspect we employ an cnn based discriminator to signify the difference between the decoder focused aspect mt and the reader focused aspect u
then we can use this difference to guide the decoder focus on the reader focused aspect
typically the discriminator is a binary classier which can be decomposed into a tional feature extractor f shown in equation and a moid classication layer shown in equation and
x m t bf u bf where denotes the convolutional operation trainable rameter wc denotes the convolutional kernel and m and t u are both the classication probabilities
note that a token generated at time t will inuence not only the gradient received at that time but also the gradient at subsequent time steps
intuitively the decoding attention t of latter decoding step is more similar to the attention of nal summary than the earlier steps
thus we propose to dene the cumulative loss with a discount factor as the loss functions
note that the training objective for inator can be interpreted as maximizing the log likelihood for classication whether the input in equation comes from reader focused aspect or from decoder focused aspect
t u m t m in order to model the gap between reader focused aspect and decoder focused aspect we subtract the reader tion rt d resulting in attention difference t rt d shown in equation
then we use the attention difference t to sum up the document hidden states hd d ld c d lg by t rt d t
dt d t ihd i
t t where dt denotes the semantic of unfocused document pects by summary generator a

a
gap content
to age the summary generator focus on the unfocused ment aspects we feed the gap content dt to the generator as shown in equation
goal tracker since the discriminator only provides a scalar guiding nal m t at each decoding step it becomes relatively less formative when the sentence length t y goes larger
inspired by leakgan guo et al
the proposed rasg work allows discriminator to provide additional information denoted as goal vector gt
in view of there is certain ship between the goal of current decoding step and previous steps we need to model the temporal interactions between the goal of each step
more specically we introduce a goal tracker module an lstm that takes the extracted feature vector and gap content as its input at each step t and outputs a goal vector gt gt
in order to achieve higher consistency of reader focused pect we feed the goal vector gt into the generator to guide the generation of the next word as shown in equation
model training as our model is trained in an adversarial manner we split the parameters in our model into two parts tion module including the parameters of summary generator reader attention module and goal tracker discriminator module including the parameters of cnn classier
as for training generation module we sum up the loss function of denoising module ld cross entropy between ground truth lg and the result of discriminator lg c as shown in tion
we use the l to optimize the parameters of tion module
l lg ld lg c next we train the discriminator module to maximize the probability of assigning the correct label to both generated aspect mt and reader focused aspect u
more specically we optimize the parameters of discriminator module according to the loss function ld calculated in equation
experimental setup research questions we list four research questions that guide the experiments does rasg outperform other baselines what is the effect of each module in rasg does rasg capture useful information from noisy comments can goal tracker give a helpful guidance to decoder dataset we collect the document summary comments pair data from weibo which is the largest social network website in china and users can read a document and post a ment about the document on this website
each sample of data contains a document a summary and several reader comments
most comments are about the readers opinion of their focused aspect in the document
in order to train the denoising module we should give a ground truth label i for i th comment
when there is at least one common word in summary and comment we regard such comment is helpful for generating summary
accordingly we give the i to i comment when it contains at least one mon word and give when it does not
in total our training dataset contains training samples
the erage length of document is
words average length of comment is
words and average length of summary is
words
the average comments number of a document is

evaluation metrics for evaluation metrics we adopt rouge score lin which is widely applied for summarization evaluation sun et al
chen et al

the rouge metrics compare generated summary with the reference summary by puting overlapping lexical units including igram bi gram and rouge l longest mon subsequence
table ablation models for comparison
table rouge scores of different ablation models
acronym gloss rouge l rasg dm rasg denoising module rasg g rasg gt rasg gtd rasg goal tracker discriminator rasg gap content rasg goal tracker table rouge scores comparison between baselines
rasg dm rasg g rasg gt rasg gtd rasg














rouge l cgu rasg textrank

















comparison methods in order to prove the effectiveness of each module in rasg we conduct some ablation models introduced in table
to evaluate the performance of our proposed dataset and model we compare it with the following baselines sequence to sequence framework sutskever vinyals and le has been proposed for language generation task
we simply add the reader tention on attention distribution t in each decoding step
cgu lin et al
propose to use the volutional gated unit to rene the source representation which achieves the state of the art performance on social media text summarization dataset
is a commonly used baseline nallapati zhai and zhou see liu and manning which selects the rst tence of document as the summary
textrank cea et al
propose to build a graph then add each tence as a vertex and use link to represent semantic ity
sentences are sorted based on nal scores and a greedy algorithm is employed to select summary sentences
implementation details we implement our experiments in tensorflow abadi et al
on an nvidia gpu
the word embedding mension is set to and the number of hidden units is
we set the in the equation and
in tion and
we use adagrad optimizer duchi hazan and singer as our optimizing algorithm
we employ beam search with beam size to generate more uency mary sentence
experimental results overall performance for research question we examine the performance of our model in terms of rouge
table lists mances of all comparisons in terms of rouge score
we see that rasg achieves a

and
increment over the state of the art method cgu in terms of and rouge l respectively
it is worth noticing that the baseline model achieves better performance than which demonstrates the effectiveness of rating reader focused aspect in summary generation
ever when compared with rasg achieves lower formance in terms of all rouge score
thus simply adding the reader focused aspect into generation procedure is not a good reader aware summarization method
ablation study next we turn to research question
we conduct lation tests on the usage of denoising module supervisor as well as the goal tracker and the rouge score result is shown in table
the discriminator provides the scalar training signal lg c for generator training and the feature tor for goal tracker
consequently there is an crement of
from rasg gtd to rasg gt in terms of rouge l which demonstrates the tiveness of discriminator
as for the effectiveness of goal tracker compared with rasg and rasg gt rasg gtd offers a decrease of
and
in terms of respectively
this demonstrates that the goal tracker with the feature from discriminator plays an tant role in producing better summary
however using the goal tracker without the feature extracted by the nator does not help improve the performance of the mary generator shown by the performance of rasg gtd
finally rasg dm offers a decrease of
compared with rasg in terms of rouge l which strates the effectiveness of denoising module
denoising ability next we turn to research question
due to the fact that the denoising module is learned in a supervised way there is a ground truth label associated with each comment
thus when the predict salience score i
we classify it as a helpful comment and vice versa
as the denoising module can be regarded as a binary classier to classify each ment to i or i we calculate the classication recall score of comments to measure the performance of this module
the recall curve is shown in figure
as the ing progresses the recall score is on a steady upward curve which proves the improved performance of denoising ule
to conclude the denoising module can give a ful salience score for the subsequent process
figure a cosine distance between decoding attention and reader attention
recall score of denoising module
table consistency and uency comparison by human evaluation
fluency consistency mean variance mean variance
cgu
rasg









analysis of goal tracker in this section we turn to research question
the main purpose of employing goal tracker is to help the summary generator utilize the reader focused aspect
intuitively we want to know whether the summary generator follows the goal set by the goal tracker
therefore we calculate the sine distance between decoder attention t y rt d in equation and reader attention rt d in equation
in figure we compare the cosine distance between the lation model rasg gtd and rasg
rasg observes a decrease of cosine distance and conversely the rasg gtd observes an increment of cosine distance
the fact that rasg can narrow the cosine distance proves that goal tracker and discriminator can lead the generator to follow the reader focused aspect
human evaluation we ask three highly educated ph
d
students to rate erated summaries of different models according to tency and uency
these annotators are all native speakers
the rating score ranges from to and is the best
we take the average score of all summaries as the nal score of each model as shown in table
it can be seen that rasg outperforms other baseline models in both sentence uency and consistency by a large margin
we calculate the kappa statistics in terms of uency and consistency and the score is
and
respectively
to prove the signicance of the above results we also do the paired student t test between our model and cgu model row with shaded background the value are
and
for uency and tency respectively
case analysis figure shows a document and its corresponding maries generated by different methods
we can observe that figure examples of the generated summary by rasg and other models
does generate uent summary
however the generated aspect is contradictory to the focused aspect of reader or ground truth summary
meanwhile rasg overcomes this shortcoming by using goal vector and gap content given by goal tracker and supervisor at training stage and produces the summary that is not only uent but also consistent with main aspect of document
conclusion in this paper we propose a new framework named aware summary generator rasg which aims to generate summaries for document from social media incorporating the reader comments
in order to capture the reader focused aspect we design a reader attention component with a noising module to capture the alignment between comments and document
we employ a supervisor to measure the mantic gap between generated summary and reader focused aspect
a goal tracker uses the information of semantic gap and the feature extracted by the discriminator to produce a goal vector to guide the summary generator
in our iments we have demonstrated the effectiveness of rasg and have found signicant improvements over state of art baselines in terms of rouge and human evaluations
moreover we have veried the effectiveness of each module in rasg for improving the summarization performance
cosinedistancebetweencurrentcontentandreadercontent
consistencyanduencycomparisonbyhumanevaluation












consistencyanduency

wenallytaketheaverageacrosssummariesandannotators

inta
toprovethesignicanceoftheaboveresults wealsodothepairedstudentt testbetweenourmodelandbaselinemeth ods thep

sistencyrespectively
eratedsummariesbydifferentmethods
butarecontradictorytothefocusofreaderandgroundtruthsummary
however rasgovercomesthisshortcomingbyusinggoalvectorandgapcontentgivenbysupervisorandgoaltrackerattrainingstage andproducethesummarynotonlyuentbutalsocon sistentwiththefocusofreaders
conclusioninthispaper wehaveproposedthetaskofreader awaresummarygeneration whichaimstogeneratesummariesfortextfromsocialmediaincorporatethecommentsofread ers
toaddressthistask wehaveproposedreader to sequencebasedsummarygeneratorisusedtoencodethedocumentandthengeneratethesummarysentencewithattentionandcopymechanism
inordertocapturethefocusedaspectbythereaders weuseareaderattentionmoduletomodelthealign examplesofthegeneratedsummarybyrasgandothermodels

q xyi s carryouteducationandtrainingforpractitioners andstrivetocreatenewthenationalteamandthemainforceofthemediaplatform
seelongweibofordetails
comments ownedcapitalmustbeinvolved

tralnetworkofceshouldcuretheseblacksheep
eh establishingstate s na
q thecentralnetworkofcewillprovidepolicysupporttoestablishstate ownedcapital

q s workinghardtobuildanationalteamofnewmediaplat form
mentofcommentsanddocument
weemployasupervi sortomeasurethesemanticgapbetweenaspectofgener atedsummaryandreader focuseddocument
finally agoaltrackerusestheinformationofsemanticgapandthefea tureextractedbythediscriminatortoproduceagoalvec tortoguidethesummarygenerator
inourexperiments wehavedemonstratedtheeffectivenessofrasgandhavefoundsignicantimprovementsoverstate of the artbase linesintermsofrougeandhumanevaluations
moreover wehaveveriedtheeffectivenessofeachmoduleinrasgforimprovingreader awaresummarygeneration
futureworkinvolvesextendingourmodeltoimprovedadversarialtrainingskillslikewassersteingan
also weplantopursueanoveltextmatchingmethodtodenoisingmoduleforimprovingtheaccuracy
acknowledgments we would like to thank the anonymous reviewers for their constructive comments
we would also like to thank jun zhang sicong jiang for their helps on this project
this work was supported by the national key research and development program of china no
the national science foundation of china nsfc no
no
alibaba innovative research air fund
rui yan was sponsored by ccf tencent open research fund and microsoft research asia msra laborative research program
references abadi m
barham p
chen j
chen z
davis a
dean j
devin m
ghemawat s
irving g
isard m
et al

tensorow a system for large scale machine ing
in osdi volume
bahdanau d
cho k
and bengio y

neural chine translation by jointly learning to align and translate
in iclr
bansal m
and chen y


fast abstractive rization with reinforce selected sentence rewriting
in acl
chen x
gao s
tao c
song y
zhao d
and yan r

iterative document representation learning towards summarization with polishing
in emnlp
chopra s
auli m
and rush a
m

abstractive sentence summarization with attentive recurrent neural works
in hlt naacl
duchi j
c
hazan e
and singer y

adaptive subgradient methods for online learning and stochastic mization
jmlr
gao s
ren z
zhao y
e
zhao d
yin d
and yan r

product aware answer generation in e commerce question answering
in wsdm
gu j
lu z
li h
and li v
o
k

ing copying mechanism in sequence to sequence learning
corr

guo j
lu s
cai h
zhang w
yu y
and wang j

long text generation via adversarial training with leaked information
corr

hu b
chen q
and zhu f

lcsts a large scale chinese short text summarization dataset
in emnlp
hu m
sun a
and lim e


comments oriented blog summarization by sentence extraction
in cikm
hu m
sun a
and lim e


comments oriented document summarization understanding documents with readers feedback
in sigir
jadhav a
and rajan v

extractive summarization with swap net sentences and words from alternating pointer networks
in acl
li p
bing l
lam w
li h
and liao y

aware multi document summarization via sparse coding
in ijcai
li p
wang z
lam w
ren z
and bing l

salience estimation via variational auto encoders for document summarization
in aaai
li p
bing l
and lam w

reader aware document summarization an enhanced model and the rst dataset
in proceedings of the workshop on new frontiers in summarization
lin j
sun x
ma s
and su q

global encoding for abstractive summarization
in acl
lin c


rouge a package for automatic evaluation of summaries
text summarization branches out
ma s
sun x
lin j
and ren x

a hierarchical end to end model for jointly improving text summarization and sentiment classication
in ijcai
ma s
sun x
lin j
and wang h

coder as assistant supervisor improving text representation for chinese social media text summarization
in acl
mihalcea r
and tarau p

textrank bringing order into text
in emnlp
nallapati r
zhai f
and zhou b

summarunner a recurrent neural network based sequence model for tive summarization of documents
in aaai
narayan s
cohen s
b
and lapata m

ranking sentences for extractive summarization with reinforcement learning
in naacl hlt
nguyen m

tran c

tran d

and nguyen m
l

solscsum a linked sentence comment dataset for social context summarization
in cikm
see a
liu p
j
and manning c
d

get to the point summarization with pointer generator networks
arxiv preprint

sun m
hsu w
t
lin c

lee m

min k
and tang j

a unied model for extractive and abstractive summarization using inconsistency loss
in acl
sutskever i
vinyals o
and le q
v

sequence to sequence learning with neural networks
in nips
tao c
gao s
shang m
wu w
c
zhao d
and yan r

get the point of my utterance learning towards effective responses with multi head attention mechanism
in ijcai
vinyals o
fortunato m
and jaitly n

pointer networks
in nips
wang l
yao j
tao y
zhong l
liu w
and du q

a reinforced topic aware convolutional sequence sequence model for abstractive text summarization
in cai
yang z
cai k
tang j
zhang l
su z
and li j


social context summarization
in sigir
zhou q
yang n
wei f
and zhou m

selective encoding for abstractive sentence summarization
in acl
zhou q
yang n
wei f
and zhou m

sequential copying networks
in aaai

