bottom up abstractive summarization sebastian gehrmann yuntian deng

school of engineering and applied sciences harvard university gehrmann dengyuntian alexander rush t
c o l c
s
c
v
v i x r a abstract neural network based methods for tive summarization produce outputs that are more uent than other techniques but perform poorly at content selection
this work poses a simple technique for addressing this issue use a data efcient content selector to over determine phrases in a source document that should be part of the summary
we use this selector as a bottom up attention step to constrain the model to likely phrases
we show that this approach improves the ability to compress text while still generating uent summaries
this two step process is both pler and higher performing than other end end content selection models leading to nicant improvements on rouge for both the cnn dm and nyt corpus
furthermore the content selector can be trained with as little as sentences making it easy to transfer a trained summarizer to a new domain
introduction text summarization systems aim to generate ural language summaries that compress the mation in a longer text
approaches using ral networks have shown promising results on this task with end to end models that encode a source document and then decode it into an tive summary
current state of the art neural stractive summarization models combine tive and abstractive techniques by using generator style models which can copy words from the source document gu et al see et al
these end to end models produce uent abstractive summaries but have had mixed success in content selection deciding what to summarize compared to fully extractive models

there is an appeal to end to end models from a modeling perspective however there is evidence that when summarizing people follow a two step source document german chancellor angela merkel did not look too pleased about the weather during her annual easter holiday in italy as britain basks in sunshine and temperatures of up to mrs merkel and her husband chemistry professor joachim sauer had to settle for a measly degrees
the chancellor and her spouse have been spending easter on the small island of ischia near naples in the mediterranean for over a decade
not so sunny angela merkel and her husband chemistry professor joachim sauer are spotted on their
annual easter trip to the island of ischia near naples
the couple traditionally spend their holiday at the star miramare spa hotel on the south of the island which comes with its own private beach and conies overlooking the ocean

reference angela merkel and husband spotted while on italian island holiday


baseline approach angela merkel and her husband chemistry professor joachim sauer are spotted on their annual easter trip to the island of ischia near naples


bottom up summarization angela merkel and her husband are spotted on their easter trip to the island of ischia near naples


figure
example of two sentence summaries with and without bottom up attention
the model does not low copying of words in although it can erate words
with bottom up attention we see more explicit sentence compression while without it whole sentences are copied verbatim
approach of rst selecting important phrases and then paraphrasing them anderson and hidi
jing and mckeown
a similar argument has been made for image captioning
son et al
develop a state of the art model with a two step approach that rst pre computes bounding boxes of segmented objects and then
plies attention to these regions
this so called bottom up attention is inspired by neuroscience
search describing attention based on properties herent to a stimulus buschman and miller
related work
motivated by this approach we consider bottom up attention for neural abstractive marization
our approach rst selects a selection mask for the source document and then constrains a standard neural model by this mask

this approach can better decide which phrases a model should include in a summary without sacricing the uency advantages of neural abstractive marizers
furthermore it requires much fewer data to train which makes it more adaptable to new domains
our full model incorporates a separate content selection system to decide on relevant aspects of the source document
we frame this selection task as a sequence tagging problem with the tive of identifying tokens from a document that are part of its summary
we show that a tent selection model that builds on contextual word embeddings peters et al can identify rect tokens with a recall of over and a cision of over
to incorporate bottom up attention into abstractive summarization models we employ masking to constrain copying words to the selected parts of the text which produces grammatical outputs
we additionally experiment with multiple methods to incorporate similar straints into the training process of more plex end to end abstractive summarization els either through multi task learning or through directly incorporating a fully differentiable mask
our experiments compare bottom up attention with several other state of the art abstractive tems
compared to our baseline models of see et al
bottom up attention leads to an provement in rouge l score on the cnn daily mail cnn dm corpus from to while being simpler to train
we also see comparable or better results than recent reinforcement learning based methods with our mle trained system

thermore we nd that the content selection model is very data efcient and can be trained with less than of the original training data
this vides opportunities for domain transfer and resource summarization
we show that a rization model trained on cnn dm and ated on the nyt corpus can be improved by over points in rouge l with a content selector trained on only in domain sentences
there is a tension in document summarization tween staying close to the source document and allowing compressive or abstractive modication
many non neural systems take a select and press approach
for example dorr et al
introduced a system that rst extracts noun and verb phrases from the rst sentence of a news
ticle and uses an iterative shortening algorithm to compress it
recent systems such as durrett al
also learn a model to select sentences and then compress them

in contrast recent work in neural network based data driven extractive summarization has focused on extracting and ordering full sentences cheng and lapata dlikman and last
lapati et al
use a classier to determine whether to include a sentence and a selector that ranks the positively classied ones
these ods often over extract but extraction at a word level requires maintaining grammatically correct output cheng and lapata which is cult
interestingly key phrase extraction while
grammatical often matches closely in content with human generated summaries bui et al
a third approach is neural abstractive marization with sequence to sequence models
sutskever et al bahdanau et al

these methods have been applied to tasks such as headline generation rush al and article summarization nallapati al
chopra et al
show that attention approaches that are more specic to summarization can further prove the performance of models
gu et al
were the rst to show that a copy mechanism troduced by vinyals et al
can combine the advantages of both extractive and abstractive summarization by copying words from the source

see et al
rene this pointer generator
proach and use an additional coverage mechanism
tu al that makes a model aware of its attention history to prevent repeated attention
most recently reinforcement learning rl
proaches that optimize objectives for tion other than maximum likelihood have been shown to further improve performance on these tasks paulus et al li et al
ilmaz et al
paulus et al
approach the coverage problem with an intra attention in which a decoder has an attention over previously generated words
however rl based training can be difcult to tune and slow to train
our method does not utilize rl training although in theory
this approach can be adapted to rl methods
also papers
several explore multi pass

extractive abstractive summarization
pati et al
create a new source document comprised of the important sentences from the source and then train an abstractive system
liu al
describe an extractive phase that extracts full paragraphs and an abstractive one that determines their order
finally zeng al
introduce a mechanism that reads a source document in two passes and uses the information from the rst pass to bias the second
our method differs in that we utilize a completely abstractive model biased with a powerful content selector
other recent work explores alternative proaches to content selection
for example cohan et al
use a hierarchical attention to detect relevant sections in a document li et al
generate a set of keywords that is used to guide the summarization process and pasunuru and bansal develop a loss function based on whether salient keywords are included in a summary
other approaches investigate the content selection at the sentence level
tan et al
describe a based attention to attend to one sentence at a time
chen and bansal
rst extract full sentences from a document and then compress them and hsu et al
modulate the attention based on how likely a sentence is included in a summary
background neural summarization throughout this paper we consider a set of pairs of texts x y where x corresponds to source tokens
xn and y y to a summary

ym with m
abstractive summaries are generated one word at a time
at every time step a model is aware of the previously generated words
the problem is to learn a function parametrized by that imizes the probability of generating the correct sequences
following previous work we model the abstractive summarization with an attentional sequence to sequence model
the attention bution for a decoding step j culated within the neural network represents an embedded soft distribution over all of the source tokens and can be interpreted as the current focus of the model
the model additionally has a copy figure overview of the selection and generation cesses described throughout section
nism vinyals et al
to copy words from the source
copy models extend the decoder by predicting a binary soft switch zj that determines whether the model copies or generates
the copy distribution is a probability distribution over the source text and the joint distribution is computed as a convex combination of the two parts of the model

zj
zj where the two parts represent copy and generation distribution respectively
following the generator model of see et al
we reuse the attention distribution as copy tribution the copy probability of a token in the source w through the copy attention is computed as the sum of attention towards all occurrences of
during training we maximize marginal hood with the latent switch variable
bottom up attention
we next consider techniques for incorporating a content selection into abstractive summarization illustrated in figure
content selection
we dene the content selection problem as a level extractive summarization task
while there has been signicant work on custom extractive summarization see related work we make a plifying assumption and treat it as a sequence ging problem
let
tn denote binary tags for each of the source tokens if a word is copied in the target sequence and otherwise
while there is no supervised data for this task we can generate training data by aligning the maries to the document
we dene a word xi as sourcemasked sourcesummarycontent selectionbottom up attention copied if it is part of the longest possible sequence of tokens s xij i for integers i k n i if s and s y and there exists no earlier sequence u with s

we use a standard bidirectional lstm model trained with maximum likelihood for the sequence labeling problem
recent results have shown that better word representations can lead to cantly improved performance in sequence tagging tasks peters et al
therefore we rst map each token wi into two embedding channels
the embedding represents a i static channel of pre trained word embeddings glove
pennington et al
the are contextual embeddings from a pretrained language model elmo peters et al which uses a character aware token embedding kim et al followed by two bidirectional lstm ers
the contextual embeddings are i ne tuned to learn a task specic embedding
i as a linear combination of the states of each lstm layer and the token embedding and

i and i
i sj
i with and as trainable parameters
since these embeddings only add four additional eters to the tagger it remains very data efcient despite the high dimensional embedding space
both embeddings are concatenated into a gle vector that is used as input to a bidirectional lstm which computes a representation hi for a word wi
we can then calculate the probability that the word is selected as bs with trainable parameters ws and bs
bottom up copy attention inspired by work in bottom up attention for ages anderson et al which restricts
tion to predetermined bounding boxes within an image we use these attention masks to limit the available selection of the pointer generator model

as shown in figure a common mistake made by neural copy models is copying very long
in the quences or even whole sentences
line model over of copied tokens are part of copy sequences that are longer than tokens whereas this number is only for reference summaries
while bottom up attention could also be used to modify the source encoder tions we found that a standard encoder over the full text was effective at aggregation and therefore limit the bottom up step to attention masking

concretely we rst train a pointer generator model on the full dataset as well as the content selector dened above
at inference time to erate the mask the content selector computes lection probabilities n for each token in a source document
the selection probabilities are used to modify the copy attention distribution to only clude tokens identied by the selector
let ai j note the attention at decoding step j to encoder word
given a threshold the selection is plied as a hard mask such that qi

ow

to ensure that eq
still yields a correct ity distribution we rst multiply by a normalization parameter and then malize the distribution
the resulting normalized distribution can be used to directly replace a as the new copy probabilities
end to end alternatives two step bottom up attention has the tage of training simplicity
in theory though dard copy attention should be able to learn how to perform content selection as part of the end to end training
we consider several other end to end proaches for incorporating content selection into neural training
method mask only we rst consider
whether the alignment used in the bottom up proach could help a standard summarization tem
inspired by nallapati et al
we vestigate whether aligning the summary and the source during training and xing the gold copy
tention to pick the correct source word is cial
we can think of this approach as limiting the set of possible copies to a xed source word
here the training is changed but no mask is used at test time
method multi task
next we investigate whether the content selector can be trained side
the abstractive system
we rst test this pothesis by posing summarization as a multi task problem and training the tagger and tion model with the same features
for this setup we use a shared encoder for both abstractive marization and content selection
at test time we apply the same masking method as bottom up tention
method diffmask finally we sider training the full system end to end with the mask during training
here we jointly
timize both objectives but use predicted tion probabilities to softly mask the copy attention
which leads to a fully differentiable model
this model is used with the same soft mask at test time


inference
several authors have noted that longer form neural generation still has signicant issues with rect length and repeated words than in short form problems like translation
proposed solutions
clude modifying models with extensions such as a coverage mechanism tu et al see et al or intra sentence attention cheng et al

paulus et al
we instead stick to the theme of modifying inference and modify the scoring function to include a length penalty lp and a coverage penalty cp and is dened as y
log
length to encourage the generation of longer sequences we apply length normalizations during beam search
we use the length penalty by wu et al
which is formulated as
with a tunable parameter where increasing leads to longer summaries
we additionally set a minimum length based on the training data
repeats copy models often repeatedly attend to the same source tokens generating the same phrase multiple times
we introduce a new mary specic coverage penalty
n max n aj i
m
intuitively this penalty increases whenever the decoder directs more than of total attention within a sequence towards a single encoded ken
by selecting a sufciently high this penalty blocks summaries whenever they would lead to repetitions
additionally we follow paulus et al and restrict the beam search to never repeat trigrams
data and experiments we evaluate our approach on the cnn dm pus hermann et al nallapati et al and the nyt corpus sandhaus which are both standard corpora for news summarization

the summaries for the cnn dm corpus are let points for the articles shown on their respective websites whereas the nyt corpus contains maries written by library scientists
cnn dm summaries are full sentences with on average tokens and bullet points
nyt maries are not always complete sentences and are shorter with on average tokens and bullet points
following see et al
we use the non anonymized version of the cnn dm corpus and truncate source documents to kens and the target summaries to tokens in training and validation sets
for experiments with the nyt corpus we use the preprocessing scribed by paulus et al
and additionally remove author information and truncate source documents to tokens instead of
these changes lead to an average of tokens per
cle a decrease from the tokens with token truncated articles
the target non copy
lary is limited to tokens for all models

the content selection model uses pre trained glove embeddings of size and elmo with size
the bi lstm has two layers and a den size of
dropout is set to and the model is trained with adagrad an initial learning rate of and an initial accumulator value of
we limit the number of training examples to on either corpus which only has a small impact on performance
for the jointly trained content selection models we use the same uration as the abstractive model

for the base model we re implemented the pointer generator model as described by see et al

to have a comparable number of ters to previous work we use an encoder with hidden states for both directions in the one layer lstm and for the one layer decoder
the embedding size is set to
the model is trained with the same adagrad conguration as the tent selector
additionally the learning rate halves after each epoch once the validation perplexity does not decrease after an epoch
we do not use dropout and use gradient clipping with a mum norm of
we found that increasing model size or using the transformer vaswani et al method r l pointer generator see et al
pointer generator coverage see et al
ml intra attention paulus et al ml rl paulus et al

saliency entailment reward pasunuru and bansal
key information guide network li et al
inconsistency loss hsu et al
sentence rewriting chen and bansal pointer generator our implementation
pointer generator coverage penalty
copytransformer coverage penalty
pointer generator mask only pointer generator multi task pointer generator diffmask bottom up summarization bottom up summarization copytransformer
table results of abstractive summarizers on the cnn dm the rst section shows encoder decoder abstractive baselines trained with cross entropy
the second section describes reinforcement learning based proaches
the third section presents our baselines and the attention masking methods described in this work
can lead to slightly improved performance
but at the cost of increased training time and rameters
we report numbers of a transformer with copy attention which we denote former
in this model we randomly choose one of the attention heads as the copy distribution and otherwise follow the parameters of the big former by vaswani et al

all inference parameters are tuned on a
ample subset of the validation set
length penalty parameter and copy mask differ across els with ranging from to and ranging from to
the minimum length of the erated summary is set to for cnn dm and for nyt
while the pointer generator uses a beam size of and does not improve with a larger beam we found that bottom up attention requires a larger beam size of
the coverage penalty parameter is set to and the copy attention tion parameter to for both approaches
we use allennlp gardner et al for the tent selector and opennmt py for the abstractive models klein al
and reproduction instructions can be found at bottom up summary results compare on the non anonymized version of this corpus used by see et al
the best results on the anonymized version are
from results table shows our main results on the dm corpus with abstractive models shown in the top and bottom up attention methods at the bottom
we rst observe that using a age inference penalty scores the same as a full coverage mechanism without requiring any tional model parameters or model ne tuning
the results with the copytransformer and coverage penalty indicate a slight improvement across all three scores but we observe no signicant ence between pointer generator and former with bottom up attention
we found that none of our end to end models lead to improvements indicating that it is cult to apply the masking during training
out hurting the training process
the mask only model with increased supervision on the copy mechanism performs very similar to the task model
on the other hand bottom up tion leads to a major improvement across all three scores
while we would expect better content lection to primarily improve the fact all three increase hints that the uency is not
ing hurt specically
our cross entropy trained celikyilmaz al
we compare to their dca model on the nyt corpus
method ml
dca point
coverage pen
bottom up summarization r l table results on the nyt corpus where we pare to rl trained models
marks models and results by paulus et al
and results by celikyilmaz et al

proach even outperforms all of the learning based approaches in and while the highest reported rouge l score by
chen and bansal falls within the dence interval of our results
table shows experiments with the same tems on the nyt corpus
we see that the point improvement compared to the baseline
generator maximum likelihood approach carries over to this dataset
here the model outperforms the rl based model by paulus et al
in and but not l and is comparable to the results of celikyilmaz al except for rouge
the same can be observed when comparing ml and our pointer generator
we suspect that a difference in summary lengths due to our inference parameter choices leads to this difference but did not have access to their els or summaries to investigate this claim
this shows that a bottom up approach achieves petitive results even to models that are trained on summary specic objectives
the main benet of bottom up summarization seems to be from the reduction of mistakenly copied words
with the best pointer generator models the precision of copied words is compared to the reference
this precision creases to which mostly drives the increase in
an independent samples t test shows that this improvement is statistically signicant with p
we also observe a decrease in average sentence length of summaries from to words when adding content selection pared to the pointer generator while holding all other inference parameters constant
domain transfer while end to end training has become common there are benets to a step method
since the content selector only needs figure the auc of the content selector trained on cnn dm with different training set sizes ranging from to data points
to solve a binary tagging problem with pretrained vectors it performs well even with very limited training data
as shown in figure with only sentences the model achieves an auc of over
beyond that size the auc of the model increases only slightly with increasing training data
to further evaluate the content selection we consider an application to domain transfer

in this experiment we apply the pointer generator trained on cnn dm to the nyt corpus
in dition we train three content selectors on and thousand sentences of the nyt set and use these in the bottom up summarization
the results shown in table demonstrates that even a model trained on the smallest subset leads to an improvement of almost points over the model without bottom up attention
this improvement increases with the larger subsets to up to points

while this approach does not reach a ble performance to models trained directly on the nyt dataset it still represents a signicant
crease over the not augmented cnn dm model and produces summaries that are quite readable

we show two example summaries in appendix
this technique could be used for low resource mains and for problems with limited data ability
analysis and discussion extractive summary by content selection
given that the content selector is effective in junction with the abstractive model it is
ing to know whether it has learned an effective extractive summarization system on its own

ble shows experiments comparing content

tion to extractive baselines
the baseline is a commonly used baseline in news
tion that extracts the rst three sentences from an with increasing training auc r l data
novel verb noun
adj cnndm

reference vanilla pointer generator bottom up attention
table results of the domain transfer ment
auc numbers are shown for content selectors
rouge scores represent an abstractive model trained on cnn dm and evaluated on nyt with additional copy constraints trained on training ples of the nyt corpus
method
neusum
zhou et al
sents cont
select
oracle phrase selector content selector r l table results of extractive approaches on the cnn dm dataset
the rst section shows extractive scores
the second section rst shows an oracle score if the content selector selected all the rect words according to our matching heuristic
finally we show results when the content selector extracts all phrases above a selection probability threshold
article
shows the performance when we extract the top three sentences by average copy probability from the selector
interestingly with this method only of the top three sentences are not within the rst three further reinforcing the strength of the baseline
our naive sentence extractor performs slightly worse than the highest reported extractive score by zhou et al

that is specically trained to score nations of sentences
the nal entry shows the performance when all the words above a threshold are extracted such that the resulting summaries are approximately the length of reference summaries

the oracle score represents the results if our model had a perfect accuracy and shows that the tent selector while yielding competitive results has room for further improvements in future work

this result shows that the model is quite tive at nding important words but less effective at chaining them together
similar to paulus et al
we nd that the decrease in indicates a lack of uency and grammaticality of the generated summaries
a table novel shows the percentage of words in a summary that are not in the source document
the last three columns show the part of speech tag distribution of the novel words in generated summaries
typical example looks like this a man food his rst hamburger

fully for years
michael hanline was convicted of murder for the ing of truck driver jt mcgarry in on judge charges
this particular ungrammatical example has a of
this further highlights the benet of the combined approach where up predictions are chained together uently by the abstractive system
however we also note that the abstractive system requires access to the full source document
distillation experiments in which we tried to use the output of the selection as training input to abstractive models showed a drastic decrease in model performance
analysis of copying while pointer generator models have the ability to abstract in summary the use of a copy mechanism causes the summaries to be mostly extractive
table shows that with copying the percentage of generated words that are not in the source document decreases from to while reference summaries are much more abstractive with novel words
bottom up attention leads to a further reduction to only a half percent
however since generated summaries are typically not longer than words the ference between an abstractive system with and without bottom up attention is less than one novel word per summary
this shows that the benet of abstractive models has been less in their
ity to produce better paraphrasing but more in the ability to create uent summaries from a mostly extractive process
table also shows the part of speech tags of the novel generated words and we can observe an interesting effect
application of bottom up
tion leads to a sharp decrease in novel adjectives data pointer generator length penalty coverage penalty trigram repeat r l table results on cnn dm when adding one ence penalty at a time
conclusion this work presents a simple but accurate tent selection model for summarization that ties phrases within a document that are likely

cluded in its summary
we showed that this tent selector can be used for a bottom up tion that restricts the ability of abstractive marizers to copy words from the source
the combined bottom up summarization system leads to improvements in rouge scores of over two points on both the cnn dm and nyt corpora
a comparison to end to end trained methods showed that this particular problem can not be easily solved with a single model but instead requires tuned inference restrictions
finally we showed that this technique due to its data efciency can be used to adjust a trained model with few data points making it easy to transfer to a new
main
preliminary work that investigates similar bottom up approaches in other domains that
quire a content selection such as grammar tion or data to text generation have shown some promise and will be investigated in future work
acknowledgements

we would like to thank barbara grosz for ful discussions and feedback on early stages of this work
we further thank the three anonymous reviewers
this work was supported by a sung research award
yd was funded in part by a bloomberg research award
sg was funded in part by nih grant

references peter anderson xiaodong he chris buehler damien teney mark johnson stephen gould and lei zhang

bottom up and top down attention arxiv preprint for image captioning and


valerie anderson and suzanne hidi

figure for all copied words we show the
tion over the length of copied phrases they are part of

the black lines indicate the reference summaries and the bars the summaries with and without bottom up tention
and nouns
whereas the fraction of novel words that are verbs sharply increases
when looking at the novel verbs that are being generated we notice a very high percentage of tense or number changes indicated by variation of the word say for example said or says while novel nouns are mostly morphological variants of words in the source
figure shows the length of the phrases that are being copied
while most copied phrases in the reference summaries are in groups of to words the pointer generator copies many very long quences and full sentences of over words
since the content selection mask interrupts most long copy sequences the model has to either ate the unselected words using only the tion probability or use a different word instead
while we observed both cases quite frequently in generated summaries the fraction of very long copied phrases decreases
however either with or without bottom up attention the distribution of the length of copied phrases is still quite different from the reference

inference penalty analysis
we next analyze the effect of the inference time loss functions
ble presents the marginal improvements over the simple pointer generator when adding one penalty at a time
we observe that all three ties improve all three scores even when added on top of the other two
this further indicates that the unmodied pointer generator model has ready learned an appropriate representation of the abstractive summarization problem but is limited by its ineffective content selection and inference methods
generatorbottom up attentioncopy actions of different
ing students to summarize
educational leadership
dzmitry bahdanau kyunghyun cho and yoshua gio

neural machine translation by jointly arxiv preprint learning to align and translate



duy duc an bui guilherme del fiol john f hurdle and siddhartha jonnalagadda

extractive text summarization system to aid data extraction from full text in systematic review development
journal of biomedical informatics

timothy j buschman and earl k miller

down bottom up control of attention in the science prefrontal and posterior parietal cortices



asli celikyilmaz antoine bosselut xiaodong he and yejin choi

deep communicating agents for
in proceedings of the abstractive summarization

conference of the north american chapter of the association for computational linguistics
man language technologies volume long pers volume pages
yen chun chen and mohit bansal

fast tive summarization with reinforce selected sentence rewriting
arxiv preprint
jianpeng cheng li dong and mirella lapata


long short term memory networks for machine reading
arxiv preprint
greg durrett taylor berg kirkpatrick and dan klein


learning based single document tion with compression and anaphoricity constraints
arxiv preprint
matt gardner joel grus mark neumann oyvind tafjord pradeep dasigi nelson liu
matthew ters michael schmitz and luke zettlemoyer


allennlp a deep semantic natural language cessing platform arxiv preprint
jiatao gu zhengdong lu hang li and victor ok
incorporating copying mechanism in arxiv preprint li

sequence to sequence learning


karl moritz hermann tomas kocisky edward grefenstette lasse espeholt will kay mustafa leyman and phil blunsom

teaching chines to read and comprehend
in advances in
ral information processing systems pages
wan ting hsu chieh kai lin ming ying lee kerui min jing tang and min sun

a ed model for extractive and abstractive arxiv preprint rization using inconsistency loss


hongyan jing and kathleen r mckeown


the decomposition of human written summary tences
in proceedings of the annual tional acm sigir conference on research and velopment in information retrieval pages
jianpeng cheng and mirella lapata
neural summarization by extracting sentences and words
arxiv preprint
yoon kim yacine jernite david sontag and der m rush

character aware neural language models
in aaai pages

sumit chopra michael auli and alexander m rush


abstractive sentence summarization with tentive recurrent neural networks
in proceedings of the conference of the north american
ter of the association for computational linguistics human language technologies pages arman cohan franck dernoncourt doo soon kim trung bui seokhwan kim walter chang and
nazli goharian

a discourse aware attention model for abstractive summarization of long documents
in proceedings of the conference of the north american chapter of the association for tional linguistics human language technologies volume short papers volume pages
alexander dlikman and mark last

using machine learning methods and linguistic features in single document extractive summarization

in pkdd ecml pages
bonnie dorr david zajic and richard schwartz


hedge trimmer a parse and trim approach to headline generation
in proceedings of the
naacl
on text summarization workshop volume pages
association for computational guistics
guillaume klein yoon kim yuntian deng
jean
senellart and alexander m rush
opennmt open source toolkit for neural machine translation
arxiv preprint

chenliang li weiran xu si li and sheng gao


guiding generation for abstractive text tion based on key information guide network

in proceedings of the conference of the north american chapter of the association for tional linguistics human language technologies volume short papers volume pages

piji li lidong bing and wai lam
critic based training framework for abstractive marization
arxiv preprint
peter j liu mohammad saleh etienne pot ben goodrich ryan sepassi lukasz kaiser and noam shazeer

generating wikipedia by arxiv preprint summarizing long sequences


ramesh nallapati feifei zhai and bowen zhou


summarunner a recurrent neural network based quence model for extractive summarization of ments
in aaai pages
ramesh nallapati bowen zhou caglar gulcehre bing xiang al

abstractive text marization using sequence to sequence rnns and yond
arxiv preprint
zhaopeng tu zhengdong lu yang liu xiaohua liu and hang li

modeling coverage arxiv preprint for neural machine translation


ashish vaswani noam shazeer niki parmar jakob
uszkoreit llion jones aidan n gomez ukasz kaiser and illia polosukhin
attention is all you need
in advances in neural information cessing systems pages oriol vinyals meire fortunato and navdeep jaitly


pointer networks
in advances in neural formation processing systems pages
yonghui wu mike schuster zhifeng chen
quoc v le mohammad norouzi wolfgang macherey maxim krikun yuan cao qin gao klaus macherey al


google s neural chine translation system bridging the gap between arxiv preprint human and machine translation

wenyuan zeng wenjie luo sanja fidler and raquel efcient summarization with arxiv preprint urtasun

read again and copy mechanism



qingyu zhou nan yang furu wei shaohan huang ming zhou and tiejun zhao

neural ment summarization by jointly learning to score and select sentences
in proceedings of the annual meeting of the association for computational guistics volume long papers volume pages
ramesh nallapati bowen zhou and mingbo ma


classify or select neural architectures for extractive document summarization
arxiv preprint

ramakanth pasunuru and mohit bansal

reward reinforced summarization with saliency and entailment
in proceedings of the conference of the north american chapter of the association for computational linguistics human language technologies volume short papers volume pages
romain paulus caiming xiong and richard socher


a deep reinforced model for abstractive marization
arxiv preprint
jeffrey pennington richard socher and christopher manning

glove
global vectors for word representation
in proceedings of the ence on empirical methods in natural language cessing emnlp pages
matthew e peters waleed ammar chandra ula and russell power
semi supervised quence tagging with bidirectional language models
arxiv preprint
matthew e peters mark neumann mohit iyyer matt gardner christopher clark kenton lee and luke zettlemoyer

deep contextualized word resentations
arxiv preprint

alexander m rush sumit chopra and jason
a neural attention model for arxiv preprint ston
stractive sentence summarization


evan sandhaus
the new york times annotated corpus
linguistic data consortium philadelphia

abigail see peter j liu and
christopher d
to the point
summarization arxiv preprint ning
get with pointer generator networks

ilya sutskever oriol vinyals and quoc v le

sequence to sequence learning with neural works
in advances in neural information ing systems pages jiwei tan xiaojun wan and jianguo xiao


abstractive document summarization with a
in proceedings of based attentional neural model
the annual meeting of the association for putational linguistics volume long papers volume pages
examples reference content selection reference
content selection generated summary green bay packers successful season is largely due to quarterback brett favre ahman green rushed for yards in victory over the giants
true dorsey levens good enough to start for most teams but now green backup contributed kickoff returns of and yards
playoff bound green bay packers beat the giants in the victory
the packers won three games and six of each other
paul byers pioneer of visual anthropology dies at age paul byers an early practitioner of mead died on dec
at his home in manhattan
he enlisted in the navy which trained him as a cryptanalyst and stationed him in australia
paul byers an early practitioner of anthropology pioneered with garet mead
table domain transfer examples
a domain transfer examples

we present two generated summaries for the cnn dm to nyt domain transfer experiment in table
refers to a pointer generator with coverage penalty trained on cnn dm that scores rouge l on the nyt dataset
the content selection improves this to rouge l without any ne tuning of the model

