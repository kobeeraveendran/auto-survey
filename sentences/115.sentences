attention based models for text dependent speaker verification f a rezaur rahman chowdhury quan wang ignacio lopez moreno li wan washington state university
wsu
edu google inc
usa quanw elnota liwan
com n a j s a
s s e e v
v i x r a abstract attention based models have recently shown great performance on a range of tasks such as speech recognition machine translation and image captioning due to their ability to summarize relevant formation that expands through the entire length of an input quence
in this paper we analyze the usage of attention mechanisms to the problem of sequence summarization in our end to end dependent speaker recognition system
we explore different gies and their variants of the attention layer and compare different pooling methods on the attention weights
ultimately we show that attention based models can improves the equal error rate eer of our speaker verication system by relatively compared to our non attention lstm baseline model
index terms attention based model sequence tion speaker recognition pooling lstm however one challenge in our architecture introduced in is that silence and background noise are not being well captured
though our speaker verication runs on a short window that is segmented by the keyword detector the phonemes are ally surrounded by frames of silence and background noise
ideally the speaker embedding should be built only using the frames sponding to phonemes
thus we propose to use an attention layer as a soft mechanism to emphasize the most relevant ements of the input sequence
this paper is organized as follows
in sec
we rst briey review our lstm based d vector baseline approach trained with the end to end architecture
in sec
we introduce how we add the attention mechanism to our baseline architecture covering ent scoring functions layer variants and weights pooling methods
in sec
we setup experiments to compare attention based els against our baseline model and present the eer results on our testing set
conclusions are made in sec


introduction
baseline architecture speaker verication sv is the process of verifying based on a set of reference enrollment utterances whether an verication utterance belongs to a known speaker
one subtask of sv is global password text dependent speaker verication td sv which refers to the set of problems for which the transcripts of reference enrollment and verication utterances are constrained to a specic phrase
in this study we focus on ok google and hey google global words as they relate to the voice match feature of google home
i vector based systems in combination with verication back ends such as probabilistic linear discriminant analysis plda have been the dominating paradigm of sv in ous years
more recently with the rising of deep learning in various machine learning applications more efforts have been cusing on using neural networks for speaker verication
currently the most promising approaches are end to end integrated tures that simulate the enrollment verication two stage process during training
for example in the authors propose architectures that semble the components of an i vector plda system
such tecture allowed to bootstrap the network parameters from pretrained i vector and plda models for a better performance
however such initialization stage also constrained the type of network architectures that could be used only deep neural networks dnn can be initialized from classical i vector and plda models
in we have shown that long short term memory lstm networks can achieve better performance than dnns for integrated end to end architectures in td sv scenarios
the author did this work during his intern at google
our end to end training architecture is described in fig

for each training step a tuple of one evaluation utterance xj and n rollment utterances xkn for n n is fed into our lstm network xj xkn where represents the features log energies from a xed length segment j and k represent the speakers of the utterances and j may or may not equal
the tuple includes a single utterance from speaker j and n ferent utterance from speaker k
we call a tuple positive if xj and the n enrollment utterances are from the same speaker i
e
j k and negative otherwise
we generate positive and negative tuples alternatively
for each utterance let the output of the lstm s last layer at frame t be a xed dimensional vector ht where t t
we take the last frame output as the d vector ht fig
and build a new tuple j kn
the centroid of tuple kn represents the voiceprint built from n utterances and is dened as follows ck n n kn
the similarity is dened using the cosine similarity function s w ck with learnable w and
the tuple based end to end loss is nally dened as ck k
fig
our baseline end to end training architecture as introduced in
here ex is the standard sigmoid function and k equals if j k otherwise equals to
the end to end loss function encourages a larger value of s when j and a smaller value of s when j
consider the update for both positive and negative tuples this loss function is very similar to the triplet loss in facenet

attention based model

basic attention layer in our baseline end to end training we directly take the last frame output as d vector ht
alternatively we could learn a scalar score et r for the lstm output ht at each frame t et ht t t
then we can compute the normalized weights t using these scores such that t
and nally as shown in fig
we form the vector as the weighted average of the lstm outputs at all frames t tht
t

scoring functions by using different scoring functions in eq
we get different attention layers bias only attention where bt is a scalar
note this attention does not depend on the lstm output
et bt
linear attention where wt is an m dimensional vector and bt is a scalar
et t ht bt
fig
lstm based d vector baseline
basic attention layer
shared parameter linear attention where the m dimensional vector w and scalar are the same for all frames
et wt ht
non linear attention where wt is an m matrix bt and vt are vectors
the dimension can be tuned on a development dataset
et vt t bt
shared parameter non linear attention where the same w b and v are used for all frames
et vt
in all the above scoring functions all the parameters are able within the end to end architecture


attention layer variants apart from the basic attention layer described in sec

here we introduce two variants cross layer attention and divided layer tention
for cross layer attention fig
the scores et and weights t are not computed using the outputs of the last lstm layer evaluation utteranceenrollmentutterance


accept rejectspeaker modelspeakerrepresentationscore function enrollmentutterance n


logistic regressioncosine similarityaveragelstm


input featureslstmlstm outputsd vector





input features


lstmlstm outputsnormalized weightsd vector


a fig
two variants of the attention layer a cross layer attention b divided layer attention
but e

the second to last layer the outputs of an intermediate lstm layer et t
however the d vector is still the weighted average of the last layer output ht
for divided layer attention fig
we double the dimension of the last layer lstm output ht and equally divide its dimension into two parts part a ha t
we use part a to build the d vector while using part b to learn the scores t and part b hb et hb t tha t
t

weights pooling another variation of the basic attention layer is that instead of rectly using the normalized weights t to average lstm outputs we can optionally perform maxpooling on the attention weights
this additional pooling mechanism can potentially make our work more robust to temporal variations of the input signals
we have experimented with two maxpooling methods fig
sliding window maxpooling we run a sliding window on the weights and for each window only keep the largest value and set other values to
fig
different pooling methods on attention weights
the tth pixel corresponds to the weight t and a brighter intensity means a larger value of the weight
global top k maxpooling only keep the largest k values in the weights and set all other values to

experiments

datasets and basic setup to fairly compare different attention techniques we use the same training and testing datasets for all our experiments
our training dataset is a collection of anonymized user voice queries which is a mixture of ok google and hey google
it has around m utterances from around k speakers
our ing dataset is a manual collection consisting of speakers
it s divided into two enrollment sets and two verication sets for each of ok google and hey google
each enrollment and evaluation dataset contains respectively an average of
and evaluation utterances per speaker
we report the speaker verication equal error rate eer on the four combinations of enrollment set and verication set
our baseline model is a layer lstm where each layer has dimension with a projection layer of dimension
on top of the lstm is a linear layer of dimension
the acoustic parametrization consists of dimensional log cients computed over a window of with of overlap
the same acoustic features are used for both keyword detection and speaker verication
the keyword spotting system isolates segments of length t frames that only contain the global password and these segments form the tuples mentioned above
the two keywords are mixed together using the multireader technique introduced in


basic attention layer first we compare the baseline model with basic attention layer sec

using different scoring function sec


the results are shown in table
as we can see while bias only and linear attention bring little improvement to the eer non linear improves the performance signicantly especially with shared parameters
the intermediate dimension of non linear scoring functions we use such that wt and w are square matrices









last layer outputsd to last layer outputs











vectorlast layer outputspart apart poolingsliding window maxpoolingglobal top k maxpooling table evaluation non attention baseline model vs
basic attention layer using different scoring functions
test data enroll verify ok google ok google ok google hey google hey google ok google hey google hey google average non attention baseline




basic attention fsl




fnl




fl




fbo




fsnl




table evaluation basic attention layer vs
variants all using fsnl as scoring function
cross layer divided layer test data ok ok ok hey hey ok hey hey average basic fsnl














table evaluation different pooling methods for tion weights all using fsnl and divided layer
test data ok ok ok hey hey ok hey hey average no pooling




sliding window top k











variants to compare the basic attention layer with the two variants sec

we use the same scoring function that performs the best in the vious experiment the shared parameter non linear scoring function fsnl
from the results in table we can see that divided layer tention performs slightly better than basic attention and cross layer at the cost that the dimension of last lstm layer is bled


weights pooling to compare different pooling methods on the attention weights as troduced in sec

we use the divided layer attention with parameter non linear scoring function
for sliding window pooling we experimented with different window sizes and steps and found that a window size of frames and a step of frames perform the best in our evaluations
also for global top k pooling we found that the performance is the best when k
the results are shown in table
we can see that sliding window maxpooling further improves the eer
we also visualize the attention weights of a training batch for different pooling methods in fig

an interesting observation is that when there s no pooling we can see a clear strand or strand pattern in the batch
this pattern corresponds to the o kay gle phoneme or hey goo gle phoneme structure of the words
our experiments for cross layer attention scores are learned from the second to last layer
fig
visualized attention weights for different pooling methods
in each image axis is time and y axis is for different utterances in a training batch
no pooling sliding window maxpooling where window size is and step is global top k ing where k
when we apply sliding window maxpooling or global top k maxpooling the attention weights are much larger at the near end of the utterance which is easy to understand the lstm has lated more information at the near end than at the beginning thus is more condent to produce the d vector

conclusions in this paper we experimented with different attention mechanisms for our keyword based text dependent speaker verication system
from our experimental results the best practice is to use a shared parameter non linear scoring function use a layer attention connection to the last layer output of the lstm and apply a sliding window maxpooling on the attention weights
after combining all these best practices we improved the eer of our baseline lstm model from
to
which is a ative improvement
the same attention mechanisms especially the ones using shared parameter scoring functions could potentially be used to improve text independent speaker verication models and speaker diarization systems
no sliding window global top k maxpooling hasim sak andrew senior and francoise beaufays long short term memory recurrent neural network architectures for large scale acoustic modeling in fifteenth annual conference of the international speech communication association
li wan quan wang alan papir and ignacio lopez moreno generalized end to end loss for speaker verication arxiv preprint

quan wang carlton downey li wan philip manseld and ignacio lopez moreno speaker diarization with lstm arxiv preprint


references pinsky now yury home
blog
google products assistant tomato google home now supports multiple users
tomato tomahto
supports multiple google users mihai matei voice match will google low to
androidheadlines
voice will allow google home to recognize your voice
html
recognize home your voice najim dehak patrick j kenny reda dehak pierre mouchel and pierre ouellet front end factor analysis for speaker verication ieee transactions on audio speech and language processing vol
no
pp

daniel garcia romero and carol y espy wilson analysis of i vector length normalization in speaker recognition systems
in interspeech pp

yann lecun yoshua bengio and geoffrey hinton deep learning nature vol
no
pp

johan rohdin anna silnova mireia diez oldrich plchot end to end dnn based arxiv pavel matejka and lukas burget speaker recognition inspired by i vector and plda preprint

georg heigold ignacio moreno samy bengio and noam shazeer end to end text dependent speaker verication in acoustics speech and signal processing icassp ieee international conference on
ieee pp

sepp hochreiter and jurgen schmidhuber long short term memory neural computation vol
no
pp

guoguo chen carolina parada and georg heigold footprint keyword spotting using deep neural networks in acoustics speech and signal processing icassp ieee international conference on
ieee pp

rohit prabhavalkar raziel alvarez carolina parada preetum nakkiran and tara n sainath automatic gain control and multi style training for robust small footprint keyword spotting with deep neural networks in acoustics speech and signal processing icassp ieee international conference on
ieee pp

jan k chorowski dzmitry bahdanau dmitriy serdyuk kyunghyun cho and yoshua bengio attention based els for speech recognition in advances in neural information processing systems pp

minh thang luong hieu pham and christopher d manning effective approaches to attention based neural machine lation arxiv preprint

kelvin xu jimmy ba ryan kiros kyunghyun cho aaron courville ruslan salakhudinov rich zemel and yoshua gio show attend and tell neural image caption generation with visual attention in international conference on machine learning pp

florian schroff dmitry kalenichenko and james philbin facenet a unied embedding for face recognition and tering in proceedings of the ieee conference on computer vision and pattern recognition pp


