a m l c
s c v
v i x r a a hierarchical end to end model for jointly improving text summarization and sentiment classication shuming xu junyang xuancheng key lab of computational linguistics school of eecs peking university of foreign languages peking university shumingma xusun linjunyang
edu
abstract text summarization and sentiment classication both aim to capture the main ideas of the text but at different levels
text summarization is to describe the text within a few sentences while sentiment classication can be regarded as a special type of summarization which summarizes the text into a even more abstract fashion i
e
a sentiment class
based on this idea we propose a hierarchical to end model for joint learning of text tion and sentiment classication where the ment classication label is treated as the further summarization of the text summarization put
hence the sentiment classication layer is put upon the text summarization layer and a archical structure is derived
experimental results on amazon online reviews datasets show that our model achieves better performance than the strong baseline systems on both abstractive tion and sentiment classication
introduction text summarization and sentiment classication are two portant tasks in natural language processing
text rization aims at generating a summary with the major points of the original text
compared with extractive summarization which selects a subset of existing words in the original text to form the summary abstractive summarization builds an ternal semantic representation and then uses natural language generation techniques to create a summary that is closer to what a human might express
in this work we mainly focus on the abstractive text summarization
sentiment tion is to assign a sentiment label to determine the attitude or the opinion inside the text
it is also known as opinion ing deriving the opinion or the attitude of a speaker
both text summarization and sentiment classication aim at mining the main ideas of the text
text summarization describes the text with words and sentences in a more specic way while ment classication summarizes the text with labels in a more abstractive way
most of the existing models are built for either rization or classication
for abstractive text tion the most popular model is the sequence to sequence model sutskever et al
rush et al
where erating a short summary for the long source text can be garded as a mapping between a long sequence and a short sequence
the model consists of an encoder and a coder
the encoder encodes the original text into a tent representation and the decoder generates the summary
some recent abstractive summarization models are the ants of the sequence to sequence model chopra et al
see et al

for sentiment classication most of the cent work uses the neural network architecture kim tang et al
such as lstm or cnn to generate a text embedding and use a multi layer perceptron mlp to dict the label from the embedding
previous hole and takalikar mane et al
proposes the models to produce both the summaries and the sentiment labels
however these models train the summarization part and the sentiment classication part independently and require rich craft the titov and mcdonald sentiment lerman et al
which aim at extracting the sentences with a certain sentiment class from the original texts
these work only focuses on the summarization and does not improve the sentiment classication
there are also some work about summarization features
some work in this work we explore a rst step towards improving both text summarization and sentiment classication within an end to end framework
we propose a hierarchical end end model which consists of a summarization layer and a sentiment classication layer
the summarization layer presses the original text into short sentences and the ment classication layer further summarizes the texts into a sentiment class
the hierarchical structure establishes a close bond between text summarization and sentiment tion so that the two tasks can improve each other
after pressing the texts with summarization it will be easier for the sentiment classier to predict the sentiment labels of the shorter text
besides text summarization can point out the important and informative words and remove the redundant and misleading information that is harmful to predict the timent
the sentiment classication can provide a more nicant supervision signal for text summarization and guides the summarization component to capture the sentiment dency of the original text which can improve the coherence between the short text and the original text
we evaluate our proposed model on amazon online views datasets
experimental results show that our model achieves better performance than the strong baseline systems on both summarization and sentiment classication
the contributions of this paper are listed as follows we treat the sentiment classication as a special type of summarization and perform sentiment classication and text summarization using a unied model
we propose a multi view attention to obtain different representation of the texts for summarization and timent classication
experimental results shows that our model outperforms the strong baselines that train the summarization and sentiment classication separately
proposed model in this section we introduce our proposed model in details
in section
we give the problem formulation
we explain the overview of our proposed model in section

then we introduce each components of the model from section
to section

finally section
gives the overall loss tion and the training methods

problem formulation given an online reviews dataset that consists of n data ples the i th data sample xi yi li contains an original text xi a summary yi and a sentiment label li
both the original content xi and the summary yi are sequences of words li yi xi xi


xi


yi xi yi where li and mi denote the number of words in the quences xi and yi respectively
the label li


k denotes the sentiment attitude of the original content xi from the lowest rating to the highest rating k
mi the model is applied to learn the mapping from the source text to the target summary and the sentiment label
for the purpose of simplicity y l is used to denote each data pair in the rest of this section where is the word sequence of an original text y is the word sequence of the corresponding summary and l is the corresponding sentiment label

model overview figure shows the architecture of our model
our model consists of three components which are the text encoder the summary decoder and the sentiment classier
the text coder compresses the original text into the context ory h with a bi directional lstm
the summary decoder is a uni directional lstm which then generates a summary tor and a sentiment vector sequentially with the tention mechanism by querying the context memory
the summary vectors are used to generate the summary with a word generator
the sentiment vectors of all time steps are collected and then fed into the sentiment classier to dict the sentiment label
in order to capture the context mation of the original text we use the highway mechanism to feed the the context memory as part of the input of the five stars sentiment classifier nice magnetic toy highway summary decoder text encoder the toy bought it figure the overview of our model
classier
therefore the classier predicts the label ing to the sentiment vectors of the summary decoder and the context memory of the text encoder

text encoder the goal of the source text encoder is to provide a series of dense representation of the original text for the decoder and in our model the original text encoder is a the classier
bi directional long short term memory network bilstm which produces the context memory


hl from the source text xt where and are the forward and the backward functions of lstm for one time step and are the forward and the backward hidden outputs respectively is the input at the t th time step and l is the number of words in sequence
although convolutional neural network cnn is also an alternative choice for the encoder bilstm is more popular for the sequence to sequence learning of text generation tasks including abstractive text summarization
besides according to our experiments bilstm achieves better performance in sentiment classication on our benchmark datasets
we give the details of the comparison of cnn and bilstm in tion

summary decoder with multi view attention the goal of the summary decoder is to generate a series of summary words and provides the summary information for the sentiment classier
in our model the summary decoder consists of a uni directional lstm a multi view attention mechanism and a word generator
the lstm rst generates the hidden output st conditioned on the historical information of the generated summary st where f is the function of lstm for one time step and is the last generated words at t th time step
given the hidden output st we implement a multi view tention mechanism to retrieval the summary information and the sentiment information from the context memory h of the original text
the motivation of the multi view attention is that the model should focus on different part of the original text for summarization and classication
for tion the attention mechanism should focus on the informative words that describe the main points best
for sentiment sication the attention mechanism should focus on the words that contains the most sentimental tendency such as great bad and so on
in implementation the multi view attention generates a summary vector for summarization t tihi n x hi n hj ti p hi tanh st t wthi where wt is a trainable parameter matrix
similar to the mary vector the sentiment vector is also generated with the attention mechanism following equation and but has different trainable parameters
the multi view attention can be regarded as two independent global attentions to learn to focus more on the summary aspect or the sentiment aspect
the word generator is used t to compute the probability distribution of the output words at t th time step given the summary vector sof where wg and bg are parameters of the generator
the word with the highest probability is emitted as t th word of the erated summary
t bg
summary aware sentiment classier after decoding the words until the end of the summary the model collects the sentiment vectors of all time step


m then we concatenate the summary sentiment vectors and the original text representation h and perform a pooling operation to obtain a sentiment context vector r which we denote as a highway operation in figure r m


hl


where denotes the operation of concatenation along the rst dimension m is the number of words in the summary and l is the number of words in the original text
the ment context vector is then fed into the classier to compute the probability distribution of the sentiment label
the classier is a two layer feed forward network with relu as the activation function
the label with the highest probability is the predicted sentiment label

overall loss function and training the loss function consists of two parts which are the cross entropy loss of summarization and that of sentiment cation ls x t yt log lc l log where yt and l are the ground truth of words and labels and and are the probability distribution of words and labels computed by equation
we jointly minimize the two losses with adam kingma and ba optimizer l ls lc where is a hyper parameter to balance two losses
we set
in this work
experiments in this section we evaluate our model on the amazon line review dataset which contains the online reviews maries and sentiment labels
we rst introduce the datasets evaluation metrics and experimental details
then we pare our model with several popular baseline systems
nally we provide the analysis and the discussion of our model

datasets amazon snap review dataset snap this dataset is part of stanford network analysis project and is vided by he and mcauley
the dataset consists of views from amazon and contains product reviews and data from amazon including
million reviews spanning may july
it includes review content product user information ratings and summaries
we pair each view content with the corresponding summary and sentiment label
we select three domains of product reviews to construct three benchmark datasets which are toys games sports outdoors and movie tv
we select the rst ples of each dataset as the validation set the following samples as the test set and the rest as the training set

evaluation metric for abstractive summarization our evaluation metric is rouge score lin and hovy which is popular for summarization evaluation
the metrics compare an ically produced summary with the reference summaries by computing overlapping lexical units including unigram gram trigram and longest common subsequence lcs
lowing previous work rush et al
hu et al
we use unigram bi gram and l lcs as the evaluation metrics in the reported tal results
for sentiment classication the evaluation metric is label accuracy
we evaluate the accuracy of both ve class
stanford
edu data web amazon
html sentiment of which the sentiment is classied into class and two class sentiment of which the sentiment is either itive or negative

experimental details model parameters the vocabularies are extracted from the training sets and the source contents and the summaries share the same ies
we tune the hyper parameters based on the performance on the validation sets
we limit the vocabulary to most frequent words pearing in the training set
we set the word embedding and the hidden size to and for toys sports and movies datasets respectively
the word embedding is dom initialized and learned from scratch
the encoder is a single layer bidirectional lstm the decoder is a single layer unidirectional lstm and the classier is a two layer forward network with a hidden dimension
the batch size is and we use dropout with probability


for toys sports and movies datasets respectively
model training we use the adam kingma and ba optimization method to train the model
for the hyper parameters of adam optimizer we set the learning rate
two tum parameters
and
respectively and
following sutskever et al
we train the model for a total of epochs and start to halve the ing rate every half epoch after epochs
we clip the ents pascanu et al
to the maximum norm of


baselines for abstractive summarization our baseline model is the sequence to sequence model for abstractive summarization following the previous work et al

we denote the sequence to sequence model without the attention anism as and that with the attention mechanism as att
for text classication we compare our model with two baseline models bilstm and cnn
for the two baseline models the bilstm model uses a bidirectional lstm with the dimension of in each direction and uses max pooling across all lstm hidden states to get the sentence embedding vector and then uses an mlp output layer with hidden states to output the classication result
the cnn model uses the same scheme but substitutes bilstm with layer of convolutional network
during training we use
dropout on the mlp
we use adam as the optimizer with a ing rate of
and a batch size of
for bilstm we also clip the norm of gradients to be

we searched parameters in a wide range and nd the aforementioned set of hyperparameters yield the highest accuracy
the above baseline models only exploit part of the tated data either summaries or sentiment labels
for fairer comparison we also implement a joint model of att and bilstm att bilstm and both the annotated bels of summaries and sentiments are used to train this line model
we compare our model with this model in order to analyze the improvements of our model given exactly the toys games et al
att et al
att bilstm hssc this work rg l











sports outdoors et al
att et al
att bilstm hssc this work rg l











movie tv et al
att et al
att bilstm hssc this work rg l











table comparison between our model and the sequence sequence baseline for abstractive summarization on the amazon snap test sets
the test sets include three domains toys gamse sports outdoors and movie tv
and rg l note and rouge l respectively
same annotated data
in this baseline model att and stm share the same encoder and the att produces the summary with a lstm decoder while the bilstm predicts the sentiment label with a mlp
we tune the hyper parameter on the validation set
we set the word embedding and the hidden size to and
the batch size is and the dropout rate is p


for toys sports and movies datasets respectively

results we denote our hierarchical summarization and sentiment classication model as hssc
abstractive summarization first we compare our model with the sequence to sequence baseline on the amazon snap test sets
we report the rouge score of our model and the baseline models on the test sets
as shown in table our hssc model has a large margin over both and att models on all of the three test sets which shows that the supervision of the ment labels improves the representation of the original text
moreover given exactly the same annotated data summary sentiment label our hssc model still has an improvement over the att bilstm baseline which indicates that hssc learns a better representation for summarization
all hssc achieves the best performance in terms of and rouge l over the three baseline models on the three test sets
the summarization task on the online review texts is much more difcult and complicate so the rouge scores on the snap dataset are lower than other summarization datasets such as duc
the documents in duc datasets are originally from news website so the texts are formal and the summaries in duc are manually selected and well written
the snap dataset is constructed with the reviews on the amazon and toys games cnn bilstm bilstm att hssc this work sports outdoors cnn bilstm bilstm att hssc this work movie tv cnn bilstm bilstm att hssc this work class



class



class



class



class



class



table comparison between our model and the sequence sequence baselines for sentiment classication on the amazon snap test sets
the test sets include three domains toys games sports outdoors and movie tv
class and class denote the accuracy of ve class sentiment and two class sentiment tively
both the original reviews and the corresponding summaries are informal and full of noise
sentiment classication we compare our model with two popular sentiment tion methods which are cnn and bilstm on the amazon snap test sets
we report the accuracy of ve grained timent and two class sentiment on the test sets
as shown in table has a slightly improvement over the cnn baseline showing that bilstm has a better performance to represent the texts on these datasets
therefore we select bilstm as the encoder of our model
hssc obtains a better performance over the two widely used baseline models on all of the test sets mainly because of the benet of more labeled data and better representation
what s more hssc forms the att bilstm baseline showing that the formation from summary decoder helps to predict the ment labels
overall hssc achieves the best performance in terms of class accuracy and class accuracy over the three baseline models on the three test sets
we have conducted signicance tests based on t test
the signicance tests suggest that hssc has a very signicant improvement over all of the baselines with p
in all of rouge metrics for summarization in three benchmark datasets p
for sentiment classication in both toys games and movies tv datasets and p
for ment classication in the sports outdoors datasets

ablation study in order to analyze the effect of each components we remove the components of multi view and highway in order and uate the performance of the rest model
we rst remove the multi view attention
as shown in table the model out multi view attention has a drop of performance on both class accuracy and rouge l
it can be concluded that the toys games multi view highway hssc full model class rg l





sports outdoors multi view highway hssc full model class rg l





movie tv multi view highway hssc full model class rg l





table ablation study
class denotes the accuracy of ve grained sentiment and rg l denotes rouge l for summarization
multi view attention improves the performance of both stractive summarization and sentiment classication
we ther remove the highway part and nd the highway nent benets not only the sentiment classication bot also the abstractive summarization
the benet mainly comes from the fact that the gradient of the sentiment classier can be directly propagated to the encoder so that it learns a better representation of the original text for both classication and summarization

visualization of multi view attention as shown in table we present the heatmap of the tion scores of three examples
the multi view attention lows the model to represent the text from the sentiment view and from the summary view
in order to analyze whether the multi view attention captures the sentiment information and the summary information we give the heatmap of the sentiment view attention and the summary view attention spectively
we take the average of the attention scores in the decoder outputs at all time steps and mark the high scores with deep color and the low scores with light color
from the table we conclude that the sentiment view attention cuses more on the sentimental words e

best ful great fun and comfortable
the summary view attention concentrates on the informative words that best scribes the opinion of the authors e

i think that this is one of the best movie and a great book very fun
moreover the sentiment view attention focuses more on the individual words while the summary view pays more attention on the word sequences
besides the sentiment view attention and the summary view attention share the focus on the tive words showing the benet from the multi view attention
related work rush et al
rst proposes an abstractive based marization model which uses an attentive cnn encoder to compress texts and a neural network language model to erate summaries
chopra et al
explores a recurrent structure for abstractive summarization
to deal with of vocabulary problem nallapati et al
proposes a i saw this movie times in the theater and i think that this is one of the best movies ever made and the best movie made about christ and his passion
god bless all those responsible for the creation of this powerful lm
my daughter who is now years old received this as a christmas gift when she was
it has been ready many times and since been passed along to my son who is now
my children enjoy the tactile quality of the monkeys faces
it is helpful learning counting when there is something they can feel
i have always enjoyed reading the sing song story
it does not take long to read and after all these years i pretty much have it memorized
a great book very fun
this mattress is too narrow to be comfortable
you t on it ne but because of the air i found that it was a balancing act to switch positions
i tried more and less air to no effect
i think if you sleep on your back and stay in that position it would be ne but unfortunately that is not how i sleep
the strong vinyl smell does go away after airing out though
a sentiment view of the original text
i saw this movie times in the theater and i think that this is one of the best movies ever made and the best movie made about christ and his passion
god bless all those responsible for the creation of this powerful lm
my daughter who is now years old received this as a christmas gift when she was
it has been ready many times and since been passed along to my son who is now
my children enjoy the tactile quality of the monkeys faces
it is helpful learning counting when there is something they can feel
i have always enjoyed reading the sing song story
it does not take long to read and after all these years i pretty much have it memorized
a great book very fun
this mattress is too narrow to be comfortable
you t on it ne but because of the air
i found that it was a balancing act to switch positions
i tried more and less air to no effect
i think if you sleep on your back and stay in that position it would be ne but unfortunately that is not how i sleep
the strong vinyl smell does go away after airing out though
summary view of the original text
table visualization of multi view attention
above is the heatmap of the sentiment view attention and below is the heatmap of the summary view attention
deeper colors means higher attention scores
generator pointer model so that the decoder is able to erate words in source texts
gu et al
also solves this issue by incorporating copying mechanism allowing parts of the summaries to be copied from the source contents
see et al
further discusses this problem and incorporates the pointer generator model with the coverage mechanism
hu et al
builds a large corpus of chinese social dia short text summarization
chen et al
introduces a distraction based neural model which forces the attention mechanism to focus on the difference parts of the source puts
ma et al
proposes a neural model to improve the semantic relevance between the source contents and the predicted summaries
there are some work concerning with both summarization and sentiment classication
hole and takalikar and mana et al
propose the models to produce both the summaries and the sentiment labels
however these els train the summarization part and the sentiment tion part independently and require rich hand craft features
some work has improved the summarization with the help of classication
cao et al
proposes a model to train the summary generator and the text classier jointly but only improves the performance of the text summarization
titov and mcdonald proposes a sentiment summarization method to extract the summary from the texts given the ment class
lerman et al
builds a new summarizer by training a ranking svm model over the set of human ence judgments and improves the performance of sentiment summarization
different from all of these works our model improves both text summarization and sentiment tion and does not require any hand craft features
conclusions in this work we propose a model to generate both the timent labels and the human like summaries hoping to marize the opinions from the coarse grained sentiment labels to the ne grained word sequences
we evaluate our proposed model on several online reviews datasets
experimental sults show that our model achieves better performance than the baseline systems on both abstractive summarization and sentiment classication
acknowledgements this work was supported in part by national natural science foundation of china no
national high nology research and development program of china program no
and the national thousand young talents program
xu sun is the corresponding author of this paper
references cao et al
ziqiang cao wenjie li sujian li and furu wei
improving multi document summarization via text classication
in aaai pages
chen et al
qian chen xiaodan zhu zhenhua ling si wei and hui jiang
distraction based neural networks for modeling documents
in ijcai new york ny july
aaai
cheng and lapata jianpeng cheng and mirella pata
neural summarization by extracting sentences and words
in acl
chopra et al
sumit chopra michael auli and alexander m
rush
abstractive sentence summarization with attentive recurrent neural networks
in naacl hlt pages
gu et al
jiatao gu zhengdong lu hang li and incorporating copying mechanism in victor o
k
li
sequence to sequence learning
in acl
he and mcauley ruining he and julian mcauley
ups and downs modeling the visual evolution of ion trends with one class collaborative ltering
in www pages
hole and takalikar vikrant hole and mukta likar
real time tweet summarization and sentiment ysis of game tournament
international journal of science and research
et al
baotian hu qingcai chen and fangze zhu
lcsts a large scale chinese short text tion dataset
in emnlp pages
kim yoon kim
convolutional neural networks for in emnlp pages sentence classication

kingma and ba diederik p
kingma and jimmy ba
adam a method for stochastic optimization
corr

lerman et al
kevin sentiment goldensohn and ryan t
mcdonald
summarization evaluating and learning user preferences
in eacl pages
lerman sasha lin and hovy chin yew lin and eduard h
hovy
automatic evaluation of summaries using n gram occurrence statistics
in hlt naacl
et al
shuming ma xu sun jingjing xu houfeng wang wenjie li and qi su
improving tic relevance for sequence to sequence learning of chinese social media text summarization
in acl pages
et al
shuming ma xu sun wei li sujian li wenjie li and xuancheng ren
query and output ating words by querying distributed word representations for paraphrase generation
in naacl
mane et al
vinod l mane suja s panicker and vidya b patil
summarization and sentiment analysis from user health posts
in pervasive computing icpc international conference on pages
ieee
nallapati et al
ramesh nallapati bowen zhou ccero nogueira dos santos c aglar gulcehre and bing xiang
abstractive text summarization using sequence sequence rnns and beyond
in conll pages
pascanu et al
razvan pascanu tomas mikolov and yoshua bengio
on the difculty of training recurrent ral networks
in icml pages
rush et al
alexander m
rush sumit chopra and jason weston
a neural attention model for abstractive sentence summarization
in emnlp pages
see et al
abigail see peter j
liu and pher d
manning
get to the point summarization with in acl pages pointer generator networks

sun et al
xu sun xuancheng ren shuming ma and houfeng wang
meprop sparsied back propagation for accelerated deep learning with reduced overtting
in icml pages
sun et al
xu sun bingzhen wei xuancheng ren and shuming ma
label embedding network learning label representation for soft training of deep networks
corr

sutskever et al
ilya sutskever oriol vinyals and quoc v
le
sequence to sequence learning with neural networks
in nips pages
takase et al
sho takase jun suzuki naoaki okazaki tsutomu hirao and masaaki nagata
neural headline generation on abstract meaning representation
in emnlp pages
tang et al
duyu tang bing qin and ting liu
ument modeling with gated recurrent neural network for in emnlp pages sentiment classication

titov and mcdonald ivan titov and ryan t
donald
a joint model of text and aspect ratings for ment summarization
in acl pages
xu et al
jingjing xu xu sun xuancheng ren dp gan junyang lin binzhen wei and wei li
diversity promoting generative adversarial network for corr generating informative and diversied text


xu et al
jingjing xu xu sun qi zeng xiaodong zhang xuancheng ren houfeng wang and wenjie li
unpaired sentiment to sentiment translation a cycled inforcement learning approach
in acl

