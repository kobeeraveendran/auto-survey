deep communicating agents for abstractive summarization asli antoine xiaodong and yejin research g
allen school of computer science engineering university of washington ai research institute for articial intelligence
com xiaodong

com antoineb
washington
edu g u a l c
s c v
v i x r a abstract we present deep communicating agents in an encoder decoder architecture to dress the challenges of representing a long document for abstractive tion
with deep communicating agents the task of encoding a long text is divided across multiple collaborating agents each in charge of a subsection of the input text
these encoders are connected to a gle decoder trained end to end using forcement learning to generate a focused and coherent summary
empirical results demonstrate that multiple communicating encoders lead to a higher quality summary compared to several strong baselines cluding those based on a single encoder or multiple non communicating encoders
introduction we focus on the task of abstractive tion of a long document
in contrast to extractive summarization where a summary is composed of a subset of sentences or words lifted from the put text as is abstractive summarization requires the generative ability to rephrase and restructure sentences to compose a coherent and concise mary
as recurrent neural networks rnns are capable of generating uent language variants of encoder decoder rnns sutskever et al
bahdanau et al
have shown promising sults on the abstractive summarization task rush et al
nallapati et al

the fundamental challenge however is that the strong performance of neural models at encoding short text does not generalize well to long text
the motivation behind our approach is to be able to dynamically attend to different parts of the input work done while author was at microsoft research figure illustration of deep communicating agents presented in this paper
each agent a and encodes one paragraph in multiple layers
by passing new sages through multiple layers the agents are able to ordinate and focus on the important aspects of the input text
to capture salient facts
while recent work in marization addresses these issues using improved attention models chopra et al
pointer networks with coverage mechanisms see et al
and coherence focused training objectives paulus et al
jaques et al
an tive mechanism for representing a long document remains a challenge
simultaneous work has investigated the use of deep communicating agents sukhbaatar et al
for collaborative tasks such as logic puzzles foerster et al
visual dialog das et al
and reference games lazaridou et al

our work builds on these approaches to propose the rst study on using communicating agents to encode long text for summarization
the key idea of our model is to divide the hard task of encoding a long text across multiple orating encoder agents each in charge of a ent subsection of the text figure
each of these agents encodes their assigned text independently and broadcasts their encoding to others allowing agents to share global context information with one another about different sections of the ment
all agents then adapt the encoding of their aroma

howtouseit massageadabofaromatherapeuticbalmoroil messages aagent b figure multi agent encoder decoder overview
each agent a encodes a paragraph using a local encoder followed by multiple contextual layers with agent communication through concentrated messages at each layer
a a are condensed into agent context c communication is illustrated in figure
the word context vectors ct t
agent specic generation probabilities pt a enable voting for the suitable out of vocabulary words e

yen in the nal distribution
assigned text in light of the global context and peat the process across multiple layers ing new messages at each layer
once each agent completes encoding they deliver their information to the decoder with a novel contextual agent tion figure
contextual agent attention enables the decoder to integrate information from multiple agents smoothly at each decoding step
the work is trained end to end using self critical forcement learning rennie et al
to ate focused and coherent summaries
empirical results on the cnn dailymail and new york times datasets demonstrate that ple communicating encoders lead to higher quality summaries compared to strong baselines ing those based on a single encoder or multiple non communicating encoders
human evaluations indicate that our model is able to produce more cused summaries
the agents gather salient mation from multiple areas of the document and communicate their information with one another thus reducing common mistakes such as missing key facts repeating the same content or including unnecessary details
further analysis reveals that our model attains better performance when the coder interacts with multiple agents in a more anced way conrming the benet of representing a long document with multiple encoding agents
model we extend the commnet model of sukhbaatar et al
for sequence generation
notation each document d is a sequence of paragraphs xa which are split across multiple coding agents

m e

encodes the rst paragraph the second paragraph so on
each paragraph is a quence of i words
we construct a v ulary from the training documents from the most frequently appearing words
each word wa i is embedded into a n dimensional vector ea i
all w variables are linear projection matrices

multi agent encoder each agent encodes the word sequences with the following two stacked encoders
local encoder the rst layer is a local encoder of each agent a where the tokens of the sponding paragraph xa are fed into a single layer bi directional lstm blstm producing the cal encoder hidden states h i h i i rh h h i h h i where h is the hidden state dimensionality
the local encoderb lstmsource tiredofcountingb lstmb lstmsource youdonthaveb lstmwordattentionb lstmsource melatoninsupplement b lstmword context vector agentattention word context vector word context vector agent context vector fragrances start thatmakeyoufeeldecoderlstm vocabulary distributionazooazoocalmyen final distributionagent attention agent attention contextual encoderlayers b lstmb lstmb lstm output of the local encoder layer is fed into the contextual encoder
contextual encoder our framework enables agent communication cycles across multiple coding layers
the output of each contextual coder is an adapted representation of the agent s encoded information conditioned on the tion received from the other agents
at each layer

k each agent a jointly encodes the mation received from the previous layer see ure
each cell of the contextual layer is a blstm that takes three inputs the hidden h rh states from the adjacent lstm cells rh the hidden state from the and the message vector from other h or ous layer agents and outputs rh i i h i h i h i h h i i h i where

i indicates the index of each token in the sequence
the message received by any agent a in layer k is the average of the outputs of the other agents from layer k m m i where m i is the last hidden state output from the kth contextual layer of each agent where m a
here we take the average of the messages ceived from other encoder agents but a parametric function such as a feed forward model or an tion over messages could also be used
the message is projected with the agent previous encoding of its document i vt i where and are learned parameters shared by every agent
equation combines the information sent by other agents with the context of the current token from this paragraph
this yields different features about the current context in relation to other topics in the source document
at each layer the agent modies its tion of its own context relative to the information received from other agents and updates the mation it sends to other agents accordingly
figure multi agent encoder message passing
agents b and c transmit the last hidden state output i of the current layer k as a message which are passed through an average pool eq

the receiving agent a uses the new message as additional input to its next layer
a
decoder with agent attention the output from the last contextual encoder layer of each agent a i i which is a sequence of hidden state vectors of each token i is sent to the decoder to calculate word attention tions
we use a single layer lstm for the decoder and feed the last hidden state from the rst agent as the initial state
at each time step t the decoder predicts a new word in the summary wt and computes a new state st by attending to relevant input context provided by the agents
the decoder uses a new hierarchical attention mechanism over the agents
first a word attention distribution a bahdanau et al
is puted over every token a i i for each agent a lt a a where is the attention over all tokens in a paragraph xa and are learned rameters
for each decoding step t a new decoder context is calculated for each agent ct a a a i i which is the weighted sum of the encoder hidden states of agent a
each word context vector sents the information extracted by the agent from the paragraph it has read
here the decoder has to decide which information is more relevant to the lstmlstm current decoding step t
this is done by weighting each context vector by an agent attention yielding the document global agent attention distribution gt see figure that includes words in the document that are sidered out of vocabulary oov
a probability distribution over the extended vocabulary is puted for each agent gt p pt ap pt a w where and are learned and gt is a soft selection over m agents
then we compute the agent context vector c t t c agt act a t rh is a xed length vector the agent context c encoding salient information from the entire ument provided by the agents
it is then nated with the decoder state st and fed through a multi layer perception to produce a vocabulary distribution over all vocabulary words at time t p c t to keep the topics of generated sentences intact it is reasonable that the decoder utilize the same agents over the course of short sequences e

within a sentence
because the decoder is signed to select which agent to attend to at each time step we introduce contextual agent attention caa to prevent it from frequently switching tween agents
the previous step s agent attention c is used as additional information to the coding step to generate a distribution over words p c t c
multi agent pointer network similar to see et al
we allow for copying candidate words from different paragraphs of the document by computing a generation probability value for each agent pt a at each timestep t using the context vector ct a decoder state st and decoder input yt pt a a vt vt where is a learned scalar is the truth predicted output depending on the ing testing time
the generation probability termines whether to generate a word from the cabulary by sampling from p or ing a word from the corresponding agent s input paragraph xa by sampling from its attention bution a
this produces an extended vocabulary where ut a w is the sum of the attention for all instances where w appears in the source ment
the nal distribution over the extended cabulary from which we sample is obtained by weighting each agent by their corresponding agent attention values p a gt ap in contrast to a single agent baseline see et al
our model allows each agent to vote for ferent oov words at time equation
in such a case only the word that is relevant to the generated summary up to time t is collaboratively voted as a result of agent attention probability gt a
mixed objective learning to train the deep communicating agents we use a mixed training objective that jointly optimizes multiple losses which we describe below
mle our baseline multi agent model uses imum likelihood training for sequence tion
given y y t as the truth output sequence human summary word quences for a given input document d we mize the negative log likelihood of the target word sequence


y lmle


y d semantic cohesion to encourage sentences in the summary to be informative without repetition we include a semantic cohesion loss to integrate sentence level semantics into the learning tive
as the decoder generates the output word quence


yt it keeps track of the end of sentence delimiter token
indices
the den state vectors at the end of each sentence q


q where are used to compute the cosine similarity between two secutively generated sentences
to minimize the similarity between end of sentence hidden states we dene a semantic cohesion loss lsem q the nal training objective is then lmle sem lmle lsem where is a tunable hyperparameter
reinforcement learning rl loss policy gradient methods can directly optimize discrete target evaluation metrics such as rouge that are non differentiable paulus et al
jaques et al
pasunuru and bansal wu et al

at each time step the word generated by the model can be viewed as an action taken by an rl agent
once the full sequence y is generated it is compared against the ground truth sequence y to compute the reward
our model learns using a self critical training approach rennie et al
which learns by ploring new sequences and comparing them to the best greedily decoded sequence
for each training example d two output sequences are generated y which is sampled from the probability tion at each time step


d and y the baseline output which is greedily generated by argmax decoding from



the training objective is then to minimize lrl


this loss ensures that with better exploration the model learns to generate sequences y that receive higher rewards compared to the baseline y creasing overall reward expectation of the model
mixed loss while training with only mle loss will learn a better language model this may not guarantee better results on global performance measures
similarly optimizing with only rl loss may increase the reward gathered at the expense of diminished readability and uency of the erated summary paulus et al

a nation of the two objectives can yield improved task specic scores while maintaining uency lmixed lrl where is a tunable hyperparameter used to ance the two objective functions
we pre train our models with mle loss and then switch to the mixed loss
we can also add the semantic cohesion loss term lmixed sem sem to analyze its impact in rl training
intermediate rewards we introduce based rewards as opposed to end of summary wards using differential rouge metrics to generating diverse sentences
rather than warding sentences based on the scores obtained at the end of the generated summary we compute cremental rouge scores of a generated sentence oq





sentences are rewarded for the increase in rouge they contribute to the full sequence suring that the current sentence contributed novel information to the overall summary
experimental setup datasets we conducted experiments on two marization datasets cnn dailymail nallapati et al
hermann et al
and new york times nyt sandhaus
we replicate the preprocessing steps of paulus et al
to tain the same data splits except that we do not anonymize named entities
for our dca models we initialize the number of agents before training and partition the document among the agents i
e
three agent three paragraphs
additional tails can be found in appendix a

training details during training and testing we truncate the article to tokens and limit the length of the summary to tokens for ing and tokens at test time
we distribute the truncated articles among agents for multi agent models preserving the paragraph and sentences as possible
for both datasets we limit the put and output vocabulary size to the most frequent tokens in the training set
we train with up to two contextual layers in all the dca models as more layers did not provide additional mance gains
we x
for the rl term in equation and
for the sem term in mle and mixed training
additional details are provided in appendix a

evaluation we evaluate our system using unigram recall bigram recall and rouge l longest common quence
we select the mle models with the lowest negative log likelihood and the models with the highest rouge l scores on a sample of validation data to evaluate on the test use pyrouge pypi
python
org pypi


model summarunner nallapati et al
graph based attention tan et al
pointer generator see et al
pointer generator coverage see et al
controlled summarization with xed values fan et al
rl with intra attention paulus et al
with intra et al
mle pgen no comm agent our pgen no comm agent our pgen no comm agent our dca pgen no comm agents dca mpgen with comm agents dca mpgen with comm with caa agents dca mpgen with comm with caa agents



























rouge l













table comparison results on the cnn daily mail test set using the variants of rouge
best model models are bolded
model ml no intra attention paulus et al
rl no intra attention paulus et al
no intra et al
mle pgen no comm agent our pgen no comm agent our pgen no comm agent our dca pgen no comm agents dca mpgen with comm agents dca mpgen with comm with caa agents dca mpgen with comm with caa agents



















rouge l









table comparison results on the new york times test set using the variants of rouge
best model models are bolded
set
at test time we use beam search of width on all our models to generate nal predictions
baselines we compare our dca models against previously published models summarunner nallapati et al
a graph based attentional neural model tan et al
an rnn based tractive summarizer that combines abstractive tures during training pointer networks with and without coverage see et al
rl based training for summarization with intra decoder tention paulus et al
and controllable stractive summarization fan et al
which allows users to dene attributes of generated maries and also uses a copy mechanism for source entities and decoder attention to reduce repetition
ablations we investigate each new component of our model with a different ablation producing seven different models
our rst three ablations are a single agent model with the same local coder context encoder and pointer network tectures as the dca encoders trained with mle loss the same model trained with additional semantic cohesion sem loss and the same model as the but trained with a mixed loss and end of summary rewards
the rest of our models use agents and crementally add one component
first we add the semantic cohesion loss
then we add multi agent pointer networks mpgen and agent communication
finally we add tual agent attention caa and train with the mixed loss
all dca els use pointer networks
results
quantitative analysis we show our results on the cnn dailymail and nyt datasets in table and respectively
overall our and models with agent encoders pointer generation and nication are the strongest models on and
while weaker on rouge l than the rl model from paulus et al
the man evaluations in that work showed that their model received lower readability and relevance scores than a model trained with mle indicating the additional boost in rouge l was not lated with summary quality
this result can also account for our best models being more tive
our models use mixed loss not just to timize for sentence level structure similarity with the reference summary to get higher rouge as reward but also to learn parameters to improve semantic coherence promoting higher abstraction see table and appendix b for generated mary examples
model agent agent agent


rouge l





table comparison of multi agent models varying the number of agents using rouge results of model from table on cnn daily maily dataset
single vs
multi agents all multi agent models show improvements over the single agent lines
on the cnn dailymail dataset compared to mle published baselines we improve across all rouge scores
we found that the agent models generally outperformed both and agent models see table
this is in part cause we truncate documents before training and the larger number of agents might be more cient for multi document summarization
independent vs
communicating agents when trained on multiple agents with no communication the performance of our dca models is ilar to the single agent baselines and
with communication the biggest jump in rouge is seen on the cnn dailymail data indicating that the encoders can better identify the key facts in the input thereby avoiding unnecessary details
contextual agent attention caa compared to the model with no contextualized agent tion the model yields better rouge scores
the stability provided by the caa helps the decoder avoid frequent switches between agents that would dilute the topical signal captured by each encoder
repetition penalty as neurally generated maries can be redundant we introduced the mantic cohesion penalty and incremental rewards for rl to generate semantically diverse maries
our baseline model optimized together with sem loss improves on all rouge scores over the baseline
similarly our model trained with reinforcement learning uses sentence based intermediate rewards which also improves rouge scores across both datasets

human evaluations we perform human evaluations to establish that our model s rouge improvements are correlated with human judgments
we measure the municative multi agent network with contextual agent attention in comparison to a single agent network with no communication
we use the lowing as evaluation criteria for generated maries non redundancy fewer of the same ideas are repeated coherence ideas are pressed clearly focus the main ideas of the document are shared while avoiding superuous details and overall the summary effectively communicates the article s content
the focus and non redundancy dimensions help quantify the pact of multi agent communication in our model while coherence helps to evaluate the impact of the reward based learning and repetition penalty of the proposed models
evaluation procedure we randomly selected samples from the cnn dailymail test set and use workers from amazon mechanical turk as judges to evaluate them on the four criteria dened above
judges are shown the original document the ground truth summary and two model maries and are asked to evaluate each summary on the four criteria using a likert scale from worst to best
the ground truth and model summaries are presented to the judges in random order
each summary is rated by judges and the results are averaged across all examples and judges
we also performed a head to head evaluation more common in duc style evaluations and domly show two model generated summaries
we ask the human annotators to rate each summary on the same metrics as before without seeing the source document or ground truth summaries
results human evaluators signicantly prefer summaries generated by the communicating coders
in the rating task evaluators preferred the multi agent summaries to the single agent cases for all metrics
in the head to head evaluation mans consistently preferred the dca summaries to those generated by a single agent
in both the head to head and the rating evaluation the largest improvement for the dca model was on the cus question indicating that the model learns to generate summaries with more pertinent details by capturing salient information from later portions of the document
human mr turnbull was interviewed about his childhood and his political stance
he also admitted he planned to run for prime minister if tony abbott had been successfully toppled in february s leadership spill
the words primed minister were controversially also printed on the cover
single malcolm turnbull is set to feature on the front cover of the gq australia in a bold move that will no doubt set tors tongues wagging
posing in a suave blue suit with a pinstriped shirt and a contrasting red tie mr turnbull s condent demeanour is complimented by the bold confronting words printed across the page primed minister
malcolm turnbull was set to run for prime minister if tony abbott had been successfully toppled in ary s leadership spill
he is set to feature on the front cover of the liberal party s newsletter
multi human daphne selfe has been modelling since the fties
she has recently landed a new campaign with vans and other single stories
the year old commands a day for her work
daphne selfe shows off the collaboration between the footwearsuper brandand theetherealhigh street store with uncompromisinggrace
daphne said of the collection in which she appears with year old o dron the other stories collection that is featured in this story is truly relaxed and timeless with a modern twist
the shoes are then worn with pieces from the brands collection
multi daphne selfe has starred in the campaign for vans and other stories
the model appears with year old o dron other hair collection
she was still commanding a day for her work
table comparison of a human summary to best and multi agent model summaries and from cnn dailymail dataset
although single agent model generates a coherent summary it is less focused and contains more unnecessary details highlighed red and misses keys facts that the multi agent model successfully captures bolded
criteria non redundancy coherence focus overall score based ma sa







head to head sa ma table head to head and score based comparison of human evaluations on random subset of cnn dm dataset
sa single ma multi agent
indicates tistical signicance at p
for focus and p
for the overall

communication improves focus to investigate how much the multi agent models discover salient concepts in comparison to gle agent models we analyze rouge l scores based on the average attention received by each agent
we compute the average attention received by each agent per decoding time step for every generated summary in the cnn daily mail test corpus bin the document summary pairs by the attention received by each agent and average the rouge l scores for the summaries in each bin
figure outlines two interesting results
first summaries generated with a more distributed tention over the agents yield higher rouge l scores indicating that attending to multiple areas of the document allows the discovery of salient concepts in the later sections of the text
second if we use the same bins and generate summaries for the documents in each bin using the agent model the average rouge l scores for the single agent summaries are lower than for the figure the average rouge l scores for summaries that are binned by each agent s average attention when generating the summary see section

when the agents contribute equally to the summary the l score increases
responding multi agent summaries indicating that even in cases where one agent dominates the tention communication between agents allows the model to generate more focused summaries
qualitatively we see this effect in table where we compare the human generated maries against our best single agent model and our best multi agent model
model generates good summaries but does not capture all the facts in the human summary while is able to include all the facts with few extra details erating more relevant and diverse summaries
related work several recent works investigate attention nisms for encoder decoder models to sharpen the agent vs single agent vs single received by agentmultisinglemulti agent vs single











context that the decoder should focus on within the input encoding luong et al
vinyals et al
bahdanau et al

for example ong et al
proposes global and local tion networks for machine translation while ers investigate hierarchical attention networks for document classication yang et al
ment classication chen et al
and dialog response selection zhou et al

attention mechanisms have shown to be crucial for summarization as well rush et al
zeng et al
nallapati et al
and pointer in particular networks vinyals et al
help address redundancy and saliency in generated summaries cheng and lapata see et al
paulus et al
fan et al

while we share the same motivation as these works our work uniquely presents an approach based on commnet the deep communicating agent work sukhbaatar et al

compared to prior multi agent works on logic puzzles foerster et al
language learning lazaridou et al
mordatch and abbeel and starcraft games vinyals et al
we present the rst study in using this framework for long text generation
finally our model is related to prior works that address repetitions in generating long text
see et al
introduce a post trained coverage work to penalize repeated attentions over the same regions in the input while paulus et al
use intra decoder attention to punish generating the same words
in contrast we propose a new mantic coherence loss and intermediate based rewards for reinforcement learning to courage semantically similar generations
conclusions we investigated the problem of encoding long text to generate abstractive summaries and strated that the use of deep communicating agents can improve summarization by both automatic and manual evaluation
analysis demonstrates that this improvement is due to the improved ability of covering all and only salient concepts and taining semantic coherence in summaries
acknowledgements this research was supported in part by nsf and darpa under the cwc program through the aro
references dzmitry bahdanau kyunghyun cho and yoshua gio

neural machine translation by jointly learning to align and translate
in iclr
huimin chen maosong sun cunchao tu yankai lin and zhiyuan liu

neural sentiment tion with user and product attention
in emnlp
jianpeng cheng and mirella lapata

neural summarization by extracting sentences and words
in acl
sumit chopra michael auli and alexander m
rush

abstractive sentence summarization with tentive recurrent neural networks
in naacl
abhishek das satwik kottur jos m
f
moura and dhruv batra stefan lee

learning ative visual dialog agents with deep reinforcement learning
in cvpr
greg durrett taylor berg kirkpatrick and dan klein

learning based single document tion with compression and anaphoricity constraints
in acl
angela fan david grangier and michael auli
in
controllable abstractive summarization


j
foerster g
farquhar t
afouras n
nardelli and s
whiteson

counterfactual multi agent icy gradients
in

jakob n
foerster yannis m
assael nando de freitas and shimon whiteson

learning to nicate with deep multi agent reinforcement learning
in nips
karl moritz hermann tomas kocisky edward grefenstette lasse espeholt will kay mustafa leyman and phil blunsom

teaching chines to read and comprehend
in nips
kai hong michel marcus and ani nenkova

system combination for multi document rization
in emnlp
kai hong and ani nenkova

improving the estimation of word importance for news document summarization
in extended technical port
natasha jaques shixiang gu dzmitry bahdanau jose miguel hernandez lobato richard e
turner and douglas eck

sequence tutor tive ne tuning of sequence generation models with kl control
in icml
angeliki lazaridou nghia the pham and marco roni

towards multi agent based language learning
in

jiwei tan xiaojun wan and jianguo xiao

abstractive document summarization with a based attentional neural model
in acl
o
vinyals t
ewalds s
bartunov p
georgiev a
s
vezhnevets m
yeo a
makhzani h
kuttler j
agapiou and j
al
schrittwieser

craft ii a new challenge for reinforcement learning
in arxiv preprint

oriol vinyals meire fortunato and navdeep jaitly

pointer networks
in nips
oriol vinyals lukasz kaiser terry koo slav petrov ilya sutskever and geoffrey hinton

mar as a foreign language
in nips
yonghui wu mike schuster zhifeng chen quoc v le mohammad norouzi wolfgang macherey maxim krikun yuan cao qin gao klaus google s neural macherey et al

chine translation system bridging the gap between arxiv preprint human and machine translation


zichao yang diyi yang chris dyer xiaodong he alex smola and eduard hovy

hierarchical attention networks for document classication
in naacl
wenyuan zeng wenjie luo sanja fidler and raquel urtasun

efcient summarization with in arxiv preprint again and copy mechanism


xiangyang zhou daxiang dong hua wu shiqi zhao d yu r yan xuan liu and h tian

tiview response selection for human computer versation
in emnlp
junyi jessy li kapil thandani and amanda stent

the role of discourse units in near extractive summarization
in proceedings of the annual meeting of the special interest group on discourse and dialog
minh thang luong hieu pham and christopher d
manning

effective approaches to based neural machine translation
in emnlp
christopher d
manning mihai surdeanu john bauer jenny finkel steven j
bethard and david closky

the stanford corenlp natural in proceedings of guage processing toolkit
annual meeting of the association for tional linguistics system demonstrations
i
mordatch and p
abbeel

grounded compositional populations
in

emergence of language in multi agent ramesh nallapati feifei zhai and bowen zhou

summarunner a recurrent neural network based quence model for extractive summarization of ments
in association for the advancement of cial intelligence
ramakanth pasunuru and mohit bansal

forced video captioning with entailment rewards
in emnlp
romain paulus caiming xiong and richard socher

a deep reinforced model for abstractive marization
iclr
jeffrey pennington richard socher and pher d
manning

glove global vectors for word representation
in empirical methods in ral language processing
steven j rennie etienne marcheret youssef mroueh jarret ross and vaibhava goel

self critical arxiv sequence training for image captioning
preprint

alexander m
rush sumit chopra and jason weston

a neural attention model for abstractive tence summarization
in emnlp
evan sandhaus

new york times annotated pus
in linguistic data consortium philadelphia
abigale see peter j
liu and christopher manning

gettothepoint summarization with generatornetworks
in acl
sainbayar sukhbaatar arthur szlam and rob fergus

learning multiagent communication with back propagation
in nips
ilya sutskever oriol vinyals and quoc v le

sequence to sequence learning with neural works
in advances in neural information ing systems
we limit the input and output vocabulary size to the most frequent tokens in the training set
we initialize word embeddings with d glove vectors pennington et al
and tune them during training
we train using adam with a learning rate of
for the mle models and for the models
we tune the gamma hyper parameter in the mixed loss by erating



in almost all dca models the
value yielded the best gains
we train our models for iterations
which took days for agents and days for agents since it has more encoder parameters to tune
to avoid repetition we prevent the decoder from generating the same trigram more than once during test following paulus et
al

in dition for every predicted out of vocabulary ken unk we replace it with its most likely origin by choosing the source word w with the a i gt largest cascaded attention w arg max a a i eq

b generated summary examples this appendix provides example documents from the test set with side by side comparisons of the human generated golden summaries and the summaries produced by our models
baseline is a single agent model trained with loss model in table while our best multi agent is optimized by mixed model loss the model in table
red highlights indicate details that should not appear in the summary but the models generated them
red indicates factual errors in the summary
green highlights indicate key facts in the human gold summary that only one of the models manage to capture
a supplementary material stats avg
tokens document avg
tokens summary total train doc summ
pair total validation doc summ
pair total test doc summ
pair input token length output token length agent input token length agent agent input token length agent agent input token length agent cnn dm nyt table summary statistics of cnn dailymail dm and new york times nyt datasets
a
datasets cnn dailymail cnn dailymail dataset lapati et al
hermann et al
is a lection of online news articles along with sentence summaries
we use the same data splits as in nallapati et al

while lier work anonymized entities by replacing each named entity with a unique identier e

minican we opted for non anonymized version
new york times nyt although this dataset has mainly been used to train extractive rization systems hong and nenkova hong et al
li et al
durrett et al
it has recently been used for the abstractive marization task paulus et al

nyt dataset sandhaus is a collection of articles lished between and
we use the scripts provided in li et al
to extract and process the nyt dataset with some modications in order to replicate the pre processing steps sented in paulus et al

similar to paulus et al
we sorted the documents by their publication date in chronological order and used the rst for training the next for idation and last for testing
they also use pointer supervision by replacing all named entities in the abstract if the type is person tion organization or misc using the stanford named entity recognizer manning et al

by contrast we did not anonymize the nyt dataset to reduce pre processing
a
training details we train our models on an nvidia gpu machine
we set the hidden state size of the coders and decoders to
for both datasets document model abbey clancy is helping to target breast cancer by striking a sultry pose in a new charity campaign
the winner of s strictly come dancing joins singer foxes victoria s secret angel lily donaldson and model alice dellal in the new series of pictures by photographer simon emmett for fashion targets breast cancer
clancy looks chic as she shows off her famous legs wearing just a plain white shirt
abbey clancy leads the glamour as she joins forces with her famous friends to target breast cancer by striking a sultry pose in a new charity campaign the model who is mother to four year old daughter sophia with footballer husband peter crouch said as a mum it makes me proud to be part of a campaign that funds vital work towards ensuring the next generation of young women do not have be afraid of a diagnosis of breast cancer
i m wearing my support and i want everyone across the uk to do the same and get behind this campaign
holding onto heaven singer foxes looks foxy in cropped stripy top and jeans
abbey says she is proud to be part of a campaign that funds vital work towards ensuring the next generation of young women do not have be afraid of a diagnosis of breast cancer victoria s secret angel lily donaldson who has been in the industry for years also adds some glamour to the charity campaign holding onto heaven singer foxes dons a stripy top and jeans for the campaign she says she s honoured to be a part of she said i m so honoured to be taking part in this year s fashion targets breast cancer and becoming part of the campaign s awesome heritage
fashion is a huge part of my life and if by taking part i can inspire women to wear their support join the ght and take on breast cancer head on then that will be something to be really proud of
now in its year the campaign has so far raised
m for breakthrough breast cancer s research funding
this year the range of clothes and accessories have been produced in conjunction with high street partners river island warehouse topshop laura ashley debenhams superga baukjen and the cambridge satchel company
they can be viewed online at www
fashiontargetsbreastcancer
org
lookbook the campaign which also stars alice dellal has so far raised
m for breakthrough breast cancer s research funding human gold single agent baseline multi agent models abbey and lily are joined by alice dellal and singer foxes
the women are pictured wearing their support
abbey says she is proud to be part of a campaign that funds vital work
campaign has raised
m for breakthrough breast cancer s research
strictly come dancing joins singer foxes victoria s secret angel lily donaldson and model alice dellal in the new series of pictures by photographer simon emmett for fashion targets breast cancer
clancy looks chic as she shows off her famous legs wearing just a plain white shirt
abbey says she is proud to be part of a campaign that funds vital work towards ensuring the next generation of young women do not have been afraid of a diagnosis of breast cancer
the campaign has raised
m for breakthrough breast cancer s research
table in this example both and multi agent models demonstrate extractive behaviors
however each select sentences from different sections of the document
while the single model extracts the second and the third sentences the multi agent model successfully selects salient sentences from sentences that are further down in the document specically sentence and
this can be attributed to the fact that agents can successfully encode salient aspects distributed in distant sections of the document
an interesting result is that even though the agent model shows extractive behaviour in this example it successfully selects the most salient sentences while the single agent model includes superuous details
document michelle pfeiffer is the latest hollywood star preparing to hit the small screen
the oscar nominated star known for her roles in iconic lms such as scarface dangerous liaisons andthe age of innocence has teamed up with katie couric to pitch a new television comedy about a morning news program
also involved in the project in which pfeiffer is attached to star is diane english the creator of murphy brown
scroll down for video michelle pfeiffer left is set to star in a new television comedy about a morning news program produced by katie couric right the series was created by diane english above with candice bergen who was behind the show murphy brown about a female news anchor according to variety pfeiffer s role will be that of a morning news anchor making it very similar to the real life role couric played as co host of today for years
couric will serve as an executive producer and help ensure the series strikes realistic notes
the creator behind the project english was previously the brains behind brown the show starring candice bergen that centered around a female news anchor and ran for ten seasons winning emmys
english would also serve as a writer producer and showrunner on the program
the ladies are currently in talks with hbo showtime amc netix and amazon to pick up the program
couric will serve as an executive producer drawing on her experience as an anchor on today for years pfeiffer would be the one of the biggest stars yet to move to television joining a group that now includes house of cards stars robin wright and kevin spacey true detective leads matthew mcconaughey and woody harrelson and even lady gaga who recently announced she would be appearing on the next season of american horror story
the actress has kept a low prole for the past years since becoming a mother only doing a handful of lms in that time
she most recently appeared alongside robert de niro in the mob comedy the family
michelle pfeiffer is set to star in a new television comedy about a morning news program
katie couric will serve as an executive producer drawing on her experience as an anchor on today for years
the series was created by diane english who was behind the show murphy brown about a female news anchor
the ladies are currently in talks with hbo showtime amc netix and amazon to pick up the program
the oscar nominated star known for her roles in iconic lmssuch as scarface dangerous liaisons and the age of innocence has teamed up with katie couric to pitch a new vision comedy about a morning news program
also involved in the project in which pfeiffer is attached to star is diane english the creator of murphy brown
human gold single agent baseline multi agent michelle pfeiffer is set to star in a new tv comedy about a morning news program
couric will the series was created by diane english the creator of murphy brown
pfeiffer is the one of the biggest stars
serve as an executive producer and showrunner on the project
table the baseline model generates non coherent summary that references the main character michelle fer in an ambiguous way towards the end of the generated summary
in contrast the multi agent model cessfully captures the main character including the key facts
one interesting feature that the multi agent model showcases is its simplication property which accounts for its strength in abstraction
specically it simplied the bold long sentence in the document starting with couric will


and only generated the salient words
document everton manager roberto martinez was forced to defend another penalty asco at the club after ross barkley missed from the spot in their win against burnley at goodison park
the untried barkley inexplicably took the minute kick awarded for a foul by david jones on aaron lennon rather than leighton baines who has scored penalties from attempts in the premier league
although there was no dispute between the team mates this time it brought back memories of everton s match against west brom in january when kevin mirallas grabbed the ball from baines to take a penalty and missed
ross barkley steps up to take a minute penalty despite the presence of leighton baines on the pitch barkley s effort is saved byburnley goalkeeper tom heaton at goodison park martinez insisted barkley was within his rights to request penalty taking duties on saturday
if romelu lukaku had been on the pitch he would have taken it
otherwise i am happy to have three or four players who can take penalties and let it depend on how they feel at that moment argued the everton manager
baines left has scored penalties from attempts in the premier league ross showed incredible responsibility to take it
i love seeing players take control of the big moments and leighton was happy to given him that responsibility
barkley s penalty was well struck but was nt put in the corner and burnley goalkeeper tom heaton dived to his right to save
fortunately for the young england player it did nt prove costly as mirallas went on to score the only goal of the game after minutes
everton boss roberto martinez issues instructions to his players during a break in play against burnley everton defeated burnley at goodison park on saturday
kevin mirallas scored the only goal of the game in the minute
ross barkley had earlier missed a minute penalty
leighton baines has scored penalties from attempts this season
everton manager roberto martinez was forced to defend another penalty asco at the club human gold after ross barkley missed from the spot in their win against burnley at goodison park
the untried barkley inexplicably took the minute kick awarded for a foul by david jones on aaron lennon rather than leighton baines who has scored penalties from attempts in the premier league
everton beat burnley at goodison park in the premier league
ross barkley steps up to take a minute penalty but missed it
barkley has scored penalties from attempts in the pitch
single agent baseline multi agent table the single agent model generates summary with superuous details and the facts are not clearly expressed
although it was able to capture the statistics of the player correctly e

penalties attempts it still missed the player who scored the only goal in the game i
e
kevin mirallas
on the other hand multi agent model was able to generate a concise summary with several key facts
however similar to single agent model it missed to capture the player who scored the only goal in the game
interestingly the document contains the word defeated but the multi agent model chose to use beat instead which does not exist in the original document

