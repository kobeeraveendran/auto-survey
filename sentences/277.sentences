a hierarchical network for abstractive meeting summarization with cross domain pretraining chenguang zhu ruochen xu michael zeng xuedong huang microsoft cognitive services research group chezhu ruox nzeng
com p e s l c
s c v
v i x r a abstract meeting transcript turns with the abundance of automatic meeting scripts meeting summarization is of great terest to both participants and other parties
traditional methods of summarizing meetings depend on complex multi step pipelines that make joint optimization intractable
while there are a handful of deep neural els for text summarization and dialogue tems
however the semantic structure and styles of meeting transcripts are quite ent from articles and conversations
in this per we propose a novel abstractive summary network that adapts to the meeting scenario
we design a hierarchical structure to modate long meeting transcripts and a role tor to depict the difference among speakers
furthermore due to the inadequacy of meeting summary data we pretrain the model on scale news summary data
empirical results show that our model outperforms previous proaches in both automatic metrics and man evaluation
for example on icsi dataset the score increases from
to

introduction meetings are a very common forum where people exchange ideas make plans and share information
with the ubiquity of automatic speech recognition systems come vast amounts of meeting transcripts
therefore the need to succinctly summarize the content of a meeting naturally arises
several methods of generating summaries for meetings have been proposed mehdad et al
murray et al
wang and cardie oya et al
shang et al
li et al

as murray et al
points out users prefer abstractive meeting summaries to extractive maries
while these methods are mostly tive they require complicated multi stage machine equal contribution


pm


another point is we have to skip the teletext because in the world of upcoming internet we think teletext is going to be a thing of the past
id


rst about how it works
really simple
everybody knows how a remote works
the user presses a button
the remote determines what button it is pm


few buttons we talked about that
docking station lcd
general functions and default materials





summary from our model sentences


the project manager announced that the project would not include a teletext feature
the industrial designer gave a presentation of the functions of the remote
the group decided on features to include in the remote to include an lcd screen and a docking station to change the layout of the interface



table example excerpt of a meeting transcript and the summary generated by our model in ami dataset
keywords are highlighted in colors
pm program ager and id industrial designer are roles of the ers
the meeting transcript contains word errors and grammatical glitches as it is the result from the matic speech recognition system
learning pipelines such as template generation tence clustering multi sentence compression didate sentence generation and ranking
as these approaches are not end to end optimisable it is hard to jointly improve various parts in the pipeline to enhance the overall performance
moreover some components e

template generation quire extensive human involvement rendering the solution not scalable or transferrable
meanwhile many end to end systems have been successfully employed to tackle document marization such as the pointer generator network see et al
reinforced summarization work paulus et al
and memory network jiang and bansal
these deep learning methods can effectively generate abstractive ment summaries by directly optimizing pre dened goals
however the meeting summarization task ently bears a number of challenges that make it more difcult for end to end training than ment summarization
we show an example of a meeting transcript from the ami dataset and the summary generated by our model in table
first the transcript and summary of a single meeting are usually much longer than those of a document
for instance in cnn daily mail dataset hermann et al
there are on average tokens per article and tokens per summary while ami meeting corpus contains meetings with tokens per transcript and tokens per mary on average
and the structure of a meeting transcript is very distinct from news articles
these challenges all prevent existing news summarization models to be successfully applied to meetings
second a meeting is carried out between tiple participants
the different semantic styles standpoints and roles of each participant all tribute to the heterogeneous nature of the meeting transcript
third compared with news there is very limited labelled training data for meeting summary meetings in ami v
s
k articles in cnn dm
this is due to the privacy of meetings and the atively high cost of writing summaries for long transcripts
to tackle these challenges we propose an end to end deep learning framework hierarchical meeting summarization network hmnet
net leverages the encoder decoder transformer chitecture vaswani et al
to produce tive summaries based on meeting transcripts
to adapt the structure to meeting summarization we propose two major design improvements
first as meeting transcripts are usually lengthy a direct application of the canonical transformer structure may not be feasible
for instance ducting the multi head self attention mechanism on a transcript with thousands of tokens is very time consuming and may cause memory overow problem
therefore we leverage a hierarchical structure to reduce the burden of computing
as a meeting consists of utterances from different ipants it forms a natural multi turn hierarchy
thus the hierarchical structure carries out both level understanding within each turn and turn level understanding across the whole meeting
during summary generation hmnet applies attention to both levels of understanding to ensure that each part of the summary stems from different portions of the transcript with varying granularities
second to accommodate multi speaker scenario hmnet incorporates the role of each to encode different semantic styles and standpoints among participants
for example a program ager usually emphasizes the progress of the project while a user interface designer tends to focus on user experience
in hmnet we train a role tor for each meeting participant to represent the speaker s information during encoding
this role vector is appended to the turn level representation for later decoding
to tackle the problem of insufcient training data for meeting summarization we leverage the idea of pretraining devlin et al

we lect summarization data from the news domain and convert them into the meeting format a group of several news articles forms a multi person ing and each sentence becomes a turn
the turns are reshufed to simulate a mixed order of ers
we pretrain the hmnet model on the news task before netuning it on meeting summarization
empirical results show that this cross domain training can effectively enhance the model quality
to evaluate our model we employ the widely used ami and icsi meeting corpus mccowan et al
janin et al

results show that hmnet signicantly outperforms previous ing summarization methods
for example on icsi dataset hmnet achieves
higher points
higher points and
higher rouge points compared with the vious best result
human evaluations further show that hmnet generates much better summaries than baseline methods
we then conduct ablation studies to verify the effectiveness of different components in our model
problem formulation we formalize the problem of meeting tion as follows
the input consists of meeting transcripts x and meeting participants p
pose there are s meetings in total
the datasets in experiments only provide role tion for each participant
in real applications we can use a vector to represent each participant when a personal identier is available
scripts are x


xs
each meeting transcript consists of multiple turns where each turn is the utterance of a participant
thus xi


pli uli where pj p j li is a participant and


wlj is the tokenized utterance from pj
the human labelled summary for meeting xi denoted by yi is also a sequence of tokens
for simplicity we will drop the meeting index script
so the goal of the system is to generate meeting summary y


given the scripts x


pm um
which acts as a user portrait and evolves as more data about this user is collected


hierarchical transformer transformer
recall that a transformer block consists of a multi head attention layer and a feed forward layer both followed by layer norm with residuals where layer can be the attention or feed forward layer vaswani et al

as the attention mechanism is position agnostic we append positional encoding to input vectors model our hierarchical meeting summarization network hmnet is based on the encoder decoder former structure vaswani et al
and its goal is to maximize the conditional probability of ing summary y given transcript x and network parameters p y

encoder

role vector meeting transcripts are recorded from various ticipants who may have different semantic styles and viewpoints
therefore the model has to take the speaker s information into account while ating summaries
to incorporate the participants information we integrate the speaker role component
in the periments each meeting participant has a distinct role e

program manager industrial designer
for each role we train a vector to represent it as a xed length vector rp p p where p is the number of roles
such distributed representation for a role person has been proved to be useful for sentiment analysis chen et al

this vector is appended to the embedding of the speaker s turn section


according to the results in tion
the vectorized representation of speaker roles plays an important part in boosting the mance of summarization
this idea can be extended if richer data is able in practice if an organization chart of participants is able we can add in representations of the lationship between participants e

manager and developers into the network
if there is a pool of registered participants each participant can have a personal vector where j stands for the j th dimension of tional encoding for the i th word in input sequence
we choose sinusoidal functions as they can extend to arbitrary input length during inference
in summary a transformer block on a sequence of n input embeddings can generate n output beddings of the same dimension as input
thus multiple transformer blocks can be sequentially stacked to form a transformer network


xn


long transcript problem
as the canonical transformer has the attention mechanism its putational complexity is quadratic in the input length
thus it struggles to handle very long quences e

tokens
however meeting transcripts are usually fairly long consisting of thousands of tokens
we note that meetings come with a natural turn structure with a reasonable number of turns e

turns per meeting on average in ami dataset
and the number of tokens in a turn is much less than that in the whole meeting
therefore we employ a two level transformer structure to encode the meeting transcript
word level transformer
the word level transformer processes the token sequence of one turn in the meeting
we encode each token in one turn using a trainable embedding matrix d
thus the token in the i th turn wi j is associated with a uniform length vector j gi j
to incorporate syntactic and semantic information we also train two embedding matrices to represent the part of speech pos and entity ent tags
fore the token wi j is represented by the vector figure hierarchical meeting summary network hmnet model structure
bos is the special start token inserted before each turn and its encoding is used in turn level transformer encoder
other tokens encodings enter the cross attention module in decoder

xi j gi j p osi j en ti
note that we add a special token before the sequence to represent the beginning of a turn
then we denote the output of the word level transformer as follows word


xi li xw


xw i li turn level transformer
the turn level former processes the information of all m turns in a meeting
to represent the i th turn we ploy the output embedding of the special token from the word level transformer i
e
xw
furthermore we concatenate it with the role tor of the speaker for this turn pi
it follows that the output of the turn level transformer is turn pm xt m






xt
decoder the decoder is a transformer to generate the mary tokens
the input to the decoder transformer contains the k previously generated summary tokens



each token is represented by a vector using the same embedding matrix d as the encoder gi
the decoder transformer uses a lower triangular mask to prevent the model to look at future kens
moreover the transformer block includes two cross attention layers
after self attention the embeddings rst attend with token level puts xw and then with turn level puts xt each followed by layer norm
this makes the model attend to different parts of the inputs with varying scales at each inference step
i li i m the output of the decoder transformer is noted as decoder






to predict the next token yk we reuse the weight of embedding matrix d to decode into a ability distribution over the vocabulary p k x we illustrate the hierarchical meeting summary network hmnet in fig

training
during training we seek to minimize the cross entropy logp x n n self attnadd normadd normfeed forwardself attnadd normadd normfeed forwardcross attnadd normcross attnadd normself attnadd normadd normfeed forwardgood softmaxlinearrole vectorpositional encodingword level transformerturn level begin masking we use teacher forcing in decoder training i
e
the decoder takes ground truth summary tokens as input
and no other speaker identication information we use a single role vector to model both speaker and role information simultaneously
inference
during inference we use beam search to select the best candidate
the search starts with the special token
we ploy the commonly used trigram blocking paulus et al
during beam search if a candidate word would create a trigram that already exists in the previously generated sequence of the beam we forcibly set the word s probability to
finally we select the summary with the highest average log likelihood per token

pretraining as there is limited availability of meeting rization data we propose to utilize summary data from the news domain to pretrain hmnet
this can warm up model parameters on summarization tasks
however the structure of news articles is very different from meeting transcripts
therefore we transform news articles into the meeting format
we concatenate every m news articles into an m meeting and treat each sentence as a gle turn
the sentences from article i is considered to be utterances from the i th speaker named as dataset i
for instance for each xsum meeting the speakers names are to xsum m
to simulate the real meeting scenario we randomly shufe all the turns in these pseudo meetings
the target summary is the concatenation of the m maries
we pretrain hmnet model with a large tion of news summary data details in section
and then netune it on real meeting summary task
experiment
datasets we employ the widely used ami mccowan et al
and icsi janin et al
meeting corpora
the two datasets contain meeting transcripts from automatic speech recognition asr respectively
we follow shang et al
to use the same train development test split for ami and for icsi
each meeting has an tive summary written by human annotators
thermore each participant has an associated role e

project manager marketing
since there is only one speaker per role in each meeting in ami there are on average words with turns in the meeting transcript and words in the summary
in icsi there are on average words with turns in the meeting transcript and words in the summary
as the transcript is produced by the asr system there is a word error rate of for ami and for icsi shang et al

the pretraining is conduct on the news rization datasets cnn dailymail hermann et al
nyt sandhaus and xsum narayan et al
containing k k and k article summary pairs
we take the union of three datasets for the pretraining
we choose groups of m news articles to match the speaker setting in ami dataset
these converted meetings contain on average words with turns and words in the summary

baseline models for comparison we select a variety of baseline systems from previous literatures the basic lines random riedhammer et al
and copy from train which randomly copies a mary from the training set as the the template based method template oya et al
the ranking systems textrank mihalcea and tarau and clusterrank garg et al
the unsupervised method uns the ment summarization model see et al
and the multi modal model mm li et al

in addition we implement the baseline model extractive oracle which concatenates top tences with the highest scores with the golden summary
the number of sentences is termined by the average length of golden summary for ami and for icsi

metrics following shang et al
we employ and rouge metrics lin to evaluate all meeting summarization models
these three metrics respectively evaluate the curacy of unigrams bigrams and unigrams plus reduce variance for each article we randomly sample times and report the averaged metrics
select the scenario meetings of ami as in shang et al
treats the whole meeting transcript as an article and generates the summary
model random template textrank clusterrank uns extractive oracle pgnet copy from train mm mm hmnet ami





















r








icsi















r







table rouge scores of generated summary in ami and icsi datasets
numbers in bold are the overall best result
the two baseline mm models require additional human annotations of topic segmentation and visual signals from cameras
results are statistically signicant at level

skip bigrams with a maximum skip distance of
these metrics have been shown to highly correlate with the human judgment lin

implementation details we employ spacy honnibal and johnson as the word tokenizer and embed pos and ner tags into dim vectors
the dimension of the role vector is
all transformers have layers and heads in attention
the dimension for each word is and thus the input and output dimensions of transformers dmodel are for the decoder for the word level former and for the turn level transformer
for all transformers the layer always has dimensionality df dmodel
hmnet has m parameters in total
we use a dropout probability of
on all layers
we pretrain hmnet on news summarization data using the radam optimizer liu et al
with


the initial learning rate is set to and linearly increased to
with warmup steps
for netuning on the meeting data the optimization setup is the same except the initial learning rate is set to

we use gradient clipping with a maximum norm of and gradient accumulation steps as

results table shows the rouge scores of generated maries in ami and icsi datasets
as shown cept for in ami hmnet outperforms all baseline models in all metrics and the result is tistically signicant at level
under paired t test with the best baseline results
on icsi dataset net achieves

and
higher rouge points than previously best results
note that mm is a multi modal model which requires human annotation of topic segmentation topicseg and visual focus on attention vfoa collected from cameras which is rarely able in practice
in comparison our model net is entirely based on transcripts from asr pipelines
still on ami dataset hmnet forms by
points in and
points in and is higher than by
points in
moreover hmnet signicantly outperforms the document summarization model pgnet ing that traditional summarization models must be carefully adapted to meeting scenarios
hmnet also compares favorably to the extractive oracle showing that human summaries are more tive rather than extractive for meetings
it s worth noting that copy from train obtains a surprisingly good result in both ami and icsi higher than most baselines including pgnet
the reason is that the meetings in ami and icsi are not isolated events
instead they form a series of related discussions on the same project
thus many project keywords appear in multiple meetings and their summaries
it also explains the relatively high rouge scores in the evaluation
however hmnet can focus on salient information and as a model r hmnet pretrain role vector hierarchy hmnet pretrain role vector hierarchy ami



icsi



















table ablation study of hmnet
result achieves a considerably higher score than copy from train baseline
ablation study
table shows the ablation study of hmnet on the test set of ami and icsi
as shown the pretraining on news summarization data can help increase the on ami by
points and on icsi by
points
when the role vector is removed the score drops
points on ami and
points on icsi
when net is without the hierarchy structure i
e
the level transformer is removed and role vectors are appended to word level embeddings the score drops as much as
points on ami and
points on icsi
thus all these components we propose both play an important role in the rization capability of hmnet

human evaluation we conduct a human evaluation of the meeting summary to assess its readability and relevance
readability measures how uent the summary guage is including word and grammatical error rate
relevance measures how well the summary sums up the main ideas of the meeting
as mm model li et al
does not have summarization text or trained model available we compare the results of hmnet and uns shang et al

for each meeting in the test set of ami and icsi we have human evaluators from amazon mechanical turk label summaries from hmnet and uns
we choose labelers with high approval rating to increase the credibility of results
each annotator is presented with the meeting transcript and the summaries
the annotator needs to give a score from to higher is better for readability whether the summary consists of figure percentage of novel n grams in the ence and the summaries generated by hmnet and uns shang et al
in ami s test set
dataset source readability relevance dataset source readability relevance hmnet



uns



ami icsi hmnet



uns



table average scores of readability and vance of summaries on ami and icsi s test sets
each summary is judged by human evaluators
standard deviation is shown in parenthesis
ent and coherent sentences and easy to understand and likewise for relevance whether the summary contains important information from the meeting
the annotators need to read both the meeting script and the summary to give evaluations
to reduce bias for each meeting the two versions of summaries are randomly ordered
table shows that hmnet achieves much higher scores in both readability and relevance than uns in both datasets
and the scores for hmnet are all above
indicating that it can generate both readable and highly relevant meeting summaries
insights
how abstractive is our model an abstractive system can be innovative by using words that are not from the transcript in the mary
similar to see et al
we measure the abstractiveness of a summary model via the ratio of novel words or phrases in the summary
a higher ratio could indicate a more abstractive system
fig
displays the percentage of novel n grams i
e
that do not appear in the meeting transcript in the summary from reference hmnet and uns
that are novelreferencehmnetuns as shown both reference and hmnet summaries have a large portion of novel n grams n
almost no grams are copied from the transcript
in contrast uns has a much lower ratio of novel n grams because it generates a summary mainly from the original word sequence in transcripts

error analysis we qualitatively examine the outputs of hmnet and summarize two major types of errors
due to the nature of long meeting transcripts the system sometimes summarizes salient tion from parts of the meeting different from the reference summaries

our system sometimes summarizes meetings at a high level e

topics decisions and not to cover all detailed items as in the reference
related work meeting summarization
there are a number of studies on generating summaries for meetings and dialogues zhao et al
liu and chen chen and metze liu et al
a
mehdad et al
uses utterance clustering an entailment graph a semantic word graph and a ranking strategy to construct meeting summaries
murray et al
and wang and cardie focus on various aspects of meetings such as cisions and action items
oya et al
ploys multi sentence fusion to construct rization templates for meetings leading to maries with higher readability and informativeness
recently shang et al
leverages a sentence compression graph and budgeted ular maximization to generate meeting summaries
in general these multi step methods make joint optimization intractable
li et al
proposes an encoder decoder structure for end to end modal meeting summarization but it depends on manual annotation of topic segmentation and sual focus which may not be available in practice
in comparison our model only requires meeting transcripts directly from speech recognition
document summarization
rush et al
rst introduces an attention based sutskever et al
model to the abstractive sentence summarization task
however the ity of the generated multi sentence summaries for long documents is often low and out of vocabulary oov words can not be efciently handled
to tackle these challenges see et al
proposes a pointer generator network that can both produce words from the vocabulary via a generator and copy words from the source text via a pointer
paulus et al
further adds reinforcement learning to improve the result
gehrmann et al
uses a content selector to over determine phrases in source documents that helps constrain the model to likely phrases and achieves state of the art results in several document summarization datasets
cently several works on using large scale pretrained language models for summarization are proposed and achieves very good performance liu zhu et al
raffel et al
lewis et al
zhang et al

hierarchical neural architecture
as a riety of nlp data e

conversation document has an internal hierarchical structure there have been many works applying hierarchical structures in nlp tasks
li et al
proposes a chical neural auto encoder for paragraph and ment reconstruction
it applies two levels of rnn one on tokens within each sentence and the other on all sentences
lin et al
applies a cal rnn language model hrnnlm to document modeling which similarly encodes token level and turn level information for better language ing performance
serban et al
puts forward a hierarchical recurrent encoder decoder network hred to model open domain dialogue systems and generate system responses given the previous context
nallapati et al
proposes the archical attention mechanism on word level and turn level in the encoder decoder structure for stractive document summarization
conclusion in this paper we present an end to end hierarchical neural network hmnet for abstractive meeting summarization
we employ a two level cal structure to adapt to the long meeting transcript and a role vector to represent each participant
we also alleviate the data scarcity problem by ing on news summarization data
experiments show that hmnet achieves state of the art mance in both automatic metrics and human ation
through an ablation study we show that the role vector hierarchical architecture and ing all contribute to the model s performance
for future work we plan to utilize organizational chart knowledge graph and topic modeling to erate better meeting summaries which can better capture salient information from the transcript
acknowledgement we thank william hinthorn for proof reading this paper
we thank the anonymous reviewers for their valuable comments
references tao chen ruifeng xu yulan he yunqing xia and xuan wang

learning user and product tributed representations using a sequence model for ieee computational sentiment analysis
gence magazine
yun nung chen and florian metze

integrating intra speaker topic modeling and temporal based inter speaker topic modeling in random walk for improved multi party meeting summarization
in thirteenth annual conference of the international speech communication association
jacob devlin ming wei chang kenton lee and kristina toutanova

bert pre training of deep bidirectional transformers for language ing
arxiv preprint

nikhil garg benoit favre korbinian reidhammer and dilek hakkani tur

clusterrank a graph based method for meeting summarization
tenth nual conference of the international speech munication association
sebastian gehrmann yuntian deng and alexander m rush

bottom up abstractive summarization
in proceedings of the conference on cal methods in natural language processing pages
karl moritz hermann tomas kocisky edward stette lasse espeholt will kay mustafa suleyman and phil blunsom

teaching machines to read and comprehend
advances in neural information processing systems pages
matthew honnibal and mark johnson

an proved non monotonic transition system for dency parsing
proceedings of the conference on empirical methods in natural language ing pages
adam janin don baron jane edwards dan ellis david gelbart nelson morgan barbara peskin thilo pfau elizabeth shriberg andreas stolcke et al

the icsi meeting corpus
ieee international conference on acoustics speech and signal processing
proceedings
i i
yichen jiang and mohit bansal

closed book training to improve summarization encoder memory
in proceedings of the conference on cal methods in natural language processing pages
mike lewis yinhan liu naman goyal jan ghazvininejad abdelrahman mohamed omer levy ves stoyanov and luke zettlemoyer

bart denoising sequence to sequence pre training for natural language generation translation and comprehension
arxiv preprint

jiwei li minh thang luong and dan jurafsky

a hierarchical neural autoencoder for paragraphs and documents
arxiv preprint

manling li lingyu zhang heng ji and richard j radke

keep meeting summaries on topic abstractive multi modal meeting summarization
proceedings of the annual meeting of the ciation for computational linguistics pages
chin yew lin

rouge a package for matic evaluation of summaries
text summarization branches out
rui lin shujie liu muyun yang mu li ming zhou and sheng li

hierarchical recurrent neural network for document modeling
proceedings of the conference on empirical methods in natural language processing pages
chunyi liu peng wang jiang xu zang li and jieping ye

automatic dialogue summary generation for customer service
in proceedings of the acm sigkdd international conference on knowledge discovery data mining pages
liyuan liu haoming jiang pengcheng he weizhu chen xiaodong liu jianfeng gao and jiawei han

on the variance of the adaptive learning rate and beyond
in international conference on ing representations
yang liu

fine tune bert for extractive rization
arxiv preprint

zhengyuan liu and nancy chen

reading turn by turn hierarchical attention architecture for ken dialogue comprehension
in proceedings of the annual meeting of the association for putational linguistics pages florence italy
association for computational linguistics
zhengyuan liu angela ng sheldon lee ai ti aw and nancy f chen

topic aware generator networks for summarizing spoken sations
arxiv preprint

iain mccowan jean carletta wessel kraaij simone ashby s bourban m flynn m guillemot thomas hain j kadlec vasilis karaiskos al

the ami meeting corpus
proceedings of the national conference on methods and techniques in behavioral research
yashar mehdad giuseppe carenini frank tompa et al

abstractive meeting summarization with linguistics volume long papers pages
iulian v serban alessandro sordoni yoshua bengio aaron courville and joelle pineau

building end to end dialogue systems using generative chical neural network models
thirtieth aaai ference on articial intelligence
guokan shang wensi ding zekun zhang toine tixier polykarpos meladianos michalis giannis and jean pierre

vised abstractive meeting summarization with sentence compression and budgeted submodular in proceedings of the annual maximization
meeting of the association for computational guistics volume long papers pages
ilya sutskever oriol vinyals and quoc v le

sequence to sequence learning with neural networks
advances in neural information processing systems pages
ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan n gomez ukasz kaiser and illia polosukhin

attention is all you need
advances in neural information ing systems pages
lu wang and claire cardie

independent abstract generation for focused ing summarization
proceedings of the annual meeting of the association for computational guistics volume long papers
jingqing zhang yao zhao mohammad saleh and ter j liu

pegasus pre training with extracted gap sentences for abstractive summarization
arxiv preprint

zhou zhao haojie pan changjie fan yan liu lin li min yang and deng cai

tive meeting summarization via hierarchical tive segmental network learning
in the world wide web conference pages
chenguang zhu ziyi yang robert gmyr michael zeng and xuedong huang

make lead bias in your favor a simple and effective method for news summarization
arxiv preprint

entailment and fusion
proceedings of the ropean workshop on natural language generation pages
rada mihalcea and paul tarau

textrank ing order into text
proceedings of the ference on empirical methods in natural language processing
gabriel murray giuseppe carenini and raymond ng

generating and validating abstracts of ing conversations a user study
proceedings of the international natural language generation conference pages
ramesh nallapati bowen zhou cicero dos santos c aglar gulcehre and bing xiang

tive text summarization using sequence to sequence in proceedings of the rnns and beyond
signll conference on computational natural guage learning pages
shashi narayan shay b cohen and mirella lapata

do nt give me the details just the summary topic aware convolutional neural networks for in proceedings of the treme summarization
conference on empirical methods in natural guage processing pages
tatsuro oya yashar mehdad giuseppe carenini and raymond ng

a template based abstractive meeting summarization leveraging summary and source text relationships
proceedings of the international natural language generation ence inlg pages
romain paulus caiming xiong and richard socher

a deep reinforced model for abstractive marization
in international conference on ing representations
colin raffel noam shazeer adam roberts katherine lee sharan narang michael matena yanqi zhou wei li and peter j liu

exploring the limits of transfer learning with a unied text to text former
arxiv preprint

korbinian riedhammer dan gillick benoit favre and dilek hakkani tur

packing the meeting marization knapsack
ninth annual conference of the international speech communication tion
alexander m rush sumit chopra and jason weston

a neural attention model for abstractive in proceedings of the tence summarization
conference on empirical methods in natural guage processing pages
evan sandhaus

the new york times annotated corpus
linguistic data consortium philadelphia
abigail see peter j liu and christopher d manning

get to the point summarization with generator networks
in proceedings of the nual meeting of the association for computational ami icsi beam size dev test dev test


n a n a n a















n a n a table hyperparameter search trials of beam size with minimum generation length xed with for ami and for icsi
the bold numbers are the best development set performance with the selected beam size
n a in the table is due to the gpu memory overow issue for large beam size
are some word errors and grammatical glitches
moreover compared with document tion tasks like cnn daily mail hermann et al
nallapati et al
the meeting transcript is pretty long and lacks the important rst structure
all of these add to the complexity of meeting summarization tasks
the summary generated by hmnet includes both individual actions proposals and group ties which is similar to the reference
in contrast the result from uns does not have a clear structure
also hmnet is more effective in selecting salient information from the lengthy transcript
more the language of summary from hmnet is smoother and has many fewer grammatical errors than uns
the reason is that hmnet learns the language pattern from the reference summary ing training while uns generates summaries by concatenating transcript word sequences
min
len
beam size ami icsi



r

table selected hyperparameters and development set rouge scores for the reported performance in table ami icsi min
len
dev test dev test























table hyperparameter search trials of minimum eration length with beam size xed as
the bold bers are the best development set performance with the selected minimum generation length
a training details all the training is conducted on tesla gpu with g memory
the batch size per gpu is tokens during pretraining and during ing
the pretraining converges after steps which runs for approximately days
the tuning for both meeting datasets converges after steps which runs for hours
we pick the model with the highest score on the development set of news and meeting datasets for pretraining and netuning respectively
due to the large computation cost of pretraining we only tune hyperparameters for the decoding namely the minimum length of the generated mary and beam size
for both ami and icsi we rst set beam size to and grid search the imum length from
after selecting the best minimum length we tune the beam size from
the tuning is based on the development set score
the selected hyperparameters for ami and icsi and the corresponding development set performance is shown in table
all hyperparameter search als and development test set performance could be found in table and
b example of meeting summary we demonstrate in table table and table examples of ami meeting transcript with speaker information and three versions of summaries ence hmnet and uns shang et al

since the transcript results are from asr pipelines there meeting transcript turns me


i ve done some research
we have we have been doing research in a usability lab where we observed users operating remote controls
we let them ll out a questionnaire
remotes are being considered ugly
f seventy ve percent of the people questioned indicated that they thought their remote were was ugly
and an additional eighty percent indicated that they would spend more money on a fancy looking remote control
fifty percent of the people indicated they only loo used about ten percent of the buttons on a remote control


id i ve got a presentation about the working design
rst about how it works
it s really simple
everybody knows how a remote works
the user presses a button
the remote determines what button it is uses the infrared to send a signal to the tv


they only use about ten percent of the buttons we should make very few buttons


pm


another point is we have to skip the teletext because in the world of upcoming internet we think teletext is going to be a thing of the past
and it s a function we do nt need in our remote control





ui but got many functions in one remote control you can see this is quite simple remote control
few buttons but this re remote control got a lot of buttons
people do nt like it so what i was thinking about was keep the general functions like they are
pm extra button info
that should be possible as
let s see what did we say
more
should be fancy to fancy design easy to learn
few buttons we talked about that
docking station lcd
general functions and default materials


and we have to be very attent in putting the corporate image in our product
so it has to be visible in our design in the way our device works





pm


i will put the minutes in the project document folder


and we have a lunch break now
reference summary sentences the project manager stated the agenda and the marketing expert discussed what functions are most relevant on a remote what the target demographic is and what his vision for the appearance of the remote is
the marketing expert also brought up the idea to include a docking station to prevent the remote from getting lost and the idea to include an lcd screen
the user interface designer pushed for a user interface with large buttons a display function a touchscreen and the capability of controlling different devices
the team then discussed teletext the target demographic the buttons the remote should have the idea of marketing a remote designed for the elderly an audio signal which can sound if the remote is lost lcd screens and language options



whether to include teletext in the design despite the new requirement which indicates that the team is not to work with teletext
the buttons are generally used but the main feature is ugly and ugly
the remote will only have a few buttons
the remote will feature a small lcd screen
the remote will have a docking station



summary from hmnet ours sentences the user interface designer and the industrial designer presented the components of a remote control device
the marketing expert presented research on the working design and selling buttons for the next meeting
the industrial designer gave a presentation of the functions of the remote
the project manager announced that the project would not include a teletext feature
the project manager will post the minutes per cent of the minutes
the user interface designer will focus on the corporate image of the company
the group decided on features to include in the remote to include an lcd screen and a docking station to change the layout of the interface



the remote will have buttons as few buttons as possible
whether to include docking station selection for it
what functions the remote should be
summary from uns sentences buttons we talked about the docking station lcd general functions fancy design easy to learn few buttons on the right places simple manner to put a lot of functions of the remote control pricing we need a great deal of people would indicated that an lcd screen in the remote control would be preferred focusing on elderly people or people forty plus they wanted to work seventy ve percent of the people indicated that the remote got lost in the room minimum number of buttons the real buttons we have to use rebecca required so most existing remote control simply table example meeting transcript in ami and summary from reference hmnet and uns
roles of participants are coded as follows pm project manager me marketing expert id industrial designer ui user interface designer
we manually capitalize some words in the summaries from hmnet and uns for better demonstration
meeting transcript turns pm


it s the conceptual design meeting
and a few points of interest in this meeting are the conceptual specication of components
conceptual specication of design
and also trend watching


me doh
i m gon na inform you about the trend watching i ve done over the past few days
we ve done some market research
we distributed some more enquetes questionnaires


ui


and we need some new a attractive functions which attract people for using it
it s like a speak speech recognition and a special button for selecting subtitles
and overall user friendly
using large buttons


id about the components design
for the energy source we can use a basic battery or a as an optional thing a kinetic energy like in a watch


for the casing the manufacturing department can deliver a at casing single or double curved casing


and the chip set really should be advanced because





id let s look at the at case
it s from the side so it s rather normal
the the single curved so i m not really what they re gon na look like but it s something like this
this type should be better for you or better should prevent repetitive strain injury a bit


pm i suggest the single curved because maybe the curve is pretty good to put the screen in


ui and to put the buttons for changing the channel over here ui maybe it s possible to make this side like let s see
colour to make this side like the right colour





pm


the user interface design it s the same story
and product evaluation
so the industrial designer and user interface designer are going to work together on this one


reference summary sentences the project manager opened the meeting and recapped the decisions made in the previous meeting
the marketing expert discussed his personal preferences for the design of the remote and presented the results of trend watching reports which indicated that there is a need for products which are fancy innovative easy to use in dark colors in recognizable shapes and in a familiar material like wood
the user interface designer discussed the option to include speech recognition and which functions to include on the remote
the industrial designer discussed which options he preferred for the remote in terms of energy sources casing case supplements buttons and chips
the team then discussed and made decisions regarding energy sources speech recognition lcd screens chips case materials and colors case shape and orientation and button orientation



the industrial designer and user interface designer will work together
the remote will use a conventional battery and a docking station which recharges the battery
whether to use kinetic energy or a conventional battery with a docking station which recharges the remote
summary from hmnet ours sentences the project manager opened the meeting by conceptual components and conceptual design
the industrial designer discussed the interior workings of a remote and suggested that the remote should feature speech recognition
the user interface designer presented two existing products and discussed the option to design a remote for the docking station
the marketing expert discussed research from trend watchers
the trend watchers have been consulted about energy sources such as voice recognition speech recognition case recognition and overall buttons
they also discussed the possibility of using a at double curved case and double curved or double curved cases
the group discussed the options for energy source and energy sources
the designers will work together on the prototype evaluation



having an lcd screen on the remote which covers on the outside of the station
kinetic energy sources are needed to add extra buttons
if the case should be used it would be fancy they could make a simple case with plastic or single curved case
summary from uns sentences older people like to shake your remote control the fresh changing channels button on the right side time look easily get screen would held in making the remote control easier leads us to some personal preferences the remote control people would pay more for speech recognition in a remote control research about the designing a y interface designer are going to work trendwatchers i consulted advise that it should be the remote control on the docking station should be telephone shape so you could imagine start by choosing a case show it people like wood but it raised the price table example of meeting transcript and summary from reference hmnet and uns
meeting transcript turns pm


i ll go over what we decided last meeting
id this is the working design presented by me
what the rst thing question i asked was what are we trying to design a device which just sends the signal to the tv to change its state whether that be the power or the channel or the volume


ui


but we are to make it unique so that people want to buy it will this two features together
so what the concept is to have a ip top model


findings most people prefer us user friendly rather than complex remote controls


i would make ip top with a trendy design
me


eighty percent would spend more money when a remote control would look fancy


are prepared to spend more money for something that s a bit nicer looking


current remote controls do not match with the operating behaviour of the user overall



me we asked those two questions the table relates to both questions so we did nt differentiate
would you prefer an lcd screen that s multi function remote and would you pay more for speech recognition in a remote control


me do we have to initially looking at the ndings here focus on a younger age group initially and then broaden out the market later
do we really have to go for everyone right away ui we could focus on the biggest market
if say people between age group of twenty to thirty ve are the biggest market


reference summary sentences the industrial designer gave his presentation on the basic functions of the remote
he presented the basic components that remotes share and suggested that smaller batteries be considered in the product design
the user interface designer presented his ideas for making the remote easy to use he discussed using a simple design and hiding complicated features from the main interface
the marketing expert presented the ndings from a lab study on user requirements for a remote control device and discussed users demand for a simple interface and advanced technology
the project manager presented the new requirements that the remote not include a teletext function that it be used only to control television and that it include the company image in its design
he group narrowed down their target marketing group to the youth market



the group then discussed the shell like shape of the remote and including several different casing options to buyers
the remote will be bundled with a docking station to recharge the remote s batteries and a user friendly instruction manual and multiple casings will be made available
the limitations of the budget will restrict the development of some features several of the features that the group wanted to include may have to be made simpler to decrease cost
summary from hmnet ours sentences the project manager recapped the decisions made in the previous meeting
the industrial designer discussed the interior workings of a remote and the team discussed options for batteries volume control and lcd screens
the marketing expert also found that users want a fancy look and feel trendy fashionable and user design
the user interface designer presented ndings from trend watching reports which indicated a need for products which are technologically innovative are attractive to a user
the group decided to include a ip and an lcd screen and discussed how well the interface would be incorporated into it
the team then discussed how to minimize the number of functions and what functions to include in their design



they discussed making the remote a prot target group and whether to include an lcd ease of noise voice recognition a locator function and the lcd screen
the marketing expert will focus on making the product more user as possible to compensate younger consumers
the remote will only have a small trendy design and will have a few buttons for the next meeting
summary from uns sentences important that the project was accessible to wide range of consumers white age groups remote you got ta press a button on top the tv and it beeps seventy ve percent of user nd most remote controls point about pressing the pound sign of the bleep are in the room stick them in a program have to control with this remote control hold in the palm of the hand set for all tv it here occur fair amount i run that it last a long time change the state of the tv all other appliances sending a signal market research at table example of meeting transcript and summary from reference hmnet and uns

