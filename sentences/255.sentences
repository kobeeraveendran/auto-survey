ted a pretrained unsupervised summarization model with theme modeling and denoising ziyi chenguang robert michael xuedong eric stanford
edu microsoft cognitive services research chezhu rogmyr nzeng
com t c o l c
s c v
v i x r a abstract text summarization aims to extract essential information from a piece of text and form the text into a concise version
existing unsupervised abstractive summarization els leverage recurrent neural networks work while the recently proposed transformer exhibits much more capability
moreover most of previous summarization models abundant unlabeled corpora resources available for pretraining
in order to address these issues we propose ted a based unsupervised abstractive tion system with pretraining on large scale data
we rst leverage the lead bias in news articles to pretrain the model on millions of labeled corpora
next we netune ted on target domains through theme modeling and a denoising autoencoder to enhance the ity of generated summaries
notably ted outperforms all unsupervised abstractive lines on nyt cnn dm and english word datasets with various document styles
further analysis shows that the summaries erated by ted are highly abstractive and each component in the objective function of ted is highly effective
introduction summarization refers to the task of condensing a document into a shorter version without losing the key information
summarization models can be categorized into two types abstractive and tive
extractive models select sentences from the input article as the summary
such process ensures a basic level of grammaticality and accuracy but also limits the model ability to copying
in trast abstractive models summarize a document using newly generated tokens and phrases that may not be found in the original article which involves equal contribution
work was done during rst author internship at microsoft
a process requiring an advanced ability to rene paraphrase and re organize language information see et al
narayan et al
gunel et al

like most machine learning algorithms marization models can also be divided into vised and unsupervised categories
supervised proaches require in domain parallel data i
e
both input articles and corresponding reference maries must be present for the teacher forcing ing hermann et al
liu and lapata
unfortunately high quality paired data are not ways available across different text domains and styles
moreover considering the fact that rization is not an easy task even for people able human labeled data are also difcult to obtain
therefore several unsupervised summarization proaches have been proposed which do not require reference summaries for the target domain
we introduce these methods as follows
unsupervised extractive models
textrank mihalcea and tarau encodes sentences in the article as nodes in an undirected graph
the weights of edges are measured by sentences larity
the centrality of a node sentence is puted by pagerank brin and page to decide whether a sentence should be included in the nal summary
zheng and lapata advances upon textrank by encoding sentences with bert resentation devlin et al
to compute pairs similarity and build graphs with directed edges cided by the relative positions of sentences
unsupervised abstractive models
baziotis et al
leverages differentiable sampling and optimizes by re constructing the input article from the generated summary
chu and liu poses a similar idea in the multi document marization setting
wang and lee uses adversarial training and reinforcement learning to make the summary human readable
fevry and phang adopts denoising autoencoders inally used in sentence compression
however most of these models are only tested on datasets with considerably small article summary length
also previous models usually utilize the recurrent neural networks rnns
however transformers vaswani et al
devlin et al
have shown superior performances over rnns on ous nlp tasks including machine translation ing comprehension sentiment analysis
few efforts have been made to leverage transformers in unsupervised abstractive summarizations
pretraining language model
in recent years pretraining language models have proved to be quite powerful in solving numerous nlp tasks
the state of the art pretrained models include cove mccann et al
elmo peters et al
gpt radford et al
bert devlin et al
and unilm dong et al

taking advantage of corpora with billions of tokens the pretrained language models learn universal and bust representations for various semantic structures and linguistic relationships
as a result pretrained models have been widely used with considerable success in applications such as question answering zhu et al
sentiment analysis peters et al
and passage reranking nogueira and cho
furthermore unilm dong et al
leverages its sequence to sequence capability for abstractive summarization the bert model has been employed as an encoder in bertsum liu and lapata for supervised extractive and abstractive summarization
in this paper we present ted a pretrained pervised abstractive summarization model which is netuned with theme modeling and denoising on in domain data
ted utilizes a transformer based encoder decoder structure and the pretraining ages large scale corpora containing millions of labeled articles
our primary contributions are fold as follows
first we leverage the lead bias in news articles to pretrain ted
the lead bias is introduced by the journalistic convention of writing using an inverted pyramid structure placing the most important mation in the beginning of an article
we propose to use the leading sentences as the target summary and train the model to predict it during pretraining
in this way we pretrain a summarization model on a large scale corpus with
m news articles
the model yields better performance than most existing unsupervised methods
second to netune on specic datasets ted is further trained with a theme modeling loss and a denoising autoencoder
the role of the theme modeling module is to make the generated mary semantically close to the article
the module uses a semantic classier trained using a native objective function
furthermore to optimize on the generated summary tokens we adopt the gumbel softmax jang et al
estimator to replace the non differentiable arg max
the noising autoencoder has been previously used in unsupervised machine translation lample et al
and sentence compression fevry and phang and we employ it to help the model extract salient information from corrupted text
instead of classical word tokenization we adopt the sentencepiece tokenization kudo and son to alleviates the long standing out vocabulary oov problem in language generation tasks luong et al
sennrich et al

we test ted on several benchmark datasets
the experimental results show that ted outperforms all unsupervised abstractive baselines on all datasets
for example on the cnn dm dataset it forms the state of the art unsupervised abstractive model by more than points and pares favorably with most unsupervised extractive models
we further show that ted is capable of generating novel words and phrases in summaries and is a highly abstractive system even compared with supervised systems
methodology in this section we will go through the model ture of ted i
e
the transformer encoder and coder
then we introduce the pretraining method and two in domain netuning objectives theme modelling and the denoising autoencoder
the overall architecture of ted is illustrated in fig


transformer encoder and decoder previous unsupervised summarization methods are based on the sequence to sequence model sutskever et al
that primarily uses the rnn model
as the transformer structure vaswani et al
has been successfully applied in a large ber of nlp tasks ted employs the multi layer transformer encoder decoder architecture
we low the standard transformer design in ted works and refer readers to vaswani et al
figure overall structure of our model
ted rst pretrains on news articles and then netunes with theme modeling and denoising
from left to right
for more technical details on transformers
denote the number of layers i
e
transformer blocks as l the number of self attention heads as h and the hidden size as n
we explore two different urations in experiments layers heads with n and layers heads with n
ue


ue denote the input article tokens sequence as x


xn and each token is rst ferred to a vector by a trainable embeddings trix v
the output from transformer encoder e is a sequence of encoded vectors ue n
the decoder can be viewed as a conditional language model to generate the mary depending on the generator outputs
given k input summary tokens w


wk the cross attention layer in the decoder d attends with encoder outputs ue
the decoder outputs are


ud


wk ud k
the probability distribution over the vocabulary for is given by ud i n p k ud in traditional tokenization algorithms efforts have been made to address the out of vocabulary oov issue yang et al
at the cost of ing semantic information such as mapping oov words to a special unk token
to mitigate the open vocabulary problem we adopt piece kudo and richardson a data driven method that trains tokenization models from tences in large scale corpora
the advantage of the sentencepiece model is that its subwords can cover all possible word forms and the subword vocabulary size is controllable
in the evaluation experiments we train a sentencepiece subword vocabulary of size
note for supervised summarization models ing training the inputs to the decoder are the groundtruths reference summary tokens for supervised learning input tokens are generated in the previous pass i
e
one new token is ated in one pass
more details are available in section



pretraining with unlabeled corpora leveraging large scale unlabeled text corpora to pretrain models has been proven as an effective method in multiple nlp tasks devlin et al

however such approach has not yet been utilized in text summarization
news articles follow an inverted pyramid ture i
e
front loading the most salient information
this so called lead bias for news summarization is so strong that see et al
have shown that using the rst sentences in a news article as a summary can score higher than many sophisticated deep learning models
although this poses a great challenge to previous research we take advantage of this property in our favor in the pretraining phase of ted
for a news article we set the target summary to be the rst three sentences
this allows the model to exploit the structural bias of the news domain and infer the most important information using the background materials in the remainder of the cle
to collect data for pretraining we obtain three years of online news articles from to via an industrial search engine
the search engine indexes major online news domain for instance new york times and bloomberg
then we lect the parsed articles within the time range as the raw data
note that this time span does not overlap any of three test datasets we use in this paper therefore the pretraining should not lead to data leakage in test
it is also worth noting that this idea of utilizing structural bias for scale summarization pretraining is not limited to pretrained transformerencoders decoderstheme lossarticlegeneratedsummarydenoised sent
noisy sent
cross entropylosssent
add noisetransformerencoders decoderspretrain likely to elaborate on the beginning part
we keep those articles with the ratio of overlapping words higher than

we pick this threshold based on observations in the cnn dm dataset where the median overlapping ratio of non stopping words between golden summary and the article is
and the median ratio between the top three tences and the rest of the article is

setting the threshold at
makes the nal training set size t with the available computation resources and ensures that the leading sentences contain enough information
finally we end up with
m articles out of which articles are randomly sampled as the validation set
we conduct pretraining for epochs and pick the model with the best rouge l score on the validation set
the pretraining task is to predict to the rst three sentences of an article using the rest of the article so pretraining will not teach the model to simply copy the leading three sentences since they are removed from the input to the transformers
note that ted does not start off from other pretrained models like bert
after pretraining in order to adapt ted to a specic target dataset for evaluation we netune ted on the target dataset in an unsupervised ner
the netuning objective functions includes the following theme modeling and denoising coder

theme modeling theme modeling aims to make the generated mary semantically close to the input article
we employ differential sampling to enable tion on generated summaries and train a classier to improve the semantic relatedness between the output summary and article


differentiable sampling in order to optimize the transformers using put summaries we need to make the generation of summary tokens differentiable
recall the ditional probability distribution of token is p k ud
let note p k
one can use arg max on to obtain the token in the forward pass however it is not differentiable in the dient back propagation
although arg max can be avoided by obtaining the embedding of as a weighted sum of the vocabulary embeddings v this results in an undesirable gap between the training weighted sum and the inference discrete figure an example of the pretraining task predict the sentences as the target summary using the rest of the article
specic types of models and it can be applied to other types of text as well academic papers with abstracts novels with editor s notes books with tables of contents
however one should carefully examine and clean the source data to take advantage of lead bias as the top three sentences may not always form a good summary
therefore we conduct strict data cleaning to remove irrelevant distracting content and lter out articles whose top three sentences do not form a good summary first many news articles begin with media names reporter names dates or other irrelevant information for summarization e

new york cnn adam smith june
we tomatically clean these using regular expressions
second we only include articles whose top three sentences contain between and words and remaining sentences contain between and words
the criterion on top three sentences is set to lter out articles with either extremely short leading sentences e

phrases of one or two words which contain too little information to be reasonable summaries or exceedingly long ing sentences to reduce the pretraining time
the limit on total number of words in the article is to lter out very long articles to reduce memory sumption
another purpose is to remove very short articles of which the information is too condensed and not suitable for summarization pretraining
third we also remove articles in which the rst three sentences may not contain the major mation in the article
we use a simple and easy compute metric overlapping words
we compute the portion of non stopping words in the top three sentences that also appear in the rest of an article
a higher ratio indicates that the rest of the article is
antoniamarshall confirmedthatfergusonhaslaunchedherownlifestylebrand


itisstillunclearwhatproductswillbeavailablefromsarahsenses
butbasedonmarshallsphoto ithasbeenconfirmedthattherearescentdiffusersandtea
itisalsolikelyforpartoftheproceedsfromsarahsensestobedonatedtostreetchilduk acharitythatfergusonsupports
antoniamarshall confirmedthatfergusonhaslaunchedherownlifestylebrand


itisstillunclearwhatproductswillbeavailablefromsarahsenses
butbasedonmarshallsphoto ithasbeenconfirmedthattherearescentdiffusersandtea
itisalsolikelyforpartoftheproceedsfromsarahsensestobedonatedtostreetchilduk acharitythatfergusonsupports pretrain transformerencoders decoders sampling on the forward pass generation
to solve this issue we employ the straight through softmax estimator jang et al
as in yang et al
baziotis et al

specically the forward pass in training still uses arg max pling but for gradient computation the following gumbel softmax distribution is used as a tiable approximation for the arg max operation i where gk are i
i
samples drawn from the gumbel distribution and denotes the softmax temperature
as shown in jang et al
as the gumbel softmax tion converges to the categorical one hot bution as inf the gumbel softmax bution converges to the uniform distribution
though this gradient estimator is biased we nd that this method works well in practice
we choose
based on the cnn dm validation set and use this value in all the experiments
denote the input article as d the generated summary as s


wm
the generation of s lows the recursive process that input to the transformer decoder to obtain then input to compute and so on
the rst put token is always the special beginning token start


encoder transformer as a semantic figure theme modeling is essentially updating ted with a semantic classier
the input sentence pair is rst processed by adding a class token in the ning and a separation token between the two tences
then the sentence pair is fed into the former encoder and the rst output vector is classied to similar or distinct
in fig
the packed sequence is then fed as input into ted s transformer encoder
the output vector associated with the token cls is then classied into similar distinct categories by a two layer fully connected network
we use the following entropy loss to optimize the encoder such that the is semantically similar to and s is also closed to d while is semantically distinct from
ltheme classier
denoising autoencoder as the generated summary may be off the article theme at the beginning of netuning we also mize ted such that the generated summaries are semantically closed to the input articles
we frame the semantic similarity problem in a tive setting
to better adapt to the target domain data we add sentence pairs from training articles to facilitate similarity computation
concretely during training we pick two utive sequences of tokens and from an article to form a positive sequence pair
second sequence is chosen from another random cle in the dataset to form the negative sequence pair
following devlin et al
each sequence pair is packed into one single sequence by inserting a special token sep between them and adding trainable segment embeddings
a cial classication token cls is also added to the beginning of the packed sequence
as shown the idea of denoising autoencoder vincent et al
has been used in unsupervised machine lation artetxe et al
lample et al
to prevent the model from learning to merely copy every input word one by one
this denoising cess imitates text simplication and helps to rene essential semantic information
in detail a sequence of n consecutive tokens from the input article is injected with two types of noise
first we insert noisy tokens sampled from other articles in the same dataset into the original sequence at random positions obtaining a new quence with length where is larger than n
next similar to lample et al
the sequence is slightly shufed by applying a tation such that i where the permutation distance k is set to be of the length of
the nal corrupted sequence is denoted as
ted model is then trained to self attentionclassifysimilar normalizeadd normalizefeed recover the original token sequence given the rupted sequence ldenoise where ce denotes the mean of token level entropy loss
denotes the sequence of probability distribution outputs from the coder with inputting to the encoder
the nal objective function is the mean of eq
and eq
we empirically nd that equal weights between the two terms work well enough in practice lted ltheme ldenoise it is worth pointing out that we do not conduct pretraining on target evaluation datasets
this is because for a target dataset we do not know forehand whether the lead x sentences will make a quality summary or not
we do have the option to do so on datasets where lead x are good maries however it is potentially cherry picking datasets
also we do not conduct supervised tuning with ground truths summaries in evaluation datasets because we want to have an entirely pervised summarization system with motivations stated in the introduction section
experiments
datasets we evaluate our model on three benchmark marization datasets nyt cnn dm and english gigaword containing k k and
m news articles respectively
the detailed statistic tion on the datasets can be found in the appendix
in nyt following liu and lapata we choose examples as the validation set and lter out examples with summaries of fewer than words
in cnn dm similar to see et al
and liu and lapata input articles are truncated to tokens
in english gigaword we lter out data examples with articles containing onlyunk tokens

baseline and metrics we compare ted with the following baselines
unsupervised abstractive systems brief wang and lee baziotis et al
radford et al
without supervised tuning with ground truths summaries
pervised extractive systems textrank mihalcea and tarau lead x
supervised tive and abstractive models trained with truths summaries pacsum zheng and ata pgnet see et al
refresh narayan et al
and sumo liu et al

ted is unsupervised abstractive and therefore not directly comparable with supervised baselines
the purpose of supervised systems here is for references
we describe the implementation details of our model in appendix
we measure the quality of generated summaries by rouge score lin including unigram bigram and longest common quence rouge l

results results on english gigaword dataset are shown in table ted outperforms all unsupervised lines
table shows the experimental results on nyt and cnn dm datasets
in nyt the vised ne tuning of ted improves upon the trained model by


on rouge l respectively
note that rouge metric prefers extractive systems that serve original phrasing see et al

ing this factor ted achieves results that are petitive with unsupervised extractive baselines and surpasses all unsupervised abstractive models
in cnn dm ted with a larger model size outperforms all unsupervised abstractive methods and compares favorably with unsupervised tive baselines
note that ted outperforms a powerful transformer based language generation model pretrained on large scale webpage textual data by signicant margins
again ted further improves upon pretrained models on both and congurations
table results on the english gigaword dataset
formances of baseline models are collected from their original papers
the best performance in each metric is in bold
model ted ours pretrained ours ted ours pretrained ours brief













rl






table rouge scores on cnn dm and nyt datasets
rl stands for l respectively
best results in each unsupervised category is in bold
results of baseline models are obtained from their original papers or running open sourced codes
model rl rl cnn dm nyt ted ours pretrained ours ted ours pretrained ours brief textrank tf idf textrank skip thought textrank bert pacsum tf idf pacsum skip thought pacsum bert













unsupervised abstractive






unsupervised extractive




















sumo pgnet refresh supervised abstractive extractive





















































article after exposing potential security risks with airlines entertainment systems one of the top experts on counter threat intelligence in the world was pulled off a ight by fbi agents
chris roberts who featured in a string of fox news reports was yanked off his plane after it landed in syracuse new york on wednesday night by two fbi agents and two uniformed ofcers
roberts who works for security intelligence company one world labs was questioned for the next four hours


ted summary chris roberts who works for security intelligence company one world labs was pulled off a plane in syracuse new york on wednesday night by two fbi agents and two uniformed ofcers
the incident occurred only a few hours after a report about roberts research was released by the government accountability ofce earlier this week
reference chris roberts of one world labs grabbed after plane landed in syracuse
two fbi agents spent four hours questioning him about cyberhacking
agents conscated electronic devices and computer les from roberts
he ew in to give talk at aerospace conference about plane vulnerabilities
roberts featured on fox news on the record with greta van susteren
regarded as one of the world s top experts on counter threat intelligence
figure an example of a generated summary by ted
the reference summary and parts of the input article are also included
discussion
ablation study the ablation studies shown in table verify the effectiveness of each component in ted
ing the transformer encoder decoder from scratch yields reasonable performance
pretraining on large scale data results in more than ment on all three metrics on training ted from scratch
pretraining plus either theme modeling or denoising improves upon the pretrained model by more than
the full ted model pretraining with theme modeling and denoising produces the best result overall
table ablation study of different components in ted on the nyt dataset
we test with the model conguration
model rl train from scratch pretrained only pretrained theme modeling pretrained denoise loss full model














few hours after a report about roberts research was released



it shows that fact cross checking is a potential future research direction
abstractiveness
to examine how abstractive ted is we compute the proportion of novel grams in the summary output fig

the erence summary and the output from pgnet are included for comparison
although ted is pervised it includes more novel grams than the pervised model pgnet
the reference summaries have the highest proportion of n grams

comparison with previous unsupervised models ted is an innovative unsupervised summarization model with several distinctive features setting it apart from previous approaches such as meansum and
first ted leverages the structure of news articles for an effective large scale pretraining
second although both meansum and have a loss to make the summary similar to the input article they leverage the classical cosine similarity on text embeddings
in contrast ted innovatively encodes the similarity by a transformer encoder with much more modeling capability
third the denoising module in ted is completely distinct from the idea of reconstruction in and sum
in ted s denoising module the corrupted texts are input to the transformer and the model is trained to lter the added noises
the original clean document is not used as input and thus unseen by ted in the forward pass
however the tion process in meansum and employs the original document to generate a summary which is then used to reconstruct the original document
figure proportion of novel grams in summaries erated by different models on the cnn dm test set
conclusion
model analysis example
we showcase a sample summary from cnn dm dataset along with the input article and the reference summary fig

as shown ted is able to capture and organize the essential mation into uent and highly readable language
we attribute the grammatical correctness to the pretraining process and the denoising autoencoder
however we also note that although ted ages to recognize the temporal information related to reported event a few hours after fox news ports it makes a mistake by summarizing as a in this paper we propose ted an unsupervised abstractive summarization model
first we duce an effective large scale pretraining approach leveraging the lead bias in news articles
the training employs automatic ltering mechanism and does require any human labeled data
we then develop a netuning scheme to induce the tic similarity between summaries and input articles together with a denoising autoencoder to improve the quality of generated summaries
experiments across three datasets show that ted signicantly outperforms unsupervised abstractive baselines
of novel n gramspgnettedreference references mikel artetxe gorka labaka eneko agirre and kyunghyun cho

unsupervised neural chine translation
arxiv preprint

christos baziotis ion androutsopoulos ioannis stas and alexandros potamianos

seq differentiable sequence to sequence to sequence autoencoder for unsupervised abstractive sentence compression
arxiv preprint

sergey brin and lawrence page

the anatomy of a large scale hypertextual web search engine
in computer networks and isdn systems pages
elsevier science publishers b
v
eric chu and peter j liu

meansum a neural model for unsupervised multi document abstractive summarization
arxiv preprint

jacob devlin ming wei chang kenton lee and kristina toutanova

bert pre training of deep bidirectional transformers for language ing
arxiv preprint

li dong nan yang wenhui wang furu wei xiaodong liu yu wang jianfeng gao ming zhou and hsiao wuen hon

unied language model pre training for natural language arxiv preprint understanding and generation


thibault fevry and jason phang

vised sentence compression using denoising encoders
arxiv e prints page

beliz gunel chenguang zhu michael zeng and dong huang

mind the facts boosted coherent abstractive text summarization
arxiv preprint

karl moritz hermann tomas kocisky edward stette lasse espeholt will kay mustafa suleyman and phil blunsom

teaching machines to read and comprehend
in advances in neural information processing systems pages
eric jang shixiang gu and ben poole

ical reparameterization with gumbel softmax
arxiv preprint

taku kudo and john richardson

sentencepiece a simple and language independent subword enizer and detokenizer for neural text processing
arxiv preprint

guillaume lample alexis conneau ludovic denoyer and marcaurelio ranzato

unsupervised chine translation using monolingual corpora only
arxiv preprint

liyuan liu haoming jiang pengcheng he weizhu chen xiaodong liu jianfeng gao and jiawei han

on the variance of the adaptive learning rate and beyond
arxiv preprint

yang liu and mirella lapata

text tion with pretrained encoders
arxiv e prints page

yang liu ivan titov and mirella lapata

in gle document summarization as tree induction
proceedings of the conference of the north american chapter of the association for tional linguistics human language technologies volume long and short papers pages
minh thang luong ilya sutskever quoc v le oriol vinyals and wojciech zaremba

addressing the rare word problem in neural machine translation
arxiv preprint

bryan mccann james bradbury caiming xiong and richard socher

learned in translation textualized word vectors
in advances in neural formation processing systems pages
rada mihalcea and paul tarau

textrank ing order into text
in proceedings of the ference on empirical methods in natural language processing pages
shashi narayan shay b cohen and mirella lapata

ranking sentences for extractive arxiv preprint tion with reinforcement learning


rodrigo nogueira and kyunghyun cho

arxiv preprint sage re ranking with bert


matthew e peters mark neumann mohit iyyer matt gardner christopher clark kenton lee and luke zettlemoyer

deep contextualized word sentations
arxiv preprint

alec radford karthik narasimhan tim salimans and improving language ilya sutskever

standing by generative pre training
alec radford jeffrey wu rewon child david luan dario amodei and ilya sutskever

language models are unsupervised multitask learners
openai blog
abigail see peter j liu and christopher d manning

get to the point summarization with generator networks
in proceedings of the nual meeting of the association for computational linguistics volume long papers pages
chin yew lin

rouge a package for automatic in text summarization evaluation of summaries
branches out pages
rico sennrich barry haddow and alexandra birch

neural machine translation of rare words with subword units
arxiv preprint

ilya sutskever oriol vinyals and quoc v le

sequence to sequence learning with neural networks
in advances in neural information processing tems pages
ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan n gomez ukasz kaiser and illia polosukhin

attention is all you need
in i
guyon u
v
luxburg s
bengio h
wallach r
fergus s
vishwanathan and r
nett editors advances in neural information cessing systems pages
curran ciates inc
pascal vincent hugo larochelle yoshua bengio and pierre antoine manzagol

extracting and composing robust features with denoising in proceedings of the international coders
conference on machine learning pages
acm
yau shian wang and hung yi lee

learning to encode text as human readable summaries ing generative adversarial networks
arxiv preprint

zichao yang zhiting hu chris dyer eric p xing and taylor berg kirkpatrick

unsupervised text style transfer using language models as tors
in advances in neural information processing systems pages
ziyi yang chenguang zhu vin sachidananda and embedding imputation with arxiv preprint eric darve

grounded language information


hao zheng and mirella lapata

sentence ity revisited for unsupervised summarization
arxiv preprint

chenguang zhu michael zeng and xuedong huang

sdnet contextualized attention based deep network for conversational question answering
arxiv preprint

a implementation details for pretraining we use a dropout rate of
for all inputs to transformer layers
we use radam liu et al
as the optimizer with a learning rate of
also due to the different numerical scales of the positional embedding and initialized sentence piece embeddings we divide the tional embedding by before feeding it into the transformer
we pretrain one model for epochs
after each epoch the model is evaluated on idation data
we pick the check points with the highest rouge l
for unsupervised netuning on specic datasets the learning rate is set to and dropout ratio stays the same as in pretraining
the batch size is and the vocabulary embeddings are also updated in the training process
during the test phase we generate the summarization from trained encoder and decoder by beam search
the rouge version we use for evaluation is


this is consistent with benchmark models whose version of rouge are available in open sourced codes and original papers
at test time we limit the longest length of erated summaries which is set based on validation dataset
for instance the maximum generation length for cnn dm dataset is
b datasets information for a better understanding of the evaluation tocols the statistical information of evaluation datasets is summarized in table
table average document and summary length in number of words and sentences on nyt cnn dm and english gigaword datasets test set
dataset docs avg
document words avg
summ
sen
sen
words cnn dm nyt gigaword








