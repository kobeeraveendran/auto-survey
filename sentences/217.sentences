clustering of deep contextualized representations for summarization of biomedical texts milad moradi matthias samwald institute for artificial intelligence and decision support center for medical statistics informatics and intelligent systems medical university of vienna vienna austria milad
moradivastegani matthias

ac
at abstract from summarizers contextualized recent years in that incorporate domain knowledge into the process of text summarization have outperformed generic methods especially for summarization of biomedical texts
however construction and maintenance of domain knowledge bases are intense tasks requiring significant manual annotation
in this paper we demonstrate representations that extracted the pre trained deep language model bert can be effectively used to measure the similarity between sentences and to quantify the informative content
the results show that our based the performance of biomedical summarization
although the summarizer does not use any sources of domain knowledge it can capture the context of sentences more accurately than the comparison methods
the source code and data are available at
com biotextsu mm bert based summ
summarizer can improve introduction text summarization is the process of identifying the most important contents within a document and producing a shorter version of the text that conveys those important ideas
many publicly available summarizers use generic features such as the position and length of sentences the term frequency the presence of some cue phrases
to assess importance of sentences
specifically in the biomedical domain it has been shown that these generic measures can not be as efficient as domain specific methods that incorporate sources of domain knowledge to the as such knowledge represent the text on a concept based level
much effort has been invested in using sources of domain ontologies taxonomies controlled vocabularies to capture the context in which the input text appears
these methods have improved the performance of biomedical summarization since they quantify the informative content by considering the semantics behind the sentences rather than considering only generic features
however building maintaining and utilizing sources of domain knowledge can be challenging and time consuming tasks leading the research community to develop a new generation of context aware methods that use neural network based language models
in recent years the usage of pre trained deep language models received significant attention for a wide variety of natural language processing nlp tasks
in this approach unsupervised training is conducted on a large corpus of text and the resulting model can then be fine tuned on a supervised task or can be used directly to extract numeric features for input text
the usage of deep pre trained language models has recently obtained state of the art results for a wide variety of nlp tasks
from transformers in this paper we propose a novel biomedical text summarizer that uses the bidirectional encoder representations bert language model to capture the context in which sentences appear within an input document
bert was pre trained on large text corpora wikipedia and bookcorpus and after a tuning step it can achieve state of the art results on a wide variety of nlp tasks
it is also possible to directly extract and use contextualized embeddings learnt by bert without any further training or fine tuning steps as we do in this paper
the sentences more accurately summarizers that use domain knowledge than showing clustering of deep that can representations contextualized improve the performance of biomedical text summarization
summarization method our summarization method consists of four main steps
figure illustrates the overall architecture of the summarizer

preprocessing the the summarizer performs a preprocessing step in order to prepare the input text for the subsequent steps
those parts of the text that seem to be unimportant for appearing in the summary are discarded
these parts can vary based on the input document and user structure of requirements
in our case since evaluations are performed on biomedical scientific articles the main text of input article is retained and any other parts such as headers of sections and subsections figures and tables
are discarded
this information can be added to the final summary if needed
next the text is split into sentences and each sentence is split into tokens
for this purpose we use the natural language tool kit nltk library

mapping text representations to contextualized in the second step we utilize bert to extract contextualized embeddings
the tokens are used as the input of the feature extraction script
the output is a json file containing activation values of the hidden layers
two different versions of bert with different model sizes are currently available bert base contains layers hidden units in each layer attention heads per unit and a total number of million parameters
bert large contains layers hidden units in each layer attention heads per unit and a total number of million parameters
we use both bert base and bert large in our experiments to assess the impact of different model sizes on the quality of summaries
after the feature extraction step each token is represented as a contextualized embedding with a size of or based on the size of bert model
next contextualized representation is computed for each sentence by a figure the overall architecture of our based biomedical text summarizer
summarizer uses to quantify utilizing the bert language model our summarizer computes a contextual representation i
e
an n dimensional vector for every sentence
it applies a hierarchical clustering algorithm to find multiple groups of sentences such that those sentences nearby in the vector space fall into the the same cluster
the the contextualized embeddings informative content of sentences and assess the similarity between them
the idea is that those sentences within the same cluster share similar context
subsequently the summarizer selects the most informative sentences of each cluster to generate the final summary
we evaluate the performance of our bert based summarizer on a corpus of biomedical scientific articles
the results show that our summarizer can improve the performance of biomedical text summarization compared to generic methods and biomedical summarizers that utilize domain knowledge
the main contributions of this paper can be summarized as follows utilizing a pre trained bidirectional unsupervised language model biomedical text summarization for demonstrating the bert based summarizer can capture the context of that averaging over all the representations of tokens belonging to the sentence

sentence clustering the contextualized embedding of each sentence represents the context in which the sentence appears
therefore nearby sentences in the vector space can share similar context
the summarizer uses a clustering step to group the sentences into a number of clusters such that those in the same cluster are the most similar in terms of their representations in the vector space
we use an agglomerative hierarchical clustering algorithm in this step
the clustering algorithm starts by specifying the number of final clusters i
e
the parameter k
in each iteration those two clusters that are the most similar or the nearest are merged and the number of clusters reduces by one
the similarity or distance between two clusters is computed by averaging over all similarity or distance values between each sentence of the first cluster and each sentence of the second one
the clustering algorithm proceeds until the number of clusters reaches k
the similarity or distance between sentences can be computed using different measures
we run the clustering step with two widely used measures separately i
e
cosine similarity and euclidean distance
let rn and qn be the contextualized representations of two given sentences
cosine similarity and euclidean distance between these two vectors are computed as follows
at the end of this step there is a set of clusters each one containing a set of related sentences

summary generation now the summarizer needs to decide which sentences are the most relevant and informative to be included in the summary
since those sentences within the same cluster share some important content of the input text the summarizer selects sentences from all the clusters to cover as many important ideas as possible
each cluster contributes to the summary in proportion to its size as follows where ni is the number of sentences that should be selected from ith cluster n is the size of summary specified by the compression rate is the size of ith cluster and is the size of input document
in order to select the most informative and related sentences of each cluster a within cluster score is computed for each sentence as follows where wcsi j is the within cluster score of ith sentences belonging to jth cluster is the size of jth cluster and sq is the similarity between two sentences si and sq such that sisq
note that the value of sq is computed using either measures cosine similarity or euclidean distance just as same as the measure used in the clustering algorithm
next the summarizer ranks the sentences of each cluster based on the within cluster scores
for each cluster ci top ranked sentences are extracted according to ni
the summarizer arranges the selected sentences in the same order they appear in the input text and produces the final summary
experiments and results
evaluation corpora and metrics we randomly retrieve and articles from biomed central to construct development and evaluation corpora respectively
the abstract of each article is used as the model summary
this approach of creating corpora has been widely adopted in biomedical text summarization
according to the size of both the corpora is large enough to allow the results to be statistically significant
we use the rouge toolkit to assess the quality of summaries produced by automatic methods
higher scores returned by rouge metrics refer to higher content overlap between system and model summaries
in our evaluations we use and metrics
and quantify the content overlap in terms of shared unigrams and bigrams respectively

parameterization the parameter k specifies the number of final clusters in the clustering algorithm
a similarity measure is used in both the sentence clustering and summary generation steps
we assess the performance of our summarization method in
comparison to other summarizers we evaluate the performance of our summarization method against four comparison methods i
e
cibs the bayesian biomedical summarizer summa and
cibs uses umls concepts in combination with itemset mining and clustering to identify and extract important sentences
the bayesian summarizer applies a probabilistic heuristic on concepts to produce an informative summary
summa and texlexan employ generic features such as the length and position of sentences the frequency of terms the presence of cue terms
table presents rouge scores obtained by the methods
the bert based summarizer reports the highest scores
compared to the scores obtained by the comparison methods the bert based summarizer can the performance of biomedical text summarization according to a wilcoxon signed rank text with a confidence interval of
significantly
improve conclusion the results show that contextualized embeddings learnt by bert can be effectively used for biomedical text summarization
it is shown that this type of contextual representations can convey the context of sentences more accurately than the comparison methods that utilize sources of domain knowledge
this study can be an initial step toward employing this type of language models for developing domain specific nlp systems text summarization
to extend our research we plan to utilize this type of language models trained on biomedical text corpora and investigate their usefulness in biomedical text summarization
future work may include the usage of contextual representations to address problems such as biomedical named entity recognition question answering and information extraction that need to accurately capture the context of text
biomedical especially in references m
gambhir and v
gupta recent automatic text summarization techniques a survey artificial intelligence review vol
pp

k cosine similarity





















euclidean distance





















table rouge scores obtained by the based summarizer in parameterization experiments
bert based summarizer cibs bayesian summarizer summa texlexan









table rouge scores obtained by our based summarizer and the comparison methods
different settings varying the number of clusters in the range and using measures of cosine similarity and euclidean distance separately
for brevity reasons we only report results obtained when the summarizer utilizes bert large since the scores are higher than those of bert base
in all experiments the compression rate is set to

table presents the rouge scores obtained by the summarizer using different settings
the scores are presented for both the cosine similarity and euclidean distance
the summarizer obtains the highest scores when
for smaller values of k some important sentences are merged with sentences of larger clusters they may lose their chance for inclusion in the summary
in this case some informative sentences may be excluded from summaries leading to a decrease in the quality of summarization
on the other hand when higher values are assigned to k some unimportant sentences leave large clusters construct a new cluster and contribute to the summary
in this case a number of non informative sentences may appear in the summary decreasing the scores

sourceforge
for arxiv transformers language preprint bidirectional understanding

c

lin looking for a few good metrics automatic summarization evaluation how many samples are enough in ntcir
h
saggion summa a robust and adaptable summarization tool traitement automatique langues vol

for recent journal approach identifying review of topic based of to intelligence text m
moradi cibs a biomedical sentence summarizer using clustering biomedical informatics vol
pp

m
moradi and n
ghadiri different approaches important concepts in probabilistic biomedical text summarization artificial intelligence in medicine vol
pp

r
mishra j
bian m
fiszman c
r
weir s
jonnalagadda j
mostafa al
text summarization in the biomedical domain a research systematic journal of biomedical informatics vol
pp

l
plaza a
daz and p
gervs a semantic biomedical graph based in summarisation artificial medicine vol
pp

m
moradi and n
ghadiri quantifying the informativeness for biomedical literature summarization an itemset mining method computer methods and programs in biomedicine vol
pp

m
moradi frequent itemsets as meaningful for summarizing events biomedical texts in international conference on computer and knowledge engineering iccke pp

m
moradi concept based and text multi document summarization isfahan university of technology
w
w
fleuren and w
alkema application of text mining in the biomedical domain methods vol
pp

j
turian l
ratinov and y
bengio word representations a simple and general method for semi supervised learning presented at the proceedings of the annual meeting of for computational linguistics uppsala sweden
m
e
peters w
ammar c
bhagavatula and sequence r
power tagging with bidirectional language models arxiv preprint

a
radford k
narasimhan t
salimans and language i
understanding by generative pre training url amazonaws
us
com openai assets covers languageunsupervised language understanding paper
pdf
semi supervised the association in graphs improving biomedical sutskever m
e
peters m
neumann m
iyyer m
gardner c
clark k
lee al
deep contextualized word representations arxiv preprint

j
devlin m

chang k
lee and k
toutanova bert pre training of deep
