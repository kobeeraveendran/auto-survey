autoencoder as assistant supervisor improving text representation for chinese social media text summarization shuming xu junyang houfeng key lab of computational linguistics school of eecs peking university learning lab beijing institute of big data research peking university of foreign languages peking university shumingma xusun linjunyang
edu
abstract most of the current abstractive text marization models are based on the sequence to sequence model
the source content of social media is long and noisy so it is difcult for to learn an accurate semantic representation
compared with the source content the notated summary is short and well ten
moreover it shares the same in this work ing as the source content
we supervise the learning of the tation of the source content with that of the summary
in implementation we regard a summary autoencoder as an assistant pervisor of
following previous work we evaluate our model on a popular chinese social media dataset
tal results show that our model achieves the state of the art performances on the benchmark dataset
introduction text summarization is to produce a brief summary of the main ideas of the text
unlike extractive text summarization radev et al
woodsend and lapata cheng and lapata which lects words or word phrases from the source texts as the summary abstractive text summarization learns a semantic representation to generate more human like summaries
recently most models for abstractive text summarization are based on the sequence to sequence model which encodes the source texts into the semantic representation with an encoder and generates the summaries from the representation with a decoder
the contents on the social media are long and contain many errors which come from spelling mistakes informal expressions and grammatical mistakes baldwin et al

large amount of errors in the contents cause great difculties for text summarization
as for rnn based it is difcult to compress a long sequence into an accurate representation li et al
because of the gradient vanishing and exploding problem
compared with the source content it is easier to encode the representations of the summaries which are short and manually selected
since the source content and the summary share the same points it is possible to supervise the learning of the semantic representation of the source content with that of the summary
in this paper we regard a summary coder as an assistant supervisor of
first we train an autoencoder which inputs and structs the summaries to obtain a better sentation to generate the summaries
then we supervise the internal representation of with that of autoencoder by minimizing the tance between two representations
finally we use adversarial learning to enhance the sion
following the previous work ma et al
we evaluate our proposed model on a nese social media dataset
experimental results show that our model outperforms the state of art baseline models
more specically our model outperforms the baseline by the score of

and
rouge l
proposed model we introduce our proposed model in detail in this section

notation code is available at
lancopku superae given a summarization dataset that consists of n data samples the ith data sample xi yi a m l c
s c v
v i x r a training stage zs where zs is a function which measures the distance between zs and zt
is a tunable parameter to balance the loss of the supervision and the other parts of the loss and nh is the ber of the hidden unit to limit the magnitude of the distance function
we set
based on the performance on the validation set
the distance between two representations can be written as
adversarial learning we further enhance the supervision with the versarial learning approach
as shown in eq
we use a xed hyper parameter as a weight to measure the strength of the supervision of the toencoder
however in the case when the source content and summary have high relevance the strength of the supervision should be higher and when the source content and summary has low relevance the strength should be lower
in order to determine the strength of supervision more namically we introduce the adversarial learning
more specically we regard the representation of the autoencoder as the gold representation and that of the sequence to sequence as the fake resentation
a model is trained to discriminate between the gold and fake representations which is called a discriminator
the discriminator tries to identify the two representations
on the trary the supervision which minimizes the tance of the representations and makes them ilar tries to prevent the discriminator from ing correct predictions
in this way when the criminator can distinguish the two representations which means the source content and the summary has low relevance the strength of supervision will be decreased and when the discriminator fails to distinguish the strength of supervision will be proved
in implementation of the adversarial learning the discriminator objective function can be written as log pd y log pd y where pd y is the probability that the criminator identies the vector z as the gold resentation while pd y is the probability that the vector z is identied as the fake sentation and d is the parameters of the test stage figure the overview of our model
the model consists of a sequence to sequence model and an autoencoder model
at the training stage we use the autoencoder to supervise the sequence sequence model
at the test stage we use the sequence to sequence model to generate the maries
tains a source content xi


xm and a summary yi


yl while m is the number of the source words and l is the ber of the summary words
at the training stage we train the model to generate the summary y given the source content
at the test stage the model decodes the predicted summary given the source content

supervision with autoencoder figure shows the architecture of our model
at the training stage the source content encoder presses the input contents into the internal sentation zt with a bi lstm encoder
at the same time the summary encoder compresses the ence summary y into the representation
then both zt and zs are fed into a lstm decoder to erate the summary
finally the semantic tation of the source content is supervised by the summary
we implement the supervision by minimizing the distance between the semantic representations zt and zs and this term in the loss function can be written as ls zs nh source content encodersummary encodersummary decodersupervisesource content encodersummary inator
when minimizing the discriminator tive we only train the parameters of the nator while the rest of the parameters remains changed
the supervision objective to be against the criminator can be written as log pd y log pd y pairs in part i pairs in part ii and pairs in part iii
all the text summary pairs in part ii and part iii are manually annotated with relevant scores ranged from to
we only reserve pairs with scores no less than leaving pairs in part ii and pairs in part iii
following the previous work hu et al
we use part i as training set part ii as validation set and part iii as test set
when minimizing the supervision objective we only update the parameters of the encoders

evaluation metric n n
loss function and training there are several parts of the objective functions to optimize in our models
the rst part is the cross entropy losses of the sequence to sequence and the autoencoder lae the second part is the loss of the supervision as written in equation
the last part is the sarial learning which are equation and tion
the sum of all these parts is the nal loss function to optimize
we use the adam kingma and ba timization method to train the model
for the hyper parameters of adam optimizer we set the learning rate
two momentum eters
and
respectively and
we clip the gradients pascanu et al
to the maximum norm of

experiments following the previous work ma et al
we evaluate our model on a popular chinese social media dataset
we rst introduce the datasets evaluation metrics and experimental tails
then we compare our model with several state of the art systems

dataset large scale chinese social media text marization dataset lcsts is constructed by hu et al

the dataset consists of more than text summary pairs constructed from a famous chinese social media website called sina weibo
it is split into three parts with
com our evaluation metric is rouge score lin and hovy which is popular for tion evaluation
the metrics compare an ically produced summary with the reference maries by computing overlapping lexical units including unigram bigram trigram and longest common subsequence lcs
following previous work rush et al
hu et al
we use unigram bi gram and rouge l lcs as the evaluation metrics in the reported experimental results

experimental details the vocabularies are extracted from the training sets and the source contents and the summaries share the same vocabularies
in order to alleviate the risk of word segmentation mistakes we split the chinese sentences into characters
we prune the vocabulary size to which covers most of the common characters
we tune the hyper parameters based on the rouge scores on the validation sets
we set the word embedding size and the hidden size to and the number of lstm layers is
the batch size is and we do not use dropout srivastava et al
on this dataset
following the ous work li et al
we implement the beam search and set the beam size to

baselines we compare our model with the following of the art baselines
rnn and rnn cont are two sequence sequence baseline with gru encoder and coder provided by hu et al

the ference between them is that rnn context has attention mechanism while rnn does not
rnn dist chen et al
is a based neural model which the attention models r l


rnn et al
et al



rnn cont et al






rnn et al



et al



copynet et al



et al



rnn et al



et al



our impl



superae this paper


adversarial learning table comparison with state of the art models on the lcsts test set
and r l note and rouge l spectively
the models with a sufx of w in the table are word based while the rest of models are character based
mechanism focuses on the different parts of the source content
copynet gu et al
incorporates a copy mechanism to allow parts of the erated summary are copied from the source content
srb ma et al
is a sequence sequence based neural model with improving the semantic relevance between the input text and the output summary
drgd li et al
is a deep recurrent generative decoder model combining the coder with a variational autoencoder
is our implementation of the sequence to sequence model with the tion mechanism which has the same mental setting as our model for fair son

results for the purpose of simplicity we denote our vision with autoencoder model as superae
we report the rouge score of our model and the baseline models on the test sets
table summarizes the results of our superae model and several baselines
we rst compare our model with baseline
it shows that our models superae class


class


table accuracy of the sentiment tion on the amazon dataset
we train a er which inputs internal representation provided by the sequence to sequence model and outputs a predicted label
we compute the class and class accuracy of the predicted labels to evaluate the quality of the text representation
superae model has a large improvement over the baseline by

and
rouge l which demonstrates the ciency of our model
moreover we compare our model with the recent summarization systems which have been evaluated on the same training set and the test sets as ours
their results are directly reported in the referred articles
it shows that our superae outperforms all of these models with a relative gain of

and
rouge l
we also perform ablation study by removing the adversarial learning component in order to show its contribution
it shows that the adversarial learning improves the performance of

and
rouge l
we also give a summarization examples of our model
as shown in table the seqseq model captures the wrong meaning of the source content and produces the summary that china united airlines exploded in the airport
our superae model captures the correct points so that the erated summary is close in meaning to the ence summary

analysis of text representation we want to analyze whether the internal text resentation is improved by our superae model
since the text representation is abstractive and hard to evaluate we translate the representation into a sentiment score with a sentiment classier and evaluate the quality of the representation by means of the sentiment accuracy
we perform experiments on the amazon fine foods reviews corpus mcauley and leskovec
the amazon dataset contains users ing labels as well as the summary for the reviews making it possible to train a classier to predict the sentiment labels and a model to generate summaries
first we train the superae model and source last night several people were caught to smoke on a ight of china united airlines from chendu to beijing
later the ight porarily landed on taiyuan airport
some sengers asked for a security check but were nied by the captain which led to a collision tween crew and passengers
reference several people smoked on a ight which led to a collision between crew and passengers
china united airlines exploded in the airport leaving several people dead
superae several people smoked on a ight from chendu to beijing which led to a collision between crew and passengers
table a summarization example of our model compared with and the reference
the model with the text summary pairs until convergence
then we transfer the encoders to a sentiment classier and train the classier with xing the parameters of the encoders
the classier is a simple feedforward neural network which maps the representation into the label tribution
finally we compute the accuracy of the predicted class labels and class labels
as shown in table the model achieves
and
accuracy of class and class respectively
our superae model forms the baselines with a large margin of
and

propose a generator pointer model so that the coder is able to generate words in source texts
gu et al
also solved this issue by rating copying mechanism allowing parts of the summaries are copied from the source contents
see et al
further discuss this problem and incorporate the pointer generator model with the coverage mechanism
hu et al
build a large corpus of chinese social media short text summarization which is one of our benchmark datasets
chen et al
introduce a tion based neural model which forces the tion mechanism to focus on the difference parts of the source inputs
ma et al
propose a neural model to improve the semantic relevance between the source contents and the summaries
our work is also related to the sequence sequence model cho et al
and the toencoder model bengio liou et al

sequence to sequence model is one of the most successful generative neural model and is widely applied in machine translation sutskever et al
jean et al
luong et al
text summarization rush et al
chopra et al
nallapati et al
and other ral language processing tasks
autoencoder gio is an articial neural network used for unsupervised learning of efcient representation
neural attention model is rst proposed by danau et al

conclusion we propose a novel model in which the coder is a supervisor of the sequence to sequence model to learn a better internal representation for abstractive summarization
an adversarial learning approach is introduced to further improve the supervision of the autoencoder
tal results show that our model outperforms the sequence to sequence baseline by a large margin and achieves the state of the art performances on a chinese social media dataset
related work acknowledgements rush et al
rst propose an abstractive based summarization model which uses an tive cnn encoder to compress texts and a neural network language model to generate summaries
chopra et al
explore a recurrent ture for abstractive summarization
to deal with out of vocabulary problem nallapati et al
our work is supported by national natural ence foundation of china no
no
national high technology research and development program of china and the national gram no
thousand young talents program
xu sun is the corresponding author of this paper
references dzmitry bahdanau kyunghyun cho and yoshua neural machine translation by corr bengio

jointly learning to align and translate


timothy baldwin paul cook marco lui andrew mackinlay and li wang

how noisy cial media text how diffrnt social media sources in sixth international joint conference on ral language processing ijcnlp nagoya japan october pages
yoshua bengio

learning deep architectures for ai
foundations and trends in machine learning
qian chen xiaodan zhu zhenhua ling si wei and hui jiang

distraction based neural networks for modeling documents
in proceedings of the international joint conference on articial gence ijcai new york ny
aaai
jianpeng cheng and mirella lapata

neural summarization by extracting sentences and words
in proceedings of the annual meeting of the sociation for computational linguistics acl august berlin germany volume long papers
kyunghyun cho bart van merrienboer c aglar gulcehre dzmitry bahdanau fethi bougares ger schwenk and yoshua bengio

learning phrase representations using rnn encoder decoder in proceedings for statistical machine translation
of the conference on empirical methods in natural language processing emnlp pages
sumit chopra michael auli and alexander m
rush

abstractive sentence summarization with in naacl hlt tentive recurrent neural networks
the conference of the north american chapter of the association for computational guistics human language technologies pages
jiatao gu zhengdong lu hang li and victor o
k
incorporating copying mechanism in li

in proceedings of sequence to sequence learning
the annual meeting of the association for putational linguistics acl
baotian hu qingcai chen and fangze zhu

sts a large scale chinese short text summarization dataset
in proceedings of the conference on empirical methods in natural language ing emnlp lisbon portugal september pages
diederik p
kingma and jimmy ba

adam corr a method for stochastic optimization


jiwei li minh thang luong and dan jurafsky

a hierarchical neural autoencoder for paragraphs and documents
in proceedings of the annual meeting of the association for computational guistics and the international joint conference on natural language processing of the asian eration of natural language processing acl july beijing china volume long papers pages
piji li wai lam lidong bing and zihao wang

deep recurrent generative decoder for stractive text summarization
in proceedings of the conference on empirical methods in natural language processing emnlp copenhagen denmark september pages
chin yew lin and eduard h
hovy

matic evaluation of summaries using n gram occurrence statistics
in human language ogy conference of the north american chapter of the association for computational linguistics naacl
cheng yuan liou wei chen cheng jiun wei liou and daw ran liou

autoencoder for words
neurocomputing
cheng yuan liou jau chi huang and wen chie yang

modeling word perception using the elman network
neurocomputing
thang luong hieu pham and christopher d
ning

effective approaches to attention based in proceedings of the neural machine translation
conference on empirical methods in natural language processing emnlp pages
shuming ma xu sun wei li sujian li wenjie li and xuancheng ren

query and output erating words by querying distributed word in naacl sentations for paraphrase generation

shuming ma xu sun jingjing xu houfeng wang wenjie li and qi su

improving semantic relevance for sequence to sequence learning of nese social media text summarization
in ings of the annual meeting of the association for computational linguistics acl ver canada july august volume short papers pages
sebastien jean kyunghyun cho roland memisevic and yoshua bengio

on using very large in get vocabulary for neural machine translation
proceedings of the annual meeting of the sociation for computational linguistics acl pages
julian john mcauley and jure leskovec

from amateurs to connoisseurs modeling the evolution of user expertise through online reviews
in ternational world wide web conference www rio de janeiro brazil may pages
austin texas usa november pages
kristian woodsend and mirella lapata

matic generation of story highlights
in acl proceedings of the annual meeting of the sociation for computational linguistics pages
jingjing xu xu sun xuancheng ren junyang lin binzhen wei and wei li

gan diversity promoting generative adversarial work for generating informative and diversied text
corr

jingjing xu xu sun qi zeng xiaodong zhang ancheng ren houfeng wang and wenjie li

unpaired sentiment to sentiment translation a cled reinforcement learning approach
in acl
ramesh nallapati bowen zhou ccero nogueira dos santos c aglar gulcehre and bing xiang

abstractive text summarization using sequence in proceedings of the sequence rnns and beyond
signll conference on computational natural language learning conll berlin germany august pages
razvan pascanu tomas mikolov and yoshua bengio

on the difculty of training recurrent neural networks
in proceedings of the international conference on machine learning icml lanta ga usa june pages
dragomir r
radev timothy allison sasha goldensohn john blitzer arda c elebi stanko dimitrov elliott drabek ali hakim wai lam danyu liu jahna otterbacher hong qi horacio saggion simone teufel michael topper adam winkel and zhu zhang

mead a platform for multidocument multilingual text summarization
in proceedings of the fourth international ence on language resources and evaluation lrec
alexander m
rush sumit chopra and jason weston

a neural attention model for abstractive in proceedings of the tence summarization
conference on empirical methods in natural guage processing emnlp lisbon portugal september pages
abigail see peter j
liu and christopher d
manning

get to the point summarization with generator networks
in proceedings of the nual meeting of the association for computational linguistics acl pages
nitish srivastava geoffrey e
hinton alex krizhevsky ilya sutskever and ruslan nov

dropout a simple way to prevent neural journal of machine networks from overtting
learning research
xu sun xuancheng ren shuming ma and houfeng wang

meprop sparsied back propagation for accelerated deep learning with reduced ting
in icml pages
xu sun bingzhen wei xuancheng ren and shuming ma

label embedding network learning bel representation for soft training of deep networks
corr

ilya sutskever oriol vinyals and quoc v
le

sequence to sequence learning with neural works
in advances in neural information ing systems annual conference on neural mation processing systems pages
sho takase jun suzuki naoaki okazaki tsutomu rao and masaaki nagata

neural headline generation on abstract meaning representation
in proceedings of the conference on empirical methods in natural language processing emnlp
