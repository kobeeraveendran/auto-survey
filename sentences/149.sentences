autoencoder as assistant supervisor improving text representation for chinese social media text summarization shuming xu junyang houfeng key lab of computational linguistics school of eecs peking university learning lab beijing institute of big data research peking university of foreign languages peking university shumingma xusun linjunyang abstract
most of the current abstractive text marization models are based on the sequence to sequence model

the source content of social media is long and noisy so it is difcult for to learn an accurate semantic representation

compared with the source content the notated summary is short and well ten
moreover it shares the same
in this work ing as the source content
we supervise the learning of the tation of the source content with that of the summary
in implementation we regard a summary autoencoder as an assistant pervisor of
following previous work we evaluate our model on a popular chinese social media dataset
tal results show that our model achieves the state of the art performances on the benchmark introduction text summarization is to produce a brief summary of the main ideas of the text
unlike extractive text summarization radev et al woodsend and lapata cheng and lapata which lects words or word phrases from the source texts as the summary abstractive text summarization learns a semantic representation to generate more human like summaries
recently most models for abstractive text summarization are based on the sequence to sequence model which encodes the source texts into the semantic representation with an encoder and generates the summaries from the representation with a decoder

the contents on the social media are long and contain many errors which come from spelling mistakes informal expressions and grammatical mistakes baldwin et al
large amount of errors in the contents cause great difculties for text summarization
as for rnn based it is difcult to compress a long sequence into an accurate representation li et al because of the gradient vanishing and exploding problem
compared with the source content it is easier to encode the representations of the summaries which are short and manually selected
since the source content and the summary share the same points it is possible to supervise the learning of the semantic representation of the source content with that of the summary

in this paper we regard a summary coder as an assistant supervisor of
first we train an autoencoder which inputs and
structs the summaries to obtain a better sentation to generate the summaries
then we supervise the internal representation of with that of autoencoder by minimizing the tance between two representations
finally we use adversarial learning to enhance the sion
following the previous work ma al we evaluate our proposed model on a nese social media dataset
experimental results show that our model outperforms the state of art baseline models
more specically our model outperforms the baseline by the score of and rouge proposed model
we introduce our proposed model in detail in this section
notation code is available at lancopku superae given a summarization dataset that consists of n data samples the ith data sample xi yi a m l c
s
c
v
v
i x r a training stage zs
where zs is a function which measures the distance between zs and zt
is a tunable parameter to balance the loss of the supervision and the other parts of the loss and nh is the
ber of the hidden unit to limit the magnitude of the distance function
we set based on the performance on the validation set
the distance between two representations can be written as adversarial learning
we further enhance the supervision with the versarial learning approach
as shown in eq
we use a xed hyper parameter as a weight to measure the strength of the supervision of the toencoder
however in the case when the source content and summary have high relevance the strength of the supervision should be higher and when the source content and summary has low relevance the strength should be lower
in order to determine the strength of supervision more namically we introduce the adversarial learning

more specically we regard the representation of the autoencoder as the gold representation and that of the sequence to sequence as the fake resentation
a model is trained to discriminate between the gold and fake representations which is called a discriminator
the discriminator tries to identify the two representations
on the trary the supervision which minimizes the tance of the representations and makes them ilar tries to prevent the discriminator from ing correct predictions
in this way when the criminator can distinguish the two representations which means the source content and the summary has low relevance the strength of supervision will be decreased and when the discriminator fails to distinguish the strength of supervision will be proved

in implementation of the adversarial learning the discriminator objective function can be written as
log pd y
log pd y where pd y is the probability that the criminator identies the vector z as the gold resentation while pd y is the probability that the vector z is identied as the fake sentation and d is the parameters of the test stage figure the overview of our model
the model consists of a sequence to sequence model and an autoencoder model
at the training stage we use the autoencoder to supervise the sequence sequence model
at the test stage we use the sequence to sequence model to generate the maries
tains a source content xi xm and a summary yi yl while m is the number of the source words and l is the
ber of the summary words
at the training stage we train the model to generate the summary y given the source content at the test stage the model decodes the predicted summary given the source content supervision with autoencoder figure shows the architecture of our model
at the training stage the source content encoder presses the input contents into the internal sentation zt with a bi lstm encoder
at the same time the summary encoder compresses the ence summary y into the representation zs
then both zt and zs are fed into a lstm decoder to erate the summary
finally the semantic tation of the source content is supervised by the summary
we implement the supervision by minimizing the distance between the semantic representations zt and zs and this term in the loss function can be written as ls zs

nh source content encodersummary encodersummary decodersupervisesource content encodersummary inator
when minimizing the discriminator tive we only train the parameters of the nator while the rest of the parameters remains changed
the supervision objective to be against the criminator can be written as
log pd y
log pd y pairs in part i pairs in part ii and pairs in part iii
all the text summary pairs in part ii and part iii are manually annotated with relevant scores ranged from to
we only reserve pairs with scores no less than leaving pairs in part ii and pairs in part iii

following the previous work hu et al we use part i as training set part ii as validation set and part iii as test set
when minimizing the supervision objective we only update the parameters of the encoders
evaluation metric n
n loss function and training
there are several parts of the objective functions to optimize in our models
the rst part is the cross entropy losses of the sequence to sequence and the autoencoder
lae
the second part is the loss of the supervision as written in equation
the last part is the sarial learning which are equation and tion
the sum of all these parts is the nal loss function to optimize
we use the adam kingma and ba

timization method to train the model
for the hyper parameters of adam optimizer we set the learning rate two momentum eters and respectively and

we clip the gradients pascanu al to the maximum norm of experiments following the previous work ma al we evaluate our model on a popular chinese social media dataset
we rst introduce the datasets evaluation metrics and experimental tails
then we compare our model with several state of the art systems
dataset
large scale chinese social media text marization dataset lcsts is constructed by
hu et al

the dataset consists of more than text summary pairs constructed from a famous chinese social media website called sina
it is split into three parts with
our evaluation metric is rouge score lin and hovy which is popular for tion evaluation
the metrics compare an ically produced summary with the reference maries by computing overlapping lexical units including unigram bigram trigram and longest common subsequence lcs
following previous work rush al hu et al we use unigram bi gram and
rouge l lcs as the evaluation metrics in the reported experimental results
experimental details
the vocabularies are extracted from the training sets and the source contents and the summaries share the same vocabularies
in order to alleviate the risk of word segmentation mistakes we split the chinese sentences into characters
we prune the vocabulary size to which covers most of the common characters
we tune the hyper parameters based on the rouge scores on the validation sets
we set the word embedding size and the hidden size to and the number of lstm layers is
the batch size is and we do not use dropout srivastava et al on this dataset
following the ous work li et al we implement the beam search and set the beam size to
baselines
we compare our model with the following of the art baselines
rnn and rnn cont are two sequence sequence baseline with gru encoder and coder provided by hu et al

the ference between them is that rnn context has attention mechanism while rnn does not
rnn dist chen et al is a based neural model
which the attention models r l
rnn al
et al

rnn cont al
rnn et al

et al
copynet al al
rnn et al
et al
our impl
superae this paper

adversarial learning table comparison with state of the art models on the lcsts test set
and r l note and rouge l
spectively
the models with a sufx of w in the table are word based while the rest of models are character based
mechanism focuses on the different parts of the source content

copynet
gu et al incorporates a copy mechanism to allow parts of the erated summary are copied from the source content
srb ma al is a sequence sequence based neural model with improving the semantic relevance between the input text and the output summary
drgd
li et al is a deep recurrent generative decoder model combining the coder with a variational autoencoder
is our implementation of the sequence to sequence model with the tion mechanism which has the same mental setting as our model for fair son
results for the purpose of simplicity we denote our vision with autoencoder model as superae
we report the rouge score of our model and the baseline models on the test sets
table summarizes the results of our superae model and several baselines
we rst compare our model with baseline
it shows that our models superae class class
table accuracy of the sentiment tion on the amazon dataset
we train a er which inputs internal representation provided by the sequence to sequence model and outputs a predicted label
we compute the class and class accuracy of the predicted labels to evaluate the quality of the text representation
superae model has a large improvement over the baseline by and rouge l which demonstrates the ciency of our model
moreover we compare our model with the recent summarization systems which have been evaluated on the same training set and the test sets as ours
their results are directly reported in the referred articles
it shows that our superae outperforms all of these models with a relative gain of and rouge we also perform ablation study by removing the adversarial learning component in order to show its contribution
it shows that the adversarial learning improves the performance of and rouge
we also give a summarization examples of our model
as shown in table the seqseq model captures the wrong meaning of the source content and produces the summary that china united airlines exploded in the airport
our superae model captures the correct points so that the erated summary is close in meaning to the ence summary
analysis of text representation
we want to analyze whether the internal text resentation is improved by our superae model

since the text representation is abstractive and hard to evaluate we translate the representation into a sentiment score with a sentiment classier and evaluate the quality of the representation by means of the sentiment accuracy
we perform experiments on the amazon fine
foods reviews corpus mcauley and leskovec
the amazon dataset contains users ing labels as well as the summary for the reviews making it possible to train a classier to predict the sentiment labels and a model to generate summaries
first we train the superae model and source


last night several people were caught to smoke on a ight of china united airlines from chendu to beijing
later the ight porarily landed on taiyuan airport
some sengers asked for a security check but were nied by the captain which led to a collision tween crew and passengers

reference

several people smoked on a ight which led to a collision between crew and passengers


china united airlines exploded in the airport leaving several people dead

superae

several people smoked on a ight from chendu to beijing which led to a collision between crew and passengers
table a summarization example of our model compared with and the reference
the model with the text summary pairs until convergence
then we transfer the encoders to a sentiment classier and train the classier with xing the parameters of the encoders
the classier is a simple feedforward neural network which maps the representation into the label tribution
finally we compute the accuracy of the predicted class labels and class labels

as shown in table the model achieves and accuracy of class and class respectively
our superae model forms the baselines with a large margin of and
propose a generator pointer model so that the coder is able to generate words in source texts
gu et al
also solved this issue by rating copying mechanism allowing parts of the summaries are copied from the source contents

see et al
further discuss this problem and incorporate the pointer generator model with the coverage mechanism
hu et al
build a large corpus of chinese social media short text summarization which is one of our benchmark datasets
chen et al
introduce a tion based neural model which forces the tion mechanism to focus on the difference parts of the source inputs
ma et al
propose a neural model to improve the semantic relevance between the source contents and the summaries
our work is also related to the sequence sequence model cho et al and the toencoder model bengio liou et al
sequence to sequence model is one of the most successful generative neural model and is widely applied in machine translation sutskever et al jean et al luong et al text summarization rush al chopra et al nallapati et al and other ral language processing tasks
autoencoder gio is an articial neural network used for unsupervised learning of efcient representation

neural attention model is rst proposed by danau et al

conclusion
we propose a novel model in which the coder is a supervisor of the sequence to sequence model to learn a better internal representation for abstractive summarization
an adversarial learning approach is introduced to further improve the supervision of the autoencoder
tal results show that our model outperforms the sequence to sequence baseline by a large margin and achieves the state of the art performances on a chinese social media dataset
related work acknowledgements rush al
rst propose an abstractive based summarization model which uses an tive cnn encoder to compress texts and a neural network language model to generate summaries

chopra et al
explore a recurrent ture for abstractive summarization
to deal with out of vocabulary problem nallapati et al
our work is supported by national natural ence foundation of china no no
national high technology research and development program of china and the national gram no
thousand young talents program
xu sun is the corresponding author of this paper
references dzmitry bahdanau kyunghyun cho and yoshua neural machine translation by corr bengio
jointly learning to align and translate

timothy baldwin paul cook marco lui andrew mackinlay and li wang

how noisy cial media text how diffrnt social media sources

in sixth international joint conference on ral language processing ijcnlp nagoya japan october pages
yoshua bengio

learning deep architectures for ai
foundations and trends in machine learning
qian chen xiaodan zhu zhenhua ling si wei and hui jiang

distraction based neural networks for modeling documents
in proceedings of the international joint conference on articial gence ijcai new york ny
aaai
jianpeng cheng and mirella lapata

neural summarization by extracting sentences and words

in proceedings of the annual meeting of the sociation for computational linguistics acl august berlin germany volume long papers
kyunghyun cho bart van merrienboer c aglar
gulcehre dzmitry bahdanau fethi bougares ger schwenk and yoshua bengio

learning phrase representations using rnn encoder decoder
in proceedings for statistical machine translation
of the conference on empirical methods in natural language processing emnlp pages
sumit chopra michael auli and alexander rush


abstractive sentence summarization with
in naacl hlt tentive recurrent neural networks

the conference of the north american chapter of the association for computational guistics human language technologies pages
jiatao gu zhengdong lu hang li and
victor
incorporating copying mechanism in li


in proceedings of sequence to sequence learning
the annual meeting of the association for putational linguistics acl
baotian hu qingcai chen and fangze zhu

sts
a large scale chinese short text summarization dataset
in proceedings of the conference on empirical methods in natural language ing emnlp lisbon portugal september pages

diederik kingma and jimmy ba
adam
corr a method for stochastic optimization


jiwei li minh thang luong and dan jurafsky


a hierarchical neural autoencoder for paragraphs and documents
in proceedings of the annual meeting of the association for computational guistics and the international joint conference on natural language processing of the asian eration of natural language processing acl july beijing china volume long papers pages
piji li wai lam lidong bing and zihao wang


deep recurrent generative decoder for stractive text summarization
in proceedings of the conference on empirical methods in natural language processing emnlp copenhagen denmark september pages

chin yew lin and eduard hovy

matic evaluation of summaries using n gram occurrence statistics
in human language ogy conference of the north american chapter of the association for computational linguistics naacl
cheng yuan liou wei chen cheng jiun wei liou and daw ran liou

autoencoder for words

neurocomputing

cheng yuan liou jau chi huang and wen chie yang

modeling word perception using the elman network
neurocomputing
thang luong hieu pham and christopher
ning

effective approaches to attention based
in proceedings of the neural machine translation

conference on empirical methods in natural language processing emnlp pages
shuming ma xu sun wei li sujian li wenjie li and xuancheng ren

query and output erating words by querying distributed word
in naacl sentations for paraphrase generation


shuming ma xu sun jingjing xu houfeng wang wenjie li and qi su

improving semantic relevance for sequence to sequence learning of nese social media text summarization
in ings of the annual meeting of the association for computational linguistics acl ver canada july august volume short papers pages sebastien jean kyunghyun cho roland memisevic and yoshua bengio
on using very large
in get vocabulary for neural machine translation

proceedings of the annual meeting of the sociation for computational linguistics acl pages julian john mcauley and jure leskovec

from amateurs to connoisseurs modeling the evolution of user expertise through online reviews
in
ternational world wide web conference www rio de janeiro brazil may pages
austin texas usa november pages
kristian woodsend and mirella lapata

matic generation of story highlights
in acl proceedings of the annual meeting of the sociation for computational linguistics pages

jingjing xu xu sun xuancheng ren junyang


lin binzhen wei and wei li

gan diversity promoting generative adversarial work for generating informative and diversied text

corr
jingjing xu xu sun qi zeng xiaodong zhang

ancheng ren houfeng wang and wenjie li

unpaired sentiment to sentiment translation a cled reinforcement learning approach
in acl
ramesh nallapati bowen zhou ccero nogueira dos santos c aglar gulcehre and bing xiang


abstractive text summarization using sequence
in proceedings of the sequence rnns and beyond

signll conference on computational natural language learning conll berlin germany august pages

razvan pascanu tomas mikolov and yoshua bengio


on the difculty of training recurrent neural networks
in proceedings of the international conference on machine learning icml lanta ga usa june pages
dragomir radev timothy allison sasha

goldensohn john blitzer arda c elebi
stanko dimitrov elliott drabek ali hakim wai lam danyu liu jahna otterbacher hong qi horacio saggion simone teufel michael topper adam winkel and zhu zhang

mead a platform for multidocument multilingual text summarization

in proceedings of the fourth international ence on language resources and evaluation lrec
alexander rush sumit chopra and jason weston


a neural attention model for abstractive
in proceedings of the tence summarization

conference on empirical methods in natural guage processing emnlp lisbon portugal september pages
abigail see peter liu and christopher manning


get to the point summarization with generator networks
in proceedings of the nual meeting of the association for computational
linguistics acl pages
nitish srivastava geoffrey hinton alex krizhevsky ilya sutskever and
ruslan nov
dropout a simple way to prevent neural journal of machine networks from overtting

learning research
xu sun xuancheng ren shuming ma and houfeng wang

meprop sparsied back propagation for accelerated deep learning with reduced ting
in icml pages
xu sun bingzhen wei xuancheng ren and shuming ma

label embedding network learning bel representation for soft training of deep networks

corr
ilya sutskever oriol vinyals and quoc le

sequence to sequence learning with neural works
in advances in neural information ing systems annual conference on neural mation processing systems pages
sho takase jun suzuki naoaki okazaki tsutomu rao and masaaki nagata

neural headline generation on abstract meaning representation

in proceedings of the conference on empirical methods in natural language processing emnlp
