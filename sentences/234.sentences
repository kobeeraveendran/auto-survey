topic aware pointer generator networks for summarizing spoken conversations zhengyuan liu angela ng sheldon lee ai ti aw nancy f
institute for infocomm research singapore changi general hospital singapore t c o l c
s c v
v i x r a abstract due to the lack of publicly available resources conversation summarization has received far less attention than text marization
as the purpose of conversations is to exchange information between at least two interlocutors key tion about a certain topic is often scattered and spanned across multiple utterances and turns from different speakers
this phenomenon is more pronounced during spoken tions where speech characteristics such as backchanneling and false starts might interrupt the topical ow
moreover topic diffusion and intra utterance topic drift are also more common in human to human conversations
such linguistic characteristics of dialogue topics make sentence level tive summarization approaches used in spoken documents suited for summarizing conversations
pointer generator works have effectively demonstrated its strength at integrating extractive and abstractive capabilities through neural ing in text summarization
to the best of our knowledge to date no one has adopted it for summarizing conversations
in this work we propose a topic aware architecture to ploit the inherent hierarchical structure in conversations to further adapt the pointer generator model
our approach nicantly outperforms competitive baselines achieves more efcient learning outcomes and attains more robust mance
index terms dialogue summarization neural works attention mechanism conversation technology
introduction automatic summarization condenses lengthy materials into shorter versions which focuses on the essential tion and the overall meaning
such summaries can enable users to browse and digest information more effectively and this research was supported by funding for digital health from the tute for infocomm research and the science and engineering research council project no

this work was conducted ing resources from the human language technology unit at
we thank r
e
banchs l
f
dharo p
krishnaswamy h
lim f
a
suhaimi and s
ramasamy at and w
l
chow a
ng h
c
oh and s
c
tong at changi general hospital for insightful discussions
efciently which is especially useful in the digital era of formation overload
thus summarization has attracted much research attention over the years
when humans generate summaries they usually rst comprehend the entire content and then extract and concatenate the keywords or salient tences
to obtain more succinct and readable results they further distill and rephrase the most important information
this gives rise to the two main paradigms in automatic summarization extractive and abstractive
recently with sophisticated neural architectures representation learning of linguistic elements and large scale available corpora driven approaches have made much progress in both two paradigms
most work in text summarization focuses on documents such as news or academic articles
on the contrary due to the scarcity of publicly available corpora with ground truth summaries the speech summarization task of human to human spoken conversations has received far less attention even though there is high industry demand and many potential applications across different domains
different from passages human to human spoken versations are a dynamic ow of information exchange which is often informal verbose and repetitive sprinkled with false starts backchanneling reconrmations tions and speaker interruptions
the key information about a certain topic is often at the sub sentence or utterance level and scattered and spanned across multiple utterances and turns from different speakers leading to lower information density more diffuse topic coverage and utterance topic drifts
these spoken characteristics pose technical challenges for sentence level extractive approaches that would inevitably include unnecessary spans of words in the generated summaries
pointer generator networks a neural sequence to sequence design produce summaries via word level extraction and abstractive generation
thus we propose to exploit its advantages to tackle the tioned challenges
meanwhile although conversations are often less structured than passages they are inherently nized around the dialogue topics in a coarse grained structure
topic segmentation has also been shown to be useful in dialogue based information retrieval
such prior ses and investigations inspire us to augment pointer generator networks with a topic level attention mechanism to more gantly attend to the underlying yet often interrupted topical ow in human to human conversations
the dialogue setting in this work is based on a real world scenario where nurses discuss symptom information with post discharge patients to follow up with their status
the conversation summarization task here is targeted at matically generating notes that describe the symptoms the patients are experiencing
our proposed topic aware generator framework exploits the inherent hierarchical ture in dialogues and is able to address the technical lenges of spoken dialogue summarization empirical results outperform competitive baseline architectures signicantly while achieving more efcient and robust learning outcomes

related work in speech and text summarization traditional approaches are studied more on the extractive methods utilizing rule based statistical and graph based algorithms with ous linguistic features like lexical similarity semantic structure or discourse relation
recently end to end neural approaches have been widely adopted in text marization due to their capability exibility and scalability
neural extractive models adopt sentence labeling or ing strategies semantic vector representations and sequential context modeling
for abstractive tasks sequence to sequence models use a neural decoder to ate informative and readable summaries word by word
various methods have been proposed for improvement the attention mechanism helps the decoder concentrate on priate parts of source content the pointer mechanism is effective in handling out of vocabulary words the coverage mechanism is used to reduce generative repetitions
extractive abstractive models have also been proposed to obtain better results
while text summarization focuses on passages such as news articles and academic publications speech summarization has been investigated in monologues such as broadcasts and lectures and multi party dialogues such as meetings
recently neural modeling approaches have also been adopted goo et al
used a sequence sequence model to write headlines for meetings and liu et al
used a hierarchical extractive model to summarize logue spoken documents
in this paper we propose a aware pointer generator architecture for hierarchical context modeling to generate summary notes of spoken conversations

conversation corpus and setup we sampled dialogues as the training set and another distinct dialogues as the validation set according to section

while the training and validation sets were constructed using simulated data the test set was derived from turn dialogues that took place between nurses and patients in the healthcare setting
topic segmentation section
and ground truth summary construction section
were ducted on all these subsets


nurse to patient dialogue data this corpus was inspired by a pilot set of conversations that took place in the clinical setting where nurses inquire about symptoms of patients
linguistic structures at the tic syntactic discourse and pragmatic levels were cally abstracted from these conversations to construct plates for automatically simulating multi turn dialogues
the informal and spontaneous styles of spoken interactions such as interlocutor interruption backchanneling hesitation false starts repetition and topic drift were preserved see ure for an example
a team of linguistically trained personnel rened stantiated and corrected the simulated dialogues by enriching verbal expressions through considering paraphrasing ent regional english speaking styles american british and asian through word usage and sentence patterns validating logical correctness through considering if the dialogues were natural and not disobeying common sense and verifying the clinical content by consulting certied nurses
these sations cover topics symptoms e

headache cough


topic segmentation in dialogue analysis a change in topic corresponds to a change in cognitive attention acknowledged and acted upon by speakers which is usually related to content themes
in this work we specify the dialogue topics according to the symptoms in nurse to patient conversations
figure shows an example of different topic segments where each spans across different utterances and speakers
note that various types of spoken characteristics e

false start could break up topical congruence
a rule based lexical algorithm was used to detect the boundaries between dialogue topics
labels and were respectively added before and after each topic segment
position indices of segment labels are used in section

human verication was conducted to ensure quality control


ground truth summaries the goal of this conversation summarization task is to obtain a concise description characterizing different attributes of a specied symptom
for this particular clinical scenario the summary notes are preferred to be represented in a very tured format to facilitate indexing searching retrieving and extracting in a variety of downstream workow applications e

decision support medical triage
thus paraphrases of a particular symptom are represented using the same entity e

shortness of breath and breathlessness are both resented as symptom breathlessness


attentive architecture the attentive model is similar to that in
it adds an attention layer to a vanilla network to lter out unnecessary contextual information at each decoding step
sequence encoding given a document input one hot word representation an embedding layer converts it to vector representations by a look up operation using the embedding matrix obtaining a vector sequence v


vn where vi rd and d is the embedding dimension
then a bi directional long short term memory bi lstm layer is used to encode vi with forward and backward temporal hi respectively and concatenate them pendencies as as the hidden representation


hn where d is the hidden dimension size
hi and hi hi hi hi hi sequence decoding with attention the decoder is a layer unidirectional lstm generating words step by step
for each decoding step t it receives the word embedding of the previous token then calculates the decoder state st
attention scoring is conducted through concatenation at st battn where we wattn and battn are trainable parameters and is the concatenation operation
attention scores can be viewed as the importance over the input content guiding the decoder to concentrate on the appropriate positions of context for erating the next word
next these attention scores are used to produce a weighted pooling of the encoded hidden states as a xed size representation of what has been read from the source for this step namely the context vector hc t
then hc t is concatenated with the decoder state st and fed through two dense layers to produce a distribution on the vocabulary pt vocab w st w where w vocab is a probability distribution over all words in the vocabulary which will be used to generate decoded tokens
are trainable parameters
pt and

pointer generator networks pointer generator networks are a variant of ture by adding a pointer network
aside from ing words over a xed vocabulary the model is able to copy words via pointing to the source content thus bypassing of vocabulary issues
in the pointer generator model the quence encoding representation h attention distribution at and context vector hc t are calculated as in section

we scribe how the pointer generator model achieves word level extraction and abstractive generation below
fig

a dialogue example of multi turn conversation on nurse patient symptom discussion
colored spans are ances of the same topic
spoken characteristics are preserved and represented in bold font generated ground truth mary
colored spans indicate corresponding topics in the given dialogue example
the summary format is shown in figure where each symptom is listed separately with corresponding attributes such as frequency of symptom or severity of symptom that if a symptom was were mentioned in the conversations
mentioned it will be included in the summary
if there is no signal of a symptom e

cough in the discussion between the nurse and the patient the summary for the tom is represented as cough none while the others would be recorded with key information from the dialogue e

headache every night only a bit
human verication was conducted to ensure quality control

approach in a sequence to sequence model the encoder ceives a token sequence of content


xn of length n and the decoder outputs a token sequence of mary y


ym of length m
the decoder ates words step by step and previously generated words are encoded to provide contextual information at each time step
the task is to learn a function with a parameter set that maximizes the probability to generate readable and ful output text
in this section we describe two baselines an attentive model and a pointer generator network and demonstrate how we integrate topic level attention neural mechanisms
fig

the architecture of our proposed topic aware pointer generator network
pointer mechanism to directly copy words from the source content the attentive distribution at in equation is garded as the copy probability on input sequence ically the token on position with max probability will be tracted as the output
pointer generator switching at each decoding step there is a switching probability to determine whether to generate a token from the xed vocabulary or copy one from the source
pgen for step t is calculated from the context vector ht c the decoder input and the decoder state st gen pt c st bptr where wptr and bptr are trainable parameters and is the moid function
then pt gen is used as a soft switch to choose between copying a word from the input sequence with tion distribution at or generating a word from the lary distribution pt vocab in equation
for each sample we extend the vocabulary with the unique words from the input content
we obtain the following probability distribution over the extended vocabulary tseg


tseg to score topic level attention to obtain a topic aware context representation
we delineate how we integrate topic aware attention to the baseline models below see figure
topic level states we obtain the representations hseg of topic segments by collecting hidden states from h in tion with the topic level segment position indices tseg tseg where k is the topic segment number of the dialogue content
for the output of bi directional lstm we collect the states of forward and backward directions and then concatenate them into one
initial decode state in the two baseline models the last den state of sequence encoding representation is used as the initial state fed to the decoder
here we denote pooling of topic level states hseg as
topic aware contextual representation in each decoding time step t we calculate topic level attention and use aware context vectors as the guide for the ne grained level prediction
first the topic level attention scores are culated via dense layers and softmax normalization aseg t seg e bseg output pt gen pt pt gen at i i wi w next we multiply the attention score with topic level states to obtain the topic aware context vector contrary to the vanilla model which is restricted to a xed vocabulary the ability to copy and generate words is one of the primary advantages of the pointer generator sign
another improvement over vanilla is its age mechanism that avoids repetitions during decoding
more details can be found in
h t hsegaseg t i then the pointing distribution in equation and ulary distribution in equation are inuenced by the aware contextual representation and nal output is produced as in equation

topic aware attentive architecture to exploit topical structure in dialogue modeling with a erarchical architecture another attention layer is introduced at h t st battn pt vocab w st t h model attn attn ta pg net pg net ta proposed



precision recall











precision recall







rouge l



precision recall







table
evaluation results of baselines and the proposed model

experiments

training setup the experiments are conducted on the nurse to patient versation corpus described in section
we implemented an attentive attn and a pointer generator work pg net as baselines a topic aware attentive attn model as control and our proposed aware pointer generator model pg
cross entropy is used to measure the loss between diction and ground truth
for time step t the negative log likelihood of the target word yt is dened as losst and the overall loss is the sum from all the time steps
teacher forcing strategy is applied during training the input is the previous word from the ground truth at test time the input is the previous word predicted by the decoder
in our setup for both the encoder and decoder the sion of word embeddings and hidden states was set to
we adopted pre trained word embedding glove and out vocabulary words and segment labels were initiated with dom vectors
embedding weight sharing strategy was applied by sharing the same embedding matrix wemb for both coder and decoder
this sharing signicantly reduced eter size and boosted the performance by reusing the semantic and syntactic information in one embedding space
the learning rate was xed to
and the batch size was set to
we adopted gradient clipping with a maximum gradient norm of

adam algorithm was used for stochastic optimization with


the vocabulary size was k
we limited source contents to tokens and the decoding length to tokens
we adopted early stop egy with validation in each training epoch
during testing we set the beam search size to


empirical evaluations


evaluation i effectiveness summarization performance is measured using and rouge l scores as shown in table
between the two baselines the pointer generator model tains higher performance than attentive model
fig

evaluation results of learning efciency
itative analysis section

shows that a certain tion of generated tokens are directly copied from the source content demonstrating the effectiveness of using the pointer mechanism in our task
moreover with topic aware attentive modeling both baseline models obtain signicant ment and the proposed pg achieves the best mance
the gains are more prominent for the precision scores see table indicating that the proposed approach generates fewer unnecessary tokens while preserving key information in the generated summary



evaluation ii learning efciency to evaluate the learning efciency of the models we recorded their loss values during training
as shown in figure in the rst batches of iterations loss of the generator decreases faster than that of attentive
moreover by adding topic level attention both attentive and pointer generator are improved and our posed pg performs best
having demonstrated the strength of pointer generator works over attentive model we will focus on the mer in the following experiments



evaluation iii performance model robustness spoken conversations are often verbose with low information density scattered with topics not central to the main dialogue theme especially since speakers chit chat and get distracted during task oriented discussions
to evaluate such ios we adopted model independent addsent where we randomly extracted sentences from squad and inserted fig

switching between pointer generator tokens with higher switching probability are darker in color and are generated from the vocabulary
visualizing topic level attention scores
the darker the shade the higher the scores
model pg net pg net ta rouge l











table
scores from lengthy sample evaluation
eted values denote absolute decrease of model performance in section


them before or after topically coherent segments
the average length of the augmented test set is increased from to
as shown in table topic level attention helps the generator model become more robust to lengthy samples



evaluation iv low resource training limited amount of training data is a major pain point for dialogue based tasks as it is time consuming and intensive to collect and annotate natural dialogues at a scale
we expect our model to perform better in low resource scenarios because it can take advantage of the inherently hierarchical dialogue structure with induction bias
we ducted experiments over a range of training sizes from to
as shown in figure our proposed pg lead to steeper learning curves and always outperforms the baseline


visualization analysis in this section we probe deeper into the proposed neural architecture to examine the innerworkings of how the pointer and generator switches and how topic level tion interacts with the decoded sequence
fig

evaluation results of low resource training
entities and punctuation tokens are generated from the cabulary lexicon
this generation behavior resonates with the rationale used in constructing the ground truth summary in section
enabling the model to normalize symptom entities and handle out of vocabulary words in symptom tributes



topic level attention scoring to show how the proposed framework conducts topic aware contextual modeling we illustrate topic level attention scores aseg in equation
as shown in figure during the summary decoding process of a dialogue with three topics at each step the model concentrates on one topic segment
we also observe smooth topic transition from the attention layer which aligns well with the topical ow of the dialogue content
such topic level modeling can help improve marization performance by ltering out nonessential details at the word level modeling layers

conclusion


pointer generator switching we illustrate the probability of the pointer generator ing pgen in equation that indicates the probability of words generated from the vocabulary to show how the posed model summarizes dialogue
one summary example produced from a dialogue with three topics is shown in figure the attribute information of a symptom and segment tags are directly copied from the source content while symptom in this work we automatically summarized spoken dialogues from nurse to patient conversations
we presented an tive and efcient neural architecture that integrates topic level attention mechanism in pointer generator networks utilizing the hierarchical structure of dialogues
we demonstrated that the proposed model signicantly outperforms competitive baselines obtains more efcient learning outcomes is robust to lengthy dialogue samples and performs well when there is limited training data

references hongyan jing and kathleen r
mckeown the position of human written summary sentences in ceedings of the annual international acm sigir conference on research and development in tion retrieval new york ny usa sigir pp
acm
ramesh nallapati feifei zhai and bowen zhou marunner a recurrent neural network based sequence model for extractive summarization of documents in thirty first aaai conference on articial intelligence
ramesh nallapati bowen zhou cicero dos santos c aglar gulcehre and bing xiang abstractive text summarization using sequence to sequence rnns and in proceedings of the signll beyond ference on computational natural language learning berlin germany aug
pp
association for computational linguistics
t
liu s
liu and b
chen a hierarchical neural summarization framework for spoken documents in icassp ieee international conference on acoustics speech and signal processing icassp may pp

karl moritz hermann tomas kocisky edward stette lasse espeholt will kay mustafa suleyman and phil blunsom teaching machines to read and hend in advances in neural information processing systems pp

abigail see peter j
liu and christopher d
manning get to the point summarization with pointer generator networks in proceedings of the annual meeting of the association for computational linguistics ume long papers vancouver canada july pp
association for computational tics
ilya sutskever oriol vinyals and quoc v le quence to sequence learning with neural networks in advances in neural information processing systems pp

boufaden guy lapalme and yoshua bengio topic segmentation a rst stage to dialog based mation extraction in in natural language processing pacic rim symposium
citeseer
kai hong and ani nenkova improving the estimation of word importance for news multi document rization in proceedings of the conference of the european chapter of the association for computational linguistics pp

gunes erkan and dragomir r
radev lexrank based lexical centrality as salience in text tion j
artif
int
res
vol
no
pp
dec

horacio saggion and thierry poibeau automatic text in summarization past present and future source multilingual information extraction and rization pp

springer
noemie elhadad m y kan judith l klavans and kathleen r mckeown customization in a unied framework for summarizing medical literature cial intelligence in medicine vol
no
pp

t
hirao m
nishino y
yoshida j
suzuki n
yasuda and m
nagata summarizing a document by trimming the discourse tree ieee acm transactions on audio speech and language processing vol
no
pp
nov
justin jian zhang ho yin chan and pascale fung proving lecture speech summarization using rhetorical in ieee workshop on automatic information speech recognition understanding asru
ieee pp

chih wen goo and yun nung chen abstractive logue summarization with sentence gated modeling timized by dialogue acts in ieee spoken guage technology workshop slt
ieee pp

harvey sacks emanuel a schegloff and gail jefferson a simplest systematics for the organization of turn ing for conversation in studies in the organization of conversational interaction pp

elsevier
chris kedzie kathleen mckeown and hal daume iii content selection in deep learning models of rization in proceedings of the conference on pirical methods in natural language processing sels belgium oct

pp
ation for computational linguistics
shashi narayan shay b
cohen and mirella lapata ranking sentences for extractive summarization with in proceedings of the reinforcement learning conference of the north american chapter of the sociation for computational linguistics human guage technologies volume long papers new leans louisiana june pp
tion for computational linguistics
jeffrey pennington richard socher and christopher manning glove global vectors for word tion in proceedings of the conference on cal methods in natural language processing emnlp doha qatar oct
pp
association for computational linguistics
sepp hochreiter and jurgen schmidhuber long term memory neural computation vol
no
pp

sumit chopra michael auli and alexander m
rush abstractive sentence summarization with attentive current neural networks in proceedings of the conference of the north american chapter of the sociation for computational linguistics human guage technologies san diego california june pp
association for computational linguistics
romain paulus caiming xiong and richard socher a deep reinforced model for abstractive tion in proceedings of the international ence on learning representations
wan ting hsu chieh kai lin ming ying lee kerui min jing tang and min sun a unied model for tractive and abstractive summarization using tency loss in proceedings of the annual meeting of the association for computational linguistics ume long papers melbourne australia july pp
association for computational tics
sebastian gehrmann yuntian deng and alexander rush bottom up abstractive summarization in ceedings of the conference on empirical methods in natural language processing brussels belgium oct

pp
association for putational linguistics
alexander m
rush sumit chopra and jason weston a neural attention model for abstractive sentence in proceedings of the conference marization on empirical methods in natural language processing lisbon portugal sept
pp
association for computational linguistics
sameer maskey and julia hirschberg comparing ical acoustic prosodic structural and discourse features for speech summarization in ninth european ence on speech communication and technology
shasha xie and yang liu improving supervised ing for meeting summarization using sampling and gression computer speech language vol
no
pp

zhengyuan liu hazel lim nur farah ain suhaimi shao chuen tong sharon ong angela ng sheldon lee michael r
macdonald savitha ramasamy tra krishnaswamy wai leng chow and nancy f
chen fast prototyping a dialogue comprehension system for nurse patient conversations on symptom monitoring in proceedings of the conference of the north ican chapter of the association for computational guistics human language technologies minneapolis june pp
association for computational linguistics
pararth shah dilek hakkani tur bing liu and gokhan tur bootstrapping a neural conversational agent with dialogue self play crowdsourcing and on line in proceedings of the ment learning ence of the north american chapter of the association for computational linguistics human language nologies volume industry papers new orleans louisiana june pp
association for putational linguistics
mike schuster and kuldip k paliwal bidirectional current neural networks ieee transactions on signal processing vol
no
pp

thang luong hieu pham and christopher d
manning effective approaches to attention based neural machine in proceedings of the conference translation on empirical methods in natural language processing lisbon portugal sept
pp
tion for computational linguistics
oriol vinyals meire fortunato and navdeep jaitly in advances in neural pointer networks tion processing systems c
cortes n
d
lawrence d
d
lee m
sugiyama and r
garnett eds
pp

curran associates inc

diederik p kingma and jimmy ba adam a method for stochastic optimization in proceedings of the international conference for learning representations
chin yew lin rouge a package for automatic uation of summaries in text summarization branches out barcelona spain july pp
tion for computational linguistics
robin jia and percy liang adversarial examples for in evaluating reading comprehension systems ceedings of the conference on empirical methods in natural language processing copenhagen mark sept
pp
association for putational linguistics

