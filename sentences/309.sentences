n u j l c
s c v
v i x r a seal segment wise extractive abstractive long form text summarization yao zhao google research brain team
com mohammad saleh google research brain team
com peter j
liu google research brain team
com abstract most prior work in the sequence to sequence paradigm focused on datasets with put sequence lengths in the hundreds of tokens due to the computational constraints of common rnn and transformer architectures
in this paper we study long form abstractive text summarization a sequence to sequence setting with input sequence lengths up to tokens and output sequence lengths up to tokens
we propose seal a transformer based model featuring a new encoder decoder tention that dynamically extracts selects input snippets to sparsely attend to for each output segment
using only the original documents and summaries we derive proxy labels that provide weak supervision for extractive layers simultaneously with regular supervision from abstractive summaries
the seal model achieves state of the art results on existing long form summarization tasks and outperforms strong baseline models on a new dataset task we introduce with much longer input text
since content selection is explicit in the seal model a desirable side effect is that the selection can be inspected for enhanced interpretability
introduction text summarization is a language generation task that seeks to output concise and informative content given possibly multiple input documents
abstractive summarization aims to summarize text beyond solely copying text segments usually whole sentences using novel words and phrases to be more concise comprehensive or achieve a certain style
neural abstractive summarization is a data driven summarization approach that trains sequence to sequence models on large numbers of document summary pairs and have demonstrated promising results on summarizing relatively short texts where the input documents typically contain up to document tokens and up to summary tokens a few sentences or a paragraph
despite progress in relatively short form settings the development of long form neural abstractive summarization lf nas was constrained due to the lack of large scale long form datasets
recently there has been increasing interest in collecting such datasets for single document sds including for scientic articles from arxiv pubmed and multi document summarization mds although the challenge remains of developing model architectures that can cope with that increased data scale in particular sequence length
prior work on lf nas typically divided the task into two separate stages mainly the result of memory and computation constraints associated with processing long input sequences with rnn or transformer models
the rst extractive stage selects a small fraction of the input text for the second abstractive stage which typically relies on some avor of sequence to sequence model to process extracted inputs into summaries
in this work we systematically study single models for lf nas trained jointly with weak supervision for extractive layers and with regular supervision for abstractive layers
to use a single approach for preprint
under review
both sds and mds we break input documents into sequences of snippets section
and consider three general categories of modeling approach the rst truncates inputs to leading snippets section
the second compressive abstractive compresses all snippets into a xed length representation that is used as input to the abstractive attention layers section
the third extractive abstractive encodes each snippet independently then sparsely selects snippets when applying abstractive attention layers section

the sparse selection is performed by a scorer sub network and gating function
the scorer is weakly supervised by proxy extractive labels automatically calculated from input snippets and gold summaries similar to nallapati et al

the abstractive attention layers employ full encoder decoder attention on the resulting input representations along with self attention which is supervised using the abstractive summaries
during training the extractive and abstractive losses are added together and optimized
we propose the segment wise extractive abstractive long form model section
or seal that generalizes and improves the extractive abstractive approach by augmenting the scorer inputs with previously decoded tokens for each segment allowing the model to dynamically select input snippets during decoding
although interpretability was not our goal as a nice side effect the model shows the link between selected input snippets and decoded segments resulting in better interpretability than soft attention models
we are able to apply our models to very long input documents and achieve state of the art results with the seal model on the arxiv pubmed datasets
to further demonstrate the advantage of the seal model over other approaches we collected a massive new dataset which we use to generate full wikipedia pages from top search results with up to input tokens
related work sequence to sequence has become a dominant paradigm in abstractive summarization using decoder architectures based on rnns and more recently transformer
transformers have shown to be more effective at incorporating long term dependencies than rnns but have issues scaling running into computation and memory limitations quickly beyond sequence tokens in sequence length n
one option is to simply truncate sequences but depending on the task may drop important information
another option is to factor the summarization problem into extractive and abstractive stages
the extract abstract framework has received increased attention recently as a natural way to reduce the input size for abstractive sequence to sequence models
in past work the abstractive model was trained separately from the extractive stage
chen and bansal trained extractor and abstractor sub modules separately then used reinforce to ne tune the non differentiable extractive selection end to end
however the reinforce slows training down signicantly and is not scalable for our setting
amplayo and lapata proposed encoding documents using an encoder derived from a pre trained autoencoder followed by a pooling layer before processing with a separately trained decoder which they call condense abstract
our proposed seal model and baseline models loosely t in the extract abstract or abstract frameworks except they are trained jointly in a single model end to end and without pre training
for unconditional language modeling child et al
investigated scaling transformer by using sparse attention reducing complexity to p n kitaev et al
replaced dot product attention by one that uses locality sensitive hashing changing its complexity to roy et al
endowed self attention with a sparse routing module based on online k means while reducing its complexity to
while dai et al
grave et al
cache state across attention frames to incorporate longer context
pointer networks are supervised to copy input tokens to the output whereas in our tor scorer it is supervised to provide input to the subsequent abstractor layers
shazeer et al
gross et al
used a gating network to save computation in very large networks our scorer gate mechanism also saves computation and memory by restricting attention to small subsets of the input data
in our model the extraction can be interpreted similarly to hard attention which restricts what the downstream model layers decoder see
hard attention has been designed into sequence models previously to increase the interpretability of predictions lei et al

pre training sequence to sequence models using massive external data using a self supervised tive as in raffel et al
song et al
dong et al
zhang et al
has lead to improvements in downstream summarization tasks although this line of work is orthogonal to our focus on scaling the transformer to long inputs
thus we train from randomly initialized weights
sauper and barzilay generated wikipedia extractively from search results for a few categories
liu et al
augmented search results with cited references and generated articles focusing on lead section abstractively with sequence to sequence architectures
our work differs from the latter by focusing on generating full articles and using only search results although many more vs
models figure model architectures
e d s s are encoder decoder and scorers that contains trainable parameters
g is the gating function that selects and concatenates top scored snippet representations up to a certain maximum length
xi are inputs snippet ids each xi is a sequence of ids i are encoded compressed representations of input snippets
y ysegj are current decode ids all previous decode ids and previous decode ids in segment j
in this gure there are in total inputs snippets and decoders always attend up to input representations the seal model is decoding the third segment
figure losses and how gradients ow
the left side are trunc and ca model
the right side are ea model and seal model
and are abstractive and extractive loss red arrows are gradients
in this section we discuss architectures fig
losses fig
and attention maps fig
of three general approaches and our proposed method to deal with lf nas truncated input section
compressive abstractive section
extractive abstractive section
and seal section
our proposed more general form of extractive abstractive model
we encode the text using sub word tokenization similar to with a vocabulary size of
each model generates an output sequence ids y from a sequence of input snippets ids section

a snippet is dened as a continuous text span such as one or few sentences or a paragraph
all our models components are based on transformer
a transformer encoder maps an input sequence of tokens to a sequence of high dimensional vectors in ird using self attention and feed forward layers
a ea seal trunc ca modeltransformer encodera input snippet encoded snippet decodertransformer scorergate next decode decode segment of previous decode trunc ea transformer decoder auto regressively by self attention generates an output sequence of tokens while attending to this input representation by encoder decoder cross attention

unied approach for single and multi document summarization to unify our approach to sds and mds we break a input document or a list of documents into snippets with the following criteria concatenate tokens in continuous sentences until reaching a maximum snippet length lsnpt
this helps to reduce the percentage of paddings empty tokens in each snippet for better compute efciency in the unlikely case of a single sentence exceeding the maximum length truncate that sentence ensure that snippets do not span across document boundaries order by their natural order of appearances within each document
in mds they are ordered by the order in the data section stop adding snippets when their number reaches maximum snippets nsnpt
figure illustration of encoder self attention and encoder decoder attention maps for four models considered
are inputs snippets encoders inputs i are encoded compressed representations encoders outputs decoders inputs that correspond to input snippets
ysj are decode segments decoder s outputs representing parts of the long decode sequence
encoder self attentions from xi to i to ysj are colored in red
note each square represents a sequence of tokens in a input snippet or a decode segment not a single token
i are colored in blue and encoder decoder attentions from
truncated input model trunc one limitation of a standard transformer model is it does not scale well to longer inputs due to the complexity of encoder self attention to input length
we truncate the input sequences as in liu et al
to a maximum input length linput only including the leading few snippets fig

we refer this model as trunc
encoder e and decoder d parameterized by and respectively
as shown in fig
the trunc model is trained using the standard teacher forcing and cross entropy loss over all generated tokens y y
y is the target decode sequence and t is the length of decode
the few snippets have full self attention among themselves and the decoder has full attention over the snippets fig


compressive abstractive ca model the second approach compressive abstractive encodes and compresses continuous snippets into shorter representations and concatenates all representations as the decoder input fig

where c is the transformer encoder that also compresses the input snippets
the ca model is trained with a similar loss la as the trunc model fig

amplayo and lapata pooled each input representation xld into a single vector xd l is the sequence length and d is the representation dimension whereas we compress a snippet group into a self dec short sequence of vectors xcd for richer representation c is the compressed size
a snippet group is a block of continuous k snippets for sds or all snippets within the same document for mds
the compression is implemented by concatenating learnable vectors to transformer encoders inputs and retrieving the processed vectors as compressed representations
as shown in fig
the compressed representation is derived from full self attention to snippets within the compression group and the decoder has full attention to all compressed representations

extractive abstractive ea model the third approach extractive abstractive rst encodes each input snippet separately with e assigns scores to encoded snippets with scorer s a transformer encoder then selects encoded snippets by scores through gating function g
the decoder attends to sequences selected by g fig

each input snippet only has encoder self attention to itself the decoder has attention to selected snippets through the gating function fig

the scorer s utilizes a transformer encoder to map a list of input snippets representations xld n to a list of scores l is the sequence length is the representation dimension and n is the number of snippets
it consists of a attention pooling layer transformer encoder and a feed forward layer
the attention pooling layer wang et al
reduces snippets representations from xld n to xd i n
the transformer encoder process the concatenation of pooled snippets xnd for contextual representation across snippets
the feed forward layer assigns a scores for each contextual snippets representation xd i
in mds we assign a document i d to each snippet and add learnable document embedding to the pooled snippet representation
i i after each snippet is assigned a score by the scorer we apply a gating function g to select snippets based on their scores
it is implemented in the following way sort the snippets by their predicted scores concatenate each snippet representation until their total length reaches a limit
we refer to this length limit as maximum extractive length lext
note concatenation is done by matrix multiplication of the sorting mask an one hot matrix mapping inputs positions to sorted concatenated positions and encoded snippets thus gradients can back propagate through the gating function to the encoder
the encoder e scorer s and decoder d are jointly trained with two losses and
the abstractive loss la is the same as trunc and ca model section

the extractive loss provides supervision for the scorer and encoder to assign higher scores to better snippets for the decoder to attend to
during training we calculate text similarities between gold summary and input snippets as weakly supervised proxy labels and minimize the distance between model predicted scores and proxy labels we minimize the sum of the two losses la and during training as shown in fig

the la loss back propagates to d and e through g while the loss back propagates to s and e
this differs from two stage models where the extractive and abstractive stages use different encoders and trained separately
i n

segment wise extractive abstractive long form seal model the seal model encodes snippets in the same way as the ea model
on the decoder side the model divides the process into non overlapping segments each with size segment length lseg fig

different snippets are selected for the decoder to attend to at each decode segment
s where s is current decoding step s segment with size s starting index
the inputs to the scorer s are all encoded snippets unchanged and prior decode segments changed at the start of each decode segment
the gating function g selects subsets of snippets based on s assigned scores at each segment
this model has the same self attention mask as the ea model and an encoder decoder attention that dynamically changes between decode segments as shown in fig

the dynamic selection of snippets allows more efcient usage of attention memory more targeted proxy labels section and improved interpretability section
segj k into representations ysd segj at the start of each decode segment the encoder e encodes the tokens of each k previous segment ys k is the number of previous decode segments
the wise scorer consists of an attention pooling layer and a transformer encoder
the attention pooling layer pools ysd k and concatenates them to ydk
the transformer encoder s not only processes segj the pooled input snippets xnd by self attention same as in the ea model but also attends to ydk by encoder decoder cross attention such that the scorer is aware of what has been decoded
refer to in bert tensorow the is pooled snippet representations the is pooled previous decode segments
training the seal model is similar to the ea model fig
except the number of supervised labels increases from n to n m where n is the number of input snippets and m is the number of decode segments
the proxy labels are calculated as similarities between each gold summary segment and input snippet
thus the extractive loss is j sij mn
decoding segments are trained in parallel while attending to different inputs snippets
i the seal model is a more general version of the ea and trunc models
when lseg ldec the seal model reduces to an ea model
when lext linput the ea model reduces to a trunc model
datasets table statistics of the long text summarization tasks
the lengths are calculated in the number of subword tokens word on average equals to
subword
input length mean
target length mean

arxiv pubmed
m examples dataset several existing datasets are suitable as benchmarks for lf nas statistics in table

cohan et al
collected scientic publications from arxiv and pubmed and used articles bodies to generate abstracts
to visualize selections by the seal model we use a popular relatively short summarization dataset cnn non anonymized version as in see et al
for ease of presentation
it contains newspaper articles paired with bullet point summaries
liu et al
approached generating the lead section of english wikipedia article as multi document summarization from reference documents wikipedia references and web search results and troduced the wikisum dataset
to demonstrate the performance of our models on much longer input and target sequences we created a similar lf nas dataset named
the dataset consists of full english wikipedia articles as the target summaries and a collection of search result documents about the topic as input documents
the main differences are the input uses the top search result documents compared to the top in wikusum
this is important to demonstrate the effectiveness of our proposed models on longer input sequences we drop wikipedia references which could vary a lot per page
it allows generating pages for entities not currently in wikipedia to make the dataset more abstractive we apply a stronger ltering of wiki clone documents retrieved from search please refer to appendix b for more details
the order of documents presented to model is by their search relevance in descending order
experiments and results experimental details our transformer block has hidden size dmodel feed forward size df and attention heads
we set number of transformer layers to in encoder in scorer and in decoder making the total number of parameters m for the seal model trunc ca and ea model have similar number of parameters
all models are trained with adafactor
com google research bert blob master modeling
py optimizer batch size of dropout rate of
learning rate of
with square root decay
we train the models until perplexity stopped decreasing on dev set steps for arxiv pubmed and steps for
in order to have a clear comparison between models all our models are decoded with greedy decoding
figure on the arxiv dataset trunc models trained on different maximum input length linput
ea models trained on different maximum extractive length lext
effect of segment length lseg and maximum extractive length lext for seal model on the arxiv dataset
amount of input context we show that the amount of input context the decoder attends to is important for lf nas
we limited the amount of input context to the trunc model to linput to and tokens on the arxiv dataset
the performance of the model increases signicantly as the length grows fig
suggesting longer input context is crucial for lf nas
we observe similar trends on other lf nas datasets which is consistent with
input snippets selection we show that the required amount of context the decoder attends to greatly reduces when better snippets are selected
we trained ea models with maximum extractive length lext section
of and tokens on the arxiv dataset
with same number of tokens the ea model achieves much better results compared to the trunc model and plateaus at tokens fig

methods of creating proxy extractive labels we investigated methods to automatically create proxy labels based on multiple text similarity measures including or precision recall scores and whether to create the labels sequentially as in nallapati et al

appendix a shows that specic choice of labeling method does nt make large differences
for simplicity we chose non sequential f in all of our models
segment length and extractive length for the seal model when the segment length lseg is large the decoder effectively attends to less context
on the other hand when lseg is as small as a few words the similarity labels become very spurious and are not meaningful to guide extraction
fig
shows that larger maximum extractive lengths lext are always better and is the optimal segment length for the arxiv datasets
therefore in all other experiments we set the segment length to one eighth of maximum decode length lseg
values of ldec can be found in appendix c on different dataset
table comparison of our models with state of the art baselines
the metrics are rl rouge l
best numbers for each dataset are bolded
there are two types of rouge l scores sentence level and summary level denotes the summary level
denotes pretrained model
model trucated input pegasus trunc ca ea sea y n n y y n n n arxiv rl
























pubmed rl
























rl











input frougel extractive


























f comparison between our models and other works we compare trunc ca ea and seal models on arxiv pubmed and datasets in table
the inputs to trunc models are leading snippets on arxiv pubmed datasets and top snippets ranked by tf idf on dataset similar to liu et al

the inputs are concatenated and truncated to the rst tokens
between our models the ca and ea models have similar performance and consistently outperform the trunc model on all datasets
the seal model performs better than trunc ca and ea on long output datasets
on which has larger number of training examples and longer input sequences the advantage of the seal model is more obvious
prior work on lf nas typically truncates the input or target in some way
truncating the changes the nature of the problem usually making it easier and leading to higher evaluation metrics
in this work we ensure that the model s maximum input linput and decode ldec lengths appendix c exceed the percentile of the corresponding data distribution table so that we closely tackle the intended summarization problem dened by the dataset
the seal model has better performances comparing to previous state of the art extractive and abstractive approaches on lf nas
interpretability decode amir khan wants to ght kell brook in the next year
brook has previously refused to ght brook but now promises it will take place within months
khan has previously promised to ght brook in the ring input khan wants to ght kell brook within the next year paving the way for a spectacular return to wembley stadium for british boxing
as recently as monday khan was talking








rematch with george groves at wembley
said i love to go in the ring and ght kell brook
after watching his brook last performance i know i can do a deal winner takes all
pictured celebrating his win over dan on saturday appears to have got his wish to ght khan
has previously refused a ght with brook but now promises it will take place within months
he added it s all about











trust me i do plans promoter eddie hearn had of staging the ght on june he has a provisional booking with wembley are scuppered by khan s claim to have promised a ght to a different opponent
sportsmail understands khan








figure visualization of the seal model on an cnn dailymail example best viewed in color
segments of decodes are colored differently
input snippets each segment attends to are are colored accordingly and the segment ids are inserted to the front
when multiple segment attend to the same input snippet it is colored as the rst segment
the seal model provides a natural way to inspect what input sequence the decoder is likely using when generating a particular segment of the summary
in this section we show how this enhances interpretability on an example from the cnn dailymail dataset for ease of presentation fig
in the appendix we show more examples on long sds and mds datasets
on cnn dailymail dataset we nd a seal model with lseg lsnpt lext nsnpt achieves of

which is on par with a trunc model of input length
when generating the rst decode segment the model attended to two snippets and copied from the summary like lead sentence
the second segment nishes the rst sentence while continuously attending to the lead snippet and begins a second sentence rephrasing three other snippets
the nal segment writes a new sentence combining multiple snippets
conclusion in this work we studied and compared different lf nas models
we showed that models formance heavily depends on seeing longer sequences from the input and sparsely selecting better content relieves this requirement
we proposed the seal model which encodes input snippets separately and dynamically selects sparse input snippets to attend to when generating different segments of the summary
the seal model achieves state of the art results on existing lf nas datasets including arxiv pubmed and outperform baseline models on our new much longer dataset
broader impact this work may bring more attention to long document summarization research and spur new plications
if successful producers of summaries from long multiple documents may benet from higher productivity due to less manual work while consumers may benet from reduced information overload
a failure case of such a system is that it may generate text that is unfaithful to the source material i
e
factually inaccurate a risk that must be taken into account when deploying
the models biases present in the training data may also be reected in the model output
acknowledgments and disclosure of funding we thank david grangier for feedback and reviewing the manuscript and ben goodrich for helping with earlier iterations of the models
references alexander m
rush sumit chopra and jason weston
a neural attention model for abstractive sentence summarization
in emnlp
ramesh nallapati bowen zhou cicero dos santos caglar gulcehre and bing xiang
stractive text summarization using sequence to sequence rnns and beyond
proceedings of the signll conference on computational natural language learning
doi

url



ilya sutskever oriol vinyals and quoc v
le
sequence to sequence learning with neural networks
abigail see peter j
liu and christopher d
manning
get to the point summarization with pointer generator networks
proceedings of the annual meeting of the association for computational linguistics volume long papers


url



yang liu and mirella lapata
text summarization with pretrained encoders
proceedings of the conference on empirical methods in natural language processing and the international joint conference on natural language processing emnlp ijcnlp
doi

url



sebastian gehrmann yuntian deng and alexander m
rush
bottom up abstractive rization
in emnlp
li dong nan yang wenhui wang furu wei xiaodong liu yu wang jianfeng gao ming zhou and hsiao wuen hon
unied language model pre training for natural language derstanding and generation
in conference on neural information processing systems neurips
arman cohan franck dernoncourt doo soon kim trung bui seokhwan kim walter chang and nazli goharian
a discourse aware attention model for abstractive summarization of long documents
in naacl hlt
peter j
liu mohammad saleh etienne pot ben goodrich ryan sepassi lukasz kaiser and noam shazeer
generating wikipedia by summarizing long sequences
sepp hochreiter and jrgen schmidhuber
long short term memory
neural comput
november
issn

neco




url


neco




junyoung chung caglar gulcehre kyunghyun cho and yoshua bengio
empirical evaluation of gated recurrent neural networks on sequence modeling
ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan n
gomez lukasz kaiser and illia polosukhin
attention is all you need
ramesh nallapati feifei zhai and bowen zhou
summarunner a recurrent neural network in proceedings of the based sequence model for extractive summarization of documents
thirty first aaai conference on articial intelligence pages
aaai press
url
acm
org citation


alec radford jeff wu rewon child david luan dario amodei and ilya sutskever
language models are unsupervised multitask learners

chenliang li weiran xu si li and sheng gao
guiding generation for abstractive text summarization based on key information guide network
in proceedings of the conference of the north american chapter of the association for computational linguistics human language technologies volume short papers pages new orleans louisiana june
association for computational linguistics


url
aclweb
org anthology
sebastian gehrmann yuntian deng and alexander rush
bottom up abstractive summarization
in proceedings of the conference on empirical methods in natural language processing pages brussels belgium october november
association for computational linguistics


url
aclweb
org
sandeep subramanian raymond li jonathan pilault and christopher pal
on extractive and abstractive neural document summarization with transformer language models
yang liu and mirella lapata
hierarchical transformers for multi document summarization
proceedings of the annual meeting of the association for computational linguistics


url



yen chun chen and mohit bansal
fast abstractive summarization with reinforce selected sentence rewriting
in proceedings of the annual meeting of the association for putational linguistics volume long papers pages melbourne australia july
association for computational linguistics


url
aclweb
org anthology
ronald j
williams
simple statistical gradient following algorithms for connectionist forcement learning
machine learning may
issn
doi

url


reinald kim amplayo and mirella lapata
informative and controllable opinion summarization
rewon child scott gray alec radford and ilya sutskever
generating long sequences with sparse transformers
url
com blog sparse transformers
nikita kitaev ukasz kaiser and anselm levskaya
reformer the efcient transformer
aurko roy mohammad saffar ashish vaswani and david grangier
efcient content based sparse attention with routing transformers
zihang dai zhilin yang yiming yang jaime carbonell quoc le and ruslan salakhutdinov
transformer xl attentive language models beyond a xed length context
proceedings of the annual meeting of the association for computational linguistics


url



edouard grave armand joulin and nicolas usunier
improving neural language models with a continuous cache
oriol vinyals meire fortunato and navdeep jaitly
pointer networks
in c
cortes n
d
lawrence d
d
lee m
sugiyama and r
garnett editors advances in neural information processing systems pages
curran associates inc

url papers
nips
cc pointer networks
pdf
noam shazeer azalia mirhoseini krzysztof maziarz andy davis quoc le geoffrey hinton and jeff dean
outrageously large neural networks the sparsely gated mixture of experts layer
sam gross marcaurelio ranzato and arthur szlam
hard mixtures of experts for large scale weakly supervised vision
ieee conference on computer vision and pattern recognition cvpr jul

cvpr


url


cvpr


tao lei regina barzilay and tommi jaakkola
rationalizing neural predictions
in proceedings of the conference on empirical methods in natural language processing pages austin texas november
association for computational linguistics


url
aclweb
org anthology
colin raffel noam shazeer adam roberts katherine lee sharan narang michael matena yanqi zhou wei li and peter j
liu
exploring the limits of transfer learning with a unied text to text transformer
arxiv e prints
kaitao song xu tan tao qin jianfeng lu and tie yan liu
mass masked sequence to sequence pre training for language generation
in international conference on machine learning pages
jingqing zhang yao zhao mohammad saleh and peter j
liu
pegasus pre training with extracted gap sentences for abstractive summarization
christina sauper and regina barzilay
automatically generating wikipedia articles a aware approach
in proceedings of the joint conference of the annual meeting of the acl and the international joint conference on natural language processing of the afnlp pages suntec singapore august
association for computational linguistics
url
aclweb
org anthology
yonghui wu mike schuster zhifeng chen quoc v
le mohammad norouzi wolfgang macherey maxim krikun yuan cao qin gao klaus macherey jeff klingner apurva shah melvin johnson xiaobing liu ukasz kaiser stephan gouws yoshikiyo kato taku kudo hideto kazawa keith stevens george kurian nishant patil wei wang cliff young jason smith jason riesa alex rudnick oriol vinyals greg corrado macduff hughes and jeffrey dean
google s neural machine translation system bridging the gap between human and machine translation
aurko roy and david grangier
unsupervised paraphrasing without translation
proceedings of the annual meeting of the association for computational linguistics
doi

url



wenhui wang nan yang furu wei baobao chang and ming zhou
gated self matching in proceedings of the networks for reading comprehension and question answering
annual meeting of the association for computational linguistics volume long papers pages vancouver canada july
association for computational linguistics
doi

url
aclweb
org anthology
karl moritz hermann tom kocisk edward grefenstette lasse espeholt will kay mustafa suleyman and phil blunsom
teaching machines to read and comprehend
noam shazeer and mitchell stern
adafactor adaptive learning rates with sublinear memory cost
chin yew lin
rouge a package for automatic evaluation of summaries
in text tion branches out pages barcelona spain july
association for computational linguistics
url
aclweb
org anthology
ramesh nallapati feifei zhai and bowen zhou
summarunner a recurrent neural network based sequence model for extractive summarization of documents
wen xiao and giuseppe carenini
extractive summarization of long documents by combining global and local context
a choice of text similarity measure self supervision labeling method ngram sequential false true type precision recall precision recall precision recall precision recall metric























rl











table comparison of ea models trained with different self supervised labeling method on arxiv dataset using the extractive abstractive model
b clone detection in wikisum to detect whether a source document d is a clone of a wikipedia article a the maximum recall of unigrams between each section of a and d is computed as follows a max a clone is detected if a

while this approach detects and lters most of the clones we observed many near clone documents left undetected in the wikisum dataset
most of these near clones are documents that copy small parts of the wikipedia article rather than the whole article or a whole section
to lter these near clones more effectively and make the dataset more abstractive we extended the equation above to a maximum recall of n grams between each section of a and as follows a n max n we experimented with different values of n
a near clone is detected in if a

c model dimensions dataset arxiv pubmed linput lsnpt nsnpt lext ldec lseg nseg table dimensions of the models inputs and outputs for ca ea and seal
length are all in unit of subword tokens
linput is the maximum input length
lsnpt is the snippet length
nsnpt is the maximum number of snippets
linput lsnpt nsnpt
lext is the maximum extractive length
ldec is the maximum decode length
for the seal models lseg is the decode segment length and nseg is the maximum number of segments
ldec lseg nseg

