neural network interpretation via fine grained textual summarization pei guo connor anderson kolton pearson ryan farrell brigham young university p e s v c
s c v
v i x r a abstract recent visualization based neural network interpretation zhou et al
selvaraju et al
algorithms suffer from lacking semantic level information hindering their plication for tasks like ne grained recognition
in this per we introduce the novel task of interpreting classication models using textual summarization
our explanation tence consists image level visual attributes most important for the decision making found by a bayesian inference gorithm
this process amounts to mimicking humans ligence to summarize the content of an image
central to our algorithm is the lter level attribute probability density tion learned as a posterior probability with the input images as latent variables
we generate textual explanation for the dataset with visual attributes extracted from image captions
to demonstrate the accuracy of proposed gorithm we devise two experiments attribute grounding and attribute based image retrieval and provide qualitative and quantitative analysis
we further show that our textual marization can help in understanding network failure terns and can provide clues for further improvements for grained recognition
code will be available upon acceptance
introduction given a convolutional network we re interested in knowing what features it has learned for making classication sions
despite their tremendous success on various computer vision krizhevsky sutskever and hinton simonyan and zisserman he et al
tasks deep neural work models are still commonly viewed as black boxes
the difculty for neural network understanding mainly lies in the end to end learning of the feature extractor sub network and the classier sub network which often contain millions of parameters
debugging an over condent network which assigns the wrong class label to an image with high bility can be extremely difcult
this is also true when versarial noise goodfellow shlens and szegedy is added to deliberately guide the network to a wrong sion
it is therefore desirable to have some textual output plaining which features were responsible for triggering the error just like an intelligent compiler does for a grammar bug in code
network interpretation is also crucial for tasks copyright association for the advancement of articial intelligence www
aaai
org
all rights reserved
figure comparison of visualization based tion zhou et al
and interpretation by summarization the proposed approach
the latter has more semantic details useful for analyzing incorrect predictions
involving humans like autonomous driving and medical age analysis
it is therefore important to distill the edge learned by deep models and represent it in an easy understand way
fine grained recognition concerns the problem of criminating between visually similar sub categories like ferent species of gulls or different versions of bmw cars
humans are usually good at tasks like attribute prediction keypoint annotation and image captioning but we usually nd ne grained recognition to be extremely hard without proper training
network interpretation is therefore useful for ne grained recognition to nd the network s failure terns and to educate humans about what the network thinks as informative features
it is worth noting that the proposed algorithm is not constrained to ne grained recognition
it is equally effective to be applied to general image dataset given accurate image level attribute annotations
current network interpretation are largely based
algorithms like cam zhou et al
and cam selvaraju et al
work by highlighting a region in the image that s important for decision making
however we show in figure that visualization is often inefcient in localizing discriminative parts or providing semantic formation for tasks like ne grained recognition
humans on the other hand can justify their conclusions using ural language
for instance a knowledgeable person ing at a photograph of a bird might say i think this is a anna s hummingbird because it has a straight bill a rose pink throat and crown
it s not a broad tailed bird because the later lacks the red crown
this kind of prediction carolina wrenground truth house wrencam visualizationthis is a carolina wren because it has brown and black crown white breast white eyebrow and short beakthis is a house wren because it has brown crown white eyebrow long tail and pointy and long beaktextual summarization textual description carries rich semantic information and is easily understandable
natural language is a logical medium in which to ground the interpretation of deep convolutional models
in this paper we propose the novel task of summarizing the decision making process of deep convolutional models using ne grained textual descriptions
see figure for an example
to be specic we aim to nd a list of visual tributes that the network bases its decision on
our algorithm is dependent only on image level visual attributes which can be obtained through ground truth annotation or image tion decomposition
central to our algorithm is a method to associate the nal convolutional layer lter with visual tributes that represents its activating patterns
we discover that in our experiment the model lters do nt necessarily activate on a narrow beam of patterns
we therefore mulate the relationship of the filter attribute pair as conditional multinomial probability distribution
an tribute is more likely to represent a lter if images with this attribute better activate this lter
attributes are not directly involved in the network so we introduce images as hidden variables
based on the lter attribute probability density function p


we rank the attributes by re weighting each lter attribute


with class specic weights a step lar to cam zhou et al
or grad et al

our nal textual explanation is a template sentence with top attributes as supportive evidence
in order to demonstrate the accuracy of the proposed gorithm we devise two by product tasks as sanity checks
the rst task is visual attribute grounding
specically we localize the sub region in an image that is related to a query attribute
note this task is weakly supervised only by gory labels
this is achieved by the linear combination of the nal layer feature map according to the lter attribute



the second experiment is visual attribute based age retrieval
for a query attribute we obtain a list of date images containing this attribute
decent results on these two tasks serve as a strong indicator that the core algorithm works properly
a direct application of the proposed textual explanation algorithm is to work as a network debugger and ate error messages when the network prediction is wrong
we summarize the three major failure patterns for the grained dataset
the rst and most common failure pattern is the network fails to identify true native features
the network is confused by small inter class variation and large intra class variation
the second failure pattern is the network is not robust to image perturbations such as color distortion and low image quality
the last ure pattern is caused by incorrect human labels
there re roughly annotation errors in cub
our main contributions in this paper can be summarized as follows we propose the novel task of network interpretation using textual summarization
we identify lter attribute


as our core problem and propose a bayesian inference framework to learn it
we devise two tasks for automatic quantitative evaluation figure a the lter attribute association is to nd a ping from the abstract functional space to the semantic tribute space
b the composite lter function can be dinate transformed to be dened on the attribute space
c top activation images of a lter a yellow head detector
of the learned p


demonstrating the accuracy of the proposed algorithm
we employ the proposed framework for network ging in ne grained recognition and unveil common ure patterns useful for further improvement
related works network interpretation there are two main approaches to network interpretation in the literature lter level pretation erhan et al
szegedy et al
dran and vedaldi nguyen yosinski and clune google nguyen et al
bau et al
zhou et al
yosinski et al
springenberg et al
zeiler and fergus and holistic level interpretation monyan vedaldi and zisserman zhou et al
selvaraju et al

the goal of lter level interpretation is to understand and visualize the features that specic rons learn
while it s easy to directly visualize the rst volutional layer lter weight to get a sense of the patterns they detect it makes little sense to directly visualize deeper layer lter weights because they act as complex ite functions of lower layers operations
early examples of lter level understanding include nding the maximally tivated input patches zeiler and fergus and ing the guided back propagation gradients springenberg et al

some works nguyen yosinski and clune try to synthesize visually pleasant preferred input image of each neuron through back propagation into the image space
nguyen et al
applies a generator network to ate images conditioned on maximally activating certain layer neurons
the plug and play paper nguyen et al
further extends nguyen et al
to introduce a ized adversarial learning framework for lter guided image generation
network dissection bau et al
measures the interpretability of each neuron by annotating them with predened attributes like color texture part
zhang et al
proposes represent the image content and structure by knowledge graph
attempts at holistic summarization mainly focus on sualizing important image subregions by re weighting nal convolutional layer feature maps
examples include cam zhou et al
and grad cam selvaraju et al

however the visualization based method only red faceyellow bellyabstract functional spaceattribute spaceyellow crownyellow headorange crowninput imageactivation functiontop activation images figure textual explanations generated by our method vides coarse level information and it remains hard to itively know what feature or pattern the network has learned to detect
more importantly the holistic heat map tation is sometimes insufcient to justify why the network favors certain classes over others when the attentional maps for different classes overlap heavily
see figure for ple
vqa and image caption other tasks that combine text generation and visual explanation include image captioning and visual question answering vqa
although it sounds like a similar task image captioning farhadi et al
is fundamentally different from ours
image captioning is usually done in a fully supervised manner with the goal of generating a caption that describes the general content of an image
our textual interpretation task aims to loyally reects the knowledge learned by a classication model in an pervised way
visual question answering antol et al
is a task that requires understanding the image and ing textual questions
our task can be viewed as a special case of unsupervised vqa that focuses more specically on questions such as why does the model think the image longs to class x
text grounding rohrbach et al
is a language to vision task that tries to locate the object in an image referred to by a given text phrase
we note that dricks al
denes a task similar to ours to explain and justify a classication model
their model is learned in a supervised manner with explanations generated from an lstm network which only implicitly depends on the nal feature maps
it is essentially an image captioning task that generates captions with more class discriminative mation
our method is unsupervised and does not rely on another black box network to generate descriptions
fine grained recognition fine grained recognition aims to discriminate between subcategories like species of birds dogs and different make and model of cars aircrafts
the difculty of ne grained recognition lies in the tremely large intra class variance and small inter class ance
representative works include bilinear pooling lin roychowdhury and maji which computes the outer product of the nal layer feature maps
attention based els sermanet frome and real works by focusing tention to discriminative parts of an image object
part based models zhang et al
works by decomposing the age into part features to be readily compared
fine grained recognition is special because it usually performs better than non expert humans
it s therefore interesting to unveil the knowledge it learns towards decision making
bayesian inference framework as a fundamental step toward network interpretation we re interested in representing network lter with its representing activation patterns in terms of visual attributes
constructing a paired filter attribute dataset is unrealistic cause the lter as a composite function is not a well dened concept with concrete examples
instead we propose aging off the shelf image attribute annotations because they contain rich textual references to visual concepts
the tuition behind our lter attribute association is simple the model lters can be represented by the images that strongly activate them
the corresponding image attributes should have a high probability of representing an activated lter
the joint consensus of all textual attributes from the whole dataset can serve as a good indicator of the lter pattern provided the network is properly trained
more formally the composite lter function takes an age as input and produces a feature map whose strength dicates the existence of certain patterns
the lter tation task aims to nd a mapping from the abstract tional space to the semantic visual attribute space figure
with the help of image level attributes we consider a ordinate transformation operation that transforms the input of lter function from image to image attributes
the attribute probability distribution obeys multinomial tion and can be approximated by the attribute probability density function which is a key component of the proposed algorithm
a
this is a american pipit because it has short beak yellow and white breast brown crown and yellow throat
b
this is a eared grebe because it has black crown long neck black head red eye and white throat
c
this is a groove billed ani because it has black head thick and pointy and black beak and black crown
d
this is a parakeet auklet because it has black and red crown black head white breast and black throat
e
this is a philadelphia vireo because it has yellow breast grey head yellow throat short beak and white eyebrow
f
this is a great grey shrike because it has white breast black crown white throat black beak and black cheek patch
filter attribute probability density function we note f


n as the group of model lters
in this paper we are only interested in the nal convolutional layer lters as they are the input to the fully connected layer
we denote x


m as the set of input ages
the lter s output is naturally written as f which we call a feature map or lter activation
we consider els he et al
huang et al
with a global pooling layer and one fully connected layer
the fully connected layer produces class label predictions c


o with the weight matrix w on
a list of textual attributes t


l is attached to each image
we loosely ti if ti is contained in image s attribute list
we propose a bayesian inference framework to learn the probability of visual attributes that can represent lter terns
we call as the lter attribute probability sity function p

and it can be formulated as a posterior probability is the prior probability for visual attribute tj
we consider the relative importance of attributes because they carry different information entropy
for example small bird has less information than orange beak because the latter appears less in the text corpora and corresponds to a more important image feature
we employ the normalized tf idf feature as the attribute prior
measures the likelihood of attribute tj activating lter fi
as attributes are not directly involved in the neural network we introduce input images as latent variables measures the likelihood of the image xk and the attribute tj is the reason for lter fi s activation
we assume fi is conditionally independent to tj given xk where is the normalization function and is the global pooling layer output
the strength of the feature map measures how likely an image can activate a lter
this proximation neglects the fact that when an image activates a lter the feature map favors certain attributes than ers
for example if the highlights the head area of a bird attributes related to head beak or eyes should be assigned with higher probabilities than attributes related to wings and feet
this naive approximation though signs equal probability to every visual attribute
this imation actually works decently as the joint consensus of all input images highlights the true attributes and suppresses false ones
one way to associate the spatial distribution of the feature map with corresponding visual attribute is to ploit other forms of annotations like keypoints or part mentation
if the feature map overlaps highly with certain part segmentation higher probability will assigned to the corresponding visual attributes
this approach is dependent on additional forms of human annotations and hinders the generalization of the proposed algorithm so it s not used in this paper
measures the likelihood that tj is an attribute of image xk
it takes when tj is in the attribute list of and otherwise tj xk otherwise
aggregating filter attribute p


s for holistic tion with the help of lter attribute


we can gure out what features the network has learned for image cation
this problem can be formulated as the probability of visual attributes given the fact that the network produces certain class label for certain input image
we introduce nal convolutional layer lters as hidden variables here cm xi cm cm cm where cm is the probability that tj is the reason that the network predicts xi as class cm
we assume tj is tionally independent to and cm given
is the lter attribute



cm measures the importance of a lter fk in the decision making process cm wm k where is the normalization function wm k is the weight from the classier weight matrix connecting lter fk to class prediction cm and is the global pooling layer put
we call c the image class attribute p



we generate a natural sentence to describe the network decision making process using the image class attribute



although it s popular to employ a recurrent model for sentence generation our task is to faithfully reect the internal features learned by the network and introducing other network could result in more uncertainty
we instead propose a simple template based method which has the lowing form this is a class name because it has attribute attribute


and attribute n
we consider only the top attributes to make the sentence shorter and more precise
steps are taken to merge adjectives related to the same nouns
another important aspect of model interpretation is to compare the reasons behind certain choices as opposed to others i
e
why the network thinks the input belongs to class ci instead of cj
we can easily summarize the relation and the difference between two predictions by comparing their image class attribute p



for example while both birds have long beaks the class ci favors a green crown while the class cj tends to have a blue crown
an example is shown in figure

explain away mechanism the lter attribute


obeys a multinomial distribution
it does not necessarily tivate on only one narrow beam of features
instead it may behaves like a multi modal gaussian distribution that vates on several totally different features
for example the lter fi is likely to detect both blue head and black head with high probability
the interpretability of the lter could suffer from this multi modal characteristic
this is cially true for the image description task because it becomes hard to know exactly which feature activates the lter
tasks with experiments and provide qualitative and tive analysis in the experiment section
other potential plications are left to future work
figure example of caption annotations on cub
the tracted visual attributes are highlighted
however we observe that other lters can act in a plimentary way to help explain away the probability of related patterns
for instance there could be another lter fk activates for blue head but not for black head
if both lters activate then the probability of blue head is high
if only the fi activates then black head is the more ble pattern
the joint consensus of all the lters makes the generated explanation reasonable
class level description given the lter attribute


s we are interested in knowing which features are important for each class
this task can be formulated as the probability of visual attributes given the fact the network predicting an image as class cm cm where is the lter attribute


and we again sume tj is conditionally independent to cm given f
measure the importance of a lter for class ci and is simply k where is the normalization function and wm k is the weight from the classier weight matrix connecting lter fk to class prediction cm
different from the image class attribute p


class level description weights attributes based only on the classier weight and the lter attribute



for difcult tasks like ne grained recognition deep models often perform better than non expert users
the knowledge distilled from class level description could potentially be used to teach users how to discriminate in challenging domains
applications for textual summarization in order to demonstrate the accuracy of the learned p


s we devise two by product tasks namely visual attribute grounding and attribute based image retrieval as sanity checks
success in these tasks would serve as a strong dicator of the effectiveness of our method
one direct cation of the proposed textual summarization algorithm is to understand the network s failure patterns and provide gestions for future improvement
we validate the proposed figure unnormalized lter attribute


s along with the top activation images
visual attribute grounding given a query visual tribute we would like to know which image region it refers to
we show how the lter attribute


can help with this task
suppose ti is a visual attribute associated with image xj we formulate the image region of interests roi y as a linear combination of nal convolutional layer feature maps y k n where is the normalization function and k
tuitively we re weight the lter responses according to lter attribute p



this task is weakly supervised by image bels with no ground truth roi phrase pairs to learn from
if the algorithm fails to learn accurate lter attribute


we would expect the grounded region to be less accurate
attribute based image retrieval we would like to be able to search a database of images using textual attribute queries and return images that match
for example we would like to nd all images of birds with white head and black throat
the image class attribute p


provides a simple method to rank images based on the probability of containing the desired attributes
given a query visual tribute we simply return all the images that contains the query in the generated textual explanation sentence
network debugging when the network produces a wrong prediction we would like to understand the reason
for the ne grained dataset we generate textural summarization for all failure cases to explain why the network favors the wrong prediction instead of ground truth prediction
we unveil common failure patterns of the network that are helpful for network improvement
experiments we demonstrate the effectiveness of our algorithm on the ne grained dataset wah et al
with training images and testing images
image level visual attributes can be obtained directly from binary tribute annotation or by image caption reed et al
this bird has a long black bill red cheek patches grey head and black spotted feathers
this bird has a red cheek patch brown and grey head grey neck black throat and light brown coverts with black specks
a multicolored bird with black spots a red malar strip and a long pointed bill
orange billlong white necklong neckblack crownwhite necklarge orange beaklong billlong orange beakwhite throatlong yellow breastred chestbright red breastblack headred throatblack crownred crownblack beakwhite beakwhite bill decomposition
we choose the second route because the age captions contain rich and diverse visual attributes better suited for our purpose
one example is shown in figure
we use as our convolutional model a which is trained on imagenet deng et al
and ne tuned on cub
we use bounding box cropped images to reduce ground noise
figure attribute based image retrieval
each row shows an attribute query on the left followed by the ranked results in terms of probability that the image tains the query attributes
visual attribute extraction we rst extract visual tributes from the image captions
we follow the pipeline of word tokenization part of speech tagging and noun phrase chunking
for simplicity we only consider adjective noun type attributes with no recursion
we end up with dependent attributes
the term frequency tf of phrase t is computed as the number of occurrences of t in the same captioning le
for cub each image has a caption le with different captions
the inverse document frequency idf is d where n is the the total number of les and d in the number of les containing phrase
filter attribute p
d
f
and textual summarization we show examples of lter attribute


s in figure
we see a clear connection between the top activated images an the top ranked attributes
this validates our idea of using tual visual attributes to represent the lter pattern
we show examples of generated textual explanations for image sication in figure
we can see that the generated nations capture the class discriminative information present in the images
visual attribute grounding in figure we show amples of query attributes and the generated raw heatmaps indicating what part of the image the visual attribute refers to
each column denotes a different visual attribute and the heatmap shown as a transparency map indicates the region of highest activation within the image
we can see tively that the proposed approach is reasonably good at lighting region of interests
as the visual attributes are highly correlated with points to quantitatively measure the performance of the posed visual attribute grounding algorithm we compare the generated heatmap max value location with ground truth keypoint locations
we present the pck percentage of rect keypoints score for the top most frequent visual attributes with corresponding keypoint annotations
in means the predicted location is within the distance of object size from the ground truth keypoint
note that visual attribute grounding is neither supervised by keypoints nor optimized for keypoint detection
we compare with two baseline methods
one is to randomly assign a location for the attribute
the other baseline is similar to the proposed method except that the lter attribute


is constant for all attributes
we show in table that our learned p


forms better than both baseline methods demonstrating the accuracy of the proposed algorithm
table for attribute grounding random constant p


proposed











attribute based image retrieval in figure three amples of attribute based image search using text based tributes are shown
images are ranked from high to low using the probability that the image contains the query attributes
the results are very encouraging each image clearly tains the query attributes
we measure the performance of attribute based image trieval by comparing it with the ground truth caption based retrieval for the top attributes as seen in table
note these numbers are only approximations as the ground truth caption does nt necessarily contain every true attribute in the image
the fact our method performs better than random trieval demonstrates the accuracy of the underlying class attribute p



figure examples of text grounding
each column resents a different attribute and examples are shown as heatmaps indicating the region where the attribute is most present
table image retrieval measurements recall true negative accuracy


network debugging in gure we show three major patterns of network failure through textual summarization
in the rst example a tree sparrow is incorrectly nized as a chipping sparrow because the network enly thinks long tail is a discriminative feature
failing to identify effective features for discrimination is the most common source of errors across the dataset
in ne grained classication the main challenge is to identify white eyebrowwhite headblack breastyellow headyellow breast figure analysis of network failures for network debugging
each row represents a network failure an incorrectly predicted class label
from left to right each row shows the query image canonical images for the ground truth and incorrectly predicted classes and explanations for each of these classes
the box below the rst row provides background on differences between tree sparrows and chipping sparrows
tive features for visually similar classes differences which are often subtle and localized to small parts
pect the explanations for incorrectly classied images tain noisy features and thus less accurate
the second example shows a seaside sparrow that has mistakenly been recognized as a blue grosbeak
from the textual explanations we ascertain that the low image quality mistakenly activates lters that correspond to blue head and blue crown
the underlying source of this error is complex the generalization ability of the network is limited such that small perturbations in the image can result in unwanted lter responses
such failures imply the critical importance of improving network robustness
in the third case the network predicts the image as a yellow warbler however the ground truth label is bellied flycatcher
according to a bird expert the network got this correct the ground truth label is an error
the work correctly identies the yellow crown and yellow head both obvious features of the yellow warbler
errors like this are not surprising because according to van horn et al
roughly of the class labels in the cub dataset are incorrect
the mistake shown in figure could also be a false negative and it indicates the classier may not learn to assign correct weights to discriminative features
to quantitatively measure the accuracy of generated planations we compute their sentence bleu scores which measure the similarity of the generated sentence with ground truth caption annotations
we show in table that erally explanations are more accurate for correctly ed images
our textual explanation is nt directly optimized to mimic the image caption annotations but we would table blue score correct wrong overall


conclusion in this paper we propose a novel task for network tion that generates textual summarization justifying the work decision
we use publicly available captioning tions to learn the filter attribute relationships in an unsupervised manner
the approach builds on the intuition that lter responses are strongly correlated with specic mantic patterns
leveraging a joint consensus of attributes across the top activated images we can generate the level attribute p



this further enables holistic level planations by combining visual attributes into a natural tence
we demonstrate the accuracy of the proposed rithm by visual attribute grounding and attribute based age retrieval
we employ the textual explanation as network debugging tool and summarize common failure patterns for ne grained recognition
future work includes experiments on additional models and datasets
the algorithm can also be generalized to ing from weaker class level caption annotations
word bedding methods such as mikolov et al
american tree sparrows left have a rufous stripe through the eye on chipping sparrows right it black
tree sparrows also have a spot in the middle of the breast and a bicolored bill that chipping sparrows do have
this is a chipping sparrow because it has long tail white breast short beak and brown and red crown
this is a tree sparrow because it has black throat white breast short beak and brown and red crown
this is a seaside sparrow because it has brown and yellow crown black throat yellow eyebrow and short beakthis is a blue grosbeak because it has brown and blue crown black throat short beak and blue headthis is a yellow bellied flycatcher because it has bright yellow breast red crown and gray headthis is a yellow warbler because it has red and yellow body yellow crown yellow head and yellow breast query ground truth class predicted class rationale for ground truth rationale for predicted figure top most frequent noun phrases
can be utilized for learning to embed and group semantically similar words together
keypoint based annotations can be used to assign different weights for attributes according to the spatial distribution of the feature map
potential tions include explaining adversarial examples and based zero shot learning
references antol s
agrawal a
lu j
mitchell m
batra d
lawrence zitnick c
and parikh d

vqa visual question answering
in the ieee international conference on computer vision iccv
bau d
zhou b
khosla a
oliva a
and torralba a

network dissection quantifying interpretability of deep visual representations
in computer vision and pattern recognition
deng j
dong w
socher r
li l

li k
and imagenet a large scale hierarchical fei l

age database
in computer vision and pattern recognition
cvpr
ieee conference on
ieee
erhan d
bengio y
courville a
and vincent p

visualizing higher layer features of a deep network
nical report university of montreal
farhadi a
hejrati m
sadeghi m
a
young p
rashtchian c
hockenmaier j
forsyth david e
k
maragos p
and paragios n

every picture tells in computer a story generating sentences from images
vision eccv
berlin heidelberg springer berlin heidelberg
goodfellow i
j
shlens j
and szegedy c

ing and harnessing adversarial examples
arxiv e prints
google
inceptionism going deeper into neural works

googleblog
inceptionism going deeper into neural
html
he k
zhang x
ren s
and sun j

deep residual learning for image recognition
in cvpr
he k
gkioxari g
dollar p
and girshick r

mask r cnn
in the ieee international conference on puter vision iccv
hendricks l
a
akata z
rohrbach m
donahue j
schiele b
and darrell t

generating visual in european conference on computer vision planations

springer
huang g
liu z
van der maaten l
and weinberger k
q

densely connected convolutional networks
in the ieee conference on computer vision and pattern recognition cvpr
krizhevsky a
sutskever i
and hinton g
e

agenet classication with deep convolutional neural works
in nips
lin t

roychowdhury a
and maji s

ear cnn models for fine grained visual recognition
in iccv
mahendran a
and vedaldi a

understanding deep image representations by inverting them
in the ieee conference on computer vision and pattern recognition cvpr
mikolov t
chen k
corrado g
and dean j

cient estimation of word representations in vector space
arxiv preprint

nguyen a
dosovitskiy a
yosinski j
brox t
and clune j

synthesizing the preferred inputs for rons in neural networks via deep generator networks
in vances in neural information processing systems
nguyen a
clune j
bengio y
dosovitskiy a
and yosinski j

plug play generative networks tional iterative generation of images in latent space
in the ieee conference on computer vision and pattern tion cvpr
nguyen a
yosinski j
and clune j

deep neural networks are easily fooled high condence predictions for unrecognizable images
in the ieee conference on puter vision and pattern recognition cvpr
reed s
akata z
yan x
logeswaran l
schiele b
and lee h

generative adversarial text to image thesis
arxiv preprint

rohrbach a
rohrbach m
hu r
darrell t
and schiele b

grounding of textual phrases in images by reconstruction
arxiv e prints
black crownshort beakwhite breastpointy beakblack beakblack headwhite throatblack billbrown crownsmall beakyellow breastwhite bodyblack eyesmall billlong billlong tailpointed billred crownlong neckyellow billshort billsmall headshort pointy billwhite headyellow beakblack eyeringblack throatwhite crownlong beakyellow crowngrey crowngrey headblue crownbrown headyellow bodyblue headblack bodyyellow throatblack cheek patchwhite chestpointed beakblack taillong black billbrown beakgray crownblack wingred headwhite eyebrowyellow headsharp beakbrown breastblack breastwhite eyeringsmall black billwhite neckbrown bodyblack tarsuslight browndark brownred breastlong black beakcheek patchwhite wingblack backthick billsharp billsmall black beaklarge beakgray headwhite napeshort black billdark greywhite eyelarge headwhite beakshort pointed billbrown eyered eyeblack napegreen crownred beakgreen headsmall brownshort black beakwhite billlarge billbright orangesharp black beakgray bodyyellow chestbright yellowred throatwhite stripeblack faceblack stripeshort tailwhite eye ringgray beakflat billgrey selvaraju r
r
cogswell m
das a
vedantam r
parikh d
and batra d

grad cam visual nations from deep networks via gradient based localization
in the ieee international conference on computer vision iccv
sermanet p
frome a
and real e

attention for fine grained categorization
in iclr
simonyan k
and zisserman a

very deep volutional networks for large scale image recognition
in iclr
simonyan k
vedaldi a
and zisserman a

deep inside convolutional networks visualising image cation models and saliency maps
arxiv e prints
springenberg j
t
dosovitskiy a
brox t
and miller m

striving for simplicity the all lutional net
arxiv e prints
szegedy c
zaremba w
sutskever i
bruna j
erhan d
goodfellow i
and fergus r

intriguing ties of neural networks
arxiv e prints
van horn g
branson s
farrell r
haber s
barry j
ipeirotis p
perona p
and belongie s

building a bird recognition app and large scale dataset with citizen scientists the ne print in ne grained dataset collection
in proceedings of the ieee conference on computer vision and pattern recognition
wah c
branson s
welinder p
perona p
and belongie s

the caltech ucsd dataset
technical report cns
yosinski j
clune j
nguyen a
fuchs t
and lipson h

understanding neural networks through deep visualization
arxiv e prints
zeiler m
d
and fergus r

visualizing and standing convolutional networks
eccv
zhang n
donahue j
girshick r
and darrell t

part based r cnns for fine grained category detection
in eccv
zhang q
cao r
shi f
nian wu y
and zhu s

interpreting cnn knowledge via an explanatory
graph
aaai
zhou b
khosla a
lapedriza a
oliva a
and torralba a

object detectors emerge in deep scene cnns
iclr
zhou b
khosla a
lapedriza a
oliva a
and torralba a

learning deep features for discriminative ization
in the ieee conference on computer vision and pattern recognition cvpr

