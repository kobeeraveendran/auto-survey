n a j g l
s c v
v i x r a journal of ieee transactions on data engineering vol
xx no
xx xxxx attentive representation learning with adversarial training for short text clustering wei zhang member ieee chao dong jianhua yin and jianyong wang fellow ieee abstract short text clustering has far reaching effects on semantic analysis showing its importance for multiple applications such as summarization and information retrieval
however it inevitably encounters the severe sparsity of short text representations making the previous clustering approaches still far from satisfactory
in this paper we present a novel attentive representation learning model for shot text clustering wherein cluster level attention is proposed to capture the correlations between text representations and cluster representations
relying on this the representation learning and clustering for short texts are seamlessly integrated into a unied model
to further ensure robust model training for short texts we apply adversarial training to the unsupervised clustering setting by injecting perturbations into the cluster representations
the model parameters and perturbations are optimized alternately through a minimax game
extensive experiments on four real world short text datasets demonstrate the superiority of the proposed model over several strong competitors verifying that robust adversarial training yields substantial performance gains
index terms short text clustering representation learning attention mechanisms robust adversarial training introduction r ecent years have witnessed the fast growing trade of short text data in various kinds of social media for example twitter instagram and sina weibo
as a consequence short text clustering the task of automatically grouping multiple unlabeled texts into a number of clusters has become increasingly important
it can benet multiple content centric downstream applications such as event exploration trend detection online user clustering cluster based retrieval to name a few
compared with general text clustering short text clustering is more challenging
this is because text representations in the original lexical space are usually sparse and this issue is further amplied for short texts
thus the key to the success of short text clustering is to learn an effective short text representation scheme suitable for clustering
on the basis of classical general clustering algorithms such as k means current developments in short text clustering mostly fall into two branches bayesian topic models e

latent let allocation lda and deep learning approaches
the former one realizes probabilistic text clustering by assuming that each document is associated with a distribution over topics and each topic is a distribution over words
in this way a topic is usually regarded as a cluster
to model short text some rate topic models are presented by changing the text generation process
however one major limitation remains within most of these topic models the input representation of short text is commonly based on the bag of words assumption and one hot w
zhang corresponding author and c
dong are with the school of computer science and technology east china normal sity shanghai china e mail zhangwei

com
ecnu
edu
cn
j
yin is with department of computer science and technology shandong university qingdao shandong china e mail
edu
cn
j
wang is with the department of computer science and technology tsinghua university beijing china and also with the jiangsu collaborative innovation center for language ability jiangsu normal university xuzhou china e mail
edu
cn
encoding which might be sparse and lack the expressive ability
aiming to leverage the power of representation learning for short text clustering the studies utilize word dings and deep convolutional neural networks to build a multi stage framework
better text representations are rstly learned to be fed into the conventional k means algorithm for improving the clustering performance
however the optimization process is partitioned into separate stages
thus it is incapable of guiding text representation learning by the clustering objective
some other deep representation learning approaches for general clustering problems have been trained in an end to end fashion whereas they are not tailored for textual data or require an additional step to obtain short text tions
specically hand crafted text representations such as term frequency inverse document frequency tf idf are commonly taken as model input
hence they overlook the usage of word level distributional representations which are important to grasp the text semantic relatedness
in this paper we concentrate on bridging the gap between short text representation learning and short text clustering by fusing them in a unied model
inspired by the roaring success of tion mechanisms in natural language processing nlp we devise the novel attentive representation learning arl model tailored for the short text clustering task
it rst leverages dimensional word embeddings to build dense representations with the simple but effective mean pooling technique for short texts
cluster level attention is then developed to capture the correlations between the short text representations and cluster representations which shares a similar spirit with gsdmm tailored for short text modeling that assumes all the words in a short text should belong to the same cluster topic
the derived attention weights can be used as an evidence to determine cluster assignments
to enable the unsupervised learning of word and cluster tions we propose to reconstruct short text representations through the weighted combination of cluster representations
an objective function combining the pairwise ranking loss and point wise loss is employed to measure the reconstruction gap
journal of ieee transactions on data engineering vol
xx no
xx xxxx to improve the effectiveness of learning short text clustering we further incorporate robust adversarial training into the nal model naming it as arl adv
robust adversarial ing requires to feed both original real examples and intentional adversarial examples into the model during the training cess
it has obtained impressive performance in the supervised and semi supervised classication tasks
concretely arl adv associates continuous cluster representations with adversarial turbations to form an additional adversarial objective function
as a supplement to the original objective function adversarial perturbations play a role of adaptive regularization providing the optimization process with more robustness and being benecial for short texts which sometimes contain noise
the model parameters and perturbations are optimized through a minimax game where the model parameters are for minimizing the aforementioned reconstruction gap and by contrast the perturbations are for maximizing the reconstruction gap
to sum up our contributions are as follows we present a novel attentive representation learning approach arl to couple short text representation learning and tering in a unied model
by proposing to reconstruct text representations through cluster level attention the model can be optimized in an unsupervised manner while cluster assignments are learned as well
to enhance the robustness of learning cluster label ments for short texts we introduce robust adversarial training by customizing adversarial examples arl adv
to our best knowledge this is the rst study to apply robust adversarial training into the eld of the unsupervised clustering setting
we conduct extensive experiments on four real world short text datasets
three of them are publicly available and the remaining one is constructed following a recent work
by comparing arl adv with several strong baselines we demonstrate its signicant improvements and verify that the adversarial training further boosts performance
to ensure the reproducibility of this paper we make the event related dataset released for evaluation see section

related work this section briey reviews recent achievements on text clustering methods especially for short texts and generative adversarial text representation schemes networks for clustering
besides attention mechanisms and adversarial training are discussed

text clustering methods while general text clustering has been extensively studied in the literature short text clustering receives less attention until recent years due to the ourish of user generated content in online social media
currently there are two main categories of approaches in the literature
the rst category is about topic modeling approaches ing the ideas from lda to uncover latent topic distributions by implicitly modeling word co occurrence patterns
unlike lda that assumes to generate a topic for each word in a document gsdmm supposes all the words in a short text share the same topic which is naturally regarded as the cluster of the text
another advanced model is btm
it explicitly models co occurrence patterns through bigrams dened as pairs of unordered words in a given document
the cluster determination in btm depends on the topic proportions of bigrams in a target short text
more recently two topic models are developed for clustering streaming short texts incrementally
in this paper we primarily concentrate on the general clustering setting and leave how to adapt our model to streaming short texts in the future
inspired by the success of deep learning the latter category promotes its development in the clustering eld
the most relevant text clustering
study is stcc designed for short however it is not trained in an end to end fashion leaving room for performance improvement
gaussian lda incorporates word embeddings into the generation process of topic models but it is not very compatible with short texts just as lda
recently some deep learning approaches are proposed for general clustering tasks but not crafted for texts
dec utilizes the idea of self training to measure the gap between the distribution of soft cluster assignments and its corresponding auxiliary distribution through kullback leibler kl divergence
stc adopts training as well meanwhile smoothed inverse frequency sif is introduced to weight word embeddings in each document
yet stc obtains text representations in a separate step and can not optimize word embeddings along with learning text clustering
vade extends variational auto encoder to integrate a gaussian mixture model to generate latent vectors instead of just using a gaussian distribution
however all the above models do not address the importance of representation learning for short texts making their clustering performance not very satised in this situation

generative adversarial networks for clustering beneting from generative adversarial networks gans which aim to generate data that matches a target distribution several approaches utilize gans for clustering tasks
specically premachandran et al
employed gans to extract features which are later fed into the k clustering algorithm
therefore it is not learned in an end to end fashion for clustering
dac leverages a discriminator to push forward the tion of the data representations generated by a trainable encoder to a gaussian mixture distribution
ganmm revisits the expectation maximization based clustering algorithm and replaces the expectation step and the maximization step with classication models and gans
mrabah et al
extended dec by adding a regularization term through gans to constrain the reconstructed data points to be similar to realistic data points
clustergan consists of a generator a discriminator and an encoder to learn mappings between discrete class labels and continuous image pixels
the innovation lies in the design of a discrete latent space z thereby providing sufcient signals for the generator to have samples belonging to a specic class and further beneting clustering
the study shares a similar spirit with clustergan by devising a special latent space for ease of clustering
moreover self paced learning is adopted to distinguish easy and hard examples for achieving a reliable training process
in summary most of the gan based methods are welcomed for clustering images due to their ability to handle continuous values whereas it is nontrivial for applying gan based methods to clustering the discrete textual data due to the difculty of optimization
a surrogate way is to represent text as xed continuous vectors e

tf idf or pre trained word embeddings for gan based models
however it might inevitably limit the power of learning text representations
journal of ieee transactions on data engineering vol
xx no
xx xxxx
text representation schemes traditionally a single word is represented by a one hot vector with a xed dimensional space
the space has the same size as that of a specied vocabulary
accompanied by this vector space models with tf idf are commonly utilized to denote short text representations
to alleviate the issue of sparse exact word overlap in similarity computation low dimensional techniques such as principal component analysis pca and probabilistic topic models e

lda are applied to the text domain
in particular word embedding techniques have gained lots of attention in recent years due to their improved ability to grasp semantic relatedness
consequently much progress has been served in this respect including contextualized word embeddings like bert and
beneting from the advantage of word embeddings the pre training and ne tuning paradigm becomes the state of the art for text classication and some other nlp tasks
nevertheless the embedding based text clustering approaches could not ne tune the ne grained word level representations by the guidance of clustering tives thus leaving room for improving text representations
this motivates the proposed models arl and arl adv to mitigate this issue by jointly learning word embeddings and text clustering see the degraded performance of not ne tuning word embeddings in table

attention mechanisms attention mechanisms are growing in popularity for learning representations for image text and other different modalities
for a given context query it typically culates the prominence of each representation key in sub layers to form an integrated representation value
in the literature of text modeling attention mechanisms allow for dependency modeling regardless of the distance between words compared with recurrent neural networks
the recent study develops a header self attention mechanism for representing input and output sequences
while the approach is entirely composed of attention blocks it outperforms several recurrence and convolution based models in machine translation tasks
based on this self attention mechanism large scale pre trained language models e

bert have been proposed and beneted many nlp tasks
for the cic text classication task ma et al
introduced a mutual tention mechanism to assign importance weights to both semantic related long distance words and local semantic features
in another aspect the works consider incorporating user factors into the attention computation process for words and sentences
the work also adopts word level attention to extract related words and de emphasize irrelevant words
different from these studies we perform cluster level attention to associate the representation learning with the automatic determination of cluster assignments in a unied model to achieve the state of the art short text clustering performance

robust adversarial training since the pioneering work proposes to investigate the benet of supervised learning on intentional adversarial examples for age classication robust adversarial training has made continuous progress in different elds
for example miyato et al
applied virtual adversarial training a variant of adversarial training for semi supervised learning to the text classication task
he et al
learned a pairwise ranking based matrix factorization model for achieving better recommendation performance in an adversarial manner
in a nutshell the ties of robust adversarial training are to proactively inject small intentional perturbations to the original data and train models to behave well on them thereby enhancing the robustness of model training
to the best of our knowledge this is the rst study to leverage robust adversarial training in the literature of unsupervised clustering
it is worth noting that robust adversarial training has a able difference with gans due to the following reasons
first of all unlike gans aiming at generating new data to match a specied target distribution robust adversarial training targets at ensuring a robust model training process
secondly there is no need for robust adversarial training to have a tailored discriminator like gans since the target is not to distinguish real or synthetic data
because of this some studies that couple gans with attention mechanisms do not share the same technological thought let alone they are not proposed for clustering tasks
besides although wei et al
adopted adversarial attacks for clustering their approach is largely different from the proposed arl adv since it relies on a few labeled data in its clustering procedure
the grid based clustering and adversarial attack are separated in different stages
the adversarial part does not involve model parameter learning
the computational model model overview figure depicts the overall architecture of the proposed model arl adv with given short text examples
from a whole perspective the model contains ve computational modules short text representation cluster level attention short text reconstruction objective function and robust adversarial training
taking the given short text i
e
text i as an illustrative example it is rst fed into the short text representation module see section
where the words e

tax in the text are associated with trainable word embeddings to constitute its text representation
afterwards cluster level attention see section
is conducted to compute the probability relevance weights between the text representation and different cluster representations e

that are trainable as well
the weights are consequently adopted to combine cluster representations for reconstructing short text representation see section

to build the pairwise ranking based clustering objective function see section
short text j is sampled from the given corpus as a pseudo negative text
eventually the adversarial training module see section injects perturbations e

c into the cluster representations to constitute another short text reconstruction i for ranking thereby providing the ability of robust training
after arl adv is well trained the cluster label of short text i is determined by sorting the corresponding probability weights and returning the cluster index e

with the maximal weight e

pi k

short text representation text clustering problem we have a corpus d for the short containing multiple short texts i
e
where is the size of d
taking the i short text di d for illustration it is associated with a matrix based representation di where li is the length of the text and xi t is the column vector of one hot encoding at position t with the size journal of ieee transactions on data engineering vol
xx no
xx xxxx fig
architecture of the proposed arl adv
four types of marks with different colors are used to label different information ow
the dotted lines in the middle left of the gure denote how the cluster label of short text i is derived after the model is well trained
and correspond to the normal objective function equation and the adversarial objective function equation respectively
equal to the number of words in a pre specied vocabulary v
it is worth noting that here di is the original text representation while some other strategies such as tf idf can be regarded as the hand crafted feature engineering for texts
to convert each one hot sparse representation xi t t li to its low dimensional dense embedding we rst adopt a look up encoding procedure exi wi where e is a trainable word embedding matrix and k is the embedding size
given these dense word embeddings a simple mean pooling technique is empirically adopted to construct an informative representation of the short text li wi t
the motivation behind the above operation is that short texts commonly have insufcient contextual information and might sometimes contain noisy word information
as such plex contextual text representation methods including pre trained contextual word representations e

bert attention based document level representations recurrent sequential sentations and convolution based representations do not exhibit signicant improvement and are sometime even worse than mean pooling in our local tests
we have also attempted vlawe for text representations whereas it is not very suitable because the word embeddings are hard to be ne tuned when learning short text clustering due to its separate procedure of word embedding clustering
consequently we deem that pooling is suitable for the simultaneous learning of short text representations and clustering

cluster level attention to represent the presumed m clusters for the specic d a trainable cluster representation matrix c cm rkm is established
as di has contained useful global semantic information for the i short text leveraging it to select a suitable cluster can be feasible which is the primary motivation of level attention
in general an attention function takes a query and a set of key value pairs as input
each attention score characterizes the compatibility of its corresponding key with the query and is later used for the weighted combination of all values
we let both key and value correspond to the representations of clusters and regard short text representations as queries
it is intuitive to perform selection based on a probability distribution over each cluster where m m
formally the distribution is dened as follows zm mdi
the above computational manner is the realization of level attention which captures the text cluster interaction
the idea behind it also conforms to the successful experience of probabilistic topic models for short text modeling
specically in lda like models the topic of each word in a long text is separately determined from their sampled topics
by contrast gsdmm assumes that all the words in the same short text belong to the same topic
it exhibits improved performance in short text modeling
arl adv is consistent with gsdmm which has only one latent topic zm for a whole short text as shown in equation
we denote as pi zm for short
by referring to the probability values pi zm for different m it is sufcient to identify information of negative textinformation of positive textinformation of cluster embeddingsperturbation of cluster embeddingscluster level attentioncluster embedding adversarial positive textlookup meanword embeddingnegative textlookup meanword text america can afford the highest corporate tax rate
how can that not make farmers market in bonsall is not very big but some nice surprises here
short text sample journal of ieee transactions on data engineering vol
xx no
xx xxxx the most relevant cluster given di
besides each probability value could be employed to rescale the representations of the corresponding clusters for yielding ci m where ci m pi zm cm

short text reconstruction since short text clustering is indeed an unsupervised learning problem we leverage the cluster representations to reconstruct the short text representation which is later utilized to guide the training of the whole model
by assuming the reconstructed representation of the i th text as we calculate it based on a linear combination of the dependent cluster matrix relevance score with the original representation negative samples at the given margin
than with as a supplement we adopt the relevance maximization of
to keep consistent with the optimization direction of we dene the following loss function e c which can be regarded as a pointwise loss function aiming to predict as similar as possible given the combination of all cluster representations
by fusing and it ensures that not only has larger relevance than those of pseudo negative text pairs but also a large value itself
taking all the short texts in d into consideration we dene the overall objective loss function as follows ci m
m beneting from the different contributions of clusters we can rebuild an expressive representation of the text
for a specic text the derivation of cluster level attention in equation encourages those clusters that are more similar to the text representation to contribute more in the linear combination in equation
when the derived reconstruction is driven to be closer to the text representation cluster level attention may gradually favor one single cluster as the training goes on
the visualization in figure can partly explain the consequence
in the experiments we nd the average probabilities of the inferred maximum cluster are over

these results mean each text is highly concentrated to a single cluster rather than an arbitrary value

objective function given the obtained short text representation and its structed representation from cluster representations we dene their relevance score to be the cosine similarity between the two representations i
we deem that when the relevance score gets larger the result of the reconstruction is better
to achieve this we propose a hybrid objective function consisting of two parts a pairwise ranking part and a relevance maximization part
as for the rst part our model seeks to minimize a margin based pairwise ranking loss
specically for the i short text we sample several other texts from the corpus and regard them as a pseudo negative text set ni
and for text dj ni we obtain the text similarity of the two short texts i
e
dj based on the cosine similarity of their text representations culated by equation where embedding dj is obtained with the same procedure as shown in equation
afterwards we formulate the pairwise ranking loss for the i short text as follows e c dj jni where denotes the number of pseudo negative text ically setting to and is the margin between positive and negative ones empirically setting to
we can see minimizing the above loss would make the reconstructed to have a larger c e e
from a whole perspective arl adv contains word embedding matrix e and cluster representation matrix c as its parameters
each short text is directly encoded into a low dimensional space through its word embeddings so that no extraordinary parameters are required
adversarial training for clustering we propose to apply robust adversarial training to facilitate the representation learning process of arl adv
in essence robust adversarial training provides a new objective function based on adversarial perturbations to complement the original optimization procedure
this objective requires the model to perform well on both original samples and adversarial samples thus beneting the model in its training robustness especially for short texts
since typical adversarial perturbations take continuous values and are added to real valued vectors like images they are not directly applied to discrete tokens like words
following the study we add continuous adversarial perturbations to the cluster representations
it is worth noting that we do not add versarial perturbations to the word embedding matrix
the reasons lie in the two aspects i
e
efciency and effectiveness
first the number of words extends far beyond that of clusters
thus adding perturbations to words instead of clusters entails many more parameters perturbations to be optimized which might reduce the model training efciency
second we empirically validate that applying perturbations to words is less effective than adding perturbations to cluster representations the results of which are shown in table
formally we dene the adversarial cluster level perturbations in arl adv as c rkm
consequently we propose a new objective function formulated as follows c c e c e c where the new cluster representations c c accompanied by the corresponding probability distribution i zm where m m are leveraged to constitute another reconstructed for short text di
the optimization of the above representation i objective function can be viewed from two aspects
for learning c the optimal condition is as follows c arg max c c c
journal of ieee transactions on data engineering vol
xx no
xx xxxx on the contrary the optimization target of e and c is to minimize
typically a norm based constraint should be adopted to restrict the scale of c
by integrating the two objective functions i
e
and the nal optimization target is given as c c c c c e c c arg min e c c c max c where controls the relative strength of
the optimization of equation involves playing a minimax game
at each step the worst case perturbations to cluster representations are rst identied by increasing the value of as much as possible
afterwards e and c are optimized to be robust to such intentional perturbations in while retaining satisfactory performance in
in such a way for text representation its correlations with the normal reconstruction and the adversarial reconstruction are simultaneously considered
this ensures the robustness i of model training which conforms to the short length and noisy situation of short texts
algorithm summarizes the whole optimization process
as usual stochastic gradient descent algorithms such as adam are leveraged to learn the word and clustering embeddings
adv calculates the gradients of e and c over equation through back propagation and updates the parameters accordingly
the learning of adversarial perturbations c follows the mated linearizing methodology leading to fast solutions with analytic forms
with the adding of norm to each column of c i
e
m m we can easily derive the following updating rule for the perturbation of each cluster representation c m where gc m cm
gc m after nishing the training of alr adv the cluster ments are determined based on the probability weights over the learned cluster embedding matrix c while the adversarial perturbations c are ignored
experiments in this section we conduct experiments to answer the following pivotal questions does the proposed arl adv outperform standard and vanced text clustering models on short text datasets are the main components of the proposed model including robust adversarial training benecial for the performance of short text clustering how good are the learned short text embeddings and cluster representations compared to other baselines
datasets we utilize four real world short text datasets
a brief introduction of each dataset and some common text preprocessing procedures are provided in the following
the dataset is from the text retrieval conference on tweet tracks
they are organized by their sponding queries and evaluated into several relevance levels
we

com mstream algorithm robust adversarial training for arl adv
input short text corpus d cluster number m adversarial strength adversarial norm and some other hyper parameters such as embedding size k
output word embeddings e cluster representations c cluster level attention weights m and i
begin initialize word embeddings e and cluster representations c for iter to max iter do sample a mini batch dbatch d for i dbatch do constructing text pairs i j where j dbatch learning adversarial perturbations c through equation updating word embeddings e and cluster representations c by gradient descent of equation retain tweets labeled relevant or highly relevant to their queries to ensure the quality of labels
this dataset is composed of groups of news titles and snippets clawed from google
manual observation has conrmed its favorable grouping quality
event following the study we extract event related tweets from an off the shelf tweet dataset crawled in
prior knowledge about the events including the time window relevant entities and keywords is fetched from wikipedia
it is created based on the questions posted in stack overow
we require each of the selected questions to be associated with only one tag
and the tags are regarded as the ground truth cluster labels for the questions
the created dataset is substantially larger than the above three datasets hoping to provide a more convincing empirical study
for simplicity we sometimes use the name so to denote this dataset
for preprocessing we utilize typical procedures in text cessing which consists of converting words into lowercase moving stop words and irregular words and applying porter ming
in addition words with frequencies below are discarded
the detailed statistics of all datasets can be found in table
cluster and text refer to the actual number of clusters and texts for each dataset respectively
vocabulary is the number of remaining word tokens after preprocessing
avg
len
stands for the average length of texts counted in words

methods for comparisons

baselines the selected representative baselines are categorized into tional text clustering methods i deep learning based general tering models and tailored methods for short text clustering iii
the details of the baselines are given below for clarity


google
com



org download stackexchange journal of ieee transactions on data engineering vol
xx no
xx xxxx table detailed statistics of each dataset
table default settings for arl adv
dataset trec googlenews event stackoverow cluster text vocabulary avg
len




k

k means it is a classical and simple clustering approach relying on hand crafted features for text clustering
two ture types are adopted in our experiments i
e
tf idf and low dimensional representations obtained by pca
we note k means running on these two different features as k idf and k respectively
for we have tried the number of primary components in
it turns out is the more suitable choice so we report results based on this setting
the euclidean distance is utilized as the internal similarity metric
for each run random centroids are initialized
hieclu similar as k means hierarchical clustering hieclu is another simple baseline for clustering
the results of taking textual features from pca are reported due to its better mance than tf idf
lda lda is a classical and standard generative cal model which learns a topic cluster distribution for each document
we assign a text with the topic that has the largest probability value inferred by lda
following we set and
where k is the number of topics
other parameter settings are tried as well but we observe no signicant improvement in performance
btm btm regards bi grams as the representations of short texts and generates them conditioned on different topics
similar to lda we tune and set
based on performance
the number of training iterations is set to for btm to reach convergence
gsdmm gsdmm is tailored for short text clustering and assumes that each text is generated by one topic which is fundamentally different from lda that generates one word from one topic
we set the parameters and
and train the model for iterations
dec it is a deep embedded clustering model that leverages autoencoder with tf idf features as input to map documents into low dimensional embeddings
then the ping function and cluster representations are rened based on the idea of self training
similar to vade dec has specied transforming and parameter settings for text data
we perform pre training for iterations and training for iterations to ensure its convergence
sgd is the optimizer during both pre training and training phases with momentum set to

the batch size is set to
empirical results show that setting the dimension of the hidden layer in dec to can help it get consistent and robust performance
stcc this model mainly consists of three separate steps
it rst trains a convolutional neural network with the help of autoencoders
afterwards the well trained model is ployed to get text embeddings which are fed into k means for nal clustering
we set the kernel width to for convolutional

com xiaohuiyan btm

com gsdmm

com piiswrong dec

com jacoxu layers with k in its k max pooling
the number of feature maps on the rst and second convolutional layer is and respectively
adam is used for batch based optimization with the learning rate of and batch size of
vade vade extends canonical variational auto encoding approaches to support clustering tasks by utilizing the idea of gaussian mixture model
we partially follow the parameter settings for text data in the implementation of vade and keep the procedure of tf idf based feature transformation
setting pre training to iterations and training to iterations results in the convergence of vade on an empirical basis
both phases use adam as the optimizer with the learning rate of
the default conguration of adam is adopted i
e

and

we adjust the size of the hidden representations to and batch size to for more general and stable outcomes
clustergan clustergan is recently proposed to age gans to encode continuous data to discrete cluster labels
considering its requirement of continuous data representations we have tried tf idf and average word embedding to denote each short text and reported the better performance
stc stc adopts self training inspired by dec and uses smoothed inverse frequency sif to compute a weighted average of pre trained word embeddings in a stage independent of optimizing its clustering model
the word embeddings are initialized by the same as arl adv and their size is tuned to be
the dimension of its autoencoder is set to
other parameters are kept the same as their original settings
all the above models are tuned for the experimental datasets
to ensure statistical signicance all results in the experiments are averaged over ten runs
we also tried and introduced in section

however the results show that gaussianlda could not obtain competitive clustering mance than other baselines
and training ganmm incurs a heavy computational burden especially when the cluster number is not small
therefore we omit their details in this paper


variants of the proposed model to verify the effectiveness of the main components in the proposed model we consider some of its variants which are introduced below
arl adv this is the full version of our proposed model which adopts the optimization target as equation and enables adversarial training
arl only is employed to train the model
by ing arl with the full model arl adv we can verify the effectiveness of robust adversarial training for the unsupervised clustering task
arl train w this method optimizes parameters other than the word embedding matrix
comparison against

com vade

com clustergan

com hadifar clustering

com rajarshd gaussian lda

com eyounx ganmm journal of ieee transactions on data engineering vol
xx no
xx xxxx table performance comparison over all methods
type i ii iii dataset metrics lda k idf k hieclu clustergan dec vade stcc btm gsdmm stc arl arl adv nmi












trec ari












acc












googlenews ari












acc












nmi












nmi












event ari












acc












stackoverow ari












nmi












acc












it can show the necessity of learning word level semantic representations
arl train c the cluster representation matrix is not learned in this model hoping to partially verify the benet of combining representation learning and clustering
arl adv the pairwise ranking loss is removed from arl adv
that is the short texts do not interact with their negative samples
arl adv it means arl adv does not consider the supplementary pointwise loss
both arl adv and arl adv still perform adversarial training
arl random this variant takes random noise as bations with the same scale level as intentional adversarial perturbations
arl it shares a very similar spirit with adv except that adversarial perturbations are added to word embeddings instead of cluster representations
for arl adv and its variants word embeddings are trained on each dataset with for iterations
over we initialize centroids of the clusters by performing k means on short text embeddings the same as the study
without loss of generality the default hyper parameters are set to the ones shown in table
k is the dimension of embeddings and is the size of a batch
unless otherwise stated the results will be reported under this setting

evaluation metrics i
e
normalized mutual three commonly used metrics for text clustering we adopt performance evaluation information nmi adjusted rand index ari and clustering curacy acc
suppose i denotes the number of short texts in the i th true topic and represents the number of short texts in the j th inferred cluster
we use ij to denote the number of short texts simultaneously appearing in two clusters
in addition m denotes the set of all possible one to one mappings between the generated clusters and real topics
for an efcient search of the best mapping hungarian algorithm can be adopted
formally nmi ari and acc can be formulated as follows nmi ij ij log i j log i i log i ari ij j i i j j i acc max mm
higher values evaluated by nmi ari and acc indicate better clustering quality
they are all equal to when a perfect match is achieved in cluster assignments on a whole corpus
both nmi and ari penalize unnecessary splits of texts from the same true cluster to several inferred clusters
ari further penalizes undesirable merging of texts from different true clusters into the same inferred cluster making it a more rigorous metric

software conguration we conduct all the experiments on a server with cpus and gpus
for all baselines the software ments are congured according to their specic requirements
extensively used in the literature scikit learn is adopted for the implementations of k means hieclu and lda the visualization tool t sne and the metrics nmi ari and acc
our models arl and arl adv are built based on python and tensorow with cuda and to enable the usage of gpus
we release the event described in section

with the publication of this paper the source code will be prepared for relevant studies

model comparison we report the evaluation results in table for arl arl adv and all the baselines
the cluster numbers here are set to be the same as those in table
from a whole perspective all the methods do not perform the same w

t
the three metrics
for example k idf gains better results than lda w

t
nmi and acc but exhibits worse performance w

t
ari
this phenomenon shows the necessity of adopting all the three metrics
since they focus on different properties of the clustering results using all of them can provide more comprehensive comparisons from distinct perspectives
as expected the models belonging to the conventional gory i do not show competitive performance than other types of baselines in most cases since they are not tailored for short texts and do not fully leverage the power of representation learning


com gensim models
html

com codeupload e arl adv journal of ieee transactions on data engineering vol
xx no
xx xxxx table ablation study of the proposed model
dataset metrics arl train w arl train c arl adv arl adv arl arl random arl arl adv nmi







trec ari







acc







googlenews ari







acc







nmi







nmi







event ari







acc







stackoverow ari







acc







nmi







specically lda works poorly here because assigning different clusters to words in the same short text can aggravate sparsity as the corresponding results demonstrate
although pca converts the sparse representation of tf idf to continuous low dimensional space k shows no improvements over k idf
this result indicates that introducing representation that is independent of the model training process may not easily work
besides although the classical and simple model hieclu its adopts another mechanism i
e
agglomerative clustering performance is still at the same level as k means
both dec and vade are deep learning based approaches ii which can learn low dimensional representations of targets using their original representations such as the pixel values of images and tf idf based representations of texts
therefore though proposed for general clustering tasks they may not be limited to texts
as shown in table they yield better results in most cases than the category of conventional methods
by further investigating the results of clustergan we nd it does not exhibit the satised clustering performance
this might be attributed to the reason that the model is incapable of learning representations for discrete text tokens due to its gan based design
among the baselines originally proposed for short text tering iii stcc performs not very well
this comparison shows that separating the representation learning and clustering into different stages tends to be sub optimal
the reason might be that the representation learning process lacks proper guidance from the feedback of clustering if they are not learned together
compared to stcc gsdmm and btm perform much better in most cases
in particular gsdmm outperforms the general deep learning based clustering models in most cases
we attribute this phenomenon to the proper assumption in gsdmm that short texts are generated from only one topic along with its proper approximation in the posterior distribution
stc exhibits good performance of trec and stackoverow still worse than adv but not so good results on the other two datasets
it makes sense since stc can not optimize word embeddings when training its clustering method
to sum up our full model arl adv is superior since its improvements over the baselines are statistically signicant on all the datasets veried by t test
this might be attributed to the following reasons arl adv utilizes word embeddings to obtain short text embeddings and learns word embeddings with the optimization of the whole model rather than learning it separately
arl adv combines the representation learning and short text clustering in an end to end learning fashion
robust adversarial training adopted in arl adv can effectively improve clustering performance
the last part of table shows that adv improves arl consistently in the four datasets especially for googlenews event and stackoverow verifying the third reason
in the next section we empirically demonstrate the benet of learning word embeddings and cluster representations by the ablation study on the model

analysis of the proposed model table shows the results of the ablation study so we can tigate the role of critical components in our proposed model
by rst comparing arl train w and arl train c with arl adv we can see their performance drops signicantly
this phenomenon shows the learning of word representations and cluster representations accompanied by the automatic selection of clusters is indeed indispensable
besides arl train c behaves better than arl train w indicating the training of word embeddings might be more critical for the model
the second part of table shows the contributions of the pairwise ranking loss and pointwise loss
for trec the two losses play similar roles and removing either of them does not obviously damage the performance
on the contrary the pairwise ranking loss is more crucial for the clustering performance on the other three datasets and adding the pointwise loss could strengthen short text clustering in most cases no improvement observed in stackoverow
actually we could add a parameter in equation and to control the relative inuence of the pointwise loss for a specic dataset
the third part of table further explores the effect of versarial cluster level perturbations by comparing it with level random perturbations arl random and adversarial level perturbations arl
based on the results we observe that arl random obtains slightly worse results than arl
this reveals that simply adding random perturbations without learning may not bring more useful information to the model
arl performs marginally better than arl in most cases but not as well as arl adv especially for googlenews event and stackoverow
for example compared to arl arl adv gains the relative improvements of

and
evaluated by acc for the three datasets respectively

qualitative studies to explore the representations generated by alr adv and other baselines we use t sne to visualize their short text dings in figure where one color denotes a cluster
we nd that arl adv and arl generate highly separable semantic clusters compared with high dimensional yet sparse tf idf based features and low dimensional embeddings from vade and dec
this sualization explains the outstanding scores of arl adv evaluated by the three metrics
dec shows better clustering visualization than the other two baselines
however the central areas in trec journal of ieee transactions on data engineering vol
xx no
xx xxxx trec tf idf trec vade c trec dec trec arl e trec arl adv googlenews tf idf g googlenews vade googlenews dec i googlenews arl j googlenews arl adv event tf idf l event vade m event dec event arl o event arl adv p so tf idf q so vade r so dec so arl t so arl adv fig
visualization of all short text representations in a dimensional space using t sne
the quality of tf idf features short text embeddings from vade dec and the proposed models are investigated
table representative topical words of trec
table representative topical words of event
topic arl adv card consolid credit debt loan unsecur advic settlement counsel creditor bailout gifford gabriell buildup uid congresswoman rehabilit icu recoveri brain hospit doctor drone uav deliveri airspac southwest amazon chair requir commerci propos slice gsdmm debt credit consolid card loan settlement relief bad negoti post unsecur combin lower gifford gabriell rehab doctor recoveri brain news congresswoman uid buildup drone amazon propos airspac deliveri commerci zone sky i uav high spe plan topic arl adv daallo plane somalia land somali beachfront shabaab au hablod mogadishu airplan plow plough bastilleday nice revel speed promenad driven franceattack truck lahoreblast pakistanbomb lahor peshawar prayforlahor peshawarblast guess gsdmm somalia plane land emerg explos forc daallo airlin injur make passeng hole blast truck crowd attack prayfornic innoc heart shatter tie enjoy parad plow leader lahoreblast islamabad pakistan pm armi punjab govt oper blast lahor minist call and googlenews are somehow fuzzy and clusters in event and stackoverow are not separated so well
even for humans the clusters provided by arl adv and arl are easy to identify exhibiting its potential as the basis or initialization for downstream applications that require insights into data manifold
besides there exist non negligible differences between the visualizations of arl adv and arl showing robust adversarial training indeed affects cluster representations
for example embeddings provided by arl adv tend to be more compact than those provided by arl which is more apparent for event and stackoverow
we further evaluate the cluster level quality by comparing the keywords obtained by arl adv and gsdmm presented in table and
one way of regarding a topic as being detected is to judge if it is held by most short texts belonging to a cluster
hence we can align clusters from different methods according to the detected topics
stcc vade and dec may not afford journal of ieee transactions on data engineering vol
xx no
xx xxxx fig
inuence of on the model performance
fig
inuence of on the model performance
such information since they do not directly model the correlations between words and clusters
while gsdmm chooses the words based on the learned word distributions for different topics adv selects representative words that have larger cosine similarity values with the learned cluster representations
the words that without identiable meanings are discarded and the top listed words from the remaining are chosen
both arl adv and gsdmm share lots of keywords in mon showing that they detect similar topics and reach consensus to a certain degree on words that better reect the topics
taking topic of trec as an example words like brain luid and recoveri imply the case of a cured illness related to the entity gifford or gabriell
however gsdmm only identies doctor while arl adv further points out hospit and icu
the two words are even not ranked higher in the word lists of gsdmm yet they are indeed highly correlated to the topic
for event we can observe a similar phenomenon that overlapped keywords still exist between the two methods conrming their capability of providing an overview of a cluster
as revealed in topic of event while both of them succeed to identify dallo somalia and other words arl adv goes further and extracts highly correlated entities like beachf ront and shabaab
these evidences consolidate our viewpoint that arl adv can capture highly correlated keywords which tend to be neglected by conventional topic models like is welcomed by better semantic gsdmm
hence our model quality at the cluster level

hyper parameter analysis

effect of as described in equation adjusts the strength of and thus constrains the contribution of adversarial perturbations to the nal loss function
figure shows how the clustering performance of arl adv alters along with the selection of which helps us to understand the role of loss
note that is set to the default value as shown in table
the gures show that a wide range of from
to can already provide a signicant gain in most cases among the datasets and metrics
moreover selecting around
achieves better results than most values in trec googlenews and stackoverow
thus reaching a balance between the loss function c and its adversarial version c c is a preferable choice in arl adv and we x
throughout the experiments
in summary arl adv relies but not completely on robust adversarial learning showing that it appropriately and effectively takes the advantage of perturbations


effect of figure presents the performance of arl adv when varying
as dened in equation constrains the norm of adversarial perturbations on cluster representations
the larger is the larger the adversarial perturbations will be during the training process
under the setting of discussed above we select some values for in the range from to and show how it affects the




















arl arl arl arl























arl arl arl arl




















arl arl arl arl










arl arl arl arl












arl arl arl arl










arl arl arl arl arl journal of ieee transactions on data engineering vol
xx no
xx xxxx fig
inuence of the cluster number
the ratio of intended cluster number to the original topic number is used
fig
performance over iterations


effect of iterations table training time costs time unit
data trec so clustergan

dec

vade

stc

arl

arl adv

all situations by a large margin
this might be attributed to that they tend to keep their assignment strategies still even when the cluster counts are more than sufcient
as such arl adv is not only effective but also adaptive and robust enough to determine the proper mapping for texts when facing more options
we show the performance of the proposed arl adv over tions in figure
the curves are averaged over runs and we choose a few points at each interval for clarity
due to the effect of adversarial perturbations arl adv experiences a relatively moderate growth in its performance
after or more iterations arl adv starts to attain a stable state on all metrics sharing a good convergence property

time complexity analysis in this section we move to compare the training time efciencies for the adopted representation learning approaches on a single gpu
probabilistic topic models like gsdmm are not included since their gibbs sampling based optimization implementations do not leverage the computational power of gpu well
we also do not report the efciencies of k means algorithms since they demand multiple processors to speed up the solution of assigning each short text to its closest center
to make fair comparisons we set the batch size to uniformity when testing the time costs of the methods to be compared
table shows the time costs needed for each training iteration in average from which we have the following three key tions
first the computational efciencies of the proposed models are comparable with the existing approaches
second adopting robust adversarial training does not additionally incur a very heavy burden
third an approximate linear increase in computational complexity of arl and arl adv is observed by the time cost and data size comparisons between trec and stackoverow so
since both arl and arl adv require the same or less training iterations usually less than as shown in figure for convergence compared to the other approaches through their computational the experiments we could conclude that complexities are at a good level
the results meet expectation clustering performance
we can infer from the gures that positive impact can be achieved locally when falls in the range in most cases
noticeably while increasing based on the chosen range indeed shows a growing tendency for event it does not apply to the other three datasets
thus we do not alter to larger values since it may not reect the universal property of arl adv
besides varying below does not seem to be helpful so we do not list the results here due to the limited space
consequently we can summarize that the restriction imposed on the norm of c should lie in a proper scope to get reasonable results


effect of cluster number figure shows how arl adv adapts to the change of cluster number on the rst three datasets w

t
nmi
the ratio of the predetermined cluster number to the original topic number is used for convenience
the initialization of cluster representations is the same as described in section


we choose to compare arl adv against gsdmm which is one of the most competitive baseline and k which offers insights from the perspective of spectral clustering
for all the considered methods a dramatic increase is found when the ratio goes from
to

this phenomenon is intuitive since the models are forced to merge texts from different topics into several clusters which will almost inevitably lead to worse clustering results w

t
nmi
after that the evaluation scores continue to grow and gradually get saturated
k fails to keep its performance when the ratio gets larger
therefore spectral based clustering algorithms might lack exibility in the context of short text clustering and turn out to heavily rely on human expertise in the datasets
on the other hand while increasing the ratio does not help too much for both arl adv and gsdmm arl adv outperforms gsdmm in




















advgsdmmk




















advgsdmmk




















advgsdmmk











journal of ieee transactions on data engineering vol
xx no
xx xxxx table results of cluster based retrieval
noting that denotes the original language model for retrieval without using clusters
clustering method event stackoverow k gsdmm dec arl adv









since the proposed models have simple architectures and the model parameters only involve word embeddings cluster representations and adversarial perturbations of clusters

application on cluster based retrieval to verify the effect of short text clustering models on the stream applications we choose the cluster based retrieval task as a specic scenario for testing
the task assumes that if a document is relevant to a given query then the document cluster it belongs to should be related to the query as well
specically we select cbdm a variant of language models with cluster smoothing as the retrieval model to validate the contribution of text clusters to retrieval performance
the clusters we choose are produced by arl adv and some representative baselines including k gsdmm and dec
table presents the retrieval results on the event and overow datasets evaluated by from which we observe that using the clusters generated by our model adv indeed boosts the performance of the downstream application i
e
cluster based retrieval
the gains brought by arl adv are the largest among the chosen clustering methods
conclusion in this paper we have developed the novel clustering model adv
it fuses the short text representation learning and clustering in a unied model through the proposed cluster level attention
adversarial perturbations are further added to cluster tions enhancing the robustness and effectiveness of model training through a minimax game
extensive studies on the four real life datasets show that arl adv can achieve superior performance even compared to the state of the art methods for short text tering and the recently developed deep learning based clustering models
further analysis of arl adv is performed to explain the contributions of its components
references t
b
brown b
mann n
ryder m
subbiah j
kaplan p
dhariwal a
neelakantan p
shyam g
sastry a
askell al
language models are few shot learners
arxiv
c
carpineto and g
romano
consensus clustering based on a new ieee probabilistic rand index with application to subtopic retrieval
tpami
h
chen m
sun c
tu y
lin and z
liu
neural sentiment classication with user and product attention
in emnlp pages
r
das m
zaheer and c
dyer
gaussian lda for topic models with word embeddings
in acl pages
j
devlin m
chang k
lee and k
toutanova
bert pre training of deep bidirectional transformers for language understanding
in naacl pages
w
feng c
zhang w
zhang j
han j
wang c
c
aggarwal and j
huang
streamcube hierarchical spatio temporal hashtag clustering for event exploration over the twitter stream
in icde pages
t
fu s
tai and h
chen
attentive and adversarial learning for video summarization
in wacv pages
l
gao x
li j
song and h
t
shen
hierarchical lstms with adaptive attention for visual captioning
ieee tpami
k
ghasedi x
wang c
deng and h
huang
balanced self paced learning for generative adversarial clustering network
in cvpr pages
i
goodfellow j
pouget abadie m
mirza b
xu d
warde farley s
ozair a
courville and y
bengio
generative adversarial nets
in nips pages
i
j
goodfellow j
shlens and c
szegedy
explaining and harnessing adversarial examples

a
hadifar l
sterckx t
demeester and c
develder
a self training approach for short text clustering
in pages
w
harchaoui p

mattei and c
bouveyron
deep adversarial gaussian mixture auto encoder for clustering
in iclr workshop
r
he w
s
lee h
t
ng and d
dahlmeier
an unsupervised neural attention model for aspect extraction
in acl pages
x
he z
he x
du and t
chua
adversarial personalized ranking for recommendation
in sigir pages
s
hochreiter and j
schmidhuber
long short term memory
neural computation
r
huang g
yu z
wang j
zhang and l
shi
dirichlet process ieee mixture model for document clustering with feature partition
tkde
l
hubert and p
arabie
comparing partitions
journal of classication
r
t
ionescu and a
m
butnaru
vector of locally aggregated word embeddings vlawe a novel document level representation
in naacl pages
a
k
jain
data clustering years beyond k means
pattern recognition letters
z
jiang y
zheng h
tan b
tang and h
zhou
variational deep embedding an unsupervised and generative approach to clustering
in ijcai pages
n
kalchbrenner e
grefenstette and p
blunsom
a convolutional neural network for modelling sentences
in acl pages
y
kim
convolutional neural networks for sentence classication
in emnlp pages
d
p
kingma and j
ba
adam a method for stochastic optimization
d
p
kingma and m
welling
auto encoding variational bayes
in iclr in iclr

h
w
kuhn
the hungarian method for the assignment problem
naval research logistics
y
lecun y
bengio and g
hinton
deep learning
nature c
c
aggarwal and c
zhai
a survey of text clustering algorithms
in
mining text data pages
springer
y
li c
luo and s
m
chung
text clustering with feature selection m
ailem f
role and m
nadif
sparse poisson latent block model for by using statistical data
ieee tkde
document clustering
ieee tkde
s
liang e
yilmaz and e
kanoulas
dynamic clustering of streaming d
bahdanau k
cho and y
bengio
neural machine translation by jointly learning to align and translate
arxiv preprint

s
banerjee k
ramanathan and a
gupta
clustering short texts using wikipedia
in sigir pages
short documents
in sigkdd pages
s
liang e
yilmaz and e
kanoulas
collaboratively tracking interests for user clustering in streams of short texts
ieee tkde
x
liu and w
b
croft
cluster based retrieval using language models
y
bengio
learning deep architectures for ai
foundations and trends in sigir pages
in machine learning
d
m
blei a
y
ng and m
i
jordan
latent dirichlet allocation
jmlr
q
ma l
yu s
tian e
chen and w
w
y
ng
global local mutual attention model for text classication
ieee acm taslp
journal of ieee transactions on data engineering vol
xx no
xx xxxx l
v
d
maaten and g
hinton
visualizing data using t sne
jmlr
h
zhang i
j
goodfellow d
n
metaxas and a
odena
self attention generative adversarial networks
in icml pages
c
d
manning h
and p
raghavan
introduction to information retrieval
cambridge university press
m
mathioudakis and n
koudas
twittermonitor trend detection over the twitter stream
in sigmod pages
t
mikolov i
sutskever k
chen g
s
corrado and j
dean
distributed representations of words and phrases and their compositionality
in nips pages
t
miyato a
m
dai and i
goodfellow
adversarial training methods for semi supervised text classication
in iclr
t
miyato s

maeda m
koyama k
nakae and s
ishii
distributional smoothing with virtual adversarial training
in iclr
n
mrabah m
bouguessa and r
ksantini
adversarial deep embedded clustering on a better trade off between feature randomness and feature drift
arxiv
s
mukherjee h
asnani e
lin and s
kannan
clustergan latent in aaai pages space clustering in generative adversarial networks

v
premachandran and a
l
yuille
unsupervised learning using generative adversarial training and clustering
arxiv
a
rangrej s
kulkarni and a
v
tendulkar
comparative study of in www pages clustering techniques for short text documents

a
ritter e
wright w
casey and t
mitchell
weakly supervised in www pages extraction of computer security events from twitter

m
rosen zvi t
l
grifths m
steyvers and p
smyth
the topic model for authors and documents
in uai pages
r
socher a
karpathy q
v
le c
d
manning and a
y
ng
grounded compositional semantics for nding and describing images with sentences
tacl
a
strehl and j
ghosh
cluster ensembles a knowledge reuse work for combining multiple partitions
jmlr
d
tang b
qin and t
liu
document modeling with gated recurrent in emnlp pages neural network for sentiment classication

a
vaswani n
shazeer n
parmar j
uszkoreit l
jones a
n
gomez
kaiser and i
polosukhin
attention is all you need
in nips pages
w
wang w
zhang j
wang j
yan and h
zha
learning sequential correlation for user generated textual content popularity prediction
in ijcai pages
w
wei b
xi and m
kantarcioglu
adversarial clustering a grid based clustering algorithm against active adversaries
arxiv
s
xiao j
yan m
farajtabar l
song x
yang and h
zha
learning time series associated event sequences with recurrent point process networks
ieee tnnls
j
xie r
girshick and a
farhadi
unsupervised deep embedding for clustering analysis
in icml pages
q
xipeng s
tianxiang x
yige s
yunfan d
ning and h
xuanjing
pre trained models for natural language processing a survey
science china technological sciences
j
xu p
wang g
tian b
xu j
zhao f
wang and h
hao
short text clustering via convolutional neural networks
in naacl pages
j
xu b
xu p
wang s
zheng g
tian and j
zhao
self taught convolutional neural networks for short text clustering
neural networks
x
yan j
guo y
lan and x
cheng
a biterm topic model for short texts
in www pages
z
yan y
guo and c
zhang
deep defense training dnns with improved adversarial robustness
in neurips pages
z
yang d
yang c
dyer x
he a
j
smola and e
h
hovy
hierarchical attention networks for document classication
in naacl pages
s
yeung o
russakovsky n
jin m
andriluka g
mori and l
fei
every moment counts dense detailed labeling of actions in complex videos
ijcv
j
yin d
chao z
liu w
zhang x
yu and j
wang
model based clustering of short text streams
in sigkdd pages
j
yin and j
wang
a dirichlet multinomial mixture model based approach for short text clustering
in sigkdd pages
l
yu w
zhang j
wang and y
yu
seqgan sequence generative adversarial nets with policy gradient
in aaai pages
in ijcai pages y
yu and w
zhou
mixture of gans for clustering


