g u a l c
s c v
v i x r a a baseline analysis for podcast abstractive summarization chujie zheng
edu university of delaware usa kunpeng zhang
edu university of maryland usa harry jiannan wang
edu university of delaware usa ling fan
edu
cn tongji university china abstract podcast summary an important factor aecting end users ing decisions has often been considered a critical feature in cast recommendation systems as well as many downstream cations
existing abstractive summarization approaches are mainly built on ne tuned models on professionally edited texts such as cnn and dailymail news
dierent from news podcasts are ten longer more colloquial and conversational and noisier with contents on commercials and sponsorship which makes automatic podcast summarization extremely challenging
this paper presents a baseline analysis of podcast summarization using the spotify podcast dataset provided by trec
it aims to help researchers understand current state of the art pre trained models and hence build a foundation for creating better models
introduction the podcast industry has been dramatically growing and gaining massive market appeal
for example spotify spent approximately million on the acquisition of gimlet media in
however the discovery and understanding of podcast content seem less gressive as compared to other types of media such as music movie and news
this calls for more computationally eective methods for podcast analysis including automatic summarization
with the rapid development in natural language processing especially the success of attention mechanism and transformer chitecture the text summarization task has received ing attention and many models have been proposed to achieve good performance especially in the news summarization eld
they are all trained and tested using well known cnn and dailymail cnn dm dataset where the headlines are served as the ground truth of summaries
in this short paper the dataset we study is the recently released trec spotify podcasts dataset which consists of podcast episodes with audio les transcripts generated using google asr episode summaries and other show information
dierent from news podcasts have unique characteristics such as lengthy multi modal more colloquial and conversational and nosier with contents on commercials and sponsorship which makes podcast in this study we aim to summarization task more challenging
share our preliminary results on data preprocessing and some line analysis which is expected to empirically show the tioned data specialty and build a foundation for subsequent cast analyses
the code and pre trained models will be released after the trec competition
data preprocessing the spotify podcast dataset has podcast episodes from shows produced by creators
the average duration of a gle episode is minutes while the longest can be over hours and the shortest is only seconds
the trec podcast track nizers form the brass set by cutting down the dataset to podcast episodes using the following rules remove episodes with descriptions that are too long acters or too short characters remove duplicate episodes with similar descriptions by ducting similarity analysis remove episodes with descriptions that are similar to the responding show descriptions which means the episode description may not reect the episode content
on top of the brass set we impose several extra constraints to form a cleaner dataset as follows remove episodes with emoji dominated descriptions i
e
scriptions with less than characters after removing jis
remove episodes longer than minutes to control the length of the episode descriptions
this constraint can be easily altered or relaxed if necessary
remove episodes with profanity language in the episode or show descriptions
remove episodes with non english descriptions
remove episodes with sponsorship advertisement dominated descriptions
after preprocessing the dataset has episodes left which serves the dataset for all analyses in this study see table for details
baseline models the abstractive summarization task aims to automatically generate the podcast episode summaries based on the episode transcripts
the ground truth is the summary written by the podcast creators
the performance of summarization models is often measured ing the rouge score particularly the scores of and rouge l
we also report recall r and precision p
we design two simple heuristic baselines for model isons baseline select the rst k tokens from the transcript as the summary

com podcast summarization baseline
org project dataset preprocessing trec spotify podcasts dataset after ltering by the trec organizer brass set after removing episodes with emoji dominated descriptions after removing episodes longer than minutes after removing episodes with profanity language after removing episodes with non english descriptions after removing episodes with sponsorship advertisement dominated descriptions table data preprocessing and the number of episodes of episodes baseline select the last k tokens from the transcript as the summary
the idea behind both baselines is that the beginning or the end of the podcast may contain more important content information
their performance is shown in table with k being varied tween and
we choose the maximum value of k to because bert and other transformer based models as we will discuss in the next section truncate the input to kens
the results exhibit an obvious pattern that longer summary tends to capture more words measured by and phrases measured by and rouge l that are also in the true summary which often leads to higher recall but lower precision
the key takeaways are
choosing yields the best bined score which means tokens words are long enough to capture the major summarization information
this is ble with the distribution of the true summaries where the average summary length is and the maximal length is

line has the highest scores which means the starting part of podcasts contains more useful and related information to podcast summaries than the ending part
this is also consistent with our observation that podcast episodes often give some overview at the beginning to tell the listeners what to expect
sota model experiments in this section we conduct a number of experiments for the cast summarization task using three current state of the art sota summarization models including bart and net
more specically we use the pre trained models tune them using the news datasets cnn and dailymail datasets and the preprocessed podcast dataset from section
the goal is to get an overview idea about the performance of the sota models which builds a foundation for better model innovation
all experiments are conducted under a machine with two tesla gpus
we split our processed podcast dataset into training validation and testing sets by and at random resulting in observations in the training set and observations in both idation and testing sets
based on the baseline analysis in the vious section we choose the beginning part of the episode scripts as the input we use the default settings that use tokens for bart and and tokens for prophetnet and the episode this paper we use distilbart provided by hugging face
it achieves better formance than the original bart model in our experiment
description from creators as the summarization ground truth
ble shows the experiment results from which we have the lowing observations

the performance of the sota models is comparable to the baseline models which indicates that there is plenty of headroom for improvements and calls for more research in this emerging area

the scores for rouge and l of prophetnet on the cnn dm dataset are


but the ing best scores in table for the podcast dataset are only

and

this huge performance gap implies that the cast summarization task could be more challenging than the news headline summarization task due to the podcast s unique teristics aforementioned

fine tuning the pre trained models on the cnn dm dataset for podcast summarization may result in lower performance pared with the vanilla pre trained models e

bart and net
this urges us to think more about the lexicon dierences tween the podcast dataset and other existing datasets used in marization tasks such as cnn dm gigaword bigpatent and
we also provide some sample generated podcast summaries from dierent models in our repository
based on the baseline analysis in this paper we discuss a ber of directions for future research summarization based on long narrative structure as discussed in simple position heuristics are not sucient for long narratives such as podcast transcripts tion
how to dene a narrative structure for better podcast summarization is interesting and worthy of the topic
conversation summarization podcasts are often conversational colloquial and multi people
how to leverage existing search such as to help podcast summarization is still largely missing
multi modal podcast analysis the audio les of podcasts tain much richer information than the text transcripts such as music emotion pitch
we believe the multi modal analysis is critical for podcast understanding and thus should play an important role in podcast summarization and ommendation
long document transformer how to leverage recent research on and to potentially use the full podcast scripts during training
model baseline baseline p









f









r









p









r









f









rouge l p









f









r









table model performance for two baseline models model baseline baseline cnn dm podcast cnn dm podcast prophetnet prophetnet cnn dm prophetnet podcast p










f










r










p










r










f










rouge l p










f










r










distilbart we use hugging face transformers model sshleifer distilbart we use hugging face transformers model small prophetnet we use released prophetnet gb checkpoint fine tuned on cnn dm dataset fine tuned on podcast dataset table performance comparison of dierent models conclusion in this paper we present the performance of podcast tion using two baselines and sota models on the spotify podcast dataset
we discuss several directions for future research in this eld
we hope this pioneering baseline analysis and tion can help researchers make more much needed innovation in this exciting emerging research area
references tadas baltrusaitis chaitanya ahuja and louis philippe morency

modal machine learning a survey and taxonomy
ieee transactions on pattern analysis and machine intelligence
iz beltagy matthew e peters and arman cohan

longformer the document transformer
arxiv preprint

ann clifton aasish pappu sravana reddy yongze yu jussi karlgren ben carterette and rosie jones

the spotify podcasts dataset
arxiv preprint

arman cohan franck dernoncourt doo soon kim trung bui seokhwan kim walter chang and nazli goharian

a discourse aware attention model for abstractive summarization of long documents
arxiv preprint

jacob devlin ming wei chang kenton lee and kristina toutanova

bert pre training of deep bidirectional transformers for language understanding
arxiv preprint

prakhar ganesh and saket dingliwal

abstractive summarization of ken and written conversation
arxiv preprint

nikita kitaev lukasz kaiser and anselm levskaya

reformer the cient transformer
arxiv preprint

mike lewis yinhan liu naman goyal marjan ghazvininejad abdelrahman mohamed omer levy ves stoyanov and luke zettlemoyer

bart ing sequence to sequence pre training for natural language generation lation and comprehension
arxiv preprint

chin yew lin

rouge a package for automatic evaluation of summaries
in text summarization branches out

ramesh nallapati bowen zhou caglar gulcehre bing xiang al

stractive text summarization using sequence to sequence rnns and beyond
arxiv preprint

pinelopi papalampidi frank keller lea frermann and mirella lapata

screenplay summarization using latent narrative structure
arxiv preprint

colin rael noam shazeer adam roberts katherine lee sharan narang michael matena yanqi zhou wei li and peter j liu

exploring the its of transfer learning with a unied text to text transformer
arxiv preprint

alexander m rush sumit chopra and jason weston

a neural attention model for abstractive sentence summarization
arxiv preprint

eva sharma chen li and lu wang

bigpatent a large scale dataset for abstractive and coherent summarization
arxiv preprint

arpit sood thanvir p mohamed and vasudeva varma

topic focused summarization of chat conversations
in european conference on information retrieval
springer
ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan n gomez lukasz kaiser and illia polosukhin

attention is all you need
in advances in neural information processing systems

yu yan weizhen qi yeyun gong dayiheng liu nan duan jiusheng chen ruofei zhang and ming zhou

prophetnet predicting future n gram for sequence to sequence pre training
arxiv preprint

xiaodan zhu and gerald penn

summarization of spontaneous tions
in ninth international conference on spoken language processing

