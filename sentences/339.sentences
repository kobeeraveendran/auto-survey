cdevalsumm an empirical study of cross dataset evaluation for neural summarization systems yiran chen pengfei ming zhong zi yi danqing wang xipeng qiu xuanjing huang shanghai key laboratory of intelligent information processing fudan university school of computer science fudan university songhu road shanghai china mellon university
edu
cn zdou
cmu
edu t c o l c
s c v
v i x r a abstract neural network based models augmented with unsupervised pre trained knowledge have achieved impressive performance on text marization
however most existing evaluation methods are limited to an in domain setting where summarizers are trained and evaluated on the same dataset
we argue that this proach can narrow our understanding of the generalization ability for different tion systems
in this paper we perform an depth analysis of characteristics of different datasets and investigate the performance of ferent summarization models under a dataset setting in which a summarizer trained on one corpus will be evaluated on a range of out of domain corpora
a comprehensive study of representative summarization tems on datasets from different domains veals the effect of model architectures and generation ways i
e
abstractive and tive on model generalization ability
further experimental results shed light on the tions of existing summarizers
brief tion and supplementary code can be found in
com cdevalsumm
introduction neural summarizers have achieved impressive formance when evaluated by rouge lin on in domain setting and the recent success of trained models drives the state of the art results on benchmarks to a new level liu and lapata liu zhong et al
zhang et al
lewis et al
zhong et al

however the superior performance is not a guarantee of a fect system since exsiting models tend to show fects when evaluated from other aspects
for ple zhang et al
observes that many tive systems tend to be near extractive in practice
these two authors contributed equally
corresponding author
figure ranking descending order of current top scoring summarization systems abstractive els are red while extractive ones are blue
each tem is evaluated based on three diverse evaluation ods averaging each system s in scores over ve datasets c evaluating tems using our designed cross dataset measures stable sec

notably bertmatch and bart are two state of the art models for extractive and stractive summarization respectively highlighted by blue and red boxes
cao et al
wang et al
kryscinski et al
maynez et al
durmus et al
reveal that most generated summaries are factually incorrect
these non mainstream tion methods make it easier to identify the model s weaknesses
orthogonal to above two evaluation aspects we aim to diagnose the limitation of existing systems under cross dataset evaluation in which a marization system trained on one corpus would be evaluated on a range of out of dataset corpora
instead of evaluating the quality of summarizers solely based on one dataset or multiple datasets individually cross dataset evaluation enables us to evaluate model performance from a different gle
for example fig
shows the ranking of summarization systems studied in this paper under different evaluation metrics in which the ranking list a in dataset is obtained by traditional ranking criteria while other two are based on our designed cross dataset measures
intuitively we observe that there are different denitions of a good system in various evaluation aspects abstractive and extractive systems exhibit diverse behaviors when evaluated under the cross dataset setting
the above example recaps the general tion of this work encouraging us to rethink the generalization ability of current top scoring marization systems from the perspective of dataset evaluation
specically we ask two tions as follows how do different neural architectures of summarizers inuence the cross dataset tion performances when designing tion systems a plethora of neural components can be adopted zhou et al
chen and bansal gehrmann et al
cheng and lapata nallapati et al

for example will copy gu et al
and coverage see et al
mechanisms improve the cross dataset eralization ability of summarizers is there a risk that bert based summarizers will perform worse when adapted to new areas compared with the ones without bert so far the generalization ability of current summarization systems when transferring to new datasets still remains unclear which poses a signicant challenge to design a reliable system in realistic scenarios
thus in this work we take a closer look at the effect of model architectures on cross dataset generalization setting
do different generation ways extractive and abstractive of summarizers inuence the dataset generalization ability extractive and stractive models as two typical ways to summarize texts usually follow diverse learning frameworks and favor different datasets
it would be absorbing to know their discrepancy from the perspective of cross dataset generalization
e

whether tive summarizers are better at generating tive or faithful summaries on a new test set to answer the questions above we have ducted a comprehensive experimental analysis which involves eleven summarization systems cluding the state of the art models ve benchmark datasets from different domains and two tion aspects
tab
illustrates the overall sis framework
we explore the effect of different architectures and generation ways on model eralization ability in order to answer and
semantic equivalency e

rouge and framework semantic equivalency e

rouge factuality e

factcc architecture e

transformer v
s
lstm generation way e

bert v
s
bart sec


sec

sec


sec

table overall analysis framework
ity are adopted to characterize the different aspects of cross dataset generalization ability
ally we strengthen our analysis by presenting two views of evaluation holistic and ne grained views sec

our contributions can be summarized as cross dataset evaluation is orthogonal to other uation aspects e

semantic equivalence ity which can be used to re evaluate current marization systems accelerating the creation of more robust summarization systems
we have sign two measures stiffness and stableness which could help us to characterize generalization ity in different views encouraging us to diagnose the weaknesses of state of the art systems
we conduct dataset bias aided analysis sec

and suggest that a better understanding of datasets will be helpful for us to interpret systems behaviours
representative systems although it s intractable to cover all neural marization systems we try to include more sentative models to make a comprehensive tion
our selection strategy follows the source codes of systems are publicly available systems with state of the art performance or the top mace on benchmark datasets e

cnndm pati et al
systems equipped with typical neural components e

transformer lstm or mechanism e

copy

extractive summarizers extractive summarizers directly choose and output the salient sentences or phrases in the original document
generally most of the existing tive summarization systems follow a framework consisting of three major modules sentence coder document encoder and decoder
in this per we investigate extractive summarizers with different choices of encoders and decoders
lstmnon kedzie et al
this summarizer adopts convolutional neural network as sentence encoder and lstm to model the cross sentence relation
finally each sentence will be selected in a non autoregressive way
transnon liu and lapata the formerext model in liu and lapata similar to above setting except that the document encoder is replaced with the transformer layer
transauto zhong et al
the decoder is replaced with a pointer network to avoid the tion autoregressive
bertnon liu and lapata the sumext model in liu and lapata this model is an extension of transnon by introducing a bert devlin et al
layer
bertmatch zhong et al
this is the isting state of the art extractive summarization system which introduce a matching layer using siamese bert

abstractive summarizers the abstractive approach involves paraphrasing the inputs using novel words
the current tive summarization systems mainly focus on the encoder decoder paradigm
see et al
the model is a lstm ptr based sequence to sequence summarizer with copy and coverage mechanism
we remove the coverage module and keep other parts unchanged
this model is implemented by removing the pointer network of the above summarizer
t liu and lapata a sequence to quence model with transformer as the encoder and decoder
t liu and lapata a sequence to quence model with bert as encoder and former as decoder
bart lewis et al
a fully pre trained sequence to sequence model
it is the existing of the art abstractive summarization system
datasets we explore ve typical summarization datasets cnndm xsum pubmed bigpatent b and reddit tifu
cnndm nallapati et al
and xsum narayan et al
are news domain summarization datasets which are various in their publications and abstractiveness
pubmed cohan et al
is a scientic paper dataset which can be used to investigate the generalization ity of models on scientic domain
bigpatent b sharma et al
is the b category of bigpatent a dataset consisting of patent uments from google patents public datasets
reddit tifu kim et al
is a dataset with less formal posts collected from the online discussion forum reddit
detailed statistics and troduction of datasets are presented in the appendix section
evaluation for summarization existing summarization systems are usually ated on different datasets individually based on an automatic metric r s m where d s represents a dataset e

cnndm and system e

respectively
m denotes an evaluation metric e

rouge
figure different metrics characterized by a relation chart among generated summaries gsum references ref and input documents doc
to evaluate the quality of generated summaries metrics can be designed from diverse perspectives which can be abstractly characterized in fig

specically semantic equivalence is used to tify the relation between generated summaries gsum and references ref while factuality aims to characterize the relation between generated maries gsum and input documents doc
besides evaluation metrics in this paper we also introduce some measures that quantify the relation between input documents doc and references ref
we claim that a better understanding of dataset biases can help us interpret models crepancies

semantic equivalence rouge lin is a classic metric to evaluate the quality of model generated summaries by ing the number of overlapped n grams between the evaluated summaries and the ideal references

factuality apart from evaluating the semantic equivalence between generated summaries and the references another evaluation aspect of recent interest is ality
in order to analyze the generalization mance of models in different perspectives in this gsumfactualitysem
equa
data biasdocref a cnn
xsum pubmed bigatent b e reddit figure characteristics of test set for each dataset the train set possesses almost the same property thus is not displayed here coverage copy length novelty sentence fusion score repetition
here we choose gram to calculate the novelty and gram for the repetition
work we also take the factuality evaluation into consideration
factcc factcc kryscinski et al
is duced to measure the fact consistency between the generated summaries and source documents
it is a model based metric which is weakly supervised
we use the proportion of summary sentences that factcc predicts as factually consistent as the ality score in this paper

dataset bias we detail several measures that could quantify the characteristics of datasets which are helpful for us to understand the differences among models
coverage grusky et al
illustrates the lap rate between document and summary it is ned as the proportion of the copied segments in summary
copy length measures the average length of ments in summary copied from source document
novelty see et al
is dened as the portion of segments in the summaries that have nt appeared in source documents
the segments can be instantiated as n grams
repetition see et al
measures the rate of repeated segments in summaries
similar to the above measure we choose n gram n ranges from one to four as segment unit
sentence fusion score is calculated using the sult of the algorithm proposed by lebanoff et al
which is to nd whether summary sentence is compressed from one sentence or fused from several sentences
then sentence fusion score is calculated as the proportion of fused sentences tences that are fused from two or three document sentences to all summary sentences
a high value of coverage and copy length gests the dataset is more extractive while novelty represents the rate of novel units in summary and sentence fusion score represents the proportion of sentences that is fused from more than two ment sentences
zhong et al
also explores dataset bias to aid the analysis of model mance but they only focus on metrics for extractive summarizers

dataset bias analysis according to the coverage and copy length sults in fig
cnndm is the most extractive dataset
bigpatent b also exhibits relatively higher copy rate in summary but the copy ments is shorter than cnndm
on the other hand bigaptent b xsum obtain higher sentence sion score which suggests that the proportion of fused sentences in these two datasets are high
xsum and reddit obtain more gram novel units in summary reecting these two datasets are more abstractive
in terms of repetition in fig
only pubmed and bigpatent b contain more gram repeated phrases in summary
rouge models cnn
cnn
xsum pubm
patent b red
lstmnon



transnon



transauto







bertnon bertmatch



ext
abs
ptr t t bart













































table representative summarizers studied in this paper and their corresponding performance score on different datasets cnndm xsum pubmed bigpatent b reddit
we implement all systems on ve datasets by ourselves
all implemented results can outperform or slightly lower than the performances reported in original papers the column of cnn

lengthnoveltysent
lengthnoveltysent
lengthnoveltysent
lengthnoveltysent
lengthnoveltysent
fusionrepetition ua b a ub b a measures ua ub a a stiff
b b stable
table illustration of two views stiffness ru and bleness r to characterize the cross dataset and b generalization based on model a and b
ua and ub represent two cross dataset matrix of two models
means the model b gains a better cross dataset absolute performance while suggests the model a is more robust
cross dataset evaluation despite recent impressive results on diverse marization datasets modern summarization tems mainly focus on extensive in dataset tecture engineering while ignore the tion ability which is indispensable when systems are required to process samples from new datasets or domains
therefore instead of evaluating the quality of summarization system solely based on one dataset we introduce cross dataset evaluation a summarizer e

trained on one dataset e

cnndm will be evaluated on a range of other datasets e

xsum
methodologically we form cross dataset evaluation from two views grained and holistic and we will detail them below

methodology given a summarization system s a set of datasets d dn and evaluation metric m we can design different evaluation function to quantify the system s quality s m
ing on different forms of function eval could be instantiated as either a scalar or a vector or matrix


fine grained measures once r the cross dataset evaluation result is stantiated as a matrix we can characterize the given system in a ne grained way
specically we ne r as u rn where each cell ui j refers to the metric result e

rouge when a summarizer is trained in dataset di and tested in dataset dj n refers to the number of datasets
additionally we can normalize each cell by the diagonal value r uij ujj u uij ujj measures how close the out of dataset performance trained in di and tested in dj of a system is to its in dataset performance trained in dj and tested in dj


holistic measures instead of using a matrix holistically we can tify the cross dataset generalization ability of each summarization system using a scalar
specically we propose two views to characterize the dataset generalization
stiffness this measure reects the absolute formance of a system under cross dataset setting
given a system its stiffness can be calculated as r i j uij n n intuitively a higher value of stiffness suggests the system obtains better performance when ferred to new datasets
stableness it characterizes the relative mance gap between in dataset and cross dataset test
i j uij ujj n n generally a higher value of stableness suggests that the variance between in dataset and dataset results is smaller
tab
gives an example to characterize ization ability in two views
it shows that stiffness and stableness are not always unanimous a model with higher stiffness may obtains lower stableness
stiffness r stableness figure illustration of stiffness and stableness of scores for various models
yellow bars stand for extractive models and grey bars stand for stractive models
experiment in what follows we analyze different tion systems in terms of semantic equivalence and factuality
moreover the results are studied in tic and ne grained views based on the measures dened above
holistic results are showed in fig
analysis aspect model type compare models holistic analysis cnn
xsum pubm
patent b red
avg cnn
xsum pubm
patent b red
avg e g u o r n i g i r o
i l a m r o n architecture ext abs bertmatch vs
bertnon stiff

vs

stable

vs

bertnon vs
transnon stiff

vs

stable

vs

vs
stiff

vs

stable

vs

ptr vs
stiff

vs

stable

vs

generation way lstm lstmnon vs
stiff

vs

stable

vs

bertsum bertnon vs
t stiff

vs

stable

vs

ne grain analysis cnn
xsum pubm
patent b red
avg cnn
xsum pubm
patent b red
avg cnn
xsum pubm
patent b red
avg cnn
xsum pubm
patent b red
avg cnn
xsum pubm
patent b red
avg cnn
xsum pubm
patent b red
avg g h c i d j e k l table the difference of scores between different model pairs
every column of the table represents the compared results of one pair of models
the line of holistic analysis displays the overall stiffness and stableness of compared models
the rest of the table is ne grained results the rst line of which is the origin compared results ua ub for model pairs a and b and the second line is the normalized compared results ua ub for model pairs a and b
for all heatmap grey and red represent positive and negative respectively
here we only display compared results for limited pairs of models all other results are displayed in appendix
and fig

on the other hand tab
and tab
display the ne grained observations
tab
palys the in dataset results of all models on ve benchmark datasets

semantic equivalence analysis we conduct pair wise wilcoxon signed rank nicant test with

the null hypothesis is that the expected performances stiffness and stableness of a pair of summarization models are identical
we report the observations that are tically signicant


architecture match based reranking improves stiffness nicantly bertmatch which using semantic match scores to rerank candidate summaries hances the stiffness of model signicantly in fig
while obtaining comparable stableness with other extractive models in fig

this indicates that bertmatch not only increases the absolute mance but also retaining robustness
bertmatch is not stable when transferred from other datasets to bigpatent b as tab
g shows when compared to bertnon bertmatch obtains larger in dataset and cross dataset mance gap when tested in bigpatent b
this is because bigpatent b possesses higher tence fusion score and higher repetition compared with other datasets as sec

demonstrates
when served as test set such dataset brings great lenge for bertmatch to correctly rank the didate summaries while it provides more ing signals when served as training set
thus the in dataset bigpatent b trained model obtain much higher score compared with cross dataset models which trained from other datasets and cause lower stableness
non autoregressive decoder is more robust than autoregressive for extractive models
garding the decoder of extractive systems as shown in fig
and fig
the non autoregressive tractive decoder transnon is more stable while it possesses lower stiffness than its autoregressive counterpart transauto
pointer network and coverage mechanism are instrumental in improving stiffness and ness of abstractive systems
the pointer work and coverage mechanism do enhance the solute performance of abstractive system as fig
demonstrates ptr
also the stableness results of and in fig
reveals that once removing the pointer mechanism the value of r for decreases which suggests that the system will be more stable if it s augmented the ability to directly extract text spans from the the source document
however pointer network brings trivial provement when tested in xsum and reddit the absolute model performance improvement of pointer network is trival when tested in xsum and reddit as showed in tab
which is in line with expectations because these two datasets are more abstractive as analyzed in sec


on the other hand coverage is not that ful when tested in reddit and xsum and even harmful when trained in xsum
the heatmap of ptr vs
in tab
shows that when tested in reddit and xsum the improvement of coverage mechanism is trivial
these two datasets possess less repetition thus coverage can not vide much help when transferred to these datasets
moreover when trained in xsum ptr gets lower stiffness compared with which is in accordance with the normalized result in tab

this is because the gold summaries of xsum exhibit lower repetition score as analyzed in sec

thus ca nt provide enough learning nals for coverage mechanism
bert sometimes brings unstableness
as shown in fig
there is no doubt that once marizers extractive or abstractive are equipped with pre trained encoder the stiffness will increase signicantly e

t t ing that the overall cross dataset performance has been improved
however we are surprised to nd from fig
that bert sometimes leads to bleness i
e

this result enlightens us to search for other tures or learning schemas to offset the unstableness brought by bert
as the heatmap of bertnon vs
transnon in tab
shows bert brings unstableness cially when tested in reddit and xsum
bert sometimes can even harm the absolute cross dataset performance
bertnon performs worse than transnon in some cells e

trained in xsum and tested in cnndm in tab
bart shows superior performance in terms of stiffness and stableness
as fig
shows bart obtains the highest stiffness among all stractive models and is even comparable with bertmatch
in addition bart is also outstanding in terms of stableness when compared with other abstractive models fig

the performance gap between bart and t proves that for tive models pre training the whole sequence to sequence model works better than using the trained model in either side of encoder or decoder


generation ways extractive models are superior to abstractive models in terms of stiffness and robustness
stiffness r stableness figure illustration of stiffness and stableness of tuality scores for various models
yellow bars stand for extractive systems and grey bars stand for abstractive systems
extractive models show superior advantage of solute performance as shown in fig

moreover comparing the stableness of abstractive and tive models in fig
we surprisingly nd that abstractive approaches except for bart are tremely brittle since their r value is much lower than any extractive approaches with a maximum margin of and the gap can be reduced by troducing pointer network
this observation poses a great challenge to the development of the tive systems encouraging research to pay more tention to improve the generalization ability
also we have provided hints for the solution such as enabling the model to extract granular information from the source document or using the well trained sequence to sequence model e

bart
when tested in xsum and reddit abstractive systems possess comparable or even better formance
the supremacy of extractive models is not retained in all datasets tab
and tab
though extractive models obtain higher stiffness scores when tested in cnndm and pubmed stractive approaches t obtained higher or comparable stiffness scores when tested at xsum and reddit
this is because xsum and reddit are more abstractive as analyzed in sec



factuality analysis all extractive models can achieve higher ity scores while all abstractive models obtain quite lower ones fig

one interesting observation is for extractive models not all factuality scores under the in dataset setting are in tab
diagonal values which reveals the limitation of ext models cnn
xsum pubm
patent b red
avg cnn
xsum pubm
patent b red
avg transnon bertmatch cnn xsum pubm
patent b reddit avg











cnn xsum pubm
patent b reddit avg

















t







































































bart









































abs models cnn
xsum pubm
patent b red
avg cnn
xsum pubm
patent b red
avg table cross dataset factuality scores for extractive and abstractive models
existing factuality checker
bart can signicantly improve the ability to generate factual summaries compared with other abstractive models as showed in fig
even pared with which equipped with pointer network and tend to copy from source document
abstractive models obtain higher stableness of factuality scores in fig
which surpass
this is because when tested in abstractive datasets e

xsum as sec

shows abstractive marizers trained in dataset tend to be more stractive and obtain lower factuality score while it gets higher factuality score when trained on other datasets which are more extractive e

cnndm
the superiority of cross dataset results over dataset results thus leads to higher stableness
related work our work is connected to the following threads of topics of nlp research
cross dataset generalization in nlp cently more researchers shift their focus from vidual dataset to cross dataset evaluation aiming to get a comprehensive understanding of system s generalization ability
fried et al
explores the generalization ability of different constituency parsers
talmor and berant on the other hand shows the generalization ability of reading comprehension models can be improved by training on one or two other reading sion datasets
fu et al
studies the model generalization in the eld of ner
they point out the bottleneck of the existing ner systems through in depth analyses and provide suggestions for ther improvement
different from the above works we attempt to explore generalization ability for summarization systems
diagnosing limitations of existing tion systems beyond rouge some recent works try to explore the weaknesses of existing tems from divese aspects
zhang et al
tries to gure out to what extent the neural abstractive summarization systems are abstractive and ers many of abstractive systems tend to perform near extractive
on the other hand cao et al
and kryscinski et al
study the factuality problem in modern neural summarization systems
the former puts forward one model that combining source document and preliminary extracted fact scription and prove the effectiveness of this model in terms of factuality correctness
while the ter contributes to design a model based automatic factuality evaluation metric
abstractiveness and factuality error the above works studied are onal to this work and can be easily combined with cross dataset evaluation framework in this paper as sec

shows
moreover wang et al
hua and wang attempt to investigate the domain shift problem on text summarization while they cus on a single generation way either abstractive or extractive
we also investigate the tion of summarizers when transferring to different datasets but include more datasets and models
conclusion by performing a comprehensive evaluation on eleven summarization systems and ve mainstream datasets we summarize our observations below abstractive summarizers are extremely tle compared with extractive approaches and the maximum gap between them reaches in terms of the measure stableness rouge dened in this paper
bart sota system is superior over other abstractive models and even comparable with extractive models in terms of stiffness rouge
on the other hand it is robust when transferring between datasets as it possesses high stableness rouge
bertmatch sota system performs excellently in terms of stiffness while still lacks bleness when transferred to bigpatent b from other datasets
the robustness of models can be improved through either equipped the model with ability to copy span from source document i
e
lebanoff et al
or make use of well trained sequence to sequence pre trained model bart
simply adding bert on encoder could improve the stiffness rouge of model but will cause larger cross dataset and in dataset mance gap a better way should be found to merge bert into abstractive model or a better training strategy should be applied to offset the negative inuence it brings
existing factuality checker factcc is limited in predictive power of positive samples sec


out of domain systems can even surpass in domain systems in terms of ality
sec

acknowledgements we would like to thank the anonymous ers for their detailed comments and constructive suggestions
this work was supported by the tional natural science foundation of china no
and science and technology on parallel and distributed processing laboratory pdl
references ziqiang cao furu wei wenjie li and sujian li

faithful to the original fact aware neural abstractive summarization
in thirty second aaai conference on articial intelligence
yen chun chen and mohit bansal

fast tive summarization with reinforce selected sentence rewriting
in proceedings of the annual ing of the association for computational linguistics volume long papers volume pages
jianpeng cheng and mirella lapata

neural in marization by extracting sentences and words
proceedings of the annual meeting of the sociation for computational linguistics volume long papers volume pages
arman cohan franck dernoncourt doo soon kim trung bui seokhwan kim walter chang and nazli goharian

a discourse aware attention model for abstractive summarization of long documents
in proceedings of the conference of the north american chapter of the association for tional linguistics human language technologies volume short papers pages
jacob devlin ming wei chang kenton lee and kristina toutanova

bert pre training of deep bidirectional transformers for language ing
arxiv preprint

esin durmus he he and mona diab

feqa a question answering evaluation framework for fulness assessment in abstractive summarization
arxiv pages
daniel fried nikita kitaev and dan klein

cross domain generalization of neural constituency in proceedings of the annual parsers
ing of the association for computational tics pages florence italy
association for computational linguistics
jinlan fu pengfei liu qi zhang and xuanjing huang

rethinking generalization of neural els a named entity recognition case study
arxiv preprint

sebastian gehrmann yuntian deng and alexander rush

bottom up abstractive summarization
in proceedings of the conference on cal methods in natural language processing pages
max grusky mor naaman and yoav artzi

newsroom a dataset of
million summaries with diverse extractive strategies
in proceedings of the conference of the north american chapter of the association for computational linguistics man language technologies volume long pers volume pages
jiatao gu zhengdong lu hang li and victor ok incorporating copying mechanism in arxiv preprint li

sequence to sequence learning


karl moritz hermann tomas kocisky edward stette lasse espeholt will kay mustafa suleyman and phil blunsom

teaching machines to read in advances in neural and comprehend
tion processing systems pages
xinyu hua and lu wang

a pilot study of main adaptation effect for neural abstractive rization
arxiv preprint

chris kedzie kathleen mckeown and hal daume iii

content selection in deep learning models of in proceedings of the summarization
ference on empirical methods in natural language processing pages
byeongchang kim hyunwoo kim and gunhee kim

abstractive summarization of reddit posts in with multi level memory networks
ings of the conference of the north american chapter of the association for computational guistics human language technologies volume long and short papers pages
wojciech kryscinski bryan mccann caiming xiong and richard socher

evaluating the factual consistency of abstractive text summarization
arxiv preprint

logan lebanoff kaiqiang song franck dernoncourt doo soon kim seokhwan kim walter chang and fei liu

scoring sentence singletons and pairs for abstractive summarization
arxiv preprint

mike lewis yinhan liu naman goyal jan ghazvininejad abdelrahman mohamed omer levy ves stoyanov and luke zettlemoyer

bart denoising sequence to sequence pre training for natural translation and comprehension
arxiv preprint

language generation chin yew lin

rouge a package for matic evaluation of summaries
text summarization branches out
yang liu

fine tune bert for extractive marization
yang liu and mirella lapata

text in proceedings of tion with pretrained encoders
the conference on empirical methods in ural language processing and the international joint conference on natural language processing emnlp ijcnlp pages
joshua maynez shashi narayan bernd bohnet and ryan mcdonald

on faithfulness and ality in abstractive summarization
arxiv preprint

ramesh nallapati feifei zhai and bowen zhou

summarunner a recurrent neural network based quence model for extractive summarization of ments
in thirty first aaai conference on articial intelligence
ramesh nallapati bowen zhou cicero dos santos c a glar and bing xiang

tive text summarization using sequence to sequence rnns and beyond
conll page
shashi narayan shay b cohen and mirella lapata

do nt give me the details just the summary topic aware convolutional neural networks for in proceedings of the treme summarization
conference on empirical methods in natural guage processing pages
myle ott sergey edunov alexei baevski angela fan sam gross nathan ng david grangier and fairseq a fast extensible michael auli

in proceedings of toolkit for sequence modeling
naacl hlt demonstrations
abigail see peter j liu and christopher d ning

get to the point summarization with in proceedings of the pointer generator networks
annual meeting of the association for tational linguistics volume long papers ume pages
eva sharma chen li and lu wang

bigpatent a large scale dataset for abstractive and coherent summarization
arxiv preprint

alon talmor and jonathan berant

multiqa an empirical investigation of generalization and fer in reading comprehension
in proceedings of the annual meeting of the association for putational linguistics pages florence italy
association for computational linguistics
ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan n gomez ukasz kaiser and illia polosukhin

attention is all you need
in advances in neural information cessing systems pages
alex wang kyunghyun cho and mike lewis

asking and answering questions to evaluate the factual consistency of summaries
arxiv preprint

danqing wang pengfei liu ming zhong jie fu xipeng qiu and xuanjing huang

exploring domain shift in extractive text summarization
arxiv preprint

fangfang zhang jin ge yao and rui yan

on the abstractiveness of neural document in proceedings of the conference on tion
empirical methods in natural language processing pages brussels belgium
association for computational linguistics
jingqing zhang yao zhao mohammad saleh and ter j liu

pegasus pre training with extracted gap sentences for abstractive summarization
arxiv preprint

ming zhong pengfei liu yiran chen danqing wang xipeng qiu and xuanjing huang

extractive summarization as text matching
ming zhong pengfei liu danqing wang xipeng qiu and xuan jing huang

searching for tive neural extractive summarization what works and what s next
in proceedings of the annual meeting of the association for computational guistics pages
ming zhong danqing wang pengfei liu xipeng qiu and xuan jing huang

a closer look at data bias in neural extractive summarization models
in proceedings of the workshop on new frontiers in summarization pages
qingyu zhou nan yang furu wei shaohan huang ming zhou and tiejun zhao

neural ment summarization by jointly learning to score and select sentences
in proceedings of the annual meeting of the association for computational guistics volume long papers volume pages
a appendices a
detailed dataset introduction cnn dailymail the cnn dailymail tion answering dataset hermann et al
ied by nallapati et al
is commonly used for summarization
the dataset consists of online news articles with paired human generated maries
for the data preprocessing we use the anonymized data as see et al
which does nt replace named entities
xsum xsum narayan et al
is a dataset consists of the articles and the single sentence swers of the question what is the article about as summary
it is more abstractive compared with cnn dailymail
pubmed pubmed cohan et al
is drawn from scientic papers specically medical journal articles from the pubmed open access subset
we use the introduction as source document and the abstract as summary here
bigpatent bigpatent sharma et al
consists of
million records of u
s
patent uments and the corresponding summaries are ated by human
according to cooperative patent classication cpc the dataset is divided to nine categories
one of the nine categories is chosen as a dataset in difference domain in our experiment category b performing operations ing
reddit tifu reddit tifu kim et al
is a dataset with less formal posts compared with datasets mentioned above which mostly use formal documents as source
it is collected from the online discussion forum reddit
they regard the body text as source the title as short summary and the summary as long summary thus ing two sets of datasets tifu short and tifu long
tifu long is used in this paper
a
dataset statistics the detailed dataset statistics are presented in tab
datasets statistics topics oracle lead k m news news cnndm xsum pubmed bigpatent b m patents posts reddit
m
m scientic




m




table detailed statistics of ve datasets
lead dicates score of the rst k sentences in the document and oracle indicates the globally mal combination of sentences in terms of scores with ground truth the latter represents the upper bound of extractive models
a
experimental setup a

extractive summarizers we use the same training setup in zhong et al

we use cross entropy as loss function to train lstmnon and transauto
the hidden state mension of lstm in lstmnon is set to and the hidden state dimension of transformer in transauto is
we use transformer with heads
bertnon and transnon is constructed according to liu and lapata
all documents and maries are truncated to tokens when training
bertnon and transnon are trained for steps the gradient is accumulated every two steps
we use adam as optimizer and the learning rate is set to
bertmatch is trained as in zhong et al

it uses the base version of bert as base model
we use adam optimizer with warming up
the learning rate schedule follows vaswani et al

a

abstractive summarizers and ptr are trained using the torch reproduced version code of see et al

we use the same size of hidden state dimension and word embedding sion as in the paper
all of three models are trained with maximum training steps we use adagrad to train the models with learning rate of

t and t is constructed according to liu and lapata
we use two separate optimizers for the decoder and encoder regarding t to set the mismatch of encoder and decoder since the former is pre trained while the latter is not
ing rates for the optimizers of encoder and decoder are
and
respectively
on the other hand t and t are trained with gradient lation every ve steps training step for which is
bart uses the large pre trained sequence to quence model in lewis et al

the total ing step when ne tuning is set to with steps warming up
we use adam as optimizer and learning rate is
a
in dataset rouge results for all models tab
displays in dataset rouge l scores
a
the score difference of all model pairs which are meaningful to compare the holistic and ne grained results of pair wise comparison are displayed in tab

cnndm xsum pubmed bigpatent reddit models rl rl rl rl rl ext



lstmnon


transnon


transauto bertnon


bertmatch






























































abs
ptr t t bart

























































































table representative summarizers we have studied in this paper and their correspond performance rouge l on different datasets
a
cross dataset factuality results of all models the cross dataset factcc results for abstractive els are shown in tab
and the factcc results of extractive models are demonstrated in tab

a
code urls a

training code urls the models and their training code urls are listed below lstmnon and transauto are trained from the code in zhong et al
the code url is
com maszhongming effective extractive summarization
we use the code from liu and lapata for bertnon transnon t and t
code url is
com nlpyang presumm
bertmatch uses the code from zhong et al
and the code url is
maszhongming matchsum
and ptr are trained from the code of see et al
code url is
com atulkum pointer summarizer
we use code in fairseq ott et al
to ne tune bart the code url is
pytorch fairseq tree master examples bart
a

evaluation code urls the evaluation metrics code urls are listed below
bheinzerling pyrouge to evaluate the rouge performance of models
pyrouge use we the url for factcc kryscinski et al
is
com salesforce factcc
the url for other metrics for dataset bias is
com cdevalsumm master data bias metrics
ptr t t bart abs models
n n c m u s x
m u p b t n e t a p
e r g v a
n n c m u s x
m u p b t n e t a p
e r g v a
n n c m u s x
m u p b t n e t a p
e r g v a
n n c m u s x
m u p b t n e t a p
e r g v a
n n c m u s x
m u p b t n e t a p
e r g v a
n n c m u s x
m u p b t n e t a p
e r g v a cnn xsum





pubm
patent b red
avg

















































































































































































































table factcc result for abstractive models lstmnon transnon transauto bertnon bertmatch ext models
n n c m u s x
m u p b t n e t a p
e r g v a
n n c m u s x
m u p b t n e t a p
e r g v a
n n c m u s x
m u p b t n e t a p
e r g v a
n n c m u s x
m u p b t n e t a p
e r g v a
n n c m u s x
m u p b t n e t a p
e r g v a cnn





























xsum



























































pubm






























patent b





























red






























avg table factcc result for extractive models vs
stiff

vs

stable

vs

ptr vs
stiff

vs

stable

vs


n n c m u s x
m u p t n e t a p
e r g v a
n n c m u s x
m u p t n e t a p
e r g v a t vs
stiff

vs

stable

vs


n n c m u s x
m u p t n e t a p
e r g v a bart vs
t bart vs
bart vs
t stiff

vs
stable

vs


n n c m u s x
m u p t n e t a p
e r g v a stiff

vs

stable

vs


n n c m u s x
m u p t n e t a p
e r g v a stiff

vs

stable

vs


n n c m u s x
m u p t n e t a p
e r g v a stiff

vs

stable

vs


n n c m u s x
m u p t n e t a p
e r g v a architecture abs t vs
t architecture ext transnon vs
lstmnon stiff

vs

stable

vs

transauto vs
transnon stiff

vs

stable

vs


n n c m u s x
m u p t n e t a p
e r g v a
n n c m u s x
m u p t n e t a p
e r g v a bertmatch vs
bertnon stiff

vs

stable

vs


n n c m u s x
m u p t n e t a p
e r g v a bertnon vs
transnon stiff

vs

stable

vs

lstm lstmnon vs
stiff

vs

stable

vs

generation way bertsum transnon vs
t stiff

vs

stable

vs

transformer bertnon vs
t stiff

vs

stable

vs


n n c m u s x
m u p t n e t a p
e r g v a
n n c m u s x
m u p t n e t a p
e r g v a
n n c m u s x
m u p t n e t a p
e r g v a
n n c m u s x
m u p t n e t a p
e r g v a analysis aspect model type compare models holistic analysis ne grain analysis e g u o r e g u o r n i g i r o
i l a m r o n n i g i r o
i l a m r o n cnn
xsum pubm
patent b red
avg cnn
xsum pubm
patent b red
avg cnn
xsum pubm
patent b red
avg cnn
xsum pubm
patent b red
avg analysis aspect model type compare models holistic analysis ne grain analysis table the difference of scores between different models pairs
every column of the table resents the compared result of one pair of models
the line of holistic analysis displays the overall stiffness and stableness of compared models
the rest of the table is the ne grained results the rst and third lines of which are the origin compared result ua ub for models pairs a and b and the second and fourth lines are the normalized compared result ua ub for models pairs a and b
for all heatmap grey represents positive red represents negative and white represents approximately zero

