n u j l c
s c v
v i x r a improving semantic relevance for sequence to sequence learning of chinese social media text summarization shuming xu jingjing houfeng wenjie qi key laboratory of computational linguistics peking university of electronics engineering and computer science peking university of computing the hong kong polytechnic university of foreign languages peking university shumingma xusun jingjingxu wanghf
edu
cn
polyu
edu
hk abstract current chinese social media text rization models are based on an decoder framework
although its ated summaries are similar to source texts literally they have low semantic relevance
in this work our goal is to improve mantic relevance between source texts and summaries for chinese social media marization
we introduce a semantic evance based neural model to encourage high semantic similarity between texts and summaries
in our model the source text is represented by a gated attention coder while the summary representation is produced by a decoder
besides the similarity score between the tions is maximized during training
our experiments show that the proposed model outperforms baseline systems on a social media corpus
introduction text summarization is to produce a brief mary of the main ideas of the text
for long and normal documents extractive summarization achieves satisfying performance by selecting a few sentences from source texts radev et al
woodsend and lapata cheng and lapata
however it does not apply to chinese social media text summarization where texts are comparatively short and often full of noise
fore abstractive text summarization which is based on encoder decoder framework is a better choice rush et al
hu et al

for extractive summarization the selected tences often have high semantic relevance to the text
however for abstractive text tion current models tend to produce grammatical text last night several people were caught to ke on a ight of china united airlines from chendu to beijing
later the ight ly landed on taiyuan airport
some ers asked for a security check but were denied by the captain which led to a collision en crew and passengers
rnn china united airlines exploded in the airport leaving several people dead
gold several people smoked on a ight which led to a collision between crew and passengers
figure an example of rnn generated it has high similarity to the text literally mary
but low semantic relevance
and coherent summaries regardless of its semantic relevance with source texts
figure shows that the summary generated by a current model rnn encoder decoder is similar to the source text ally but it has low semantic relevance
in this work our goal is to improve the tic relevance between source texts and generated summaries for chinese social media text rization
to achieve this goal we propose a mantic relevance based neural model
in our model a similarity evaluation component is duced to measure the relevance of source texts and generated summaries
during training it mizes the similarity score to encourage high mantic relevance between source texts and maries
the representation of source texts is duced by an encoder while that of summaries is computed by a decoder
we introduce a gated attention encoder to better represent the source text
besides our decoder generates summaries and provide the summary representation
iments show that our proposed model has better performance than baseline systems on the social media corpus
background chinese abstractive text summarization current chinese social media text summarization model is based on encoder decoder framework
encoder decoder model is able to compress source texts


xn into continuous tor representation with an encoder and then erate the summary y


ym with a coder
in the previous work hu et al
the encoder is a bi directional gated recurrent neural network which maps source texts into sentence vector


hn
the decoder is a directional recurrent neural network which duces the distribution of output words yt with vious hidden state and word sof tmaxf where f is recurrent neural network output tion and is the last hidden state of encoder hn
attention mechanism is introduced to source ter texts bahdanau et al

attention vector ct is represented by the weighted sum of encoder hidden states information capture context of ct tihi n x ti hi n hj p where hi is a relevant score between coder hidden state st and encoder hidden state hi
when predicting an output word the decoder takes account of attention vector which contains the alignment information between source texts and summaries
proposed model our assumption is that source texts and maries have high semantic relevance so our posed model encourages high similarity between y z x y attention cos a b c figure our semantic relevance based neural model
it consists of decoder above encoder below and cosine similarity function
their representations
figure shows our posed model
the model consists of three nents encoder decoder and a similarity function
the encoder compresses source texts into tic vectors and the decoder generates summaries and produces semantic vectors of the generated summaries
finally the similarity function uates the relevance between the sematic vectors of source texts and generated summaries
our ing objective is to maximize the similarity score so that the generated summaries have high semantic relevance with source texts

text representation there are several methods to represent a text or a sentence such as mean pooling of rnn output or reserving the last state of rnn
in our model source text is represented by a gated attention coder hahn and keller
every upcoming word is fed into a gated attention network which measures its importance
the gated attention work outputs the important score with a ward network
at each time step it inputs a word vector et and its previous context vector ht then outputs the score t
then the word vector et is multiplied by the score t and fed into rnn coder
we select the last output hn of rnn coder as the semantic vector of the source text vt
a natural idea to get the semantic vector of a summary is to feed it into the encoder as well
however this method wastes much time because we encode the same sentence twice
actually the last output sm contains information of both source text and generated summaries
we simply pute the semantic vector of the summary by tracting hn from sm vs sm hn previous work has proved that it is effective to represent a span of words without encoding them once more wang and chang

semantic relevance our goal is to compute the semantic relevance of source text and generated summary given tic vector vt and vs
here we use cosine larity to measure the semantic relevance which is represented with a dot product and magnitude vt vs vt kvskkvtk source text and summary share the same language so it is reasonable to assume that their semantic vectors are distributed in the same space
cosine similarity is a good way to measure the distance between two vectors in the same space

training given the model parameter and input text the model produces corresponding summary y and mantic vector vs and vt
the objective is to mize the loss function l vt where is the conditional probability of summaries given source texts and is computed by the encoder decoder model
vt is cosine similarity of semantic vectors vs and vt
this term tries to maximize the semantic relevance between source input and target output
experiments in this section we present the evaluation of our model and show its performance on a popular cial media corpus
besides we use a case to plain the semantic relevance between generated summary and source text

dataset of more than
million text summary pairs structed from a famous chinese social media site called sina
it is split into three parts with pairs in part i pairs in part ii and pairs in part iii
all the summary pairs in part ii and part iii are ually annotated with relevant scores ranged from to and we only reserve pairs with scores no less than
following the previous work we use part i as training set part ii as development set and part iii as test set

experiment setting to alleviate the risk of word segmentation takes xu and sun we use chinese ter sequences as both source inputs and target puts
we limit the model vocabulary size to which covers most of the common characters
each character is represented by a random ized word embedding
we tune our parameter on the development set
in our model the ding size is the hidden state size of decoder is and the size of gated attention work is
we use adam optimizer to learn the model parameters and the batch size is set as
the parameter is

both the encoder and decoder are based on lstm unit
ing the previous work hu et al
our uation metric is f score of rouge and rouge l lin and hovy

baseline systems rnn
we denote rnn as the basic sequence sequence model with bi directional gru encoder and uni directional gru decoder
it is a widely used language generated framework so it is an portant baseline
rnn context
rnn context is a sequence sequence framework with neural attention
tion mechanism helps capture the context tion of source texts
this model is a stronger line system

results and discussions we compare our model with above baseline tems including rnn and rnn context
we refer to our proposed semantic relevance based neural model as srb
besides srb with a gated tion encoder is denoted as attention
table our dataset is large scale chinese short text summarization dataset lcsts which is structed by hu et al

the dataset consists
sina
com model rnn w hu et al
rnn c hu et al
rnn context w hu et al
rnn context c hu et al
rnn context srb c attention c rouge l

















table results of our model and baseline systems
our models achieve substantial improvement of all rouge scores over baseline systems
w word level c character level
text bat ppspptv with careful calculation there are many ssful internet companies in shanghai but few of them becomes giant company like bat
is is also the reason why few internet ies are listed in top hundred companies of ing tax
some of them are merged such as ay tudou pps pptv yihaodian and so on
others are satised with segment market for years
gold why shanghai comes out no giant company rnn context shanghai s giant company
srb shanghai has few giant companies
figure an example of rnn generated mary on lcsts corpus
shows the results of our models and baseline tems
we can see srb outperforms both rnn and rnn context in the f score of and rouge l
it concludes that srb generates more key words and phrases
with a gated attention encoder srb achieves a better formance with
f score of
and
rouge l
it shows that the gated attention reduces noisy and unimportant formation so that the remaining information resents a clear idea of source text
the better representation of encoder leads to a better model level rnn context word hu et al
char word gu et al
char char copynet this work




r l









table results of our model and state of the art systems
copynet incorporates copying anism to solve out of vocabulary problem so its has higher rouge scores
our model does not incorporate this mechanism currently
in the ture work we will implement this technic to ther improve the performance
word word level char character level f score of f score of r l f score of rouge l tic relevance evaluation by the similarity function
therefore srb with gated attention encoder is able to generate summaries with high semantic evance to source text
figure is an example to show the semantic evance between the source text and the summary
it shows that the main idea of the source text is about the reason why shanghai has few giant pany
rnn context produces shanghai s giant companies which is literally similar to the source text while srb generates shanghai has few giant companies which is closer to the main idea in semantics
it concludes that srb produces maries with higher semantic similarity to texts
table summarizes the results of our model and state of the art systems
copynet has the est socres because it incorporates copying anism to deals with out of vocabulary word lem
in this paper we do not implement this anism in our model
in the future work we will try to incorporates copying mechanism to our model to solve the out of vocabulary problem
related work abstractive text summarization has achieved cessful performance thanks to the sequence sequence model sutskever et al
and tention mechanism bahdanau et al

rush et al
rst used an attention based coder to compress texts and a neural network guage decoder to generate summaries
ing this work recurrent encoder was introduced to text summarization and gained better mance lopyrev chopra et al

wards chinese texts hu et al
built a large corpus of chinese short text summarization
to deal with unknown word problem nallapati et al
proposed a generator pointer model so that the decoder is able to generate words in source texts
gu et al
also solved this issue by corporating copying mechanism
besides ayana et al
proposes a minimum risk training method which optimizes the parameters with the target of rouge scores
neural attention model our work is also related to neural attention is rst model
posed by bahdanau et al

there are many other methods to improve neural attention model jean et al
luong et al
and accelerate the training process sun
conclusion our work aims at improving semantic relevance of generated summaries and source texts for nese social media text summarization
our model is able to transform the text and the summary into a dense vector and encourage high similarity of their representation
experiments show that our model outperforms baseline systems and the erated summary has higher semantic relevance
acknowledgements this work was supported in part by national high technology research and development program of china program no
and national natural science foundation of china no

xu sun is the corresponding thor of this paper
references ayana shiqi shen zhiyuan liu and maosong sun

neural headline generation with minimum risk training
corr

dzmitry bahdanau kyunghyun cho and yoshua neural machine translation by corr bengio

jointly learning to align and translate


jianpeng cheng and mirella lapata

neural summarization by extracting sentences and words
in proceedings of the annual meeting of the sociation for computational linguistics acl august berlin germany volume long papers
sumit chopra michael auli and alexander m
rush

abstractive sentence summarization with tentive recurrent neural networks
in naacl hlt the conference of the north american chapter of the association for computational guistics human language technologies san diego california usa june
pages
jiatao gu zhengdong lu hang li and victor o
k
incorporating copying mechanism in li

in proceedings of sequence to sequence learning
the annual meeting of the association for computational linguistics acl august berlin germany volume long papers
michael hahn and frank keller

modeling in man reading with neural attention
ings of the conference on empirical ods in natural language processing emnlp austin texas usa november
pages
baotian hu qingcai chen and fangze zhu

sts a large scale chinese short text summarization dataset
in proceedings of the conference on empirical methods in natural language ing emnlp lisbon portugal september
pages
sebastien jean kyunghyun cho roland memisevic and yoshua bengio

on using very large target vocabulary for neural machine translation
in proceedings of the annual meeting of the association for computational linguistics and the international joint conference on natural guage processing of the asian federation of natural language processing acl july beijing china volume long papers
pages
chin yew lin and eduard h
hovy

matic evaluation of summaries using n gram occurrence statistics
in human language ogy conference of the north american chapter of the association for computational linguistics naacl edmonton canada may june
konstantin lopyrev

generating news corr lines with recurrent neural networks


thang luong hieu pham and christopher d
ning

effective approaches to attention based in proceedings of the neural machine translation
conference on empirical methods in natural language processing emnlp lisbon gal september
pages
ramesh nallapati bowen zhou ccero nogueira dos santos c aglar gulcehre and bing xiang

abstractive text summarization using sequence in proceedings of the sequence rnns and beyond
signll conference on computational natural language learning conll berlin germany august
pages
dragomir r
radev timothy allison sasha goldensohn john blitzer arda c elebi stanko dimitrov elliott drabek ali hakim wai lam danyu liu jahna otterbacher hong qi horacio saggion simone teufel michael topper adam winkel and zhu zhang

mead a platform for multidocument multilingual text summarization
in proceedings of the fourth international ence on language resources and evaluation lrec may lisbon portugal
alexander m
rush sumit chopra and jason weston

a neural attention model for abstractive in proceedings of the tence summarization
conference on empirical methods in natural guage processing emnlp lisbon portugal september
pages
xu sun

asynchronous parallel learning for ral networks and structured models with dense tures
in coling international ence on computational linguistics proceedings of the conference technical papers december osaka japan
pages
ilya sutskever oriol vinyals and quoc v
le

sequence to sequence learning with neural works
in advances in neural information ing systems annual conference on neural formation processing systems december montreal quebec canada
pages
wenhui wang and baobao chang

graph based dependency parsing with bidirectional lstm
in proceedings of the annual meeting of the sociation for computational linguistics acl august berlin germany volume long papers
kristian woodsend and mirella lapata

matic generation of story highlights
in acl proceedings of the annual meeting of the sociation for computational linguistics july uppsala sweden
pages
jingjing xu and xu sun

dependency based gated recursive neural network for chinese word mentation
in meeting of the association for putational linguistics
pages

