exploring domain shift in extractive text summarization danqing wang pengfei liu ming zhong jie fu xipeng qiu xuanjing huang school of computer science fudan university mila and polytechnique montreal
edu
cn jie

ca g u a l c
s c v
v i x r a abstract although domain shift has been well explored in many nlp applications it still has received little attention in the domain of extractive text summarization
as a result the model is utilizing the nature of the training data due to ignoring the difference in the distribution of training sets and shows poor generalization on the unseen domain
with the above tation in mind in this paper we rst extend the conventional denition of the domain from categories into data sources for the text marization task
then we re purpose a domain summarization dataset and verify how the gap between different domains inuences the performance of neural summarization els
furthermore we investigate four learning strategies and examine their abilities to deal with the domain shift problem
experimental results on three different settings show their different characteristics in our new testbed
our source code including bert based learning methods for multi domain rization learning and the re purposed dataset multi sum will be available on our project
com
introduction text summarization has been an important research topic due to its widespread applications
ing research works for summarization mainly volve around the exploration of neural architectures cheng and lapata nallapati et al
and design of training constraints paulus et al
wu and hu
apart from these eral works try to integrate document characteristics e

domain to enhance the model performance haghighi and vanderwende cheung and penn cao et al
isonuma et al
these two authors contributed equally
corresponding author
wang et al
narayan et al
or make interpretable analysis towards existing neural marization models zhong et al

despite their success only a few literature ung and penn hua and wang probes into the exact inuence domain can bring while none of them investigates the problem of domain shift which has been well explored in many other nlp tasks
this absence poses some challenges for current neural summarization models how will the domain shift exactly affect the performance of existing neural architectures how to take better advantage of the domain information to improve the performance for current models whenever a new model is built which can perform well on its test set it should also be employed to unseen mains to make sure that it learns something useful for summarization instead of overtting its source domains
the most important reason for the lack of proaches that deal with domain shift might lay in the unawareness of different domain denitions in text summarization
most literature limits the cept of the domain into the document categories or latent topics and uses it as the extra loss cao et al
isonuma et al
or feature embeddings wang et al
narayan et al

this denition presumes that category information will affect how summaries should be formulated
ever such information may not always be obtained easily and accurately
among the most popular ve summarization datasets only two of them have this information and only one can be used for training
besides the semantic categories do not have a clear the ve datasets are duc et al
cnn daily et al
the new york times annotated corpus and et al

only duc and nyt are annotated with document categories and duc is designed only for petition test
denition
both of these prevent previous work from the full use of domains in existing datasets or building a new multi domain dataset that not only can be used for multi domain learning but also is easy to explore domain connection across datasets
in this paper we focus on the extractive rization and demonstrate that news publications can cause data distribution differences which means that they can also be dened as domains
based on this we re purpose a multi domain tion dataset multi sum and further explore the issue of domain shift
methodologically we employ four types of els with their characteristics under different tings
the rst model is inspired by the joint ing strategy and the second one builds the tion between large scale pre trained models and multi domain learning
the third model directly constructs a domain aware model by introducing domain type information explicitly
lastly we tionally explore the effectiveness of meta learning methods to get better generalization
by ing their performance under in domain out domain and cross dataset we provide a liminary guideline in section
for future research in multi domain learning of summarization tasks
our contributions can be summarized as follows we analyze the limitation of the current main denition in summarization tasks and extend it into article publications
we then purpose a dataset multi sum to provide a sufcient multi domain testbed in domain and out of domain
to the best of our knowledge this is the rst work that introduces domain shift to text marization
we also demonstrate how domain shift affects the current system by designing a verication experiment
instead of pursuing a unied model we aim to analyze how different choices of model signs inuence the generalization ability of dealing with the domain shift problem ding light on the practical challenges and vide a set of guidelines for future researchers
domains in text summarization in this section we rst describe similar concepts used as the domain in summarization tasks
then for example dining and wine in nyt refers to food and drink in duc
we extend the denition into article sources and verify its rationality through several indicators that illustrate the data distribution on our re purposed multi domain summarization dataset

common domain denition although a domain is often dened by the content category of a text li and zong blitzer et al
or image saenko et al
the initial motivation for a domain is a metadata attribute which is used in order to divide the data into parts with different distributions joshi et al

for text summarization the differences between data distribution are often attributed to the ment categories such as sports or business or the latent topics within articles which can be caught by classical topic models like latent dirichlet location lda blei et al

although vious works have shown that taking consideration of those distribution differences can improve marization models performance isonuma et al
wang et al
few related them with the concept of the domain and investigated the rization tasks from a perspective of multi domain learning

publications as domain in this paper we extend the concept into the article sources which can be easily obtained and clearly
three measures we assume that the tions of news may also affect data distribution and thus inuence the summarization styles
in order to verify our hypothesis we make use of three indicators coverage density and sion dened by grusky et al
to measure the overlap and compression between the ment summary pair
the coverage and the density are the word and the longest common subsequence lcs overlaps respectively
the compression is the length ratio between the document and the mary
two baselines we also calculate two strong marization baselines for each publication
the hua and wang studied domain adaptation tween news stories and opinion articles from nyt
ever their model was just trained in a single domain and was adapted to another which was different from our domain training and evaluation settings
most existing benchmark datasets are a mixture of ple publications with the idea of collecting a larger amount of data such as cnn dailymail gigward and newsroom
statistics measures ext oralce train valid test cov
den
comp
r l r l fn cnn ma nyt wtp avg nydn wsj usat tg time avg













































lead





























































table the statistics of the multi sum dataset
three measures refer to coverage density and pression respectively
lead and ext oracle are two common baselines for summarization
all measures and baselines are calculated on the test set of the corresponding publication
the top ve publication are used as source domains for training and the bottom ones are viewed as out of domain
lead baseline concatenates the rst few tences as the summary and calculates its rouge score
this baseline shows the lead bias of the dataset which is an essential factor in news ticles
the ext oracle baseline evaluates the performance of the ground truth labels and can be viewed as the upper bound of the extractive marization models nallapati et al
narayan et al

multi sum the recently proposed dataset newsroom grusky et al
is used which was scraped from major news publications
we select top ten publications nytimes post foxnews theguardian nydailynews wsj usatoday cnn time and mashable and process them in the way of see et al

to obtain the ground truth labels for extractive summarization task we follow the greedy approach introduced by nallapati et al

finally we randomly divide ten domains into two groups one for ing and the other for test
we call this re purposed subset of newsroom multi sum to indicate it is specially designed for multi domain learning in summarization tasks
from table we can nd that data from those news publications vary in indicators that are closely relevant to summarization
this means that ment summary pairs from different publications will have unique summarization formation and models might need to learn different semantic tures for different publications
furthermore we follow the simple experiment by torralba et al
to train a classier for the top ve domains
a simple classication model with glove ing words can also achieve
accuracy the chance is which ensures us that there is a built in bias in each publication
therefore it is reasonable to view one publication as a domain and use our multi publication multi sum as a multi domain dataset
analytical experiment for domain shift domain shift refers to the phenomenon that a model trained on one domain performs poorly on a different et al
gopalan et al

to clearly verify the existence of main shift in the text summarization we design a simple experiment on multi sum dataset
concretely we take turns choosing one domain and use its training data to train the basic model
then we use the testing data of the remaining domains to evaluate the model with the automatic metric rouge lin and hovy basic model like a few recent approaches we dene extractive summarization as a sequence beling task
formally given a document s sisting of n sentences sn the summaries are extracted by predicting a sequence of label y yi for the document where yi represents the i th sentence in the document should be included in the summaries
and rouge l show similar trends and their results are attached in appendix
fn cnn ma nyt wtp nydn wsj usat tg time fn cnn ma nyt wtp nydn wsj usat tg time



































































































table results matrix v of the verication experiment based on the multi sum dataset
the scores of the model which is trained and tested on the same domain rii are shown on the diagonal line
it is regarded as benchmark scores
the other cells vij rij rjj i j which represents that for the same test domain j how many improvements we obtained when we switch from training domain i to j
positive values are higher than the benchmark and negative values are less than the benchmark
in this paper we implement a simple but erful model based on the encoder decoder tecture
we choose cnn as the sentence encoder following prior works chen and bansal and employ the popular modular transformer vaswani et al
as the document encoder
the detailed settings are described in section

results from table we nd that the values are negative except the diagonal which indicates els trained and tested on the same domain show the great advantage to those trained on other mains
the signicant performance drops strate that the domain shift problem is quite rious in extractive summarization tasks and thus pose challenges to current well performed els which are trained and evaluated particularly under the strong hypothesis training and test data instances are drawn from the identical data tion
motivated by this vulnerability we investigate the domain shift problem under both multi domain training and evaluation settings
multi domain summarization with the above observations in mind we are ing an approach which can alleviate the domain shift problem effectively in text summarization
specically the model should not only perform well on source domains where it is trained on but also show advantage on the unseen target domains
this involves the tasks of multi domain learning and domain adaptation
here we begin with eral simple approaches for multi domain rization based on multi domain learning

four learning strategies y k i to facilitate the following description we rst set up mathematical notations
assuming that there are k related domains we refer to dk as a dataset with nk samples for domain
dk represent i a sequence of sentences and the corresponding bel sequence from a document of domain k spectively
the goal is to estimate the conditional probability p y by utilizing the ities among different domains
where and y k i nk i modeli base this is a simple but effective model for multi domain learning in which all domains are aggregated together and will be further used for training a set of shared parameters
notably domains in this model are not explicitly informed of their differences
therefore the loss function of each domain can be written as i where basic denotes our cnn transformer coder framework as described in section
means that all domains share the same parameters
analysis the above model benets from the joint training strategy which can allow a lithic model to learn shared features from different domains
however it is not sufcient to alleviate the domain shift problem because two potential limitations remain the joint model is not aware of the differences across domains which would lead to poor performance on in task evaluation since some task specic features shared by other tasks
negative transferring might happened on new domains
next we will study three different approaches to address the above problems
modelii bert more recently unsupervised training has achieved massive success in nlp munity devlin et al
peters et al
which usually provides tremendous external edge
however there are few works on building the connection between large scale pre trained els and multi domain learning
in this model we explore how the external knowledge unsupervised pre trained models bring can contribute to domain learning and new domain adaption
we achieve this by pre training our basic model m odeli base with bert devlin et al
which is one of the most successful learning frameworks
then we investigate if bert can provide domain information and bring the model good domain adaptability
to avoid introducing new structures we use the feature based bert with its parameters xed
analysis this model instructs the processing of multi domain learning by utilizing external trained knowledge
another perspective is to dress this problem algorithmically
t ag the domain type can also be duced directly as a feature vector which can ment learned representations with domain aware ability
specically each domain tag will be bedded into a low dimensional real valued vector and then be concatenated with sentence embedding
the loss function can be formulated as i iii it is worth noting that on unseen domains the formation of real domain tags is not available
thus we design a domain tag x for unknown domains and randomly relabeled examples with it during training
since the real tag of the data tagged with x may be any source domain this embedding will force the model to learn the shared features and makes it more adaptive to unseen domains
in the experiment this improves the performance on both source domains and target domains
analysis this domain aware model makes it possible to learn domain specic features while it still suffers from the negative transfer problem since private and shared features are entangled in shared space bousmalis et al
liu et al

specically each domain has permission to with our work radford et al
also apply pre trained language model to a wide range of nlp tasks in a zero shot setting
we will discuss the differences in the related work section
figure the gradient update mechanism of the meta learning strategy of modeliv m eta
modify shared parameters which makes it easier to update parameters along different directions
m eta modeliv in order to overcome the above itations we try to bridge the communication gap between different domains when updating shared parameters via meta learning finn et al
li et al
liu and huang
here the introduced communicating protocol claims that each domain should tell others what its updating details gradients are
through its different updating behaviors of different domains can be more consistent
formally given a main domain a and an iliary domain b the model will rst compute the gradients of a la with regard to the model rameters
then the model will be updated with the gradients and calculate the gradients of b
our objective is to produce maximal mance on sample y b lab min y b so the loss function for each domain can be nally written as iv lkj where is the weight coefcient and l can be instantiated as li eqn
lii or liii eqn

analysis to address the multi domain ing task and the adaptation to new domains bert modeliii t ag modeliv modelii m eta take different angles
specically modelii bert utilizes a scale pre trained model while modeliii t ag proposes to introduce domain type information explicitly
lastly modeliv m eta is designed to update ters more consistently by adjusting the gradient direction of the main domain a with the auxiliary domain b during training
this mechanism indeed puries the shared feature space via ltering out the domain specic features which only benet a
experiment domains basic modelii bert modeliii t ag modeliv m eta we investigate the effectiveness of the above four strategies under three evaluation settings domain out of domain and cross dataset
these settings make it possible to explicitly uate models both on the quality of domain aware text representation and on their adaptation ability to derive reasonable representations in unfamiliar domains

experiment setup we perform our experiments mainly on our domain multi sum dataset
source domains are dened as the rst ve domains in domain in table and the other domains out domain are totally invisible during training
the evaluation under the in domain setting tests the model ability to learn different domain distribution on a multi domain set and later out of domain investigates how models perform on unseen mains
we further make use of cnn dailymail as a cross dataset evaluation environment to provide a larger distribution gap
we use modeli basic as a baseline model build modelii bert with feature based bert and modeliii t ag with domain embedding on it
we further develop modeliii t ag as the instantiation of modeliv m eta
for the detailed dataset statistics model settings and hyper parameters the reader can refer to appendix

quantitative results we compare our models by scores in table
note that we select two sentences for multi sum domains and three sentences for cnn daily mail due to the different average lengths of reference summaries
basic vs modeli t ag from table we serve that the domain aware model outperforms the monolithic model under both in domain and of domain settings
the signicant improvement of in domain demonstrates domain information is effective for summarization models trained on multiple domains
meanwhile the superior mance on out of domain further illustrates that the awareness of domain difference also benets under the zero shot setting
this might suggest that the domain aware model could capture specic features by domain tags and have learned domain invariant features at the same time which can be transferred to unseen domains
in domain setting fn cnn ma nyt wtp average ntdn wsj usat tg time average











out of domain setting



































r



cross dataset setting cnn dm



table performance of our four learning strategies on the multi sum dataset
domain of
a smaller r indicates corresponding model has a better generalization ability
bold numbers are the best results and red ones indicate the minimum formance gap between source and target domains
the grey rows show the models average performance der three evaluation settings
basic vs modeliv modeli m eta despite a little drop under in domain setting the narrowed mance gap as shown in r of table indicates modeliv m eta has better generalization ability as a compensation
the performance decline mainly lies in the more consistent way to update eters which puries shared feature space at the expense of ltering out some domain specic tures
the excellent results under cross dataset settings further suggest the meta learning strategy successfully improve the model transferability not only among the domains of multi sum but also across different datasets
modelii supported by the smaller r bert pared with modeli base we can draw the sion that bert shows some domain generalization ability within multi sum
however this ity is inferior to modeliii m eta which further leads to the worse performance on dataset
thus we can not attribute its success in multi sum to the ability to address domain learning nor domain adaptation
instead t ag and modeliv give a specic experiment and analyze why bert with bert can achieve domain generalization modelii in appendix
figure relative position of selected sentence in the original document across ve source domains
we overlap the ground truth labels with the model results in order to highlight the differences
the two rows correspond to model i and model iii in section
model r l see et al



narayan et al
zhang et al
chen and bansal dong et al
zhou et al
our basic model basic model tag basic model meta basic model bert basic model bert tag





























table comparison between our strategies with other extractive summarization models on non anonymized cnn daily mail provided by see et al

the red up arrows indicate performance improvement over our base model and the green down arrows denote the degradation
we suppose the vast external knowledge of bert provides its superior ability for feature extraction
that causes modelii bert to overt multi sum and perform excellently across all domains but fails on the more different dataset cnn daily mail
this observation also suggests that although supervised pre trained models are powerful enough radford et al
still it can not take place the role of supervised learning methods i
e
modeliii t ag and modeliv m eta which is designed specically for addressing multi domain learning and new domain adaptation
the best generalization ability at the cost of tively lower in domain performance
therefore using modeliv m eta is not a good choice if in domain performance matters for end users
modelii bert can achieve the best performance under in domain settings at expense of training time and shows worse generalization ability than modeliv m eta
if the training time is not an issue modelii bert could be a good supplement for other methods

results on cnn dailymail inspired by such observations we further ploy our four learning strategies to the mainstream summarization dataset cnn dailymail see et al
which also includes two different data sources cnn and dailymail
we use the tion as the domain and train our models on its training set
as table shows our basic model has comparable performance with other extractive marization models
besides the publication tags can improve rouge scores signicantly by
points in and the meta learning egy does not show many advantages when dealing with in domain examples what we have expected
bert with tags achieves the best performance though the performance increment is not as much as what publication tags bring to the basic model which we suppose that bert itself has contained some degree of domain information
analysis of different model choices to marize modeliii t ag is a simple and efcient method which can achieve good performance under domain setting and shows certain generalization ability on the unseen domain
modeliv m eta shows
qualitative analysis we furthermore design several experiments to probe into some potential factors that might tribute to the superior performance of aware models over the monolithic basic model





truthmodel
























truthmodel



















positionpercentage the import of the auxiliary domain hurts the model ability to learn domain specic features
however results under both out of domain and dataset settings indicate the loss of b which is informed of a s gradient information helps the model to learn more general features thus ing the generalization ability
related work we briey outline connections and differences to the following related lines of research
domains in summarization there have been several works in summarization exploring the cepts of domains
cheung and penn plored domain specic knowledge and associated it as template information
hua and wang investigated domain adaptation in abstractive marization and found the content selection is ferable to a new domain
gehrmann et al
trained a selection mask for abstractive rization and proved it has excellent adaptability
however previous works just investigated els trained on a single domain and did not explore multi domain learning in summarization
multi domain learning mdl domain adaptation da we focus on the testbed that requires both training and evaluating performance on a set of domains
therefore we care about two questions how to learn a model when the ing set contains multiple domains involving mdl
how to adapt the multi domain model to new mains involving da
beyond the investigation of some effective approaches like existing works we have rst veried how domain shift inuences the summarization tasks
semi supervised pre training for zero shot transfer it has a long history of ne tuning downstream tasks with supervised or unsupervised pre trained models le and mikolov vlin et al
peters et al

however there is a rising interest in applying large scale trained models to zero shot transfer learning ford et al

different from the above works we focus on addressing domain shift and ization problem
one of our explored methods is semi supervised pre training which combines pervised and unsupervised approaches to achieve zero shot transfer
in domain out of domain c cross dataset figure loss weight coefcients for model iv
the y axis is the mean score of and rouge l and different bins correspond to different values
label position sentence position is a well known and powerful feature especially for tive summarization kedzie et al

we pare the relative position of sentences selected by our models with the ground truth labels on source domains to investigate how well these models t the distribution and whether they can distinguish between domains
we select the most tive models t ag illustrated in figure
base and modeliii the percentage of the rst sentence on foxnews is signicantly higher than others unaware of different domains modeli base learns a similar tribution for all domains and is seriously affected in its density by this extreme distribution
togram the probability of the rst sentence being selected is much higher than the ground truth on the other four domains
compared with modeli base domain aware models are more robust by learning different relative distributions for different domains
modeliii t ag constrains the extreme trend especially obviously on cnn and mashable
weight for modeliv m eta we investigate eral to further probe into the performance of modeliv m eta
in eqn
is the weight coefcient of main domain a
when the model ignores a and focuses on the auxiliary domain b and when it is trained only on the loss of main main a the same as the instantiation modeliii t ag
as figure shows with the increase of the rouge scores rise on in domain while decline on out of domain and cross dataset
the formances under in domain settings prove that plot the density histogram of the relative locations of ground truth labels for both source and target domains and attach it in appendix
compared with table we can nd that the relative position of ground truth labels is closely related to rouge performance of the basic model
whole picture in the appendix illustrates the four models performance
in







of














conclusion in this paper we explore publication in the text of the domain and investigate the domain shift problem in summarization
when veried its tence we propose to build a multi domain testbed for summarization that requires both training and measuring performance on a set of domains
der these new settings we propose four learning schemes to give a preliminary explore in istics of different learning strategies when dealing with multi domain summarization tasks
acknowledgment we thank jackie chi kit cheung for useful ments and discussions
the research work is ported by national natural science foundation of china no
and hai municipal science and technology sion and hai municipal science and technology major
zjlab
references david m blei andrew y ng and michael i jordan

latent dirichlet allocation
journal of chine learning research
john blitzer mark dredze and fernando pereira

biographies bollywood boom boxes and blenders in domain adaptation for sentiment classication
proceedings of the annual meeting of the ciation of computational linguistics pages
konstantinos bousmalis george trigeorgis nathan silberman dilip krishnan and dumitru erhan

domain separation networks
in advances in neural information processing systems pages
ziqiang cao wenjie li sujian li and furu wei

improving multi document summarization via text classication
proceedings of the conference on articial intelligence aaai
yen chun chen and mohit bansal

fast tive summarization with reinforce selected sentence rewriting
in proceedings of the annual ing of the association for computational linguistics volume long papers volume pages
jianpeng cheng and mirella lapata

neural in marization by extracting sentences and words
proceedings of the annual meeting of the sociation for computational linguistics volume long papers volume pages
jackie chi kit cheung and gerald penn

abilistic domain modelling with contextualized tributional semantic vectors
in proceedings of the annual meeting of the association for tational linguistics volume long papers ume pages
jackie chi kit cheung and gerald penn

wards robust abstractive multi document rization a caseframe analysis of centrality and main
in proceedings of the annual meeting of the association for computational linguistics ume long papers volume pages
jacob devlin ming wei chang kenton lee and kristina toutanova

bert pre training of deep bidirectional transformers for language ing
arxiv preprint

yue dong yikang shen eric crawford herke van hoof and jackie chi kit cheung

banditsum extractive summarization as a contextual bandit
in empirical methods in natural language ing emnlp
chelsea finn pieter abbeel and sergey levine

model agnostic meta learning for fast adaptation of deep networks
in international conference on chine learning pages
sebastian gehrmann yuntian deng and alexander m
rush

bottom up abstractive summarization
in empirical methods in natural language ing emnlp
raghuraman gopalan ruonan li and rama lappa

domain adaptation for object in nition an unsupervised approach
national conference on computer vision pages
ieee
max grusky mor naaman and yoav artzi

newsroom a dataset of
million summaries with diverse extractive strategies
in proceedings of the conference of the north american chapter of the association for computational linguistics man language technologies volume long pers volume pages
aria haghighi and lucy vanderwende

ing content models for multi document tion
in proceedings of human language gies the annual conference of the north american chapter of the association for tional linguistics pages
association for computational linguistics
karl moritz hermann tomas kocisky edward stette lasse espeholt will kay mustafa suleyman and phil blunsom

teaching machines to read in advances in neural and comprehend
tion processing systems pages
xinyu hua and lu wang

a pilot study of main adaptation effect for neural abstractive rization
arxiv preprint

masaru isonuma toru fujino junichiro mori yutaka matsuo and ichiro sakata

extractive marization using multi task learning with document in proceedings of the classication
ence on empirical methods in natural language processing pages
mahesh joshi william w cohen mark dredze and carolyn p rose

multi domain learning in proceedings of the when do domains matter joint conference on empirical methods in ural language processing and computational ral language learning pages
tion for computational linguistics
chris kedzie kathleen mckeown and hal daum

content selection in deep learning models of summarization
in empirical methods in natural language processing emnlp
quoc v
le and tomas mikolov

distributed representations of sentences and documents
in ceedings of icml
da li yongxin yang yi zhe song and timothy m hospedales

learning to generalize learning for domain generalization
arxiv preprint

shoushan li and chengqing zong

domain sentiment classication
in proceedings of the annual meeting of the association for putational linguistics on human language nologies short papers pages
association for computational linguistics
chin yew lin and eduard hovy

matic evaluation of summaries using n gram occurrence statistics
in proceedings of the man language technology conference of the north american chapter of the association for tional linguistics
pengfei liu and xuanjing huang

learning multi task communication
arxiv preprint

pengfei liu xipeng qiu and xuanjing huang

adversarial multi task learning for text tion
in proceedings of the annual meeting of the association for computational linguistics ume long papers volume pages
ramesh nallapati feifei zhai and bowen zhou

summarunner a recurrent neural network based quence model for extractive summarization of ments
in thirty first aaai conference on articial intelligence
courtney napoles matthew gormley and benjamin in van durme

annotated gigaword
ceedings of the joint workshop on automatic edge base construction and web scale knowledge extraction pages
association for tional linguistics
shashi narayan shay b cohen and mirella just the ata

do nt give me the details summary topic aware convolutional neural works for extreme summarization
arxiv preprint

shashi narayan shay b
cohen and mirella lapata

ranking sentences for extractive rization with reinforcement learning
romain paulus caiming xiong and richard socher

a deep reinforced model for abstractive marization
arxiv preprint

matthew peters mark neumann mohit iyyer matt gardner christopher clark kenton lee and luke zettlemoyer

deep contextualized word sentations
in proceedings of the conference of the north american chapter of the association for computational linguistics human language technologies volume long papers volume pages
alec radford jeffrey wu rewon child david luan dario amodei and ilya sutskever

language models are unsupervised multitask learners
kate saenko brian kulis mario fritz and trevor rell

adapting visual category models to new in european conference on computer domains
sion pages
springer
evan sandhaus

the new york times annotated corpus
linguistic data consortium philadelphia
abigail see peter j liu and christopher d manning

get to the point summarization with generator networks
in proceedings of the nual meeting of the association for computational linguistics volume long papers volume pages
antonio torralba alexei a efros al

ased look at dataset bias
in cvpr volume page
citeseer
ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan n gomez ukasz kaiser and illia polosukhin

attention is all you need
in advances in neural information cessing systems pages
li wang junlin yao yunzhe tao li zhong wei liu and qiang du

a reinforced aware convolutional sequence to sequence model for abstractive text summarization
arxiv preprint

yuxiang wu and baotian hu

learning to extract coherent summary via deep reinforcement learning
in thirty second aaai conference on articial telligence
xingxing zhang mirella lapata furu wei and ming zhou

neural latent extractive document summarization
ming zhong pengfei liu danqing wang xipeng qiu and xuan jing huang

searching for tive neural extractive summarization what works and what s next
in proceedings of the ence of the association for computational tics pages
qingyu zhou nan yang furu wei shaohan huang ming zhou and tiejun zhao

neural ment summarization by jointly learning to score and select sentences
in proceedings of the annual meeting of the association for computational guistics volume long papers volume pages

