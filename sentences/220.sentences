unsupervised text summarization via mixed model back translation yacine jernite facebook ai research new york ny
com g u a l c
s c v
v i x r a abstract back translation based approaches have cently lead to signicant progress in pervised sequence to sequence tasks such as machine translation or style transfer
in this work we extend the paradigm to the problem of learning a sentence summarization system from unaligned data
we present several initial models which rely on the asymmetrical nature of the task to perform the rst back translation step and demonstrate the value of combining the data created by these diverse initialization methods
our system outperforms the rent state of the art for unsupervised sentence summarization from fully unaligned data by over rouge and matches the performance of recent semi supervised approaches
introduction machine summarization systems have made nicant progress in recent years especially in the domain of news text
this has been made possible among other things by the popularization of the neural sequence to sequence paradigm kalchbrenner and blunsom sutskever et al
cho et al
the development of methods which combine the strengths of extractive and abstractive approaches to summarization see et al
gehrmann et al
and the ability of large training datasets for the task such as gigaword or the cnn daily mail which comprise of over
m shorter and k longer articles and aligned summaries respectively
fortunately the lack of datasets of similar scale for other text genres remains a limiting factor when tempting to take full advantage of these modeling advances using supervised training algorithms
in this work we investigate the application of back translation to training a summarization tem in an unsupervised fashion from unaligned full text and summaries corpora
back translation has been successfully applied to unsupervised training for other sequence to sequence tasks such as machine translation lample et al
or style transfer subramanian et al

we outline the main differences between these tings and text summarization devise initialization strategies which take advantage of the rical nature of the task and demonstrate the vantage of combining varied initializers
our proach outperforms the previous state of the art on unsupervised text summarization while using less training data and even matches the rouge scores of recent semi supervised methods
related work rush et al
s work on applying neural systems to the task of text tion has been followed by a number of works proving upon the initial model architecture
these have included changing the base encoder ture chopra et al
adding a pointer nism to directly re use input words in the summary nallapati et al
see et al
or itly pre selecting parts of the full text to focus on gehrmann et al

while there have been comparatively few attempts to train these els with less supervision auto encoding based proaches have met some success miao and som wang and lee
miao and blunsom s work endeavors to use summaries as a discrete latent variable for a text auto encoder
they train a system on a combination of the classical log likelihood loss of the supervised setting and a reconstruction jective which requires the full text to be mostly recoverable from the produced summary
while their method is able to take advantage of belled data it relies on a good initialization of the encoder part of the system which still needs to be learned on a signicant number of aligned pairs
wang and lee expand upon this approach by replacing the need for supervised data with adversarial objectives which encourage the summaries to be structured like natural guage allowing them to train a system in a fully unsupervised setting from unaligned corpora of full text and summary sequences
finally song et al
uses a general purpose pre trained text encoder to learn a summarization system from fewer examples
their proposed mass scheme is shown to be more efcient than bert devlin et al
or denoising auto encoders dae vincent et al
fu et al

this work proposes a different approach to supervised training based on back translation
the idea of using an initial weak system to create and iteratively rene articial training data for a vised algorithm has been successfully applied to semi supervised sennrich et al
and pervised machine translation lample et al
as well as style transfer subramanian et al

we investigate how the same general paradigm may be applied to the task of summarizing text
mixed model back translation let us consider the task of transforming a quence in domain a into a corresponding quence in domain b e

sentences in two guages for machine translation
let da and db be corpora of sequences in a and b without any mapping between their respective elements
the back translation approach starts with initial models ab and f ba which can be hand crafted or learned without aligned pairs and uses them to create articial aligned training data ab ba a dbo dao let s denote a supervised learning algorithm which takes a set of aligned sequence pairs and turns a mapping function
this articial data can then be used to train the next iteration of models which in turn are used to create new cial training sets a and b can be switched here ab ab ba a dao the model is trained at each iteration on articial inputs and real outputs then used to create new training inputs
thus if the initial system is nt too far off we can hope that training pairs get closer to the true data distribution with each step allowing in turn to train better models
in the case of summarization we consider the domains of full text sequences df and of maries ds and attempt to learn summarization ff s and expansion fsf functions
ever contrary to the translation case df and ds are not interchangeable
considering that a mary typically has less information than the sponding full text we choose to only dene initial f s models
we can still follow the proposed procedure by alternating directions at each step

initialization models for summarization to initiate their process for the case of machine translation lample et al
use two different initialization models for their neural nmt and phrase based pbsmt systems
the former lies on denoising auto encoders in both languages with a shared latent space while the latter uses the pbsmt system of koehn et al
with a phrase table obtained through unsupervised cabulary alignment as in grave et al

while both of these methods work well for chine translation they rely on the input and output having similar lengths and information content
in particular the statistical machine translation rithm tries to align most input tokens to an put word
in the case of text summarization ever there is an inherent asymmetry between the full text and the summaries since the latter press only a subset of the former
next we pose three initialization systems which implicitly model this information loss
full implementation details are provided in the appendix
procrustes thresholded alignment pr thr the rst initialization is similar to the one for smt in that it relies on unsupervised vocabulary alignment
specically we train two skipgram word embedding models using fasttext janowski et al
on df and ds then align them in a common space using the wasserstein procrustes method of grave et al

then we map each word of a full text sequence to its nearest neighbor in the aligned space if their tance is smaller than some threshold or skip it erwise
we also limit the output length keeping only the rst n tokens
we refer to this function as f pr f
original france took an important step toward power market liberalization monday braving union anger to announce the partial privatization of state owned behemoth electricite france
pr thr france launched a partial unk of state controlled utility the privatization agency said
dbae france s state owned gaz de france sa said tuesday it was considering partial partial privatization of france s state owned nuclear power plants
france launches an initial public announcement wednesday as the european union announced it would soon undertake a partial privatization
title france launches partial edf privatization table full text sequences generated by f pr and during the rst back translation loop
denoising bag of word auto encoder dbae similarly to both lample et al
and wang and lee we also devise a starting model based on a dae
one major difference is that we use a simple bag of words bow encoder with xed pre trained word embeddings and a layer gru decoder
indeed we nd that a bow encoder trained on the summaries reaches a construction rouge l f score of nearly on the test set indicating that word presence tion is mostly sufcient to model the summaries
as for the noise model for each token in the put we remove it with probability and add a word drawn uniformly from the summary lary with probability
the bow encoder has two advantages
first it lacks the other models bias to keep the word order of the full text in the summary
secondly when using the dbae to predict summaries from the full text we can weight the input word dings by their corpus level probability of ing in a summary forcing the model to pay less attention to words that only appear in df
the denoising bag of words auto encoder with put re weighting is referred to as f
f first order word moments matching we also propose an extractive initialization model
given the same bow representation as for the dbae function s v predicts the probability that each word v in a full text sequence s is present in the summary
we learn the parameters of f by marginalizing the output probability of each word over all full text sequences and matching these rst order moments to the marginal probability of each word s presence in a summary
that is let v s denote the vocabulary of ds then v v s v psdf f and s v psds we minimize the binary cross entropy bce tween the output and summary moments arg min x vv s psdf s v s v we then dene an initial extractive summarization model by applying f to all words of an put sentence and keeping the ones whose output probability is greater than some threshold
we fer to this model as f f

articial training data we apply the back translation procedure outlined above in parallel for all three initialization els
for example f yields the following quence of models and articial aligned datasets f s f sf f s sf f s


finally in order to take advantage of the various strengths of each of the initialization models we also concatenate the articial training dataset at each odd iteration to train a summarizer e

f f s f s f s experiments data and model choices we validate our proach on the gigaword corpus which comprises of a training set of
m article headlines sidered to be the full text and titles summaries along with k validation pairs and we report test performance on the same k set used in rush et al

since we want to learn systems from fully unaligned data without giving the model an opportunity to learn an implicit mapping we also pbsmt pre pr











r l





table test rouge for trivial baseline and tion systems
and lee
further split the training set into m examples for which we only use titles and
m for headlines
all models after the initialization step are mented as convolutional architectures ing fairseq ott et al

articial data eration uses sampling with a minimum length of for full text and a maximum length of for summaries
rouge scores are obtained with an output vocabulary of size k and a beam search of size to match wang and lee
initializers table compares test rouge for different initialization models as well as the ial baseline which simply copies the rst words of the article
we nd that simply olding on distance during the word alignment step of pr thr does slightly better then the full pbsmt system used by lample et al

our bow denoising auto encoder with word weighting also performs signicantly better than the full dae initialization used by wang and lee pre dae
the moments based initial model scores higher than either of these with scores already close to the full pervised system of wang and lee
in order to investigate the effect of these three different strategies beyond their rouge statistics we show generations of the three corresponding rst iteration expanders for a given summary in table
the unsupervised vocabulary alignment in pr thr handles vocabulary shift especially changes in verb tenses summaries tend to be in the present tense but maintains the word der and adds very little information
conversely the expansion function which is learned from purely extractive summaries re uses most words in the summary without any change and adds some new information
finally the encoder based dbae signicantly increases the sequence length and variety but also strays from pr advers
sup
r l






k
k


















k


k



m


table comparison of full systems
the best scores for unsupervised training are bolded
results from and lee et al
and blunsom and et al
the original meaning more examples in the pendix
the decoders also seem to learn facts about the world during their training on article text edf gdf is france s public power company
full models finally table compares the marizers learned at various back translation ations to other unsupervised and semi supervised approaches
overall our system outperforms the unsupervised adversarial reinforce of wang and lee after one back translation loop and most semi supervised systems after the ond one including song et al
s mass pre trained sentence encoder and miao and som forced attention sentence sion fsc which use k and k aligned pairs respectively
as far as back translation proaches are concerned we note that the model performances are correlated with the initializers scores reported in table iterations and low the same pattern
in addition we nd that combining data from all three initializers before training a summarizer system at each iteration as described in section
performs best suggesting that the greater variety of articial full text does help the model learn
conclusion in this work we use the translation paradigm for unsupervised training of a summarization system
we nd that the model benets from combining initializers matching the performance of semi supervised approaches
references piotr bojanowski edouard grave armand joulin and tomas mikolov

enriching word vectors with subword information
tacl
kyunghyun cho bart van merrienboer dzmitry danau and yoshua bengio

on the properties of neural machine translation encoder decoder proaches
in proceedings of eighth workshop on syntax semantics and ture in statistical translation doha qatar tober pages
sumit chopra michael auli and alexander m
rush

abstractive sentence summarization with tentive recurrent neural networks
in naacl hlt the conference of the north american chapter of the association for computational guistics human language technologies san diego california usa june pages
jacob devlin ming wei chang kenton lee and kristina toutanova

bert pre training of deep bidirectional transformers for language standing
corr

zhenxin fu xiaoye tan nanyun peng dongyan zhao and rui yan

style transfer in text in proceedings of the exploration and evaluation
thirty second aaai conference on articial gence the innovative applications of articial intelligence and the aaai symposium on educational advances in articial intelligence new orleans louisiana usa february pages
sebastian gehrmann yuntian deng and alexander m
rush

bottom up abstractive summarization
in proceedings of the conference on empirical methods in natural language processing brussels belgium october november pages
edouard grave armand joulin and quentin berthet

unsupervised alignment of embeddings with wasserstein procrustes
corr

american chapter of the association for tational linguistics hlt naacl edmonton canada may june
guillaume lample myle ott alexis conneau dovic denoyer and marcaurelio ranzato

phrase based neural unsupervised machine in proceedings of the conference on lation
empirical methods in natural language ing brussels belgium october november pages
yishu miao and phil blunsom

language as a latent variable discrete generative models for sentence compression
in proceedings of the conference on empirical methods in natural guage processing emnlp austin texas usa november pages
ramesh nallapati bowen zhou ccero nogueira dos santos c aglar gulcehre and bing xiang

abstractive text summarization using sequence in proceedings of the sequence rnns and beyond
signll conference on computational natural language learning conll berlin germany august pages
myle ott sergey edunov alexei baevski angela fan sam gross nathan ng david grangier fairseq a fast and michael auli

tensible toolkit for sequence modeling
corr

alexander m
rush sumit chopra and jason weston

a neural attention model for abstractive in proceedings of the tence summarization
conference on empirical methods in natural guage processing emnlp lisbon portugal september pages
abigail see peter j
liu and christopher d
manning

get to the point summarization with generator networks
in proceedings of the nual meeting of the association for computational linguistics acl vancouver canada july august volume long papers pages
sergey ioffe and christian szegedy

batch malization accelerating deep network training by in proceedings reducing internal covariate shift
of the international conference on machine learning icml lille france july pages
rico sennrich barry haddow and alexandra birch
improving neural machine translation
in proceedings of the els with monolingual data
annual meeting of the association for tational linguistics acl august berlin germany volume long papers
nal kalchbrenner and phil blunsom

recurrent in proceedings of continuous translation models
the conference on empirical methods in ral language processing emnlp tober grand hyatt seattle seattle ton usa a meeting of sigdat a special interest group of the acl pages
philipp koehn franz josef och and daniel marcu
in
statistical phrase based translation
man language technology conference of the north kaitao song xu tan tao qin jianfeng lu and yan liu

mass masked sequence to quence pre training for language generation
in ceedings of the international conference on machine learning long beach california
sandeep subramanian lample denoyer eric michael marcaurelio ranzato and y lan boureau

multiple attribute text style transfer
corr

guillaume ludovic smith ilya sutskever oriol vinyals and quoc v
le

sequence to sequence learning with neural works
in advances in neural information ing systems annual conference on neural formation processing systems december montreal quebec canada pages
pascal vincent hugo larochelle yoshua bengio and pierre antoine manzagol

extracting and composing robust features with denoising in machine learning proceedings of toencoders
the twenty fifth international conference icml helsinki finland june pages
yau shian wang and hung yi lee

learning to encode text as human readable summaries using generative adversarial networks
in proceedings of the conference on empirical methods in ral language processing brussels belgium ber november pages
a implementation choices for initialization and models we describe the modeling choices for tion models pr thr dbae and
all hyper parameters for each of these systems are set based on the models rouge l score on the idation set
unless otherwise stated all models use skipgram word embeddings which are shared across the input and output layers
the dimension embeddings are trained on the catenation of the full text and summary sequences df ds
v is the full vocabulary and v f and v s are the vocabularies of df and ds tively
all trained models use the adam mizer with learning rate
the convolutional models use the fconv tecture previded in with pre trained input and output word embeddings a vocabulary size of k for the full text and of k for the maries
for the expander generations we collapse contiguous unk tokens and cut the sentence at the rst full stop even when the model did not generate an eos token yielding outputs that are sometimes shorter than words
procrustes thresholded alignment pr thr for this model we train two sets of word beddings on df and ds separately and compute aligned vectors using the fasttext implementation of the grave et al

we then map each word in an input sequence to its closest word in v s in the aligned space unless the est neighbor is the eos token or the distance to the nearest neighbor in the aligned space is greater than a threshold
the output sequence then sists in the rst n mapped words in the order of the input sequence
we found that using dings of dimension threshold
and maximum output length n yields the best validation rouge l
we compare pr thr to a pbsmt baseline in table
we use the unsupervisedmt of lample et al
with the same pre trained embedding and also perform a hyper parameter search over maximum length which sets n


readthedocs
io test models
html
com fasttext tree master alignment
com unsupervisedmt tree master pbsmt denoising bag of word auto encoder dbae the dbae is trained on all sentences in ds
the encoder of the dbae averages the input word beddings and applies a linear transformation lowed by a batch normalization layer ioffe and szegedy
the decoder is a layer gru recurrent neural network with hidden dimension
the encoder output is concatenated to the initial hidden state of both layers then projected back down to the hidden dimension
to use the model for summarization we form two changes from the auto encoding setting
first we perform a weighted instead of a standard average where words that are less likely to appear in ds than in df are down weighted and words that are in v f but not in v s are dropped
ically given a word v v s its weight wv in the summarization weighted bow encoder is given as v psdf f and s v psds wv max s v f v secondly we implement something like a pointer mechanism by adding to the score of each of the input words in the output of the gru before the softmax
at test time and when creating articial data we decode with beam search and a beam size of size maximum output length n and input word bias
first order word moments matching the moments matching model uses the same coder as the dbae followed by a linear ping to the summary vocabulary followed by a sigmoid layer the log score of all words that do not appear in the input is set to
nately computing the output probabilities for all sentences in the corpus before computing the nary cross entropy is impractical and so we plement a batched version of the algorithm
let corpus level moments f v be dened as in equation
let bf be a batch of full text quences we dene v and s v psbf f and s v
s v f v f v for each batch the algorithm then takes a gradient step for the loss x vv s psbf s v s v the prediction is similar as for the pr thr tem except that we threshold on f s rather than on the nearest neighbor distance with old
the maximum output length is also n b more examples of model predictions we present more examples of the expander and summarizer models outputs in tables and
table shows more expander generations for all three initial models after one back translation epoch
they follow the patterns outlined in tion with dbae showing more variety but ing less faithful to the input
table show tions from the expander models at different translation iteration
it is interesting to see that each of the three models slowly overcome their the dbae expander s third initial limitations version is much more faithful to the input than its rst while the moments based approach starts ing rephrases and modeling vocabulary shift
the procrustes method seems to benet less from the successive iterations but still starts to produce longer outputs
finally table provides maries produced by the nal model
while the model does produce likely summaries we note that aside from the occasional synonym use or bal tense change and even though we do not use an explicit pointer mechanism beyond the standard attention the model s outputs are mostly extractive
over n nnn ancient graves found in greek metro dig pr thr over n nnn ancient graves were found in a greek metro unk
dbae the remains of n nnn graves on ancient greek island have been found in three ancient graves in the past few days a senior police ofcer said on friday
over n nnn ancient graves have been found in the greek city of alexandria in the northern greek city of salonika in connection with the greek metro and dig deep underground
ukraine crimea dreams of union with russia pr thr ukraine crimea unk of the union with russia
dbae ukraine has signed two agreements with ukraine on forming its european union and ukraine as its membership
ukraine s crimea peninsula dreams of unk one of the soviet republic s most unk country with russia the itar tass news agency reported
malaysian opposition seeks international help to release detainees pr thr the malaysian opposition thursday sought international help to release detainees
the malaysian opposition news reports said
dbae malaysian prime minister abdullah ahmad badawi said tuesday that the government s decision to release nnn detainees a report said wednesday
malaysian opposition parties said tuesday it seeks to help the release of detainees
russia to unify energy transport networks with georgia rebels pr thr russia is to unify energy transport networks with georgia rebels
dbae russian government leaders met with representatives of the international energy giant said monday that their networks have been trying to unify their areas with energy supplies
russia is to unify its energy and telecommunication networks to cope with georgia s separatist rebels and the government
eu losing hope of swift solution to treaty crisis pr thr the eu has been losing hope of a unk solution to the maastricht treaty crisis
dbae the european union is losing hope it will be a swift solution to the crisis of the eu eu hoping that it s in an urgent referendum
eu governments have already come under hope of a swift solution to a european union treaty that ended the current nancial crisis
table more examples of articial data after the rst back translation iteration
original malaysia has drafted its rst legislation aimed at punishing computer hackers an ofcial said wednesday
pr malaysia has enacted a draft the rst law on a unk computer hacking
pr malaysia has issued a draft of the law on computer hacking
pr malaysia has drafted a rst law on the computer hacking and internet hacking
malaysia s parliament friday signed a bill to allow computer users to monitor unk law
the country has been submitted to parliament in nnnn passed a bill wednesday in the rst reading of the computer system ofcials said monday
malaysia s national defense ministry has drafted a regulation of computer hacking in the country the prime minister said friday
malaysia will have drafts the rst law on computer hacking
malaysia has started drafts to be the rst law on computer hacking
malaysia today presented the nation s rst law on computer hacking in the country news reports said wednesday
title malaysia drafts rst law on computer hacking table evolution of generated full text sequences across iterations
article chinese permanent representative to the united nations wang guangya on wednesday urged the un and the international community to continue supporting timor leste
pred chinese permanent representative urges un to continue supporting timor leste title china stresses continued international support for timor leste article macedonian president branko crvenkovski will spend orthodox christmas this weekend with the country s troops serving in iraq his cabinet said thursday
pred macedonian president to spend orthodox christmas with troops in iraq title macedonian president to visit troops in iraq article televangelist pat robertson it seems is nt the only one who thinks he can see god s purpose in natural disasters
pred evangelist pat robertson thinks he can see god s purpose in disasters title editorial blaming god for disasters article the sudanese opposition said here thursday it had killed more than nnn government soldiers in an ambush in the east of the country
pred sudanese opposition kills n government soldiers in ambush title sudanese opposition says nnn government troops killed in ambush table example of model predicitons for f

