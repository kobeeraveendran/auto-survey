the links have it infobox generation by summarization over linked entities kezun zhang yanghua xiao hanghang tong haixun wang wei wang shawyh
edu
cn
ccny
cuny
edu
com school of computer science shanghai key laboratory of data science fudan university shanghai china city college cuny ny usa google research usa about an entity
many wikipedia articles contain structured formation such as table image text citation
all of which are the targets of information extraction
more importantly many entities are associated with an infobox which consists of a set of property value pairs about the entities
as an example figure shows the wikipedia article about steve jobs with an infobox on the right side wherein the rst property is born and its value is steven paul jobs february san francisco california us

such structured information is the core ing block behind many applications including search engines for answering user questions about these entities
n u j r i
s c v
v i x r a abstract online encyclopedia such as wikipedia has become one of the best sources of knowledge
much eort has been devoted to panding and enriching the structured data by automatic tion extraction from unstructured text in wikipedia
although remarkable progresses have been made their eectiveness and ciency is still limited as they try to tackle an extremely dicult natural language understanding problems and heavily relies on supervised learning approaches which require large amount eort to label the training data
in this paper instead of performing information extraction over unstructured natural language text directly we focus on a rich set of semi structured data in wikipedia articles linked entities
the idea of this paper is the following if we can summarize the relationship between the entity and its linked entities we ately harvest some of the most important information about the entity
to this end we propose a novel rank aggregation approach to remove noise an eective clustering and labeling algorithm to extract knowledge
we conduct extensive experiments to strate the eectiveness and eciency of the proposed solutions
ultimately we enrich wikipedia with million new facts by our approach
keywords labeling knowledge extraction rank aggregation clustering cluster
introduction online encyclopedia has become one of the best sources of knowledge
a typical example is which contains
million articles for english language and covers a wide range of human knowledge
another fast growing online encyclopedia is which contains million entities and is the largest knowledge base in chinese
wikipedia and baidubaike are nized in similar ways and have become the caliber of other online encyclopedias
in this paper we focus on these two encyclopedias for information extraction
among others one critical reason that makes online pedias extremely valuable is that part of their data is structured and hence machine processible
usually a wikipedia article is
wikipedia
org
baike
baidu
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for prot or commercial advantage and that copies bear this notice and the full citation on the rst page
to copy otherwise to republish to post on servers or to redistribute to lists requires prior specic permission a fee
copyright acm acm xxxxx xx x xx xx




figure fragment of steve jobs in wikipedia
despite much eort to enrich structured data the current fobox in wikipedia is often incomplete and inconsistent
this is mainly due to the fact that most infobox is generated by human editing which is not just labor intensive but also error prone
to be specic about wikipedia articles do not have infobox
these are not only those less popular articles but also new cles
for articles that have infobox the information in the infobox is often incomplete
some important ties may be missing and values of certain properties may be incomplete
information in infoboxs is often inconsistent across dierent articles and entities
for example the property place of birth in some infoboxes is also expressed as birthplace in other infoboxes the property value america and united states us america refer to the same country
in order to address the drawbacks of human editing recently extensive eort has focused on expanding and enriching the tured data by automatic information extraction from tured text in wikipedia
although remarkable gresses have been made their eectiveness and scalability are still somewhat limited mainly for the two reasons
first these ods rely on several natural language understanding tasks e

named entity recognition dependency parsing and relationship extraction which themselves are extremely challenging and ror prone
second many of the existing approaches are costly since they are essentially supervised learning methods and hence require large amount of labeled training examples
in this paper we propose an alternative approach for enriching instead of performing information extraction structured data
over unstructured natural language text directly we focus on a rich set of semi structured data in wikipedia articles linked tities
a wikipedia article typically consists of many links to other wikipedia articles
intuitively the author of the article in describing a wikipedia entity refers the reader to many other tities that are important or related to the entity
the key idea of this paper is the following if we can summarize the relationship between the entity and its linked entities then we immediately harvest some of the most important information about the entity
table entities toy story cars film brave film intel dell apple inc
blood pressure the public theater apple i apple lisa maria shriver lev grossman table knowledge property pixar animated films electronic companies american writers apple inc
hardware value toy story cars film brave film dell intel apple inc
maria shriver lev grossman apple i apple lisa let us use the example in figure to illustrate the intuition of our approach
table lists some linked entities in the wikipedia article of steve jobs which cover a variety of dierent aspects of the article entity steve jobs
if we further convert these linked entities into something shown in table where we assign a erty label to a linked entity or a set of linked entities the result provides a comprehensive structured summary of the entity steve jobs
in order to fulll this basic idea there are the following lenges we need to address as follows

how to accurately summarize linked entities
in order to convert the unstructured linked entities list in table to structured property value pairs in table we need to group the similar linked entities i
e
values together as well as assign a label i
e
property for each group
here our key tion is that it is relatively easier to summarize a group of ties than an individual because the group members disambiguate each other
we thus propose a cluster then label approach we divide linked entities into dierent semantic groups and then give each group a semantic label a property
more specically we propose a g means based clustering algorithm to cluster the linked entities into dierent semantic groups
in the steve jobs example we obtain four clusters
we further propose a label generating algorithm to generate a label for each group
each labeled group is eventually a candidate property value pair for the infobox

how to remove unrelated linked entities
although most linked entities are semantically related to the article entity some might have weak or no semantic relevance to the article entity
take the steve jobs example again we can see that some linked entities e

blood pressure and the public theater are not related to steve jobs
to remove these irrelevant linked entities we propose a novel ranking aggregation approach that integrates dierent ranking mechanisms to detect noisy linked entities
contributions
in summary this paper proposes an tive radically dierent approach for infobox generalization for online encyclopedia
by focusing on linked entities we bypass including all the diculties posed by the existing approaches the challenging nlp tasks manual labeling and human editing
more specially the main contributions of the paper are three fold
first to extract knowledge from the linked entities we propose an eective clustering and labeling algorithm
second we pose a novel rank aggregation approach to detect and remove noisy linked entities for wiki articles
third we conduct sive experimental evaluations to show that our method generates comprehensive infobox with better quality
the rest of the paper is organized as follows
in section we give a detailed description of handling noisy linked entities
in section we introduce the cluster and label algorithm
datasets and experiments are described in section
in section we duce some related works
in section we conclude our paper

remove noisy linked entities in this section we show how we remove noisy linked entities
we rst show that the noisy entities are nontrivial problem in online encyclopedias by empirical studies
then we propose a novel ranking aggregation approach to identify the noisy linked entities

empirical studies in typical online encyclopedias some linked entities have weak relevance to the article entity
these entities become noises for the understanding of the semantic of the article entity
for ample steve jobs in wikipedia also has links to blood pressure the public theater
each of which obviously has a weak tionship to steve jobs
they are linked just because they have a corresponding entry in the knowledge base
we need to identify and remove them
next we design an experiment to show that noisy entities are not trivial phenomenon
that is most articles have noisy linked entities
for each article in wikipedia or baidubaike we calculate a semantic distance between the article and each of its linked entity
in our study we use google distance inspired distance which is dened as t where b represents linked entities of article b and w represents entire articles in wikipedia
we regard the linked entity as noise if the distance is larger than threshold

we summarize the cumulative distribution of the percentage of noisy links and the results on wikipedia and baidubaike are shown in figure
we found that in wikipedia nearly of articles have noisy linked entities only articles have no noisy entities and articles have more than noisy linked tities
in baidubaike nearly articles have noisy links articles have no noisy entities and articles have more than noisy linked entities
the existence of the noisy linked tities makes it dicult to accurately understand the semantic understanding of entities
noisy links distribution in wikipedia noisy links distribution in baidubaike y t i t n e o g a t n e c r e p













y t i t n e o g a t n e c r e p

















percentage of noise wikipedia percentage of noise baidubaike figure noisy linked entities distribution in wikipedia and baidubaike

position aware ranking aggregation the basic idea to remove noisy linked entities is to rank all linked entities by their semantic relatedness to the article tity and then remove the semantically unrelated entities
thus ranking the semantic relatedness becomes a key issue
there are many individual ranking schemes of semantic relatedness
ever in general each individual ranking can only characterize the a specic aspect of the semantic relatedness
thus an aggregated ranking is necessary for the accurate identication of non related entities
many existing ranking aggregation approaches have been proposed
however most of them assume the uniform quality tribution of the ranking
that is the ranking results have the same quality for any two elements in the ordering
however we found that the individual ranking we used in this paper has a non uniform quality distribution which motivates us to propose a position aware ranking aggregation approach
preliminaries
we rst formalize the preliminary concepts
a ranking ri can be considered as a linear ordering on the linked entities
this means that given the linked entities set u with n elements ri is an one to one mapping from u to


n
we always assume that elements of higher or topper rankings have smaller value
the quality function qr of the ranking r is dened as a function q

n
measures our belief on the fact that the i th element under the ranking r owns the ranking position
hence qr is a function of the position of the ranking
suppose we are given two rankings such that their quality function have opposite monotonicity
that is i increases and i decreases with i
thus or n quantify the quality of e ranking or
we refer to them as the credit of e in the ranking
the smaller or n is the more credit that e owns in or


metrics our aggregated ranking is developed upon two wildly used sures co occurrence based metric and overlap coecient
this subsection elaborates these two measures
co occurrence
entities may co occur in a common page as linked entities
if two entities always co occur in a page they are more likely semantically related
for example milk and bread always co occur in pages describing food and hence they are evant in semantic
we use pmi pointwise mutual information to measure the degree of the co occurrence for a pair of entities
p m i of entity and entity y is dened as non uniform quality distribution of rankings
we have two ndings about these two rankings

first the quality of an element under each ranking varies with its position in the the ranking
that is to say both qw j c i and qp m i i depends on i

second qw j c i and qp m i i have opposite monotonicity
in our case we found that p m i is good at identifying the noisy entities but w jc is good at discovering the strongly related entities
these ndings imply that the we should develop position aware aggregation approaches
we give an example about entity apple inc
to justify the above two ndings
more support will be found in the ment sections
we compare the ranked list of w jc and p m i as well as the aggregated measure that will be propped in the following text in table
we can see that w jc can recognize the strongly related entities and p m i can correctly nd the noisy linked entities
but w jc regard related entities as noises e

iphone and p m i regard the unrelated entity e

software in contrast our ranking aggregation update as related entity
method take advantage of both two individual measures has less false positive and false negative results
wjc macintosh table dierent ranking strategies for apple inc
aggregation apple worldwide velopers conference os x mountain lion steve jobs apple tv magic mouse fortune magazine apple inc
advertising pmi apple battery charger steve jobs os x apple worldwide velopers conference os x mountain lion


apple time capsule business model greenpeace tional iphone software update


ireland cork city chancellor of the chequer india macbook pro


cork city video calling broadway books greenpeace tional p m y log y

position aware ranking aggregation our new ranking aggregation is based on the linear tion
given two rankings on u the generic linear tion dene the combined ranking as where y is the probability that and y co occur in the same entity page or is the probability that entity or y occurs in all co occurrence pairs
p m i is zero when and y are independent and maximizes when and y are perfectly lated i
e
when y equals to or
compared to the direct co occurrence number p m i evaluate their relatedness by statistical independence which penalizes the independent pairs with high co occurrence number
overlap coefcient
entities may share some common linked entities
a pair of entities has a larger overlap of linked entities is intuitively more relevant in semantic
for example the related entity pair milk and bread share a large number of common linked entities like food and drinks
we use the weighted jaccard coecient wjc to quantify the overlap ratio for an entity pair
for two entities and y w jc is dened as where nx is the linked entities of
here is used as the weight of e dened as w y penxny penxny idf e log n

for any e u where is used to control the preference to dierent rankings
in the naive linear combination is a static constant
that is we use the same for any e u
however previous observation implies that the preference to dierent rankings is dependent on the position of the entity under dierent rankings
hence in our new ranking aggregation we regard as a function of and so that it can express the best preference to rankings for dierent entities
specically we dene as where is a parameter used to control the speed that the curve approaches to the climax
based on we dene our new scoring function of e when we have the new scoring function as n where n is total number of articles and represents the number of articles containing a link to entity e
compared to the naive jaccard w jc use the idf e as the weight to suppress the general entities
like jaccard coecient the higher the wjc is the more related the entity pair is
if all entities have the same weight w jc will degrade into the naive jaccard coecient
it is easy to check that n
given the new score values of linked entities we rst normalize them
we use some articles as training data and label their linked entities as related or unrelated
we build a binary classication model and draw its roc curve nding that
is the best threshold to distinguish unrelated linked entities from others
we use this threshold for all the other articles




s e i t i n e o g a t n e c r e



e a h p a l
ratio ratio of entities as a function of ratio figure ratio and alpha
rationality
next we show how we derive the new ranking
given two rankings with oppositely monotonic quality functions the aggregated ranking should bias towards to the one with higher quality
specically for any entity e we evaluate according to
there are three specic cases
case
in this case e owns lar credit in and
hence and should be assigned a similar weight close to


case
in this case e owns more credits in than in
hence should bias toward
that means the weight of should be larger than
in the linear combination

case
it is the reverse case of case
in this case should bias toward
clearly a sigmoid function can express the desired relationship between and the ratio
specically we use the most widely used logistic function which is dened as where is a parameter used to control the speed that the curve approaches to the climax
furthermore if we replace the ratio by the its log ratio all the requirement in the three cases can be satised
the log ratio is dened as ln n substituting with the the log ratio we have the as dened in eq

selection of
we use linked entities of shanghai ple inc
steve jobs china new york city barack obama as samples
for each of these linked entity we calculate its ratio function value we use pmi and wjc as and respectively
we plot their distribution of ratio in figure
from the tribution we can see that most ratio values lie in the range of which hosts of all sampled linked entities
we also give the simulation of as a function of see eq
with set as dierent values in figure
the simulation shows that the larger is the sharper increase happens around
the simulation also reveals that when the range of ratio in which a signicant can be derived almost overlaps with the real range observed from the samples
hence typically we set

clustering and labeling after removing the noisy linked entities we keep only the mantically related linked entities
next we use a g means based clustering approach to divide them into dierent semantic groups
then we label each group with an appropriate property name
in this way we discover a new property and its value for an entity from its linked entities
distance metric is key for a clustering algorithm
hence we rst elaborate the distance metric

feature selection and distance metric to dene the distance metric we rst need to identify the fective features to characterize the objects to be clustered
here we use category information of entities for the clustering
in wikipedia or baidubaike an entity is usually assigned one or more categories by editors
a category is widely used to sent the concept of an entity
hence if a pair of entities has the similar categories they probably belong to same concept or main topic
category or concepts information has been shown to be eective for the document clustering and topic cation which motivates us to use categories to construct the feature vector for the entity
problem statement
the naive solution is using direct gories of entities as the features
let fe be the feature set for entity e
in the naive solution fe contains all the direct egories of e
let n be the number of all categories in wiki
we dene a n dimensional feature vector for each entity i
e



where measures the signicance that concept ci characterizes e
in general when ci is not in fe otherwise is dened by a certain measurement such as tf idf functions we will elaborate it in later texts
given two feature vectors of two entities a their distance is dened by the cosine distance fb kfak kfbk however using the direct categories for the distance metrics has the following two weaknesses first many categories are not hypernyms of the entity
some categories express the semantics other than isa lationship
for example steve jobs has category can buddhists which is an isa relationship
but it also has births a property apple inc works for ship and many other semantics other than isa
in general it is hard to use these non isa categories to characterize the concept of an entity
second many direct categories are quite specic
we late the frequency of all categories in wikipedia
we found that among the most frequent categories is in the form of year of birth or year of death
ous these are specic categories that characterize a specic property of the entity
in general the more specic the egory is the less possible two semantically close concepts can be matched in terms of the category
for example in category graph shows in figure apple inc
can only match with mosys an ip rich fabless semiconductor pany in term of a more abstract category technology panies instead of the specic one electronics companies
weight of c wc idf c as in eq
to eq
mark c as a feature of e corresponding weight is wc algorithm feature selection and weighting algorithm require entity e set of concept category pair c ensure feature and corresponding weight of e isa taxonomy graph g isa ae reachable categories from e in g for c in ae do end for return function isa end for return g end function g isa taxonomy graph a directed graph a threshold parameter for each concept category in c do weight of category for concept is calculated by eq
add an edge concept category to g if weight isa taxonomy construction
to overcome the above nesses we need to extend the feature set from the direct egories to high level categories described in algorithm
we may recursively use the categories of the categories for the pansion
however the extension is not trivial
because we need to ensure the expanded category can characterize the entity curately
that is to say we expect to improve the recall without sacricing the precision
for this purpose we generally need a certain constraints on the extension to ensure the accuracy
a general constraint is to only select the categories that are nyms of the entity
because a hypernym is a concept of the entity which is a natural interpretation of the entity
thus the problem is reduced to identication of a category that is a hypernym of an entity
we dene a scoring function to characterize the condence on category c being a hypernym of entity e
the denition of depends on the hierarchal structure of the hypernyms of e
for each entity we can construct a quality hierarchical taxonomy just according to the wiki gories
the taxonomy for entity e denoted by ee we is a direct acyclic graph with each edge assigned a weight w which reects our condence on the fact that is a hypernym of
algorithm to construct the taxonomy
given a threshold parameter we construct the isa taxonomy ee we for an entity or category e by a level wise solution
let c e
suppose we have nished the i th level i starts from
the i th level is as follows
for each category of any element say in ci such that we add the direct edge from to into ee and use as the edge weight
and add into c and ve if cj
these newly added categories constitute
we add the direct edge from to into ee and use as the edge weight
the procedure is repeated until no more valid category can be found
it is easy to prove that ge is a direct acrylic graph
figure category graph in wikipedia
scoring functions
next we dene
an observation is that many real hypernyms contains many frequent occurring words among the categories of the entities
this inspiration plies that we can use the word frequency to dene
cally for an entity or category e and its categories in wiki
we rst score the words in hypernym c for e
let s be the number of categories in that contains word s
we have s let kc be the number of unique words in c
the condence that the category c is an appropriate hypernym of e is dened as kc x wc let pec be the set of all the paths from e to c and pec be one of such path
now we are ready to dene the condence score for any category c in ge as a hypernym for e
max pecpec y ci cj pec
the score is dened as the maximal accumulative product of the edge weight over all paths connecting e to c
the larger the maximal produce the more possible the concept is a hypernym of the entity
we give example to illustrate our scoring functions
example scoring function
consider apple inc
its direct categories in wikipedia are electronics companies home computer hardware companies electronics companies of the united states computer companies of the united states steve jobs apple inc
establishments in california



the most frequent words in the categories are companies electronics computer united states
thus the categories containing these words are likely hypernyms of apple inc
such as electronics companies of the united states electronics companies
but steve jobs will be dropped in our approach since it contains less frequent words
in the construction of the isa taxonomy for the apple entity some high level categories such as technology companies will be covered
consequently many indirect category will be used to characterize an entity
improved distance metric
finally we are ready to dene our improved distance metric which share the same expression as eq
but with two improvements
first fe is extend into ve e
that is all categories in ge except e itself will be used as features
second is dened according to
we use the tf idf framework to dene
we rst dene the idf of a concept c i
e
idf c as idf c log n where n is the total number of entities in the wiki and is the number of entities whose isa taxonomy contains c
thus the nal weight of each feature is idf c to see the eectiveness of the above measurement we rank the categories of entity apple inc
by in table
we can see that most categories of higher rank can characterize the entity accurately and expressively
table category ranking for apple inc
computer companies of the united states electronics companies technology companies of the united states networking hardware companies retail companies of the united states home computer hardware companies


steve jobs apple inc
warrants issued in hong kong stock exchange
clustering algorithm we may directly use k means approach as the basic framework for clustering given the distance metric
but in our case the naive k means leads to bad results due to the following reasons

first in naive k means the parameter k is specied by users which is impossible when millions of entity clustering tasks need to be executed

second the naive k means randomly selection initial ters
the selection of initial center is inuential on the nal results
a smart selection strategy is expected to obtain a better clustering result
to solve these problems we propose a new clustering approach
the basic idea is using statistical test proposed in g means to guide the selection of best k and using a dynamically center selection strategy proposed in k to determine the best initial central points
our clustering algorithm is described in algorithm
the algorithm accepts the set of data points x as the input and return k clusters
the algorithm recursively bi partition the data until the stop criteria is reached
the bi partition procedure consists of three major steps
in the rst step we select two data points g as the initial centers by k
k is smarter than the random generation of two cluster centers
it lows the principle that the probability of a datapoint to be center should be proportional to the distance from the ready selected centers
following the idea we rst choose a datapoint uniformly at random from the group x
then we select another datapoint from the group with probability pxx where represents distance between and

in the second step we run k means on data points in g with k and the initial center as
after the means reaches to the convergence state or gets maximal iteration we get two clusters and their new centers

in the third step project datapoint in g onto vector i
and let z be which the cumulative distribution of d i
finally we test whether the anderson darling statistic value lies in the range of non critical values at signicance level
if true keep the original group and abandon the splitting
otherwise replace the group with two subclusters and continue bi partition them until no new clusters emerging
algorithm g means clustering algorithm require datapoints x signicance level ensure k clusters k g x clusters bi return clusters function bi select two datapints from group g by k run k means with k and the initial center as let be the two clusters and be the if gaussiant then if datapoints in g sponding two cluster centers follow gaussian distribution return g else end function end if k k return bip for example we remove the noisy entities for apple inc
in wikipedia and cluster them in above algorithm clusters show in table

labeling the cluster next we assign a semantic label for each group
in this way we explain why group of linked entities are linked to the article entity
the semantic label as well as the group of entities thus becomes a property of the target entity and its corresponding value
this information is a good supplement of the current infobox
for example a cluster which contains google maps ios ibooks xsan itunes if we assign the semantic label ios software for the cluster we successfully enrich the infobox of apple inc
with a software
that is the problem of cluster labeling some researches have already conducted on cluster labeling
a popular method for labeling cluster is applying the statistic technologies to select quency features
that is identifying the most common terms from the text that best represent the cluster topic
but the frequent terms may not convey meaningful message of the cluster
because some popular terms are also frequently occur in other clusters
as a result an appropriate cluster label should characterize the common topic of entities in each cluster and simultaneously informative
a good cluster label should satisfy two requirements
completeness
it should cover most entities in the ter
e
g
for rst cluster in table label tunisian jewish descent only cover one entity in the cluster
so we want a wildly covered label which can represents the group rectly

informativeness
we hope the label is the most specic label while covering all entities in the cluster
e

in rst cluster people by status covers all entities in the cluster but it is not informative
the completeness and the informativeness are contradicted to each other
in general the more abstract a label the more entities that it can cover
some improvements have been done to generate a meaningful label
inverse frequent term takes both frequency and weight of a term into consideration
a meaningful label for a cluster is a term with maximal inverse frequency
baseline labeling strategies
we rst give two naive methods to label clusters
however the naive solution in general has one or more weakness which motivates us to a least common ancestor lca based solution
in the previous subsection we have built the isa taxonomy graph ge for each entity e
all categories in ge will be used for the labeling
given a cluster x


ek let c be the union of each vei
we have two baseline labeling strategies

most frequent category mf for short
the direct solution is labeling the cluster using the most popular egory
let tf c be the number of ge such that ve for all entities in the cluster
thus mf selection strategy is arg max cc tf c
most frequent yet informative category mfi for short apparently mf tend to select popular concept and most popular concept are abstract concept
thus the formativeness is sacriced
to avoid this we take the idf like factor into account
formally mfi selection strategy is arg max cc tf c idf c idf c is dened by eq

however the above labeling methods have the following ness
mf tends to select general with good completeness but less informative label
mfi can recognize specic labels but in many cases maybe over specic
because some specic concepts own a large idf weight
next we propose a least common ancestor model to handle the tricky tradeo between the informativeness and complexness


lca based solution the lca model is dened on the isa taxonomy graph for the cluster x to be labeled
given a cluster x


ek we rst construct the isa taxonomy graph for x gx
we dene gx as the union of all isa taxonomy graph ge such that e x
here we ignore the weight of gx
thus the union of two isa taxonomy graphs and is the graph e with v and e
obviously gx is a dag
we can also dene g as the union of all isa taxonomy ge for each entity e
definition isa taxonomy graph for cluster x
the isa taxonomy graph for cluster x gx is the union of all isa taxonomy graph ge for each e x
proposition
for a set of entities x gx is a directed acrylic graph
problem model
given gx nding a best cluster label for x thus is reduced to the problem of nding a least common ancestor of x from gx
given two nodes u in g if u has a path to v then v is ancestor of u
for a set of entities x a lca in gx is an ancestor of all entities in x which has no descendant that is an ancestor of entities in x
the direct lca model clearly can ensure we nd a general enough concept to cover all entities
however the model may rice the informativeness
hence we need a more exible model allowing us to control the tradeo between informativeness and completeness
we introduce a coverage restraint into lca to tune the tradeo between coverage and informativeness
note that there may exist more than one lca
we use idf function dened in eq

to help select the best lca
we propose maximal lca to reect all these requisites
problem definition maximall lca
given an isa taxonomy graph gx for the entity cluster x nd an node a from gx such that a is the lca of at least entities in x and idf a is maximized
solution
to nd the best solution we rst give the tonicity property of the idf function dened in eq

the lemma states that if a category is ancestor of in g then it is obviously true
because according to idf idf
eq
the number of descendants of is no less than that of
the lemma suggests that bottom up level wise search solution for the maximal lca of x
because the lower level close to the entities will certainly have a larger idf value than the upper level
for example an isa taxonomy graph g shows in figure compose of entities and categories
is parent category of so is ancestor of
thus occurs in feature of and occurs in feature of then idf and idf
similarly is ancestor of both entities then idf
clearly idf of a category is always no larger than its descendant category
specically we use li i to denote the categories to be tested in the i th level
is dened as the parents of x in gx
in the ith level we rst let li be the parents of categories of
then we calculate the coverage of each category in li
if any category cover at least entities we return the one with maximal idf value from li as the result
otherwise the procedure proceeds into the i level
note that in each level we use the idf function to select the most specic one among all lca discovered in the same level
we also highlight that li many overlap with
the above level wise search can certainly nd the optimal solution due to lemma lemma monotonicity
given two categories if is an ancestor of in g we have idf idf
example
we give the example to show how maximal lca can be found
suppose there is cluster x compose of in figure and we set
first for categories in their coverage is

respectively
both coverage is lower than so we continue search upper level here both coverage of and is
hence and satisfy the requirement of lca we select the most specic one as the maximal lca since idf weight of is larger than
implementation optimizations
in real implementations we have two issues to address
first we set a maximal layer limit to boost the search procedure
second we need to handle cases where no appropriate a lca is found
next we elaborate our solutions to each issue
we set a upper limit for the search level due to two reasons
on one hand x may have no valid lca
on the other hand even if we nd a lca in a higher layer
the category we found may be too general thus is meaningless
note that our algorithm may return no result due to two sons
first the constraint posed by is too stricter
second the upper limit may although boosted the search but may miss some valid solution occurring in upper level
to solve this problem we run the maximal lca search iteratively with varying from to
obviously the iterative search can certainly nd a solution if at least category occur in gx
with increment as
experiment in this section we present our experimental results
we run the experiments on wikipedia released in january
the basic statistics of wikipedia before and after revoking the noisy entities are shown in table
we refer to the linked entity with at least one category as valid linked entity because we need to use the category information for the clustering
we run all experiments on a bit windows server system with intel xeon
cores cpu and g memory
we implement all the programs in java
we totally nd
m clusters for
m articles
for each cle we nd cluster on average
each cluster contains
entities on average
if we treat the article entity property an entity in a cluster as a single fact we extracted overall m facts
table statistics of wikipedia before after ing the noisy linked entities item article categories article has linked entity linked entity per article article has valid entity before
m
m
m
m after
m
m
m
m
effectiveness in this subsection we justify the eectiveness of our system with the comparison to two state of the art systems to extract knowledge from wikipedia
both of the two competitors extract the relationship of entity pairs by handling natural language tences
the rst system nds the sentences in an article mentioning two entities
the sentences will be parsed to drive a dependency tree and the shortest dependency path from one tity to the other entity gives the syntactic structure expressing the relationship between the entity pair
however an entity may be expressed in dierent formats known as the coreference lution problem which results into the low recall of
to solve the coreference resolution problem in the second system we borrow the idea from to extract many syntactic patterns of an entity then use to extract facts from wikipedia
we evaluate the precision and user satisfactory for all the tems
we randomly select wikipedia articles and recruited volunteers to manually evaluate the quality of the extracted facts of these articles
we present the existing infobox as reference to them and ask them to evaluate the systems
each volunteer was asked to rate the knowledge by one of the options in fectly sensible well sensible somewhat sensible not sensible at all
we assign each option with a score from sensible at all to sensible
the comparison results are shown in table where time cost per fact is the average time cost on generating one fact the processing time including nding the sentences is not considered in and
precision is measured as the percentage of sensible knowledge all three options except not sensible at all
recall is the percentage of linked entities that can be found a relationship between it and the article entity
user satisfactory is the average score for all samples
note that we also give the user satisfactory for the existing infobox
we can see from tbale that our system is signicantly more ecient than the two competitor systems
besides this our system outperforms the competitors signicantly in precision recall and user satisfactory
we highlight that the precision of our system is almost

the recall of our system is

the reason is that some linked entities are regarded as noises or do not have category information and consequently can not be clustered
if table comparison to baseline systems matric time cost per precision recall user satisfactory







infobox



we did nt count them in the recall computation we will get an even better recall
the user satisfactory of our system is close to that on the existing infobox suggesting that our extraction system has close quality to existing infobox
comparing to has a higher recall but a lower precision because it can discover more sentences containing the article entity and linked entity

remove noisy linked entities in this subsection we evaluate the eectiveness of our rank aggregation approach
the statics of wikipedia after removing all unrelated linked entities are shown in table
to quantify the goodness of a ranking scoring we rst manually label each linked entity as related or unrelated
this manually labeled data set is used as the ground truth
then for each ranking measure we generate an ordering by the measures and evaluate the ordering with the comparison to the ground truth by m
m t where m is the set of linked entities labeled with related and k is the set of top k entities in the ordering
by varying k from to the number of elements to be ordered we can draw the curve of m
we can further quantify the closeness of a ranking measure r with respect to a range s t k s m m p t s where m and m are the m curve of the measure r and the ground truth respectively means the range from top s to top t
the s t actually characterizes the average closeness in the range of
when s and t n n is the number of all elements we have measures the entire closeness to the ground truth of the ranking measure r
comparison to individual rankings
we use steve jobs apple inc
to evaluate the eectiveness of dierent ranking sures
results on other articles are similar to them
in our periment we order linked entities of the two samples by dierent rankings
the m curves are shown in figure in which we compare our aggregated measure to the two individual ranking measures p m i and w jc
we also give the m curve for the ground truth
the closer to the ground truth curve the ter the measure is
we can see that p m i is better than w jc in noise detection since p m i in general is closer than w jc to the ground truth curve
in general the curve of our aggregated measure is closer to the ground truth curve than the two ual measures
hence our rank aggregation is better than either p m i or w jc and outperforms them in both detecting strongly related entities and recognizing noisy entities
comparison to other aggregated measures
we next compare our aggregated ranking to the naive linear tion method with static
we vary from to with ment of
so that we can compare to the dierent linearly bined measures
for the two samples we calculate the closeness between the ground truth and dierent ordering measure r
the results are shown in figure where the zontal line is our aggregated measure
we can see that that our aggregated is superior to the naive linearly combined measure consistently over dierent
only in the case of apple inc
with raining from
to
the linearly combined measure can reach the same goodness as our measure
but in general users have no aggregation pmi wjc ground truth aggregation pmi wjc ground truth


k num



k num
steve jobs apple inc
figure for dierent ranking strategies num is number of entities in the ordering

k m







s s e n e s o c l
k m



s
s e n e s o c
l









alpha steve jobs alpha apple inc
figure comparison to other aggregated measures
prior knowledge to set an appropriate value for
instead our method automatically computes the appropriate and achieves the best performance
rationality of the motivation
next we justify the tion of renaming aggregation method
recall that our tion is based on the fact that p m i is good at identifying the semantically unrelated entities and w jc is good at identifying the semantically related entities
to verify this we need to alyze the entities in the head and tail part of the orderings
we select articles randomly and manually label their linked tities as related and unrelated
for each measure we calculate the closeness for the top head and last tail entities spectively by eq

for comparison we also give the result of a random ordering
the results are shown in figure

we can see that in the head part w jc is better than p m i and both outperforms the random ordering
but in the tail part p m i is better than w jc and random order
in both head and tail part the aggregated measure perfumes the best which justify again the eectiveness of our ranking aggregation approach

clustering and labeling we rst give the metrics used for the evaluation then present the experiment results some clustering and labeling results are shown in table
pmi wjc random aggregation s s e n e s o c l



head tail figure closeness for head and tail part in the order
mj coverage for dierent labeling strategies metrics for the evaluation of clustering
to evaluate the eectiveness of a cluster we use both the subjective and objective metric
the objective metrics include the inter cluster distance average distance between cluster centers and intra cluster tance average distance between entities and corresponding ter center
the two individual metrics can be furthered combined as a synthesis score known as valid index
formally let k be the number of clusters mi be the center of cluster ci we have inter k k x x j k k x x ecj valid inter intra intra e a good clustering result has a large inter distance and a small intra distance which induces a large valid index
when the cluster is labeled we may alternatively use subjective metric to evaluate the quality of the clustering
we adopt cision to evaluate the quality of the extracted knowledge
for a certain entity suppose its linked entities are clustered into c


ck and each cluster ci has label li
the precision of c under label set l li is dened as p c l x cic li where li is the percentage of entities in cluster ci that can be appropriately labeled by the li
li is evaluated by humans
metric for the labeling evaluation
given a cluster c ci and their label set l li we use the following metrics to evaluate the accuracy of l with respect to c
coverage
coverage of li with respect to ci is the age of entities in ci which is the descendant of li in the isa taxonomy graph gc
thus the coverage of l with respect to c is the average coverage of each label li with respect to corresponding ci
correctness
we use p c l to measure correctness of l with respective to c
clustering results
to evaluate the performance of ing we cluster the linked entities for china shanghai apple inc
steve jobs barack obama new york city and using our clustering approach with
and iteration
we give the results in table
to calculate p c l we use the labels generated by maximal lca
we can see that average valid of clusters is around
and average precision is approach to which suggests that the generated clusters are of high quality
table evaluation of clustering results entity shanghai steve jobs apple inc
barack obama china new york city average linked entity cluster time ms valid l













correctness for dierent labeling strategies figure evaluation of cluster labeling strategies
labeling results
we compare our labeling approaches to the baseline approaches including mf mfi and a state of the art approach score
sp uses wikipedia as nal source from which candidate cluster labels can be extracted
given a cluster sp rst generate some concepts and categories as candidate labels from wikipedia by measuring the relevance to terms in the cluster
for a cluster sp rst calculate the quency score of keywords in all candidate labels then propagate the score from keywords to label
finally the label with highest score is selected as the cluster label
we run maximal lca with

for clusters generated from linked entities of above sample entities we use coverage and correctness to evaluate dierent labeling strategies
the sults are shown in figure
we can see from the figure that coverage of m f is larger than m f i and sp that is reasonable because the category voted by m f is the feature of most entities in the cluster
and maximal lca has the largest coverage which approach to because the selected category is at least the ancestor of entities in the cluster
for correctness m f is a little better than m f i and obviously outperform sp and also maximal lca performs better than other approaches
table labeled clusters generated from apple inc
line in column label represents mf mfi sp and maximal lca separately and each label is given with its coverage no
cluster alan kay gil amelio andy hertzfeld ronald wayne guy kawasaki g bbc online electronic product mental assessment tool enhanced data for gsm evolution google maps ios ibooks xsan itunes dell foxconn ibm intel label people by status
tunisian jewish descent

apple inc
employees
tele conferencing
tele conferencing
open standards

ios software
ios software
ios software
ios software
computer hardware companies
computer hardware companies
computer hardware companies
computer hardware companies
we also give the clustering results for apple inc
under ent labeling approaches in table
we can see that mi in general can nd the frequent but general category such as the rst ter
mfi tends to nd the specic label which in general has a further propose a novel position aware rank aggregation method to detect the semantic related entities
we also propose an tive cluster reuse strategy to run clustering for millions of entities in wilkipeida
with these eective and ecient approach we extracted million new facts from wikipedia
low coverage such as the second cluster
the performance of sp is not stable which may generate either the general or specic label see the rst and second clusters of sp
compared to these methods maximal lca method can generate specic label of high coverage in most clusters
maximal lca enables us to nd knowledge such as apple inc
ios software google maps ios ibooks xsan itunes

related works data mining on encyclopedia
many works have been done in online encyclopedia to achieve some applications especially in wikipedia one of the most valuable online data source
esa and wikirelate use wikipedia to compute semantic ness for an entity pair
and and use wikipedia as external knowledge for clustering or labeling cluster which enrich the resentation of document with additional features from wikipedia
structural knowledge extraction
in the work of structural knowledge extraction
knowitall and textrunner tract open information from free text and some challenging task such as ner dependency parsing and relationship extraction are commonly use in text analysis
some structural knowledge have also been extracted from wikipedia like yago and dbpedia
dbpedia represents in rdf is a large scale tured knowledge base who extracts structured information from wikipedia and also links to other datasets on the web to wikipedia
but dbpedia is built on existing infobox in wikipedia and tural knowledge in other datasets
to make wikipedia more structural semantic wikipedia proposes a formalism to ture wikipedia s content as a collection of statements the ment can explain the relationship between article and linked tities
and try to extract relationship of linked entity use syntactic and semantic information and refer to relationships in infobox
these article related relationships can be good ment for infobox
specically to supply attribute value for complete infobox kylin ipupulator and ibminer learn models from structured information to guide the text processing
for example kylin rst predicts what attributes a sentence may contain and further use crf to extract attribute values from the candidate sentences
document summarization
instead of mining relationship of single linked entity we focus on all the linked entities for an article
since each linked entity direct to a specic article in wikipedia multi document summarization is a good solution to handle it
we can summarize the linked entities to groups and generate a theme for each group
in document summarization selects important sentences or paragraphs in the set of ments and build a summary with these passages
and forms the summary of documents to dierent event theme by using lda to capture the events being covered by the documents
ing is another widely used method to do summarization such as xdox and select a representative passage from the cluster after clustering
in this paper we use clustering method to summarize linked entities
and dierent from above structural knowledge tion methods we use the structured entities and categories only in wikipedia to extract
in this way we can avoid the text processing problem such as ner and dependency parsing

conclusion discovering and enriching structural information in online cyclopedia is valuable and challenging work
dierent from vious free text focused methods in this paper we propose an novel semi structured information based approach
we extract knowledge from wikipedia using rich set of linked entities
we propose an cluster then label approach which clusters the linked entities into dierent semantic groups and then give each group a semantic label a property
in this way we can get groups of facts in the form of cluster and semantic label
we
