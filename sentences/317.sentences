summeval re evaluating summarization evaluation alexander r
fabbri wojciech krysci nski bryan mccann caiming xiong richard socher dragomir radev yale university salesforce research alexander
fabbri dragomir

edu kryscinski bmccann cxiong
com abstract the scarcity of comprehensive up to date studies on evaluation metrics for text marization and the lack of consensus garding evaluation protocols continue to hibit progress
we address the existing shortcomings of summarization evaluation methods along ve dimensions we evaluate automatic evaluation metrics in a comprehensive and consistent fashion using neural summarization model outputs along with expert and crowd sourced man annotations we consistently mark recent summarization models ing the aforementioned automatic tion metrics we assemble the largest lection of summaries generated by models trained on the cnn dailymail news dataset and share it in a unied format we plement and share a toolkit that provides an extensible and unied api for ing summarization models across a broad range of automatic metrics we assemble and share the largest and most diverse in terms of model types collection of human judgments of model generated summaries on the cnn daily mail dataset annotated by both expert judges and crowd source ers
we hope that this work will help mote a more complete evaluation protocol for text summarization as well as advance research in developing evaluation metrics that better correlate with human judgments
introduction text summarization aims to compress long into a short uent and human readable form that preserves the most salient information from the source document
the eld has beneted from advances in neural network architectures sutskever et al
danau et al
vinyals et al
vaswani equal contributions from authors al
as well as the availability of scale datasets sandhaus hermann et al
grusky et al
narayan et al

recent advances in pretrained language models such as bert devlin et al
have vated a corresponding shift to pretraining methods in summarization liu and lapata zhang et al
dong et al
ziegler et al
raffel et al
lewis et al

a standard dataset for training summarization models is the cnn dailymail hermann et al
originally a question answering task which was repurposed for summarization by lapati et al

the dataset consists of news articles and associated human created bullet point summaries
the rouge lin metric which measures lexical overlap between generated and target summaries is then typically used gether with crowd sourced human annotations for model evaluation
while the current setup has come standardized we believe several factors vent a more complete comparison of models thus negatively impacting the progress of the eld
as noted by hardy et al
recent papers vastly differ in their evaluation protocol
existing work often limits model comparisons to only a few baselines and offers human evaluations which are largely inconsistent with prior work
additionally despite problems associated with rouge when used outside of its original setting liu and liu cohan and goharian as well as the introduction of many variations on rouge zhou et al
ng and abrecht ganesan shaeibavani et al
and other text generation metrics peyrard zhao et al
zhang et al
scialom et al
clark et al
rouge has remained the fault automatic evaluation metric
we believe that the shortcomings of the current evaluation col are partially caused by the lack of easy to use resources for evaluation both in the form of e f l c
s c v
v i x r a plied evaluation toolkits and large collections of model outputs
in parallel there is an issue with how uation metrics are evaluated themselves
many of the currently used metrics were developed and assessed using the document ing conference duc and text analysis ference tac shared tasks datasets dang and owczarzak
however it has recently been shown that the mentioned datasets contain human judgments for model outputs scoring on a lower scale compared to current summarization systems putting into question the true performance of those metrics in the new setting peyrard
we address these gaps in complementary ways we re evaluate automatic evaluation metrics in a comprehensive and consistent fashion using outputs from recent neural summarization models along with expert and crowd sourced human notations we consistently benchmark cent summarization models using the tioned automatic evaluation metrics we release aligned summarization model outputs from pers model outputs published between and trained on the cnn dailymail dataset to allow for large scale comparisons of recent marization models we release a toolkit of evaluation metrics with an extensible and unied api to promote the reporting of additional rics in papers we collect and release expert as well as crowd sourced human judgments for model outputs on articles over dimensions to further research into human correlated tion metrics
code and data associated with this work is available at
yale lily summeval
related work previous work examining the research setup of text summarization can be broadly categorized into three groups based on the subject of sis evaluation metrics datasets and models
dealing with evaluation methods lin examined the effectiveness of the rouge metric in various duc tasks
the authors concluded that evaluating against multiple references results in higher correlation scores with human judgments however a single reference setting is sufcient for the metric to be effective
owczarzak et al
studied the effects of inconsistencies in man annotations on the rankings of evaluated marization systems
results showed that level rankings were robust against annotation consistencies however summary level rankings were not stable in such settings and largely et from improving annotator consistency
rankel et al
analyzed the performance of ferent variants of the rouge metric using tac datasets
the authors found that higher order and less commonly reported rouge settings showed a higher correlation with human judgments
in a similar line of work graham conducted a large scale study of the effectiveness of different rouge metric variants and compared it against the bleu metric on the duc datasets
its sults highlighted several superior non standard rouge settings that achieved strong correlations with human judgments on model generated maries
in chaganty et al
the authors vestigated using an automatic metric to reduce the cost of human evaluation without introducing bias
together with the study the authors released a set of human judgments over several model outputs limited to a small set of model types
peyrard showed that standard metrics are in ment when dealing with summaries in the scoring range found in tac summaries but vastly differ in the higher scoring range found in current els
the authors reported that additional human annotations on modern model outputs are sary to conduct a conclusive study of evaluation metrics
hardy et al
underscore the ences in approaches to human summary evaluation while proposing a highlight based reference less evaluation metric
other work has examined the problems with applying rouge in settings such as meeting summarization liu and liu and summarization of scientic articles cohan and goharian
we build upon this line of search by examining the performance of several automatic evaluation methods including rouge and its variants against the performance of expert human annotators
in relation to datasets dernoncourt et al
presented a detailed taxonomy of existing rization datasets
the authors highlighted the ferences in formats of available corpora and called for creating a unied data standard
in a lar line of research grusky et al
offered a thorough analysis of existing corpora ing their efforts on news summarization datasets
the authors also introduced several metrics for generated summaries the queen s guard was left red faced after he slipped on a manhole cover he lost his ing and slid sideways knocking his bearskin on the side
the embarrassed soldier quickly scrambled to his feet as his colleagues marched past as if nothing had happened
tourist david meadwell recorded the unscheduled manouevre outside buckingham palace on day afternoon
holidaymaker david meadwell recorded the unscheduled manouevre outside buckingham palace
he lost his footing and slid sideways knocking bearskin on the side of the box
queen s guard was left red faced after he slipped on manhole cover
the entire incident was caught on a manhole cover
the embarrassed soldier quickly scrambled to his feet as his colleagues marched past
buckingham palace guard slipped on manhole cover in front of hundreds of horried tourists
the queen s guard was left red faced after he slipped on a manhole cover
he lost his footing and dropped his rie on the side of the box and dropping his rie
the incident was caught on camera camera camera
the guard is thought to have slipped because of metal shutters nailed to the soles of his boots
reference summaries river plate admit they dream of manchester united striker radamel falcao
the colombia international spent eight years with the argentine club
falcao has managed just four goals in premier league appearances
read falcao still has faith that he could continue at man utd next season
click here for the latest manchester united news
the incident occurred on april north of poland in the baltic sea
u
s
says plane was in international airspace
russia says it had transponder turned off and was ying toward russia expert scores avg
coh
con
flu
rel
crowd worker scores avg
coh
con
flu
rel
coh
con
flu
rel
coh
con
flu
rel
coh
con
flu
rel
coh
con
flu
rel
expert scores avg
coh
con
flu
rel
coh
con
flu
rel
crowd worker scores avg
coh
con
flu
rel
coh
con
flu
rel
generated summary examples illustrate common problems found in model outputs such as ambiguous nouns incorrect references and repetitive content
reference summaries highlight issues found in the cnn dailymail dataset such as click baits and references to other articles as well as unreferenced dates and low coherence caused by concatenating bullet point summaries
table example summaries with the corresponding averaged expert and crowd sourced annotations for coherence consistency uency and relevance
expert annotations better differentiate coherence consistency and uency among the examples when compared to the crowd sourced annotations
evaluating the extractiveness of summaries which are included in the toolkit implemented as part of this work
kryscinski et al
showed that news related summarization datasets such as cnn dailymail contain strong layout biases
the authors revealed that datasets in the current mat where each news article is associated with a single reference summary leave the task of marization underconstrained
the paper also lighted the problem of noisy low quality data in automatically collected news datasets
looking into models zhang et al
analyzed the level of abstraction of several cent abstractive summarization models
the thors showed that word level extractive models achieved a similar level of abstraction to fully abstractive models
in kedzie et al
the authors examined the inuence of various model components on the quality of content selection
the study revealed that in the current setting the training signal is dominated by biases present in summarization datasets preventing models from learning accurate content selection
kryscinski et al
investigate the problem of factual correctness of text summarization models
the authors concluded that the issue of ing facts touches up to of generated maries and list common types of errors made by generative models
closely related to that work maynez et al
conducted a large scale study of abstractive summarizers from the perspective of faithfulness
the authors reached similar sions stating that improving factual faithfulness is a critical issue in summarization
the results also showed that currently available evaluation ods such as rouge and bertscore are not cient to study the problem at hand
durmus et al
and wang et al
similarly examine faithfulness evaluation both proposing question answering frameworks as a means of evaluating factual consistency
insights and contributions coming from our work are complementary to the conclusions of vious efforts described in this section
to the best of our knowledge this is the rst work in neural text summarization to offer a large scale tent side by side re evaluation of summarization model outputs and evaluation methods
we also share resources that we hope will prove useful for future work in analyzing and improving rization models and metrics
shortly before publishing this manuscript a brary for developing summarization metrics was released by deutsch and roth
our toolkit is complementary to their work as their toolkit cludes only of our evaluation metrics
evaluation metrics and summarization models we briey introduce metrics included in our uation toolkit as well as the summarization models for which outputs were collected at the time of leasing this manuscript

evaluation metrics our selection of evaluation methods includes eral recently introduced metrics that have been plied to both text generation and summarization standard machine translation metrics and other miscellaneous performance statistics
rouge lin recall oriented study for gisting evaluation measures the ber of overlapping textual units n grams word quences between the generated summary and a set of gold reference summaries
rouge we ng and abrecht extends rouge by using soft lexical matching based on the cosine similarity of mikolov et al
embeddings
peyrard et al
is a model based metric that uses previously proposed evaluation metrics such as rouge js divergence and we as input features for predicting the evaluation score
the model is trained on human judgment datasets from tac conferences
bertscore zhang et al
computes larity scores by aligning generated and reference summaries on a token level
token alignments are computed greedily to maximize the cosine ilarity between contextualized token embeddings from bert
moverscore zhao et al
measures the mantic distance between a summary and ence text by making use of the word mover s tance kusner et al
operating over n gram embeddings pooled from bert representations
sentence mover s similarity sms clark et al
extends word mover s distance to view documents as a bag of sentence embeddings as well as a variation which represents documents as both a bag of sentences and a bag of words
scialom al
applies a bert based question answering model to answer cloze style questions using generated summaries
questions are generated by masking named ties in source documents associated with evaluated summaries
the metric reports both the overlap score and qa model condence
blanc vasilyev et al
is a reference less metric which measures the performance gains of a pre trained language model given access to a ument summary while carrying out language derstanding tasks on the source document s text
supert gao et al
is a reference less metric originally designed for multi document summarization which measures the semantic ilarity of model outputs with pseudo reference summaries created by extracting salient sentences from the source documents using soft token ment techniques
bleu papineni et al
is a corpus level precision focused metric which calculates n gram overlap between a candidate and reference ance and includes a brevity penalty
it is the mary evaluation metric for machine translation
chrf popovic calculates character based n gram overlap between model outputs and ence documents
meteor lavie and agarwal computes an alignment between candidate and reference sentences by mapping unigrams in the generated summary to or unigrams in the reference based on stemming synonyms and paraphrastic matches
precision and recall are computed and reported as a harmonic mean
cider vedantam et al
computes gram co occurrences between the candidate and reference texts down weighting common n grams and calculating cosine similarity between the grams of the candidate and reference texts
data statistics grusky et al
dene three measures of the extractiveness of a dataset
coherence consistency metric rouge l rouge su rouge w rouge rouge rouge pyr resp bertscore p bertscore r bertscore f moverscore sms summaqa blanc supert bleu chrf cider meteor length novel unigram novel bi gram novel tri gram repeated unigram repeated bi gram repeated tri gram stats coverage stats compression stats density



































































fluency relevance



































































table kendall s tau correlation coefcients of expert annotations computed on a system level along four quality dimensions with automatic rics using reference summaries per example
denotes metrics which use the source document
the ve most correlated metrics in each column are bolded
tractive fragment coverage is the percentage of words in the summary that are from the source ticle measuring the extent to which a summary is a derivative of a text
density is dened as the erage length of the extractive fragment to which each summary word belongs
compression ratio is dened as the word ratio between the articles and its summaries in addition to these measures we also include the percentage of n grams in the summary not found in the input document as a novelty score and the percentage of n grams in the summary which repeat as a score of redundancy
for a comprehensive explanation of each metric please refer to the corresponding paper

summarization models we broadly categorize the models included in this study into extractive and abstractive approaches
for each model we provide a model code m as well as a descriptive model name which will allow for easy matching with the released data
extractive methods neusum zhou et al
jointly scores and selects sentences by rst building a cal representation of a document and considering the partially outputted summary at each time step
banditsum dong et al
treats tive summarization as a contextual bandit lem where the document is the context and the quence of sentences to include in the summary is the action
latent zhang et al
propose a latent variable extractive model which views vance labels of sentences in a document as latent variables refresh narayan et al
propose using reinforce williams to extract summaries approximating the search space ing training by limiting to combinations of vidually high scoring sentences
rnes wu and hu propose a ence model to capture cross sentence coherence combining output from the coherence model and rouge scores as a reward in a reinforce framework
jecs xu and durrett rst extracts sentences from a document and then scores sible constituency based compressed units to duce the nal compressed summary
strass bouscarrat et al
extracts a summary by selecting the sentences with the closest embeddings to the document embedding learning a transformation to maximize the larity between the summary and the ground truth reference
abstractive methods pointer generator see et al
pose a variation of encoder decoder models the pointer generator network where the decoder can choose to generate a word from the vocabulary or copy a word from the input
a coverage anism is also proposed to prevent repeatedly tending to the same part of the source document
fast abs rl chen and bansal pose a model which rst extracts salient sentences with a pointer network and rewrites these tences with a pointer generator network
in tion to maximum likelihood training a rouge l reward is used to update the extractor via force williams
bottom up gehrmann et al
troduce a bottom up approach whereby a content selection model restricts the copy attention bution of a pretrained pointer generator network during inference
improve kryscinski et al
tend the model of paulus et al
by ing the decoder with an external lstm language model and add a novelty rl based objective ing training
unied ext hsu et al
propose to use the probability output of an extractive model as sentence level attention to modify word level attention scores of an abstractive model ing an inconsistency loss to encourage consistency between these two levels of attention
rougesal pasunuru and bansal propose a keyphrase based salience reward as well as an entailment based reward in addition to using a rouge based reward in a reinforce ting optimizing rewards simultaneously in nate mini batches
multi task ent qg guo et al
propose question generation and entailment eration as auxiliary tasks in a multi task work along with a corresponding multi task tecture
closed book decoder jiang and bansal build upon a pointer generator network by adding copy less and attention less decoder ing training time to force the encoder to be more selective in encoding salient content
seneca sharma et al
propose to use entity aware content selection module and an abstractive generation module to generate the nal summary
raffel et al
perform a atic study of transfer learning techniques and ply their insights to a set of tasks all framed as text input to text output generation tasks ing summarization
neuraltd bhm et al
learn a ward function from human judgments which is used in a reinforcement learning setting
bertsum liu and lapata troduce a novel document level encoder on top of bert devlin et al
over which they duce both an extractive and an abstractive model
ziegler et al
build off of radford et al
and ne tune the model by using human labels of which of four sampled summaries is the best to direct ne tuning in a reinforcement learning framework
unilm dong et al
introduce a model pretrained on three language modeling tasks unidirectional bidirectional and to sequence prediction
it is thus applicable to ural language understanding tasks and generation tasks such as abstractive summarization
bart lewis et al
introduce a noising autoencoder for pretraining sequence to sequence tasks which is applicable to both ral language understanding and generation tasks
pegasus zhang et al
introduce a model pretrained with a novel objective function designed for summarization by which important sentences are removed from an input document and then generated from the remaining sentences
resources we now describe the resources collected and leased together with this manuscript

model outputs the model output collection contains summaries associated with recent papers on neural text summarization described in section

we tained a total of model outputs as many papers include variations of the main model
all models were trained on the cnn dailymail news corpus and the collected summaries were generated ing the test split of the dataset without constraints limiting the output length
outputs were solicited from the authors of papers to ensure comparability between results presented in this paper with those in the original works
they are shared publicly with the consent of the authors
model outputs were transformed into a ed format and are shared with ids of the inal cnn dailymail examples so that generated summaries can be matched with corresponding source articles
pairing model outputs with inal articles was done using a heuristic approach that relied on aligning reference summaries
the pairing process revealed that examples in the cnn dailymail test split contained duplicate erence summaries preventing those examples to be correctly aligned
however this problem involves only
of the available data and should not have a signicant impact on downstream results
figure pairwise kendall s tau correlations for all automatic evaluation metrics
ids of duplicate examples are provided together with the data

evaluation toolkit the evaluation toolkit contains automatic uation metrics described in section
dated into a python package
the package vides a high level easy to use interface ing all of the underlying metrics
for each ric we implement both and functions that return the metric s score on and corpus levels cordingly
function inputs and outputs are also unied across all metrics to streamline metric evaluation and result processing
the toolkit comes with a standard conguration bling the most popular settings for each of the rics to enable easy out of the box use
however each metric can be further congured using ternal gin conguration les
we also provide a command line tool to evaluate a summarization model with several metrics in parallel

human annotations the collection of human annotations contains summary evaluations of recent neural rization models solicited from crowd sourced and expert judges
annotations were collected for articles randomly picked from the cnn dailymail test set
to ensure high quality of annotations each summary was scored by crowd sourced and expert workers amounting to level annotations
model outputs were evaluated along the following four dimensions as in cinski et al
coherence the collective quality of all sentences
we align this dimension with the duc quality question dang of structure and coherence whereby the summary should be well structured and well organized
the summary should not just be a heap of related information but should build from sentence to sentence to a coherent body of information about a topic
consistency the factual alignment between the summary and the summarized source
a factually consistent summary contains only statements that are entailed by the source document
annotators were also asked to penalize summaries that tained hallucinated facts
fluency the quality of individual sentences
drawing again from the duc quality guidelines sentences in the summary should have no ting problems capitalization errors or obviously ungrammatical sentences e

fragments missing components that make the text difcult to read
relevance selection of important content from the source
the summary should include only important information from the source document
annotators were instructed to penalize summaries which contained redundancies and excess mation
the data collection interface provided judges with the source article and associated summaries grouped in sets of
each group of summaries contained the reference summary associated with the source article to establish a common point of reference between groups
summary ing and order within groups were randomized for each annotator
judges were asked to rate the maries on a likert scale from to higher better along the four mentioned dimensions
crowd sourced annotators were hired through the amazon mechanical turk platform
the ing criteria were set to a minimum of proved hits and an approval rate of or higher
geographic constraints for workers were set to united states united kingdom and tralia to ensure that summaries were evaluated by native english speakers
compensation was fully calculated to ensure an average wage of usd per hour
gillick and liu showed that summary judgments obtained through non experts may fer greatly from expert annotations and could hibit worse inter annotator agreement
as a result in addition to the hired crowd sourced workers we enlisted three expert annotators who have written papers on summarization either for academic ferences or as part of a senior thesis
the expert annotators were asked to evaluate the same set of summaries under the same instructions as the hired crowd sourced workers
for expert ments we proceeded with two rounds of tion to correct any obvious mistakes as well as to conrm judgments and ensure a higher quality of annotations
in the second round annotators were asked to check all examples for which their score of a dimension differed from another annotator by more than points and where the other annotators were within point of each other
in cases where a score differed by more than points for which such a pattern did not exist all annotators ined the annotation
when re evaluating ples judges were allowed to see scores assigned by other expert annotators in the rst round of notations
while such a setting could undermine the wisdom of the crowd and shift the re assigned scores towards the average judgment from the rst round we encouraged experts to remain critical method cnn dm reference summary coherence consistency fluency relevance extractive models abstractive models neusum banditsum rnes pointer generator fast abs rl bottom up improve unied ext abs rougesal multi task ent qg closed book decoder zero bart pegasus pegasus dynamic mix







































































table human ratings of summaries along four evaluation dimensions averaged over three pert annotators broken down by extractive and stractive models
the m codes follow the tion described in section

the three rated models in each column are in bold
and discuss contested examples when necessary
for completeness the data collection user face and additional details regarding the data lection process are presented in the appendix
metric re evaluation
human annotations considering the concerns raised in previous work gillick and liu about the quality ferences between crowd sourced and expert tations we study this issue using the human tations collected as part of this work
to evaluate the inter annotator agreement of collected crowd sourced and expert annotations we computed the krippendorff s alpha cient krippendorff
we found the annotator interval kappa to be below an acceptable range
and
for the crowd sourced workers and the rst round of expert annotations accordingly
however the second round of expert annotations improved the inter annotator ment achieving a kappa coefcient of

for further insights we computed standard deviations of annotator scores within the respective groups and present histograms of those statistics in ure
plots of crowd sourced annotations show strong similarities across all evaluated dimensions
such an effect could be caused by an cient distinction made by the annotators between zero shot model was used for evaluation
figure histogram of standard deviations of inter annotator scores between crowd sourced annotations rst round expert annotations second round expert annotations respectively
the scored axes where the overall quality of a summary biased scores of the individual sions
the histograms also show that while the second round of expert annotations lowered the standard deviation of scores and substantially creased inter annotator agreement relevance and coherence remained the most disagreed on sions between experts
this could be attributed to the subjective nature of relevance and coherence as an evaluation dimensions kryscinski et al

to assess the similarity of annotations between the crowd sourced and expert annotators we aged the assigned scores per example within the respective annotator groups and computed son s correlation coefcient
the statistic returned a value close to indicating no correlation tween expert and crowd sourced judges
we also manually inspected the human tations and present examples of annotated maries both generated and reference as well as the differences in human judgments in table
the rst row shows a well written sive summary
the high quality of the summary is reected by top scores assigned by expert notators while being rated as average by sourced workers
the second row shows a mary with ambiguous pronoun usage and factual inconsistencies
the errors result in a decrease in coherence consistency and relevance scores in the expert annotations but do not see a sponding decrease in crowd worker annotations
the third row presents a factually correct mary that contains token and phrase repetitions
the errors were caught by the expert annotators resulting in a low uency score while sourced annotators incorrectly classied them as issues with factual consistency
these examples again illustrate the disparities in the ing of evaluated dimensions between judges and underscore our observation above about the formity of crowd sourced annotations the sourced annotations tend to be similar across ity dimensions even when distinctions exist which are captured in the expert annotations
results presented in this section highlight the difculties of crowd sourcing high quality tions and the necessity for protocols for improving human evaluation in text summarization

automatic metrics many automatic metrics have been proposed for evaluating both summarization and other text eration models
however the eld lacks a hensive study that would offer a consistent by side comparison of their performance
we dress this issue with the following experiments
in table we show kendall s tau rank relations between automatic metrics and human judgments calculated on a system level ing louis and nenkova
the statistics were computed using the available expert tations to avoid possible quality problems ciated with crowd sourced ratings as highlighted in the previous subsection
automatic metrics were computed in a multi reference setting ing the original reference summary included in the cnn dailymail dataset and additional maries coming from kryscinski et al
and the length of model outputs was not constrained
we report correlations without differentiating tween abstractive and extractive models as most metrics did not exhibit large differences in lation when reported separately
correlation results show several trends
we nd that most metrics have the lowest tion within the coherence dimension where the correlation strength can be classied as weak or moderate
this nding follows intuition as the majority of metrics rely on hard or soft quence alignments which do not measure well the interdependence between consecutive sentences
low and moderate correlation scores were also found for the relevance dimension
as discussed in the previous subsection such trends could sult from the inherent subjectiveness of the mension and the difculty of collecting tent human annotations
model correlations crease considerably across the consistency and ency dimensions
while unexpected the strong correlation with consistency could be attributed to the low abstractiveness of most neural els which could increase the effectiveness of rics using higher order n gram overlap such as or extractive density
referring back to the previous subsection both of the mentioned dimensions achieved high inter annotator ment between expert judges which could also itively affect the correlation scores
additionally the results show a substantially higher correlation between all evaluated dimensions and rouge scores computed for higher order n grams in parison to rouge l which corroborates with ndings of rankel et al

to examine the dependencies between ent metrics we computed kendall s tau rank lation coefcients pairwise between all metrics
results are presented as a correlation matrix in figure
following intuition we observe a strong correlation between all metrics that compute plicitly or explicitly the lexical overlap between generated and reference summaries
metrics suring the n gram novelty and repetitiveness show a weak negative correlation with all related metrics
length as a feature is weakly related with most metrics apart from blanc and supert which might suggest the mentioned metrics favor longer summaries
worth noting is also the weak correlation of reference less maqa blanc and supert metrics with most other evaluated metrics
results presented in this section highlight the evaluation dimensions that are not reliably covered by currently available metrics and pave the way for future work in model evaluation
model re evaluation we now turn to an analysis of model scores across human evaluations and automatic metrics
the evaluated models were released between and represent different approaches to rization abstractive extractive and hybrid and their architectures reect the trends in tion research
although in many cases we tained multiple variants of the same model in the study we focus on the versions with the highest rouge l scores
table contains the results of human tion across the four dimensions described in tion

scores for ground truth summaries are included as a point of reference
we nd that trained models such as pegasus bart and consistently performed best on most dimensions
notably the mentioned models scored highest on consistency and uency while obtaining lower scores for relevance and coherence
scores for extractive models highlight the known ings of such approaches which are lack of herence of summaries and issues with selecting relevant content
abstractive model ratings show an increasing trend with respect to the date of publication
this is a promising result as it gests that the quality of models is improving with time
worth noting is also the fact that ence summaries did not score well on tency coherence and relevance
upon tion of the annotations we found that the ence summaries often contained extraneous mation such as hyperlinks and click bait tions of other articles
as this information was not present in the source documents nor relevant for the summaries the annotators interpreted it as hallucinations and assigned lower consistency and relevance scores
additionally many reference summaries in the cnn dailymail dataset were constructed by naively concatenating bullet point summaries into contiguous sequences
such cessing steps negatively affected the coherence of examples
similar trends in human studies of erence summaries were reported by stiennon et al

examples of noisy reference summaries are shown in table
table show scores for model outputs across all automatic evaluation metrics
parameters of metrics used in this study can be found in the evaluation toolkit repository listed in section
the results align with insights coming from the human evaluation of models
we found that for most metrics the highest scores were assigned to large models pretrained on vast quantities of data
l pyr resp bertscore moverscore sms blanc supert method neusum banditsum latent refresh rnes jecs strass























































pointer generator fast abs rl bottom up improve unied ext abs rougesal multi task ent qg closed book decoder seneca neuraltd bertsum abs supervised unilm bart pegasus dynamic mix pegasus huge news






















































































































rouge extractive models























abstractive models


























































































































































































































































a model scores from summarization specic evaluation metrics
method bleu chrf cider length stats cov comp den repeated neusum banditsum latent refresh rnes jecs strass pointer generator fast abs rl bottom up improve unied ext abs rougesal multi task ent qg closed book decoder seneca neuraltd bertsum abs supervised unilm bart pegasus dynamic mix pegasus huge news

















































meteor extractive models







abstractive models
























































































































































































































model scores from other text generation evaluation metrics
table model scores from automatic evaluation metrics available in the evaluation toolkit
the ve highest scores for each metric and lowest for length and are bolded
however several metrics such as sms chrf and meteor tended to favor tractive models assigning the highest scores to their outputs
presented results provide a comprehensive spective on the current state of the eld and light directions for future modeling work
conclusions include we introduced summeval a set of resources for summarization model and evaluation research that a collection of summaries erated by recent summarization models on the cnn dailymail dataset an extensible and unied toolkit for summarization model evaluation and a diverse collection of human annotations of model outputs collected from the crowd source and pert annotators
using the accumulated resources we re evaluated a broad selection of current els and evaluation metrics in a consistent and prehensive manner
we hope that this work will prove to be a valuable resource for future research on text summarization evaluation and models
we also encourage the research community to join our efforts by contributing model outputs and ing the evaluation toolkit with new metrics
acknowledgements we thank all authors for sharing model outputs and tony wong for assistance with annotations
references dzmitry bahdanau kyunghyun cho and yoshua bengio

neural machine translation by jointly learning to align and translate
arxiv preprint

florian bhm yang gao christian m
meyer ori shapira ido dagan and iryna gurevych

better rewards yield better summaries in ing to summarise without references
ceedings of the conference on empirical methods in natural language processing and the international joint conference on ural language processing emnlp ijcnlp pages hong kong china
tion for computational linguistics
lo bouscarrat antoine bonnefoy thomas peel and ccile pereira

strass a light and effective method for extractive summarization in based on sentence embeddings
ings of the annual meeting of the ciation for computational linguistics student research workshop pages florence italy
association for computational tics
arun chaganty stephen mussmann and percy liang

the price of debiasing automatic metrics in natural language evalaution
in ceedings of the annual meeting of the sociation for computational linguistics ume long papers pages bourne australia
association for tional linguistics
yen chun chen and mohit bansal

fast stractive summarization with reinforce selected sentence rewriting
in proceedings of the annual meeting of the association for tational linguistics volume long papers pages melbourne australia
tion for computational linguistics
elizabeth clark asli celikyilmaz and noah a
smith

sentence mover s similarity tomatic evaluation for multi sentence texts
in proceedings of the annual meeting of the association for computational linguistics pages florence italy
association for computational linguistics
arman cohan and nazli goharian

ing summarization evaluation for scientic cles
in proceedings of the tenth international conference on language resources and uation pages portoro slovenia
european language resources ciation elra
hoa trang dang

overview of duc
in proceedings of the document understanding conference volume pages
hoa trang dang and karolina owczarzak

overview of the tac update summarization task
in tac
hoa trang dang and karolina owczarzak

overview of the tac summarization track
in proceedings of the text analysis conference
franck dernoncourt mohammad ghassemi and walter chang

a repository of in proceedings pora for summarization
of the eleventh international conference on language resources and evaluation lrec miyazaki japan
european language resources association elra
daniel deutsch and dan roth

sacrerouge an open source library for using and developing summarization evaluation metrics
jacob devlin ming wei chang kenton lee and kristina toutanova

bert pre training of deep bidirectional transformers for language in proceedings of the understanding
conference of the north american chapter of the association for computational tics human language technologies volume long and short papers pages minneapolis minnesota
association for putational linguistics
li dong nan yang wenhui wang furu wei xiaodong liu yu wang jianfeng gao ming zhou and hsiao wuen hon

unied guage model pre training for natural language understanding and generation
in advances in neural information processing systems pages
yue dong yikang shen eric crawford herke van hoof and jackie chi kit cheung

banditsum extractive summarization as a textual bandit
in proceedings of the ference on empirical methods in natural guage processing pages brussels belgium
association for computational guistics
esin durmus he he and mona diab

feqa a question answering evaluation work for faithfulness assessment in abstractive summarization
in proceedings of the nual meeting of the association for tional linguistics pages online
association for computational linguistics
kavita ganesan

rouge
updated and improved measures for evaluation of rization tasks
yang gao wei zhao and steffen eger

pert towards new frontiers in unsupervised evaluation metrics for multi document in proceedings of the annual rization
meeting of the association for computational linguistics acl online july pages
association for tional linguistics
sebastian gehrmann yuntian deng and der rush

bottom up abstractive in proceedings of the marization
ference on empirical methods in natural guage processing pages brussels belgium
association for computational guistics
dan gillick and yang liu

non expert uation of summarization systems is risky
in proceedings of the naacl hlt shop on creating speech and language data with amazon s mechanical turk pages los angeles
association for tional linguistics
yvette graham

re evaluating automatic summarization with bleu and shades of in proceedings of the rouge
ference on empirical methods in natural guage processing pages lisbon tugal
association for computational tics
max grusky mor naaman and yoav artzi

newsroom a dataset of
million summaries with diverse extractive strategies
in ings of the conference of the north ican chapter of the association for tional linguistics human language gies volume long papers pages new orleans louisiana
association for putational linguistics
han guo ramakanth pasunuru and mohit bansal

soft layer specic multi task summarization with entailment and question generation
in proceedings of the annual meeting of the association for computational linguistics volume long papers pages melbourne australia
association for computational linguistics
hardy hardy shashi narayan and andreas vlachos

highres highlight based reference less evaluation of summarization
in proceedings of the annual meeting of the association for computational linguistics pages florence italy
association for computational linguistics
karl moritz hermann tomas kocisky edward grefenstette lasse espeholt will kay mustafa suleyman and phil blunsom

teaching machines to read and comprehend
in advances in neural information processing systems pages
wan ting hsu chieh kai lin ming ying lee kerui min jing tang and min sun

a unied model for extractive and abstractive in summarization using inconsistency loss
proceedings of the annual meeting of the association for computational linguistics ume long papers pages bourne australia
association for tional linguistics
yichen jiang and mohit bansal

book training to improve summarization in proceedings of the coder memory
conference on empirical methods in natural language processing pages sels belgium
association for computational linguistics
chris kedzie kathleen mckeown and hal daum iii

content selection in deep in learning models of summarization
ceedings of the conference on cal methods in natural language processing pages brussels belgium
tion for computational linguistics
chin yew lin

looking for a few good metrics automatic summarization how many samples are enough in ntcir
klaus krippendorff

computing dorff s alpha reliability
wojciech kryscinski nitish shirish keskar bryan mccann caiming xiong and richard socher

neural text summarization a critical evaluation
in proceedings of the conference on empirical methods in ral language processing and the tional joint conference on natural language processing emnlp ijcnlp pages hong kong china
association for tional linguistics
wojciech kryscinski bryan mccann caiming xiong and richard socher

evaluating the factual consistency of abstractive text in proceedings of the marization
ference on empirical methods in natural guage processing emnlp pages online
association for computational tics
wojciech kryscinski romain paulus caiming xiong and richard socher

ing abstraction in text summarization
in ceedings of the conference on cal methods in natural language processing pages brussels belgium
tion for computational linguistics
matt kusner yu sun nicholas kolkin and ian weinberger

from word embeddings to document distances
in international ence on machine learning pages
alon lavie and abhaya agarwal

teor an automatic metric for mt evaluation with high levels of correlation with human ments
in proceedings of the second workshop on statistical machine translation pages prague czech republic
association for computational linguistics
mike lewis yinhan liu naman goyal marjan ghazvininejad abdelrahman mohamed omer levy veselin stoyanov and luke zettlemoyer

bart denoising sequence to sequence language generation pre training for natural translation and comprehension
arxiv preprint

chin yew lin

rouge a package for in text automatic evaluation of summaries
summarization branches out pages barcelona spain
association for tional linguistics
feifan liu and yang liu

correlation tween rouge and human evaluation of in proceedings tractive meeting summaries
of hlt short papers pages columbus ohio
association for tational linguistics
yang liu and mirella lapata

text in marization with pretrained encoders
ceedings of the conference on empirical methods in natural language processing and the international joint conference on ural language processing emnlp ijcnlp pages hong kong china
tion for computational linguistics
annie louis and ani nenkova

cally assessing machine summary content out a gold standard
computational linguistics
joshua maynez shashi narayan bernd bohnet and ryan t
mcdonald

on ness and factuality in abstractive tion
in proceedings of the annual ing of the association for computational guistics acl online july pages
association for tional linguistics
tomas mikolov ilya sutskever kai chen greg s corrado and jeff dean

distributed representations of words and phrases and their compositionality
in c
j
c
burges l
bottou m
welling z
ghahramani and k
q
berger editors advances in neural information processing systems pages
ran associates inc
ramesh nallapati bowen zhou caglar gulcehre bing xiang al

abstractive text marization using sequence to sequence rnns and beyond
arxiv preprint

shashi narayan shay b
cohen and mirella lapata

ranking sentences for tive summarization with reinforcement ing
in proceedings of the conference of the north american chapter of the association for computational linguistics human guage technologies volume long papers pages new orleans louisiana
sociation for computational linguistics
jun ping ng and viktoria abrecht

ter summarization evaluation with word dings for rouge
in proceedings of the conference on empirical methods in natural language processing pages bon portugal
association for computational linguistics
karolina owczarzak peter a
rankel hoa trang dang and john m
conroy

assessing the effect of inconsistent assessors on rization evaluation
in the annual ing of the association for computational guistics proceedings of the conference july jeju island korea volume short papers pages
the association for computer linguistics
kishore papineni salim roukos todd ward and wei jing zhu

bleu a method for automatic evaluation of machine tion
in proceedings of the annual ing of the association for computational guistics pages philadelphia sylvania usa
association for computational linguistics
ramakanth pasunuru and mohit bansal

multi reward reinforced summarization with saliency and entailment
in proceedings of the conference of the north american ter of the association for computational guistics human language technologies ume short papers pages new orleans louisiana
association for tional linguistics
romain paulus caiming xiong and richard socher

a deep reinforced model for arxiv preprint abstractive summarization


maxime peyrard

studying summarization evaluation metrics in the appropriate scoring range
in proceedings of the annual ing of the association for computational guistics pages florence italy
sociation for computational linguistics
maxime peyrard teresa botschen and iryna gurevych

learning to score system summaries for better content selection in proceedings of the workshop on ation
new frontiers in summarization pages copenhagen denmark
association for putational linguistics
maja popovic

chrf character n gram score for automatic mt evaluation
in ings of the tenth workshop on statistical chine translation pages lisbon tugal
association for computational tics
alec radford jeffrey wu rewon child david luan dario amodei and ilya sutskever

language models are unsupervised multitask learners
openai blog
colin raffel noam shazeer adam roberts katherine lee sharan narang michael matena yanqi zhou wei li and peter j
liu

exploring the limits of transfer learning with a unied text to text transformer
arxiv e prints
peter a
rankel john m
conroy hoa trang dang and ani nenkova

a decade of tomatic content evaluation of news summaries reassessing the state of the art
in proceedings of the annual meeting of the association for computational linguistics volume short papers pages soa bulgaria
ciation for computational linguistics
evan sandhaus

the new york times notated corpus
linguistic data consortium philadelphia
thomas scialom sylvain lamprier benjamin wowarski and jacopo staiano

answers unite unsupervised metrics for reinforced marization models
in proceedings of the conference on empirical methods in natural language processing and the international joint conference on natural language cessing emnlp ijcnlp pages hong kong china
association for tional linguistics
abigail see peter j
liu and christopher d
ning

get to the point summarization in with pointer generator networks
ings of the annual meeting of the sociation for computational linguistics ume long papers pages canada
association for computational linguistics
elaheh shaeibavani mohammad ebrahimi raymond wong and fang chen

a graph theoretic summary evaluation for in proceedings of the rouge
ference on empirical methods in natural guage processing pages brussels belgium
association for computational guistics
eva sharma luyang huang zhe hu and lu wang

an entity driven framework for abstractive summarization
in proceedings of the conference on empirical ods in natural language processing and the international joint conference on ral language processing emnlp ijcnlp pages hong kong china
tion for computational linguistics
nisan stiennon long ouyang jeff wu daniel m
ziegler ryan lowe chelsea voss alec ford dario amodei and paul christiano

learning to summarize from human feedback
corr

ilya sutskever oriol vinyals and quoc v le

sequence to sequence learning with in advances in neural ral networks
tion processing systems pages
oleg v
vasilyev vedant dharnidharka and john bohannon

fill in the blanc free quality estimation of document summaries
corr

ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan n gomez ukasz kaiser and illia polosukhin

tention is all you need
in advances in neural information processing systems pages
of the ieee conference on computer vision and pattern recognition pages
oriol vinyals meire fortunato and navdeep in advances jaitly

pointer networks
in neural information processing systems pages
alex wang kyunghyun cho and mike lewis

asking and answering questions to evaluate the factual consistency of summaries
in proceedings of the annual meeting of the association for computational tics pages online
association for computational linguistics
ronald j williams

simple statistical gradient following algorithms for connectionist reinforcement learning
machine learning
yuxiang wu and baotian hu

learning to extract coherent summary via deep ment learning
in thirty second aaai ence on articial intelligence
jiacheng xu and greg durrett

neural tractive text summarization with syntactic in emnlp ijcnlp pages pression
hong kong china
association for computational linguistics
fangfang zhang jin ge yao and rui yan

on the abstractiveness of neural document in proceedings of the marization
ference on empirical methods in natural guage processing pages brussels belgium
association for computational guistics
jingqing zhang yao zhao mohammad saleh and peter j liu

training with extracted gap sentences for arxiv preprint stractive summarization


pegasus tianyi zhang varsha kishore felix wu kilian q
weinberger and yoav artzi

bertscore evaluating text generation with bert
in international conference on learning resentations
ramakrishna vedantam c lawrence zitnick and devi parikh

cider consensus based in proceedings image description evaluation
xingxing zhang mirella lapata furu wei and ming zhou

neural latent extractive in proceedings of document summarization
the conference on empirical methods in natural language processing pages brussels belgium
association for tional linguistics
xingxing zhang furu wei and ming zhou

hibert document level pre training of hierarchical bidirectional transformers for in proceedings of document summarization
the annual meeting of the association for computational linguistics pages florence italy
association for computational linguistics
wei zhao maxime peyrard fei liu yang gao christian m
meyer and steffen eger

moverscore text generation evaluating with contextualized embeddings and earth mover distance
in emnlp ijcnlp pages hong kong china
association for putational linguistics
liang zhou chin yew lin dragos stefan munteanu and eduard hovy

paraeval using paraphrases to evaluate summaries in proceedings of the human matically
guage technology conference of the naacl main conference pages new york city usa
association for computational guistics
qingyu zhou nan yang furu wei shaohan huang ming zhou and tiejun zhao

neural document summarization by jointly learning to score and select sentences
in acl pages melbourne australia
association for computational linguistics
daniel m ziegler nisan stiennon jeffrey wu tom b brown alec radford dario amodei paul christiano and geoffrey irving

fine tuning language models from human erences
arxiv preprint

appendix data collection the data collection interface used by both crowd source and expert annotators is presented in figure
in the annotation process judges were rst asked to carefully read the tent of the source article and next proceed to uating the associated summaries along four axes relevance consistency uency and coherence
figure example of the data collection interface used by crowd source and expert annotators

