l u j l c
s c v
v i x r a dataset for automatic summarization of russian news ilya moscow institute of physics and technology moscow russia ilya

edu abstract
automatic text summarization has been studied in a variety of domains and languages
however this does not hold for the russian language
to overcome this issue we present gazeta the rst dataset for summarization of russian news
we describe the properties of this dataset and benchmark several extractive and abstractive models
we demonstrate that the dataset is a valid task for methods of text marization for russian
additionally we prove the pretrained model to be useful for russian text summarization
keywords text summarization russian language dataset introduction text summarization is the task of creating a shorter version of a document that captures essential information
methods of automatic text summarization can be extractive or abstractive
extractive methods copy chunks of original documents to form a summary
in this case the task usually reduces to tagging words or sentences
the ing summary will be grammatically coherent especially in the case of sentence copying
however this is not enough for high quality summarization as a good summary should paraphrase and generalize an original text
recent advances in the eld are usually utilizing abstractive models to get better summaries
these models can generate new words that do not exist in original texts
it allows them to compress text in a better way via sentence fusion and paraphrasing
before the dominance of sequence to sequence models the most common approach was extractive
the approach s design allows us to use classic machine learning methods various neural network architectures such as rnns or transformers and pretrained models such as bert
the approach can still be useful on some datasets but modern abstractive methods outperform extractive ones on cnn dailymail dataset since pointer generators
various pretraining tasks such as mlm masked language model and nsp next sentence prediction used in bert or denoising autoencoding used in bart allow models to incorporate rich language knowledge to understand original documents and generate grammatically correct and reasonable summaries
ilya gusev in recent years many novel text summarization datasets have been revealed
xsum focuses on very abstractive summaries newsroom has more than a million pairs multi news reintroduces multi document summarization
however datasets for any language other than english are still scarce
for sian there are only headline generation datasets such as ria corpus
the main aim of this paper is to x this situation by presenting a russian rization dataset and evaluating some of the existing methods on it
moreover we adapted the model initially used for machine translation to the summarization task
the bart model was successfully used for text summarization on english datasets so it is natural for mbart to handle the same task for all trained languages
we believe that text summarization is a vital task for many news agencies and news aggregators
it is hard for humans to compose a good summary so automation in this area will be useful for news editors and readers
more text summarization is one of the benchmarks for general natural language understanding models
our contributions are as follows we introduce the rst russian tion dataset in the news
we benchmark extractive and abstractive methods on this dataset to inspire further work in the area
finally we adopt the mbart model to summarize russian texts and it achieves the best results of all benchmarked
data
source there are several requirements for a data source
first we wanted news maries as most of the datasets in english are in this domain
second these summaries should be human generated
third no legal issues should exist with data and its publishing
the last requirement was hard to fulll as many news agencies have explicit restrictions for publishing their data and tend not to reply to any letters
gazeta
ru was one of the agencies with explicit permission on their website to use their data for non commercial purposes
moreover they have summaries for many of their articles
there are also requirements for content of summaries
we do not want maries to be fully extractive as it would be a much easier task and consequently it would not be a good benchmark for abstractive models
we collected texts dates urls titles and summaries of all articles from the website s foundation to march
we parsed summaries as the content of a meta tag with description property
a small percentage of all articles had a summary

com ilyagusev gazeta
com ilyagusev summarus dataset for automatic summarization of russian news
cleaning after the scraping we did cleaning
we removed summaries with more than words and less than words texts with more than words pairs with less than unigram intersection and more than unigram intersection
the examples outside these borders contained either fully extractive summaries or not summaries at all
moreover we removed all data earlier than the of june because the meta tag texts were not news summaries
the complete code of a cleaning phase is available online with a raw version of the dataset

statistics the resulting dataset consists of text summary pairs
to form training validation and test datasets these pairs were sorted by time
we dene the rst pairs as the training dataset the proceeding pairs as the validation dataset and the remaining pairs as the test dataset
it is still essential to randomly shue the training dataset before training any models to reduce time bias even more
statistics of the dataset can be seen in table
summaries of the training part of the dataset are shorter on average than summaries of validation and test parts
we also provide statistics on lemmatized texts and summaries
we compute normal forms of words using the package
numbers in the common ul row show size of an intersection between lemmas vocabularies of texts and summaries
these numbers are almost similar to numbers in the unique lemmas row of summaries columns
it means that almost all lemmas of the summaries are presented in original texts
table
dataset statistics after lowercasing train validation test text summary text summary text summary











unique words uw unique lemmas ul























dates pairs common ul min words max words avg words avg sentences avg uw avg ul we depict the distribution of tokens counts in texts in figure and the distribution of tokens counts in summaries is in figure
the training dataset
com kmike ilya gusev has a smoother distribution of text lengths in comparison with validation and test datasets
it also has an almost symmetrical distribution of summaries lengths while validation and test distributions are skewed
fig

documents distribution by count of tokens in a text fig

documents distribution by count of tokens in a summary to evaluate the dataset s bias towards extractive or abstractive methods we measured the percentage of novel n grams in summaries
results are presented in table and show that more than of summaries bi grams do not exist in original texts
this number decreases to if we consider dierent word forms and calculate it on lemmatized bi grams
although we can not directly compare these numbers with cnn dailymail or any other english dataset as this statistic is heavily language dependent we should state that it is for cnn dailymail and for xsum
from this we can conclude that the bias towards extractive methods can exist
another way to evaluate the abstractiveness is by calculating metrics of oracle summaries the term is dened in

to evaluate all benchmark models we used rouge metrics
for cnn dailymail oracle summaries score
f and for our dataset it is
f
dataset for automatic summarization of russian news table
average of novel n grams uni grams lemmatized uni grams bi grams lemmatized bi grams tri grams train




val




test





bpe we extensively utilized byte pair encoding bpe tokenization in most of the described models
for russian the models that use bpe tokenization performs better than those that use word tokenization as it enables the use of rich phology and decreases the number of unknown tokens
the encoding was trained on the training dataset using the sentencepiece library

lowercasing we lower cased all texts and summaries in most of our experiments
it is a troversial decision
on the one hand we reduced vocabulary size and focused on the essential properties of models but on the other hand we lost important information for a model to receive
moreover if we speak about our rization system s possible end users it is better to generate summaries in the original case
we provide a non lower cased version of the dataset as the main version for possible future research
benchmark methods we used several groups of methods
textrank and lexrank are fully unsupervised extractive summarization methods
summarunner is a vised extractive method
pg copynet are abstractive summarization methods

unsupervised methods this group of methods does not have any access to reference summaries and utilizes only original texts
all of the considered methods in this group extract whole sentences from a text not separated words
textrank textrank is a classic graph based method for unsupervised text summarization
it splits a text into sentences calculates a similarity matrix for every distinct pair of them and applies the pagerank algorithm to obtain nal ilya gusev scores for every sentence
after that it takes the best sentences by the score as a predicted summary
we used textrank implementation from the summa library
it denes sentence similarity as a function of a count of common words between sentences and lengths of both sentences
lexrank continuous lexrank can be seen as a modication of the trank that utilizes tf idf of words to compute sentence similarity as idf modied cosine similarity
a continuous version uses an original similarity trix and a base version performs binary discretization of this matrix by the threshold
we used lexrank implementation from lexrank python
lsa latent semantic analysis can be used for text summarization
it structs a matrix of terms by sentences with term frequencies applies singular value decomposition to it and searches right singular vectors maximum values
the search represents nding the best sentence describing the kth topic
we evaluated this method with sumy

supervised extractive methods methods in this group have access to reference summaries and the task for them is seen as sentences binary classication
for every sentence in an original text the algorithm must decide whether to include it in the predicted summary
to perform the reduction to this task we rst need to nd subsets of original sentences that are most similar to reference summaries
to nd these so called oracle summaries we used a greedy algorithm similar to summarunnner per and bertsumext paper
the algorithm generates a summary consisting of multiple sentences which maximize the score against a reference summary
summarunner summarunner is one of the simplest and yet eective neural approaches to extractive summarization
it uses layer hierarchical rnn and positional embeddings to choose a binary label for every sentence
we used our implementation on top of the allennlp framework along with generator implementation

abstractive methods all of the tested models in this group are based on a sequence to sequence work
pointer generator and copynet were trained only on our training dataset and mbart was pretrained on texts of languages extracted from the mon crawl
we performed no additional pretraining though it is possible to utilize russian headline generation datasets here

com summanlp textrank
com crabcamp lexrank
com miso belica sumy
com allenai allennlp dataset for automatic summarization of russian news pointer generator pointer generator is a modication of a sequence sequence rnn model with attention
the generation phase samples words not only from the vocabulary but from the source text based on attention bution
furthermore the second modication the coverage mechanism prevents the model from attending to the same places many times to handle repetition in summaries
copynet copynet is another variation of sequence to sequence rnn model with attention with slightly dierent copying mechanism
we used the stock plementation from allennlp
for summarization bart and are sequence sequence transformer models with autoregressive decoder trained on the noising autoencoding task
unlike the preceding pretrained models like bert they focus on text generation even in the pretraining phase
mbart was pretrained on the monolingual corpora for languages ing russian
in the original paper it was successfully used for machine tion
bart was used for text summarization so it is natural to try a pretrained model for russian summarization
we used training and prediction scripts from fairseq
however it is possible to convert the model for using it within huggingface s
we had to truncate input for every text to tokens to t the model in gpu memory
we also used unk token instead of language codes to condition
results
automatic evaluation we measured the quality of summarization with three sets of automatic metrics rouge bleu meteor
all of them are used in various text generation tasks and are based on the overlaps of n grams
rouge and teor are prevalent in text summarization research and bleu is a primary automatic metric in machine translation
blue is a precision based metric and does not take recall into account while rouge uses both recall and based metrics in a balanced way and meteor weight for the recall part is higher than weight for the precision part
all three sets of metrics are not perfect as we only have only one version of a reference summary for each text while it is possible to generate many correct summaries for a given text
some of these summaries can even have zero n gram overlap with reference ones

com pytorch fairseq
com huggingface transformers ilya gusev we lower cased and tokenized reference and predicted summaries with razdel tokenizer to unify the methodology across all models
we suggest to all further researchers to use the same evaluation script
table
automatic scores for all models on the test set



greedy oracle
textrank
lexrank
lsa
summarunner
copynet
pg small
pg words pg big
pg small coverage
finetuned mbart
rouge













l













bleu meteor



























we provide all the results in table
and are the most basic baselines where we choose the rst the rst two or the rst three sentences of every text as our summary
is a strong baseline as it was in cnn dailymail dataset
the oracle summarization is an upper bound for extractive methods
unsupervised methods give summaries that are very dissimilar to the original ones
lexrank is the best of unsupervised methods in our experiments
the summrunner model has the best meteor score and high bleu and rouge scores
in figure summarunner has a bias towards the sentences at the beginning of the text compared to the oracle summaries
in contrast lexrank sentence positions are almost uniformly distributed except for the rst sentence
it seems that more complex extractive models should perform better on this dataset but unfortunately we did not have time to prove it
to evaluate an abstractiveness of the model we used extraction and giarism scores
the plagiarism score is a normalized length of the longest common sequence between a text and a summary
the extraction score is a more sophisticated metric
it computes normalized lengths of all long non overlapping common sequences between a text and a summary and ensures that the sum of these normalized lengths is between and
as for abstractive models has the best result among all the models in terms of rouge and bleu
however figure shows that it has fewer dataset for automatic summarization of russian news fig

proportion of extracted sentences according to their position in the original document
novel n grams than pointer generator with coverage
consequently it has worser extraction and plagiarism scores table
table
extraction scores on the test set extraction score plagiarism score reference pg small coverage finetuned summarunner








human evaluation we also did side by side annotation of mbart and human summaries with yandex
a russian crowdsourcing platform
we sampled text and summary pairs from the test dataset and generated a new summary for every text
we showed a title a text and two possible summaries for every example
nine people annotated every example
we asked them which summary is better and provided them three options left summary wins draw right summary wins
the side of the human summary was random
annotators were required to pass training exam and their work was continuously evaluated through the control pairs honeypots

yandex
ilya gusev fig

proportion of novel n grams in model generated summaries on the test set majority table
human side by side evaluation votes for winner reference wins wins table shows the sults of the annotation
there were no full draws so we exclude them from the table
wins in more than cases
we can not just conclude that it performs on a superhuman level from these results
we did not ask our annotators to evaluate the ness of the summaries in any way
reference summaries are usually too provocative and subjective while generates highly extractive summaries without any errors and with many essential details and annotators tend to like it
the annotation task should be changed to evaluate the abstractiveness of the model
even so that is an cellent result for
table shows examples of mbart losses against reference summaries
in the rst example there is an unnamed entity in the rst sentence by them
in the second example the factual error and repetition exist
in the last example the last sentence is not cohesive
dataset for automatic summarization of russian news table
mbart summaries that lost









lacma art lm gala gucci

conclusion we present the rst corpus for text summarization in the russian language
we demonstrate that most of the text summarization methods work well for russian without any special modications
moreover performs exceptionally well even if it was not initially designed for text summarization in the russian language
we wanted to extend the dataset using data from other sources but there were signicant legal issues in most cases as most of the sources explicitly forbid any publishing of their data even in non commercial purposes
in future work we will pre train bart ourselves on standard russian text collections and open news datasets
furthermore we will try the headline eration as a pretraining task for this dataset
we believe it will increase the performance of the models
references
sutskever i
vinyals i
le q
sequence to sequence learning with neural works
in proceedings of the international conference on neural information processing systems vol
pp
cambridge mit press

wong k
wu m
li w
extractive summarization using supervised and supervised learning
in proceedings of the international conference on putational linguistics pp
coling organizing committee
ilya gusev
hochreiter s
schmidhuber j
long short term memory
in neural computation vol
issue pp

nallapati r
zhai f
zhou b
summarunner a recurrent neural network based sequence model for extractive summarization of documents
in proceedings of the thirty first aaai conference on articial intelligence pp


vaswani a
shazeer n
parmar n
uszkoreit j
jones l
gomez a
kaiser
polosukhin
i
attention is all you need
in advances in neural information processing systems pp


devlin j
chang m
lee k
toutanova k
bert pre training of deep tional transformers for language understanding
in proceedings of the ence of the north american chapter of the association for computational tics human language technologies vol
pp
minneapolis minnesota

see a
liu p
manning c
get to the point summarization with generator networks
in proceedings of the annual meeting of the association for computational linguistics vol
pp
association for computational linguistics vancouver

liu y
lapata m
text summarization with pretrained encoders
in proceedings of the conference on empirical methods in natural language processing and the international joint conference on natural language processing ijcnlp pp
association for computational linguistics hong kong

lewis m
liu y
goyal n
ghazvininejad m
mohamed a
levy o
anov v
zettlemoyer l
bart denoising sequence to sequence pre training for natural language generation translation and comprehension
in proceedings of the conference on empirical methods in natural language processing and the international joint conference on natural language processing ijcnlp pp
association for computational linguistics hong kong

liu y
gu j
goyal n
li x
edunov s
ghazvininejad m
lewis m
zettlemoyer l
multilingual denoising pre training for neural machine tion
arxiv preprint

narayan s
cohen s
lapata m
do nt give me the details just the mary aware convolutional neural networks for extreme summarization
in proceedings of the conference on empirical methods in natural language processing brussels

grusky m
naaman m
artzi y
newsroom a dataset of
million maries with diverse extractive strategies
in proceedings of the conference of the american chapter of the association for computational linguistics human language technologies association for computational linguistics new orleans

fabbri a
li i
she t
li s
radev d
multi news a large scale document summarization dataset and abstractive hierarchical mode
in ings of the annual meeting of the association for computational linguistics pp
association for computational linguistics florence

gavrilov d
kalaidin p
malykh v
self attentive model for headline generation
in azzopardi l
stein b
fuhr n
mayr p
hau c
hiemstra d
eds advances in information retrieval
ecir
lecture notes in computer science vol
springer cham dataset for automatic summarization of russian news
mihalcea r
tarau p
textrank bringing order into text
in proceedings of the conference on empirical methods in natural language processing pp
association for computational linguistics barcelona

erkan g
radev d
lexrank graph based lexical centrality as salience in text summarization
in journal of articial intelligence research vol
issue ai access foundation

barrios f
lopez f
argerich l
wachenchauzer r
variations of the similarity function of textrank for automated summarization
arxiv preprint

bahdanau d
cho k
bengio y
neural machine translation by jointly learning to align and translate
in international conference on learning representations

gardner m
grus j
neumann m
tafjord o
dasigi p
liu n
peters m
schmitz m
zettlemoyer l
allennlp a deep semantic natural language cessing platform
arxiv preprint

gu j
lu z
li h
li v
incorporating copying mechanism in sequence sequence learning
in proceedings of the annual meeting of the association for computational linguistics vol
pp
association for computational linguistics

gong y
liu x
generic text summarization using relevance measure and latent semantic analysis
in proceedings of the annual international acm sigir conference on research and development in information retrieval pp


lin c
rouge a package for automatic evaluation of summaries
in text marization branches out pp
barcelona

papineni k
roukos s
ward t
zhu w
j
bleu a method for automatic evaluation of machine translation annual meeting of the association for putational linguistics pp


denkowski m
lavie a
meteor universal language specic translation uation for any target language
in proceedings of the eacl workshop on statistical machine translation

kudo t
richardson j
sentencepiece a simple and language independent word tokenizer and detokenizer for neural text processing proceedings of the conference on empirical methods in natural language processing system strations pp


cibils a
musat c
hossmann a
baeriswyl m
diverse beam search for increased novelty in abstractive summarization
arxiv preprint

ott m
edunov s
baevski a
fan a
gross s
ng n
grangier d
auli m
fairseq a fast extensible toolkit for sequence modeling
in proceedings of naacl hlt demonstrations
korobov m
morphological analyzer and generator for russian and ukrainian languages
in analysis of images social networks and texts pp

