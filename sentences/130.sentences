query focused abstractive summarization incorporating query relevance multi document coverage and summary length constraints into models tal baumel dept
of computer science ben gurion university beer sheva israel
bgu
ac
il matan eyal dept
of computer science ben gurion university beer sheva israel
bgu
ac
il michael elhadad dept
of computer science ben gurion university beer sheva israel
bgu
ac
il abstract query focused summarization qfs has been addressed mostly using extractive ods
such methods however produce text which suffers from low coherence
we vestigate how abstractive methods can be applied to qfs to overcome such tions
recent developments in neural attention based sequence to sequence models have led to state of the art results on the task of stractive generic single document tion
such models are trained in an end to end method on large amounts of training data
we address three aspects to make stractive summarization applicable to qfs since there is no training data we porate query relevance into a pre trained stractive model since existing abstractive models are trained in a single document ting we design an iterated method to embed abstractive models within the multi document requirement of qfs c the abstractive els we adapt are trained to generate text of specic length about words while we aim at generating output of a different size about words we design a way to adapt the target size of the generated summaries to a given size ratio
we compare our method relevance sensitive attention for qfs to tractive baselines and with various ways to combine abstractive models on the duc qfs datasets and demonstrate solid improvements on rouge performance
introduction the query focused summarization qfs task was rst introduced in duc dang
this task provides a set of queries paired with vant document collections each collection ing a topic
the expected output is a short mary answering the query according to data in the documents
current state of the art methods for the task daume iii fisher and roark feigenblat et al
are extractive i
e
the produced summary is a set of sentences tracted from the document set
extractive methods tend to produce less ent summaries than manually crafted ones
some examples of the weaknesses of extractive methods include unresolved anaphora unreadable sentence ordering and lack of cohesiveness in text
other problem of extractive methods is the lack of ability to extract salient information from a long sentence without including less salient tion included in the sentence once the system is committed to a sentence the full sentence will be extracted
it has been well documented that tractive algorithms haghighi and vanderwende nallapati et al
tend to prefer longer sentences
while most of the reasons for the weaknesses of extractive summarization methods are hard to quantify we can illustrate the high probability of achieving incoherent text when applying tractive methods for qfs
we assume that a tence can not be well understood without its text if it starts with a connective phrase which we identify by matching a closed set of tives or breaks a co reference chain sentences where a non proper denite noun phrase or a noun refers to a noun phrase from a preceding sentence we identied co reference chains ing core nlp lee et al

the percent of sentences in duc that passed the two tions mentioned was lower than so only of the sentences in duc can be understood without context
this data on the risks on producing low ence text is a great incentive to test abstractive summarization methods for the task of qfs
in this work we aim at adapting abstractive single document summarization methods to handle the qfs task
n a j l c
s c v
v i x r a tecture with attention mechanism which has been adopted by most abstractive approaches
we tematically explore the stage at which query evance is most benecial to the qfs abstractive process
further we experiment with a method to build a summary through an iterative process of extraction abstraction pairs batches of vant content from multiple documents are ed then abstracted into a sequence of coherent segments of text
we compare our system both with top extractive methods and with various combinations of a trained abstractive model with relevance ing and multiple document input
we evaluate the proposed model called relevance sensitive stractive qfs rsa qfs on the traditional duc datasets
our experiments demonstrate the tial of abstractive qfs models with solid rouge gains over those baselines
previous work
extractive methods current state of the art methods on the task of qfs on the duc dataset could be categorized into unsupervised methods and small scale supervised methods unsupervised methods search for a set tences that optimizes a gain function
the cross entropy summarizer ces feigenblat et al
optimizes relevance under length straint
this method achieves current state of art rouge scores on duc datasets
small scale supervised methods use small datasets usually previous duc datasets to learn a representation of the dataset and using this resentation optimize a gain function
a recent ample of this approach is docrebuild ma et al
which trains a neural network to nd a set of sentences that minimize that original document reconstruction error
the method uses duc to learn word representations and obtains results slightly lower than ces
all of the extractive methods suffer from the herence problems mentioned above

sequence to sequence models for abstractive summarization figure comparison of the output of the unmodied model of see et al
vs
our model rsa qfs on a qfs data sample
the unmodied summary lacks coherence and is not relevant to the input query
the rst obstacles we face are no ing data is available for training end to end qfs in a way similar to what was recently done for single document generic abstractive tion existing abstractive models can not dle multiple documents as input and do not clude an explicit query relevance criterion in their computation of salient summary content c isting abstractive models have been trained to duce short summaries regardless of the tion density of the input document
for all these reasons a direct application of an existing state of the art abstractive model to a qfs data sample produces inappropriate output see fig

we hypothesize that an existing trained tive model encapsulates reusable linguistic edge which we can leverage for the qfs task
we investigate ways to augment such a pre trained single document abstractive model with explicit modeling of query relevance the ability to handle multiple input documents and to adjust the length of the produced summary accordingly
we validate this hypothesis within the work of a sequence to sequence abstractive methods in generation have emerged as practical tools since
at this point the most successful attempts at abstractive rization are on the task of generic single document summarization rush et al
nallapati et al
see et al
paulus et al
and are based on the sequence to sequence proach with attention mechanism bahdanau et al

these models include the following ponents encoder a neural network transforms a list of words into a list of dense vector representations
these dense representations aim to capture both the word and its context
encoders are most commonly implemented using a word embedding layer followed by a recurrent neural network rnn i
e
a long short term memory lstm component hochreiter and schmidhuber or gated recurrent units gru chung et al

decoder a neural network generates the next word in the summary conditioned on the sentation of the prex of the generated text and a dense context vector representing the input quence
the decoder is commonly implemented by an rnn a fully connected layer with the mension of the output matching the size of the cabulary and a softmax layer that turns a vector into a distribution over the vocabulary
attention mechanism a neural network mines the importance of each encoded word at each decoding step and maps the variable length list of encoded words representations into a size context representation
the attention anism is commonly implemented using multiple levels of fully connected layers to calculate the unnormalized attention weight of each word in the input and a softmax layer to normalize these weights
the training of such models for abstractive gle document summarization has been made sible by the availability of large scale datasets such as gigaword graff and cieri the new york times dataset sandhaus and cnn daily news hermann et al
which contain pairs of source text short text examples
for example the cnn daily mail corpus was automatically curated by matching articles to the summary created by the site editor
the dataset cludes k documents from cnn and k uments from the daily mail
the average size of an abstract in the corpus is words and the size of input documents is about words
in contrast the average abstract length in the duc qfs dataset is words
no such large scale dataset is currently able for the qfs task under the duc settings
we hypothesize that models trained on such datasets capture the linguistic capability to combine small windows of coherent sentences into concise phrases
accordingly our objective is to adapt such a pre trained generic abstractive tion architecture to the more complex task of qfs
recent work in abstractive qfs summarization nema et al
attempt to solve the issue of missing training data by introducing a new dataset for abstractive qfs based on debatepedia
the dataset introduced is however very different from the duc qfs datasets since the summaries sented are debate key points that are not more than a single short sentence with on average words per summary vs
words in the duc data the input texts are also short snippets of text with an average of words vs
duc that can reach more than words
because of the distinct size differences between the duc and dia datasets we can not compare the methods rectly
in this work we focus on adapting a cic architecture the pointer generator with erage mechanism network of see et al
to the qfs task
this model achieves state the art rouge lin and readability scores on the single document generic abstractive marization task
although the pointer generator with coverage mechanism network includes nicant modications pointer network and erage mechanisms it still adheres to the general encoder decoder attention architecture
we thus present our proposed modication in the ed context of the generic architecture as the dling of relevance is orthogonal to the processing of rare words using switch generator and the erage mechanism ability to avoid redundancy
our experiments are using the full network
query relevance we adopt the approach to qfs formulated in baumel et al
the qfs task is split into two stages a relevance model determines the tent to which passages in the source documents are relevant to the input query and a generic methods
incorporating relevance in with attention models as discussed above the lack of a large scale dataset similar to qfs task presented in duc prevents us from attempting an end to end solution that learns to generate a relevant mary using a the documents and the query as put
in order to overcome this obstacle we split the problem in two tasks a relevance model and an abstractive model that takes relevance into count
relevance can be introduced into an isting with attention model in different ways filter the input to include only sentences with high relevance score and pass the ltered put to the model at generation time we test this method as a baseline
inject the relevance score into the pre trained model
given a document and a query we calculate the relevance of each sentence to the query as a pre processing step and use this relevance as an additional input to the network
the relevance model predicts the relevance of a sentence given the query
we project the relevance score of tences to all the words in the sentence to obtain a word level relevance score
at each decoding step in the abstractive model we multiply each unnormalized attention score of each word calculated by the model by the pre computed relevance score as illustrated in fig

in the unmodied model we adapted see et al
the unnormalized tention of word i at step t is calculated by vt wsst battn et where wh ws and battn are trained ters hi is the encoder output for word i and st is the decoder state at step
the attention scores are later normalized using a softmax function
in our model we multiply each word by its relevance score before normalization reli et et i where reli is the relevance score of wi which combines sentence relevance and lexical vance as predicted using the relevance ranking model all words in the same sentence are given the same relevance score
we discuss below the figure two stage query focused summarization scheme
rization method is applied to combine the relevant passages into a coherent summary
the relevance model identies redundant un ordered passages using information retrieval methods whereas the summarization model selects the most salient tent removes redundancy and organizes the target summary
this schematic approach is illustrated in figure
this method achieves good rouge results when using simple extractive tion methods such as kl sum haghighi and derwende when the relevance model is of high quality
accordingly in order to adapt abstractive ods to qfs the rst baseline we consider consists of ltering the input documents according to vance and then pass the ltered relevant passages to an abstractive model
we hypothesize that this approach will not adapt well for abstractive ods because the input that is generated by the tering process is quite different from the type of documents on which the abstractive model was trained it is not a well structured coherent ument
abstractive models rely critically on the sequential structure of the input to take decision at generation time
our method aims at preserving the document structure while infusing relevance into the abstractive model during decoding
in this paper we consider very simple relevance models and do not attempt to optimize them we compare relevance measures based on unigram overlap between query and sentences and and encodings with cosine distance tween the query and sentences
to get an upper bound on the impact a good relevance model can have we also consider an oracle relevance model where we compare sentences with the gold maries using the word count cosine measure
our focus is to assess whether the mechanism we pose in order to combine relevance and abstractive capabilities is capable of producing uent and evant summaries given a good relevance model
document setdocument setdocument setdocument setqueryretrieverelevant passagesgenericsummarizationsummary figure illustration of the rsa qfs architecture relv ector is a vector of the same length as the put n where the ith element is the relevance score of the ith input word
relv ector is calculated in advance and is part of the input
range of relevance models with which we imented and how the relevance scores are brated in the model
in this scheme the adapted model is able to irrelevant sentences at generation time while still beneting from their context information at encoding time
this is in contrast to the ing baseline where the encoder is not fed relevance sentences at all
we hypothesize that in our proposed scheme the encoder will produce a better representation of the input documents than in the filtered baseline because it is used in the same regime in which it was trained
it is important to note that we do not re train any of the model the original parameters of the baseline encoder decoder attention model are used unchanged

calibrating the relevance score unlike other normalization methods the softmax function is very sensitive to the scale of the put values when the scale of the input is lower the variance of the softmax output is similarly low see figure
when the variance of the softmax output is low there is no single word that receives most of the normalized attention and the model is unable to focus on a single word
since most attention models use softmax to normalize the tention weights it is important to keep their scale when multiplying them by the relevance scores to keep well calibrated attention scores
to address this issue we multiplied the cosine similarity scores by in order to increase the scale from to before applying softmax figure a demonstration of the scale sensitivity of the softmax function
both gures illustrate a softmax operation over samples from a uniform tion left is sampled from the range and the right from
normalization
this scale modication had a nicant impact on the reported rouge mance

adapting abstractive models to multi document summarization with long output such as the summarization datasets mail cnn include single document input and short summary about words where duc quires words
we need to adapt the trained abstractive model to handle the document scenario and produce longer output
one possible solution is to use an extractive summarization method to generate the input and apply an abstractive method over it
while this method may handle multiple documents as input it suffers from two problems it can only crease recall since it is unlikely that the abstractive method can introduce relevant information not cluded in the input and it will suffer from the stractive model bias for short output we can not directly encourage the abstractive model to ate longer text to cover more content
instead we use the following simple eager gorithm to produce summaries from multiple uments and control the length of the output
we rst sort the input documents by overall tf idf cosine similarity to the query then iteratively marize the documents till the budget of words is achieved
to avoid redundancy we lter out generated sentences from the generated summary when more than half of their words are already cluded in the current summary
this algorithm ignores document structure and topic progression and uses a simplistic model of redundancy
we leave for future work the parison of this baseline algorithm with more algorithm iterative version documents sort by output summary new summary for document documents do summary rsa word for sentence summary do if summary sent budget then return output summary end if if is summary sentence then output sentence end if end for end for phisticated models of content redundancy and course
experiments the goals of the experiments are to compare rsa qfs with the baseline where the input uments are ltered according to relevance to test whether the method to incorporate relevance within the attention mechanism on a single ment input produces readable and relevant output c to measure the impact of the quality of ent relevance models on the output of rsa qfs on a single document input and to evaluate the iterative version of rsa qfs vs
a state of the art extractive qfs method ces

evaluation we tested the various scenarios using the qfs track data from the duc and datasets dang hoa
we also pared rsa qfs on the debatepedia dataset spite the differences in sizes discussed above
we use rouge metrics for all performance isons
we evaluate separately the incorporation of the relevance model with a pre trained abstractive model on a single document as an ablation study and we test the iterative algorithm to handle tiple input documents in a second round of iments
in the rst round of experiments we compare various abstractive baselines on the longest input document from the qfs topic set we also perimented with the most relevant document but obtained lower rouge performance
for such comparisons we use and rouge l metrics rouge l measures recall on the longest common substrings
when ing rsa qfs to the extractive method we use and rouge which are most usually reported for extractive method performance
the rouge values obtained in the document ablation study are expected to be much lower than competitive qfs results for two main reasons we use as reference the duc ence summaries with no modications
these erence summaries were created manually to cover the full topic set in contrast in the ablation study we read only a single document and the best mary we can generate will lack coverage the pointer generator abstractive model was trained to generate a words summary while duc datasets summaries contain words
still the reported rouge performance indicates trends to detect whether the generated short summaries manage to capture relevance

abstractive baselines we compare rsa qfs with the following lines blackbox we run the document through the pointer generator abstractive model without any modications
this weak baseline indicates whether our method improve qfs performance vs
an abstractive method that completely ignores the query
filtered we ltered half of the document tences by selecting the ones with the highest evance score
we maintained the original ing of the sentences
we then used the ltered document as an input to the pointer generator stractive model
the relevance score we used was the count of shared words betwen the query and the sentence see below the list of other relevance models we tested this model of relevance vided the best results on the ltered baseline
relevance sensitive attention rsa qfs this method is the main contribution of this work
we tested the method using the following vance score functions word count between the query and a given sentence
is a simple count of word overlap rsa tfidf we generated a tf idf sentation of the entire document set for each topic and aggregated the sentence scores using cosine similarity between the query and the sentence idf vectors rsa tfidf
rsa we use a model mikolov et al
pre trained on the google news dataset
relevance is measured as the sine similarity between the summed representation vector of each word in the query and in the tence
words that did not appear in the pre trained model vocabulary were ignored
results are given in table
as expected the blackbox method which ignores the query pletely performs poorly
more surprisingly we observe that the filtered model where we lter the input document according to the word count relevance model and then apply the abstractive model does not behave any better than the box unmodied model
in contrast rsa qfs improve signicantly all improvements are nicant within except over the filtered pipeline while processing exactly the same input material as the filtered method
this indicates that the way we porate relevance within the attention mechanism is more effective than directly adjusting the input representation
the word count relevance model achieves the highest rouge scores when compared with other relevance models
on all the datasets it performs the ltered baseline by a large amount
the based method is close and tently within condence interval of the word count method
we speculate that the fact that out of cabulary words are ignored and the fact that duc queries tend to be verbose and do not need much expansion explain the fact that does not improve on the word count model
the based method performed poorly
we presume this is due to the fact that the rouge did not eliminate stop words and frequent words for the evaluation

extractive baselines in this part of the experiments we compare the rsa qfs method extended with the iterative gorithm to consume multiple documents and a query under the exact duc conditions and duce summaries comparable to existing extractive methods
we compare with ces the current state of the art extractive algorithm on qfs
results are in table
we compare two relevance models with default settings as used in the pointer generator evaluation see et al
qfs the word count model which we identied as the best performing one in the ablation study and the oracle model
in the oracle model we pute the relevance of an input sentence by ing it to the reference models instead of with the query
this gives us a theoretical upper bound on the potential benet of more sophisticated retrieval ranking methods
we observe that rsa qfs is competitive with state of the art extractive methods and forms them in the metric
the oracle baseline shows that a more sophisticated relevance method has the potential to improve performance by a nicant amount and way above the current tive top model

evaluation using the debatepedia dataset we used the debatepedia qfs dataset nema et al
to evaluate our method vs
the lstm based diversity attention trained end to end on the debatepedia dataset model
we compare with the rouge recall results provided in the original paper
while the result in table may suggest our method outperforms the model that is trained on the actual dataset it must be noted that our model yielded summaries ten times longer than required and achieved very low rouge cision
we did not compare precision score since it was not provided in the original research
this comparison indicates the datasets are not directly comparable but that even on a completely ferent domain the abstractive capability lated in the model provides readable and realistic summaries
analysis
output abstractiveness in order to test if our model is truly abstractive instead of simply copying relevant fragments batim from the input documents we counted the amount of sentences from the summary generated by our model using word count similarity tion which are substrings of the original text
we found that on average only about of the tences were copied from the original document and that the average word edit distance between each generated sentence and the most similar tence is about edits tested on duc marized by the iterative rsa word count method
we observed that the generated sentences while single document l l l blackbox filtered rsa word count rsa tfidf rsa












































table incorporating relevance on a single longest document input multi document ces iterative rsa word count iterative rsa oracle


























table iterative rsa qfs vs
extractive methods recall rouge debatepedia l rsa word count





table results for debatepedia qfs dataset signicantly different from the source sentences do not introduce many new content words
almost all generated words are present in the source ments
while these two measures indicate a good level of abstractiveness it remains a challenge to measure abstractiveness in an interpretable and quantitative manner
cursory reading of the erated summaries still feels very literal
we assessed readability by reading the maries generated by the best performing methods the rsa word count an example can be seen in fig
and the oracle based iterative method
we found that the summaries produced by the gle document variant maintained the readability of the unmodied model
we did notice that the erage mechanism was affected due to our cation and some sentences were repeated in the summaries our model produced compared to the original abstractive model
the iterative version did not suffer from repeated sentences since they are dismissed by the algorithm but did suffer from lack of coherence between sentences indicating a better discourse model is required than the simple eager iterative model we used
improved ence also requires better evaluation metrics than the rouge metrics we have used
the produced summaries the methods and the code required to produce them are available at
for all all talbaumel rsasummarization
conclusion in this work we present rsa qfs a novel method for incorporating relevance into a neural models with attention mechanism for stractive summarization to the qfs task out additional training
rsa qfs signicantly improves rouge scores for the qfs task when compared to both unmodied models and a two steps ltered qfs scheme while preserving ability of the output summary
the method can be used with various relevance score functions
we compared the method with state of the art tive methods and showed it produces competitive rouge scores for the qfs task even with very simple relevance models and a simple iterative model to account for multiple input documents
when using an ideal oracle relevance model our method achieves very high rouge results pared to extractive methods
this study frames future work on document abstractive summarization we need to design quantitative measures of abstractiveness how much re formulation is involved in ing a summary given the input documents and of summary coherence to overcome the known itations of rouge evaluation when applied to non extractive methods
we also nd that vance models remain a key aspect of tion and the gap between oracle and practical vance models indicates there is potential for much improvement on these models
heeyoung lee angel chang yves peirsman nathanael chambers mihai surdeanu and dan jurafsky

deterministic coreference tion based on entity centric precision ranked rules
computational linguistics
chin yew lin

rouge a package for matic evaluation of summaries
in text tion branches out proceedings of the shop
barcelona spain volume
shulei ma zhi hong deng and yunlun yang

an unsupervised multi document summarization framework based on neural document model
in coling
pages
tomas mikolov ilya sutskever kai chen greg s rado and jeff dean

distributed tions of words and phrases and their in advances in neural information processing ity
systems
pages
ramesh nallapati feifei zhai and bowen zhou

summarunner a recurrent neural network based quence model for extractive summarization of ments
hi si d
ramesh nallapati bowen zhou caglar gulcehre bing xiang al

abstractive text rization using sequence to sequence rnns and yond
arxiv preprint

preksha nema mitesh khapra anirban laha and balaraman ravindran

diversity driven tion model for query based abstractive tion
arxiv preprint

romain paulus caiming xiong and richard socher

a deep reinforced model for abstractive summarization
corr


org

alexander m rush sumit chopra and jason a neural attention model for arxiv preprint ston

stractive sentence summarization


evan sandhaus

the new york times annotated corpus
linguistic data consortium philadelphia
abigail see peter j
liu and christopher d
manning

get to the point summarization with generator networks
corr


org

references dzmitry bahdanau kyunghyun cho and yoshua neural machine translation by corr
org bengio

jointly learning to align and translate




tal baumel raphael cohen and michael elhadad

topic concentration in query focused marization datasets
in aaai
pages
junyoung chung caglar gulcehre kyunghyun cho and yoshua bengio

empirical evaluation of gated recurrent neural networks on sequence ing
arxiv preprint

hoa trang dang

overview of duc
in ceedings of the document understanding conference
volume pages
hal daume iii

bayesian query focused rization
corr


org

guy feigenblat haggai roitman odellia boni and david konopnicki

unsupervised query focused multi document summarization in proceedings ing the cross entropy method
the international acm sigir of ence on research and development in tion retrieval
acm new york ny usa sigir pages




seeger fisher and brian roark

query focused summarization by supervised sentence ranking and in proceedings of the skewed word distributions
document understanding conference new york usa
david graff and c cieri

english gigaword pus
linguistic data consortium
aria haghighi and lucy vanderwende

ing content models for multi document in proceedings of human language tion
nologies the annual conference of the north american chapter of the association for tational linguistics
association for computational linguistics pages
karl moritz hermann tomas kocisky edward grefenstette lasse espeholt will kay mustafa leyman and phil blunsom

teaching chines to read and comprehend
in advances in ral information processing systems nips

org

td hoa

overview of duc
in document understanding conference
sepp hochreiter and jurgen schmidhuber

neural computation long short term memory


