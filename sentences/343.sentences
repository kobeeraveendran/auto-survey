summarize outline and elaborate long text generation via hierarchical supervision from extractive summaries xiaofei sun chun fan zijun sun yuxian meng fei and jiwei of computer science and technology zhejiang university computer center of peking university cheng laboratory shannon
ai xiaofei sun zijun sun yuxian meng jiwei
com
edu
cn
edu
abstract long text generation remains a challenge
the difculty of generating coherent long texts lies in the fact that existing models ingly focus on the tasks of local word tion and can not make high level plans on what to generate or capture the high level discourse dependencies between chunks of texts
inspired by how humans write where a list of bullet points or a catalog is rst outlined and then each bullet point is expanded to form the whole article we propose soe a pipelined system tha involves of summarizing outlining and elaborating for long text generation the model rst outlines the summaries for ent segments of long texts and then elaborates on each bullet point to generate the ing segment
to avoid the labor intensive cess of summary soliciting we propose the construction strategy which extracts segment summaries in an unsupervised manner by lecting its most informative part to reconstruct the segment
the proposed generation system comes with the following merits the summary vides high level guidances for text generation and avoids the local minimum of individual word predictions the high level discourse dependencies are captured in the conditional dependencies between summaries and are served during the summary expansion process and additionally we are able to consider signicantly more contexts by representing contexts as concise summaries
extensive experiments demonstrate that soe produces long texts with signicantly better quality along with faster convergence speed
introduction despite that recent large scale pretrained language models plms peters et al
devlin et al
liu et al
yang et al
clark et al
radford et al
li et al
brown et al
are able to produce high quality passages that can be hardly recognized by humans zellers et al
most of the generated good texts are within very limited length e

hundreds of tokens for most cases guo et al
bao et al
yan et al
and generating ent long texts remains a challenge radford et al
tan et al

the difculty lies in the fact that existing models generate texts in a by word manner predicting each subsequent token given its proceeding contexts using the softmax objective
this word by word strategy ingly focuses on the prediction of local words and can not make high level plans on what to generate
this results in the fact that long texts generated by current models are usually repetitive generic and self contradictory shen et al

to address these issues the coarse eration strategy is proposed fan et al
xu et al
yao et al
mao et al

in coarse generation a list of keywords or a short prompt is rst generated serving as a mary of the original text
the prompt is then fed to a model as an input to output the complete text
the coarse generation strategy cantly improves generation over the word by word strategy but still suffers from the following comings a limited capacity of the prompt a gle keyword list or prompt does not have enough capacity to summarize all the texts of long sages since long texts are usually consists of eral parts each of which focuses on a specic pect or topic zhou et al
narayan et al
guan et al

the usage of the generation strategy is thus limited to texts that can be summarized by a single prompt e

short stories
this explains why text length erated by the progressive generation model is still limited e

the introduced writingprompts dataset t c o l c
s c v
v i x r a in fan et al
has an average length of ries around and the average length of prompts is ignorance of high level discourse pendency the coarse generation strategy does not capture discourse level dependencies li and jurafsky jernite et al
which handle the high level information ow and tions between segments of texts
the ignorance of discourse level dependencies results in texts ing for coherence
humans write in a hierarchical top down ner before writing a thousand word long essay a human usually rst prepares a list of bullet points or a catalogue and then expands them to form the whole article
the sentence level coherence tween these bullet points is preserved when the bullet points are expanded providing guarantees that the full text is coherent
to mimic this top down manner of human ing in this paper we propose soe a pipelined system tha involves of summarizing outlining and expanding for long text generation the model rst outlines the summaries for different segments of long texts which actually mimics the process of humans outlining bullet points next the model elaborates on each bullet point to generate the responding segment
the proposed strategy comes with the following merits since each segment is associated with its own summary rather than the entire text sharing a single prompt the capacity of summaries to reconstruct the full text can be guaranteed the conditional generation bility between summaries captures the high level discourse dependencies and these dependencies are preserved when they are expanded to segments
this naturally resolves the incapability of modeling discourse level dependencies in the coarse c this model is able to generation approach
consider signicantly larger amount of contexts by representing chunks of contexts as concise maries
empirically we do not readily have summaries for segments in hand
the model thus needs to learn to summarize in an unsupervised manner
spired the gehrmann et al
zhou et al
liu et al
zhong et al
we propose the reconstruction strategy which extracts segment summaries by selecting its most tive part to reconstruct the segment
extensive experiments demonstrate that soe produces long texts with signicantly better quality than existing baselines
the rest of this paper is organized as follows section presents related works followed by tion reviewing some backgrounds
section troduces our proposed approach in detail
section and section respectively present the experiment results and ablation studies
last we give a sion in section
related work
generating long texts there are two lines of work to generate long text this rst line of work tackles the problem from the model perspective
new model structures kitaev et al
child et al
dai et al
ye et al
guo et al
sukhbaatar et al
correia et al
beltagy et al
zaheer et al
li et al
are designed to give the model the ability to congest more texts given limited memories or computing power
for example transformer xl dai et al
a modier to transformers vaswani et al
uses a segment level recurrence mechanism to able learning long term dependencies child et al
correia et al
kitaev et al
beltagy et al
zaheer et al
proposed to sparsify transformers by focusing only on a tion of attention connections tay et al
placed the dot product self attention with learned synthetic attention weights li et al
used an lstm predictor to automatically learn attention connections adapted to downstream tasks
the second line of researches focuses on oping new generation strategies
efforts have been devoted to the idea of planning then generation or coarse generation wiseman et al
sha et al
gehrmann et al
wiseman et al
moryossef et al
puduppully et al
hua and wang shen et al
fu et al
which greatly inspires this work
in coarse generation a list of keywords or a short sentence is rst generated providing ances to generate the full text
a recent work from tan et al
takes a multi step strategy which progressively renes the generated incomplete text until reaching a specied stage
similar ideas have also been applied to text summarization where gehrmann et al
proposed a bottom up method that rst identies phrases within a ment that are likely included in its summary
our work is also inspired by the strategy of hierarchical generation li et al
yu et al
lapati et al
liu and lapata which consider text units with bigger granularity li et al
proposed hierarchical lstms that arrange tokens sentences and paragraphs in a cal structure with different levels of lstms turing compositionality
shen et al
used multi level structures to learn a vae model for generating long coherent text
similar strategies are applied to the video captioning problem where yu et al
exploited hierarchical rnns for video caption generation

extractive summarization extractive summarization refers to the problem of selecting part of the input text as its summary
a fundamental problem in extractive summarization is to score constituent texts units e

phrases sentences or paragraphs and select highly ranked as the summary
haghighi and vanderwende used word frequencies in the input text to assign scores to words which are then in turn used to score sentences
higher ranked sentences are selected as the summary of the input text
liu et al
presented a two stage extractive abstractive framework which rst coarsely identies salient information followed by a generation model used to rene it
neural models have been widely used for scoring cao et al
ren et al
zhou et al

liu and lapata tuned bert devlin et al
to score each sentence for extractive summarization zhang et al
computed token similarity in each sentence using bert contextual embeddings to serve as an automatic evaluation metric for text generation
background we begin by reviewing the task of text generation
language modeling refers to the process of culating the probability of a sequence y yt where each yi denotes a constituent token of
the probability can be computed by decomposing the joint distribution into a product of conditional distributions over tokens t t where y is the partial sequence of tokens generated previously
during training the model is optimized to minimize the negative log likelihood nll yd log
during inference the model decodes a token at each time step t according to t based on the softmax functions yt where wout is the output word embedding trix and ht is the hidden state at time step t
ous smoothing models have been proposed to avoid overtting et al
pereyra et al

sequence to sequence generation models generate a target sequence y conditioning on a given source sequence which differs from language models lms in terms of whether or not conditioning on another input sequence
similar to lms the probability of the target sequence can be typically factorized as t t models are also optimized to minimize the nll log
in the rest of this paper we unify the notation of and by setting for lms
different architectures have been proposed to model t ing transformers vaswani et al
lstms luong et al
and cnns dauphin et al

at test time sequences are usually ated using beam search or its variants to promote diversity vijayakumar et al

model details for soe in this section we describe the details of soe

notations a long sequence of tokens y yk is rst sliced into a series of snippets yis where k denotes the number of constituent snippets
here we use the bold font to denote snippets and the normal font y to denote tokens
the ber of tokens n within each snippet is a parameter
we also use superscript i to denote the index of a snippet and subscript l to denote the index of a token
each yi consists a sequence of tokens yi yi ni where ni denotes the length of yi
our goal is to generate a subset of y denoted by yjk yk given its proceeding snippets denoted by j
each snippet yi is associated with a short summary si l denotes tokens and mi is the number of tokens in si
mi where si yi that contain the word which can be expressed as nw log nd where nw is the word count nd is ndw the total number of documents and ndw is the total number of documents containing the word
textrank textrank mihalcea and tarau is a weighted graph with tokens as nodes and the similarity between nodes as edges
we use bert devlin et al
to compute the similarities between sentences and then rank them based on the textrank algorithm
reconstruction a summary should be more formative than non summary sentences that is a summary should have the most ability to reconstuct the full text
to measure the degree of a sentence s reconstruction ability we use a model to predict the original given text the summary tence the probability of which is regarded as the reconstruction score
suppose there are n sentences in yi e

n and yi and yi j denotes the j th sentence in yi
the struction score for yi j denoted by j is given as follows j log j to obtain j we train another model where the input is yi j for each j and the output is yi by sequentially predicting tokens in yi
given the trained model we are to rank all sentences in yi and use the one with the highest score as the golden summary

outlining segment summaries in the summary generation stage we can not serve yjk and our goal is to sequentially generate sjk given y j i i s i the generation of summary si can be factorized into sequentially generating the constituent word within it i s i l y i s i this process ends until generating a special of sequence token eos or reaching a specied summary length m
we use the transformer base architecture vaswani et al
as the backbone
figure an overview of the proposed method
given proceeding tokens i we rst sequentially generate summaries sjk for each snippet
next we expand each summary s to form the full text yjk

pipeline overview instead of generating all constituent words in y one by one we adopt a hierarchical strategy
the process of generating yjk is decoupled into the following two stages
outlining segment summaries we quentially generate the summary si for each snippet conditioning on the summaries for previous pets
this mimics the process of catalog generation when humans write
expanding summaries to texts we pand each summary to the full segment by quentially generating its constituent words
an overview of the proposed method is shown in figure

extracting golden summaries at the training time we need to learn to generate summaries
but this is not straightforward because the golden summary si for the snippet yi is not readily at hand
manually soliciting summaries like fan et al
is both costly and slow
we thus propose to take the idea of unsupervised tractive summarization and for each snippet yi we extract its summary unsupervisedly and use the extracted si as the golden summary for learning
we investigate the following extractive methods to access the importance of selecting summary tences the rst three of which are similar to liu et al

random for comparing purposes we use a dom sentence as the summary
tf idf we take the sentence with the highest average tf idf score ramos as the golden summary
a word is assigned a score by idf that scales proportionally to the number of times the word appears in the document and is offset by the number of documents in the corpus source target segment segment segment k k k to take into account more contexts we adopt the segment level recurrence strategy similar to dai et al
where the hidden states computed for far away snippets are xed and cached to be reused for the next new snippet
gradients are not propagated to these far away snippets for memory and computation efciency
this strategy allows the model to exploit information in history to the largest extent

expanding summaries to texts next we expand each summary si to the full text for each segment by sequentially generating its constituent words i si l y i which has the same termination conditions as in the summarization generation

training and inference training for summary generation the takes i s i as the input former model and is optimized by minimizing the nll loss log i s i
due to the memory tion we limit y i to proceeding tokens and i to tokens at training
it is worth noting that the tokens of y i mostly come from the ment right before i
e
while s i comes from multiple proceeding segments since the summary is more concise
takes i as for the summary expanding stage the former model input and is optimized by minimizing the nll loss log i
the two models i
e
the mary generation and the summary expansion model share parameters with a task specic token pended to the start to notify the model on what to generate summaries or segments
inference at test time we rst use beam search with beam size to generate summaries
given the generated summary beam search is used again to generate the corresponding segment
we consider more contexts at test time where y i is limited to tokens and s i is limited to tokens
additionally we augment the vanilla beam search with the strategy of mutual information reranking li et al
fang et al

the key point of mutual information is to instead of merely handling the uni directional dependency from the source to target based on the forward probability log it models the tual dependency between the source and target in sequence to sequence generation i
e
the tion of the forward probability log and the backward probability log
specically in our case for summary generation is generated as follows arg max log i s log where i si is the backward probability of predicting the proceeding summary given
since direct decoding from eq
is infeasible we follow the practical solution in li et al
where we rst generate an n list based on the forward probability i s and then rerank the n list by combing the forward ability and the backward probability
similar strategy can also be applied to the mary expanding stage where yi is obtained as lows yi arg max log i si yi log the backward probability predicts the proceeding segment given the current segment
again beam search is combined with reranking to approximately nd the optimal result

slicing texts based on coherence scores one more thing we need to care about is how to slice the text into segments
the simplest way is to slice the full text equally
but this is sub optimal since the break point could be in the middle of two closely related sentences and one segment might contain multiple aspects
we thus propose a slicing strategy based on sentence level coherent scores
using the next sentence prediction nsp from bert devlin et al
we are able to measure the coherence score between two consecutive sentences with dex i and i denoted by i
given a full text y


let t denote the number sentences in y and denote the ith sentence
given a xed value k for the number sliced segments y will be sliced into k segments simplify as where we train a model to predict the proceeding summary given the current summary
i
e



yk where each yk consists of a group of consecutive sentences from y
let gk denotes the list of indexes of sentence in original y where denotes the index of the rst sentence in gk denotes the second sentence
let rk denote the number of sentences in gk
we wish to maximize the coherence scores tween two consecutive sentences within the same segment and minimize the score between two secutive sentences belonging to different segments giving the following objective to optimize k l where the coherence score between the ending sentence of a segment and the starting sentence of the next segment
given j eq
can be readily solved using linear programming
experiments in this section we present experiment results
for different methods to generate summaries we nd that the performance of reconstruction consistently outperforms the rest in our preliminary results
we thus only report results from reconstruction in the section
we will get back to analysis on different summary generation methods in the ablation study section

datasets we need a corpus of contiguous and long text to test soe
we use two word level datasets merity et al
and the bookcorpus dataset zhu et al

contains m training words from k articles with an average length of
k words per article
can be used to test the ability of modeling long term dependencies
the bookcorpus dataset is a more suitable dataset for our purpose with much longer and more contiguous texts
it contains a total number of roughly billion words and million sentences from books with an average length of k words for each book
the average number of words per sentence is
for both datasets we predict the last tokens at test time

baselines transformer xl transformers with level recurrence strategy dai et al
rally constitutes a baseline
the model sequentially generates texts in a word by word fashion
writingprompts rst predicts a list of keywords or a single prompt and then generates the full text given the prompt fan et al

different from fan et al
where golden prompts for stories are available we do not readily have the golden prompts
we thus use the extractive strategies scribed in section
i
the tf idf method to pick the keyword list as the prompt denoted by writingprompts keyword and the reconstruction method to select the highest ranking sentence as the prompt denoted by writingprompts sentence
progressive writingprompts the progressive strategy proposed in tan et al
which volves multiple stages of prompt generation
each stage produces a more ne grained sequence than the stage that comes before and is used as the put to generate the prompt for the next stage
we follow the protocols in tan et al
and use the tf idf score to obtain golden prompts for each stage
the number of stages is set to
for all models we used use adam kingma and ba with learning rate of

rate warmup over the rst steps and linear decay of the learning rate
we use a dropout rate of
on all layers including the softmax layer

evaluations we use the following evaluation metrics to evaluate the quality of different generation models from different perspectives
perplexity ppl perplexity measures how ent a piece of generated text could be dai et al

we use ppl as the basic evaluation metric in our experiments
diversity perplexity can not measure how diverse the generated text is
we thus use the scaled ber of distinct unigrams and bigrams to demonstrate the degree of diversity li et al
for generated texts
adversarial success inspired by adversarial evaluations bowman et al
kannan and vinyals li et al
we use the ial success metric which is dened as the fraction model bookcorpus perplexity parameters perplexity parameters vanilla writingprompts keyword writingprompts sentence progressive writingprompts soe vanilla soe






base large






m m m m m m m m m m m m m m table perplexity of different models on and bookcorpus
vanilla stands for our implementation of transformer xl dai et al

model diversity adversarial success adversarial success s level coherence nsp vanilla writingprompts keyword writingprompts sentence progressive writingprompts soe





base





























msj





table results of different models in terms of diversity adversarial success msj and sentence level coherence on the bookcorpus corpus
vanilla stands for our implementation of transformer xl dai et al

d n stands for distinct and mi stands the results for mutual information reranking
model large vanilla soe





table results of different models with large volumes in terms of diversity on the bookcorpus dataset
of a model successfully fooling a trained ator to believe that machine generated texts are from humans
the evaluator is a binary tion model
at the training time it takes as inputs machine generated texts and original texts and are trained to discriminate them
at test time sarial success is the value acc where acc notes the accuracy of the trained evaluator ing machine generated texts as machine generated
higher values of adversarial success denotes better text quality
ms jaccard msj msj measures the similarity of the n gram frequencies between the generated texts and the golden texts montahaei et al

we report and
sentence level coherence ppl msj and sity scores do not reect the sentence level ence of generated texts
we adopt the strategy in tan et al
where next sentence prediction nsp from pretrained bert model devlin et al
is used as a metric to measure the ence between each sentence and its next sentence
we report average nsp scores for all consecutive sentence pairs within the generated text

results table shows the results of perplexity for ent models on the and bookcorpus datasets
on both datasets soe achieves the lowest ppl compared to baselines transformer xl dai et al
writingprompts fan et al
and progressive tan et al

in particular for we gain a ppl decrease

and
against our implemented transformer xl writingprompts sentence and progressive while having the same or even fewer parameters
similar trend can be observed on bookcorpus
table and table show the results for msj diversity adversarial success and level coherence scores
as can be seen writingprompt based models generally outperform the transformer xl model which adopts the by word generation strategy
this validates the superiority of two step generation strategy over the naive word by word generation strategy for long text generation
the progressive prompt model which involves multi step of eration and expanding outperforms the one step the writingprompt keyword and sentence model which is in accord with our pectation
soe achieves signicantly better results compared to vanilla writingprompts and sive models in terms of all evaluation metrics ing that the proposed method can produce more uent coherent and diverse texts
the consistent performance boosts on all metrics demonstrate the importance of modeling discourse level cies and necessity of summary expanding strategy for long text generation
additionally enhanced by mutual information mi we observe additional performance boosts pecially for diversity and adversarial success
this is in accord with our expectation since mutual information is able to build bidirectional dencies between the source and the target models enhanced with mutual information can generate better summaries and the phenomenon of generic and repetitive generation can be alleviated li et al
leading to more diverse results
ablation studies
the effect of segment length the size of the segment can be neither too big nor too small extremely long segments might tain too many aspects or topics for the summary to summarize in which case the model will erate into the writingprompts model fan et al

for too short segments the summary not provide high level guidance
we thus need to nd the sweet spot for the segment length
figure shows results on the bookcorpus dataset
it is clear from the gure that too short segments and too long segments both lead to inferior performances

the effect of summary generation strategies it is worthwhile to explore how different summary extraction methods affect the nal performances
to this end we conduct experiments on the corpus dataset using different summary extraction methods i
e
random textrank tf idf and construction
table shows the results
we rst compare the ppl for summary generation where the reconstruction model achieves the lowest ppl figure ppl on the bookcorpus dataset w

t
ent segment lengths
method summary ppl text vanilla random textrank tf idf reconstruction













table performances of different summary extracion methods described in section

vanilla is the plain model that generates tokens one by one without maries
and thus produces summaries that are the easiest it is also to predict given proceeding contexts
interesting to see that across all summary tion strategies ppl for summarization generation is signicantly larger than text prediction which is reasonable since summaries for the upcoming segment requires more generalization abilities and there are more diverse options for what the next segment should talk about than the local choices for what the next sentence should talk about
for the nal text generation ppl struction achieves the best results in terms of ppl and
textrank and tf idf are better than vanilla
interestingly the strategy of using random sentences as summaries performs worse than without summaries which can be explained by providing no guidances is better than incorrect guidances

the effect of coherence based text slicing we replace the coherence based text slicing egy with the naive equal slicing strategy and see how this will negatively affect the performance
on the bookcorpus dataset we observe an increase of summary generation ppl from
to
and






idfreconstruction an
increase of ppl from
to
in ken generation which demonstrates the importance of slicing text into coherent segments for tion
but it is also worth noting that even with the native equal slicing strategy soe still performs signicantly better than other baseline models

decoupling the effects of summaries the positive effects from summaries are two fold it provides high level guidances for segment generation and with far away segments being concisely represented by summaries it gives the model the ability to consider longer contexts
to quantitatively measure the inuences from both aspects we conduct the following experiments at test time for the computation i s i and i the model can only access maries for segments that are used as contexts
in other words only summaries within the kens of proceeding contexts can be fed as inputs
this is different from the original version of soe in which s can extend to proceeding contexts until the limit of tokens is reached
we did not train the model but add this limitation at test time
on the bookcorpus dataset this leads to an crease of
in ppl
vs
and a decrease of
and
in
vs
and
vs


simplifying i s i here we explore different simplications for i s i
for i s i the current mary is generated based on both previous maries and segment tokens
we can simplify it as i where previous segment tokens are not fed as inputs to predict the summary which will signicantly decreases computing complexity
on the bookcorpus dataset we observe an increase of ppl in summary generation from
to
which subsequently leads to an
increase of ppl from
to
in token generation

convergence speed figure convergence speed for different models
model converges faster than then vanilla because of the high level guidance from prompts
conclusion in this paper we propose a two step hierarchical generation strategy for long text generation the model rst generates the summary for each ment conditioning on previous summaries and next each summary is expanded to form the full text segment
the proposed strategy provides level guidances for local text generation and ables high level discourse dependencies to be tured
extensive experiments demonstrate that soe produces long texts with signicantly better quality references hangbo bao li dong furu wei wenhui wang nan yang xiaodong liu yu wang songhao piao feng gao ming zhou and hsiao wuen hon

pseudo masked language models for ed language model pre training
iz beltagy matthew e
peters and arman cohan

longformer the long document transformer
samuel r
bowman luke vilnis oriol vinyals drew m
dai rafal jozefowicz and samy gio

generating sentences from a continuous space
at last we investigate how quickly different els converge
results are shown in figure
with the guidance of extracted summaries soe has a conspicuously faster convergence speed where at about k training steps it has approximately reached the best result while the other two models vanilla and writingprompts do not converge until k training steps
the writingprompts tom b brown benjamin mann nick ryder melanie subbiah jared kaplan prafulla dhariwal arvind neelakantan pranav shyam girish sastry amanda askell al

language models are few shot learners
arxiv preprint

ziqiang cao furu wei li dong sujian li and ming zhou

ranking with recursive neural works and its application to multi document rization
in aaai pages
citeseer
keywordours rewon child scott gray alec radford and generating long ilya sutskever

quences with sparse transformers
arxiv preprint

kevin clark minh thang luong quoc v le and christopher d manning

electra pre training text encoders as discriminators rather than tors
arxiv preprint

goncalo m
correia vlad niculae and andre f
t
martins

adaptively sparse transformers
in proceedings of the conference on empirical methods in natural language processing and the international joint conference on natural guage processing emnlp ijcnlp pages hong kong china
association for tional linguistics
zihang dai zhilin yang yiming yang jaime bonell quoc le and ruslan salakhutdinov

transformer xl attentive language models beyond in proceedings of the a xed length context
annual meeting of the association for tional linguistics pages florence italy
association for computational linguistics
yann n dauphin angela fan michael auli and david grangier

language modeling with gated volutional networks
in international conference on machine learning pages
jacob devlin ming wei chang kenton lee and kristina toutanova

bert pre training of deep bidirectional transformers for language ing
arxiv preprint

angela fan mike lewis and yann dauphin

erarchical neural story generation
arxiv preprint

hao fang saurabh gupta forrest iandola rupesh k srivastava li deng piotr dollar jianfeng gao aodong he margaret mitchell john c platt al

from captions to visual concepts and back
in proceedings of the ieee conference on computer vision and pattern recognition pages
yao fu yansong feng and john p
cunningham

paraphrase generation with latent bag of words
sebastian gehrmann falcon dai henry elder and alexander rush

end to end content and in plan selection for data to text generation
ceedings of the international conference on natural language generation pages tilburg university the netherlands
association for putational linguistics
sebastian gehrmann yuntian deng and alexander m rush

bottom up abstractive summarization
arxiv preprint

the aaai conference on articial intelligence ume pages
jiaxian guo sidi lu han cai weinan zhang yong yu and jun wang

long text generation via adversarial training with leaked information
arxiv preprint

qipeng guo xipeng qiu pengfei liu yunfan shao xiangyang xue and zheng zhang

transformer
in proceedings of the conference of the north american chapter of the association for computational linguistics human language technologies volume long and short papers pages minneapolis minnesota
ation for computational linguistics
aria haghighi and lucy vanderwende

ing content models for multi document tion
in proceedings of human language gies the annual conference of the north american chapter of the association for tional linguistics pages
xinyu hua and lu wang

sentence level tent planning and style specication for neural text generation
yacine jernite samuel r bowman and david tag

discourse based objectives for fast supervised sentence representation learning
arxiv preprint

anjuli kannan and oriol vinyals

ial evaluation of dialogue models
arxiv preprint

diederik p kingma and jimmy ba

adam a method for stochastic optimization
arxiv preprint

nikita kitaev ukasz kaiser and anselm levskaya

reformer the efcient transformer
arxiv preprint

chunyuan li xiang gao yuan li xiujun li baolin peng yizhe zhang and jianfeng gao

mus organizing sentences via pre trained modeling of a latent space
arxiv preprint

jiwei li michel galley chris brockett jianfeng gao and bill dolan

a diversity promoting tive function for neural conversation models
arxiv preprint

jiwei li michel galley chris brockett jianfeng gao and bill dolan

a diversity promoting jective function for neural conversation models
in proceedings of the conference of the north american chapter of the association for tional linguistics human language technologies pages san diego california
association for computational linguistics
jian guan yansen wang and minlie huang

story ending generation with incremental encoding in proceedings of and commonsense knowledge
jiwei li and dan jurafsky

neural net models for open domain discourse coherence
arxiv preprint

jiwei li minh thang luong and dan jurafsky

a hierarchical neural autoencoder for paragraphs and documents
arxiv preprint

jiwei li will monroe tianlin shi sebastien jean alan ritter and dan jurafsky

ial learning for neural dialogue generation
arxiv preprint

xiaoya li yuxian meng qinghong han fei wu and jiwei li

sac accelerating and structuring self attention via sparse adaptive connection
arxiv preprint

peter j liu mohammad saleh etienne pot ben goodrich ryan sepassi lukasz kaiser and noam shazeer

generating wikipedia by arxiv preprint summarizing long sequences


yang liu and mirella lapata

hierarchical transformers for multi document summarization
in proceedings of the association for computational linguistics pages florence italy
association for tational linguistics
the annual meeting of yang liu and mirella lapata

text tion with pretrained encoders
yinhan liu myle ott naman goyal jingfei du dar joshi danqi chen omer levy mike lewis luke zettlemoyer and veselin stoyanov

roberta a robustly optimized bert pretraining proach
arxiv preprint

minh thang luong hieu pham and christopher d manning

effective approaches to based neural machine translation
arxiv preprint

huanru henry mao bodhisattwa prasad majumder lian mcauley and garrison w cottrell

proving neural story generation by targeted common sense grounding
arxiv preprint

stephen merity caiming xiong james bradbury and richard socher

pointer sentinel mixture els
arxiv preprint

rada mihalcea and paul tarau

textrank ing order into text
in proceedings of the ference on empirical methods in natural language processing pages
ehsan montahaei danial alihosseini and mahdieh leymani baghshah

jointly measuring sity and quality in text generation models
arxiv preprint

amit moryossef yoav goldberg and ido dagan

step by step separating planning from realization in neural data to text generation
ramesh nallapati bowen zhou caglar gulcehre bing xiang al

abstractive text tion using sequence to sequence rnns and beyond
arxiv preprint

shashi narayan shay b cohen and mirella just the ata

do nt give me the details summary topic aware convolutional neural works for extreme summarization
arxiv preprint

gabriel pereyra george tucker jan chorowski ukasz kaiser and geoffrey hinton

izing neural networks by penalizing condent output distributions
arxiv preprint

matthew peters mark neumann mohit iyyer matt gardner christopher clark kenton lee and luke zettlemoyer

deep contextualized word in proceedings of the resentations
ence of the north american chapter of the ation for computational linguistics human guage technologies volume long papers pages new orleans louisiana
association for computational linguistics
ratish puduppully li dong and mirella lapata

data to text generation with content selection and planning
alec radford jeffrey wu rewon child david luan dario amodei and ilya sutskever

language models are unsupervised multitask learners
openai blog
j
ramos

using tf idf to determine word vance in document queries
pengjie ren zhumin chen zhaochun ren furu wei jun ma and maarten de rijke

leveraging contextual sentence relations for extractive rization using a neural attention model
in ings of the international acm sigir ence on research and development in information retrieval pages
lei sha lili mou tianyu liu pascal poupart sujian li baobao chang and zhifang sui

planning neural text generation from structured data
arxiv preprint

dinghan shen asli celikyilmaz yizhe zhang liqun chen xin wang jianfeng gao and lawrence carin

towards generating long and coherent text with multi level latent variable models
arxiv preprint

tianxiao shen victor quach regina barzilay and tommi jaakkola

blank language models
arxiv preprint

sainbayar sukhbaatar edouard grave piotr janowski and armand joulin

adaptive tention span in transformers
in proceedings of the annual meeting of the association for tational linguistics pages florence italy
association for computational linguistics
bowen tan zichao yang maruan ai shedivat eric p
xing and zhiting hu

progressive generation of long text
hierarchical recurrent neural networks
in ings of the ieee conference on computer vision and pattern recognition pages
manzil zaheer guru guruganesh avinava dubey joshua ainslie chris alberti santiago ontanon philip pham anirudh ravula qifan wang li yang et al

big bird transformers for longer quences
arxiv preprint

rowan zellers ari holtzman hannah rashkin yonatan bisk ali farhadi franziska roesner and yejin choi

defending against neural fake news
in advances in neural information ing systems pages
tianyi zhang varsha kishore felix wu kilian q weinberger and yoav artzi

bertscore arxiv preprint uating text generation with bert


ming zhong pengfei liu yiran chen danqing wang xipeng qiu and xuanjing huang

tive summarization as text matching
arxiv preprint

deyu zhou linsen guo and yulan he

ral storyline extraction model for storyline tion from news articles
in proceedings of the conference of the north american chapter of the association for computational linguistics human language technologies volume long papers pages new orleans louisiana
ation for computational linguistics
qingyu zhou nan yang furu wei shaohan huang ming zhou and tiejun zhao

neural ment summarization by jointly learning to score and select sentences
arxiv preprint

yukun zhu ryan kiros richard zemel ruslan salakhutdinov raquel urtasun antonio torralba and sanja fidler

aligning books and movies towards story like visual explanations by watching movies and reading books
yi tay dara bahri donald metzler da cheng juan zhe zhao and che zheng

synthesizer thinking self attention in transformer models
arxiv preprint

ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan n gomez ukasz kaiser and illia polosukhin

attention is all you need
in i
guyon u
v
luxburg s
bengio h
wallach r
fergus s
vishwanathan and r
nett editors advances in neural information cessing systems pages
curran ciates inc
ashwin k vijayakumar michael cogswell prasath r selvaraju qing sun stefan lee david crandall and dhruv batra

diverse beam search decoding diverse solutions from neural quence models
arxiv preprint

sam wiseman stuart m
shieber and alexander m
rush

challenges in data to document ation
sam wiseman stuart m
shieber and alexander m
rush

learning neural templates for text eration
ziang xie sida i wang jiwei li daniel levy aiming nie dan jurafsky and andrew y ng

data noising as smoothing in neural network language models
arxiv preprint

jingjing xu xuancheng ren yi zhang qi zeng xiaoyan cai and xu sun

a based model for promoting coherence among tences in narrative story generation
arxiv preprint

yu yan weizhen qi yeyun gong dayiheng liu nan duan jiusheng chen ruofei zhang and ming zhou

prophetnet predicting future n gram for sequence to sequence pre training
zhilin yang zihang dai yiming yang jaime bonell russ r salakhutdinov and quoc v le

xlnet generalized autoregressive pretraining for language understanding
in advances in neural formation processing systems pages
lili yao nanyun peng ralph weischedel kevin knight dongyan zhao and rui yan

and write towards better automatic storytelling
zihao ye qipeng guo quan gan xipeng qiu and zheng zhang

bp transformer modelling long range context via binary partitioning
arxiv preprint

haonan yu jiang wang zhiheng huang yi yang and wei xu

video paragraph captioning using throughout her career ayola has been outspoken on the subject of racial discrimination in the tainment industry
describing her motivation she states i am not an overtly political person
i just want fairness
ayola believes that black actors receive less recognition than their white counterparts explaining if youu get a show with six stars and one is black you are more likely to see interviews with the ve white actors
they are not being sold as a reason to watch
shh e believes that her career would have taken her in a different direction were she not of ethnic origin stating i could not have played any of the roles i have playee d on tv if i was white
i am very aware of where the glass ceiling is and it s still very low and expectations are still very low
she has noted having casting directors accept the notion of characters being both black and welsh to be a particular problem explaining that i get offered a lot of very different roles but tt hey re never welsh
the one time i was asked to play a welsh character on screen was in tiger bay for bbc wales but i know if that series had been called radyy r park or cyncoed close i wouldn t have been in it
in ayola founded a production company and directed a short lm entitled persephone s playground
she presented the lm at the cannes lm festival using it as part of her campaign for increased black representation in theatre lms and television
the project however was largely unsuccessful with ayola stating it just made me decide that if there s anything i don t want to do it s produce lms because i m rubbish
i was so bad with the budget that i just said yes to everything and then had too worry about how to pay for things at the end
in ayola offered her support to the action for southern africa campaign dignity period aiming to provide affordable sanitary protection to zimbabwean women
the hand over of command of military operations from interfet to untaet was completed on february
australia continued to support the un peacekeeping operation with between and personnel as well as landing craft and blackhawk helicopters and remained the largest contributor of personnel to the peacekeeping mission
during these operations australian forces regularly clashed with pro indonesian militia and on a number of occasions indonesian forces as well especially along the border with west timor
signicant actions occurred in unk mota unk and at unk in october
however with the security situation stabilised the bulk of the australian and un forces were withdrawn by
two australians died from non battle related causes while a number were wounded in action
the unexpected deployment to east timor in led to signicant changes in australian defence policy and to an enhancement of the adf s ability to conduct operations outside australia
this successful deployment was the rst time a large australian military force had operated outside of australia since the vietnam war and revealed shortcomings in the adf s ability to mount and sustain such operations
in response the defence white paper placed a greater emphasis on preparing the adf for overseas deployments
the australian government committed to improve the adf s capabilities by improving the readiness and equipment of adf units
in may adf personnel were again deployed to east timor as part of operation astute following unrest between elements of the timor leste defence force
australian forces were involved in a number skirmishes during this time including a heavy clash with rebels commanded by alfredo reinado at same on march
however by early the security situation had been stabilised and just australian personnel remained to train the local security forces as part of a small international force
following a drawdown the international stabilisation force commenced withdrawing from timor leste in november a process which was completed in april
in the spanish conquistador hernan cortes passed within a few kilometres of the ruins of tikal but did not mention them in his letters
after spanish friar andres avendano became lost in the peten forests in early he described a ruin that may well have been tikal
as is often the case with huge ancient ruins knowledge of the site was never completely lost in the region
it seems that local people never forgot about tikal and they guided guatemalan expeditions to the ruins in the
some or third hand accounts of tikal appeared in print starting in the century continuing through the writings of john lloyd stephens in the early century stephens and his illustrator frederick catherwood heard rumors of a lost city with white building tops towering above the jungle during their travels in the region
because of the site s remoteness from modern towns however no explorers visited tikal until modesto mendez and ambrosio tut respectively the commissioner and the governor of peten visited it in
artist eusebio lara accompanied them and their account was published in germany in
several other expeditions came to further investigate map and photograph tikal in the century including alfred p
maudslay in and the early century
pioneering archaeologists started to clear map and record the ruins in the
in a small airstrip was built at the ruins which previously could only be reached by several days travel through the jungle on foot or mule
in the tikal project began to map the city on a scale not previously seen in the maya area
from through major archaeological excavations were carried out by the university of pennsylvania tikal project
they mapped much of the site and excavated and restored many of the structures
excavations directed by edwin m
shook and later by william coe of the university investigated the north acropolis and the central plaza from to
the tikal project recorded over monuments at the site
in the guatemalan government began a further archeological project at tikal which continued through to
table examples of extracted summaries from the reconstruction method
extracted summaries are marked in purple

