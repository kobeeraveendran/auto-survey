n a j g l
s c v
v i x r a adversarial machine learning in text analysis and generation a preprint izzat alsmadi department of computing and cyber security texas san antonio san antonio tx
edu january abstract the research eld of adversarial machine learning witnessed a signicant interest in the last few years
a machine learner or model is secure if it can deliver main objectives with acceptable accuracy efciency
while at the same time it can resist different types attempts of adversarial attacks
this paper focuses on studying aspects and research trends in adversarial machine learning specically in text analysis and generation
the paper summarizes main research trends in the eld such as gan algorithms models types of attacks and defense against those attacks
keywords adversarial machine learning text generation generative adversarial networks gan introduction a basic generative adversarial network gan model includes two main modules a generator and a discriminator
the generator and discriminator are implicit function expressions usually implemented by deep neural networks creswell et al

applying gan in natural language processing nlp tasks such as text generation is challenging due to the discrete nature of the text
consequently it is not straightforward to pass the gradients through the discrete output words of the generator haidar and rezagholizadeh
as text input is discrete text generators model the problem as a sequential decision making process
in the model the state is the previously generated characters words or sentences
the action or prediction to make is the next character word sentence to be generated
the generative net is a stochastic policy that maps current state to a distribution over the action space
a generative adversarial network gan can create new data instances that resemble original training data
original gan described by goodfellow et al
has the following components and workow two nns a discriminator and a generator
the discriminator role is as of a simple classier that should distinguish real instances positive examples from the original training dataset from fake instances i
e
negative example created by the generator
the generator tries to fool the discriminator by synthesizing fake instances that resemble real ones
as training progresses the generator gets closer to producing instances that can fool the discriminator
if generator trained very well the discriminator gets worse at telling the difference between real and fake
generally speaking the generator module task is harder than that of the discriminator
for the least the discriminator job is binary but the generator job is much more complex
aml in text analysis and generation a preprint the two nns compete with two different goals
the goal of the discriminator is to discriminate between the real and the fake instances
the goal of the generator is to eventually learn more about the real instances or data and fool the discriminator
both are trained separately
each one assumes that the other module is xed at the time of cycle training to avoid dealing with a moving target that can be more complex
an accuracy of for the generator to generate fake instances indicates an accuracy of for the discriminator
as another sign of competition game between rivals the generator instances become negative training examples for the discriminator
the discriminator punishes the generator for producing incorrect instances and rewards it for producing correct instances
the generator evolves to make the discriminator punishes less and rewards more
in effect a good discriminator should not reveal enough information for the generator to make progress
in addition to discriminator and generator modules gans include the following components generator random input module a generator network that transforms the random input into a data instance generator loss that punishes the generator for failing to fool the discriminator back propagation module it adjusts weights by calculating the weight s impact on the output
discriminator generator conditional versus joint probability we can model the difference between the discriminator and the generator as the difference between conditional probability x and joint probability y in given a set of data instances x and a set of labels y
the conditional probability x is also known as the posterior probability for a
and denote the prior probability of events y and x respectively
the conditional entropy indicates how much extra information you still need to supply on average to communicate y given that the other party knows x
the joint entropy represents the amount of information needed to specify the value of two discrete random variables
there are many variations on how each one of those two components i
e
the generator and discriminator work or is trained to function
gans have a problem in text generation since the gradients from discriminator can not be passed to the generator explicitly
to deal with this issue gan based models e

seqgan yu et al
goal gan florensa et al
maligan che et al
leakgan guo et al
rankgan lin et al
maskgan fedus et al
xu et al
caccia et al
treat text generation as a sequential decision making process and utilize policy gradient williams to overcome this difculty
the score predicted by a discriminator is used as the reinforcement to train the generator yielding a hybrid model of gan and rl
other models utilize rl agents to control gans
rl agent forms a decision making network that interacts with the environment by taking available actions and collects rewards
as a scalability limitation an agent that is trained using rl is only capable of achieving the single task that is specied via its reward function

gans methods for training and back propagation applying gan in text analysis is challenging as text is discrete
consequently their is a need to pass the gradients through the discrete output words of the generator haidar and rezagholizadeh
but how do generator and discriminator modules train themselves to improve the generator module can train itself using self auto or variational encoders
the code in the decoder can randomly take values to produce different outputs
the goal of the self encoder is to make the reconstruction error smaller and smaller
the generator uses back propagation from the discriminator to improve its future instances and update model weights
the discriminator can also train itself learn a discriminator process that can distinguish an instance label as real or generated
it is trained based on real instances from the original dataset and fake instances from the generator

gan loss functions gan uses loss functions that evaluate the distance between the distribution of the data that is generated by the gan and the distribution of the real data
a gan can have two loss functions for generator and discriminator training
minimax loss the generator tries to minimize the loss function while the discriminator tries to maximize it goodfellow et al aml in text analysis and generation a preprint wasserstein loss the default loss function for tf gan estimators arjovsky et al

in those gans the discriminator will not try to make a binary decision whether an instance is real or fake but to provide a value between zero and one
a threshold can be decided in this range between values for fake instances versus values for real instances
character word sentence level attacks while most of aml research publications demonstrate on picture based datasets a growing recent trend is the applications of aml in text analysis or nlp
in one taxonomy aml in nlp can be divided into the following attack levels character level attacks
those involve different possible types of character level manipulations such as swap substitution deletion insertion repeating one hot character embedding and visual character embedding hosseini et al
zhang et al
textbugger li et al
belinkov and bisk gao et al
hotip ebrahimi et al
brown et al
pruthi et al
eger et al
le et al

while character level attacks are simple it is easy to defend against when deploying a spell check and proofread algorithms
word level attacks similar to character level attacks approaches to word level attacks through manipulation include word embedding language models lter words through synonyms
substitutes
e

ebrahimi et al
ebrahimi et al
kuleshov et al
yang et al
et al
wallace et al
gao et al
garg and ramakrishnan et al
ribeiro et al
et al
alzantot et al
li et al
li et al
ribeiro et al
wang and zhang wang et al

the search algorithms include gradient descent genetic algorithms saliency based greedy algorithm sampling papernot et al
sato et al
gong et al
alzantot et al
zhou et al
liang et al
ren et al
jin et al

in comparison with character level attacks the attacks created by word level approaches are more ble for humans and more difcult for machine learning algorithms to defend
sentence level attacks
those attacks are usually based on text paraphrasing demand longer time in adversary text generation
examples of research publications in sentence level include jia and liang iyyer et al
cheng et al
michel et al
lei et al
zheng et al
jethanandani and tang
hybrid or multi level attacks attacks which can use a combination of character word and sentence level approaches hotflip ebrahimi et al
blohm et al
wallace et al

sequence generative text generator generative models approaches natural language generation nlg techniques allow the generation of natural language text based on a given context
nlg can involve text generation based on predened grammar such as the dada engine baki et al
or leverage deep learning neural networks such as rnn yao et al
for generating text
we will describe some of the popular approaches that can be found in relevant literature in the scope of aml
classical training language models with teacher professor forcing teacher forcing is common approach to training rnns in order to maximize the likelihood of each token from the target sequences given previous tokens in the same sequence williams and zipser
in each time step s of training the model is evaluated based on the likelihood of the target t given a groundtruth sequence
teacher forcing is used for training the generator which means that the decoder is exposed to the previous groundtruth token
rnns trained by teacher forcing should be able to model a distribution that matches the target where the joint distribution is modeled properly if rnn models prediction of future steps
created error when using the model is propagated over each next or following step resulting in low performance
a solution to this is training the model using professor forcing lamb et al

in professor forcing rnn should give the same results when a ground truth is given as input when training teacher forcing as when the output is looped back into the next step
this can be forced by training a discriminator that classies wether the output is created with a teacher forced model or with a free running model aml in text analysis and generation a preprint conventional inference maximum likelihood estimation mle mle is conducted on real data samples and the parameters are updated directly according to the data samples
this may lead to an overly smooth generative model
the goal is to select the distribution that maximizes the likelihood of generating the data
for practical sample scenarios mle is prone to exposure bias issues on the training set
additionally during the inference or generation stage the error at each time step will accumulate through the sentence generation process ranzato et al

the following methods utilize mle hidden markov model hmm a hidden markov model hmm is a probability graph model that can depict the transition laws of hidden states and mine the intentional features of data to model the observable variables
the foundation of an hmm is a markov chain which can be represented by a special weighted nite state automaton
the majority of generative models require the utilization of markov chains goodfellow et al
creswell et al

the observable sequence in hmm is the participle of the given sentence in the part of speech pos tag while the hidden state is the different pos
method of moments the method of moments mom or method of learned moments is an early principle of learning pearson
there are situations in which mom is preferable to mle
one is when mle is more computationally challenging than mom ravuri et al
in the generalized method of moments gmm in addition to the data and the distribution class a set of relevant feature functions is given over the instance space hansen rabiner
other research contributions in aml mom or moment matching include salimans al
mroueh and sercu lewis and syrgkanis bennett et al

restricted boltzmann machine rbm restricted boltzmann machine rbm is a two layer neural network consisting of a visible layer and a hidden layer hinton
it is an important generative model that is capable of learning representations from data
generative models have evolved from rbm based models such as helmholtz machines hms fodor et al
and deep belief nets dbn hinton et al
to variational auto encoders vaes kingma and welling and generative adversarial networks gans cooperative training method in cooperative training method ctm a language model is trained online to offer a target distribution for minimizing the divergence between the real data distribution and the generated distribution xie et al
yin et al

rl based versus rl free text generation gan models were originally developed for learning from a continuous not discrete distribution
however the discrete nature of text input handicaps the use of gans in gans a reinforcement learning algorithm is used for policy gradient to get an unbiased gradient estimator for the generator and obtain the reward from the discriminator chen et al

rl based generation reinforcement learning rl is a technique that can be used to train an agent to perform certain tasks
due to its generality reinforcement learning is studied in many disciplines
gan models that use a discriminating module to guide the training of the generative module as a reinforcement learning policy has shown promising results in text generation guo et al

various methods have been proposed in text generation via gan e

lin et al
rajeswar et al
che et al
yu et al
che et al

there are several models of rl some of which were applied to sentence generation e

actor critic algorithm and deep q network e

sutton et al
guo bahdanau et al

one optimization challenge with rl based approaches is that they may yield high variance gradient estimates maddison et al
zhang et al

rl free gans for text generation examples of models that use an alternative to rl latent space based solutions continuous approximation of discrete sampling those models apply a simple soft argmax operator or gumbel softmax trick to provide a continuous approximation of the discrete distribution on text
examples of research efforts in this category include zhang et al
and gumbelsoftmax gan gsgan kusner and hernndez lobato jang et al
maddison et al
fm gan chen et al
gsgan kusner and hernndez lobato and relgan nie et al

aml in text analysis and generation a preprint
long versus short text generation literature in this area differentiates between the generation of short texts e

less than words and the generation of long text
applications for each one can be different from the other
the majority of publications focus on short text generation as it seems to be less challenging
different challenges are discussed in the literature specially in long text generation
for example one of the unique challenges for long text generation is the sparse reward issue in which a scalar guiding signal is only available after an entire sequence has been generated
vezhnevets et al
guo et al
sutton et al

the main disadvantage of sparse reward problem is making the training sample inefcient tuan and lee
model based rls have been proposed recently to solve problems with extremely sparse rewards pathak et al


supervised versus unsupervised text generation majority of work in this area falls in the supervised category e

robin ishii et al
bahdanau et al
bengio et al
vinyals and le wiseman et al
bhowmik and de melo puduppully et al

as a supervised problem in a particular sentence the terms words in the sentence can be seen as the input features while the next term feature is the target
examples of publications that fall in the unsupervised text generation include graves yu et al
zhang et al
hu et al
schmitt et al

unsupervised text can be generated from explainable latent topics wang et al
structured data schmitt et al
sheffer et al
or knowledge graphs kgs bhowmik and de melo koncel kedziorski et al
schmitt et al
jin et al

machine learning algorithms for text generation using rnn lstm versus gru bidirectionalrnn for text generation state of the art text generation models are based on recurrent neural networks rnns
several papers discussed using different deep learning rnn algorithms such as those mentioned above in automatic text generation e

kiddon et al
hu et al
abdelwahab and elmaghraby lu et al
nie et al
zhu et al
wang et al
mangal et al
moila and modipa mangal et al

unlike traditional methods rnn based approaches rely on data driven without manual intervention and emphasize on end to end encoder decoder structure
different performance metrics and methods are used to evaluate the output of the process such as log likelihood loss function overall processing time
the loss function that is used when training the model is the negative log likelihood or the negative log probability on the target sequence
lstm shows to be a very good model in several aspects in comparison with the other evaluated models mei et al
and wan mangal et al

many recent gans for text generation such as kusner and hernndez lobato yu et al
guo et al
lin et al
and fedus et al
are using lstm
some papers such as sutskever et al
and pouget abadie et al
have shown that standard lstm decoder does not perform well in generating long text sequences
template based rule based versus neural text generation classical approaches to text generation include template based rule based n gram based and log linear based models
rule based techniques are grammar based methods with structured rules written based on accumulated knowledge
template based approaches can be as simple as replacing words of users choices by their synonyms reiter deemter et al
wiseman et al
peng et al

n gram models are widely used in nlp tasks such as text generation
in n gram approach the last word of the n gram i
e
to be predicted can be inferred from the other words that already appear in the same n gram de novais et al

beam search and greedy search two popular deterministic decoding approaches are beem search and greedy search sutskever et al
aml in text analysis and generation a preprint bahdanau et al
zheng et al

beam search maintains a xed size set of partially decoded sequences
beam search is a common search strategy to improve results for several tasks such as text generation machine translation and dependency parsing
greedy search selects the highest probability token at each time step
greedy search can be seen as a special case of beam search
sequence to sequence models and knowledge enhancement methods seq to seq models are common architectures for text generation tasks where both the input and the output are modeled as sequences of tokens
in other words the model convert an input sequence into an output sequence
more specically the rst model encodes the input sequence as a set of vector representations using a recurrent neural network rnn
the second rnn then decodes the output sequence step by step
seq to seq models are commonly trained via maximum likelihood estimation mle chen et al

one challenge with seq to seq models is that the input text alone often does not provide enough knowledge to generate the desired output which will impact the quality of the generated output
several methods are proposed to enhance model knowledge beyond input text such as attention memory linguistic features graphs pre trained language models and multi task learning
many of those techniques are listed in yu et al
and
com kenlg reading
one of those particular enhancement techniques is attention bahdanau et al
in which an encoder compresses the input text and a decoder with an attention mechanism generates output target
the decoder is bound to generate a sequence of text tokens
recursive transition network rtn the authors in baki et al
discuss using a recursive transition network woods for generating fake content similar in nature to legitimate content
rtn is used to detect simplication constructs
nodes of the graph are labeled and arcs may be labeled with either node names or terminal symbols
rnns are essentially equivalent to an extension of context free grammars in which regular expressions are allowed on the right side of productions
relational memory the basic idea of relational memory is to consider a xed set of memory slots and allow for interactions between memory slots through using self attention mechanisms vaswani et al

rm is proposed to record key information of the generation process for example record the information from previous generation processes
the goal is to enhance the text generation process through such learning memory as well as patterns for long text generation
such rl can provide a stateful rather than stateless text generation process
self attention is also used between the memory slots to enable interaction between them and facilitate long term dependency modeling vaswani et al

several relational based text generations that showed better ability of modeling longer range dependencies are described in literature santoro et al
relgan nie et al

google lm released by google google lm is a language pre trained model that is trained on a billion word corpus a publicly available dataset containing mainly news data jozefowicz et al
chelba et al

it is based on a two layer lstm with units in each layer garbacea et al

scheduled sampling ss ss is proposed to bridge the gap between training and inference for sequence prediction tasks
it is used to avoid exposure bias in seq to seq generation bengio et al
mihaylova and martins
during the inference process of seq to seq generation true previous target tokens are unavailable
as a result they are thus replaced by tokens generated by the model itself which may yield a discrepancy between how the model is used at training and inference bengio et al

one limitation with scheduled sampling is that target sequences can be incorrect in some steps since they are randomly selected from the ground truth data regardless of how input was chosen zheng et al
ranzato et al
ranzato et al

generating text with gans gans are implicit generative or language models lms learned via a competition between a generator network and a discriminator network
the discriminator distinguishes uniquely gans from other lms
particularly for our subject aml adversarial training with the discriminator is used in gans as opposed to training based on solely maximum likelihood and categorical cross entropy in other lms
the conventional lms are not trained in a adversarial manner
unlike traditional approaches e

teacher forcing ss gans do not suffer from exposure bias rajeswar et al
tevet et al

exposure bias occurs when models are fed with their predicted data rather than the ground truth data at inference time
this causes generating poor samples due to the accumulated error yin et al

aml in text analysis and generation a preprint adversarial training techniques adversarial training is a method to help systems be more robust against adversarial attacks
below are examples of some adversarial training techniques reported in literature
fast gradient sign method fgsm fgsm is used to add adversarial examples to the training process goodfellow et al
wong et al

during training part of the original samples is replaced with its corresponding adversarial samples generated using the model being trained
kurakin et al
suggested to use iterative fgsm ifgsm fgsm ll or fgsm rand variants for adversarial training in order to reduce the effect of label leaking kurakin et al

their are also other variants of fgsm such as momentum iterative fast gradient sign method mi fgsm dong et al

pgd based training proposed by madry et al

at each iteration all the original samples are replaced with their corresponding adversarial samples generated using the model being trained
pgd was enhanced using different efforts such as optimization tricks such as momentum to improve adversary dong et al
combination with other heuristic defenses such as matrix estimation yang et al
defensive quantization lin et al
logit pairing mosbach et al
kannan et al
thermometer encoding buckman et al
feature denoising xie et al
robust manifold defense jalal et al
nonexpansive nets qian et al
jacobian regularization jakubovitz and giryes universal perturbation shafahi et al
stochastic activation pruning dhillon et al
as of today training with a pgd adversary remains empirically robust wong et al
jacobian based saliency map approach jsma jsma is a gradient based white box method that is proposed to use the gradient of loss with each class labels with respect to every component of the input papernot et al

jsma is useful for targeted miss classication attacks chakraborty et al

accelerating adversarial training the cost of adversarial training can be reduced by reusing adversarial examples and merging the inner loop of a pgd and gradient updates of the model parameters shafahi et al
zhang et al
dawnbench competition some submission projects to dawnbench competition have shown good performance results on and imagenet classiers in comparison with research reported training methods coleman et al
wong et al

text generation models tasks applications text generation refers to the process of automatic or programmable generation of text with no or least of human intervention
the sources utilized for such generation process can also vary based on the nature of the application
the types of applications from generating text in particular are growing
we will discuss just a few in this section

next word prediction for many applications that we use through our smart phones or websites next word prediction nwp also called auto completion is a typical nlp application
from a machine learning perspective nwp is a classical prediction problem where previous and current text can be the pool to extract the prediction model features and other parameters and the next word to predict is the target feature
different algorithms are proposed to approach nwp problem such as term frequencies articial intelligence n grams neural networks
aml in text analysis and generation a preprint
dialog generation human machine dialog generation prediction is an essential topic of research in the eld of nlp
it has many different applications in different domains
the quality and the performance of the process can widely vary based on available resources training pre training and also efciency
neural networks have demonstrated impressive results on dialog generation vinyals and le chang et al

gans are used in dialogue generations in several research publications e

li et al
hamilton et al
kannan et al
nabeel et al

neural machine translation neural machine translation nmt is a learning approach for automated translation with potentials to overcome weaknesses of classical phrase based translation systems or statistical machine learning
the main difference is that nmt is based on a model not based on some patterns
nmt tries to replicate the functions of the human brain and assess content from various sources before generating output
further enhancements on nmt were achieved using attention based neural machine translation
one of the popular early open source nmts is systran
systran
the rst nmt engine launched in
other examples include those of google translate facebook e bay and microsoft
adversarial nmt is introduced in which training of the nmt model is assisted by an adversary an elaborately designed convolutional neural network cnn yang et al
wu et al
zhang et al
shetty et al

text generation metrics one of the key issues in text generation is that there is no widely agreed upon automated metric for evaluating the text generated output
text generation metrics can be classied based on several categories
here is a summary of categories and metrics document similarity based metrics one of the popular approaches to measure output tg is through comparing it with some source documents or human natural language
some of the popular metrics in this category are bilingual evaluation understudy bleu papineni et al
and embedding similarity embsim zhu et al

bleu has several variants such as and
this category can also include some of the popular classical metrics such as okapi robertson and walker word mover s distance wmd kusner et al
cosine dice and jaccard measures in addition to term frequency inverse document frequency tf idf
likelihood based metrics log likelihood is the negative of the training loss function nll
nll also known as multiclass entropy outputs a probability for each class rather than just the most likely class
the typical approach in text generation is to train the model using a neural network performing maximum likelihood estimation mle by minimizing the negative log likelihood nll over the text corpus
for gans in the standard gan objective the goal or objective function is to minimize nll for the binary classication task goodfellow et al

maximum likelihood suffers from predicting most probable answers
this means that a model trained with maximum likelihood will tend to output short general answers that are very common in the vocabulary
the log likelihood improves with more dimensions as it is easier to t the hypotheses in the training step having more dimensions
consequently the hypothesis in the generating step have lower log likelihood
perplexity perplexity measures a model s certainty of its predictions
there are several advantages to using perplexity keukeleire calculating perplexity is simple and does nt require human interference it is easy to interpret i is easy to optimize a model for an improved perplexity score held out likelihood is usually presented as perplexity which is a deterministic transformation of the likelihood into an information theoretic quantity inception score is is rewards high condence class labels for each generated instance salimans et al

is can provide a aml in text analysis and generation a preprint general evaluation of gans trained on imagenet
however it has limited utility in other settings fowl et al

frechet inception distance fid fid is used to measure the distance vaserstein between two gaussians whose means and covariances are taken from embedding both real and generated data heusel et al
cfka et al

fid assumes that the training data is sufcient and does not reward producing more diversity than the training data fowl et al

n gram based metrics distinct n is a measure of diversity that computes the number of distinct n grams normalized by the number of all ngrams li et al

sentence similarity metrics sentencebert sent bert reimers and gurevych rouge metrics were mostly used for text generation video captioning and summarization tasks lin
they were introduced in as a set of metrics to evaluate machine generated text summaries
rouge has several variants such as and rouge l
rouge metrics meteor meteor metric for evaluation of translation with explicit ordering was proposed in banerjee and lavie
meteor metric was mainly used for text generation image and video captioning and question answering tasks embedding based metrics the main approach is to to embed generated sentences in latent space and then evaluate them in this space tevet et al

du and black suggest to cluster the embedded sentences with k means and then use its inertia as a measure for diversity
other less common metrics such as gleu score edit distance phoneme and diacritic error rate
metrics for gans traditional probability based lm metrics tevet et al
several papers indicated the need to use new metrics to evaluate gans e

esteban et al
zhu et al
and cao
some of the metrics proposed for gans include divergence based metrics such as f gan nowozin et al
ls gan mao et al
divergence koochali et al
and self bleu zhu et al
integral probability metrics such as wasserstein gan wga arjovsky et al
gulrajani et al

domain specic metrics e

attack success rate gao et al

random network distillation rnd burda et al

text generation datasets there are many datasets for the general tasks research of nlp such as those mentioned in the following links
org aclwiki
com task data to text generation awesome
org tokenmill awesome
com niderhoff nlp datasets
ai datasets the datasets for natural language
com datasets natural language
kdnuggets
com tag datasets we will list few datasets benchmarks that are used in gan research papers in particular coco image captions chen et al
and wmt
org translation task
html guo et al
weibodial qian et al
aml in text analysis and generation a preprint chinese poems a dataset proposed by zhang and lapata and used by other related work such as yu al
lin et al
rajeswar et al

cub captions wah et al
dada engine to create phishing emails baki et al
memory based models rnn versus lstm as we mentioned earlier vanilla rnns do not perform well when the learning sequences have long term temporal dependence due to issues such as exploding gradients bengio et al

alternatively convolutional neural networks cnns recurrent neural networks rnns gated recurrent unit gru and long short term memory lstm models are effective approaches in the eld of sequential modeling methods
the design of the forget gate is the essence of these models sun et al

an lstm model is a type of rnn that can remember relevant information longer than a regular rnn
as a result they can better learn long term patterns olah
lstm models provide a mechanism that is able to both store and discard the information saved about the previous steps limiting the accumulated error using constant error carousels hochreiter and schmidhuber manzelli et al

defense against nlp adversarial attacks generating adversarial attacks on text has shown to be more challenging than for images and audios due to their discrete nature
variations on original text can be applied on different levels character word or sentence levels
recent relevant studies showed examples of nlp vulnerabilities such as zhou et al
reading comprehension jia and liang
text classication alzantot et al
liang et al
wong machine translation cheng et al
ebrahimi et al
dialogue systems cheng et al
dependency parsing zheng et al


black versus white box attacks current adversarial attacks can be roughly be divided into three categories white box attacks black box and gray box attacks according to whether the data model architecture and parameters of the target are accessible
in black box attacks also called zero knowledge attack no or very limited information about the target model is accessible
for example a certain number of model queries i
e
oracle queries are granted
some of the defenses guo et al
xie et al
are shown to be quite robust against black box attacks
in gray box attacks limited knowledge attacks partial knowledge about the model under attack e

type of features or type of training data is assumed
on the other side is white box perfect knowledge attacks
those are attacks that exploit model internal information
they assume complete knowledge of the targeted model including its parameter values architecture training method and in some cases its training data
table shows samples of research publications in all three categories
there are some papers that are identied to more than one category
at the high level there are three classes or dimensions of attacks barreno et al
causative versus exploratory in causative attacks the training process is altered and models are trained with adversary datasets
in exploratory attacks attacker tries to exploit the existing weaknesses integrity versus availability false negatives versus false positive
targeted at a particular input or indiscriminate in which input fails reactive versus proactive a reactive defense is where one waits to be attacked and detects an adversarial example
on the other hand proactive attacks involve training the model to be more resilient against adversarial examples of defense against specic attacks with focus on nlp aml in text analysis and generation a preprint dirichlet neighborhood ensemble dne a randomized smoothing method for training a model against substitution based attacks zhou et al
adversarial training as a defense method miyato et al
sato et al
zhu et al

increasing the model robustness by adding perturbations on word embedding goodfellow et al
certied defenses some certied defenses were proposed in literature in order to provide guarantees of robustness to some specic types of attacks huang et al
jia and liang
defensive distillation defensive distillation can take an arbitrary nn and increase its robustness reducing the success rate of attacks ability carlini and wagner defense through randomization cohen et al
liu et al
summary and conclusion in this paper recent literature in adversarial machine learning for text generation tasks is summarized
our goal is to present a one stop source for researchers and interested readers to learn the basic components and research trends in this eld
we noticed a continuous expansion in the applications models and algorithms
this paper can serve as an introduction to this eld and readers may need to follow through some of the researchers and references we referred to based on their focuses or interests
references antonia creswell tom white vincent dumoulin kai arulkumaran biswa sengupta and anil a bharath
generative adversarial networks an overview
ieee signal processing magazine
md akmal haidar and mehdi rezagholizadeh
textkd gan text generation using knowledge distillation and generative adversarial networks
in canadian conference on articial intelligence pages
springer
ian j goodfellow jonathon shlens and christian szegedy
explaining and harnessing adversarial examples
arxiv preprint

lantao yu weinan zhang jun wang and yong yu seqgan
sequence generative adversarial nets with policy gradient
arxiv e prints page
arxiv preprint

carlos florensa david held xinyang geng and pieter abbeel
automatic goal generation for reinforcement learning agents
in international conference on machine learning pages
pmlr
tong che yanran li ruixiang zhang r devon hjelm wenjie li yangqiu song and yoshua bengio
likelihood augmented discrete generative adversarial networks
arxiv preprint

jiaxian guo sidi lu han cai weinan zhang yong yu and jun wang
long text generation via adversarial training with leaked information
in proceedings of the aaai conference on articial intelligence volume
kevin lin dianqi li xiaodong he zhengyou zhang and ming ting sun
adversarial ranking for language generation
in advances in neural information processing systems pages
william fedus ian goodfellow and andrew m dai
maskgan better text generation via lling in the
arxiv preprint

jingjing xu xuancheng ren junyang lin and xu sun
diversity promoting gan a cross entropy based generative adversarial network for diversied text generation
in proceedings of the conference on empirical methods in natural language processing pages
massimo caccia lucas caccia william fedus hugo larochelle joelle pineau and laurent charlin
language gans falling short
arxiv preprint

ronald j williams
simple statistical gradient following algorithms for connectionist reinforcement learning
machine learning
martin arjovsky soumith chintala and lon bottou
wasserstein gan
arxiv preprint

hossein hosseini sreeram kannan baosen zhang and radha poovendran
deceiving google s perspective api built for detecting toxic comments
arxiv preprint

xiang zhang junbo zhao and yann lecun
character level convolutional networks for text classication
advances in neural information processing systems
jinfeng li shouling ji tianyu du bo li and ting wang
textbugger generating adversarial text against real world applications
arxiv preprint

aml in text analysis and generation a preprint yonatan belinkov and yonatan bisk
synthetic and natural noise both break neural machine translation
arxiv preprint

ji gao jack lanchantin mary lou soffa and yanjun qi
black box generation of adversarial text sequences to evade deep learning classiers
in ieee security and privacy workshops spw pages
ieee
javid ebrahimi anyi rao daniel lowd and dejing dou
hotip white box adversarial examples for text classication
arxiv preprint

stephan brown petar milkov sameep patel yi zen looi ziqian dong huanying gu n sertac artan and edwin jain
acoustic and visual approaches to adversarial text generation for google perspective
in international conference on computational science and computational intelligence csci pages
ieee
danish pruthi bhuwan dhingra and zachary c lipton
combating adversarial misspellings with robust word recognition
arxiv preprint

steffen eger gzde gl sahin andreas rckl ji ung lee claudia schulz mohsen mesgar krishnkant swarnkar edwin simpson and iryna gurevych
text processing like humans do visually attacking and shielding nlp systems
arxiv preprint

thai le suhang wang and dongwon lee
malcom generating malicious comments to attack neural fake news detection models
arxiv preprint

javid ebrahimi daniel lowd and dejing dou
on adversarial examples for character level neural machine translation
arxiv preprint

classication problems

volodymyr kuleshov shantanu thakoor tingfung lau and stefano ermon
adversarial examples for natural language puyudi yang jianbo chen cho jui hsieh jane ling wang and michael i jordan
greedy attack and gumbel attack generating adversarial examples for discrete data
journal of machine learning research
di jin zhijing jin joey tianyi zhou and peter szolovits
is bert really robust a strong baseline for natural language in proceedings of the aaai conference on articial intelligence attack on text classication and entailment
volume pages
eric wallace shi feng nikhil kandpal matt gardner and sameer singh
universal adversarial triggers for attacking and analyzing nlp
arxiv preprint

siddhant garg and goutham ramakrishnan
bae bert based adversarial examples for text classication
arxiv preprint

yichao zhou jyun yu jiang kai wei chang and wei wang
learning to discriminate perturbations for blocking adversarial attacks in text classication
arxiv preprint

marco tulio ribeiro sameer singh and carlos guestrin
semantically equivalent adversarial rules for debugging nlp models
in proceedings of the annual meeting of the association for computational linguistics volume long papers pages
yuan zang fanchao qi chenghao yang zhiyuan liu meng zhang qun liu and maosong sun
word level textual adversarial attacking as combinatorial optimization
in proceedings of the annual meeting of the association for computational linguistics pages
moustafa alzantot yash sharma supriyo chakraborty huan zhang cho jui hsieh and mani b srivastava
genattack in proceedings of the genetic and evolutionary practical black box attacks with gradient free optimization
computation conference pages
jiwei li will monroe and dan jurafsky
understanding neural networks through representation erasure
arxiv preprint

jianyu wang and haichao zhang
bilateral adversarial training towards fast training of more robust models against adversarial attacks
in proceedings of the ieee international conference on computer vision pages
yajie wang haoran lv xiaohui kuang gang zhao yu an tan quanxin zhang and jingjing hu
towards a physical world adversarial patch for blinding object detection models
information sciences
nicolas papernot patrick mcdaniel xi wu somesh jha and ananthram swami
distillation as a defense to adversarial perturbations against deep neural networks
in ieee symposium on security and privacy sp pages
ieee
motoki sato jun suzuki hiroyuki shindo and yuji matsumoto
embedding space for text
arxiv preprint

interpretable adversarial perturbation in input aml in text analysis and generation a preprint zhitao gong wenlu wang bo li dawn song and wei shinn ku
adversarial texts with gradient methods
arxiv preprint

moustafa alzantot yash sharma ahmed elgohary bo jhang ho mani srivastava and kai wei chang
generating natural language adversarial examples
arxiv preprint

bin liang hongcheng li miaoqiang su pan bian xirong li and wenchang shi
deep text classication can be fooled
arxiv preprint

shuhuai ren yihe deng kun he and wanxiang che
generating natural language adversarial examples through probability weighted word saliency
in proceedings of the annual meeting of the association for computational linguistics pages
robin jia and percy liang
adversarial examples for evaluating reading comprehension systems
arxiv preprint

mohit iyyer john wieting kevin gimpel and luke zettlemoyer
adversarial example generation with syntactically controlled paraphrase networks
arxiv preprint

yong cheng lu jiang and wolfgang macherey
robust neural machine translation with doubly adversarial inputs
arxiv preprint

paul michel xian li graham neubig and juan miguel pino
on evaluation of adversarial perturbations for to sequence models
arxiv preprint

qi lei lingfei wu pin yu chen alexandros g dimakis inderjit s dhillon and michael witbrock
discrete adversarial attacks and submodular optimization with applications to text classication
arxiv preprint

haizhong zheng ziqi zhang honglak lee and atul prakash
understanding and diagnosing vulnerability under adversarial attacks
arxiv preprint

mahir jethanandani and derek tang
adversarial attacks against lipnet end to end sentence level lipreading
in ieee security and privacy workshops spw pages
ieee
matthias blohm glorianna jagfeld ekta sood xiang yu and ngoc thang vu
comparing attention based tional and recurrent neural networks success and limitations in machine reading comprehension
arxiv preprint

shahryar baki rakesh verma arjun mukherjee and omprakash gnawali
scaling and effectiveness of email masquerade attacks exploiting natural language generation
in proceedings of the acm on asia conference on computer and communications security pages
yuanshun yao bimal viswanath jenna cryan haitao zheng and ben y zhao
automated crowdturng attacks and defenses in online review systems
in proceedings of the acm sigsac conference on computer and communications security pages
ronald j williams and david zipser
a learning algorithm for continually running fully recurrent neural networks
neural computation
alex m lamb anirudh goyal alias parth goyal ying zhang saizheng zhang aaron c courville and yoshua bengio
professor forcing a new algorithm for training recurrent networks
in advances in neural information processing systems pages
marcaurelio ranzato sumit chopra michael auli and wojciech zaremba
sequence level training with recurrent neural networks
arxiv preprint

ian goodfellow jean pouget abadie mehdi mirza bing xu david warde farley sherjil ozair aaron courville and yoshua bengio
generative adversarial networks
communications of the acm
karl pearson
asymmetrical frequency curves
nature
lars peter hansen
large sample properties of generalized method of moments estimators
econometrica journal of the econometric society pages
lawrence r rabiner
a tutorial on hidden markov models and selected applications in speech recognition
proceedings of the ieee
tim salimans ian goodfellow wojciech zaremba vicki cheung alec radford and xi chen
improved techniques for training gans
arxiv preprint

youssef mroueh and tom sercu
fisher gan
in advances in neural information processing systems pages
aml in text analysis and generation a preprint greg lewis and vasilis syrgkanis
adversarial generalized method of moments
arxiv preprint

andrew bennett nathan kallus and tobias schnabel
deep generalized method of moments for instrumental variable analysis
in advances in neural information processing systems pages
jerry a fodor zenon w pylyshyn al
connectionism and cognitive architecture a critical analysis
cognition
computation
geoffrey e hinton simon osindero and yee whye teh
a fast learning algorithm for deep belief nets
neural diederik p kingma and max welling
auto encoding variational bayes
arxiv preprint

cihang xie jianyu wang zhishuai zhang zhou ren and alan yuille
mitigating adversarial effects through randomization
arxiv preprint

haiyan yin dingcheng li xu li and ping li
meta cotgan a meta cooperative training paradigm for improving adversarial text generation
in aaai pages
liqun chen shuyang dai chenyang tao haichao zhang zhe gan dinghan shen yizhe zhang guoyin wang ruiyi zhang and lawrence carin
adversarial text generation via feature mover s distance
in advances in neural information processing systems pages
sai rajeswar sandeep subramanian francis dutil christopher pal and aaron courville
adversarial generation of natural language
arxiv preprint

lantao yu weinan zhang jun wang and yong yu
seqgan sequence generative adversarial nets with policy gradient
in proceedings of the aaai conference on articial intelligence volume
richard s sutton david a mcallester satinder p singh and yishay mansour
policy gradient methods for ment learning with function approximation
in advances in neural information processing systems pages
hongyu guo
generating text with deep reinforcement learning
arxiv preprint

dzmitry bahdanau philemon brakel kelvin xu anirudh goyal ryan lowe joelle pineau aaron courville and yoshua bengio
an actor critic algorithm for sequence prediction
arxiv preprint

chris j maddison andriy mnih and yee whye teh
the concrete distribution a continuous relaxation of discrete random variables
arxiv preprint

yizhe zhang zhe gan kai fan zhi chen ricardo henao dinghan shen and lawrence carin
adversarial feature matching for text generation
arxiv preprint

matt j kusner and jos miguel hernndez lobato
gans for sequences of discrete elements with the gumbel softmax distribution
arxiv preprint

eric jang shixiang gu and ben poole
categorical reparameterization with gumbel softmax
arxiv preprint

weili nie nina narodytska and ankit patel
relgan relational generative adversarial networks for text generation
in international conference on learning representations
alexander sasha vezhnevets simon osindero tom schaul nicolas heess max jaderberg david silver and koray kavukcuoglu
feudal networks for hierarchical reinforcement learning
arxiv preprint

yi lin tuan and hung yi lee
improving conditional sequence generative adversarial networks by stepwise evaluation
ieee acm transactions on audio speech and language processing
deepak pathak pulkit agrawal alexei a efros and trevor darrell
curiosity driven exploration by self supervised prediction
in proceedings of the ieee conference on computer vision and pattern recognition workshops pages
jacques robin
revision based generation of natural language summaries providing historical background corpus based analysis design implementation and evaluation

kumiko tanaka ishii kiti hasida and itsuki noda
reactive content selection in the generation of real time soccer commentary
in coling volume the international conference on computational linguistics
samy bengio oriol vinyals navdeep jaitly and noam shazeer
scheduled sampling for sequence prediction with recurrent neural networks
advances in neural information processing systems
oriol vinyals and quoc le
a neural conversational model
arxiv preprint

aml in text analysis and generation a preprint sam wiseman stuart m shieber and alexander m rush
challenges in data to document generation
arxiv preprint

preprint

rajarshi bhowmik and gerard de melo
generating ne grained open vocabulary entity type descriptions
arxiv ratish puduppully li dong and mirella lapata
data to text generation with content selection and planning
in proceedings of the aaai conference on articial intelligence volume pages
alex graves
generating sequences with recurrent neural networks
arxiv preprint

zhishuai zhang siyuan qiao cihang xie wei shen bo wang and alan l yuille
single shot object detection with enriched semantics
in proceedings of the ieee conference on computer vision and pattern recognition pages
zhiting hu zichao yang xiaodan liang ruslan salakhutdinov and eric p xing
controllable text generation
arxiv preprint

martin schmitt sahand sharifzadeh volker tresp and hinrich schtze
an unsupervised joint system for text generation from knowledge graphs and semantic parsing
in proceedings of the conference on empirical methods in natural language processing emnlp pages
yau shian wang yun nung chen and hung yi lee
topicgan unsupervised text generation from explainable latent martin schmitt sahand sharifzadeh volker tresp and hinrich schtze
unsupervised text generation from structured data
arxiv preprint

oren sheffer or castel and raz landau
going grean a novel framework and evaluation metric for the graph to text topics

generation task
rik koncel kedziorski dhanush bekal yi luan mirella lapata and hannaneh hajishirzi
text generation from knowledge graphs with graph transformers
arxiv preprint

zhijing jin qipeng guo xipeng qiu and zheng zhang
genwiki a dataset of
million content sharing text and graphs for unsupervised graph to text generation
in proceedings of the international conference on computational linguistics pages
chlo kiddon luke zettlemoyer and yejin choi
globally coherent text generation with neural checklist models
in proceedings of the conference on empirical methods in natural language processing pages
zhiting hu haoran shi bowen tan wentao wang zichao yang tiancheng zhao junxian he lianhui qin di wang xuezhe ma al
texar a modularized versatile and extensible toolkit for text generation
arxiv preprint

omar abdelwahab and adel elmaghraby
deep learning based vs
markov chain based text generation for cross domain adaptation for sentiment classication
in ieee international conference on information reuse and integration iri pages
ieee
sidi lu yaoming zhu weinan zhang jun wang and yong yu
neural text generation past present and beyond
arxiv preprint

yaoming zhu sidi lu lei zheng jiaxian guo weinan zhang jun wang and yong yu
texygen a benchmarking platform for text generation models
in the international acm sigir conference on research development in information retrieval pages
hechong wang wei zhang yuesheng zhu and zhiqiang bai
data to text generation with attention recurrent unit
in international joint conference on neural networks ijcnn pages
ieee
sanidhya mangal poorva joshi and rahul modak
lstm vs
gru vs
bidirectional rnn for script generation
arxiv preprint

mercy m moila and thipe i modipa
the development of a sepedi text generation model using long short term memory
in proceedings of the international conference on intelligent and innovative computing applications pages
hongyuan mei mohit bansal and matthew r walter
what to talk about and how selective generation using lstms with coarse alignment
arxiv preprint

hongyu zang and xiaojun wan
towards automatic generation of product reviews from aspect sentiment scores
in proceedings of the international conference on natural language generation pages
ilya sutskever james martens and geoffrey e hinton
generating text with recurrent neural networks
in icml
aml in text analysis and generation a preprint jean pouget abadie dzmitry bahdanau bart van merrienboer kyunghyun cho and yoshua bengio
overcoming the curse of sentence length for neural machine translation using automatic segmentation
arxiv preprint

ehud reiter
nlg vs
templates
arxiv preprint cmp
kees van deemter marit theune and emiel krahmer
real versus template based natural language generation a false opposition computational linguistics
sam wiseman stuart m shieber and alexander m rush
learning neural templates for text generation
arxiv preprint

hao peng ankur p parikh manaal faruqui bhuwan dhingra and dipanjan das
text generation with exemplar based adaptive decoding
arxiv preprint

eder miranda de novais thiago dias tadeu and ivandr paraboni
improved text generation using n gram statistics
in ibero american conference on articial intelligence pages
springer
ilya sutskever oriol vinyals and quoc v le
sequence to sequence learning with neural networks
advances in neural information processing systems
dzmitry bahdanau kyunghyun cho and yoshua bengio
neural machine translation by jointly learning to align and translate
arxiv preprint

renjie zheng mingbo ma baigong zheng and liang huang
speculative beam search for simultaneous translation
arxiv preprint

liqun chen yizhe zhang ruiyi zhang chenyang tao zhe gan haichao zhang bai li dinghan shen changyou improving sequence to sequence learning via optimal transport
arxiv preprint chen and lawrence carin


wenhao yu chenguang zhu zaitang li zhiting hu qingyun wang heng ji and meng jiang
a survey of knowledge enhanced text generation
arxiv preprint

william a woods
transition network grammars for natural language analysis
communications of the acm
ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan n gomez ukasz kaiser and illia polosukhin
attention is all you need
in advances in neural information processing systems pages
adam santoro ryan faulkner david raposo jack rae mike chrzanowski theophane weber daan wierstra oriol vinyals razvan pascanu and timothy lillicrap
relational recurrent neural networks
in advances in neural information processing systems pages
rafal jozefowicz oriol vinyals mike schuster noam shazeer and yonghui wu
exploring the limits of language modeling
arxiv preprint

ciprian chelba tomas mikolov mike schuster qi ge thorsten brants phillipp koehn and tony robinson
one billion word benchmark for measuring progress in statistical language modeling
arxiv preprint

cristina garbacea samuel carton shiyan yan and qiaozhu mei
judge the judges a large scale evaluation study of neural language models for online review generation
arxiv preprint

tsvetomila mihaylova and andr ft martins
scheduled sampling for transformers
arxiv preprint

renjie zheng yilin yang mingbo ma and liang huang
ensemble sequence level training for multimodal mt osu baidu multimodal machine translation system report
arxiv preprint

guy tevet gavriel habib vered shwartz and jonathan berant
evaluating text gans as language models
arxiv preprint





eric wong leslie rice and j zico kolter
fast is better than free revisiting adversarial training
arxiv preprint alexey kurakin ian goodfellow and samy bengio
adversarial machine learning at scale
arxiv preprint yinpeng dong fangzhou liao tianyu pang hang su jun zhu xiaolin hu and jianguo li
boosting adversarial attacks with momentum
in proceedings of the ieee conference on computer vision and pattern recognition pages
aml in text analysis and generation a preprint aleksander madry aleksandar makelov ludwig schmidt dimitris tsipras and adrian vladu
towards deep learning models resistant to adversarial attacks
arxiv preprint

yuzhe yang guo zhang dina katabi and zhi xu
me net towards effective adversarial robustness with matrix estimation
arxiv preprint

ji lin chuang gan and song han
defensive quantization when efciency meets robustness
arxiv preprint

marius mosbach maksym andriushchenko thomas trost matthias hein and dietrich klakow
logit pairing methods can fool gradient based attacks
arxiv preprint

harini kannan alexey kurakin and ian goodfellow
adversarial logit pairing
arxiv preprint

jacob buckman aurko roy colin raffel and ian goodfellow
thermometer encoding one hot way to resist adversarial examples
in international conference on learning representations
cihang xie yuxin wu laurens van der maaten alan l yuille and kaiming he
feature denoising for improving adversarial robustness
in proceedings of the ieee conference on computer vision and pattern recognition pages
ajil jalal andrew ilyas constantinos daskalakis and alexandros g dimakis
the robust manifold defense adversarial training using generative models
arxiv preprint

qiao qian minlie huang haizhou zhao jingfang xu and xiaoyan zhu
assigning personality prole to a chatting machine for coherent conversation generation
in ijcai pages
daniel jakubovitz and raja giryes
improving dnn robustness to adversarial attacks using jacobian regularization
in proceedings of the european conference on computer vision eccv pages
ali shafahi mahyar najibi zheng xu john p dickerson larry s davis and tom goldstein
universal adversarial training
in aaai pages
guneet s dhillon kamyar azizzadenesheli zachary c lipton jeremy bernstein jean kossai aran khanna and anima anandkumar
stochastic activation pruning for robust adversarial defense
arxiv preprint

nicolas papernot patrick mcdaniel somesh jha matt fredrikson z berkay celik and ananthram swami
the limitations of deep learning in adversarial settings
in ieee european symposium on security and privacy pages
ieee
anirban chakraborty manaar alam vishal dey anupam chattopadhyay and debdeep mukhopadhyay
adversarial attacks and defences a survey
arxiv preprint

ali shafahi mahyar najibi mohammad amin ghiasi zheng xu john dickerson christoph studer larry s davis gavin taylor and tom goldstein
adversarial training for free in advances in neural information processing systems pages
dinghuai zhang tianyuan zhang yiping lu zhanxing zhu and bin dong
you only propagate once accelerating adversarial training via maximal principle
in advances in neural information processing systems pages
cody coleman deepak narayanan daniel kang tian zhao jian zhang luigi nardi peter bailis kunle olukotun chris r and matei zaharia
dawnbench an end to end deep learning benchmark and competition
training
jinxin chang ruifang he longbiao wang xiangyu zhao ting yang and ruifang wang
a semi supervised stable variational network for promoting replier consistency in dialogue generation
in proceedings of the conference on empirical methods in natural language processing and the international joint conference on natural language processing emnlp ijcnlp pages
jiwei li will monroe alan ritter michel galley jianfeng gao and dan jurafsky
deep reinforcement learning for dialogue generation
arxiv preprint

will hamilton zhitao ying and jure leskovec
inductive representation learning on large graphs
in advances in neural information processing systems pages
muhammad nabeel adnan riaz and wang zhenyu
cas gans an approach of dialogue policy learning based on gan and rl techniques
int
j
adv
comput
sci
appl
zhen yang wei chen feng wang and bo xu
improving neural machine translation with conditional sequence generative adversarial nets
arxiv preprint

aml in text analysis and generation a preprint lijun wu yingce xia fei tian li zhao tao qin jianhuang lai and tie yan liu
adversarial neural machine translation
in asian conference on machine learning pages
pmlr
zhirui zhang shujie liu mu li ming zhou and enhong chen
bidirectional generative adversarial networks for neural machine translation
in proceedings of the conference on computational natural language learning pages
rakshith shetty bernt schiele and mario fritz
author attribute anonymity by adversarial training of neural machine translation
in usenix security symposium usenix security pages
kishore papineni salim roukos todd ward and wei jing zhu
bleu a method for automatic evaluation of machine translation
in proceedings of the annual meeting of the association for computational linguistics pages
s
e
robertson and s
walker
some simple effective approximations to the poisson model for probabilistic weighted retrieval
in proceedings of the annual international acm sigir conference on research and development in information retrieval pages
matt j
kusner yu sun nicholas i
kolkin and kilian q
weinberger
from word embeddings to document distances
in proceedings of the international conference on international conference on machine learning volume pages
jmlr
org
pia keukeleire
correspondence between perplexity scores and human evaluation of generated tv show scripts

liam fowl micah goldblum arjun gupta amr sharaf and tom goldstein
random network distillation as a diversity metric for both image and text generation
arxiv preprint

leonid nisonovich vaserstein
markov processes over denumerable products of spaces describing large systems of automata
problemy peredachi informatsii
martin heusel hubert ramsauer thomas unterthiner bernhard nessler and sepp hochreiter
gans trained by a two time scale update rule converge to a local nash equilibrium
in advances in neural information processing systems pages
ondrej cfka aliaksei severyn enrique alfonseca and katja filippova
eval all trust a few do wrong to none comparing sentence generation models
arxiv preprint

jiwei li michel galley chris brockett jianfeng gao and bill dolan
a diversity promoting objective function for neural conversation models
arxiv preprint

chin yew lin
rouge a package for automatic evaluation of summaries
in text summarization branches out pages
satanjeev banerjee and alon lavie
meteor an automatic metric for mt evaluation with improved correlation with human judgments
in proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation summarization pages
wenchao du and alan w black
boosting dialog response generation
in proceedings of the annual meeting of the association for computational linguistics pages
cristbal esteban stephanie l hyland and gunnar rtsch
real valued medical time series generation with recurrent conditional gans
arxiv preprint

divya saxena and jiannong cao
d gan deep generative adversarial nets for spatio temporal prediction
arxiv preprint

sebastian nowozin botond cseke and ryota tomioka
gan training generative neural samplers using variational divergence minimization
in advances in neural information processing systems pages
xudong mao qing li haoran xie raymond yk lau zhen wang and stephen paul smolley
least squares generative adversarial networks
in proceedings of the ieee international conference on computer vision pages
alireza koochali peter schichtel andreas dengel and sheraz ahmed
probabilistic forecasting of sensory data with generative adversarial networks forgan
ieee access
ishaan gulrajani faruk ahmed martin arjovsky vincent dumoulin and aaron c courville
improved training of wasserstein gans
in advances in neural information processing systems pages
nan gao hao xue wei shao sichen zhao kyle kai qin arian prabowo mohammad saiedur rahaman and flora d salim
generative adversarial networks for spatio temporal data a survey
arxiv preprint

yuri burda harrison edwards amos storkey and oleg klimov
exploration by random network distillation
arxiv preprint

aml in text analysis and generation a preprint xinlei chen hao fang tsung yi lin ramakrishna vedantam saurabh gupta piotr dollr and c lawrence zitnick
microsoft coco captions data collection and evaluation server
arxiv preprint

xingxing zhang and mirella lapata
chinese poetry generation with recurrent neural networks
in proceedings of the conference on empirical methods in natural language processing emnlp pages
catherine wah steve branson peter welinder pietro perona and serge belongie
the caltech ucsd dataset

aolan sun jianzong wang ning cheng huayi peng zhen zeng lingwei kong and jing xiao
graphpb graphical representations of prosody boundary in speech synthesis
arxiv preprint

christopher olah
understanding lstm networks

sepp hochreiter and jrgen schmidhuber
long short term memory
neural computation
rachel manzelli vijay thakkar ali siahkamari and brian kulis
an end to end model for automatic music generation combining deep raw and symbolic audio networks
in proceedings of the musical metacreation workshop at international conference on computational creativity salamanca spain
catherine wong
dancin fooling text classiers with adversarial text example generation
arxiv preprint

minhao cheng jinfeng yi pin yu chen huan zhang and cho jui hsieh
evaluating the robustness of sequence to sequence models with adversarial examples
in aaai pages
marco barreno blaine nelson anthony d joseph and j doug tygar
the security of machine learning
machine learning
yi zhou xiaoqing zheng cho jui hsieh kai wei chang and xuanjing huang
defense against adversarial attacks in nlp via dirichlet neighborhood ensemble
arxiv preprint

takeru miyato andrew m dai and ian goodfellow
adversarial training methods for semi supervised text classication
arxiv preprint

chen zhu yu cheng zhe gan siqi sun thomas goldstein and jingjing liu
freelb enhanced adversarial training for language understanding
arxiv preprint

po sen huang robert stanforth johannes welbl chris dyer dani yogatama sven gowal krishnamurthy dvijotham and pushmeet kohli
achieving veried robustness to symbol substitutions via interval bound propagation
arxiv preprint

nicholas carlini and david wagner
towards evaluating the robustness of neural networks
in ieee symposium on security and privacy sp pages
ieee
jeremy m cohen elan rosenfeld and j zico kolter
certied adversarial robustness via randomized smoothing
arxiv preprint

xuanqing liu minhao cheng huan zhang and cho jui hsieh
towards robust neural networks via random ensemble
in proceedings of the european conference on computer vision eccv pages

