salience estimation with multi attention learning for abstractive text summarization piji li lidong zhongyu wei wai lam department of systems engineering and engineering management the chinese university of hong kong tencent ai lab shenzhen china damo academy school of data science fudan university china
cuhk
edu
hk
inc
com
edu
abstract attention mechanism plays a dominant role in the sequence generation models and has been used to improve the performance of machine translation and abstractive text summarization
different from neural machine translation in the task of text summarization salience mation for words phrases or sentences is a ical component since the output summary is a distillation of the input text
although the cal attention mechanism can conduct text ment selection from the input text conditioned on the decoder states there is still a gap to duct direct and effective salience detection
to bring back direct salience estimation for marization with neural networks we propose a multi attention learning framework which contains two new attention learning nents for salience estimation supervised tion learning and unsupervised attention ing
we regard the attention weights as the salience information which means that the mantic units with large attention value will be more important
the context information tained based on the estimated salience is porated with the typical attention mechanism in the decoder to conduct summary tion
extensive experiments on some mark datasets in different languages strate the effectiveness of the proposed work for the task of abstractive summarization
introduction sequence to sequence framework with attention mechanism has achieved signicant provement in the eld of neural machine translation bahdanau et al

encouraged by this come some researchers transplanted the framework to tackle the problem of abstractive text summarization rush et al
chopra et al
nallapati et al
and also obtained some encouraging results
since then abstractive text summarization has bloomed into a popular research task and quite a few based works have been proposed
for example see et al
integrated the copy operation gu et al
vinyals et al
and the coverage model tu et al
into the typical attention based to generate better summaries
li et al
designed a recurrent generative decoder to capture the latent structures in the target summaries
paulus et al
employed deep reinforcement learning techniques to enhance the performance of this task
the above frameworks can improve the quality of the generated abstractive summaries to some extent
however when we immerse ourselves in designing such dazzling and complex tricks on top of the model we may tionally ignore some important characteristics cic to the task of text summarization
along the whole way of summarization research salience detectionnding the most important information words phrases or sentences from the source put text has always been the most crucial and essential component
some supervised ng et al
wang et al
or unsupervised erkan and radev mihalcea and tarau ing methods were proposed to estimate the salience score for producing better summaries
however for the attention based framework it is not straightforward to gure out how to conduct salience detection
the current attention nism for the summarization task is not as natural and effective as in some other tasks
for instance in neural machine translation it is reasonable to use the current decoding state to attend the source sequence to get the relevant information for lating the next target word
in reading sion it makes sense to use the question to attend the reading passage to retrieve relevant information for extracting the answer
but for text summarization it is difcult to connect the attention mechanism r a l c
s c v
v i x r a with the salience estimation operation
although several works have tried some strategies to conduct the salience detection there still exist some tions
for example the selective mechanism zhou et al
only implicitly performs salience tection
the graph based attention mechanism tan et al
only adopts an unsupervised method thus it is not capable to exploit the supervised nal in the training data
in this paper we propose two global attention mechanisms based on supervised learning and unsupervised learning respectively for salient formation detection
for the supervised tion mechanism we employ a supervised learning method to estimate the probability of each word in the input text to be included in the generated summary
the normalized probability value is garded as the supervised attention signal
for the unsupervised attention mechanism inspired by the pagerank page et al
based text tion methods such as lexrank erkan and radev and textrank mihalcea and tarau as well as the graph based attention mechanism tan et al
we employ the pagerank rithm to estimate the salience score of each input word which is regarded as the unsupervised tion signal
thus these two types of attention nals contain the salience information of the terms in the source text
to examine the efcacy of the obtained salience information we integrate these signals into a simple base model for abstractive summarization i
e
the attention based model
note that we do not employ more ticated and powerful models because the aim of this work is to verify that bringing back salience estimation for neural abstractive summarization is helpful to improve the performance where a ple base model allows the conclusion not biased by other modeling structures
our main contributions are summarized as lows
we investigate a crucial element of text summarization problem namely salience tion which has been overlooked by the prior neural abstractive summarization approaches
we pose a supervised attention mechanism to directly estimate the salience under the supervision signal provided by the state of the input text and an supervised attention mechanism which employs a graph algorithm to estimate the salience of each input word
we integrate the two types of tention information into a base model and propose a unied neural network based framework named multi attention learning mal to tackle the task of abstractive summarization
experimental results on some benchmark datasets in different languages demonstrate the effectiveness of the posed attention learning methods for salience mation
our framework
overview the proposed multi attention learning mal framework is shown in figure
the input is a variable length sequence x


xm representing the source text
the output ground truth is also a sequence y



we denote the generated summary sequence as



for global salience mation we add two tailor made attention learning mechanisms supervised attention learning and supervised attention learning
the aim of vised attention learning is to predict if words from the input source text should be selected into the generated summaries i
e
predicting a or bel for each word
as shown in figure the word embeddings e and the encoder recurrent neural work rnn hidden states h are taken as the input information of this supervised attention learning modular
we also design a self attention model to capture more context information from the source text for better feature representation learning
the output of this component is regarded as the vised attention information as
for unsupervised attention learning we employ the pagerank rithm to estimate the salience score of each input words in an unsupervised manner
we treat the salience score as the unsupervised attention mation au
then these two types of global attention information representing word salience are bined with the hidden states h of the input source text to obtain the global attention context
finally the attention context information is incorporated in the decoding procedure to generate the abstractive summaries

supervised attention learning the aim of supervised attention learning is to timate the probability of the words in the source text to appear in the generated summaries
with sufcient training data we can regard this problem as a supervised sequence labeling task
we employ a straightforward method to prepare the ground figure our multi attention learning mal framework for abstractive summarization
truth labels r for the source text
the words in the source text except the stopwords that appear in the ground truth summaries are annotated with the positive label
all other words and the tions are annotated with the negative label
the structure of the supervised attention learning framework we illustrate the computational logic with the rst two states is depicted on the left of figure
we rst map each input word xt into a vector xt rke by retrieving an embedding lookup table which is randomly initialized and ne tuned in the training procedure
the word bedding sequence is fed into a bi directional rnn to capture the context information
compared with lstm hochreiter and schmidhuber gru cho et al
has comparable performance but with less parameters and more efcient tion so we employ gru as the basic recurrent unit rt br zt bz gt bh ht zt zt gt where rt is the reset gate zt is the update gate to control the mixture of the previous hidden and gt to get the current hidden ht rkh and those w and b s are learnable parameters
denotes the element wise multiplication and tanh is the hyperbolic tangent activation function
we employ a bidirectional gru network to produce two hidden states at the time step t gru xt gru xt then the overall hidden state he encoder is a concatenation of both directions of the he t ht in order to capture more context information of the input sequence we integrate a self attention modeling component
the self attention weight at the time step t is calculated based on the t and all the source hidden states he ship of he i
let ae i and he j which can be calculated as i j be the attention weight between he ae i j ei j e j e hihe i we hjhe be a hi we where we a rkh and ve rkh
then the self attention context is obtained by the weighted linear combination of all the source hidden states hj be ce t e t ae where t e is the sequence length
the original den state he t can be revised using the self attention context information ce he t hhhe t we chce t be h attentionsupervised attentionrecurrent finally as shown in figure we feed the word t the t and the self attention state t into the nal output layer to get the prediction embedding vector xt the hidden state he attention context ce he r t whr t wcrce r whrhe he t be r where is the sigmoid function
the value of r represents the salience of the corresponding words in the source text
in order to get the attention information and the attention context information we rst add a malization procedure to the predicted r as i e we regard the vector as rt e attention information
as the supervised based on the supervised attention information as we can obtain one type of global attention context by the weighted linear combination of the source hidden states e cs as finally cs is incorporated in the decoder as the supervised attention context information for the summary generation

unsupervised attention learning in the traditional text summarization research the pagerank page et al
based salience tion methods play a crucial role in identifying the most important information from the source text
some classical methods such as lexrank erkan and radev and textrank mihalcea and rau were proposed to tackle the problems of text summarization and keyphrase extraction and have been applied into practical tion applications and products
tan et al
introduced the graph based attention mechanism into the framework for sentence salience estimation and obtained encouraging results
here we also employ pagerank algorithm to conduct the unsupervised attention learning for salience timation as depicted in the middle upper part of figure
the difference is that we conduct the learning on word level to estimate the salience
for an input text sequence x with length m and xt rke representing the embedding vector for the word xt to build the word based graph g we take the nonstop words as the vertex set v and the relations between the words computed with tion as edge set e
we employ a parameterized tensor method to calculate the weights of the edges
assume that the adjacent matrix is m rmm then each element can be calculated as mi j i wpxj where wp rkeke is a neural parameter to be learned
pagerank is a iterative algorithm but we can get the closed form as discussed in tan et al
p where d is a diagonal matrix and i m i is the damping factor and q rm with all the elements equal to m
then the vector rm is the estimated salience score for all the m words
we also add a normalization procedure to p au i then the vector au rm is regarded as the pervised attention information
we can also obtain the second type of global attention context by the weighted linear combination of the word dings using au cu au and cu will be incorporated with the framework as the unsupervised attention context information

summary generation the decoder of our mal framework is still a gru based recurrent neural network with improved tention modeling
the rst hidden state hd of the decoder is initialized using the average of all the source input hidden states hd he
then m m the two layers of grus are designed to conduct the attention weights calculation and decoder den states update
on the rst gru layer the den state is calculated only using the current input word embedding and the previous hidden state then the attention weights at the time step t are calculated based on the relationship of t and all the source hidden states he ad i j e j ei j i we hhhe j ba the attention context is obtained by the weighted linear combination of all the source hidden states t e cd is the output of the second gru layer jointly sidering the word the previous hidden state and the attention context cd t
the nal hidden state ad t t t cd t


for the input sequence x we have ln ll log t x n then the nal objective loss function is l ls ln ll the whole framework can be trained using the multi task learning paradigm with the propagation method in an end to end training style
adadelta zeiler with hyperparameters
and is used for gradient based optimization
the traditional framework will predict the target word based on t

datasets experimental setup

multi attention integration recall that we have obtained the supervised tion context cs in section
and the unsupervised attention context cu in section

then we grate all the attention context information here in a straightforward manner t wd csacs wd t ha ha finally the probability of generating any target word yt is given as follows cuacu bd yt hyha t bd hy hy rkykh and bd hy rky
is where wd the softmax function
in the prediction state we use the beam search algorithm koehn for decoding and generating the best summary

model training for supervised attention learning we use the entropy as the objective function which need to be minimized m i ls ri ri ri where ri and ri is the prediction and the ground truth respectively
for summary generation we employ the tive log likelihood nll as the objective tion
given the ground truth summary y we train and evaluate our framework on three ular benchmark datasets
gigawords is an english sentence summarization dataset prepared based on annotated by extracting the rst tence from a news report together with the headline to form a source and summary pair i
e
the rst tence and headline
we directly download the pared dataset used in rush et al

it roughly contains
m training pairs k validation pairs and test pairs
the test set is identical to the one used in all the comparative methods
is another english dataset only used for ing where we directly apply the model trained from gigawords
it contains documents
each document contains model summaries written by experts
the length of the summary is limited to bytes
lcsts is a large scale chinese short text summarization dataset consisting of pairs of short text and summary collected from sina hu et al

we take part i as the training set part ii as the development set and part iii as the test set
there is a score in the range of beled by human to indicate the relevance between an article and its summary
we only make use of those pairs with scores no less than
thus the three parts contain
m
and data points respectively
in our experiments we directly take the chinese character sequences as input without performing word segmentation

ldc
upenn
edu
nist
gov
weibo
com
evaluation metrics we use rouge lin with standard options as our evaluation metric
the idea of rouge is to count the number of overlapping units tween the generated summaries and the reference summaries such as overlapped n grams word quences and word pairs
f measures of and rouge l r l are reported for gigawords and lcsts datasets
rouge recalls are reported for the duc dataset

comparative methods we compare our mal with a bunch of previous methods
since the datasets are quite standard so we just extract the results from their papers if ported
therefore the compared methods on ent datasets may be slightly different
topiary zajic et al
is the best on for compressive text summarization
it combines a system using linguistic based transformations and an unsupervised topic detection algorithm for compressive text summarization
rush et al
uses a phrase based statistical machine translation system trained on gigaword to produce summaries
abs and rush et al
are both the neural network based models with local attention modeling for abstractive sentence rization
rnn and rnn context hu et al
are two architectures
rnn context grates attention mechanism to model the context
copynet gu et al
integrates a copying mechanism into the framework
distract chen et al
uses a new attention mechanism by distracting the historical attention in the decoding steps
ras lstm and ras elman chopra et al
both consider words and word positions as input and use convolutional encoders to handle the source information
for the attention based sequence decoding process ras elman lects elman rnn elman as its decoder and ras lstm selects lstm architecture hochreiter and schmidhuber
lenemb kikuchi et al
uses a mechanism to control the summary length by considering the length embedding tor as the input
miao and blunsom uses a generative model with attention anism to tackle the sentence compression lem
and nallapati et al
utilize a trick to control the vocabulary size to improve the training efciency
seass zhou et al
integrates a selective gated network system abs ras lstm ras elman asc seass our sion drgd



























r l













table rouge on gigawords
into the framework to control the mation ow from encoder to decoder
drgd li et al
proposes a deep recurrent generative decoder to enhance the modeling ability of latent structures in the target summaries

experimental settings for the experiments on the english dataset of words we set the dimension of word embeddings to and the dimension of hidden states and latent variables to
the maximum length of documents and summaries is and tively
for the maximum length of summaries is bytes
for the dataset of lcsts the dimension of word embeddings is
we also set the dimension of hidden states and latent ables to
the maximum length of documents and summaries is and chinese characters spectively
the damping factor d of the pagerank algorithm for the unsupervised attention learning is set to

the beam size of the decoder is set to
our neural network based framework is mented using theano theano development team
results and discussions
rouge evaluation the results on the english datasets of gigawords and are shown in table and table respectively
among the ablations our version is the typical attention based framework implemented by us
system topiary abs ras elman ras lstm lenemb seass our sion drgd system rnn rnn context copynet rnn distract drgd our sion drgd





















































r l















r l










table rouge recall on
table rouge on lcsts
is the ablation method only considering vised attention information
only considers unsupervised attention tion
is our proposed framework
from the experimental results we can see that our mal framework performs better than the typical method as well as some other strong parisons which means that the multi attention text information can indeed improve the mance of the typical summarization els
it is worth noting that the methods and utilize linguistic features such as parts of speech tags named entity tags and tf and idf statistics of the words as part of the document representation
generally more useful features can indeed improve the performance
nevertheless our framework is still better than them which strates the effectiveness of our salience detection components
the results on the chinese dataset lcsts are shown in table
our mal also achieves the best performance
although copynet employs a ing mechanism to improve the summary quality rnn distract considers attention information versity in their decoders and drgd integrates a recurrent variational auto encoder into the typical framework our model is still better than these methods demonstrating that the effectiveness of the incorporation of the multi attention context information
it is expectable that integrating the copying mechanism and coverage diversity in our framework will further improve the summarization performance


highlight discussion note that in our framework we integrate the attention information with a simple base model namely the attention based model
thus the performance of the whole framework is deed limited
and the evaluation results are not as good as some very strong recent methods such as seass zhou et al
pointer generator see et al
and the reinforced model paulus et al

however the purpose of this work is to investigate the performance of applying the ditional salience detection intuitions in the simple attention based framework and such a ple base model allows the conclusions not biased by other modeling complications
the experimental analysis can demonstrate its effectiveness fore our study in this paper not only reminds the peer researchers that the crucial salience detection component for summarization should be ine in the scope of neural network based models but also presents a practical approach to solving this problem
if the two types of attention signals are appropriately integrated into the above recent models we believe that their performance can be improved as well
moreover our attention learning framework can also help revise the design of the copy mechanism as well as the coverage ing strategy
all these are worthwhile directions to investigate for the future works
system suatt unatt giga duc lcsts





table evaluation for the words tracted from suatt and unatt
toyota rally world europe banned japan s toyota team europe were banned from the world rally championship for one year here on friday in a crushing ruling by the world council of the international automobile tion a
golden toyota are banned for a year
suatt championship team ruling year a unatt world council europe federation ternational japan ruling friday championship banned a powerful bomb exploded outside a navy base near the sri lankan capital colombo tuesday seriously wounding at least one person military ofcials said
golden bomb attack outside srilanka navy base
suatt sri bomb base navy colombo ankan powerful military wounding exploded unatt sri military capital tuesday bomb powerful navy exploded base ofcials palestinian prime minister ismail haniya insisted friday that his hamas led government was continuing efforts to secure the release of an israeli soldier captured by militants
golden efforts still underway to secure dier s release hamas pm
suatt palestinian release haniya hamas led soldier israeli government secure efforts ister unatt government prime palestinian friday israeli militants efforts continuing minister secure table words extracted from suatt and unatt receptively for samples in gigawords

attention analysis we regard the supervised attention and the vised attention as the salience score for the words in the source text
so we also design experiments to verify the performance of the two attention nisms for nding important words
for each input sequence as and au are the two attention vectors obtained by supervised attention learning and supervised attention learning respectively
the ement value ai a represents the word salience score
therefore we can select the top k words from the input sequence according to the salience scores in as and au
intuitively the extracted k words are very important and may have a large overlapping with the ground truth summary
to verify it quantitatively we regard the top words as summaries and conduct rouge evaluation on them
because the order of the top words is ignored so we employ the f measure score of as the evaluation metric
the experimental results on those three datasets are given in table
we set k to here
the results illustrate that both methods can extract the important words from the source text and the quality of the top words tracted from the supervised attention as i
e
suatt is better than those extracted from the unsupervised attention au i
e
unatt
this adheres to our ition because the suatt method can obtain stronger supervision signals than the unsupervised method unatt
however from the rouge results presented in tables and we nd that the performance of is similar to or even better than
this phenomenon may be because that both the method of and suatt can receive supervision signals to guide the training but unatt is an unsupervised salience detection method which may nd some complementary mation to further improve the summarization formance
in order to show the differences vividly we present the extracted top words in table
and all the words are ranked based on the ing salience scores
from the results we know that suatt and unatt can indeed assign large salience scores to the important words
for instance suatt can extract words of toyoda banned and year which are the core elements of the golden summary toyota are banned for a year
the result of unatt is more diversied
although the performance of suatt and unatt are different the integration of them performs well in the quantitative evaluation experiments in the previous subsection which may be because that different attention methods can ture different aspects of the source text and they can complement each other
japan s toyota team europe were banned from the world rally championship for one year here on friday in a crushing ruling by the world council of the international automobile tion a
golden toyota are banned for a year
toyota s world rally europe banned from world rally championship
mal toyota barred from world rally pionship
slovaks started voting at am on day in elections to the parliament with centre right prime minister mikulas dzurinda ghting to continue far reaching but painful forms
golden slovaks start voting in legislative tions
slovakia s parliament begins voting
mal slovaks start voting in early elections
the thai government has set aside million baht about
million u
s
dollars to support new eco tourism plans during according to a report of the thai news agency tna tuesday
golden tourism
thailand to support new eco tourism in
mal thailand to support new eco tourism plans
thai government to support table examples of the generated summaries

summary case analysis finally some examples of the source texts golden summaries and the generated summaries by the typical attention based framework and our proposed mal framework are shown in table
from these cases we can see that the generated summaries by mal generally have better quality
moreover because of the attention learning ponents for salience detection our framework has the ability to assign small salience scores to portant words and uninformative symbols while the summary generated by contains more noisy symbols as the cases shown in
related works most important content of the original text ument nenkova and mckeown
tional summarization methods can be classied into three categories extraction based methods erkan and radev min et al
based methods li et al
wang et al
li et al
and abstraction based methods barzilay and mckeown bing et al

recently some researchers employ neural work based frameworks to tackle the abstractive summarization problem and obtain encouraging performance
rush et al
proposed a ral model with local attention modeling which is trained on the gigaword corpus but combined with an additional log linear extractive summarization model with handcrafted features
nallapati et al
utilized a trick to control the vocabulary size to improve the training efciency
gu et al
integrated a copying mechanism into a framework to improve the quality of the generated summaries
chen et al
proposed a new attention mechanism that not only considers the important source segments but also distracts them in the decoding step in order to better grasp the overall meaning of input documents
miao and blunsom extended the framework and proposed a generative model to capture the latent summary information
zhou et al
tegrated a selective gated network into the framework to control the information ow from encoder to decoder
li et al
proposed a deep recurrent generative decoder to enhance the modeling ability of latent structures in the target summaries
see et al
employed pointer networks and converge mechanism to improve the quality of the generated summaries
paulus et al
proposed a reinforcement learning based framework to enhance the performance of rization
chen et al
proposes a generative bridging network in which a bridge module is troduced to assist the training of the sequence diction model
li et al
employ actor critic training paradigm to enhance the quality of the generated summaries
meanwhile some researchers also combine the traditional salience estimation methods into the frameworks in order to enhance the marization performance
tan et al
automatic summarization is the process of matically generating a summary that retains the researchers regard the compression approach as a special case of the extraction approach
porated the graph based attention information tained by the pagerank algorithm into their work
hsu et al
weighted the attention mechanism using sentence salience information calculated by a traditional supervised method
in contrast we consider both the supervised salience information and unsupervised salience information in our framework to generate better summaries
conclusions in this work we investigate the effect of adding the traditional salience detection of text rization back to the typical attention based framework for abstractive summarization
we pose a multi attention learning mal framework which contains two new attention learning nents namely supervised attention learning and unsupervised attention learning for salience mation
the salience information obtained based on these two types of attentions is incorporated with the typical attention mechanism in the decoder to conduct the summary generation
extensive periments on some benchmark datasets in different languages demonstrate the effectiveness of the posed framework for the task of abstractive marization
references dzmitry bahdanau kyunghyun cho and yoshua gio

neural machine translation by jointly learning to align and translate
in iclr
regina barzilay and kathleen r mckeown

tence fusion for multidocument news tion
computational linguistics
lidong bing piji li yi liao wai lam weiwei guo and rebecca passonneau

abstractive document summarization via phrase selection and merging
in acl pages
qian chen xiaodan zhu zhenhua ling si wei and hui jiang

distraction based neural networks for document summarization
in ijcai pages
wenhu chen guanlin li shuo ren shujie liu zhirui zhang mu li and ming zhou

tive bridging network in neural sequence prediction
naacl
kyunghyun cho bart van merrienboer caglar cehre dzmitry bahdanau fethi bougares holger learning schwenk and yoshua bengio

phrase representations using rnn encoder decoder for statistical machine translation
in emnlp pages
sumit chopra michael auli alexander m rush and seas harvard

abstractive sentence marization with attentive recurrent neural networks
naacl hlt pages
jeffrey l elman

finding structure in time
nitive science
gunes erkan and dragomir r radev

lexrank graph based lexical centrality as salience in text summarization
journal of articial intelligence search
jiatao gu zhengdong lu hang li and victor ok incorporating copying mechanism in in acl pages li

sequence to sequence learning

sepp hochreiter and jurgen schmidhuber

neural computation long short term memory

wan ting hsu chieh kai lin ming ying lee kerui min jing tang and min sun

a ed model for extractive and abstractive arxiv preprint rization using inconsistency loss


baotian hu qingcai chen and fangze zhu

sts a large scale chinese short text summarization dataset
in emnlp pages
yuta kikuchi graham neubig ryohei sasano hiroya takamura and manabu okumura

ling output length in neural encoder decoders
in emnlp pages
philipp koehn

pharaoh a beam search coder for phrase based statistical machine in conference of the association for tion models
machine translation in the americas pages
springer
chen li fei liu fuliang weng and yang liu

document summarization via guided sentence pression
in emnlp pages
piji li lidong bing and wai lam

actor critic based training framework for abstractive tion
arxiv preprint

piji li wai lam lidong bing weiwei guo and hang li

cascaded attention based unsupervised information distillation for compressive tion
in emnlp
piji li wai lam lidong bing and zihao wang

deep recurrent generative decoder for in emnlp pages stractive text summarization

chin yew lin

rouge a package for automatic in text summarization evaluation of summaries
branches out proceedings of the workshop volume
yishu miao and phil blunsom

language as a latent variable discrete generative models for tence compression
in emnlp pages
david zajic bonnie dorr and richard schwartz

in hlt naacl bbn umd at topiary
pages
rada mihalcea and paul tarau

textrank ing order into text
in emnlp
matthew d zeiler

adadelta an adaptive ing rate method
arxiv preprint

qingyu zhou nan yang furu wei and ming zhou

selective encoding for abstractive sentence summarization
in acl pages
ziheng lin min yen kan chew and lim tan

exploiting category specic information for document summarization
coling pages
ramesh nallapati bowen zhou caglar gulcehre bing xiang al

abstractive text tion using sequence to sequence rnns and beyond
arxiv preprint

ani nenkova and kathleen mckeown

a survey in mining text of text summarization techniques
data pages
springer
jun ping ng praveen bysani ziheng lin min yen kan and chew lim tan

exploiting specic information for multi document tion
coling pages
lawrence page sergey brin rajeev motwani and terry winograd

the pagerank citation ing bringing order to the web
technical report stanford infolab
romain paulus caiming xiong and richard socher

a deep reinforced model for abstractive marization
iclr
alexander m rush sumit chopra and jason weston

a neural attention model for abstractive tence summarization
in emnlp pages
abigail see peter j liu and christopher d manning

get to the point summarization with generator networks
in acl volume pages
jiwei tan xiaojun wan and jianguo xiao

abstractive document summarization with a based attentional neural model
in acl volume pages
theano development team

theano a python framework for fast computation of mathematical pressions
arxiv e prints

zhaopeng tu zhengdong lu yang liu xiaohua liu and hang li

modeling coverage for neural machine translation
in acl volume pages
oriol vinyals meire fortunato and navdeep jaitly

pointer networks
in nips pages
lu wang hema raghavan vittorio castelli radu rian and claire cardie

a sentence pression based framework to query focused in acl pages document summarization


