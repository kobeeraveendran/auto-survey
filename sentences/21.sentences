t c o r i
s c v
v i x r a artex is another text summarizer juan manuel torres laboratoire informatique davignon bp avignon cedex france juan manuel
avignon
fr cole polytechnique montral cp
succursale centre ville montral qubec canada abstract this paper describes artex another algorithm for automatic text summarization
in order to rank sentences a simple inner product is calculated between each sentence a document vector text topic and a lexical vector vocabulary used by a sentence
maries are then generated by assembling the highest ranked sentences
no ruled based linguistic post processing is necessary in order to obtain summaries
tests over several datasets coming from document understanding conferences duc text analysis ference tac evaluation campaigns
in french english and spanish have shown that artex summarizer achieves interesting results
keywords automatic text summarization space vector model text extraction stemming introduction automatic text summarization ats is the process to automatically generate a compressed version of a source document
query oriented summaries focus on a user s request and extract the information related to the specied topic given explicitly in the form of a query
generic mono document summarization tries to cover as much as possible the tion content
multi document summarization is a oriented task to create a summary from a heterogeneous set of documents on a focused topic
over the past years extensive iments on query oriented multi document summarization have been carried out
extractive summarization produces summaries choosing a subset of representative sentences from original documents
sentences are ordered and then assembled according to their relevance to generate the nal summary
this article introduces a new method of summarization based in sentences extraction on vector space model vsm
we score each sentence by calculating their inner product with a pseudo sentence vector and a pseudo word vector
results show that artex not only preserves the content of the summaries generated using this new representation but often surprisingly the performance can be improved
artex could be an interesting and simple algorithm using the extractive summarization paradigm
our tests on trilingual corpora english spanish and french evaluated by the fresa algorithm without human references conrm the good performance of artex
in this paper related work is given in section
section presents the new algorithm of automatic text summarization
experiments are presented in section followed by results in section and conclusions in section
related works research in automatic text summarization was introduced by h
p
luhn in
in the strategy proposed by luhn the sentences are scored for their component word values as termined by like weights
scored sentences are then ranked and selected from the top until some summary length threshold is reached
finally the summary is generated by bling the selected sentences in the original source order
although fairly simple this extractive methodology is still used in current approaches
later on extended this work by adding ple heuristic features such as the position of sentences in the text or some key phrases indicate the importance of the sentences
as the range of possible features for source characterization widened choosing appropriate features feature weights and feature combinations have become a central issue
a natural way to tackle this problem is to consider sentence extraction as a classication task
to this end several machine learning approaches that uses document summary pairs have been proposed an hybrid method mixing statistical and linguistics algorithms is presented in
and propose a good state of art of automatic text summarization tasks and algorithms

document pre processing the rst step to represent documents in a suitable space is the pre processing
as we use extractive summarization documents have to be chunked into cohesive textual segments that will be assembled to produce the summary
pre processing is very important because the selection of segments is based on words or bigrams of words
the choice was made to split documents into full sentences in this way obtaining textual segments that are likely to be grammatically corrects
afterwards sentences pass through several basic normalization steps in order to reduce computational complexity
the process is composed by the following steps
sentence splitting
simple rule based method is used for sentence splitting
documents are chunked at the period exclamation and question mark

sentence ltering
words lowercased and cleared up from sloppy punctuation
words with less than occurrences are eliminated hapax legomenon presents once in a document
words that do not carry meaning such as functional or very common words are removed
small stop lists depending of language are used in this step

word normalization
remaining words are replaced by their canonical form using lemmatization stemming ultra stemming or none of them raw text
four methods of normalization were applied after ltering lemmatization by simples dictionaries of morphological families
these dictionaries have
m k and k words entries in spanish english and french tively
porter s stemming available at snowball web site
tartarus
org texts stemmersoverview
html for english spanish french among other guages
ultra stemming
this normalization seems be very ecient and it produces a pact matrix representation
ultra stemming consider only the n rst letters of each word
for example in the case of ultra stemming rst letter inected verbs like sing song sings singing


or proper names smith snowboard sex


are replaced by the letter s

text vectorization
documents are vectorized in a matrix of p sentences and n columns
each element j represents the occurrences of an object j a letter in the case of ultra stemming a word in the case of lemmatization or a stem for stemming j


n in the sentence i i


p
another text summarizer artex is a simple extractive algorithm for automatic text summarization
the main idea is the next one first we represent the text in a suitable space model vsm
then we construct an average document vector that represents the average the global topic of all sentences vectors
at the same time we obtain the lexical weight for each sentence i
e
the number of words in the sentence
after that it is calculated the angle between the average document and each sentence narrow angles indicate that the sentences near of the global topic should be important and therefore extracted
see on the gure the vsm of words p vector sentences and the average global topic are represented in a n dimensional space of words
figure the global topic in a vector space model of n words
next a score for each sentence is calculated using their proximity with the global topic and their lexical weight
in the gure the lexical weight is represented in a vsm of p sentences
finally the summary is generated concatenating the sentences with the highest scores lowing their order in the original document
french artex est un autre rsumeur textuel
wordsglobal topicsentenceangle figure the lexical weight in a vector space model of p sentences

algorithm formally artex algorithm computes the score of each sentence by calculating the inner product between a sentence vector an average pseudo sentence vector the global topic and an average pseudo word vector the lexical weight
once a pre processing word normalization and ltering of stop words is completed it is created a matrix using the vector space model that contains n words or letters and p sentences
let


sn be a vector of the sentence i i


p
we dened the average pseudo word vector as the average number of occurrences of n words used in the sentence i ai n j j bj p j i and the average pseudo sentence vector as the average number of occurrences of each word j used trough the p sentences the score or weight of each sentence is calculated as follows si bj ai


p j


n n p j the computed by equation must be normalized between the interval
the calculation of indicates the proximity between the sentence and the average sentence
the product weigh this proximity using the average pseudo word ai
sentenceslexical weight if a sentence is near of and their corresponding element ai has a high value will have therefore a high score
moreover a sentence i far of main topic i
e
is near or a less informative sentence i i
e
ai are near will have a low score
in computational terms it is not really necessary to divide the scalar product by the constant i j
n p because the angle
between and is the same if we use the element ai is only a scale factor that does not modify
in fact if the matrix is approximated to a binary p where each element we can normalize vectors and matrix s as follows i j has a probability of p p i j p p n i j n n p n n i j n i j j i j j vectors then will be represented in hyper spheres of n or p dimensions and the normalized score in this space would be n n p n p n si bj ai j si bj ai


p j


n j n is a constant value i
e
a simple scale factor and then the however the term calculated using the equation and the using the equation are both equivalent
experiments artex algorithm described in the previous section has been implemented and evaluated in corpora in several languages
we have conducted our experimentation with the following languages summarization tasks summarizers and data sets generic multi document summarization in english with the corpus generic single document summarization in spanish with the medicina clnica and generic single document summarization in french with the pistes
we have applied the summarization algorithms and nally the results have been evaluated using fresa while processing times for each summarizer have been measured and compared
the following subsections present formally the details of the summarizers corpora and evaluations studied in dierent experiments
is a reasonable approximation in this context because is a sparsed matrix with many term occurrences equal to one or zero

other summarizers to compare the performances two other summarization systems were used in our experiments cortex and enertex
to be in the same conditions these two systems have used exactly the same textual representation based on vector space model described in section

cortex is a single document summarization system using several metrics and an optimal decision algorithm
enertex is a summarization system based in textual energy concept text is sented as a spin system where spins represents words that their occurrences are f spins if the word is not present

summarization corpora description to study the impact of our summarizer we used corpora in three languages english spanish and french
the corpora are heterogeneous and dierent tasks are representatives of matic text summarization generic multi document summary and mono document guided by a subject
corpus in english
piloted by nist in document understanding duc the task of aims to produce a short summary of a cluster of related documents
we studied generic multi document summarization in english using data from
this corpus with k words types is compound of clusters documents each
corpus in spanish
generic single document summarization using a corpus from the scientic journal medicina which is composed of medical articles in spanish each one with its corresponding author abstract
this corpus contains k words types
corpus in french
we have studied generic single document summarization using the canadian french sociological articles corpus generated from the journal perspectives interdisciplinaires sur travail sant
it contains sociological articles in french each one with its corresponding author abstract
this corpus contains near k words types

summaries content evaluation duc conferences have introduced the rouge content evaluation wich measures the lap of n grams between a candidate summary and reference summaries written by humans
however to write the human summaries necessaries for rouge is a very expensive task
recently metrics without references have been dened and experimented at duc and text analysis conferences workshops
fresa content evaluation is similar to rouge evaluation but human reference summaries are not necessary
fresa calculates the divergence of probabilities between the
nist
gov nlpir
nist
gov projects duc
html
elsevier
revistas
pistes
uqam

nist
gov tac candidate summary and the document source
among these metrics kullback leibler kl and jensen shannon js divergences have been widely used by to evaluate the tiveness of summaries
in this article we use fresa based in kl divergence with dirichlet smoothing like in the and inex edition to evaluate the informative content of summaries by comparing their n gram distributions with those from source documents
fresa only considered absolute log di between the terms occurrences of the source and its the summary
let t be the set of terms in the source
for every t t we denote by c t t occurrences in the source and c s t its occurrences in the summary
the fresa package computed the divergence between the document source and the maries as follows log log tt c t c s t to evaluate the information content the quality of the generated summaries after moving stop words several automatic measures were computed unigrams of single stems bigrams of pairs of consecutive stems bigrams with gaps also made of pairs of consecutive stems and nally i
e
the average of all fresa values
the fresa values scores are normalized between and
high fresa values mean less divergence regarding the source document summary reecting a greater amount of information content
all summaries produced by the systems were evaluated automatically using fresa package
results in this section we present the results for each corpus with dierent summarizers and the eral normalization strategies used
based on these results rstly we have veried that stemming improves the performance of summarizers
secondly we show that artex is a system that has a similar performances in terms of information content and processing times to other state of art summarizers

content evaluation english corpus
figure shows the performance of the three summarizers using stemming and lemmatization
results show that ultra stemming improves the score of the three automatic summarizer systems
artex and cortex expose a similar performances in information content
figure histogram plot of content evaluation for task with sures for each summarizer and each normalization
spanish corpus
spanish is a language with a greater variability than english
results in gure shown that artex summarizer outperforms cortex and enertex if stemming or lemmatization are used as normalization
figure histogram plot of content evaluation for spanish medicina clnica with scores for each summarizer
french corpus
french is a language with a large variability too
figure shows the score on the french corpus pistes
results show a similar behavior stemming improves the score of the three automatic summarization systems used
in particular the ecacy of artex is less sensible to word normalization than others marizers
figure histogram plot of content evaluation for french pistes with scores for each summarizer

processing times evaluation table shows processing times for each corpus following the normalization method for cortex artex and enertex
processing times of ultra stemming are shorter compared to all others methods
by example cortex is a very fast summarizer with where p n and processing times for stemming and are close
in other hand enertex summarizer has a complexity of then it needs more time to process the same corpus
performances of artex algorithm remain close to cortex
summarizer average time all corpora normalization cortex artex enertex

lemmatization

stemming




table statistics of processing times in minutes of three summarizers over three corpora
times are measured in a
gb of ram computer core m cpu
processor running under bits gnu linux ubuntu version

conclusions in this article we have introduced and tested a simple method for automatic text rization
artex is a fast and very simple algorithm based in vsm model and the extractive paradigm
the method uses a matrix representation to calculate a normalized score for each sentence using the inner product of vectors
the algorithm retains the salient information of each sentence of document
an important aspect of our approach is that it does not requires linguistic knowledge or resources which makes it a simple and ecient summarizer method to tackle the issue of automatic text summarization
summaries generated by artex system are pertinents
the results obtained on corpora in english spanish and french show that artex can achieve good results for content quality
tests with other corpora duc and tac evaluation campaigns inex
in mono and multi document guided by a subject using content evaluation with rouge evaluations or without reference summaries still in progress
references iria da cunha silvia fernndez patricia velzquez morales jorge vivaldi eric sanjuan and juan manuel torres moreno
a new hybrid summarizer based on vector space model statistical physics and linguistics
in proceedings of the mexican international conference on advances in articial intelligence pages aguascalientes mexico
springer verlag
harold daum iii
practical structured learning techniques for natural language processing
phd h
p
edmundson
new methods in automatic extraction
journal of the association for thesis los angeles ca
puting machinery
b
favre f
bchet p
bellot f
boudin m
el bze l
gillard g
lapalme and j m
moreno
the lia thales summarization system at
in proceedings of the document derstanding conference brooklyn new york united states

nist
gov
silvia fernndez eric sanjuan and juan manuel torres moreno
textual energy of tive memories performants applications of enertex algorithm in text summarization and topic segmentation
in proceedings of the mexican international conference on articial intelligence pages aguascalientes mexico
springer verlag
j
kupiec j
pedersen and f
chen
a trainable document summarizer
in proceedings of the conference acm special interest group on information retrieval pages seattle wa united states
acm press new york
chin yew lin
rouge a package for automatic evaluation of summaries
in marie francine moens and stan szpakowicz editors proceedings of the workshop text summarization branches out pages barcelone spain july
acl
annie louis and ani nenkova
automatic summary evaluation without human models
in first text analysis conference gaithersburg md united states november
h
p
luhn
the automatic creation of literature abstracts
ibm journal of research and i
mani and m
mayburi
advances in automatic text summarization
mit press cambridge development

eric sanjuan patrice bellot vronique moriceau and xavier tannier
overview of the inex question answering track
in shlomo geva jaap kamps ralf schenkel and andrew trotman editors comparative evaluation of focused retrieval volume of lecture notes in computer science pages
springer berlin heidelberg
simone teufel and marc moens
sentence extraction as a classication task
in i
mani and m
maybury editors proceedings of the acl workshop on intelligent scalable text summarization madrid spain july
j

torres moreno horacio saggion i
da cunha p
velazquez morales and e
sanjuan
uation automatique rsums avec et sans rferences
in proceedings conference ment automatique des langagues naturelles montral qc canada july
atala
j

torres moreno p

st onge m
gagnon m
el bze and p
bellot
automatic marization system coupled with a question answering system qaas
corr

juan manuel torres moreno
rsum automatique documents une approche statistique
herms lavoisier paris
juan manuel torres moreno
beyond stemming and lemmatization ultra stemming to improve automatic text summarization
corr
cs

juan manuel torres moreno horacio saggion iria da cunha and eric sanjuan
summary ation with and without references
polibits research journal on computer science and computer engineering with applications
juan manuel torres moreno patricia velzquez morales and jean guy meunier
cortex un in proceedings of the conference de algorithme pour la condensation automatique textes
lassociation pour la recherche cognitive volume pages lyon france

