contextualized rewriting for text summarization guangsheng yue school of engineering westlake university institute of advanced technology westlake institute for advanced study baoguangsheng
edu
cn n a j l c
s c v
v i x r a abstract extractive summarization suffers from irrelevance dancy and incoherence
existing work shows that abstractive rewriting for extractive summaries can improve the ness and readability
these rewriting systems consider tracted summaries as the only input which is relatively cused but can lose important background knowledge
in this paper we investigate contextualized rewriting which ingests the entire original document
we formalize contextualized rewriting as a problem with group alignments troducing group tag as a solution to model the alignments identifying extracted summaries through content based dressing
results show that our approach signicantly forms non contextualized rewriting systems without ing reinforcement learning achieving strong improvements on rouge scores upon multiple extractive summarizers
introduction extractive text summarization systems nallapati zhai and zhou narayan cohen and lapata liu and pata work by identifying salient text segments ically sentences from an input document as its summary
they have been shown to outperform abstractive systems rush chopra and weston nallapati et al
chopra auli and rush in terms of content selection and faithfulness to the input
however extractive rizers exhibit several limitations
first sentences extracted from the input document tend to contain irrelevant and dundant phrases durrett berg kirkpatrick and klein chen and bansal gehrmann deng and rush
second extracted sentences can be weak in their ence with regard to discourse relations and cross sentence anaphora dorr zajic and schwartz cheng and ata
to address these issues a line of work investigates editing of extractive summarizer outputs
while grammar tree trimming has been considered for reducing irrelevant content within sentences dorr zajic and schwartz rule based methods have also been investigated for ing redundancy and enhancing coherence durrett kirkpatrick and klein
with the rise of neural corresponding author
copyright association for the advancement of articial intelligence www
aaai
org
all rights reserved
source document thousands of live earthworms have been falling from the sky in norway


a biology teacher discovered the worms on the surface of the snow while he was skiing in the mountains near bergen at the weekend


teacher erstad told norwegian news website the local


gold summary live worms on top of the snow
extractive summary a biology teacher discovered the worms on the surface of the snow while he was skiing in the mountains near bergen at the weekend
rewritten summary biology teacher erstad ered the worms on the snow
teacher karstein erstad found thousands of figure example showing that contextual information can benet summary rewriting
works a more recent line of work considers using tive models for rewriting extracted outputs sentence by tence chen and bansal bae et al
wei huang and gao xiao et al

human evaluation shows that such rewriting systems effectively improve the ness and readability
interestingly existing rewriters do not improve the rouge scores compared with the extractive baselines
existing abstractive rewriting systems take extracted maries as the only input
on the other hand information from the original document can serve as useful background knowledge for inferring factual details
take figure for example
a salient summary can be made by extracting the sentence a biology teacher


weekend
while a rewriter can simplify the sentence for making a better summary it not provide additional details beyond the sentence unless the document context is also considered
for example the name of the teacher is not given by the extractive summary but we can infer that the teacher s name is erstad from the context sentences thereby making the summary more informative
we propose contextualized rewriting by using the full input document as a context for rewriting extractive mary sentences
rather than encoding only the extractive summary we use a neural representation model to encode the whole input document representing extractive summary as a part of the document representation
to inform the rewriter of the current sentence being rewritten we use source document our resident coach and technical expert chris meadows has plenty of experience in the sport and has worked with some of the biggest names in golf
chris has worked with more than golfers throughout his career
growing up beside nick faldo meadows learned that success in golf comes through develping a clear understanding of and being committed to your objective
a dedicated coach from an early age he soon realized his gift was the development of others
meadows simple and holistic approach to learning has been personally shared with more than golfers in a career spanning three decades
many of his instructional books have become best sellers his career recently being recognized by the professional golfers association when he was made an advanced fellow of the pga
chris has been living golf s resident golf expert since
rewritten summary chris meadows has worked with some of golf s big names
he has personally coached more than golfers
chris was made an advanced fellow of the pga
figure example of three step summarization process selecting grouping and rewriting
content based addressing graves wayne and danihelka
specically as figure shows a unique group tag is used to index each extracted sentence in the source ment matching an increasing sentence index in the tive rewriter as the rewriter generates the output where the group tags are used to guide the rst second and third rewritten summary sentences respectively
we choose the bert devlin et al
base model as the document encoder building both the extractive marizer and the abstractive rewriter by following the basic models of liu and lapata
our models are ated on the cnn dm dataset hermann et al

sults show that the contextualized rewriter gives cantly improved rouge lin scores compared with a state of the art extractive baseline outperforming a tional rewriter baseline by a large margin
in addition our method gives better compression lower redundancy and ter coherence
the contextualized rewriter achieves strong and consistent improvements on multiple extractive rizers
to our knowledge we are the rst to report improved rouge by rewriting extractive summaries
we release our code at
com baoguangsheng ctx rewriter summ
git
related work extractive summarizers have received constant research tention
early approaches such as textrank mihalcea and tarau select sentences based on weighted similarities
recently nallapati zhai and zhou use a neural sier to choose sentences and a selector to rank them
chen and bansal use a pointer network vinyals nato and jaitly to extract sentences
liu and lapata use a linear classier upon bert
this method gives the current state of the art result in extractive tion and we choose it for our baseline
rewriting systems manipulate extractive summaries for reducing irrelevance redundancy and incoherence
durrett berg kirkpatrick and klein use compression rules to reduce unimportant content within a sentence and make anaphoricity constraints to improve cross sentence ence
dorr zajic and schwartz trim unnecessary phrases in a sentence without hurting grammar correctness by nding the syntactic structures of sentences
in contrast to their work we consider neural abstractive rewriting which can solve all the above issues more systematically
recently neural rewriting has attracted much research attention
chen and bansal use a model with the copy mechanism see liu and manning to rewrite extractive summaries sentence by sentence
a reranking post process is applied to avoid repetition and the extractive model is also tuned by reinforcement learning with reward signals from each rewritten sentence
bae et al
use a similar strategy but with a bert document encoder and reward signals from the whole summary
wei huang and gao use a binary classier upon a bert document encoder to select sentences and a transformer decoder vaswani et al
with the copy mechanism to generate the summary sentence
xiao et al
build a hierarchical representation of the input document
a pointer network and a copy or rewrite mechanism are designed to choose sentences for copying or rewriting followed by a vanilla model as the rewriter
the model decisions on sentence selecting copying and rewriting are tuned by reinforcement learning
compared with these methods our method is computationally simpler thanks to the freedom from using reinforcement learning and the copy mechanism as most of the methods above do
in addition as mentioned earlier in contrast to these methods we consider rewriting by including a document level context and therefore can tentially improve details and factual faithfulness
some hybrid extractive and abstractive summarization models are also in line with our work
cheng and lapata use a hierarchical encoder for extracting words straining a conditioned language model for generating ent summaries
gehrmann deng and rush consider a bottom up method using a neural classier to select portant words from the input document and informing an abstractive summarizer by restricting the copy source in a pointer generator network to the selected content
similar to our work they use extracted content for guiding the tive summary
however different from their work which cuses on the word level we investigate sentence level straints for guiding abstractive rewriting
our method can also be regarded as using group tags to guide the reading context during abstractive summarization rush chopra and weston nallapati et al
see liu and manning where group tags are obtained using an extractive summary
compared with vanilla stractive summarization the advantages are three fold
first extractive summaries can guide the abstractive summarizer with more salient information
second the training difculty of the abstractive model can be reduced when important tents are marked as inputs
third the summarization cedure is made more interpretable by associating a crucial source sentence with each target sentence
figure architecture of the contextualized rewriter
the group tag embeddings are tied between the encoder left gure and the decoder right gure through which the decoder can address to the corresponding tokens in the document
with group alignments as a key contribution of our method we model alized rewriting as a mapping problem with group alignments
for an input sequence x and an output sequence y a group set g describes a set of segment wise alignments between x and y
the mapping problem is dened as ing estimation y argy max y g p y where x y g that denotes the number of elements in x the ber of elements in y and the number of groups
each group gk denotes a pair of text segments one from x and one from y which belongs to the same group
taking ure as an example the rst extractive sentence from the document and the rst sentence from the summary form a group
the problem can be simplied given the fact that for each group gk the text segment from x is known while the responding segment from y is dynamically decided during the generation of y
we thus separate g into two nents gx and gy and redene the mapping problem as argy max y gy p y gy gx table which is randomly initialized and jointly trained with the encoder and decoder
the vector representations of gx and gy are used to enrich vector representations of x and y respectively
as a result all the tokens tagged with k in both x and y have the same vector component through which a content based addressing can be done by the tention mechanism garg et al

here the group tag serves as a mechanism to constrain the attention from y to the corresponding part of x during decoding
unlike proaches which modify a model by using rules hsu et al
gehrmann deng and rush group tag ables the modication to be exible and trainable
contextualized rewriting system we take a three step process in generating a summary
first an extractive summarization model is used to select a set of sentences from the original document as a guiding source
second the guiding source text is matched with the nal document whereby a set of group tags are assigned to each token
third an abstractive rewriter is applied to the tagged document where the group tags serve as a guidance for summary generation
formally we use x to represent document x which contains tokens and y to sent a nal resulting summary y which contains tokens
where extractive summarizer gx gi k if wi gk else gy gj k if wj gk else so that for each group gk a group tag k is assigned through which the text segment from x in group gk are linked to the segment from y in the same group
for the example in figure gx














and gy









in the encoder decoder framework we convert gx and gy into vector representations through a shared embedding following liu and lapata we use bert to encode the input document with a special cls token being added to the beginning of each sentence and interval segments ing applied to distinguish successive sentence
on top of the bert representation of cls tokens an extractor is stacked to select sentences
the extractor uses a transformer vaswani et al
encoder to generate inter sentence resentations on which for extracting a summary an put layer with the sigmoid activation is used to calculate the probability of each sentence being extracted
encoder
we use the bert encoder bertenc to vert source document x into a sequence of token dings hx taking cls embeddings as a representation of the source sentences denoted as hc
hx hc h i x
extractor
we use a transformer encoder transenc to convert sentence embeddings hc into nal inter sentence representations hf and calculate the extraction probability on each sentence according to hf
hf p h k f the given sequence extraction where c denotes where extk means the k th sentence extracted and w and are model trainable parameters
of probabilities p the number of sentences in x we make decision on each sentence cording to three hyper parameters the minimum number of sentences to extract min sel the maximum number of sentences to extract max sel and a probability threshold
in particular we sort the c sentences in descending order based on p where sentences that rank between and min sel are selected by default while sentences that rank between min sel and max sel are decided by paring the probability value with the threshold
sentences with a probability above threshold are selected
we decide the hyper parameter values using dev experiments
note that our method is slightly different from the tive model of liu and lapata which extracts the most probable sentences as the summary
for the purpose of rewriting with a strong compression our method allows to extract more sentences as the summary for better recall
source group tagging we match the extracted summary with the original document for group tagging taking each sentence in the extracted mary as a group
so that the rst summary sentence and the matched sentence forms group one the second group two and so on
formally for document x and extractive mary e the k th summary sentence ek


k is matched to x where every token in ek is assigned with a group tag
in particular eq is instantiated as gx gi k if wi ek else where gx is the sequence of group tags for document x
contextualized rewriter the contextualized rewriter extends the abstractive rizer of liu and lapata which is a standard former sequence to sequence model with bert as the coder
as figure shows to integrate group tag guidance group tag embeddings are added to both the encoder and the decoder
formally for an extractive summary e the set of group tags is a closed set of


k
we use a lookup table wg to represent the embeddings of the group tags which is shared by the encoder and the decoder
encoder
the original document is processed in the same way as for the extractive model where a cls token is added for each sentence and interval segments are used to distinguish successive sentences
after bert encoding bertenc the representation of each token is added to the group tag embedding for producing a nal representation embwg gx where embwg gx denotes the retrieved embeddings from the lookup table wg for group tag sequence gx
decoder
summary sentences are synthesized in a single sequence with special token bos at the beginning sep between sentences and eos at the end
the decoder lows a standard transformer architecture
we treat each sentence in the summary as a group
sequently the group tag sequence gy is fully determined by the summary y
in particular all the tokens in the k mary sentence


k are assigned with a group tag
therefore eq is instantiated as gy gj k if wj yk else
during decoding the group tag is generated at each beam search step starting with after the special token bos and increasing by after each special token sep
the embedding of group tag gj is retrieved from the lookup table wg by and added to the token embedding and the position embedding
hy g embwg gy p j x gx j y g where hy g represents the tagged token embeddings and the encoder outputs
the decoder transdec dicts token probabilities on position j based on the tagged token embeddings before position j and the encoder puts consuming as the memory keys and ues for multi head attention
through which the sequence of group tags gy is used by the decoder to address the group tags gx in the encoder so that the k th rewritten sentence corresponds to the k extracted sentence in the document
training we train our extractive summarizer and abstractive rewriter separately on a pre processed dataset labeled with standard extractions
to generate gold standard extraction we match each sentence in human summary to each ment sentence choosing the sentence with the best ing score as the gold extraction for the summary sentence
specically we use the average recall of l as the scoring function which follows wei huang and gao
differing from existing work liu and lapata which aims to nd a set of sentences that maximizes rouge matching with the human summary we nd the best match for each summary sentence
as a result the ber of extracted sentences is the same as the number of tences in the human summary
this strategy is also adopted by wei huang and gao and bae et al

after matching summary y to document x we obtain a gold standard extraction e
for training our extractive model we convert gold standard extraction e into a label lk on each sentence xk in x
we set lk if xk e otherwise lk
we train the model with a binary entropy loss function lext c lk log p c lk p where c denotes the number of sentences in x
for training our abstractive rewriter we convert standard extractions e into group tags gx following eq and train the model with a negative log likelihood loss lwrt log p j x gx
experimental setup we evaluate our model on the cnn daily mail dataset mann et al
which comprises online news articles with several human written highlights on average
per article
there are samples in total
we use the non anonymized version and follow the standard splitting of hermann et al
which includes samples for training for dev testing and for testing
we preprocess the dataset following see liu and manning after splitting sentences with the stanford corenlp manning et al

we tokenize sentences into subword tokens and truncate documents to tokens
we evaluate our models automatically using rouge lin reporting the unigram overlap and the gram overlap as metrics for informativeness and the longest common subsequence rouge l as an indicator of uency
all scores are calculated using pyrouge
extractive summarizer the document encoder is initialized with pre trained cased bert base which has transformer layers and the output embedding size is
the transformer extractor is set to layers with an embedding size of and domly initialized
we use the adam optimizer kingma and ba with
and

the encoder and extractor are jointly trained for a total of steps with a learning rate schedule vaswani et al
lr
step
where warmup
the model is trained with gpus for about hours
for inference we select sentences according to the parameters min sel max sel and threshold
which are chosen by a grid search to nd the best erage score of rouge l on the dev dataset

org project

method rouge l see liu and manning bertsumext liu and lapata extractive abstractive bertsumabs liu and lapata bertsumextabs liu and lapata rnn chen and bansal bert hybrid wei huang and gao bert bae et al
xiao et al
our models bert ext oracle bert abs bert






































table results
the best scores are in bold and cantly better scores are marked with p
t test
ext and abs denotes extractive and abstractive models spectively and rl means reinforcement learning
contextualized rewriter we initialize the document encoder with pre trained uncased bert base model and initialize the decoder randomly
the transformer decoder has layers with an embedding size of and tied input output embeddings press and wolf
we use the adam optimizer and default setting
and

the model is trained for a total of steps with steps for warming up of the coder and steps for warming up of the decoder lrenc
step
enc lrdec

step
dec
we use a learning rate of
for the encoder and
for the decode applying dropout with a probability of
label smoothing szegedy et al
with a factor of
and word dropout bowman et al
with a probability of
on the decoder
we train the model with gpus on a machine for about hours
for inference we constrain the decoding sequence to a minimum length of a maximum length of a length penalty wu et al
with
and a beam size of
during beam search we block the paths on which a repeated trigram is generated namely trigram blocking paulus xiong and socher
results and analysis we compare our models with existing summarization els before analysing the contextualized rewriter
automatic evaluation the results are shown in table
the top section consists of extractive models
the middle section contains abstractive models and hybrid systems with a rewriter
the bottom tion lists our models
in comparison with bertsumext our extractive model bert ext gives lower result due to ferences in the extraction goal as discussed earlier
method rnn bertsumext bertsumextabs bert faith




read




info




conc




table human evaluation on faithfulness faith
informativeness info
and conciseness ability read
conc

compared with the extractive baseline bert ext our model bert improves l by

and
respectively
this shows the effectiveness of contextualized rewriting
to isolate the effect of the rewriter from the extractive rizer we also did an experiment using the oracle to our contextualized tractive summary as the input rewriter as shows
the gap tween our bert result and the result shows the room for further improvement when the extractive summarizer becomes stronger
the row bert abs shows the result of the bert based abstractive summarizer which copies the structure and settings of bert ext contextrewriter excluding the ponents related to group tags in figure
a contrast tween our bert model and abs model shows the usefulness of the extractive summary for guiding abstractive rewriting
compared to the rewriting system bert hybrid our increases l by bert ext contextrewriter

and
respectively
it demonstrates the fectiveness of contextualized rewriting compared to contextualized rewriting
although with the help of inforcement learning a better result can be achieved for the non contextualized rewriting system as the results of bert and the algorithm is inevitably shows increased
compared with the best rewriting system our contextualized rewriter bert ext contextrewriter still shows a signicant provement by

and
on rouge l tively despite that our model is purely generative without copying tokens from the source document
the complexity of compared with the strong extractive model sumext bert ext contextrewriter gives a better score across three rouge metrics with a signicant margin for

and
on l respectively
ering the different length of extractive summaries and ten summaries we normalize rouge scores following sun et al

the relative improvement of our model after normalization is even larger that it improves over sumext by relatively from
to
on ized score compared to the improvement by
relatively from
to
on
to our knowledge we are the rst to report improved rouge scores compared to a state of the art extractive baseline by using abstractive rewriting
human evaluation is given in the next section
we did not include bart lewis et al
in the table which reports l of

and
method oracle contextrewriter ours contextrewriter ours bertsumext tri bloc contextrewriter ours bert ext ours contextrewriter ours rouge l words



































table results of four extractive summarizers applied with contextualized rewriter
tri bloc means trigram blocking
spectively
different pre training method and data are used by bart as compared to the models in table
first we use bert base while bart for summarization uses a large model
second models in table use only the rst kens of the document while bart uses tokens
human evaluation intuitively our model can paraphrase extractive summaries instead of generating summaries from scratch thereby proving the faithfulness
furthermore the abstractiveness of contextualized rewriter can enhance the readability and the strong compression can improve the conciseness
to rm these hypothesis we conduct a human evaluation by randomly select samples from the test set scoring fulness readability informativeness and conciseness from to by independent annotators
we report the nal result by averaging across annotators
the result is shown in table
compared with contextualized rewriter rnn our ized rewriter shows obvious advantage across the four pects
compared to the extractive baseline bertsumext our rewriter enhances the readability informativeness and conciseness with a signicant margin while keeping the faithfulness
the enhancement of readability is mainly tributed by reduced redundancy and improved coherence
the improvement of conciseness conrms the strong pression of the rewriter
in comparison with the abstractive baseline bertsumextabs our rewriter improves fulness and informativeness while keeping the readability and conciseness close
the conciseness of the rewriter is
lower since it generates summaries for about one word longer than the abstractive model on average
however by having more text the rewriter obtains much improved mativeness from
to

universality of the rewriter our contextualized abstractive rewriter can serve as a eral summary rewriter
we evaluate the rewriter with four different extractive summarizers including sumext bert ext and oracle
as table shows the contextualized rewriter improves the summaries generated by all four extractive summarizers
in particular using as a basic extractive summarizer the rouge scores improve by a large margin
even with the best tractive summarizer bertsumext the rewriter still hances the summary quality especially on rouge l with a
point improvement
all the extractive summaries are figure comparison of the ability to generate redundant summaries
extractive summary oratilwe hlongwane whose name is aj is still learning to put together words but the toddler is already able to select and play music from a laptop and has become a phenomenon in south africa
two year old oratilwe hlongwane from johannesburg south africa whose name is aj is still ing to put together words but is already able to play music from a laptop making him a worldwide phenomenon
rewritten summary oratilwe hlongwane whose name is aj is still learning to put together words
he is already able to play music from a laptop making him a worldwide phenomenon
figure example of the ability to reduce redundancy
improved by more than
points on rouge l which dicates a signicant improvement on the uency
in table the rouge scores for bertsumext trigram blocking is much worse than bertsumext cause there is redundant information
however when they are applied with our rewriter they give similar scores where the difference is less than
point across l which is another proof that our rewriter is robust to input of redundant extractive summaries
analysis we further quantitatively evaluate our contextualized rewriter on the ability to reduce redundancy compress tences improve abstractiveness and enhance coherence
redundancy redundancy has been a major problem for automatic summarization
here we study the impact of trigram blocking to the model performance by comparing with the work of liu and lapata
as figure shows when the trigram blocking post process is removed all the models give lower rouge scores
bertsumext ences the most signicant drop while bertsumextabs has a smaller drop because of less redundancy in an tive summarizer
contextrewriter suffers the least drop most halving that by bertsumextabs which shows that the contextualized rewriter effectively reduces redundancy
an example shown in figure demonstrates the ability
compression as the column avg words in table shows for all the four extractive summarizers the tualized rewriter can signicantly compress the summaries
for oracle extractive summaries it compresses the size by almost a half
for the other models it compresses the maries to almost of the original summaries on average
looking into the summaries generated by we nd that of extractive mary sentences are not changed by the rewriter are compressed into shorter versions and are rewritten into new sentences
we obtain these numbers on the test dataset by adopting the edit sequence generation algorithm zhang method gold bertsumextabs bert grams


grams


grams


table percentage of novel n grams
source document a university of iowa student has died nearly three months after a fall


andrew mogni from glen ellyn illinois had only just arrived for


he was own back to chicago via


but he died on sunday



rewritten summary andrew mogni from glen ellyn illinois had only just arrived for a semester program in italy when the incident happened in uary he was own back to chicago via air ambulance on march but he died on sunday swap group tags andrew mogni was own back to chicago via air bulance on march but he died on sunday he had only just arrived for a semester program in italy when the incident happened in january figure example of the ability to maintain coherence
and litman to generate a sequence of word editing actions and mapping an extracted summary sentence to the rewritten one
we categorize a sentence as rewritten if the sequence contains an action of adding or modifying pressed if the sequence contains an action of deleting and unchanged otherwise
according to samples from the test dataset all the compressions are on phrases instead of single words
thermore most removed phrases are unimportant given the fact that only of the removed words are included in erence summaries
for instance they returned to nd reaves and the girl who has not been named lying on top of each other
is compressed into they returned to nd reaves and the girl lying on top of each other
novel n grams as a measure of abstractiveness we culate the percentage of novel n grams as table shows the results of

and

we can see that the textualized rewriter generates summaries with more novel n grams compared to bertsumextabs which suggests better abstractiveness
coherence the text generation process of a alized rewriter can be controlled by the extractive input through which we can observe the behavior of the rewriter
figure uses one output example to demonstrate how the rewriter maintains coherence
we can see that the student name is mentioned in the rst summary sentence while a pronoun is used in the second sentence
as the swap group tags section shows when we swap the group tags in the source document the content of the two summary sentences swap their positions but the student name is still presented in the rst sentence and a pronoun is used in the second sentence
from this case we can see that the cross sentence anaphora is maintained correctly
conclusion we investigate contextualized rewriting of extractive maries using a neural abstractive rewriter formalizing the task as a problem with group alignments using group tags to represent alignments and constraining the tention to rewriting sentence through content based ing
results on standard benchmarks show that using textual information from the original document is highly benecial for summary rewriting
our model outperforms existing abstractive rewriters by a signicant margin ing strong rouge improvements upon multiple extractive summarizers for the rst time
our method of with group alignments is general and can potentially be applied to other nlg tasks
acknowledgments we would like to thank the anonymous reviewers for their valuable feedback
we thank wenyu du for the inspiring discussion
references bae s
kim t
kim j
and lee s


summary level training of sentence rewriting for abstractive in proceedings of the workshop on new marization
frontiers in summarization
hong kong china sociation for computational linguistics
bowman s
r
vilnis l
vinyals o
dai a
jozefowicz r
and bengio s

generating sentences from a tinuous space
in proceedings of the signll ence on computational natural language learning
berlin germany association for computational tics
chen y

and bansal m

fast abstractive marization with reinforce selected sentence rewriting
in proceedings of the annual meeting of the tion for computational linguistics volume long papers
melbourne australia association for tional linguistics
cheng j
and lapata m

neural summarization by extracting sentences and words
in proceedings of the annual meeting of the association for computational linguistics volume long papers
berlin many association for computational linguistics
chopra s
auli m
and rush a
m

abstractive sentence summarization with attentive recurrent neural in proceedings of the conference of the networks
north american chapter of the association for tional linguistics human language technologies
san diego california association for computational guistics
devlin j
chang m

lee k
and toutanova k

bert pre training of deep bidirectional transformers for language understanding
in proceedings of the ference of the north american chapter of the association for computational linguistics human language gies volume long and short papers
neapolis minnesota association for computational guistics
dorr b
zajic d
and schwartz r

hedge mer a parse and trim approach to headline generation
in proceedings of the hlt naacl text summarization workshop
durrett g
berg kirkpatrick t
and klein d

learning based single document summarization with compression and anaphoricity constraints
in proceedings of the annual meeting of the association for tational linguistics volume long papers
berlin germany association for computational tics
garg s
peitz s
nallasamy u
and paulik m

jointly learning to align and translate with transformer models
in proceedings of the conference on ical methods in natural language processing and the international joint conference on natural language cessing emnlp ijcnlp
hong kong china association for computational linguistics
gehrmann s
deng y
and rush a

bottom up in proceedings of the abstractive summarization
conference on empirical methods in natural language processing
brussels belgium association for computational linguistics
graves a
wayne g
and danihelka i

neural ing machines
corr

hermann k
m
kocisky t
grefenstette e
espeholt l
kay w
suleyman m
and blunsom p

in cortes c
ing machines to read and comprehend
lawrence n
d
lee d
d
sugiyama m
and garnett r
eds
advances in neural information processing systems annual conference on neural information processing systems december montreal quebec canada
hsu w

lin c

lee m

min k
tang j
and sun m

a unied model for extractive and tive summarization using inconsistency loss
in ings of the annual meeting of the association for putational linguistics volume long papers
melbourne australia association for computational guistics
kingma d
p
and ba j

adam a method for stochastic optimization
in bengio y
and lecun y
eds
international conference on learning representations iclr san diego ca usa may ence track proceedings
lewis m
liu y
goyal n
ghazvininejad m
hamed a
levy o
stoyanov v
and zettlemoyer l

bart denoising sequence to sequence pre training for natural language generation translation and hension
in proceedings of the annual meeting of the association for computational linguistics
line association for computational linguistics
lin c


rouge a package for automatic ation of summaries
in text summarization branches out
barcelona spain association for computational linguistics
liu y
and lapata m

text summarization with in proceedings of the pretrained encoders
ence on empirical methods in natural language ing and the international joint conference on natural literature
in proceedings of the workshop on methods for optimizing and evaluating neural language generation
minneapolis minnesota association for tional linguistics
szegedy c
vanhoucke v
ioffe s
shlens j
and wojna z

rethinking the inception architecture for puter vision
in ieee conference on computer vision and pattern recognition cvpr
vaswani a
shazeer n
parmar n
uszkoreit j
jones l
gomez a
n
kaiser l
u
and polosukhin i

in guyon i
luxburg u
v
attention is all you need
bengio s
wallach h
fergus r
vishwanathan s
and garnett r
eds
advances in neural information ing systems
curran associates inc
vinyals o
fortunato m
and jaitly n

pointer networks
in cortes c
lawrence n
d
lee d
d
sugiyama m
and garnett r
eds
advances in neural information processing systems annual conference on neural information processing systems december montreal quebec canada
wei r
huang h
and gao y

sharing pre trained bert decoder for a hybrid summarization
in sun m
huang x
ji h
liu z
and liu y
eds
chinese tational linguistics china national conference ccl kunming china october proceedings volume of lecture notes in computer science
springer
wu y
schuster m
chen z
le q
v
norouzi m
macherey w
krikun m
cao y
gao q
macherey k
klingner j
shah a
johnson m
liu x
kaiser
gouws s
kato y
kudo t
kazawa h
stevens k
kurian g
patil n
wang w
young c
smith j
riesa j
rudnick a
vinyals o
corrado g
hughes m
and dean j

google s neural machine translation tem bridging the gap between human and machine lation
in
org

xiao l
wang l
he h
and jin y

copy or rewrite hybrid summarization with hierarchical forcement learning
proceedings of the aaai conference on articial intelligence aaai
zhang f
and litman d

sentence level rewriting detection
in proceedings of the ninth workshop on vative use of nlp for building educational applications
baltimore maryland association for tional linguistics
language processing emnlp ijcnlp
hong kong china association for computational linguistics
manning c
surdeanu m
bauer j
finkel j
bethard s
and mcclosky d

the stanford corenlp natural language processing toolkit
in proceedings of nual meeting of the association for computational tics system demonstrations
baltimore maryland association for computational linguistics
mihalcea r
and tarau p

textrank bringing order into text
in proceedings of the conference on pirical methods in natural language processing
barcelona spain association for computational tics
nallapati r
zhai f
and zhou b

summarunner a recurrent neural network based sequence model for tractive summarization of documents
in singh s
p
and markovitch s
eds
proceedings of the thirty first aaai conference on articial intelligence february san francisco california usa
aaai press
nallapati r
zhou b
dos santos c
c
and xiang b

abstractive text summarization using sequence to sequence rnns and beyond
in proceedings of the signll conference on computational ral language learning
berlin germany ation for computational linguistics
narayan s
cohen s
b
and lapata m

ing sentences for extractive summarization with ment learning
in proceedings of the conference of the north american chapter of the association for putational linguistics human language technologies ume long papers
new orleans louisiana association for computational linguistics
paulus r
xiong c
and socher r

a deep reinforced model for abstractive summarization
corr

press o
and wolf l

using the output embedding to improve language models
in proceedings of the conference of the european chapter of the association for computational linguistics volume short papers
valencia spain association for computational guistics
rush a
m
chopra s
and weston j

a neural tention model for abstractive sentence summarization
in proceedings of the conference on empirical methods in natural language processing
lisbon gal association for computational linguistics
see a
liu p
j
and manning c
d

get to the point summarization with pointer generator networks
in proceedings of the annual meeting of the tion for computational linguistics volume long papers
vancouver canada association for tional linguistics
sun s
shapira o
dagan i
and nenkova a

how to compare summarizers without target length pitfalls solutions and re examination of the neural summarization
