using local knowledge graph construction to scale models to multi document inputs angela fan fair loria claire gardent cnrs loria chloe braud cnrs loria antoine bordes fair angelafan
com claire
gardent chloe

fr t c o l c
s c v
v i x r a abstract query based open domain nlp tasks require information synthesis from long and diverse web results
current approaches tively select portions of web text as input to sequence to sequence models using ods such as tf idf ranking
we propose constructing a local graph structured edge base for each query which compresses the web search information and reduces dundancy
we show that by linearizing the graph into a structured input sequence models can encode the graph representations within a standard sequence to sequence setting
for two generative tasks with very long text put long form question answering and document summarization feeding graph resentations as input can achieve better mance than using retrieved text portions
introduction effective information synthesis is at the core of many natural language processing tions such as open domain question answering and multi document summarization
in such tasks a fundamental challenge is the ability to distill evant knowledge from hundreds of thousands of tokens of noisy and redundant input such as pages
current approaches predominantly conduct tf idf based information extraction to identify key portions of the information and then provide this as sequential input to a sequence to sequence model
the sub selected portions are limited to a few thousand words as models often struggle to encode much longer sequences
in this work we propose a mechanism to structure free text into local knowledge graphs that are then linearized into sequences creating a canonical form in which information is presented to models
by constructing a graph ary redundant information can be merged or carded producing substantially compressed input figure multi document input to linearized graph multi document input resulting from web search queries are converted to a graph structured knowledge base using erence resolution and information extraction then linearized into a sequence for models
color indicates erence resolution
node weight is indicated by circle radius and edge weight by line thickness
small enough to be fully encoded by models
such a method can be seen as merging previous work on symbolic knowledge bases for information extraction with newer approaches ing deep neural networks to encode knowledge
our approach shown in figure takes a query and its corresponding multi document web search results and builds for each query a specic local knowledge graph
we present several modeling contributions to effectively encode the entire graph as a sequence and attend to the most relevant tions within this linearization
we demonstrate the effectiveness of this approach on two scale generative tasks with both long and noisy multi document web input and paragraph length output long form question answering on the dataset fan et al
and wikipedia lead graph generation as a multi document tion problem liu et al

related work interest in generative sequence modeling has tensied due to recent improvements peters et al
devlin et al
radford et al
making the challenge of information synthesis more relevant
in contrast to extractive tasks which only require models to identify spans and can do so effectively on long documents by ing at the paragraphs independently generative quence models must combine multiple pieces of evidence from long and noisy multi document put to generate correct and convincing responses

multi document input previous work in multi document summarization barzilay et al
applies various techniques to handle long input including clustering to nd similar information honarpisheh et al
extractive methods to select relevant sentences daume iii and marcu gillick and favre berg kirkpatrick et al
di fabbrizio et al
bing et al
cao et al
cluding maximal marginal relevance fabbri et al
and incorporating queries baumel et al
and graphs ganesan et al
yasunaga et al

however there are few large scale multi document summarization datasets and many approaches have focused on extractive selection or hybrid extractive abstractive models
in this work we use graph construction to re structure document input for abstractive generation
advancements in question answering have amined performance on datasets with document input such as triviaqa joshi et al

various approaches have been proposed including leveraging tf idf and bigram ing with an rnn to nd relevant information chen et al
models that score individual paragraphs for sub selection clark and gardner and nearest neighbor search with paragraph re ranking das et al

however these approaches have been applied to extractive tion answering tasks that require span tion rather than abstractive text generation in an information synthesis setting

using knowledge bases previous work has explored various ways of resenting information in knowledge bases al
and improving these tions chen et al

knowledge bases have been leveraged to improve performance on various tasks from coreference resolution ng and cardie and question answering zheng bao et al
cui et al
sun et al
to signal processing et al

various works convert text into abstract meaning sentations liu et al
for domains such as news vossen et al
rospocher et al
and link nodes to large knowledge bases such as dbpedia auer et al

wities et al
combine open information extraction with erence and lexical inference to build knowledge representations
they apply this to tweets and analyze the accuracy on various aspects of graph construction
das et al
construct graphs from procedural text to track entity position to swer when and if entities are created destroyed or moved
in contrast we build graphs from tially longer multi document input and use them for multi sentence text generation
recently many have explored neural tectures that can encode graph structured input bruna et al
kipf and welling beck et al
zhou et al
xu et al
lai et al

these models often represent graphs as adjacency matrices to generalize architectures such as convolutional networks to graph inputs
rather than encoding a static knowledge graph or leveraging external knowledge graphs we build a local graph for each query and model these using standard models
we leave the ration of graph networks for future work
graph construction we describe how symbolic graph representations of knowledge can be constructed from text
our approach assumes a multi document input such as web pages resulting from the execution of a query
the graph construction process presses the web search input to a signicantly smaller size allowing models to encode the tirety of the compression and reduces dancy through merge operations allowing relevant information to be more easily identied
tions
edges are merged similarly with existing edges between the same two nodes
such merge operations allow strings such as the nobel prize and nobel prize to be represented as one node rather than separately
similarly coreference olution aids in merging by identifying that bert einstein and he refer to the same entity and thus merging them the construction of the graph reduces redundancy
the size of the graph can be modied by controlling which triples are added using tf idf overlap see figure step
idf overlap of the triple with the question can be used to determine if the triple contains relevant formation
this improves robustness to noisy web search input and helps lter entirely irrelevant tions such as scraped html tags
modeling graphs as sequences current models for text generation often use architectures such as the transformer vaswani et al

these models are signed to encode sequences rather than graphs
we describe now how to convert a graph into a structured input sequence
our complete model will take as input a linearized graph by coding graph attributes such as node and edge weight as embeddings
we add hierarchical and memory compressed attention mechanisms to scale models to encode the full graph and attend to the most relevant information within it figure and nally we integrate the benets of language modeling using multi task training

graph to sequence for example linearization to represent the graph as a quence for it is linearized into a tured standard form of subject node object node and predicate edge separated by indicator tokens as shown in figure
the earization sub albert einstein obj the bel prize pred won would be created
the earization is accomplished through graph sal in a manner following the directed edges formed by predicates and starting from the node with the largest weight as the root
for two nodes that are connected by multiple cates the predicates are concatenated shown in figure so a linearization such as pred won s received would indicate that albert einstein both won and received the nobel prize
figure steps of graph construction
color relates the document sentence used to produce the graph output
text to triples to graph graph construction proceeds in several steps outlined in figure
we apply coreference resolution clark and ning and open information extraction stanovsky et al
to convert sentences into a triple of the form subject predicate object
the sentence albert einstein a german cal physicist won the nobel prize would become albert einstein won the nobel prize
a graph is constructed using the triples by resenting subjects and objects as nodes connected by predicates as directed edges
for example the the triple would become albert einstein won bel prize
nodes and edges have a name property that is the text they represent
they also have a weight property that denotes the number of times that node or edge has appeared
for example in figure the node with name albert einstein has weight and edge with name won has weight
merging nodes and edges when subsequent triples are added to the graph they are merged with the existing graph if they already exist to duce information replication
to merge nodes the tf idf overlap of the new node s name is lated with the existing graph node names and the new node is merged into an existing node if the tf idf is higher than some threshold see ure steps and for example merge use the implementation available here github
com huggingface neuralcoref use the implementation available here
com supervised oie figure graph attribute embeddings
in addition to word and position embeddings models receive a graph weight embedding to encode node and edge weight and a query relevance embedding that encodes search result rank
mechanism for models to scale the graph dings
we denote the embedding for position t as et such that eword is the word embedding
t for the gw embedding models learn a gating function g based on the word and gw dings
such a mechanism provides capacity for the model to decide when the additional embeddings are useful based on the words in the input
the gate is calculated by applying an mlp w to the concatenation of the word and gw embeddings
the learned gate is then applied to gw dings to create the output h ggw t w egw t ggw hgw eword t t egw t t models learn a gating mechanism for the qr embedding in a similar manner
the full ding the model receives is as follows t epos eword t hgw t hqr t
hierarchical attention one challenge in modeling long sequences is that attention mechanisms struggle to make sharp lections when softmax ing over long sequences fan et al

when attention is blurry there lacks a strong distinction between noise and vant information
we assume that graphs are constructed from query based web search input and thus leverage this query to learn a subselection operation using hierarchical top k attention depicted in figure
the query is encoded with a transformer encoder and the linearized graph with another transformer encoder
we model the interaction between the query and the input sequence e

web search results or linearized graph by computing an tention distribution between the question and the input then selecting the top k positions with the most attention weight
such a mechanism can be thought of as building a query dependent sentation of the most relevant knowledge which is commonly done in question answering tures like bidaf seo et al

the top figure model architecture
gray indicates standard transformer elements green indicates modication
encoding graph information transformer models have two embeddings a word embedding and a position embedding
simply linearizing the graph however loses attribute information such as node and edge weight
instead we encode these attributes as embeddings in addition to the word and position embeddings
to represent graph weight gw node and edge weight is provided as an embedding for each token
the node weight and edge weight are equivalent to the number of merge operations
for example if albert einstein occurred times in the text the gw embedding for the tokens albert and einstein would be as shown in figure
we encode a query relevance qr embedding to represent the relevance of the web search to the query as ranked by the information retrieval tem e

search engine
information from the top web search results is likely more relevant than formation from the last web search results so viding this embedding allows the model to guish between these different information sources
in figure tokens representing sentences from the rst document have qr embedding and kens from the second document have value
models now have access to several different types of embeddings but all embedding mation contributes equally as there is no nism to distinguish between them
we introduce a operation limits the number of tokens making the attention mechanism sharper

scaling to encode the graph recent progress has improved the ability of language models to process longer sequences sukhbaatar et al
dai et al
but models remain limited in their capacity to code long documents
the multi document sults of query based web search have hundreds of thousands of tokens beyond the limit of current models to handle
for example the dataset provides an average of k tokens of web search input
however by compressing the web search results into a knowledge graph we icantly reduce the number of tokens by an order of magnitude and make it possible for a model to access the entirety of the search information
to represent the full graph models must scale to encode around k input tokens
the attention mechanism in transformer architectures becomes computationally expensive for sequences of this length
instead we experiment with the compressed attention mca mechanism posed for language models in liu et al
and apply it to the encoder side of els
at each self attention layer mca alternates between local attention computed between smaller chunks of tokens and strided tions to reduce the number of keys and values used in attention computation
by adding the mca mechanism to the encoder e mca we are able to encode the complete linearized graph

multi tasking with kb completion fan et al
used multi task training on guage modeling and various tasks to corporate the benets of language modeling in models
we extend this by training ditionally on knowledge graph completion
els receive at training time sequences of a earized graph with nodes edges or both tively masked and must predict the missing tent words
for example models might receive as input sub albert einstein obj mask pred won and need to predict the nobel prize
this can be seen as both a multi word extension of the masked language model training proposed in devlin et al
and as learning the task of liu et al
the mechanism is termed dmca as it is applied on the decoder side knowledge base completion lacroix et al
bordes et al

at training time the tasks are distinguished using an indicator token in the input
experimental setup we evaluate our approach on two datasets with multi document web input and multi sentence stractive output
we use models that leverage a query concatenated with web search sults that have been processed into a supporting document e

tf idf subselection linearized graph
to generate long output

datasets and evaluation first we experiment with the explain like i m five fan et al
question ing dataset of k complex questions paired with multi sentence explanatory answers words on average
to facilitate question answering the dataset provides the top web search hits from querying the question which results in k words on average
see appendix for examples
previous work fan et al
used tf idf to nd sentences in the web search that have the largest overlap with the question and created a idf extraction of about words as input for instead we construct a local models
knowledge graph for each question from the web search hits
following the average length of the tf idf support document constructed in fan et al
we experiment with modeling the rst n tokens of the linearized graph then scale to encode the entire graph using e mca
wikisum second we experiment on the isum commoncrawl liu et al
rization with
million examples
this task formulates wikipedia lead paragraph tion as a multi document summarization problem where the paragraph must be generated using the cited article references and other queried content from web search
the query used is the title of the wikipedia article
see appendix for examples
previous work liu et al
applied idf ranking to order the paragraphs of web search given a query
models receive the ordered paragraphs ranked by tf idf as input
liu et al
model the rst n words of this re ranking and then n using
com tree master wikisum mca
we construct the knowledge graph for each wikipedia article from the rst k words of the ranked web search and experiment with encoding and tokens
evaluation both tasks evaluate the sentence generation output against the gold output using rouge
on wikisum we report only rouge l following liu et al

we conduct a comparative human evaluation on the dataset
we use crowdworkers to ine the responses of two models on different questions from the test set
for each question evaluators are shown two answers and asked to choose the one they prefer
to reduce variance answers are standardized at words each

training and generation to reduce the vocabulary size of varied web ument content we apply byte pair encoding nrich et al
to generate k codes for each dataset
we implement our models in py ott et al
using the transformer big architecture and training schedule described in vaswani et al

detailed parameters are listed in the appendix
for generation we use beam search with beam size and tune a minimum and maximum length on the validation set

baselines we compare our results to the transformer quence models presented in fan et al
for and liu et al
for wikisum
we evaluate three additional baseline models sentence selection with maximal marginal relevance fan et al
used tf idf to identify relevant sentences in the web ments to form a support document of around words
however recent work fabbri et al
has shown that using maximal marginal relevance is an effective strategy for selecting relevant information while reducing redundancy
we explore using mmr to lect sentences from the web text to nate to form a support document
multi task triples to examine the impact of solely restructuring the input into open ie triples but not leveraging graph length of provided web input is around k words and maximum length is around k words model q d to a tf idf q d to a mmr multi task multi task triples multi task trip
q d to a graph multi task graph attention e mca input length avg avg avg avg k rouge

















l








table answer generation on using els receiving the question and a support document e

tf idf selection triples linearized graph to produce the answer
denotes results from fan et al

model t d to p lm d mca t d to p multi task multi task graph attention e mca inputlen rouge l k k






lm d mca
k
table lead paragraph generation on wikisum moncrawl using models receiving the title and a support document e

tf idf ranking linearized graph to produce the lead paragraph
denotes results from liu et al
that use data scraped from unrestricted web search not the static commoncrawl version
construction to reduce redundancy we periment with a triples only baseline
the triples are concatenated to form the input
multi task top triples as an ternative to using graph construction to press the full set of open ie triples we plore using tf idf overlap with the query to nd the most relevant information
we select the top triples to concatenate as input
results we examine the performance of our proposed proach and the choices made in graph construction and modeling
we analyze the quality of the pression created by graph construction and the bustness and interpretability of this process

linearized graph improves performance in table we compare our methods to various baselines on the dataset
using mmr to lect the most relevant non redundant input is ilar to the tf idf baseline from fan et al

the multi task triples baseline izes the input by forming triples but does not move redundant triples
it produces marginally better results compared to the baseline multi task model
sub selecting to the top triples is harmful as similar text has high tf idf lap with the query so redundant information is lected
creating the graph structure brings an provement of around

similar trends are seen for the wikisum dataset in table where graph construction improves the multi task model by

these provements are statistically signicant at the condence level
for both datasets a further improvement is seen by using the hierarchical attention mechanism to attend to only the most relevant information in the linearized graph input
for it brings an ditional
improvement and on isum a
improvement
by using mca to scale models to code the entire graph further gains can be seen
particularly in information synthesis tasks prior work has shown the importance of reading more information
liu et al
achieved a point rouge improvement by reading k tokens in our setting e mca improves stead of
our results around
rouge on and
rouge on wikisum
we display random ations from both datasets in the appendix
we use human evaluation to compare the task baseline to the multi task graph top tention model

of evaluations prefer the multi task graph top k attention model
we conduct a two tailed binomial test and nd this sult is statistically signicant with p


analysis of modeling choices ablation on model components table quentially removes the graph embeddings the knowledge base completion multi tasking and the multi tasking from fan et al
and reveals that each of these is important for performance
graph attribute embeddings table plays the effect of removing the graph attribute embeddings and gating mechanism
removing each is slightly harmful and the combination of all three together provide the best performance
more web search documents figure right shows that graph construction with more web model iterative removal of model components multi task graph graph embeddings kb completion multi tasking lm multi tasking from fan et al
removing graph embedding components graph gated graph weight query relevance no graph weight embedding no query relevance embedding no gating varying number of hits in graph multi task graph top k attention e mca with graph on search hits with graph on search hits with graph on search hits with graph on search hits varying k in hierarchical top k atttention multi task graph e mca top k top top top k top k
















table ablations on the validation set model q d to a q d to a shufe q d to a graph web shufe input tf idf web web shufe web shufe




table importance of web search relevance on tion for modeling input words
search information is important for answer token coverage
the graph on the top search hit alone is missing of the answer tokens but this creases as more search hits are used
table indicates that this lack of coverage of the answer tokens correlates with generation quality
models receiving a graph built on the rst search hits alone are substantially worse than all hits
top k attention table shows the effect of the top k hierarchical attention mechanism for various values of k
attending to too many tokens lowers rouge for the task of writing proximately word answers attending to input tokens likely means the model is focusing on irrelevant information and tokens is too few
figure interpretable attention of models on a subgraph when answering a question in figure left distribution of number of nodes middle number of edges right weight of the largest node in graph construction on the training set
search results is only missing
of tokens but has around k words
when analyzing just the rst tokens to match the average length of the tf idf extraction the graph is better only ing of tokens
further the merging and carding operations done during graph construction do not have a large effect on answer token age the full set of triples marginally reduces the percentage of answer tokens missing to
instead of

this indicates that much of the information in the full set triples is redundant and unnecessary for good answer token coverage

graph representation is more robust to poor search relevance ordering we analyze the robustness of our approach to the ordering of web search results in table
instead of constructing the graph from the rst web search result to the last we shufe the web search results and construct the graph on this shufed input
we compare this to modeling the web search results directly no tf idf retrieval and a model that ceives this shufed web search input
the graph is more robust to shufing as more information can be encoded in the graph due to its compression effect the search hit ordering is less critical
figure left graph construction drastically reduces input size by an order of magnitude
right graph construction encodes more tokens present in the answer compared to idf extraction and building the graph from more search hits increases answer token coverage
analysis on for both plots

graph improves answer token coverage despite compression figure displays the distribution of the number of nodes edges and the largest node weight for each local graph built on the dataset
the web search results are compressed to a few hundred nodes
by merging redundancy and ming irrelevant triples from the graph the input is reduced by an order of magnitude figure left
despite compression the graph retains more answer tokens than tf idf subselection
ure right displays the percentage of answer kens not present in the input
the tf idf traction from fan et al
is missing of tokens
the graph constructed on all web
interpretable attention on subgraphs figure shows an example of the nodes and edges the model focuses upon most when answering a question on
to construct this tion we calculate the top nodes the model attends to and then their top edges
the model tion on a sub portion of the linearized input can be visualized as an interpretable graph that sponds well to the model s generated answer
for example the relationship general relativity is einstein s theory becomes the generated sentence general relativity is a theory of albert einstein
conclusion many open domain nlp tasks rely upon document input from the web to facilitate tasks such as answering questions or writing summaries but current approaches struggle to encode the tirely of this information
we propose ing one knowledge graph per query and show that this method compresses information and reduces redundancy
we show on two abstractive tion tasks that using the linearized graph achieves better performance than tf idf retrieval
references soren auer christian bizer georgi kobilarov jens lehmann richard cyganiak and zachary ives

dbpedia a nucleus for a web of open data
in the semantic web pages
springer
junwei bao nan duan ming zhou and tiejun zhao

knowledge based question answering as in proceedings of the chine translation
nual meeting of the association for computational linguistics volume long papers volume pages
regina barzilay kathleen r mckeown and michael elhadad

information fusion in the context of multi document summarization
in proceedings of the annual meeting of the association for putational linguistics
tal baumel matan eyal and michael elhadad

query focused abstractive summarization rating query relevance multi document coverage and summary length constraints into els
arxiv preprint

daniel beck gholamreza haffari and trevor cohn

graph to sequence learning using arxiv preprint gated graph neural networks


taylor berg kirkpatrick dan gillick and dan klein

jointly learning to extract and compress
in proceedings of the annual meeting of the ciation for computational linguistics human guage technologies volume pages
sociation for computational linguistics
lidong bing piji li yi liao wai lam weiwei guo and rebecca j passonneau

abstractive document summarization via phrase selection and merging
arxiv preprint

antoine bordes jason weston ronan collobert and yoshua bengio

learning structured in twenty fifth aaai dings of knowledge bases
conference on articial intelligence
joan bruna wojciech zaremba arthur szlam and yann lecun

spectral networks and cally connected networks on graphs
arxiv preprint

jurgen martin pahl o stahlhut and c e liedtke

a knowledge based system for text dependent evaluation of remote sensing data
in joint pattern recognition symposium pages
springer
ziqiang cao wenjie li sujian li and furu wei

improving multi document summarization via text in thirty first aaai conference on classication
articial intelligence
danqi chen adam fisch jason weston and antoine bordes

reading wikipedia to answer domain questions
in acl
danqi chen richard socher christopher d ning and andrew y ng

learning new facts from knowledge bases with neural tensor works and semantic word vectors
arxiv preprint

christopher clark and matt gardner

simple and effective multi paragraph reading sion
arxiv preprint

kevin clark and christopher d manning

deep reinforcement learning for mention ranking ence models
arxiv preprint

kevin clark and christopher d manning

proving coreference resolution by learning arxiv preprint level distributed representations


wanyun cui yanghua xiao haixun wang yangqiu song seung won hwang and wei wang

kbqa learning question answering over corpora and knowledge bases
proceedings of the dowment
zihang dai zhilin yang yiming yang william w cohen jaime carbonell quoc v le and ruslan salakhutdinov

transformer xl attentive guage models beyond a xed length context
arxiv preprint

rajarshi das shehzaad dhuliawala manzil heer and andrew mccallum

step retriever reader interaction for scalable domain question answering
rajarshi das tsendsuren munkhdalai xingdi yuan adam trischler and andrew mccallum

building dynamic knowledge graphs from text ing machine reading comprehension
arxiv preprint

hal daume iii and daniel marcu

a channel model for document compression
in ceedings of the annual meeting on association for computational linguistics pages
sociation for computational linguistics
jacob devlin ming wei chang kenton lee and kristina toutanova

bert pre training of deep bidirectional transformers for language ing
corr

giuseppe di fabbrizio amanda stent and robert gaizauskas

a hybrid approach to document summarization of opinions in reviews
in proceedings of the international natural guage generation conference inlg pages
alexander r fabbri irene li tianwei she suyi li a and dragomir r radev

multi news large scale multi document summarization dataset and abstractive hierarchical model
arxiv preprint

angela fan yacine jernite ethan perez david ier jason weston and michael auli

in proceedings of long form question answering
acl
angela fan mike lewis and yann dauphin

erarchical neural story generation
in acl
kavita ganesan chengxiang zhai and jiawei han

opinosis a graph based approach to tive summarization of highly redundant opinions
in proceedings of the international conference on computational linguistics coling pages
dan gillick and benoit favre

a scalable global in proceedings of the model for summarization
workshop on integer linear programming for ral langauge processing pages
association for computational linguistics
mohamad ali honarpisheh gholamreza sani and ghassem mirroshandel

a document multi lingual automatic summarization in proceedings of the third international system
joint conference on natural language processing volume ii
mandar joshi eunsol choi daniel weld and luke zettlemoyer

triviaqa a large scale distantly supervised challenge dataset for reading sion
in acl
thomas n kipf and max welling

supervised classication with graph convolutional networks
arxiv preprint

timothee lacroix nicolas usunier and guillaume obozinski

canonical tensor decomposition arxiv preprint for knowledge base completion


yuxuan lai yansong feng xiaohan yu zheng wang kun xu and dongyan zhao

lattice cnns for matching based chinese question answering
arxiv preprint

fei liu jeffrey flanigan sam thomson norman sadeh and noah a smith

toward tive summarization using semantic representations
arxiv preprint

peter j
liu mohammad saleh etienne pot ben goodrich ryan sepassi lukasz kaiser and noam shazeer

generating wikipedia by rizing long sequences
in iclr
vincent ng and claire cardie

improving chine learning approaches to coreference resolution
in proceedings of the annual meeting on sociation for computational linguistics pages
association for computational linguistics
myle ott sergey edunov alexei baevski angela fan sam gross nathan ng david grangier and fairseq a fast extensible michael auli

in proceedings of toolkit for sequence modeling
naacl hlt demonstrations
matthew e
peters mark neumann mohit iyyer matt gardner christopher clark kenton lee and luke zettlemoyer

deep contextualized word resentations
in naacl
alec radford jeffrey wu rewon child david luan dario amodei and ilya sutskever

language models are unsupervised multitask learners
marco rospocher marieke van erp piek vossen antske fokkens itziar aldabe german rigau aitor soroa thomas ploeger and tessel bogaard

building event centric knowledge graphs from news
journal of web semantics
rico sennrich barry haddow and alexandra birch

neural machine translation of rare words with subword units
in acl
minjoon seo aniruddha kembhavi ali farhadi and hannaneh hajishirzi

bidirectional attention ow for machine comprehension
in iclr
gabriel stanovsky julian michael luke zettlemoyer and ido dagan

supervised open information extraction
in proceedings of the conference of the north american chapter of the association for computational linguistics human language technologies volume long papers pages
sainbayar sukhbaatar edouard grave piotr janowski and armand joulin

adaptive tention span in transformers
haitian sun bhuwan dhingra manzil zaheer kathryn mazaitis ruslan salakhutdinov and william w hen

open domain question answering ing early fusion of knowledge bases and text
arxiv preprint

ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan n gomez ukasz kaiser and illia polosukhin

attention is all you need
nips
piek vossen tommaso caselli and yiota zopoulou

storylines for structuring massive streams of news
in proceedings of the first shop on computing news storylines pages
rachel wities vered shwartz gabriel stanovsky meni adler ori shapira shyam upadhyay dan roth eugenio martnez camara iryna gurevych and ido dagan

a consolidated open edge representation for multiple texts
in ings of the workshop on linking models of cal sentential and discourse level semantics pages
kun xu lingfei wu zhiguo wang yansong feng michael witbrock and vadim sheinin

graph to sequence learning with arxiv preprint attention based neural networks


michihiro yasunaga rui zhang kshitijh meelu ayush pareek krishnan srinivasan and dragomir radev

graph based neural multi document summarization
arxiv preprint

zhiping zheng

question answering using web in proceedings of the news as knowledge base
tenth conference on european chapter of the sociation for computational linguistics volume pages
association for computational guistics
jie zhou ganqu cui zhengyan zhang cheng yang zhiyuan liu and maosong sun

graph ral networks a review of methods and applications
arxiv preprint

appendix
dataset examples we display input and target examples for and wikisum in figure and figure respectively

generation examples we display examples of model generations for and wikisum selected randomly in ure and figure
similar to liu et al
we observe that our models are also ble of generating full wikipedia pages
we display randomly sampled examples of wikipedia article generation in figure

implementation details training we train models using the former big architecture with the following eters dropout
attention dropout
an verse square root learning rate schedule with tial learning rate for warmup warmup updates and a minimum learning rate of
we train with and small batchsize to scale to longer sequences
to offset the batchsize crease we increase the update frequency of the gradient updates
generation we generate using beam search of size
we tune a minimum and maximum length at validation time and use minimum length and maximum length
we use a n gram straint of n following fan et al

question why consumers are still so terried of genetically modied organisms gmos yet there is little debate in the scientic community of whether they are safe or not
scientists are for gmos beginning of web search the controversial safety of gmos and the skepticism of the use of gmos college paper writing service url
diamond chemicals plc the merseyside
appendix f research question for
antisocial personality disorder affects family relations and interactions
the controversial safety of gmos and the skepticism of the use of gmos
gmo facts what is a gmo genetically modied organisms the safety of gmos is unknown poll skepticism of genetically modied foods abc news abc news network june web fernandez cornejo jorge and seth james wechsler
the controversy over gmos particularly in food continues scientists are split pros and cons of gmo s september and environmentalists and consumer groups remain unconvinced about the safety of gmos
the controversy around genetically modied foods on the surface food evolution is a resetting of the controversial conversation around genetically modied organisms gmos we just ask people by a show of hands to tell us are they concerned about gmos for their own safety or the
when gmos are the movie star can documentaries on controversial science be entertaining and the message is that gmo food is safe to eat and that naysayers are
genetically modied organisms gmos the top ve anti gmo tropes gmos are genetically modied organisms the evidence on gmo safety by ramez naam
genetically modied organisms what are gmos with the use of gm technology pure and safe equivalents can be produced using gmos and industrial scale
here s a bullet point summation of what nathanael johnson learned about gmos in gmo questions animal vegetable controversy by pretty darn safe
the controversy surrounding genetically modied organisms what do we tolerate as far as detrimental this would be a profound service to scientic skepticism with regards to gmos current gmos are safe to however


target answer there is little difference in essence between what is called gmo now and the techniques we have been using to domesticate and cultivate the food in the past
its an arbitrary line that s been drawn in the sand and people fail to realize this often
that being said i think it is more then wrong the patenting of crops and again even more then wrong to genetically modify crops to not have viable seeds so that seed washing ca nt be used to grow the next crop
so the real god damned issues are being masked and lost by this retarded polemic between gmo and more conventional genetic modication of organisms
figure example of dataset input output title dwight h
perkins economist beginning of web search memorial minute adopted by the faculty of arts and sciences harvard university abram bergson john kenneth galbraith carl kaysen raymond vernon dwight h
perkins chairman
ed was a generous and understanding man who could see the good qualities in people and bring them out
he was always determined and persistent which meant that he also saw to completion what he started
the list of projects large and small that he led is long
in he was one of the authors of the speech of secretary of state james byrnes in which the secretary announced the return of responsibility for the german economy to the germans
in he together with ray vernon did a pioneering study of the new york metropolitan region a study that tried to identify the economic political and social forces that were shaping that vast urban area
at the time the problems of urban areas were mainly the concern of architects and philosophers almost no economist and only a few political scientists knew much or cared greatly about such issues
the studies that resulted provided rich fare for a generation of urban planners to follow
mason edward s
harvard square library home biographies mason edward s
mason edward s
edward sagendorph mason was a member of the rst parish unitarian church in harvard square who exemplied liberal religion in both thought and action
in addition to his notable contributions to the science of government he served as chairman of the sloan commission on cable communication which issued recommendations for the future on the cable



target lead paragraph dwight heald perkins born in chicago illinois in is an american academic economist sinologist and professor at harvard university
he is the son of lawrence bradford perkins architect and margery blair perkins and the grandson of dwight heald perkins the architect for whom he was named
he married julie rate perkins in and they have three adult children
early life perkins earned an undergraduate degree at cornell university in
after two years military service in the us navy perkins resumed his studies at harvard
he earned a ma in economics in and a ph

in economics in
his doctoral thesis was price formation in communist china
academic career perkins teaching career at harvard began when he was still a graduate student and continued uninterrupted through when he became a research and emeritus professor
he was a member of both the department of economics of the faculty of arts and sciences at harvard and of the harvard kennedy school



figure example of wikisum commoncrawl dataset input output question why is touching microber towels such an uncomfortable feeling true answer do you mean the kind of cloths used to clean glasses and lenses i ve never noticed any uncomfortable feeling myself but i do nd touching certain cleaning cloths can be quite uncomfortable
there s a brand called e cloth which market themselves as not needing any cleaning supplies
basically the threads have tiny little hooks and barbs in them so dragging them over skin makes you feel like it s catching against you
generation the microber is made up of a bunch of tiny bers that are attached to them
when you touch them the bers that make up the microber are attracted to each other
when they touch they re attracted to the other end of the ber which is what makes them feel uncomfortable
question what happened to needing www in front of an address true answer explaining like if you were you know when you send a package in the mail you write the address and maybe if the package is fragile you ll stamp fragile all over it
why do nt you also stamp not fragile on the packages that are not fragile because it s implicit
if you do nt say anything the mailman knows it s not fragile
same thing goes for the www
since you can have multiple services on your domain ex url which means you want to share les or url which means you want to access the mail server on that domain in the beginning you d also write url to state that you wanted to access the html of the server
as the web evolved and of the average user wants to access the html version of the website website owners no longer require you to type url instead if you type url they assume you want to access the html version
just like you do nt need to stamp non fragile on your package you do nt need to type www
everyone assumes that s what you want if you say nothing otherwise
generation the www is a subdomain name that is used to refer to a web page
it s a name used to describe the web page that you re looking for
for example if you want to go to url you can go to url and then go to url
the www is the name of the site that you are looking for and it s the name that you were looking for in the rst place
so if you wanted to go back to url you could go to url and go to url and the www would be the same as the www but it would be a different name for the site
so if the www was the same you would be looking for a different site and you would nt be looking at the same site so you would have to go through the www
if the www were the same and you were nt looking for the same page you d have to look for the different sites and so on
but if you were to go from url you might have to use the www as a reference to the site but you d still have to have a www
question why do we have nightmares when stressed out true answer i can tell you that your mind and in particular your dreams are subject to the input that it the journey of your life as you have interpreted it
yours dreams tend to be your subconscious representing issues you have within whether it be past or present with things and people that are sort of preinterpreted symbology from your deep and ever power subconsciousness
all your nightmares are just dreams and your inner mind knows exactly can mess with you the most and sometimes it brings that to light it usually being an issue you may be stressed about
generation when you re stressed your brain releases a hormone called cortisol
this hormone is released by your adrenal glands and is released when you are stressed
this is why it s called ght mode
when your body is stressed it releases hormones that make you feel more stressed
the reason why you do nt feel stressed is because your brain is trying to get rid of the stress hormone which causes you to feel stressed and you re less likely to get stressed
if you re really stressed you re probably going to be more stressed than if you were nt stressed
figure random examples of question generated answer pairs in title wylie rogers true paragraph wylie rogers born march is an american ice hockey goaltender
he played with the rio grande valley killer bees of the central hockey league
prior to turning professional rogers attended the university of alaska fairbanks where he played four seasons of ncaa men s division i ice hockey with the alaska nanooks men s ice hockey team
rogers played the season in the netherlands with the heerenveen yers
generation wylie rogers is an american football coach and former player
he is currently the head football coach at the university of north carolina at chapel hill
rogers played college football at north carolina state university
he was a member of the north carolina tar heels men s football team that won the ncaa division i football championship in
he also played for the university s men s basketball team
after his playing career ended rogers was hired as the head coach at unc chapel hill in
he led the tar heels to their rst ncaa division ii championship in
true paragraph mustabad is a village and mandal in siricilla rajanna district in telangana state of south east india
musthabad is the main town for the mustabad mandal
musthabad is located km distance from its district main city siricilla
it is located km distance from its state main city hyderabad
geography mustabad is located at
n
e
generation mustabad is a village in the indian state of telangana
it is located in the region of mustabad mandal
mustabad was founded in the early by a group of local businessmen who wanted to create a village for the people of the region
the village is named after the mustabad river which is a tributary of the larger river
according to the census of india mustabad had a population of
title mustabad title wolf valley true paragraph wolf valley is a graben rift valley in the elsinore trough in western riverside county california
wolf valley is the southernmost of the graben valleys making up the elsinore trough created by the elsinore fault zone
it lies between the wildomar fault on the east at the foot of the temecula basin and the willard fault on the west at the foot of the santa ana mountains
wolf valley lies south of temecula creek
the southern end of the graben valley is where the elsinore fault zone changes the direction of strike along the southern side of agua tibia mountain northeast of pala mountain
the valley is drained by pechanga creek and its tributaries itself a tributary of the santa margarita river
the pechanga indian reservation and part of the city of temecula is located in this valley
head of wolf valley n w mouth of wolf valley n w generation wolf valley is a valley in the u
s
state of california
it is located in the foothills of the sierra nevada
the valley was named after the wolf river which ows through the valley
wolf valley was the site of the battle of wolf valley which took place in the late and early centuries
the battle was fought in the early century and the valley now holds the wolf valley war memorial
figure random examples of lead paragraph generation in wikisum kokal kokal is a village in kollam district in the state of tamil nadu india
demographics as of india census kokal had a population of transport kokal railway station is located at kokal junction
the station is well connected to other parts of the state by road
the nearest airport is at tiruchirappalli
education the primary and secondary school education is imparted by government aided and private schools
the medium of instruction followed by different schools are english telugu malayalam and other languages
in popular culture in the lm the character of the protagonist of the lm is played by a character named kokal
in the movie the protagonist is portrayed by a

ramachandran who is also the protagonist
in other media the village has a tv station called kokal kokal which is a part of the local tv station
the village also has a radio station called kollam radio which is the local radio station of the same name
the radio station is also called kollam fm and is known as the station s main radio station
donald milne donald milne is a canadian politician
he is a member of the legislative assembly of british columbia representing the electoral district of vancouver
milne was born in vancouver british columbia
he was elected to the british columbia house of commons in the provincial election defeating liberal candidate and former mayor of vancouver david campbell
electoral record by election results future milne will be a candidate for vancouver city council in the provincial election
personal life milne lives in vancouver with his wife and two children
awards and honours milne was a recipient of the queen elizabeth ii diamond jubilee medal from the queen s birthday honours in
he also received the order of the british empire in the birthday honours for services to british columbia and the community in british columbia in the birthday honours
milne is also a fellow of the royal canadian academy of arts and sciences and the royal society of canada rbc and is a past president of the vancouver chapter of the rbc society of arts and sciences
in milne was awarded an honorary doctor of laws degree by the university of vancouver in recognition of his service to the community and his contribution to the development of vancouver s arts and science as a leading gure in the vancouver art scene
in he was awarded a lifetime achievement award by the vancouver arts council for his work on vancouver art and science as well as an honorary doctorate of laws causa from the royal college of art and sciences university of toronto in the academic year
decimiana decimiana is a genus of owering plants in the legume family fabaceae
species include decimiana acuminata
muell
benth
ex benth ecuador peru bolivia brazil decimiana angustifolia benth
species there are currently two species in this genus decimiana acutifolia and decimiana costata which are native to ecuador and peru but are now considered a separate species
the following is a list of species of this genus in the world including decimiana borealis benth ecuador colombia decimiana cunninghamii benth ecuador bolivia some of these species are listed in the iucn red list of threatened species
the others are listed as endangered species by the international union for conservation of nature iucn and are listed on the endangered species list by the united states department of agriculture usda
a few of the species of a
albiora are listed by the iucn as endangered or threatened species but they are now listed as critically endangered
the others the other members of the family are listed under the iucn s red list but the other two are listed separately as endangered
the other three are listed together as endangered and listed as vulnerable species
figure random examples of full wikipedia generation in wikisum

