conditional neural generation using sub aspect functions for extractive news summarization zhengyuan liu ke shi nancy f
chen institute for infocomm research singapore liu zhengyuan shi ke
a star
edu
sg t c o l c
s c v
v i x r a abstract much progress has been made in text marization fueled by neural architectures ing large scale training corpora
however in the news domain neural models easily t by leveraging position related features due to the prevalence of the inverted pyramid ing style
in addition there is an unmet need to generate a variety of summaries for in this paper we propose a ent users
ral framework that can exibly control mary generation by introducing a set of aspect functions i
e
importance diversity sition
these sub aspect functions are lated by a set of control codes to decide which sub aspect to focus on during summary eration
we demonstrate that extracted maries with minimal position bias is rable with those generated by standard els that take advantage of position preference
we also show that news summaries generated with a focus on diversity can be more preferred by human raters
these results suggest that a more exible neural summarization work providing more control options could be desirable in tailoring to different user ences which is useful since it is often tical to articulate such preferences for different applications a priori
introduction text summarization targets to automatically erate a shorter version of the source content while retaining the most important information
as a straightforward and effective method extractive summarization creates a summary by selecting and subsequently concatenating the most salient semantic units in a document
recently neural approaches often trained in an end to end ner have achieved favorable improvements on ious large scale benchmarks nallapati et al
narayan et al
liu and lapata
despite renewed interest and avid development in extractive summarization there are still standing unresolved challenges
one major lem is position bias which is especially common in the news domain where the majority of research in summarization is studied
in many news articles sentences appearing earlier tend to be more tant for summarization tasks hong and nenkova and this preference is reected in reference summaries of public datasets
however while this tendency is common due to the classic book writing style of the inverted pyramid lan news articles can be presented in ous ways
other journalism writing styles include anecdotal lead question and answer format and chronological organization stovall
fore salient information could also be scattered across the entire article instead of being trated in the rst few sentences depending on the chosen writing style of the journalist
as the inverted pyramid style is widespread in news articles kryscinski et al
neural models would easily overt on position related tures in extractive summarization tasks because of the data driven learning setup which tags on to tures that correlate the most with the output
as a result those models would select the sentences at the very beginning of a document as best didates regardless of considering the full context resulting in sub optimal models with fancy neural architectures that do not generalize well to other domains kedzie et al

additionally according to nenkova et al
content selection is not a deterministic process salton et al
marcu mani
ferent people choose different sentences to include in a summary and even the same person can select different sentences at different times rath et al

such observations lead to concerns about the advisability of using a single human model


such observations suggest that individuals differ on what she considers key information under different circumstances
this reects the need to generate application specic summaries which is ing without establishing appropriate expectations and knowledge of targeted readers prior to model development and ground truth construction
ever publicly available datasets only provide one associated reference summary to a document
out any explicit instructions and targeted tions or user preferences ground truth construction for summarization becomes an under constrained assignment kryscinski et al

therefore it is challenging for end to end models to generate alternative summaries without proper anchoring from reference summaries making it harder for such models to reach their full potential
in this work we propose a exible neural marization framework that is able to provide more explicit control options when automatically erating summaries see figure
since rization has been regarded as a combination of aspect functions e

information layout bonell and goldstein lin and bilmes we follow the spirit of sub aspect theory and adopt control codes on sub aspects to condition summary generation
the advantages are two fold it provides a systematic approach to investigate and analyze how one might minimize position bias in extractive news summarization in neural modeling
most if not all previous work like jung et al
kryscinski et al
only focus on lyzing the degree and prevalence of position bias
in this work we take one step further to propose a research methodology direction to disentangle position bias from important and non redundant summary content
text summarization needs are often domain or application specic and cult to articulate a priori what the user preferences are thus requiring potential iterations to adapt and rene
however human ground truth construction for summarization is time consuming and intensive
therefore a more exible summary eration framework could minimize manual labor and generate useful summaries more efciently
an ideal set of sub aspect control codes should characterize different aspects of summarization well in a comprehensive manner but at the same time delineate a relatively clear boundary between one another to minimize the set size higgins et al

to achieve this we adopt the aspects dened in jung et al
importance diversity and position and assess their terization capability on the cnn daily mail news figure proposed conditional generation framework exploiting sub aspect functions
hermann et al
via quantitative yses and unsupervised clustering
we utilize trol codes based on these three sub aspect functions to label the training data and implement our tional generation approach with a neural selector model
empirical results show that given different control codes the model can generate output maries of alternative styles while maintaining formance comparable to the state of the art model modulation with semantic sub aspects can reduce systemic bias learned on a news corpus and prove potential generality across domains
in relation to other work in text summarization most benchmark datasets focus on the news domain such as nyt haus and cnn daily mail hermann et al
where the human written summaries are used in both abstractive and extractive paradigms gehrmann et al

to improve the mance of extractive summarization non neural approaches explore various linguistic and cal features such as lexical characteristics kupiec et al
latent topic information ying lang chang and chien discourse analysis rao et al
liu and chen and based modeling erkan and radev mihalcea and tarau
in contrast neural approaches learn the features in a data driven manner
based on recurrent neural networks summarunner is one of the earliest neural models nallapati et al

much development in extractive rization has been made via reinforcement learning narayan et al
jointly learning of scoring and ranking zhou et al
and deep tual language models liu and lapata
despite much development in recent neural proaches there are still challenges such as bias resulting from the prevalent inverted pyramid journalism writing style lin and hovy and system bias jung et al
stemming from sition preference in the ground truth
however to date only analysis work has been done to ize the position bias problem and its ramications such as inability to generalize across corpora or domains kedzie et al
kryscinski et al

few if any have attempted to resolve this long standing problem of position bias using ral approaches
in this work we take a rst stab to introduce sub aspect functions for conditional extractive summarization
we explore the bility of disentangling the three sub aspects that are commonly used to characterize summarization position for choosing sentences by their position importance for choosing relevant and repeating content across the document and diversity for ensuring minimal redundancy between summary sentences jung et al
during the summary generation process
in particular we use these three sub aspects as control codes for conditional ing
to the best of our knowledge this is the rst work in applying auxiliary conditional codes for extractive summary generation
in other nlp tasks topic information is used as conditional signals and applied to dialogue sponse generation xing et al
and training of large scale language models keskar et al
while sentiment polarity is used in text style transfer john et al

in image style transfer codes specifying color or texture are used to train conditional generative models mirza and osindero higgins et al

extractive oracle construction
similarity metric semantic afnity vs
lexical overlap for benchmark corpora that are widely adopted e

cnn daily mail hermann et al
there are only golden abstractive summaries written by humans with no corresponding extractive oracle summaries
to convert the human written abstracts to extractive oracle summaries most previous work used rouge score lin which counts contiguous n gram overlap as the similarity teria to rank and select sentences from the source content
since rouge scores only conduct figure cumulative position distribution of oracles built on rogue blue and bertscore orange
x axis is the ratio of article length
y axis is the tive percentage of summary sentences
cal matching using word overlapping algorithms salient sentences from the source content phrased by human editors could be overlooked as the rouge scores would be low while sentences with a high count of common words could get an inated rouge score kryscinski et al

to tackle this drawback of rouge we propose to apply the semantic similarity metric bertscore zhang et al
to rank the candidate sentences
bertscore has performed better than rouge and bleu in sentence level semantic similarity ment zhang et al

moreover bertscore includes recall measures between reference and candidate sequences a more suitable metric than distance based similarity measures wieting et al
reimers and gurevych for rization related tasks where there is an rical relationship between the reference and the generated text

oracle construction and evaluation to build oracles with semantic similarity we rst segment sentences in source documents and written gold
then we convert the text to a semantically rich distributed vector space
for each sentence in a gold summary we use bertscore to calculate its semantic similarity with candidates from the source content then the sentence with the highest recall score is chosen
candidates with a call score lower than
are excluded to streamline the selection process
we observed that the oracle summaries ated through semantic similarity differ from those chosen from n gram overlap
the positional butions of two schemes are different where early sentence bias is less signicant for the bertscore scheme see figure
to further evaluate the fectiveness of this oracle construction approach details of the corpus in appendix a
score

score

rouge oracle bertscore oracle similarity evaluation gold summaries rouge candidates bertscore candidates qa paradigm evaluation entity and event questions gold summaries rouge candidates bertscore candidates extended questions gold summaries rouge candidates bertscore candidates score

accuracy





table rouge and human evaluation scores of cle summaries built on bertscore and rouge
we conducted two assessments
rouge scores were computed with the gold summaries
table shows oracle summaries derived from bertscore are comparable though slightly lower than those from rouge which is not unexpected given that bertscore is mismatched with the rouge metric
we also conducted two human evaluations
first we ranked the candidate summary pairs of news samples based on their similarity to human written gold summaries narayan et al

four guistic analyzers were asked to consider two pects informativeness and coherence radev et al

the evaluation score represents the hood of a higher ranking and is normalized to
next we adopted the question answering paradigm liu and lapata to evaluate selected ples
for each sentence in the gold summary tions were constructed based on key information such as events and named entities
questions where the answer can only be obtained by comprehending the full summary were also included
human tators were asked to answer these questions given an oracle summary
the extractive summaries structed with bertscore are signicantly higher in all human evaluations see table
sub aspect control codes
sub aspect features in news summarization conditional generation often uses control codes as an auxiliary vector to adjust pre dened style tures
classic examples include sentiment polarity in style transfer john et al
or physical tributes e

color in image generation higgins figure sample level distribution of sub aspect tions of the bertscore oracle
values are the percentage in categorized samples which adds up to
of cnn daily mail training set
the remaining
do not belong to any of these sub aspects
et al

however for summarization it is lenging to pinpoint such intuitive or well dened features as the writing style could vary according to genre topic or editor preference
in this work we adopt position importance and diversity as a set of sub function features to terize extractive news summarization jung et al

considerations include inverted mid writing style is common in news articles thus making layout or position a salient sub aspect for summarization importance sub aspect cates the assumption that repeatedly occurring tent in the source document contains more tant information diversity sub aspect suggests that selected salient sentences should maximize the semantic volume in a distributed semantic space lin and bilmes yogatama et al


summary level quantitative analysis we apply two methods to evaluate the ity and effectiveness of the sub aspects we choose for extractive news summarization
first we duct a quantitative analysis on the cnn daily mail corpus based on the assumption that the writing style variability of summaries can be characterized through different combinations of sub aspects lin and bilmes
for each source document we converted all tences to vector representations with a pre trained contextual language model bert devlin et al

for each sentence we averaged hidden states of all tokens as the sentence embedding
ilar to jung et al
to obtain the subset of tences which correspond to importance sub aspect
com google research bert figure autoencoder with adversarial training egy for unsupervised clustering of sentence level bution of sub aspect functions
figure sentence level clustering result labeled with sub aspect features
x axis is the cluster index
y axis is the proportion of sub aspect features in each cluster
we adopted an n nearest method which calculates an averaged pearson correlation between one tence and the rest for all source sentence vectors and collected the rst k candidates with the highest scores k equals oracle summary length
to tain the subset which corresponds to the diversity sub aspect we used one of the quickhull algorithm barber et al
to nd vertices which can be regarded as sentences that maximize the volume size in a projected semantic space
for the subset that corresponds to the tion sub aspect the rst sentences in the source document were chosen
with three sets of sub aspects we quantied the distribution of different sub aspects on the tive oracle constructed in section
an oracle summary will be mapped to the importance aspect when at least two sentences in the summary are in the subset of importance sub aspect
for those oracle summaries that are shorter than tences occupying of the oracle only one sentence was used to determine which sub aspect they would be mapped to
note that the mapping is many to many i
e
each summary can be mapped to more than one sub aspect
figure displays the distribution of the three sub aspect functions of the oracle summaries where position occupies the largest area
this visualization shows that the three sub aspects represent distinct linguistic attributes but could overlap with one another

sentence level unsupervised analysis according to the mapping algorithm in the previous section summaries were not mapped to a aspect
this nding motivated us to investigate the distribution of sub aspect functions at the sentence level
thus we conducted unsupervised clustering
qhull
assuming that samples within one cluster are most similar to each other and they can be represented by the dominant feature
as shown in figure we use an autoencoder architecture with adversarial training to model the correlation between document and summary tences in the semantic space
the encoding nent receives the source document representation and one summary sentence representation as input and compresses it to a latent feature vector
then the latent vector and document vector are nated and fed to the decoding component to struct the sentence vector
to obtain a compact yet effective latent vector representing the tion between the source and summary we adopt an adversarial training strategy as in john et al

more specically the adversarial decoder we include aims to reconstruct the sentence vector directly from the latent vector
during the training process we update parameters of the autoencoder with an adversarial penalty see appendix b for implementation details
after training this coder we conduct k means clustering k on the latent representation vectors
then we analyze the clustering output with the sentence level labels of sub aspect functions as dened in section

as shown in figure sentences with position aspect is distributed relatively equally across each cluster while importance and diversity dominate in respectively different clusters
based on the tering results we assign the sub aspect function which is dominant to unmapped sentences in the same cluster
for instance diversity is assigned to unmapped sentences in cluster and while importance is assigned to those in cluster and
by doing this we reduce of unmapped sentences and further reduce unmapped maries using the same criteria in section

conditional neural generation in this section we construct a set of control codes to specify the three sub aspect features described in section and label the oracle summaries structed in section then we propose a neural extractive model with a conditional learning egy for a more exible summary generation

control code specication scheme the control codes are constructed in the form of importance diversity position to specify aspect features
we can exibly indicate the on and off state of each sub aspect by switching its corresponding value to or thus enabling disentanglement of each sub aspect function
for instance the control code would tell the model to focus more on importance during sentence scoring and selection while would focus on both diversity and position
indeed switching the position code to would help the model obtain minimal position bias
note that this does not mean the rst few sentences would not be selected as there is overlap between position importance and diversity shown in figure
there are control codes under this specication scheme and we pect this code design can provide the model with sub aspect conditions for generating summaries

neural extractive selector given a document d containing a number of tences


sn the content selector assigns a score to each sentence i indicating its probability of being included in the summary
a neural model can be trained as an extractive lector for text summarization tasks by contextually modeling the source content
here we implemented and adapted the neural extractive selector in a sequence labeling manner kedzie et al

as shown in figure the model consists of three components a contextual encoding component a selection modeling ponent and an output component
first we used bert in the contextual encoding component to obtain feature rich sentence level representations
then in the training process we concatenated these sentence embeddings with the pre calculated control code vector and fed them to the next layer which models the contextual hidden states with the conditional signals
next a linear layer with sigmoid function receives the hidden states and produces scores for each segment between and figure overview of the neural selector architecture
figure position distribution of generated summaries from a strong baseline model bertext and our tional summarization model with position code set to implementations
x axis is the position ratio
y axis is the sentence level proportion
as the probability of extractive selection
while this architecture is straightforward it has shown to be competitive when combined with state of the art contextual representation liu and lapata
in our setting sentences were processed by a word tokenizer wu et al
and their dings were initialized with dimension uncased bert devlin et al
and were xed during training
lengthy source documents were not truncated
for the selection modeling nent we applied a multi layer bi directional lstm schuster and paliwal and a transformer network vaswani et al
and it was cally shown that a two layer bi lstm performed best see appendix c for more implementation details
during testing sentences with the selection probability were extracted as output mary and we used the trigram blocking strategy paulus et al
to reduce redundancy
experimental results and analysis
quantitative analysis to test the possibility of reducing position bias by conditioning summary generation we switched the position code to and compared the position figure sub aspect mapping of generated summary with importance focus code
left panel one sentence in the summary belongs to importance aspect
right panel two sentences in the summary long to importance sub aspect
contour lines denote the number of generated summaries
figure sub aspect mapping of generated summary with diversity focus code
left panel one tence in the summary belongs to diversity sub aspect
right panel two sentences in the summary belong to diversity sub aspect
contour lines denote the number of generated summaries
of selected sentences in summaries generated by our model to the state of the art baseline bertext based on ne tuning bert liu and lapata
the results show that bertext has a chance of choosing the rst of sentences in the ument
while the proposed framework still has a stronger tendency to choose sentences from the rst of the sentences its position distribution is attened compared to that of bertext
we respectively switched importance and sity codes to and categorized the generated maries into subset of each sub aspect function as in section

as shown in figure and maries in the subset of importance and diversity weigh higher when the corresponding control codes are on
together these results demonstrate the sibility of our proposed framework which can erate output summaries of alternative styles when given different control codes

automatic evaluation we calculated rouge scores for generated summaries under control codes and compared them with the bertscore oracle see section the baseline by selecting sentences as summary and several competitive extractive els summarunner nallapati et al
formerext and liu and lapata
from table we observe that summary erated from code is similar to but can dynamically learn the positional features not limited to the rst sentences while isolating out diversity and importance features
only ing on the importance sub aspect leads to the worst performance but performance can be improved when considering other sub aspects
focusing on the diversity sub aspect i
e
code can generate results comparable to strong baselines
oracle bertscore summarunner transformerext bertext code code code code code code code code

























table rouge score evaluation with various trol codes in the form of importance diversity tion
denotes the results from corresponding paper

human evaluation in addition to automatic evaluation the human evaluation was conducted by experienced tic analysts using best worst scaling louviere et al

analysts were given news articles randomly chosen from the cnn daily mail test set and the corresponding summaries from tems the oracle bertext three codes disabling sub aspect position and one code enabling tion
they were asked to decide the best and the worst summaries for each document in terms of informativeness and coherence radev et al
narayan et al

we collected judgments from human evaluators for each comparison
for each evaluator the documents were randomized differently
the order of summaries for each ument was also shufed differently for each uator
the score of a model was calculated as the percentage of times it was labeled as best minus the percentage of times it was labeled as worst ranging from
to

since these differences come in pairs the sum of all the evaluation scores for all summary types adds up to zero
we observed that oracle bertext code code code code evaluation score





recall oracle baseline bertext code code code













table human evaluation on samples from baselines and our model with control codes in the form of portance diversity position
table inference scores on ami corpus from lines and our model with control codes in the form of importance diversity position
denotes results from kedzie et al

bertext code code code















table inference scores on samples with shufed tences
control codes are in the form of importance diversity position
values in brackets absolute crease from scores on original in order samples
summaries under diversity code are more favored than those under importance and their tion can further produce better results see table
these ndings resonate those from the matic evaluation suggesting that whether the ation metric is lexical overlap rouge or human judgement the diversity sub aspect plays a more salient role than importance
moreover both tomatic and human evaluations show that rizing with semantic related sub aspect condition codes achieves reasonable summaries
examples in appendix d show that generated summaries are not position biased yet still preserve key information from the source content

inference on samples of shufed sentences to further assess the decoupling between using aspect signals and positional information learned by the model we conducted an experiment on ples with shufed sentences similar to document shufe in kedzie et al

in our setting we only introduce the shufe process in the model ference phase
we shufed the sentences of all test samples we used in section
then applied the well trained model to generate the predicted summaries
as shown in table outputs under position sub aspect and bertext suffer a icant drop in performance when we shufe the sentence order
by comparison there is far less decrease between the shufed and in order ples under diversity and importance control code demonstrating that the latent features of these two semantic related sub aspects rely less on the tion information suggesting that applying semantic sub aspects in the training process can reduce temic bias learned by the model on a corpus with strong position preference

inference on ami meeting corpus we also conducted an inference experiment on a less position biased corpus
the ami corpus letta et al
is a collection of meetings tated with text transcriptions with human written summaries
different from news summarization meeting summaries are abstractive with extracted keywords
unlike the previous comparison work in kedzie et al
we did not train the model from scratch with the ami training set
instead we only applied the pre trained model without any ne tuning in section for summarization ence on its test set meeting transcript summary pairs
table shows summaries under importance code obtain the highest and scores better than the best reported model in kedzie et al

not surprisingly summaries under the position code do not perform well as there is less position bias in ami
these ndings suggest that our models with semantic related trol codes generalize across domains
conclusion we proposed a neural framework for conditional extractive news summarization
in particular aspect functions of importance diversity and sition are used to condition summary generation
this framework enables us to reduce position bias a long standing problem in news summarization in generated summaries while preserving comparable performance with other standard models
over our results suggest that with conditional ing summaries can be more efciently tailored to different user preferences and application needs
acknowledgments this research was supported by funding from the institute for infocomm research under ares singapore
we thank ai ti aw bin chen shen tat goh ridong jiang jung jae kim ee ping ong and zeng zeng at for sightful discussions
we also thank the anonymous reviewers for their precious feedback to help prove and extend this piece of work
references c bradford barber david p dobkin david p dobkin and hannu huhdanpaa

the quickhull rithm for convex hulls
acm transactions on ematical software toms
jaime carbonell and jade goldstein

the use of mmr diversity based reranking for reordering ments and producing summaries
in proceedings of the annual international acm sigir ence on research and development in information retrieval sigir pages new york ny usa
acm
jean carletta simone ashby sebastien bourban mike flynn mael guillemot thomas hain jaroslav kadlec vasilis karaiskos wessel kraaij melissa kronenthal al

the ami meeting corpus a pre announcement
in international workshop on machine learning for multimodal interaction pages
springer
jacob devlin ming wei chang kenton lee and kristina toutanova

bert pre training of deep bidirectional transformers for language in proceedings of the conference standing
of the north american chapter of the association for computational linguistics human language technologies volume long and short papers pages minneapolis minnesota
ation for computational linguistics
gunes erkan and dragomir r radev

lexrank graph based lexical centrality as salience in text summarization
journal of articial intelligence search
sebastian gehrmann yuntian deng and alexander rush

bottom up abstractive summarization
in proceedings of the conference on pirical methods in natural language processing pages brussels belgium
association for computational linguistics
karl moritz hermann tomas kocisky edward stette lasse espeholt will kay mustafa suleyman and phil blunsom

teaching machines to read and comprehend
in advances in neural information processing systems pages
irina higgins loic matthey arka pal christopher burgess xavier glorot matthew botvinick shakir mohamed and alexander lerchner

beta vae learning basic visual concepts with a constrained variational framework
iclr
tsutomu hirao masaaki nishino yasuhisa yoshida jun suzuki norihito yasuda and masaaki nagata

summarizing a document by trimming the ieee acm trans
audio speech discourse tree
and lang
proc

kai hong and ani nenkova

improving the estimation of word importance for news in proceedings of the document summarization
conference of the european chapter of the sociation for computational linguistics pages gothenburg sweden
association for tational linguistics
vineet john lili mou hareesh bahuleyan and olga vechtomova

disentangled representation learning for non parallel text style transfer
in ceedings of the annual meeting of the tion for computational linguistics pages florence italy
association for computational guistics
taehee jung dongyeop kang lucas mentch and uard hovy

earlier is nt always better aspect analysis on corpus and system biases in marization
in proceedings of the conference on empirical methods in natural language ing and the international joint conference on natural language processing emnlp ijcnlp pages hong kong china
association for computational linguistics
chris kedzie kathleen mckeown and hal daume iii

content selection in deep learning models of in proceedings of the summarization
ference on empirical methods in natural language processing pages brussels belgium
association for computational linguistics
nitish shirish keskar bryan mccann lav r varshney caiming xiong and richard socher

ctrl a conditional transformer language model for lable generation
arxiv preprint

diederik p kingma and jimmy ba

adam a method for stochastic optimization
in proceedings of the international conference for learning representations
wojciech kryscinski nitish shirish keskar bryan cann caiming xiong and richard socher

neural text summarization a critical evaluation
in proceedings of the conference on empirical methods in natural language processing and the international joint conference on natural guage processing emnlp ijcnlp pages hong kong china
association for tional linguistics
julian kupiec jan pedersen and francine chen

a trainable document summarizer
in proceedings of the annual international acm sigir ference on research and development in tion retrieval sigir page new york ny usa
association for computing machinery
chin yew lin

rouge a package for in text matic evaluation of summaries
rization branches out proceedings of the workshop pages barcelona spain
tion for computational linguistics
chin yew lin and eduard hovy

identifying ics by position
in fifth conference on applied ral language processing pages ton dc usa
association for computational guistics
hui lin and jeff bilmes

learning mixtures of submodular shells with application to document the summarization
eighth conference on uncertainty in articial telligence pages arlington ginia united states
auai press
in proceedings of yang liu and mirella lapata

text in proceedings of tion with pretrained encoders
the conference on empirical methods in ural language processing and the international joint conference on natural language processing emnlp ijcnlp pages hong kong china
association for computational linguistics
zhengyuan liu and nancy chen

exploiting discourse level segmentation for extractive in proceedings of the workshop on rization
new frontiers in summarization pages hong kong china
association for computational linguistics
jordan j louviere terry n flynn and anthony fred john marley

best worst scaling ory methods and applications
cambridge sity press
i
mani

overview
shop
summarization evaluation an in acl summarization daniel marcu

from discourse structures to text summaries
in intelligent scalable text tion
rada mihalcea and paul tarau

textrank bringing order into text
in proceedings of the conference on empirical methods in natural guage processing pages barcelona spain
association for computational linguistics
mehdi mirza and simon osindero

tional generative adversarial nets
arxiv preprint

ramesh nallapati feifei zhai and bowen zhou

summarunner a recurrent neural network based quence model for extractive summarization of ments
in thirty first aaai conference on articial intelligence
shashi narayan shay b
cohen and mirella lapata

do nt give me the details just the summary topic aware convolutional neural networks for in proceedings of the treme summarization
conference on empirical methods in natural guage processing pages brussels gium
association for computational linguistics
shashi narayan shay b
cohen and mirella lapata

ranking sentences for extractive tion with reinforcement learning
in proceedings of the conference of the north american ter of the association for computational linguistics human language technologies volume long pers pages new orleans louisiana
association for computational linguistics
ani nenkova rebecca passonneau and kathleen mckeown

the pyramid method ing human content selection variation in tion evaluation
acm trans
speech lang
process

romain paulus caiming xiong and richard socher

a deep reinforced model for abstractive marization
in proceedings of the international conference on learning representations iclr
dragomir r
radev eduard hovy and kathleen own

introduction to the special issue on marization
computational linguistics
gj rath a resnick and tr savage

the tion of abstracts by the selection of sentences
part i
sentence selection by men and machines
american documentation
nils reimers and iryna gurevych

bert sentence embeddings using siamese networks
in proceedings of the conference on empirical methods in natural language processing and the international joint conference on ral language processing emnlp ijcnlp pages hong kong china
association for computational linguistics
gerard salton amit singhal mandar mitra and chris buckley

automatic text structuring and marization
information processing management
evan sandhaus

the new york times annotated corpus
linguistic data consortium philadelphia
christopher
scanlan

reporting and writing sics for the century
oxford university press
qingyu zhou nan yang furu wei shaohan huang ming zhou and tiejun zhao

neural ument summarization by jointly learning to score and select sentences
in proceedings of the nual meeting of the association for computational linguistics volume long papers pages melbourne australia
association for tational linguistics
mike schuster and kuldip k paliwal

tional recurrent neural networks
ieee transactions on signal processing
abigail see peter j liu and christopher d manning

get to the point summarization with generator networks
in proceedings of the nual meeting of the association for computational linguistics volume long papers pages
nitish srivastava geoffrey hinton alex krizhevsky ilya sutskever and ruslan salakhutdinov

dropout a simple way to prevent neural networks from overtting
the journal of machine learning research
james glen stovall

writing for the mass media
prentice hall
ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan n gomez ukasz kaiser and illia polosukhin

attention is all in advances in neural information you need
cessing systems pages
john wieting kevin gimpel graham neubig and lor berg kirkpatrick

simple and effective paraphrastic similarity from parallel translations
in proceedings of the annual meeting of the ciation for computational linguistics pages florence italy
association for computational linguistics
yonghui wu mike schuster zhifeng chen quoc v le mohammad norouzi wolfgang macherey maxim krikun yuan cao qin gao klaus macherey et al

google s neural machine translation system bridging the gap between arxiv preprint man and machine translation


chen xing wei wu yu wu jie liu yalou huang ming zhou and wei ying ma

topic aware in thirty first aaai neural response generation
conference on articial intelligence
ying lang chang and j
chien

latent let in learning for document summarization
ieee international conference on acoustics speech and signal processing pages
dani yogatama fei liu and noah a
smith

extractive summarization by maximizing semantic volume
in proceedings of the conference on empirical methods in natural language processing pages lisbon portugal
association for computational linguistics
tianyi zhang varsha kishore felix wu kilian q weinberger and yoav artzi

bertscore in proceedings uating text generation with bert
of the eighth international conference on learning representations iclr
a details of the corpus the cnn daily mail corpus hermann et al
contains english news articles and ated human written summaries and is the most ular large scale benchmark in news summarization
we used the pre processed dataset as in see et al
which has training pairs validation pairs and test pairs
we did not replace the name entities with anonymised ers and used the same sentence segmentation for documents and summaries as in liu and lapata
to obtain the word embedding tion we tokenized the sentences with the sub word algorithm from bert devlin et al

b implementation details of unsupervised analysis model in this section we provide implementation details of the model in section
an autoencoder with adversarial training strategy
encoding component given a document sentation vector vdoc and a sentence representation vector vsen as input the encoding component two linear layers compresses it to a lower dimension namely the latent feature vector vlatent
in our setting the hidden dimensions of vdoc vsen and vlatent are and respectively
henc is the hidden vector dened as henc vsen vlatent where w and b are trainable parameters in each layer and denotes the concatenation operation
decoding component given a latent feature resentation vector vlatent and a document tation vdoc as input the decoding component two linear layers is targeted to reconstruct the sentence representation vsen
hdec vlatent sdec whdec where hdec and sdec are the hidden state and struction output respectively
adversarial decoding component given a tent feature representation vector vlatent as input the adversarial decoding component one linear layer is targeted to reconstruct the sentence sentation vsen
sadv wvlatent where sadv is the reconstruction output
training procedure and setting during each training batch there is a two step parameter date update the adversarial decoder with mean square error mse loss between sadv and vsen
lossadv vsen update the autoencoder with mse loss between sadv and vsen combined with a penalty from the adversarial mse to reduce the unnecessary mation leaked from vsen in the encoding nent
the adversarial loss is dened as lossadv vsen vsen where
in our training setting
adam optimizer kingma and ba was used with learning rate of and weight decay of
batch size was set to
drop out vastava et al
of rate
was applied in each linear layer
bert parameters were xed during training
the trainable parameter size was
m
tesla with g memory was used for training and we used the pytorch

as tional
selector model in this section we provide implementation details of the model in section
a neural sentence lector for extractive summarization
bert encoding component given a document d containing a number of sentences


sn as input the encoding component produces the sentence representation hi from each s which is a list of tokens



here we use the average of the token level hidden states in the last layer of bert as hi
hi m m i wbertrep i selection modeling component given the cic control code vctrl and sentence vectors h
com pytorch pytorch the lantent feature vector is dened as c implementation details of neural


as input this component use a layer bi directional lstm to model the contextual information with sub aspect conditioning
the ward and backward hidden states are concatenated as output
uf orward i lstmf vctrl ubackward i vctrl ui uf orward i ubackward i where the input embedding dimension hidden size and control code dimension is and spectively
denotes the concatenation operation
output component a linear layer is used to duce output yi for each sentence as the probability of being included in the generated summary
yi training setting binary cross entropy bce is used to measure the loss between the prediction and the ground truth yi for all time steps loss yi
adam optimizer kingma and ba was used with learning rate of and weight decay of
batch size was set to
drop out vastava et al
of rate
was applied in the modeling layer and output linear layer
bert parameters were xed during training
lengthy documents were not truncated
the trainable rameter size was m excluding the pre trained language model
we trained the model for epochs about hours on the tesla gpu
the reported models were selected based on the best validation performance according to the ection point of loss value
d generated summary examples we show summary examples from the golden groundtruth oracle the baseline bertext and the proposed conditional neural summarizer for three news articles in figure
figure three news article examples
oracle summaries are underlined summaries from a baseline model are highlighted in blue summaries from our model with specied control codes are in orange and their overlaps are in purple

