n u j l c
s c v
v i x r a abstractive text classication using sequence to convolution neural networks taehoon kim jihoon yang data mining research laboratory department of computer science and engineering sogang university taehoonkim
ac
kr abstract we propose a new deep neural network model and its training scheme for text classication
our model sequence to convolution neural consists of two blocks sequential block that summarizes input texts and tion block that receives summary of input and classies it to a label
is trained end to end to classify various length texts without preprocessing puts into xed length
we also present gradual weight method that stabilize training
gws is applied to our model s loss function
we compared our model with word based textcnn trained with different data preprocessing methods
we obtained signicant improvement in classication accuracy over word based textcnn without any ensemble or data augmentation
code is able at
com tgisaturday
introduction ever since humans began to record information in the form of text it was necessary to classify and manage information in a certain category to store and retrieve information efciently
this need encouraged many researchers to develop a good text classication technique that can assign predened categories to various kinds of text document such as emails news articles reviews or patents
in commercial world text classication techniques such as nave bayes support vector are already used in various elds including spam ltering news categorization and sentiment analysis
recent development in deep neural are also achieving excellent results in extracting information from a text and classifying it into certain classes
as convolutional neural achieved remarkable results in computer researchers also applied cnns to text and showed excellent results
training cnns on top of pretrained word or character level with hyperparameter tuning they could get similar or outperforming results compared to other text classication models
although textcnns performance in text classication is remarkable they can only be applied to data whose input has xed size
since the number of parameters in textcnn is determined by the length of input text researchers had to crop or pad input texts into a certain length to train their textcnn
this can result information loss when classifying longer texts and cause performance degradation
in section
we show that performance of textcnns can be improved by training the model with summaries of input text
there are two ways to generate the summary of a text
one is extractive preprint
work in progress
summarization mere selection of a few existing sentences extracted from the source
the other is abstractive summarization compressed paraphrasing of main contents of source potentially using vocabulary unseen in the source
both methods can change texts of various lengths into texts of xed length still maintaining important features of source texts
is a graph based ranking model for extractive text summarization
textrank gives a ranking over all sentences in a text allowing it to extract very short summaries without any training corpora
textrank is widely used in summarizing structured text like news articles
many researchers worked with sequence to sequence recurrent neural networks to model abstractive text summarization
using attention that lows neural networks to focus on different parts of their input rnns have been showing signicant results in the task of abstractive
in this paper we introduce sequence to convolution neural model that consists of two blocks sequence block and convolution block
sequence block based on attentional encoder decoder recurrent neural summarizes input texts and feeds them into lution block
convolution block based on classies input texts into certain classes using the summaries provided by sequential block
both blocks share non static word embedding layer encouraging them to collaborate for performance improvement
simply connecting two blocks and train them with single end to end procedure can not guarantee optimal results because sequential block does nt generate proper summaries in early stages of training
to solve this problem we also propose a new training scheme that gradually shifts from tuning for summarization task to ne tuning for classication task as training progresses
our model is implemented with
code is available at
com tgisaturday
related work there was similar approach of text classication with summaries using latent semantic as extractive summarization
they proposed a hybrid model for unlabeled document classication using svm classier with classication rules are generated using summaries of the training documents
although we can not directly compare the performance because of the domain difference in training data we discuss performance of textcnn trained with extractive summaries generated with
mnih et al
came up with novel rnn models with visual attention that is capable of extracting information from an image or video by adaptively selecting a sequence of regions and this idea of using attention mechanism was successfully applied to machine translation by bahdanau et al

we used bahdanau to improve the performance of our sequential block
our approach to use attentional encoder decoder rnns for abstractive summarization is closely related to nallapati et al
who were the rst to use rnns and attention model for abstractive text summarization
our implementation of sequential block is very similar to pan and
they used annotated english dataset to train their model and achieved state of the art results as of june
require training set of input text and corresponding example summary
none of datasets that we used for evaluation has any example summary
we used to generate extractive summaries out of input texts and feed them to sequential block as example summary
we chose because it can generate practical summaries even with short texts and does nt require any training corpora
the architecture of our convolution block is based on textcnn model proposed by
was the rst to use cnn in text classication
using this model as a baseline we applied batch normalization after each convolution layer and changed hyperparameters for optimization
our training scheme is closely related to the one proposed in faster r cnn
their scheme alternates between ne tuning two different tasks
we also tried to train our model with training scheme that alternates between ne tuning summarization task and classication task
however this was not effective because summaries generated in early stages of training were lled with series of unk unknown word tokens
instead our training scheme gradually changes focus of training from summary generation to text classication so that summaries generated in early stages of training do nt lower the classication accuracy of convolution block
model figure depicts the overall structure of model
figure overview of our model that consists of sequential and convolution
both blocks interact with word embedding layer to get vectorized representation of words used in summarization and classication tasks

sequential block our baseline model of sequential block corresponds to the attentional encoder decoder rnn model used in nallapati et al
which encodes a source sentence into a xed length vector from which decoder generates abstractive summaries
the encoder consists of bidirectional and decoder consists of uni directional
we used long short term memory with hidden units for encoder and decoder and bahdanau for attention mechanism
we also inserted modules between lstm layers to regularize
the forward lstm of encoder reads the input sequence as it is ordered and the backward lstm reads the sequence in the reverse order
in this way xed length vector from encoder contains the summaries of both preceding words and the following words
with the help of attention mechanism decoder decides parts of the source sentence to pay attention to and focus only on the vectors that are essential for summarization

convolution block the structure of our convolution block is based on textcnn model proposed by which gets n k vectorized representation of text where n is the number of words inside the text and k is the dimension size of word embedding
each lter windows with varying extracts one feature by performing h k convolution operation over input and apply max over time pooling operation
the model uses multiple lters to multiple features
these features are passed to a fully connected softmax layer whose output is the probability distribution over labels
convolution block gets vectorized representation of summaries generated by sequential block
we used rectied linear for non linear activation function and lter windows of with lters each
we applied batch after each convolution layer
batch accelerates training by reducing internal covariate the change in the distribution of network activations due to the change in network parameters during the training
we also applied batch normalization on vectorized representation of summaries to stabilize entire training procedure by reducing internal covariate shift between sequential block and convolution block
for regularization we inserted module between max pooling layer and fully connected layer

word embedding block word embedding block consists of word embedding layer which stores vectorized representations of each word and vocabulary lookup table that maps each word with corresponding vector representation
to make vocabulary dictionary we extracted to words from training data with minimum word excluding words that have appeared less than f times
our word embedding layer is non static and ne tuned via back propagation
in our implementation we set word embedding dimension to

loss function the main objective of is to classify texts of various lengths without losing important features of original context
for feature extraction we use abstractive summarization method using sequential block
although our model is mainly focused on classication quality of summary must be guaranteed for convolution block to successfully perform classication task
taking everything into consideration we trained our model to minimize an objective function which is weighted sum of losses in classication and summarization
our loss function for a text is dened as y i ti t i li y t i li n li j t i li lvocabj pwj w j in i is the index of a text in a mini batch of size n and pyi is the predicted probability of text i
classied as ground truth label y i
the classication loss lcls is cross entropy loss over classication classes
the summarization loss lsum is sequence loss between the summary output of sequential block ti with length li and summary example t i
in pwj is the predicted probability of the jth word wj in generated summaryto match with the jth word of summary example wj
we dened sequence loss as the average of the vocabulary losses lvocabs in ti
the vocabulary loss of jth word lvocabj is cross entropy loss over vocabulary in dictionary stored in word embedding block
total loss is weighted sum of lcls and lsum with a balancing weight normalized with n
in our implementation we used multi class softmax layer to get pyi and pwj
we normalized our loss with the mini batch size
we applied gradual weight shift to
we explained more about gradual weight shift in section


gradual weight shift in our implementation of we train sequential block and convolution block end to end by back propagation and gradient optimizer using loss function dened in

this training scheme ne tunes the model and reduces training time compared to training each model independently
however using a constant value
for balancing weight caused sudden drops of validation accuracy in later stages of training epochs
we found the main cause of this phenomenon in sequential block
in earlier stages of training epochs sequential block does not generate practical summaries and omits unk tokens instead
huge difference in quality of summary throughout the training hinders the optimization of convolution block
using larger
solves this problem a little bit by giving more weight to lsum making sequential block to converge faster
using larger gives more weight to lsum than lcls leading the model to focus on optimization of sequential block until the end
since our model is designed for classication we gradually shifted weight from lsum to lcls by exponentially decaying throughout time
our exponential decay function for is dened as t where is initial value of lambda is decay rate and t is current time step
we could stabilize the training of our model and achieve higher test accuracy by applying gradual weight shift to our loss function dened in section

we explain more about the result in section


sharing word embedding for summarization and classication we designed our model to share word embedding for summarization and classication
when back propagation happens sequential block tries to update word embedding layer in direction of minimizing sequence loss
on the other hand convolution block tries to update word embedding layer in direction of minimizing classication loss
this ne tunes word embedding layer

optimization in our implementation we trained our model end to end by back propagation and stochastic gradient using adam with

and

we used learning rate of
decayed every epoch using an exponential rate of

dropout rate for modules is

we also used gradient with gradient norm limited to
we set loss balancing weight to
when training ag s news and
for other datasets
we initialized convolution layers using he initializer and fully connected layer using initializer
all other weights were initialized with random values from a uniform distribution with minimum of and maximum of
the implementation is done using
we trained our model using single nvidia titan v gpu with mini batch size
it took s news to answers for training
we did nt used any ensemble or data augmentation techniques detailed information about datasets are given in section

experiments we evaluated the performance of our model by comparing with basic
we dened basic textcnn used in experiments as vanilla cnn
we used the same windows of with lters each for vanilla cnn and convolution block in
we trained vanilla cnn with three different data preprocessing methods full text summarize
we dened the length of text as the number of words in each text
full text this is default data preprocessing method
we just removed unnecessary characters and stopwords from the input texts and padded them into xed length with pad token
here xed length is the maximum length of text in each dataset
same preprocessing method is also applied before and summarize
using xed length as inside bracket in table we cropped each text into xed length
for example if a text is longer than words we only used words starting from the front
texts shorter than xed length is padded with pad token
summarize instead of cropping each sentence we generated extractive summary of input text using and processed the summary with
same method was used to generate summary example used in training sequential block

datasets we evaluated our model on three different datasets ag s news dbpedia and yahoo
to offer fair evaluation on the performance of sequential block in we removed input texts shorter than xed length value in table changing number of training samples in dbpedia and yahoo answers
the number of training samples in ag news is the same as the table statisics of datasets
vocabulary size is number of words used to train the model
min freq is minimum word frequency used to decide vocabulary size of each dataset
datasets classes training set test set vocabulary size min freq ag s news dbpedia yahoo answers original
we did nt apply any changes to test samples for fair comparison with best published results
we also limited the size of vocabulary into to with minimum word in table
words in test samples are not included in vocabulary dictionary of word embedding block
none of the datasets contains summary examples to train model so we generated summary examples using
we did nt feed any summary samples while evaluating with test samples
detailed statistics of each dataset is given in table

text classication table classication results of all models
numbers are test accuracy in percentage
vanilla cnn is basic model and is our model
full stands for full text crop stands for and sum stands for summarize
we labeled the best result of vanilla cnn in blue and worst result in red
best result of is labeled in green
model ag s news dbpedia yahoo answers vanilla cnn full vanilla cnn vanilla cnn vanilla cnn vanilla cnn




















extractive text classication table shows classication results of and vanilia cnn models
we rst evaluated data preprocessing methods for vanilla cnn with different xed length sizes
we labeled the best result in blue and worst result in red
there was not a single data cessing method that derived best performance for all datasets
summarization with tends to work well in most of the cases
abstractive text classication our model outperformed other models in all cases bringing average growth compared to vanilla cnn trained without any data cnn full
best published result for ag s news using is
with pretrained embedding and data augmentation using thesaurus
our model achieved competitive result on ag s news dataset without any pretrained word embedding or data augmentation technique
we can not directly compare other results due to the changes that we explained in section


text summarization algorithm can not generate proper summary if the original text is too short
as a result classication with vanilla cnn and cnn performed worst with yahoo answers dataset
is robust to short length texts as it s shown in table
even with short length texts sequential block successfully generate summaries by removing unimportant words from the original
algorithm failed to generate any summary for both examples
our implementation with tensorow we used greedy embedding helper instead of training helper for designed our implementation of textrank algorithm to return the rst sentence if the original text was inference layer
too short
table examples of output produced by sequential block with short length texts
textrank failed to generate any summary returning the rst sentence of input instead
type label sentence original sports textrank sports sequential block sports original sci tech textrank sci tech sequential block sci tech great britain s amir khan who looked so impressive in winning the pound championship at the junior international invitational boxing championships here last summer has a chance for an olympic gold medal in the lightweight division today
great britain s amir khan who looked so impressive in winning the pound championship at the junior international invitational boxing championships here last summer great britain amir khan looked impressive winning pound championship junior international invitational boxing championships last summer chance olympic gold medal lightweight division today a robot that will generate its own power by eating ies is being developed by british scientists
the idea is to produce electricity by catching ies and digesting them in special fuel cells that will break a robot that will generate its own power by eating ies is being developed by british scientists
robot generate power eating ies developed british entists idea produce electricity catching ies digesting special fuel cells break total loss sequence loss without gws with gws without gws with gws s s o l



steps




steps
figure total loss and sequence loss on the ag s news dataset with and without gradual weight

gradual weight shift optimization in sequence loss curve of figure the sequence loss of the model trained without gws converges smoothly in the early stage of training but starts to uctuate after steps
this also effects the total loss curve unstabilizing the training of the model
in contrast loss curves of the model with gws converges smoothly until the end
classication performance we evaluated the performance of gradual weight shift with ag s news dataset
in table model trained with gws achieved better results compared to the same model trained without gws
although model without gws performed better s s o l table classication results on the ag s news dataset with and without gradual weight
best result using vanilla cnn is also included for comparison
model accuracy vanilla cnn without without with with




than any other vanilla cnn models it could not outperform previous best published result of zhang et al

conclusion we have proposed sequence to convolution neural networks for efcient and accurate text classication
can be trained with texts of various lengths without any text cessing method such as cropping or summarizing
we also presented a new training theme for our model using gradual weight which can be applied to other models with multi task loss function by changing number of balancing weights
the true strength of comes from its exibility
each blocks can be replaced with other models designed for the same tasks
for example sequential block can be replaced with layer or text variational
convolution block can be replaced with other text classication models such as c recurrent char or
we adopted general sequence loss function which uses predicted probability of jth word wj in generated summary matches jth word of summary example wj to calculate vocabulary losses
however we think the sequence loss can not be evaluated accurately by comparing only the words in the same position
bahdanau et al
suggested specialized surrogate losses for encoder decoder models often used for sequence prediction tasks and brought signicant performance improvements
we also did nt use any pretrained word embeddings to initialize the word embedding block
previous results on word based textcnn suggests that initializing embedding layers with trained word vectors such as or helps improves performances of models
in the future we are planning to improve by reassembling our model with every possible methods
references martn abadi ashish agarwal paul barham eugene brevdo zhifeng chen craig citro greg s
corrado andy davis jeffrey dean matthieu devin sanjay ghemawat ian goodfellow andrew harp geoffrey irving michael isard yangqing jia rafal jozefowicz lukasz kaiser manjunath kudlur josh levenberg dandelion man rajat monga sherry moore derek murray chris olah mike schuster jonathon shlens benoit steiner ilya sutskever kunal talwar paul tucker vincent vanhoucke vijay vasudevan fernanda vigas oriol vinyals pete warden martin wattenberg martin wicke yuan yu and xiaoqiang zheng
tensorflow scale machine learning on heterogeneous systems
software available from tensorow
org
dzmitry bahdanau kyunghyun cho and yoshua bengio
neural machine translation by jointly learning to align and translate
corr

dzmitry bahdanau dmitriy serdyuk philemon brakel nan rosemary ke jan chorowski aaron c
courville and yoshua bengio
task loss estimation for sequence prediction
corr

lon bottou
large scale machine learning with stochastic gradient descent
in yves lier and gilbert saporta editors proceedings of pages heidelberg
physica verlag hd
jingnian chen houkuan huang shengfeng tian and youli qu
feature selection for text classication with nave bayes
expert systems with applications part
jan k chorowski dzmitry bahdanau dmitriy serdyuk kyunghyun cho and yoshua bengio
attention based models for speech recognition
in c
cortes n
d
lawrence d
d
lee m
sugiyama and r
garnett editors advances in neural information processing systems pages
curran associates inc

alexis conneau holger schwenk loc barrault and yann lecun
very deep convolutional networks for natural language processing
corr

xavier glorot and yoshua bengio
understanding the difculty of training deep feedforward neural networks
in yee whye teh and mike titterington editors proceedings of the thirteenth international conference on articial intelligence and statistics volume of proceedings of machine learning research pages chia laguna resort sardinia italy may
pmlr
marlene grace u rajasekhar vijayapal reddy and a vinaya babu
text classication with summaries generated using latent semantic analysis
ijrsae
a
graves a
r
mohamed and g
hinton
speech recognition with deep recurrent neural networks
in ieee international conference on acoustics speech and signal processing pages may
kaiming he xiangyu zhang shaoqing ren and jian sun
deep residual learning for image recognition
corr

kaiming he xiangyu zhang shaoqing ren and jian sun
delving deep into rectiers surpassing human level performance on imagenet classication
corr

sepp hochreiter and jrgen schmidhuber
long short term memory
neural comput
november
sergey ioffe and christian szegedy
batch normalization accelerating deep network training by reducing internal covariate shift
in proceedings of the international conference on international conference on machine learning volume pages
jmlr
org
thorsten joachims
text categorization with support vector machines learning with many relevant features
in claire ndellec and cline rouveirol editors machine learning pages berlin heidelberg
springer berlin heidelberg
armand joulin edouard grave piotr bojanowski and tomas mikolov
bag of tricks for efcient text classication
corr

yoon kim
convolutional neural networks for sentence classication
corr
diederik p
kingma and jimmy ba
adam a method for stochastic optimization
corr siwei lai liheng xu kang liu and jun zhao
recurrent convolutional neural networks for peter liu and xin pan
text summarization with tensorow
software available from rada mihalcea and paul tarau
textrank bringing order into texts
in dekang lin and dekai wu editors proceedings of emnlp pages barcelona spain july
association for computational linguistics
tomas mikolov ilya sutskever kai chen greg corrado and jeffrey dean
distributed representations of words and phrases and their compositionality
corr

volodymyr mnih nicolas heess alex graves and koray kavukcuoglu
recurrent models of visual attention
corr




text classication
tensorow
org
ramesh nallapati bing xiang and bowen zhou
sequence to sequence rnns for text rization
corr

courtney napoles matthew gormley and benjamin van durme
annotated gigaword
in proceedings of the joint workshop on automatic knowledge base construction and web scale knowledge extraction akbc wekex pages stroudsburg pa usa
association for computational linguistics
razvan pascanu tomas mikolov and yoshua bengio
understanding the exploding gradient problem
corr

jeffrey pennington richard socher and christopher d
manning
glove global vectors for word representation
in empirical methods in natural language processing emnlp pages
shaoqing ren kaiming he ross b
girshick and jian sun
faster r cnn towards real time object detection with region proposal networks
corr

alexander m
rush sumit chopra and jason weston
a neural attention model for abstractive sentence summarization
corr

m
schuster and k
k
paliwal
bidirectional recurrent neural networks
ieee transactions on signal processing nov
stanislau semeniuta aliaksei severyn and erhardt barth
a hybrid convolutional variational autoencoder for text generation
corr

karen simonyan and andrew zisserman
very deep convolutional networks for large scale image recognition
corr

nitish srivastava geoffrey hinton alex krizhevsky ilya sutskever and ruslan salakhutdinov
dropout a simple way to prevent neural networks from overtting
journal of machine learning research
ilya sutskever oriol vinyals and quoc v le
sequence to sequence learning with neural networks
in z
ghahramani m
welling c
cortes n
d
lawrence and k
q
weinberger editors advances in neural information processing systems pages
curran associates inc

christian szegedy sergey ioffe and vincent vanhoucke
inception inception resnet and the impact of residual connections on learning
corr

bing xu naiyan wang tianqi chen and mu li
empirical evaluation of rectied activations in convolutional network
corr

zhang yun tao gong ling and wang yong cheng
an improved tf idf approach for text classication
journal of zhejiang university science a aug
xiang zhang junbo jake zhao and yann lecun
character level convolutional networks for text classication
corr

chunting zhou chonglin sun zhiyuan liu and francis c
m
lau
a c lstm neural network for text classication
corr


