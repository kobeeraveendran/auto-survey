bengali text summarization by sentence extraction kamal sarkar computer science engineering department jadavpur university kolkata india
com abstract text summarization is a process to produce an abstract or a summary by selecting significant portion of the information from one or more texts
in an automatic text summarization process a text is given to the computer and the computer returns a shorter less redundant extract or abstract of the original
many techniques have been developed for summarizing english
but a very few attempts have been made for bengali text summarization
this paper presents a method for bengali text summarization which extracts important sentences from a bengali document to produce a summary
keyword bengali text summarization sentence extraction indian languages introduction now a days information overload on the world wide web www is becoming a problem for an increasingly large number of web users
to reduce this information overload problem automatic text summarization can be an indispensable tool
the abstracts or summaries can be used as the document surrogates in place of the original documents
in another way the summaries can help the reader to get a quick overview of an entire document
another important issue related to the information explosion on the internet is the problem that many documents with the same or similar topics are duplicated
this kind of data duplication problem increases the necessity for effective document summarization
in summary the following are the important reasons in support of automatic text summarization a summary or abstract saves reading time a summary or an abstract facilitate document selection and literature searches it improves document indexing efficiency machine generated summary is free from bias customized summaries can be useful in question answering systems where they provide personalized information
the use of automatic or semi automatic summarization by commercial abstract services may allow them to scale the number of published texts they can evaluate
input to a summarization process can be one or more text documents
when only one document is the input it is called single document text summarization and when the input is a group of related text documents it is called multi document summarization
we can also categorize the text summarization based on the type of users the summary is intended for user focused query focused summaries are tailored to the requirements of a particular user or group of users and generic summaries are aimed at a broad readership community mani
depending on the nature of summary a summary can be categorized as an abstract and an extract
an extract is a summary consisting of a number of salient text units selected from the input
an abstract is a summary which represents the subject matter of an article with the text units which are generated by reformulating the salient units selected from an input
an abstract may contain some text units which are not present into the input text
based on information content of the summary it can be categorized as informative and indicative summary
the indicative summary presents an indication about an article s purpose and approach to the user for selecting the article for in depth reading informative summary covers all salient information in the document at some level of detail that is it will contain information about all the different aspects such as article s purpose scope approach results and conclusions
for example an abstract of a research article is more informative than its headline
the main objective of the work presented in this paper is to generate an extract from a bengali document
we have followed a simple and easy to implement approach to bengali single document text summarization because the sophisticated summarization system requires resources for deeper semantic analysis
bengali is a resource constrained language and nlp natural language processing research activities on bengali have recently been started
in our work presented in this paper we have investigated the impact of thematic term feature and position feature on bengali text summarization
to our knowledge no generic text summarization system for bengali is available for comparison to our system
so we have compared the proposed method to the lead baseline which was defined for single document text summarization task in past two duc conferences duc and duc
lead baseline considers the first n words of an input article as a summary where n is a predefined summary length
our work is different from the work on bengali opinion summarization presented in das and bandyopadhyay because we mainly focus on generic text summarization for bengali
in section we present a brief survey on single document text summarization in english domain
the proposed summarization method has been presented in section
in section we describe the summary evaluation method and experimental results
a survey on single document text summarization in english domain in this section we present a brief survey on single document text summarization for english
although new research on text summarization in english domain has been started many years ago most works on text summarization today still rely on sentence extraction to form summary
many previous works on extractive summarization use two major steps ranking the sentences based on their scores which are computed by combining few or all of the features such as term frequency tf positional information and cue phrases baxendale edmundson luhn lin and hovy and selecting few top ranked sentences to form an extract
the very first work on automatic text summarization by luhn computes salient sentences based on word frequency number of times a word occurs in a document and phrase frequency
although subsequent research has developed sophisticated summarization methods based on various new features the work presented by edmundson is still followed today as the foundation for extraction based summarization
baxendale presented a straightforward method of sentence extraction using document title first and last sentences of the document each paragraph
lin and hovy claimed that as the discourse structures change over the domains and the genres the position method can not be as simple as in baxendale
they defined an optimal policy of locating the likely positions of topic bearing sentences in the text
mead radev et
al
computes the score of a sentence based on many features such as similarity to the centroid similarity to the first sentence of the document position of the sentence in the document sentence length
kupiec et
el
applied a machine learning approach to text summarization
they developed a summarizer using a bayesian classifier to combine features from a corpus of scientific articles and their abstracts
salton et
al
presented a sentence extraction method that exploits the semantic links between sentences in the text
the feature they used in this work may be considered as a cohesion feature
text cohesion halliday and hasan refers to the relations semantic links between words word senses or referring expressions which determine how tightly connected the text is
in this approach text is represented by a graph in which each node represents a paragraph in a document and the edges are labeled with the similarity score between two paragraphs
the paragraph that is connected to many other paragraphs with a similarity above a predefined threshold is considered as the bushy node
the paragraph representing the bushy node is considered as a salient one
barzilay and elhadad described a summarization approach that used lexical chaining method to compute the salience of a sentence
cohesion halliday and hasan is a method for sticking together different parts of the text
lexical cohesion is the simplest form of cohesion
lexical cohesion links the different parts of the text through semantically related terms co reference ellipsis and conjunctions
lexical cohesion also involves relations such as reiteration synonymy hypernymy is a relations such as dog is a kind of animal wrist is a part of hand
the concept of lexical chain was introduced in morris and hirst
they characterized lexical chain as a sequence of related words that spans a topical unit of text
in other words lexical chain is basically lexical cohesion that occurs between two terms and among sequences of related words
barzilay and elhadad used a wordnet miller to construct the lexical chains
the work in conroy and oleary considered the fact that the probability of inclusion of a sentence in an extract depends on whether the previous sentence had been included as well and applied hidden markov models hmms in sentence extraction task
osborne applied maximum entropy log linear model to decide whether a sentence will be included in a summary or not
he assumed no feature independence
the features he considered are word pairs sentence length sentence position discourse features e

whether sentence follows the introduction

compared to creating an extract automatic generation of abstract is harder and the latter requires deeper approaches which exploit semantic properties in the text
generation of an abstract from a document is relatively harder since it requires semantic representation of text units sentences or paragraphs in a text reformulation of two or more text units and rendering the new representation in natural language
abstractive approaches have used template based information extraction information fusion and compression
in information extraction based approach predefined template slots are filled with the desired pieces of information extracted by the summarization engine paice and
an automated technique has been presented in jing and mckeown to build a representing the cut and paste process used by humans so that such a corpus can then be used to train an automated summarizer
true abstraction needs more sophisticated process that requires large scale resources
headline generation can be viewed as generation of very short summary usually less than words that represents the relevant points contained in a document
a headline summary is a kind of the indicative summary
banko et
al
presented an approach that uses some statistical methods to generate headline like abstracts
hmm hidden markov model based headline generation has been presented in zajic dorr and schwartz
dorr al
developed the hedge trimmer that uses a parse and trim based approach to generate headlines
in this approach the first sentence of a document is parsed using a parser and then the parsed sentence is compressed to form a headline by eliminating the unimportant constituents of the sentence using a set of linguistically motivated rules
topiary zajic al
a headline generation system combines the compressed version of the lead sentence and a set of topic descriptors generated from the corpus to form a headline
the sentence is compressed using the approach similar to the approach in dorr al
and the topic descriptors
a number of approaches for creating abstracts have been conceptualized without much emphasis on the issue that a true abstract may contain some information not contained in the document
creating such an abstract requires external information of some kind such as ontology knowledge base

since large scale resources of this kind are difficult to develop abstractive summarization has not progressed beyond the proof of concept stage
proposed summarization method the proposed summarization method is extraction based
it has three major steps preprocessing sentence ranking summary generation

preprocessing the preprocessing step includes stop word removal stemming and breaking the input document in to a collection of sentences
for stop word removal we have used the bengali stop word list downloadable from the website of forum for information retrieval evaluation
isical
ac

txt

stemming using stemming a word is split into its stem and affix
the design of a stemmer is language specific and requires some significant linguistic expertise in the language
a typical simple stemmer algorithm involves removing suffixes using a list of frequent suffixes while a more complex one would use morphological knowledge to derive a stem from the words
since bengali is a highly inflectional language stemming is necessary while computing frequency of a term
in our work we use a lightweight stemmer for bengali that strips the suffixes using a predefined suffix list on a longest match basis using the algorithm similar to that for hindi ramanathan and rao

sentence ranking after an input document is formatted and stemmed the document is broken into a collection of sentences and the sentences are ranked based on two important features thematic term and position
thematic term the thematic terms are the terms which are related to the main theme of a document
we define the thematic terms are the terms whose tfidf values are greater than a predefined threshold
the tfidf value of a term is measured by the product of tf and idf where tf term frequency is the number of times a word occurs in a document and idf is inverse document frequency
the idf of a word is computed on a using the formula idf df where n number of documents in the and document frequency indicates the number of documents in which a word occurs
the score of a sentence k is computed based on the similarity of the sentence to the set of thematic terms in a document
the similarity of a sentence to the set of thematic terms in a document is computed as the sum of the tfidf values of the thematic terms contained in the sentence
s k w tfidf w k where tfidfw is a tfidf value of a thematic term w in a sentence and sk is the score of the sentence
one crucial issue is to determine the tfidf threshold value based on which we can decide on whether a term is a thematic term or not
in experimental section we will discuss how this threshold value has been adjusted for the best results
positional value the positional score of a sentence is computed in such a way that the first sentence of a document gets the highest score and the last sentence gets the lowest score
the positional value for the sentence k is computed using following formula kp sentence length we consider length of a sentence as a feature because we observe that if a sentence is too short but it occurs in the beginning paragraph of a document it is sometimes selected due to its positional advantage
on the other hand if a sentence is too long it is sometimes selected due to the fact that it contains many words
so we eliminate the sentences which are too short or too long
combining parameters for sentence ranking we compute the score of a sentence using the linear combination of the normalized values of thematic term based score sk and positional score pk if the sentence is not too long or too short
if a sentence is too short or too long it is assigned a score of
the final score of a sentence k is score s if l p l l l l u the values of ll lower cutoff on the sentence length l and lu upper cutoff on the sentence length l are obtained by tuning them for the best results on a subset of documents randomly selected from our corpus
in the experimental section we will discuss in detail how the values of these parameters are tuned

summary generation a summary is produced after ranking the sentences based on their scores and selecting k top ranked sentences when the value of k is set by the user
to increase the readability of the summary the sentences in the summary are reordered based on their appearances in the original text for example the sentence which occurs first in the original text will appear first in the summary
evaluation experiments and results to test our summarization system we collected bengali documents from the bengali daily newspaper ananda bazar patrika
the documents are typed and saved in the text files using format
for each document in our corpus we consider only one reference summary for evaluation
evaluation of a system generated summary is done by comparing it to the reference summary

evaluation it is very difficult to determine whether a summary is good or bad
the summary evaluation methods can be broadly categorized as human evaluation methods and automatic machine based evaluation methods
a human evaluation is done by comparing system generated summaries with reference model summaries by human judges
according to some predefined guidelines the judges assign a score in a predefined scale to each summary under evaluation
quantitative scores are given to the summaries based on the different qualitative features such as information content fluency
the main problems with human evaluation are the evaluation process is tedious it suffers from the lack of consistency
two human judges may not agree on each other s judgments
on the other hand automatic evaluation machine based is always consistent with a judgment
the automatic evaluations may lack the linguistic skills and emotional perspective that a human has
hence although automatic evaluation is not perfect compared to the human evaluation it is popular primarily because the evaluation process is quick even if summaries to be evaluated are large in number
since automatic evaluation is performed by a machine it follows a fixed logic and always produces the same result on a given summary
since automatic evaluation processes are free from human bias it provides a consistent way of comparing the various summarization systems
in several past document understanding conferences duc organized by nist the national institute of standards and technology single document text summarization systems for english have been evaluated
in duc and duc single document summarization task was to generate a summary of fixed length such as words words
a baseline called lead baseline was defined in these s conferences
lead baseline considers the first n words of an input article as a summary where n is a predefined summary length
unlike duc single document text summarization task where there was a fixed summary length for each document we believe that a generic summary of a document may be longer or shorter than a summary of another document
so we assume that the size of a system generated summary should be equal to that of the corresponding model summary but the different model summaries may not be equal in size
we adopted an automatic summary evaluation metric for comparing system generated summaries to reference summaries
when we compare a system generated summary to a reference summary we ensure that they would be of the same length
we have used the unigram overlap method stated in radev et
al for evaluating the system generated summaries
unigram overlap between a system generated summary and a reference summary is computed as follows unigram based recall s ri r is the length of the reference summary and indicates the maximum number of unigrams co occurring in the system generated summary s and the reference summary r
creation of reference summaries is a laborious task
in our experiment we have used only one reference summary for evaluating each system generated summary

experiments and results tuning and choosing appropriate threshold value for the best results and used in equation would be set appropriately
at the same time an appropriate tfidf threshold value for selecting the thematic terms discussed in subsection
should be chosen
for tuning these parameters we build a training data set by the collection of randomly selecting document summary pairs from document summary pairs in our corpus
initially we set the value of to since is the weight of the positional feature which is observed by us as a feature producing better results than the thematic term feature
we set the value of to for all the experimental cases presented in this paper
for tuning the value of we set the tfidf threshold value to and conduct experiments with the different values of that ranges from to
to obtain the different values of we step between to by

the figure shows summarization performance curve with respect to different values of on the training data

average recall score vs
when tfidf threshold value is set to and is set to
the figure shows that when the value of is set to
which is a relatively smaller value the better result is obtained
since depending on tfidf threshold value we decide on whether a term is the thematic term or not an appropriate threshold value should be determined to improve the summarization performance
for this pupose after fixing the value of to
we adjust the tfidf threshold value
the figure shows the summarization performance curve with different tfidf threshold values
figure
average recall score vs
tfidf threshold value when is set to
and is set to
the figure shows that the best result is achieved when tfidf threshold value is set to any value between
and

we set tfidf threshold value to
because at this value average recall score transits from a lower value to the best value
after fixing the value of to
and the tfidf threshold value to
we adjust the lower cutoff and the upper cutoff on the sentence length
table shows the results on training set with different values of the upper cutoff on sentence length
lu average recall score



table
results on training set with different values of the upper cutoff lu on sentence length
the results on training set with different values of lower cutoff on sentence length are shown in table
ll average recall score



table
results on training set with different values of lower cutoff ll on sentence length
table and table show that the best results are obtained when lu is set to any value between and and ll is set to
we set the value of lu to and the value of ll to when we run the system on the test data
from pairs randomly chose document summary results we document summary pairs in our corpus and considered this subset as a training set for tuning the values of several parameters discussed above
after setting the parameters to the values learnt from the training set we test our system on the entire collection of documents
from each of documents a summary of n words is generated where n is the length of the reference summary of the corresponding document
a system generated summary is compared to a reference summary and the unigram based recall score is computed using the equation
the average recall score is obtained by averaging the recall scores obtained on all the documents in the collection
of table shows the performance of the proposed system on the test data set
to our knowledge no generic text summarization system for bengali is available for comparison to our system
so we have compared the proposed method to the lead baseline
lead baseline considers the first n words of an input article as a summary where n is a predefined summary length
table shows the comparisons of our system to the lead baseline
methods average unigram based recall score proposed system
lead baseline

comparison of the proposed system to lead baseline table shows that the proposed method outperforms the lead baseline
the evaluation of generic summarization systems in the past duc conferences duc and duc proves that it is very hard to beat lead baseline on the news documents
an example the following is an article taken from the bengali daily newspaper ananda bazar patrika
k i m p n t m dn do n s a an o o k n ei p n p p i s sp st o i s s l m e pnt p o nt o ei s us n dn s ee eee a dk ps e i k i n u o dn o a m p i o t n s a i t dn i e n i o nt e o n a e d m s a k ps s a am ee n u o p o s m here is the reference summary for the article mentioned above
a si m pnt p m st n ki s i dn a an t n the following is the summary generated by the proposed system for the news article
k i m p n t m dn do o k n ei p n p o p i s sp st o conclusion this paper discusses a single document text summarization method for bengali
many techniques have been developed for summarizing english
but a very few attempts have been made for bengali text summarization
the performance of the proposed system may further be improved by improving stemming process exploring more number of features and applying learning algorithm for effective feature combination
traditionally more than one reference summaries are used for evaluating each system generated summary but in our work we have used only one reference summary for summary evaluation
in future we will consider more than one reference summaries for summary evaluation
das a
bandyopadhyay s

topic based bengali opinion summarization
references ramanathan a
rao d
d

a lightweight stemmer for hindi
in the coling posters
proceedings of eacl
dorr b
j
zajic d
schwartz r

hedge trimmer a parse and trim approach to headline generation
in proceedings of the hlt naacl text summarization workshop and document understanding conference duc pp
edmonton alberta
paice c
d
jones p
a

the identification of important concepts in highly structured technical papers
in the proceedings of the international conference on research and development in information retrieval sigir
lin c y and hovy e

identifying topics by position
in proceedings of the applied natural language processing conference
new brunswick new jersey association for computational linguistics
radev d
r
jing h
sty m
tam d

centroid based summarization of multiple documents
journal of information processing and management elsevier volume issue pp

radev d
allison t
blair goldensohn s
blitzer j
celebi a
drabek e
lam w
liu d
otterbacher j
qi h
saggion h
teufel s
topper m
winkel a
zhang z

mead a platform for multidocument multilingual text summarization
in proceedings of the international conference on language resources and evaluation lrec lisbon portugal
zajic d
dorr b
j
schwartz r

bbn umd at topiary
in the association for the north american chapter of proceedings of computational linguistics workshop on document understanding boston ma pp

zajic d
dorr b
schwartz r

automatic headline generation for newspaper stories in workshop on automatic summarization philadelphia pa pp

miller g

wordnet a lexical database for english communications of the association for computing machinery cacm
salton g
singhal a
mitra m
buckley c

automatic text structuring information processing and management
and summary
journal of
edmundson h
p

new methods in automatic extracting
journal of the association for computing machinery
luhn h
p

the automatic creation of literature abstracts
ibm journal of research development
mani i

automatic summarization volume of natural language processing amsterdam philadelphia john benjamins publishing company
jing
h

using hidden markov modeling to decompose human written summaries
computational linguistics
jing h
mckeown k

the decomposition of human written summary sentences in the proceedings of international conference on research and development in information retrieval university of california berkeley august pages
kupiec j
pedersen j
o
chen f

a trainable document summarizer
in proceedings of research and development in information retrieval pp
conroy j
m
d
p

text summarization via hidden markov models and pivoted qr matrix decomposition
tech
rep
university of maryland college park
morris j
hirst g

lexical cohesion computed by thesaural relations as an indicator of the structure of text computational linguistics
halliday m
a
k
hasan r

cohesion in text
longman london
halliday m
a
k hasan r

cohesion in english
english language series longman london
banko m
mittal v
witbrock m

headline generation based on statistical translation
in proceedings of the annual meeting of the association for comptational linguistics hong kong pp

osborne m

using maximum entropy for sentence extraction
in proceedings of the workshop on automatic summarization volume philadelphia pennsylvania annual meeting of the acl association for computational linguistics morristown
baxendale p

man made index for technical literature an experiment
ibm journal of research and development pages
barzilay r
elhadad
m

using lexical chains for text summarization
in proceedings of the workshop on intelligent scalable text summarization
pp
madrid spain

