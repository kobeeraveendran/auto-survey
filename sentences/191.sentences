c e d l c
s c v
v i x r a neural abstractive text summarization and fake news detection soheil esmaeilzadeh
edu stanford university ca gao xian peh
edu stanford university ca angela xu
edu stanford university ca abstract in this work we study abstractive text summarization by exploring different models such as lstm encoder decoder with attention pointer generator networks age mechanisms and transformers
upon extensive and careful hyperparameter tuning we compare the proposed architectures against each other for the abstractive text summarization task
finally as an extension of our work we apply our text summarization model as a feature extractor for a fake news detection task where the news articles prior to classication will be summarized and the results are compared against the classication using only the original news text
keywords lstm encoder deconder abstractive text summarization generator coverage mechanism transformers fake news detection introduction pattern recognition and data understanding has been the topic of research in multiple deep learning tasks such computer vision and natural language processing
in the natural language processing area understanding the content and main idea of a text and summarizing a corpus is of great importance
in simple words text summarization is the task of creating a summary for a large piece of text
generating meaningful summaries of long texts is of great importance in many different areas such as medical educational media social and
where the summary needs to contain the main contextual aspects of the text while reducing the amount of unnecessary information
in general text summarization can be classied into two main groups extractive summarization and abstractive summarization
extractive summarization creates summaries by synthesizing salient phrases from the full text verbatim however abstractive summarization creates an internal semantic representation of the text
unlike extractive summarization which concatenates sentences taken explicitly from the source text abstractive text summarization paraphrases the text in a way that it is closer to the human s style of summarization and this makes abstractive text summarization a challenging yet preferable approach
decent quality summaries using abstractive approaches were only obtained in the past few years by applying the sequence to sequence endoder decoder architectures with attention mechanisms common in machine translation tasks to summarization however only focused on short input texts
subsequent works attempted to perform the abstractive summarization task on longer input texts however appearance of unknown words and repetitions adversely affected the outcome of the summarization tasks
in this work we focus on abstractive text summarization as a more robust approach compared to its counterpart i
e
extractive summarization and explore recent advancements in the state of the art natural language models for abstractive text summarization
the input of our natural language model is a single document or article and the output of it is a combination of a few sentences that summarize the content of the input document in a meaningful manner
in addition to the main goal of this work after exploring the natural language models for abstractive text summarization we use the summarization model as a feature building module for fake news detection and news headline generation and show the effect of summarization on fake news detection
preprint
work in progress
approaches figure baseline sequence to sequence model s architecture with attention
baseline model in this work as the baseline model we consider an lstm encoder decoder architecture with attention as shown in figure
sequence to sequence encoder decoder the sequence to sequence framework consists of a recurrent neural network rnn encoder and an rnn decoder
the rnn encoder as a single layer bidirectional long short term memory lstm unit reads in the input sequence token by token and produces a sequence of encoder s hidden states hi that encode or represent the input
the rnn decoder as a single layer unidirectional lstm generates the decoder s hidden states st one by one which produces the output sequence as the summary
attention mechanism in the attention mechanism an attention distribution at is calculated as a probability distribution over the words in the source text that helps the decoder decide which source words to concentrate on when it generates the next word
the attention distribution at is calculated for each decoder timestep t as et i vt w st battn at where v wh ws battn are learnable parameters
on each decoder s step attention weights at i which are part of the at distribution for the source words are computed
an attention weight represents the amount of attention that should be paid to a certain source word in order to generate an output word decoder state in the decoder
the attention distribution is used to compute a weighted sum of the encoder hidden states known as the context vector h t which represents what has been read from the source for this step and can be calculated as h t at ihi
i the context vector along with the decoder s state are then used to calculate the vocabulary distribution pvocab which provides a nal distribution for predicting words w as pvocab st h where v v b and are learnable parameters
subsequently we calculate the loss for the timestep t as the negative log likelihood of the target word w as losst log the overall loss for the whole sequence is the average of the loss at each time step i
e
losst as t
loss losst
t t baseline model s problems some problems are associated with the baseline model proposed in section

one problem is the model s tendency to reproduce factual details inaccurately this happens specially when an uncommon word that exists in the vocabulary is replaced with a more common word
another problem with the baseline model is that during summary generation it repeats the already generated parts of the summary
lastly the baseline is unable to handle out of vocabulary words oov
in general it is hard for the sequence to sequence with attention model to copy source words as well as to retain longer term information in the decoder state which leads to the aforementioned issues
see proposed a so called pointer generator network that also includes a coverage mechanism in order to address these problems by combining both context extraction pointing and context abstraction generating
we revisit the model proposed by see in the following and as well compare it with a transformer based model proposed by for machine translation tasks and nally use it as a feature generation mechanism for fake news classication

pointer generator network figure pointer generator model s architecture pointer generator mechanism pointer generator is a hybrid network that chooses during training and test whether to copy words from the source via pointing or to generate words from a xed vocabulary set
figure shows the architecture for the pointer generator mechanism where the decoder part is modied compared to figure
in figure the baseline model only an attention distribution and a vocabulary distribution are calculated
however here in the pointer generator network a generation probability pgen which is a scalar value between and is also calculated which represents the probability of generating a word from the vocabulary versus copying a word from the source text
the generation probability pgen weights and combines the vocabulary distribution pvocab used for generating and the attention distribution a used for pointing to source words into the nal distribution pnal as pgen ai
i wi w based on equation the probability of producing word is equal to the probability of generating it from the vocabulary multiplied by the generation probability plus the probability of pointing to it anywhere it appears in the source text multiplied by the copying probability
compared to the lstm encoder decoder model with attention as baseline in section
the pointer generator network makes it easy to copy words from the source text by putting sufciently large attention on the relevant word
it also is able to copy out of vocabulary words from the source text enabling the model to handle unseen words while allowing to use a smaller vocabulary leading to less computation and storage space
the pointer generator model is also faster to train as it requires fewer training iterations to achieve the same performance as the baseline model in section

figure
the transformer model architecture
left scaled dot product attention
right multi head attention consists of several attention layers running in parallel coverage mechanism to reduce the repetition during summarization as a common issue with sequence to sequence models mentioned in section
we apply the coverage mechanism rst proposed by and adapted by
the coverage mechanism keeps track of a coverage vector computed as the sum of attention distributions over previous decoder time steps
this coverage vector is incorporated into the attention mechanism and represents the degree of coverage that words in the source text have received from the attention mechanism so far
thus by maintaining this coverage vector which represents a cumulative attention the model avoids attending to any word that has already been covered and used for summarization
on each timestep t of the decoder the coverage vector ct is the sum of all the attention distributions so far as
ct this coverage vector also contributes to computing the attention mechanism described in the previous section so that equation becomes vt w st wcct et i battn
intuitively this informs the attention mechanism s current timestep about the previous attention information which is captured in ct thus preventing repeated attention to the same source words
to further discourage repetition see penalizes repeatedly attending to the same parts of the source text by dening a coverage loss and adding it to the primary loss function in equation
this extra coverage loss term penalizes any overlap between the coverage vector ct and the new attention distribution at as covlosst i ct
i finally the total loss becomes loss t we have consulted the gihub repositories referenced at the end of this report
covlosst
for the aforementioned models
transformers in this part we revisit the transformers network proposed by vaswani for machine translation and investigate its performance on abstractive text summarization on our dataset
in the transformer model the encoder maps an input sequence of symbol representations as


xn to a sequence of continuous representations as z


zn
given z the decoder then generates an output sequence as y


of symbols one element at a time
at each step the model is auto regressive consuming the previously generated symbols as additional input when generating the next
the transformer follows this overall architecture using stacked self attention and point wise fully connected layers for both the encoder and decoder shown in the left and right halves of figure respectively
the encoder part of this architecture is mainly a stack of some identical layers where each one has two sublayers
the rst is a multi head self attention mechanism and the second is a simple position wise fully connected feed forward network
the decoder is also composed of a stack of identical layers
in addition to the two sub layers in each encoder layer the decoder inserts a third sub layer which performs multi head attention over the output of the encoder stack
in the transformer architecture a variation of attention mechanism called scaled dot product attention is used where the input consists of queries and keys of dimension and values of dimension dv
the dk the result goes through a dot products of the query with all keys is calculated then divided by softmax function to obtain the weights on the values
in practice the attention function is computed on a set of queries simultaneously packed together into a matrix q
the keys and values are also packed together into matrices k and v where the matrix of output can be calculated as k v softmax v
qk t in the proposed transformer model by instead of performing a single attention function they linearly project the queries keys and values different times with different learned linear projections and that way they build a multi head attention
on each of the projected versions of queries keys and values they then perform the attention function in parallel yielding multi dimensional output values which are concatenated and once again projected figure
for the transformer model we have consulted the gihub repositories referenced at the end of this report
experiments
dataset overview preprocessing to train our summarization models we use the cnn dailymail dataset a collection of news articles and interviews that have been published on the two popular news websites cnn
com and mail
com
like the common styles on newspapers and journals each article contains highlighted sections that together form the summary of the whole article
the raw dataset includes the text contents of web pages saved in separate html les
we use the cnn and dailymail dataset provided by deepmind
our dataset is split in

between training dev and test set respectively leading to training pairs validation pairs and test pairs
there is an average of tokens per news article
each reference summary contains
sentences and tokens on average
we preprocess the dataset and convert the characters all to lower case
we use the stanford corenlp library to tokenize the input articles and their corresponding reference summaries and to add graph and sentence start and end markers as and s respectively
in addition we have tried limiting our vocabulary size to and

evaluation metric we evaluate our models with the standard rouge recall oriented understudy for gisting ation score a measure of the amount of overlap between the system generated and the reference summaries
we report the precision and recall scores for and rouge l which measure respectively the word overlap bigram overlap and longest common sequence between the system generated and reference summaries
the rouge recall and precision for summarization task can be calculated as rouge recall number of overlapping words total words in reference summary rouge precision number of overlapping words total words in system summary where the system summary refers to the summary generated by a summarization model
using precision it s possible to measure essentially how much of the system summary was in fact relevant or needed and using recall rouge it s possible to measure how much of the reference summary is the summarization model generating
in terms of measuring the overlapping words in equations and considering the overlap of unigrams or bigrams or longest common sequence leads to and rouge l scores respectively for precision and recall

experimental details results and analysis text summarization in this work we investigate the performance of the summarization models presented in section namely
lstm encoder decoder with only attention mechanism baseline
lstm encoder decoder with attention and pointer generator mechanisms
lstm encoder decoder with attention pointer generator and coverage mechanisms and
transformers
table shows the and rouge l scores for the four different models that have been trained on the summarization dataset
we have trained the models upon hyperparameter tuning using adagrad optimizer for iterations epochs
our training results outperform the similar ones presented by for cases and and are very close in case
model rouge l



precision recall











precision recall











precision recall







table
lstm encoder decoder with only attention mechanism baseline
lstm decoder with attention and pointer generator mechanisms
lstm encoder decoder with attention pointer generator and coverage mechanisms and
transformers figure validation and training loss values v
s
the number of iterations for summarization models figure shows the loss on the training set and validation set for as a function of number of iterations for the summarization models for iterations epochs
the results of summarization are compared for one case v
s
its ground truth for the three summarization models in table
as it can be seen the summary generated by model contains unk instead of the word mysak in the original summary
however due to having attention and the pointer generator mechanism model has replaced the unk with the proper word from the source text
however summary of model has repeated a sentence twice
the summary generated by the pointer generator together with the coverage mechanism not only could have overcome the unk problem but also does not have repetition in the generated summary and gives a nice summary pretty close to the reference summary





reference model model model model once a super typhoon maysak is now a tropical storm with mph winds
it could still cause ooding landslides and other problems in the philippines
gained super typhoon status thanks to its sustained mph winds
it s now classied as a tropical storm
it s expected to make landfall sunday on the southeastern coast of province
tropical storm maysak approached the asian island nation saturday
it s now classied as a tropical storm according to the philippine national weather service
it s now classied as a tropical storm according to the philippine weather service
just a few days ago maysak gained super typhoon status thanks to its sustained mph winds
it s now classied as a tropical storm according to the philippine national weather service
super typhoon could weaken
new jersey but it will
philippine ocean strength
at least people are injured including
table comparison of the generated summary using the summarization models v
s
the ground truth the summary generated by the transformer model can only capture some keywords but does not convey the grasp of summary very well
fake news detection subsequent to summarization in this part we use the best summarization model that we have trained on the summarization dataset in order to create summaries of a fake news detection dataset
we will build a fake news detection model and we investigate its performance when the input is the original news text the news headline and the summarized news text generated by our summarization model
basically we use our text summarizing model as a feature generator for a fake news classication model
in fake news classication the article content contains much more information than the article headline and due to this a fake news classier performs better on article contents than on article headlines
figure fake news classication architecture full body text headline text figure confusion matrix for test set of fake news detection task using three different input features summary text input input features exp
train acc
valid loss full body text headline text summary text lstm lstm lstm lstm cells lstm lstm lstm lstm size drop out bi lstm bi lstm bi lstm bi lstm bi lstm bi lstm bi lstm bi lstm bi lstm bi lstm bi lstm bi lstm























lstm lstm lstm lstm train loss






































































valid acc
























table experiments on the fake news detection input features accuracy average length in words full body text headline text summary text


table fake news classier results for fake news classication we use a fake news dataset with headlines and article content provided by george mcintire
the dataset contains fake news articles and real articles i
e
a balanced dataset on politics from a wide range of news sources
we shufe the data and use of it for training of it for validation and for testing and also do fold cross validation
we build a long short term memory lstm network together with an embedding layer as shown in figure
table shows our hyperparameter studies for fake news classication and table shows the nal test accuracies using the three input features of full body text headline text and generated summary texts by our summarization models
as it can be seen in this table the best model using the body text as input features perform better than headline text as input
furthermore it s worth noting that the summary text as input feature leads to an even higher accuracy compared to the full body text as input feature
this nding shows that summarization model serves as a feature generator for fake news detection task which actually increases its accuracy
also this summarization model can also serve as a headline generator for the news articles as an automatic approach
conclusion as we showed in section
the pointer generator architecture with attention and coverage nisms led to the highest accuracies and could overcome the problems common in abstractive text summarization such as out of vocabulary words and repetition
furthermore as shown in section
a text summarizing model can successfully be applied as a feature generator prior to classication tasks such as fake news classication and increase the accuracy of those tasks

datasciencecentral
com proles blogs on building a fake news classication model references soheil esmaeilzadeh ouassim khebzegga and mehrad moradshahi
clinical parameters prediction for gait disorder recognition






soheil esmaeilzadeh dimitrios ioannis belivanis kilian m
pohl and ehsan adeli
to end alzheimer s disease diagnosis and biomarker identication
machine learning in medical imaging
mlmi
pp
vol



soheil esmaeilzadeh yao yang and ehsan adeli
end to end parkinson disease diagnosis using brain mr images by cnn




org

pengxiang cheng and katrin erk attending to entities for better text understanding




org
hui liu qingyu yin and william yang wang towards explainable nlp a generative nation framework for text classication



org
mehdi allahyari seyedamin pouriyeh mehdi asse saeid safaei elizabeth d
trippe juan b
gutierrez and krys kochut
text summarization techniques a brief survey




org
bonnie dorr david zajic and richard schwartz
hedge trimmer a parse and trim approach to headline generation
proceedings of the hlt naacl text summarization workshop
pp




ramesh nallapati feifei zhai and bowen zhou
summarunner a recurrent neural network based sequence model for extractive summarization of documents




org
chandra khatri gyanit singh and nish parikh
abstractive and extractive text summarization using document context vector and recurrent neural networks



https
org
shen gao xiuying chen piji li zhaochun ren lidong bing dongyan zhao and rui yan
abstractive text summarization by incorporating reader comments




org
dzmitry bahdanau kyunghyun cho and yoshua bengio
neural machine translation by jointly learning to align and translate




org
ramesh nallapati bing xiang and bowen zhou
sequence to sequence rnns for text rization
iclr
karl moritz hermann tomas kocisky edward grefenstette lasse espeholt will kay mustafa suleyman and phil blunsom
teaching machines to read and comprehend
advances in neural information processing systems
nips
abigail see peter j
liu and christopher d
manning
get to the point summarization with pointer generator networks




org

ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan n
gomez lukasz kaiser and illia polosukhin
attention is all you need



https
org

zhaopeng tu zhengdong lu yang liu xiaohua liu and hang li
modeling coverage for neural machine translation




org
danqi chen jason bolton and christopher d
manning
a thorough examination of the cnn daily mail reading comprehension task
proceedings of the annual meeting of the association for computational linguistics
acl


mahnaz koupaee and william yang wang
wikihow a large scale text summarization dataset




org
chin yew lin and marina rey
rouge a package for automatic evaluation of summaries
text summarization branches out
acl

