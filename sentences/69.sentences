different approaches for identifying important concepts in probabilistic biomedical text summarization milad moradi nasser department of electrical and computer engineering isfahan university of technology isfahan iran e mail milad

iut
ac
ir
iut
ac
ir abstractautomatic text summarization tools help users in biomedical domain to acquire their intended information from various textual resources more efficiently
some of the biomedical text summarization systems put the basis of their sentence selection approach on the frequency of concepts extracted from the input text
however it seems that exploring other measures rather than the frequency for identifying the valuable content of the input document and considering the correlations existing between concepts may be more useful for this type of summarization
in this paper we describe a bayesian summarizer for biomedical text documents
the bayesian summarizer initially maps the input text to the unified medical language system umls concepts then it selects the important ones to be used as classification features
we introduce different feature selection approaches to identify the most important concepts of the text and to select the most informative content according to the distribution of these concepts
we show that with the use of an appropriate feature selection approach the bayesian biomedical summarizer can improve the performance of summarization
we perform extensive evaluations on a corpus of scientific papers in biomedical domain
the results show that the bayesian summarizer outperforms the biomedical summarizers that rely on the frequency of concepts the domain independent and baseline methods based on the recall oriented understudy for gisting evaluation rouge metrics
moreover the results suggest that using the meaningfulness measure and considering the correlations of concepts in the feature selection step lead to a significant increase in the performance of summarization
sentence classification keywordsmedical text mining data mining bayesian classification feature selection umls concept corresponding author
address department of electrical and computer engineering isfahan university of technology isfahan iran
phone fax alternate email
com manuscript march
introduction biomedical information available for researchers and clinicians is accessible from a variety of sources such as scientific literature databases electronic health record ehr systems web documents e mailed reports and multimedia documents
the scientific literature provides a valuable source of information to researchers
it is widely used as a rich source for assessing the newcomers in a particular field gathering information for constructing research hypotheses and collecting information for interpretation of experimental results
it is interesting to know that the us national library of medicine has indexed over million citations from more than biomedical journals in its medline bibliographic
however a large amount of data can not be effectively used to attain the desirable information in a limited time
the required information should be accessed easily at the right time and in the most appropriate form
for clinicians and researchers efficiently seeking useful information from the ever increasing body of knowledge and other resources is excessively time consuming
managing this information overload is shown to be a difficult task without the help of automatic tools
automatic text summarization is a promising approach to overcome the information overload problem reducing the amount of text that must be read
it can be used to obtain the gist efficiently on a topic of interest
it helps the clinicians and researchers to save their time and effort required to seek information
five reasons have been identified for producing summaries from full text documents even when they provide abstracts
the reasons include there are variants of an ideal summary in addition to the abstract some content of the full text may be missed in the abstract customized summaries are useful in question answering systems automatic summaries allow abstract services to scale the number of documents they can evaluate and assessing the quality of sentence selection methods can be helpful in development of document summarization systems
in recent years various summarization methods have been proposed based on biomedical concepts
they have improved the performance of biomedical text summarization focusing on concepts extracted from the source text rather than terms
this concept level analysis of text is performed with the help of biomedical knowledge sources such as the unified medical language system umls
it has been shown that constructing a frequency distribution model from the concepts of the original text and following this model to create the summary yields better performance compared to traditional term based methods
another successful biomedical summarization approach relies on concept chaining identifying strong chains based on the frequency of their contained concepts and sentence scoring according to the presence of the relevant concepts from the strong chains
regarding such summarization methods some questions should be taken into account for summarizing a document
do similar approaches like probability distribution modeling of concepts yield a desirable summarization performance for concept based biomedical text summarization
nlm
nih
gov databases
html manuscript march should the summarizer consider all the concepts extracted from the source text are there any concepts which can be regarded as redundant and removed to increase the accuracy of the frequency or probability distribution model are there any criteria rather than the frequency to identify the important concepts can the model be more accurate by considering the correlations existing between concepts in the source text in this paper we address these questions describing a bayesian summarization method based on the probability distribution of concepts within the input document
we also introduce and evaluate different feature selection approaches to select the relevant concepts of the text and to use them as the classification features
the bayesian summarizer initially maps the input text to biomedical concepts contained in the umls a well known knowledge source in biomedical sciences maintained by the us national library of medicine
then it identifies important concepts and selects them as classification features
to this end we discuss five different feature selection strategies based on various criteria and methods
the first strategy is the simplest one that considers all the extracted concepts
the second method discards the concepts which seem to be potentially redundant and unnecessary
the third method filters the concepts using the ranking method and according to the frequency of the concepts
the fourth method utilizes a meaningfulness measure defined by the helmholtz principle to identify the important concepts
the fifth method discovers the correlated concepts that represent the subtopics of the text by using frequent itemset mining a well known data mining technique
after the feature selection step the summarizer represents each sentence as a vector of boolean features and specifies the value of features according to the occurrence of important concepts in the sentence
afterwards in the classification stage it classifies the sentences into summary and non summary classes using the nave bayes classification method
the classifier estimates the posterior probability of classifying a sentence using the prior and likelihood probabilities of concepts
the summarizer selects the sentences that obtain the highest posterior odds ratio por values and puts them together to form the final summary
to evaluate the performance of the proposed method we conduct a set of experiments on a of scientific papers from the biomedical domain and compare the results with other concept based biomedical summarizers
we also evaluate the usefulness of the five different feature selection approaches to determine the competency of each one for this type of summarization
the results demonstrate that when the bayesian summarizer uses the fourth and the fifth feature selection methods it performs significantly better than the other frequency based biomedical summarizers regarding the most commonly used recall oriented understudy for gisting evaluation rouge metrics
the main contributions of this paper are using the nave bayes classifier in concept based biomedical text summarization for classifying the sentences of a document based on the probability distribution of important concepts within the source text manuscript march evaluating different feature selection approaches to identify the important concepts of a document and using them as classification features using different measure i
e
the meaningfulness rather than the frequency to determine the important concepts of a document and using them as classification features and discovering the correlated concepts of a document using itemset mining and using them as classification features
the remainder of the paper is organized as follows
section gives an overview of text summarization as well as a review of the related work in biomedical summarization
in section we introduce our biomedical summarization method based on a bayesian classifier
we also define different feature selection approaches that can be utilized in our summarization process
then we describe the evaluation methodology in section
the results of the preliminary experiments and the evaluations are presented in section and discussed in section
finally section draws the conclusion and describes future lines of work

background and related work

biomedical text summarization text summarization methods can be divided into abstractive and extractive approaches
an abstractive summarizer uses natural language processing nlp methods to process and analyze the input text then it infers and produces a new version
on the other hand an extractive summarizer selects the most representative units paragraphs sentences or phrases from the original wording and puts them together into shorter form
another classification of text summarization differentiates single document and multi document inputs
a single document summarizer produces a summary which is the result of condensing only one document
in contrast a multi document summarizer gets a cluster of inputs and provides a single summary
another classification of summarization methods is based on the requirements of users generic vs
oriented also known as query focused summarizers
a general summary presents an overall implication of input without any specified preferences regarding content
while a user oriented summary is biased towards a given query or some keywords to address a user s specific information requirement
summarization systems can be supervised or unsupervised regarding whether they need training data
a supervised system learns from labeled data to select the essential content of new documents while an unsupervised system generates summaries for new documents without relying on any training data
in addition to the above categorizations there are other types of summaries including indicative informative multi lingual mono lingual cross lingual web based e mail based personalized sentiment based survey and update summaries
the bayesian summarizer described in this paper is extractive single document generic and unsupervised
manuscript march in the biomedical field various summarization methods have been proposed
these methods have been reviewed in a survey of early work and in a systematic review of recently published research
there have been some research works towards abstractive biomedical summarization
they could be regarded as tools for providing decision support data from medline citations summarizing research related to the treatment of diseases helping in evidence based medical care summarizing drug information and multi document summarization of medline citations
these methods mostly produce graphical summaries
on the other hand the majority of extractive biomedical summarization systems focus on producing textual summaries
extractive summarization methods have been widely studied in the biomedical domain for different tasks such as summarizing clinical notes developing clinical decision support tools for patient specific recommendation and treatment and the summarization of ehrs
many biomedical summarizers utilize the umls knowledge source to map the input text to a wide range of biomedical and generic concepts
this mapping helps the systems to be domain specific and act more accurately compared to traditional term based methods
there are several knowledge sources such as mesh snomed go omim uwda and ncbi taxonomy widely used in knowledge intensive data and text processing tasks in the biomedical domain
these knowledge sources along with over controlled vocabularies classification systems and additional information sources have been unified into the umls
plaza performed an investigation on the impact of different knowledge sources on the performance of a summarization system
the evaluations showed that the quality of generated summaries was improved significantly with the use of an appropriate knowledge source
we make use of the umls concepts to incorporate the domain knowledge into the text modeling and the summarization process of our summarizer
some of the biomedical summarization methods employed graph representation along with the umls concepts for semantic modeling of the input text
plaza al
proposed a graph based approach to biomedical summarization
they used the umls concepts and the semantic relations between them to construct a semantic graph that is representative of the input document
their system determined different topics within the text by applying a degree based clustering algorithm on the semantic graph
another work performed the task of summarization based on a genetic graph based clustering algorithm
using the continuity of concept relations rather than the centroid method it separated clusters and identified main topics
menendez et al
applied a combination of both genetic clustering and graph connectivity information to improve the performance of the previous semantic graph based summarization systems
compared to these approaches our bayesian summarization method utilizes a simpler modelling representing the sentences of the input document as vectors of features
the features are important concepts within the input text
merging the domain knowledge and traditional methods some domain specific tools have been proposed for biomedical summarization
one of the studies identified a set of medical cue terms and phrases and combined them with commonly used traditional features such as word frequency sentence position the similarity with the title of the article and sentence length
the summarizer used the domain specific and manuscript march generic features for sentence scoring and summary generation
sarkar al
proposed a supervised summarization method based on bagging and
decision tree as the base learner
they utilized the key terms in mesh as a source of domain knowledge as well as other features including centroid overlap first sentence overlap sentence position sentence length and acronyms
in a hybrid summarization system a classifier was utilized to learn and group sentences into six types of population intervention background outcome study and other
the system also used a learning to identify important umls concepts commonly appearing in summaries
relative sentence position and sentence length were other features used by the summarizer
another study evaluated different positional approaches for sentence extraction in a semantic graph based method for biomedical literature summarization
the study showed that sentences appearing in various sections of a biomedical article should be assigned different weights
we do not use any positional information in our summarization method
this allows the method to be applicable to input texts in which positional information may not be indicative of the importance and the informativeness of sentences
some of the biomedical summarizers use the frequency of umls concepts extracted from the source text as the basis of their summarization approach
biochain method pursued the lexical chaining idea creating chains and putting each group of semantically related terms into a chain
biochain used concepts rather than terms and put concepts belonging to the same semantic type into a chain
it computed the score of each chain using the frequency information of concepts identified strong chains and concepts and selected summary sentences according to the presence of strong concepts
freqdist method performed the task of sentence selection based on the frequency distribution of concepts within the source text
it initially created a frequency distribution model from the source text also a summary frequency distribution model
using an iterative sentence selection procedure it selected a sentence that led to the closest alignment between the summary and original text frequency distributions in each iteration
in contrast to biochain and freqdist our method does not merely make use of the concept frequency
employing the nave bayes classifier it selects the sentences according to the probability distribution of concepts within the input text
it still benefits from the frequency information in the form of two coefficients that provide the classifier with additional knowledge
compared to freqdist our method does not consider the distribution of all the extracted concepts
we evaluate different feature selection strategies to discard redundant and unnecessary concepts
compared to biochain our method does not merely rely on the concept frequency to identify important concepts
we use another metric namely the meaningfulness in the form of a feature selection method that yields better summarization performance
moreover as one of the feature selection approaches we extract correlated concepts and use each set of them as a classification feature
this correlation information provides the classifier with some additional knowledge to decide more accurately leading to an increase in the performance of the summarizer
in domain independent text summarization some methods have been proposed based on bayesian approach
one of the basic methods employed a set of features such as sentence length cut off manuscript march phrase paragraph thematic word and uppercase word to represent the sentences of a text document
it trained a bayesian classifier on a training corpus and used the classifier to summarize new documents
bayesum a supervised and query focused multi document summarizer built on bayesian inference and language modeling techniques represented documents and queries as probability distributions of words from a vocabulary
estimating a sentence model for each sentence it ranked sentences based on the language model and the similarity of sentences to the query model
wang et al
proposed a bayesian sentence based topic model for multi document summarization
they employed a variational bayesian to model the probability distribution of selecting sentences given topics and to estimate the model s parameters
compared to these methods our biomedical summarizer does not use any complicated topic modeling approaches and does not need any training data
it uses the prior probability of concepts to estimate the probability of selecting sentences for inclusion in the summary
we give the nave bayes classifier some additional knowledge in the form of two coefficients and different feature selection approaches
to the authors knowledge no biomedical summarization method has been proposed so far based on the nave bayes classifier and estimating the probability of summary sentences by following the distribution of concepts within the source text
the rationale of our approach will be presented in section
where it will be showed that for a corpus of biomedical papers the concepts within both the full text papers and the ideal summaries abstracts follow the same distribution


bayesian classification our proposed summarization scheme consists of two main phases preparation and classification
in the preparation phase we perform concept extraction feature selection and sentence representation
in the classification phase we utilize a bayesian classifier to select sentences for the final summary
in the following we give an overview of bayesian classification
in general a bayesian classifier is based on the bayes theorem defined by eq
below where c and x are random variables
in classification tasks they refer to observing class c and instance x respectively
x is a vector containing the values of features
is the posterior probability of observing class c given instance x
in classification it could be interpreted as the probability of instance x being in class c and is what the classifier tries to determine
is the likelihood which is the probability of observing instance x given class c
it is computed from the training data
and are the prior probabilities of observing class c and instance x respectively
they measure how frequent the class c and instance x are within the training data
using eq
the classifier can compute the probability of each class of target variable c given instance x and the most probable class the class that maximizes should be selected as the manuscript march result of classification
this decision rule is known as maximum a posteriori map
it is represented as follows arg is the jth class or value of target variable c
in eq
the denominator is removed because it is where constant and does not depend on
making a conditional independence assumption the nave bayes classifier reduces the number of probability values that must be estimated
it assumes that the probability of each value of feature xi is independent of the value of any other features given the class variable cj
therefore the nave bayes classifier finds the most probable class for the target variable by simplifying the joint probability calculation as follows arg max the posterior odds ratio por is a well known measure to assess the confidence of bayesian classification
the por shows a measure of the strength of evidence for a particular classification compared to other class values
it is calculated as follows is the posterior odds ratio that measures the strength of evidence in favor of classifying the against classifying the instance as class variable
where instance as class variable
the bayesian summarizer our bayesian summarization method consists of five steps including mapping text to biomedical concepts feature selection preparing sentences for classification sentence classification using nave bayes and summary generation
fig
illustrates the architecture of the bayesian summarizer
in the following subsections we give a detailed description of each step
manuscript march fig

the architecture of our proposed bayesian biomedical text summarization method


mapping text to biomedical concepts firstly the summarizer maps the input text to the concepts of the umls metathesaurus
the metathesaurus is a large multi lingual and multi purpose lexicon that contains millions of biomedical and health related concepts their relationships and their synonymous names
in addition to the metathesaurus the umls includes two main components namely specialist lexicon and semantic network
the specialist lexicon is a lexicographic information database intended to use in nlp systems
it contains commonly occurring english words and biomedical vocabulary and records their syntactic morphological and orthographic information
the semantic network consists of a set of broad subject categories known as semantic types
these semantic types provide a categorization of all the concepts included in the metathesaurus
it also contains a set of semantic relations between the semantic types
for mapping biomedical text documents to the umls metathesaurus concepts the us national library of medicine has developed metamap program
using a knowledge intensive approach based on nlp computational linguistic and symbolic techniques metamap identifies noun phrases in a text and extracts corresponding concepts
metamap may return multiple concepts when a noun phrase is mapped to more than one concept
in this situation the summarizer selects all the mappings returned by metamap
it has been shown that the all mappings strategy can be useful in concept based biomedical text summarization
metamap returns a semantic type along with each concept that determines the semantic category of the concept
as noted manuscript march above the semantic types are included in the umls semantic network
for the mapping step we use the version of metamap program and the umls release as the knowledge base
fig
shows a sample sentence and the concepts identified in the first step
fig

a sample sentence and its identified concepts from the umls metathesaurus


feature selection in this step the summarizer identifies important concepts within the input document
to this aim we introduce five feature selection approaches
in the classification step the summarizer uses the important concepts as classification features
we evaluate and discuss the impact of the feature selection approaches on the performance of the bayesian summarizer in section and
in this subsection we use a sample to present some examples of the feature selection strategies
the sample document is a scientific article about genetic overlap between autism schizophrenia and bipolar disorders
it contains sentences



first approach using all extracted concepts as features the simplest approach to feature selection for the bayesian summarizer is to consider all the distinct extracted concepts
it means that the summarizer decides about the summary and non summary sentences considering the distribution of all the contained concepts regardless of whether they are important or not
for example after concept extraction the sample document contains concepts of which are distinct concepts that are used as the classification features
from these distinct concepts concepts appear only one time in the document s sentences and the three most frequent concepts appear in and sentences
summarization method
we use the first feature selection approach as a baseline to assess the amount of improvement obtained by the other strategies
it also shows the impact of redundant and noisy features on the performance of the available at
biomedcentral
com
manuscript march


second approach filtering out generic features there are some semantic types that their concepts can be discarded in the analysis of the input document
they are generic and broad concepts and almost frequently appear in majority of documents either in general english and biomedical texts
these semantic types have been identified empirically and include functional concept qualitative concept quantitative concept temporal concept spatial concept mental process language idea or concept and intellectual product
in the second feature selection method we remove concepts that belong to these semantic types and use remaining concepts as classification features
in this way we remove a set of potentially redundant and misleading features and we expect an improvement in the quality of produced summaries
using this feature selection method the summarizer discards the following concepts from the sentence represented in fig
widening analysis aspect further relationships and etiology aspects
the sample document
as an example of the second approach distinct concepts remain after removing generic concepts from


third approach filtering features by ranking filtering is a category of feature selection methods
in filtering methods features are scored based on some ranking criteria such as relevance correlation mutual information and the frequency of occurrence
then the high ranked features are selected according to a threshold or a predefined number of features
for the third feature selection strategy we filter features by the frequency of their corresponding concepts in the text
after mapping the input text to the umls concepts and removing the potentially redundant features similar to the second approach the summarizer adds remaining features to a list named
then it ranks the features based on the frequency of corresponding concepts such that a higher rank is assigned to the feature that its corresponding concept appears more frequently
finally it filters the features of the using a threshold
it removes a feature if its frequency is less than the threshold value
in the following we introduce three possible thresholds where is the arithmetic mean of all concept frequencies in the and is the standard deviation of all concept frequencies in the
in section we evaluate these three thresholds and use the best one as an optimum threshold for the third feature selection method
manuscript march as an example when we use eq
as the threshold value from the initial distinct concepts of the sample document concepts are selected as features
when we use eq
and eq
as the threshold value and eight features are selected respectively
fig
shows the identified important concepts for the sample document along with their semantic types and frequencies
in this example we use eq
as the threshold value
the and eight of which are selected as features fig

are equal to

and
respectively
there are concepts in the fig

the identified important concepts by filtering ranking and selecting based on a given threshold for the sample document
the semantic types are represented in brackets
the features are sorted based on their frequencies



fourth approach selecting meaningful features by the helmholtz principle the helmholtz principle from the gestalt theory of human perception defines a measure of meaningfulness for rapid change detection and keyword extraction in unstructured and textual data
in data mining context the helmholtz principle states that essential features and interesting events are observed in large deviations from randomness
in text mining research the helmholtz principle has been used for document processing and keyword extraction automatic text summarization and supervised and unsupervised feature selection
the primary study dealt with words to extract the meaningful units of a text document but we deal with concepts instead
in the fourth method the feature selection process is modeled as follows let d be the input document and p be a part of d
we consider p as a paragraph but it can be any structural unit of the input document such as a sentence or a page
after mapping the input document d to the umls concepts we remove generic potentially redundant concepts similar to the second approach and add remaining concepts to the set c
for each concept c in the set c we start to compute the meaningfulness measure by calculating the number of false alarms nfa in each p
if the concept c appears m times in the p and k times in the whole document d then the nfa is calculated as follows where n is equal to
that l is the total number of concepts in the document d and b is the total number of concepts in the paragraph p
in eq
is a binomial coefficient computed as follows manuscript march to measure the meaningfulness value of concept c from d inside p the following formula is used log eventually we construct a set of meaningful concepts
we add each concept c in c that its value is greater than to the where is the maximum of over all paragraphs p and is a parameter that determines the level of meaningfulness
the summarizer selects the concepts included in the as classification features
in section and we evaluate and discuss the optimum value of the parameter
fig

the identified meaningful concepts as the features by the helmholtz principle
for the sample document
the semantic types have been represented in brackets
the features have been sorted based on the descending order of their meaning measures
for example using this feature selection approach for the sample document when is equal to
and
the number of selected features is and respectively
fig
shows the meaningful concepts of the sample document identified by the helmholtz principle with a meaningfulness level of

as can be seen there is no obvious relation between the meaning and frequency values
the meaning values depend on the appearing pattern of the concepts in the paragraphs and the whole document



fifth approach extracting correlated features by itemset mining in the four previous approaches features are representative of single concepts
however some correlations may be exist among multiple concepts
it means that some dependent concepts appear together in sentences manuscript march and point to one of the subtopics of the text
in the fifth feature selection method we utilize frequent itemset mining to extract correlated concepts and use each set of dependent concepts as a classification feature
frequent itemset mining is a data mining technique for finding items that appear together in a dataset
it can be effectively used in our context to discover correlated concepts that frequently appear in the input text
for the fifth method we utilize a well known itemset mining algorithm namely the apriori
although the apriori algorithm is usually used for association rule mining we make use of its ability to extract frequent itemsets
this algorithm works with datasets structured in a transactional format
in a transactional dataset there are several transactions each one contains some items
we can consider the input document as a transactional dataset such that the apriori algorithm deals with each sentence and its contained concepts as a transaction and its items
therefore we perform itemset mining to discover frequent itemsets containing concepts that frequently appear in the input text
each itemset has a property named itemset support calculated by dividing the number of sentences that contain the itemset by the total number of sentences in the document
an itemset is said to be frequent if its support value is greater than or equal to a given minimum support threshold
a k itemset is an itemset which contains k items
if all the subsets of a k itemset are frequent the itemset is said to be a frequent k itemset
given an input document its extracted concepts and a minimum support threshold we perform the apriori algorithm to discover correlated concepts
each set of k correlated concepts discovered as a frequent k itemset demonstrates a subtopic of the document
in section and we evaluate and discuss the optimum value of the parameter
fig

the frequent itemsets extracted as the classification features from the sample document minimum support threshold

manuscript march in the fifth feature selection strategy after mapping the input document to the umls concepts we remove generic concepts similar to the second approach and apply the apriori algorithm
the summarizer uses the extracted frequent itemsets as classification features
for instance when the minimum support threshold is equal to

and
the number of extracted frequent itemsets is and respectively
fig
shows the extracted frequent itemsets extracted from the sample document
in this example the value of minimum support threshold is

the apriori algorithm returns a total number of frequent itemsets
as can be seen seven itemsets contain more than one concept
these itemsets convey the correlations that exist among some dependent concepts


preparing sentences for classification after concept extraction and feature selection the summarizer must represent the sentences of the input document in an appropriate format to be prepared for the classification step
it considers all the selected features as boolean such that for a given sentence it sets the value of a feature to true if the corresponding concept appears in the sentence otherwise it sets the value to false
some features contain more than one concept when the summarizer uses the fifth feature selection method
for such features the summarizer sets the value to true if all the corresponding concepts appear in the sentence
after assigning feature values the summarizer discards the sentences whose all feature values are false
we consider these sentences as unimportant and do not use them in the subsequent steps
for example let the summarizer uses the third feature selection strategy and the threshold for the sample document
fig
shows the selected features
the sample document consists of sentences defined in eq
to filter the features of which sentences do not contain any important concepts
the summarizer considers these sentences as unimportant and discardeds them for the subsequent operations
therefore sentences remain for preparation and classification
note that we use this example hereafter to simplify the explanation of the remaining steps
in case of using the other feature selection approaches the summarizer performs the preparation and classification stages likewise
in this step the summarizer creates a vector of features for each remaining sentence
it assigns a number to each vector according to the corresponding sentence number
for example there are vectors for the sample document each one has eight features
each feature corresponds to an important concept identified in the previous step
the summarizer assigns the values of features in the ith vector according to the presence and absence of important concepts in the ith sentence
for each vector there is a target class or a target variable named summary which is initially unknown
in the classification step our bayesian summarizer determines the value of the target class as yes or no for all the vectors
as an example in the sample document the sentence is therefore just as for deletions it is apparent that these large cnvs confer risk of a range of neurodevelopmental phenotypes including autism mental retardation and schizophrenia
in this sentence four important concepts can be seen identified by manuscript march the threshold gene
hence in the vector the values of these four features are true and the values of the other and represented in fig
including autistic disorder schizophrenia deletion mutation and features are false
fig
shows the vector in the sample document
the summarizer assigns the feature values for all the vectors just in the same way as the above example
after this step we have a collection of vectors and their feature values
their class variables are unknown and the summarizer must classify them as summary sentences yes or non summary sentences no
every document has its particular set of concepts and the features of each text are different from others
fig

the sentence vector corresponding to the sentence in the sample document
the summary class variable is initially unknown


sentence classification the compression rate is a parameter in summarization systems which determines the percentage of the input text that must be extracted as the final summary
the summarizer does not initially know about the in eq
the prior probability of class variable values
however it knows what percentage of sentences that must be selected as summary and it can estimates the and
for instance the total number of sentences in the sample document is
suppose the compression rate is

it means that of the text about sentences must be selected for the final summary
in the preparation step the summarizer discarded the sentences that did not include any important concepts and vectors remained for the classification step
the summarizer does not know which of these vectors has yes value for the summary class variable but it knows vectors must have the yes value
hence for this example the is equal to
and is equal to

generally the is equal to the number of sentences that must be selected for the final summary according to the compression rate divided by the total number of remaining sentences for the classification step
the bayesian summarizer follows an assumption about the in eq
that simplifies the estimation of the likelihood probabilities
the summarizer assumes that the summary can convey an manuscript march informative content if it follows the distribution of important concepts within the input document
with regard to this assumption the summarizer can estimate the likelihood probabilities i
e
the probability of observing an important concept given class yes or no
for example the concept schizophrenia appears in sentences within the sample document and its distribution within all the vectors is equal to

therefore the probability of observing concept schizophrenia given class yes i
e
the would be equal to

likewise the probability of not observing concept schizophrenia given class yes i
e
would be equal to


likewise the summarizer estimates the likelihood probabilities of observing and not observing a concept given class value no
in this step the summarizer can estimate the posterior probability of class values given a vector
if the summarizer selects the value that maximizes the posterior probability of summary variable given ith vector similar to eq
the number of sentences classified as yes may be less than the number of sentences that must be selected for the summary
this comes true because in a vector the number of features having the true value is often less than the number of features having the false value
therefore the summarizer should decide about the summary class values in a different way compared with eq

the summarizer estimates the posterior probability of class values for each vector and assesses the strength of evidence for class value yes using the por measure
we incorporate two coefficients into the estimation of posterior probabilities to discriminate between the presence and absence of more important and less important concepts
in our context the presence of important concepts the true value for the features and their prior probabilities are contributing factors in the selection a sentence for the summary
on the other hand when the suumarizer uses the bayes rule it does not discriminate between the presence and the absence of more important high frequent and less relevant frequent concepts
in this case the summarizer decides based on the highest probable values of features
as we show in section

for the majority of documents even the high frequent concepts appear in less than of the sentences of a document
this shows that for the majority of documents the most probable value for all features is false
therefore for estimating the posterior probabilities of the class value yes and no the summarizer should take into account this issue
we address this problem employing two coefficients in estimation of the posterior probability of class values
the coefficients increase and decrease the impact of important concepts on the posterior probability of class values based on the frequency of concepts and whether they occur in a sentence or not
we evaluate and discuss the impact of using these coefficients on the accuracy of the bayesian summarizer in section and



the probability of inclusion in the summary to estimate the posterior probabilities firstly the summarizer estimates the posterior probability of class value yes given ith vector by rewriting eq
as follows manuscript march where is the ith vector is the posterior probability of classifying the ith vector as yes given is the kth feature in the ith vector and is the likelihood probability i
e
the probability of observing or given class variable
the value of the coefficient depends on whether the is true or false and is specified as follows k where is true or false the coefficient is the frequency of the concept corresponding to the affects the
depending on whether the value of in two ways
when the is true the frequency of corresponding concept is multiplied by the
thus the values of the and increase
consequently the presence of more frequent concepts increases the chance of selecting a sentence for the summary

when the is false the inverted frequency of corresponding concept is multiplied by the and as a result the values of the and decrease
in this case a higher frequency decreases the with a higher rate
hence the absence of more frequent concepts decreases the chance of selecting a sentence for the summary



the probability of exclusion from the summary after estimating the probability of inclusion in the summary the summarizer estimates the posterior probability of class value no given ith vector as follows where is the ith vector is the posterior probability of classifying the ith vector as no given is the kth feature in the ith vector and is the likelihood probability i
e
the probability of observing or given class variable
the value of the coefficient depends on whether the is true or false and is specified as follows manuscript march o n m where the value of
when the is the frequency of the concept corresponding to the is true or false the affects the in two ways
similar to the depending on whether is true the inverted frequency of corresponding concept is multiplied by the and as a the and decrease
in this case a higher frequency decreases the with a higher rate
hence the presence of more frequent concepts decreases the probability of not selecting the values of result a sentence for the summary

when the
thus consequently the absence of more frequent concepts increases the probability of not selecting a sentence is false the frequency of corresponding concept is multiplied by the increase
the values of the and for the summary
after estimating the probability of classifying each vector as yes and no the summarizer needs to decide which sentences should be selected for inclusion in the final summary
as mentioned earlier if the classifier selects for each vector the class value which maximizes the posterior probability of summary class variable the number of sentences classified as yes may be less than the number of sentences required for the summary
therefore we employ the por measure early explained in section
to classify the vectors
for the ith vector the summarizer computes the value of the por by rewriting eq
as follows is the posterior odds ratio of the ith vector
the values of the and where are the posterior probabilities of classifying the ith vector as yes and no given which the summarizer estimated earlier
the por demonstrates a measure of the strength of evidence for a particular class value
therefore the greater pori for a vector the higher strength of evidence in favor of classifying the vector as yes
after calculating the por value for all the vectors the summarizer can decide which sentences should be selected for the summary
it sorts the vectors in descending order of their por values and assigns the ranked n vectors to the class yes where n is the number of sentences which must be selected to make the summary specified by the compression rate
the summarizer assigns the remaining vectors to the class no
manuscript march in the following we explain a redundancy reduction method that the summarizer can use to decrease redundant information in the summary



the redundancy reduction method the problem of redundancy in text summarization concerns the same repeated information conveyed by multiple sentences in a summary
compared to multi document summarization it is less probable to find redundant information in a summary produced for a single document
however redundancy removal approaches can also be useful in single document summarization
maximal marginal relevance mmr is a well known method for removing redundancy especially in query focused summarization
it computes cosine similarities between sentences and a query also between sentences and already selected sentences
then it assigns a marginal relevance to each sentence and adds the sentence with the maximum marginal relevance to the summary
the mmr computes a linear combination of two functions i
e
relevance and novelty
the relevance function needs a query to assess the relatedness of sentences
since the bayesian summarizer does not use any query for summarization the mmr approach is not applicable to our method
hence we propose a redundancy reduction method based on our context by gradually updating the probabilities to decrease the chance of high probable concepts and increase the chance of less probable concepts for inclusion in the summary
we employ an iterative method aimed at reducing the redundancy that can emerge in the summary due to the large prior probability of high frequency concepts
when the possible redundancy does not matter to the summarizer as explained earlier the summarizer estimates the prior likelihood and posterior probabilities
then it computes the por values for all the sentences and ranks them based on their por values
finally it assigns the top n sentences to the class value yes
on the other hand when the summarizer employs the redundancy reduction method it performs the sentence selection process differently
in this case when it computes the por values it assigns only the sentence having the highest por value to the class value yes
next it estimates the prior likelihood and posterior probabilities again but it does not consider the previously selected sentences and their concepts in the subsequent estimations
accordingly it reduces the probability of observing the high frequency concepts included in the sentences already selected
moreover the summarizer increases the chance of observing the low frequency concepts in the summary
in the subsequent iterations it computes the por values based on new probabilities
it repeatedly estimates the probabilities without considering the sentences already selected for the summary and selects the sentence with the maximum por value until the number of summary sentences assigned to the class value yes reaches n
finally it assigns the remaining sentences to the class value no
in section and we evaluate and discuss the efficiency of the redundancy reduction method
manuscript march

summary generation in the previous step the summarizer assigned the sentences to the class values yes and no
in the summary generation step it adds the sentences of the class value yes to the summary
it arranges the summary sentences in the same order as they appear in the primary document
finally it adds the figures and tables in the main document referred to in the summary
fig
shows the summary of the sample document produced by the bayesian summarizer
in this example for brevity reasons the compression rate is
it means that the size of the summary must be of the input document
fig

the summary of the sample document generated by the bayesian summarizer compression

evaluation method

evaluation corpus the most common method of evaluating summaries generated by an automatic summarizer is to compare them against manually generated summaries known as model or reference summaries
in such evaluation method we measure the similarity between the content of system and model summaries
the more content shared between system and model summaries the better the performance of the summarization system
obtaining manually generated summaries is a challenging and time consuming task because they have to be provided by human experts
moreover human generated model summaries are highly subjective
to the authors knowledge there is no of model summaries for single document biomedical text manuscript march summarization
however most scientific papers have an abstract which can be considered as the model summary for evaluation
to compare our bayesian biomedical summarization method against other summarizers we use a collection of biomedical scientific papers randomly selected from the biomed central s for text mining
the size of evaluation corpus is large enough to allow the results of the assessment to be significant
we use the abstracts of the papers as the model summaries to evaluate the performance of the generated summaries
we perform our preliminary experiments using a separate development corpus containing papers randomly selected from the biomed central s corpus


evaluation metrics a common feature which we assess in the performance evaluation of text summarization systems is the informativeness
it is a feature for representing how much information from the original text is provided by the summary
in this paper we use the rouge package to evaluate the performance of the bayesian summarizer in terms of the informative content of summaries
the rouge package compares a generated summary with one or more model summaries estimates the shared content between them by calculating the proportion of shared n grams and produces different scores in terms of different metrics
the rouge metrics produce a score between and
the higher scores for a system summary the greater content overlap between the system and model summaries
in our evaluations we use the following rouge metrics
it computes the number of shared unigrams grams between the system and model summaries
summaries

it computes the number of shared bigrams grams between the system and model rouge
r

it computes the union of the longest common subsequences between the system and model summaries
it takes into account the presence of consecutive matches
rouge r
it computes the overlap of skip bigrams pairs of words having intervening word gaps between the system and model summaries
it allows a skip distance of four between bigrams
in spite of their simplicity the rouge metrics have shown a high degree of correlation with human judges


preliminary experiments and parameterization we introduce five feature selection approaches in section

the first approach selects the classification features by simply considering all the extracted concepts
the second approach tries to reduce the number of features by filtering out the generic features that seem to be potentially redundant
in the third approach we
biomedcentral
com about datamining manuscript march rank the features based on the frequency of corresponding concepts and use a threshold as a filtering criterion
we introduce three possible threshold values in section

based on the average and the standard deviation of the frequency of concepts
in feature selection experiments we evaluate these three values to specify the optimum threshold for this type of filtering
in the fourth feature selection approach we measure a meaningfulness value for each concept using the helmholtz principle
we use a parameter that determines the level of meaningfulness for the concepts selected as classification features
we perform a set of experiments to tune the parameter
in the fifth approach we utilize an itemset mining method to extract correlated concepts in the form of frequent itemsets and use them as classification features
the itemset mining extracts frequent itemsets according to a minimum support threshold
we tune the parameter performing a set of preliminary experiments
in the other set of preliminary experiments we assess the impact of the coefficients and introduced in section
on the performance of our summarization method
we evaluate the quality of the produced summaries in two situations the presence and the absence of the coefficients
in section

we introduce a redundancy reduction method to decrease the potential redundancy in the summary
we conduct another set of experiments to investigate the impact of the redundancy reduction method on the performance of the bayesian summarizer
it seems that the redundancy reduction strategy may achieve more percent of improvement under smaller compression rates
we also assess the percentage of improvement under smaller and larger compression rates than


comparison with other summarization methods we compare the bayesian summarizer with six summarization methods and two baselines
three methods of comparison systems are biomedical summarizers i
e
freqdist biochain and chainfreq
we implement these three summarizers as explained in their original papers
two comparison methods are independent and term based i
e
summa and swesum
one of the methods microsoft autosummarize is a commercial application
the two baseline methods are lead baseline and random baseline
the size of the summaries generated by all the summarizers is of the original documents
the choice of as the compression rate is based on a well accepted standard that says the size of a summary should be between and of the original text
in the following we give a brief description of the competitor methods
freqdist is a biomedical summarization method which uses concept frequency distribution to identify important sentences
it initially maps the input text to the umls concepts
then it creates an empty summary frequency distribution and a source text frequency distribution model counting the concepts
afterwards using an iterative sentence selection process freqdist creates a candidate summary and compares the frequency distribution of the candidate summary with the distribution of the source text
the method evaluates the sentences based on how much they align the frequency distribution of the summary to the original text
it manuscript march selects a sentence in each iteration and adds it to the summary such that the two frequency distributions to be aligned as closely as possible
the original study has compared five similarity functions to find the best one for evaluating the similarity of frequency distributions
we implement and compare freqdist method with the dice s coefficient as it has reported the highest rouge scores in the original study
biochain is a biomedical summarizer based on the lexical chaining idea
it extracts the umls concepts from the input document considers the semantic types as the head of chains and puts the concepts of the same semntic type in the same chain
biochain selects the strong chains based on the core concepts and their frequency identifies the strong concepts of each strong chain and uses the strong concepts to score the sentences
finally it extracts the high scoring sentences and generates the final summary
chainfreq is a hybrid summarizer which makes use of both freqdist and biochain methods
it uses biochain to identify important sentences containing strong concepts
then it sends the candidate sentences to freqdist to reduce the redundancy and to select the subset of sentences which aligns the summary frequency distribution to the source text
in the original study two variants of biochain have been evaluated for chainfreq
the first variant uses all the concepts of strong chains to score the sentences while the second one uses the most frequent concept of each strong chain
in the evaluations the first method has obtained higher rouge scores
accordingly we implement the first method as a part of chainfreq for our evaluations
we also implement freqdist using the dice s coefficient
in addition to the above biomedical summarizers we use three domain independent comparison methods in the evaluations to assess the performance of our method against traditional term based approaches
we give a description of these methods in the following
summa is a summarizer which uses generic and statistical features to score the sentences of an input document
the features that we use for the evaluations include the frequency of terms within the sentence the position of the sentence within the document the similarity between the sentence and the first sentence of the document and the similarity between the sentence and the title
swesum is an online and multi lingual summarizer based on generic features
for the evaluations we use the following set of features presence of the sentence in the first line of the text presence of numerical values in the sentence and presence of keywords in the sentence
we also set the type of text feature to academic
microsoft autosummarize is a feature of the microsoft word
this summarizer performs based on a word frequency algorithm
it assigns a score to each sentence of a document according to the frequency of words contained in the sentence
microsoft word microsoft corporation manuscript march our two baselines for the evaluations are lead baseline that returns the first n sentences of the input document as the summary and random baseline that randomly selects n sentences from the document and generates a summary

results

preliminary experiments experiment results



feature selection in this subsection we first present the results of parameterization and the preliminary experiments which specifies the best settings for the feature selection approaches
then we present the results of experiments conducted to assess the impact of the coefficients and the redundancy reduction method on the performance of the bayesian summarizer
for brevity reasons we only report the and r scores for the preliminary as explained in section
we conduct a set of preliminary experiments to tune the parameters and find the best settings of the feature selection methods
we introduce three possible threshold values for the third method which uses a ranking and filtering strategy
table shows the rouge scores obtained by the bayesian summarizer using the three thresholds
the summarizer obtains the highest scores when it uses the threshold
we use this threshold as the optimal value in the subsequent experiments
table
rouge scores obtained by the bayesian summarizer using the third feature selection approach and three threshold values
the best result for each rouge score is shown in bold type
rouge





in the fourth feature selection approach we use the helmholtz principle to identify meaningful concepts
in this method there is a parameter which specifies the meaningfulness threshold
fig
shows the rouge scores obtained by the bayesian summarizer using the fourth feature selection method and different values of the theshold between


the threshold values of
and
report the best scores
and r

we set the optimal value of this parameter to
in the subsequent experiments
fig
shows the average number of meaningful concepts selected as classification features for the different values of the threshold in the given range
manuscript march rouge



















meaningfulness level fig

rouge scores for the bayesian summarizer using the fourth feature selection method and the different values of the meaningfulness threshold









r o c s e g u o r s e r u t a e r e b m u n e g a r e v a e h t



















meaningfulness level fig

the average number of features for the different values of the meaningfulness level in the fourth feature selection method
manuscript march in the fifth feature selection approach we use frequent itemset mining to extract correlated concepts i
e
frequent itemsets and to select them as classification features
the itemset mining method employs a parameter as the minimum support threshold to discover frequent itemsets
fig
shows the rouge scores obtained by the bayesian summarizer using the fifth feature selection method and different values of the threshold between


the threshold value of
reports the best scores
and r

we choose this value as the optimal parameter for the subsequent experiments
table presents the average number of frequent itemsets selected as features for different values of the threshold in the given range
rouge r o c s e g u o r






























minimum support threshold fig

rouge scores for the bayesian summarizer using the fifth feature selection method and the different values of the minimum support threshold



the impact of the coefficients we mention in section
that for the majority of documents even the high frequent concepts appear in less than of the sentences of a document
we examine the most frequent concept of every document in the development corpus
the results show that only in nine documents there are concepts that appear in more than of the sentences of the document
this means there are only nine documents containing at least one feature with a most probable value of true
for the other documents the most probable value of all the features is false
in average the most frequent concept of a document in the development corpus appears in of sentences in the corresponding document
we also investigate these statistics for the evaluation corpus
the results show that only in documents there are concepts that appear in more than of the sentences of the document
for the other documents the most frequent concept of each document appears in less manuscript march than of the sentences
in average the most frequent concept of a document in the evaluation corpus appears in of sentences in the corresponding document
regarding these observations as noted in section
the coefficients help the summarizer to discriminate between the true and false values of features leading to a more accurate sentence classification
table
the average number of features for different values of the minimum support threshold in the fifth feature selection method extraction of correlated features by itemset mining
the average number of features the average number of features










we perform a set of experiments to assess the impact of the coefficients and on the quality of the produced summaries
we evaluate the bayesian summarizer using the best settings of all the five feature selection methods with and without using the coefficients
table shows the rouge scores for these experiments
as can be seen the summarizer reports higher scores when it utilizes the coefficients
table
rouge scores obtained by the bayesian summarizer using the five different feature selection approaches with and without using the coefficients
first approach second approach third approach fourth approach
fifth approach
rouge with the coefficients
without the coefficients
with the coefficients
without the coefficients
















manuscript march













the impact of the redundancy reduction method performing another set of preliminary experiments we assess the impact of the redundancy reduction method on the quality of the produced summaries
table gives the rouge scores for the bayesian summarizer when the five different feature selection approaches are used with and without using the redundancy reduction method
we compare the scores assigned to the bayesian summarizer in two cases of using and not using the redundancy reduction method under different compression rates
table presents the percentage of improvement obtained using the redundancy reduction method for different compression rates
as can be seen the percentage of improvement for the smaller compression rates is higher than for the greater rates
table
rouge scores obtained by the bayesian summarizer using the five different feature selection approaches with and without using the redundancy reduction method
first approach second approach third approach fourth approach
fifth approach
rouge redundancy reduction yes
redundancy reduction no
redundancy reduction yes
redundancy reduction no
















table
the percentage of improvement achieved by the bayesian summarizer using the redundancy reduction method
the percentages have been computed for scores using the five different feature selection methods under different compression rates of and
compression rate first approach second approach third approach fourth approach
fifth approach

























manuscript march table
the average number of concepts covered in the summaries produced by the bayesian summarizer using the five different feature selection approaches with and without using the redundancy reduction method
with redundancy reduction without redundancy reduction first approach second approach third approach fourth approach
fifth approach
we also assess the average number of concepts covered in the produced summaries by the bayesian summarizer with and without using the redundancy reduction method
table shows these results when the summarizer uses the five different feature selection methods
as the results show the summaries cover more concepts in average when the summarizer benefits from the redundancy reduction method


evaluation results comparing the bayesian summarizer with the other methods we evaluate the performance of our method for biomedical text summarization
table shows the rouge scores obtained by the bayesian summarizer and the comparison methods
we evaluate the bayesian summarizer all the five feature selection approaches
in order to test the statistical significance of the results we use a wilcoxon signed rank test with a confidence interval
according to the results when the bayesian summarizer uses the fourth and fifth feature selection methods it significantly outperforms all the comparison methods in terms of all the reported rouge scores p

using the third feature selection method the bayesian summarizer significantly performs better than biochain the domain independent and the baseline methods in terms of all the rouge scores p

in comparison with chainfreq and freqdist the results report a significant improvement for all the scores except for the r
p

when we run the bayesian summarizer with the second feature selection method it significantly performs better than the domain independent and baseline summarizers p

among the biomedical summarization methods its improvement is significant for all the rouge scores compared to biochain and its improvement is significant only for score with respect to freqdist p

although it obtains better scores than chainfreq its improvement is not significant for all the scores p

eventually the bayesian summarizer significantly performs better than the domain independent competitors and baselines in terms of all the scores when it uses the first feature selection method p

compared to biochain it improves all the scores but the improvement is only significant for the
manuscript march table gives the average minimum and maximum number of features selected by the five feature selection methods for documents in the evaluation corpus
table
rouge scores obtained by the bayesian summarizer and the comparison methods
the best result for each rouge score is shown in bold type
the summarizers are sorted based on the decreasing order of their scores
bayesian fifth approach



bayesian fourth approach



rouge
rouge bayesian third approach uv bayesian second approach bayesian first approach
































table
the average minimum and maximum number of features for the documents of the evaluation corpus using the five different feature selection approaches
average minimum maximum bayesian first approach bayesian second approach bayesian third approach uv bayesian fourth approach
bayesian fifth approach













chainfreq freqdist biochain summa swesum lead baseline autosummarize random baseline manuscript march
discussion

feature selection and parameterization as reported in table when the summarizer uses the third feature selection method with the threshold is the scores are higher than for the other two threshold values
for a given document the value of the always less than for the other two thresholds and the value of the preliminary experiments evaluation corpus the average number of selected features for a document using is always greater than for the other ones
for the thresholds the threshold value the number of selected features is relatively high
in this case only some of the features is equal to and respectively
this shows that when we use the and as indicate to essential concepts and the summarizer can be misled by numerous unimportant features
on the other hand when we use the this reduction in the number of features helps the summarizer to decide more accurately considering the the number of selected features decreases to almost less than one third
and features which point to important concepts indeed
as fig
shows when the summarizer uses the fourth feature selection method the best scores are reported for the both meaningfulness level of
and

in this case the average number of features is
as showed in fig
for the threshold values greater than zero the average number of features falls to nearly
this rapid decrease happens because for the majority of concepts the meaningfulness value is zero
as the number of features decreases rapidly the quality of summarization also decreases because many features which could help the summarizer to perform more accurately are no longer available
the results show that the meaningfulness threshold of
discards many features which can be considered as redundant ones
when we assign threshold values smaller than
redundant features decrease the performance of the summarizer
there are other measures such as inverse document frequency idf inverse sentence frequency isf and inverse term frequency itf that are widely adopted in text mining research
such measures may seem to be functionally similar to the meaningfulness measure defined by the helmholtz principle
however by studying the theoretical and practical functions of the measures we found the meaningfulness more useful than others for selecting important concepts in the bayesian summarizer for several reasons
first the idf isf and itf measures are generally defined for a corpus of documents but the bayesian summarizer analyzes one document at a time and the definition of such measures may not make sense for single documents
second if we define the isf or itf weights in a document concepts appearing in fewer sentences are assigned more discriminative power
such weighting scheme could not be useful in our context
on the other hand the meaningfulness measure has been adopted successfully in single document text analysis
furthermore as the example in fig
shows the weights assigned by the helmholtz principle do not have any obvious relation to the frequency of concepts
according to the formulas in section

the meaningfulness value depends on manuscript march multiple factors such as the frequency of concepts within each paragraph and within the whole document the length of each paragraph and the length of the document
as fig
shows when the summarizer uses the fifth feature selection method the best scores are reported for the minimum support threshold of

for this value the average number of features itemsets is
for the smaller thresholds which produce more number of features the rouge scores decrease slightly
this shows that more number of features could not significantly reduce the accuracy of the summarizer in this feature selection method
although the numerous features mislead the summarizer to some extent it still benefits from the knowledge about the correlated concepts provided by the itemsets
when the threshold tends to be greater than
the average number of features decreases and the performance of the summarizer also decreases
particularly when the threshold is greater than
the average number of features drops to less than and the rouge scores are reduced considerably
this shows that when the number of features decreases in fifth feature selection method the knowledge of the summarizer about important and correlated concepts is inadequate
it decides according to a limited number of high supporting itemsets whereas there are a lot of useful itemsets discarded by an extreme threshold


the coefficients and as can be seen in table for all the five feature selection methods the bayesian summarizer obtains better rouge scores when it uses the coefficients
since for the majority of documents even the most frequent features do not appear in more than of the sentences of a document the most probable value for almost every feature is false
when the method does not use the coefficients it decides based on the most probable values of the features
in this case the summarizer needs additional knowledge about the features and their importance to decide more accurately
using the coefficients when the value of a feature is true in a sentence the probability of selecting the sentence for the summary increases in proportion to the occurrence of the feature
moreover the probability of not selecting the sentence decreases
on the other hand when the value of a feature is false in a sentence the probability of selecting the sentence for the summary decreases in proportion to the occurrence of the feature
moreover the probability of not selecting the sentence increases
adopting this strategy the summarizer can discriminate between the presence and absence of important concepts
as a result the summarizer performs better and reports higher scores


the redundancy reduction method as table shows when the redundancy reduction method takes part in the summarization method the summarizer reports better scores
with the help of the redundancy reduction method the summarizer gives sentences containing low frequency concepts a higher chance to be included in the summary
therefore the summary can cover more number of concepts while it still conveys important concepts and subtopics
when summaries cover more relevant information their informativeness increases
as a results the summarizer obtains higher scores
manuscript march table suggests that the percentage of improvement for the smaller compression rates is higher than for the greater rates
this happens because for smaller compression rates fewer sentences must be selected for the summary
hence when we use the redundancy reduction method and the summarizer selects new sentences containing new information the scores report a more impressive improvement
on the other hand for greater compression rates the summarizer selects more sentences
in this way the summary automatically includes new information
the summary presents the new information along with some potentially redundant sentences which were selected earlier or would be selected later
the redundancy reduction method may discard these redundant sentences and select new relevant information
therefore the performance of the summarizer may be improved
however this improvement for greater compression rates is less than for smaller rates because new relevant information is automatically included in the summary by the greater compression rates
as table shows using the redundancy method the summaries cover more concepts in average
in addition as can be seen in table the usage of the redundancy reduction method leads to an increase in the scores for all the feature selection methods
however when we compare each pair of feature selection methods the greater average number of concepts covered in the produced summaries does not necessarily lead to better summarization performance
for example in both cases of using and not using the redundancy reduction method the average number of concepts covered in the summaries for the fifth feature selection method is less than the average for the first second and fourth methods
nevertheless the summarizer obtains the best rouge scores using the fifth method
this suggests that with the use of an appropriate feature selection method the summaries convey more informative content even if they cover fewer concepts
these results demonstrate that both the redundancy reduction method and an appropriate feature selection method are essential to enhance the performance of the summarizer
the lack of each one has a negative impact on the quality of produced summaries


comparison with other summarizers as table shows when the bayesian summarizer utilizes the first feature selection method it performs better than the domain independent competitors
this suggests that using concepts as classification features in our method can be a better approach compared to the summarizers which employ word frequency methods positional features and term similarity features
our summarizer performs slightly worse than the two biomedical summarizers i
e
freqdist and chainfreq when it considers all extracted concepts as features
in this case it seem that potentially redundant and unrelated concepts negatively affect the quality of produced summaries
comparing the results of the first and second feature selection methods we observe that when generic and potentially redundant concepts are discarded the summarizer can decide more accurately and obtains higher scores
looking at the number of features selected by the first and second methods in table it seems that almost a half of concepts extracted from a document can be considered as unnecessary
removing redundant manuscript march concepts from classification features we observe a slight increase in the performance of the summarizer
the results of the second feature selection method show that with respect to biochain considering the distribution of non generic concepts along with the redundancy reduction method improves the performance of biomedical summarization
with respect to freqdist and chainfreq we still need to make more refinement in our feature selection strategy to increase the quality of produced summaries
the third feature selection method employs a ranking and filtering strategy
as the results show the use of all extracted concepts to constructing a frequency distribution model i
e
freqdist can be outperformed by the bayesian modeling in combination with an optimized feature selection based on the filtering method
although chainfreq does not use all extracted concepts and utilizes biochain as a filtering method for its hybrid method the bayesian summarizer obtains relatively better scores using an appropriate threshold for filtering the features
when we employ the third method for feature selection the maximum minimum and average number of features for a document in the evaluation is and respectively
as the numbers in table show the third method considerably reduces the average number of features compared to the second method from to
this reduction helps the summarizer to decide more accurately
regarding the results of the second method this filtering strategy leads to a slight increase in the performance of summarization
the results of the fourth feature selection method show that the meaningfulness is a better measure than the frequency for feature selection in the bayesian summarizer
as table shows the fourth method produces a large number of features compared to the third method for the average number of features
however the fourth method yields better summarization performance
this suggests that the meaningfulness can be considered as a more efficient measure to remove unimportant concepts in the bayesian summarizer
this measure provides the summarizer with a set of indeed important concepts to decide more optimally
however some of concepts selected as features may not be considered as a frequent concept
when the summarizer utilizes this measure it still makes use of the frequency of concepts in the form of the coefficients
in fact the summarizer combines information about the meaningfulness measure and the frequency of concepts
using this approach the bayesian summarizer obtains higher scores than all the biomedical competitors
as table shows the bayesian summarizer reports the highest scores when it utilizes the fifth feature selection method and uses frequent itemsets as features
using this method the summarizer implicitly takes into account correlations and appearing patterns existing among concepts
the results show that this feature selection strategy and the bayesian modeling yield better summarization quality than the biomedical summarizers relying on the frequency of single concepts
according to the results the fifth feature selection method performs slightly better than the fourth method
this suggests that the bayesian summarizer can utilize either information about correlated concepts or the meaningfulness measure as two useful feature selection approaches to improve the performance of summarization
comparing the results of different feature selection manuscript march methods we observe that frequent itemsets can be more useful than the frequency of single concepts in the bayesian summarizer
according to the results our bayesian summarizer significantly outperforms the domain independent comparison methods i
e
summa swesum and autosummarize in biomedical text summarization
these methods utilize statistical similarity based and word frequency features for sentence selection
the results show that these term based methods can not be considered as useful summarizers for biomedical text
on the other hand using domain knowledge and efficient feature selection methods the bayesian summarizer can perform more efficiently than the comparison methods
fig

the frequency distribution of concepts within the full text papers in the evaluation corpus
fig

the frequency distribution of concepts within the abstracts in the evaluation corpus
manuscript march

justification of the basic assumption as we explained in section
the bayesian summarizer estimates the posterior probability of selecting and not selecting sentences for the summary based on the prior probability of concepts within the input document
in other words the summarizer assumes that the distribution of important concepts within the summary should be similar to the original text
this assumption has been justified by reeve et al

they selected a corpus of biomedical full text papers and used the abstracts of the papers as the ideal summaries
they constructed two frequency distribution models from the concepts of the full text papers and the abstracts and showed that these two frequency distribution models follow zipfian distribution
regarding this observation they suggested that a full text paper and a version of its ideal summary abstract have the same frequency distribution form
we perform a similar experiment using a larger corpus
we extract concepts from the full texts and the abstracts of our evaluation corpus containing biomedical papers
fig
shows the frequency distribution of concepts within the full text papers and fig
shows the frequency distribution of concepts within the abstracts
as can be observed from fig
and fig
both the texts and the abstracts considered as the ideal summaries follow zipfian distribution
this observation justifies the basic assumption of our proposed biomedical text summarization method that uses the distribution of concepts within the document to estimate the posterior probability of the sentences for inclusion in the summary

conclusion in this paper we propose a biomedical text summarization method using a bayesian classification approach
the method classifies the sentences of the input document as summary and non summary according to the distribution of important concepts within the text
we introduce different feature selection approaches to identify the important concepts of the document and using them as classification features
the summarizer uses two coefficients in probability estimation to discriminate between the presence and absence of important concepts
it also employs a simple redundancy reduction method to reduce the potential redundancy in the summary
conducting a set of preliminary experiments on a development corpus containing biomedical papers we tune the parameters of the system and assess the efficiency of the coefficients and the redundancy reduction method
the results show that the coefficients and the redundancy reduction method have a positive impact on the quality of produced summaries leading to an improvement in the performance of the summarizer
we evaluate the performance of the bayesian summarization method in comparison with the other biomedical summarizers relying on the frequency of concepts domain independent summarizers and baseline methods using an evaluation of biomedical papers
the results show that when the bayesian summarizer utilizes the meaningfulness measure rather than the frequency of single concepts for selecting manuscript march features it outperforms the other summarizers
moreover when the summarizer employs itemset mining and uses correlated concepts as classification features it significantly performs better than the comparison methods
summing up the results we can draw the following conclusions that answer to the questions raised in section a bayesian classification method can be utilized for the probability distribution modeling of concept based biomedical text summarization
an efficient feature selection method is required to enhance the accuracy the summarizer should not consider all the extracted concepts from the input document
there many redundant concepts which may have a negative impact on the accuracy of the model and can be discarded of the classification method
by the summarizer
the meaningfulness measure defined by the helmholtz principle can be a useful criterion rather than the frequency to identify important concepts and use them as classification features
using itemset mining to discover correlated concepts and incorporating these correlations into the feature selection phase provide the summarizer with a more accurate model leading to an increase in the obtained scores
in our future research we intend to concentrate on extending our bayesian biomedical summarizer to deal with multi document and query focused summarization
to do so a more complicated redundancy reduction method should be studied
it seems that there is much more room for studying the helmholtz principle from the gestalt theory in the context of concept based summarization
balinsky et al
have modeled document sentences as a small world network using the helmholtz principle and investigated some applications such as text summarization
future work can involve exploring this type of modeling for concept based biomedical text summarization
the study of using other discriminative classifiers in this type of summarization can be considered as another potential topic for future research
conflict of interest the authors declare that they have no conflict of interest
mishra r bian j fiszman m weir cr jonnalagadda s mostafa j al
text summarization in the biomedical domain a systematic review of recent research
journal of biomedical informatics

afantenos s karkaletsis v stamatopoulos p
summarization from medical documents a survey
artificial intelligence in medicine

fleuren ww alkema w
application of text mining in the biomedical domain
methods

reeve lh han h brooks ad
the use of domain specific concepts in biomedical text summarization
information processing management

manuscript march references plaza l daz a gervs p
a semantic graph based approach to biomedical summarisation
artificial intelligence in medicine

chen p verma r
a query based medical information summarization system using ontology knowledge
ieee symposium on computer based medical systems ieee
p

plaza l carrillo de albornoz j
evaluating the use of different positional strategies for sentence selection in biomedical literature summarization
bmc bioinformatics

menndez hd plaza l camacho d
combining graph connectivity and genetic clustering to improve biomedical summarization
ieee congress on evolutionary computation cec ieee
p

reeve l han h brooks ad
biochain lexical chaining methods for biomedical text summarization
proceedings of the acm symposium on applied computing acm
p

reeve lh han h nagori sv yang jc schwimmer ta brooks ad
concept frequency distribution in biomedical text summarization
proceedings of the acm international conference on information and knowledge management acm
p

nelson sj powell t humphreys b
the unified medical language system umls project
encyclopedia of library and information science

balinsky aa balinsky hy simske sj
on helmholtz principle for documents processing
proceedings of the acm symposium on document engineering acm
p

agrawal r imieliski t swami a
mining association rules between sets of items in large databases
acm sigmod record

mitchell t
generative and discriminative classifiers naive bayes and logistic regression
manuscript available at cs cm tom newchapters html

lin c y
rouge a package for automatic evaluation of summaries
text summarization branches out proceedings of the
gupta v lehal gs
a survey of text summarization extractive techniques
journal of emerging technologies in web intelligence

alguliev rm aliguliyev rm hajirahimova ms mehdiyev ca
mcmr maximum coverage and minimum redundant text summarization model
expert systems with applications

gambhir m gupta v
recent automatic text summarization techniques a survey
artificial intelligence review

workman te fiszman m hurdle jf
text summarization as a decision support aid
bmc medical informatics and decision making

zhang h fiszman m shin d miller cm rosemblat g rindflesch tc
degree centrality for semantic abstraction summarization of therapeutic studies
journal of biomedical informatics

fiszman m demner fushman d kilicoglu h rindflesch tc
automatic summarization of medline citations for evidence based medical treatment a topic oriented evaluation
journal of biomedical informatics

kilicoglu h
summarizing drug information in medline citations

fiszman m rindflesch tc kilicoglu h
abstraction summarization for managing the biomedical research literature
proceedings of the hlt naacl workshop on computational lexical semantics association for computational linguistics
p

zhang h fiszman m shin d wilkowski b rindflesch tc
clustering cliques for graph based summarization of the biomedical research literature
bmc bioinformatics

moen h peltonen l m heimonen j airola a pahikkala t salakoski t al
comparison of automatic summarisation methods for clinical free text notes
artificial intelligence in medicine

del fiol g mostafa j pu d medlin r slager s jonnalagadda sr al
formative evaluation of a patient specific clinical knowledge summarization tool
international journal of medical informatics

morid ma fiszman m raja k jonnalagadda sr del fiol g
classification of clinically useful sentences in clinical evidence resources
journal of biomedical informatics

pivovarov r elhadad n
automated methods for the summarization of electronic health records
journal of the american medical informatics association

plaza l
comparing different knowledge sources for the automatic summarization of biomedical literature
journal of biomedical informatics

menndez hd plaza l camacho d
a genetic graph based clustering approach to biomedical summarization
proceedings of the international conference on web intelligence mining and semantics acm
p

sarkar k
using domain knowledge for text summarization in medical domain
international journal of recent trends in engineering

sarkar k nasipuri m ghose s
using machine learning for medical document summarization
international journal of database theory and application

sarker a moll d paris c
extractive summarisation of medical documents using domain knowledge and corpus statistics
the australasian medical journal

manuscript march barzilay r elhadad m
using lexical chains for text summarization
advances in automatic text summarization

kupiec j pedersen j chen f
a trainable document summarizer
proceedings of the annual international acm sigir conference on research and development in information retrieval acm
p

daum iii h marcu d
bayesian query focused summarization
proceedings of the international conference on computational linguistics and the annual meeting of the association for computational linguistics association for computational linguistics
p

wang d zhu s li t gong y
multi document summarization using sentence based topic models
proceedings of the acl ijcnlp conference short papers association for computational linguistics
p

larose dt larose cd
data mining and predictive analytics john wiley sons
national library of medicine
umls metathesaurus fact sheet
national library of medicine
umls specialist lexicon fact sheet
national library of medicine
umls semantic network fact sheet
aronson ar
effective mapping of biomedical text to the umls metathesaurus the metamap program
proceedings of the amia symposium american medical informatics association
p

plaza l stevenson m daz a
resolving ambiguity in biomedical text to improve summarization
information processing management

chandrashekar g sahin f
a survey on feature selection methods
computers electrical engineering

forman g
an extensive empirical study of feature selection metrics for text classification
journal of machine learning research

balinsky a balinsky h simske s
on the helmholtz principle for data mining
hewlett packard development company lp

balinsky a balinsky h simske s
rapid change detection and text mining
proceedings of the conference on mathematics in defence ima defence academy
balinsky h balinsky a simske sj
automatic text summarization and small world networks
proceedings of the acm symposium on document engineering acm
p

tutkan m ganiz mc akyoku s
helmholtz principle based supervised and unsupervised feature selection methods for text mining
information processing management

agrawal r mannila h srikant r toivonen h verkamo ai
fast discovery of association rules
advances in knowledge discovery and data mining

ferreira r souza cabral l freitas f lins rd frana silva g simske sj al
a multi document summarization system based on statistics and linguistic treatment
expert systems with applications

carbonell j goldstein j
the use of mmr diversity based reranking for reordering documents and producing summaries
proceedings of the annual international acm sigir conference on research and development in information retrieval acm
p

lin c y
looking for a few good metrics automatic summarization evaluation how many samples are enough
mani i
summarization evaluation an overview

saggion h
summa a robust and adaptable summarization tool
traitement automatique des langues

swesum automatic text summarizer
mitkov r
the oxford handbook of computational linguistics oxford university press
blake c
a comparison of document sentence and term event spaces
proceedings of the international conference on computational linguistics and the annual meeting of the association for computational linguistics association for computational linguistics
p

balinsky h balinsky a simske s
document sentences as a small world
systems man and cybernetics smc ieee international conference on ieee
p

manuscript march
