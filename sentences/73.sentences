n u j l c
s c v
v i x r a query focused opinion summarization for user generated content lu hema claire vittorio of computer science cornell university ithaca ny usa luwang
cornell
edu ca usa
com t
j
watson research center yorktown heights ny usa
ibm
com abstract we present a submodular function based framework for query focused opinion summarization
within our framework relevance ordering produced by a statistical ranker and information coverage with respect to topic distribution and diverse viewpoints are both encoded as submodular functions
dispersion functions are utilized to minimize the redundancy
we are the rst to evaluate different metrics of text similarity for submodularity based summarization methods
by experimenting on community qa and blog tion we show that our system outperforms state of the art approaches in both automatic evaluation and human evaluation
a human evaluation task is conducted on amazon mechanical turk with scale and shows that our systems are able to generate summaries of high overall quality and information diversity
introduction social media forums such as social networks blogs newsgroups and community question answering qa offer avenues for people to express their opinions as well collect other people s thoughts on topics as diverse as health politics and software liu et al

however digesting the large amount of information in long threads on newsgroups or even knowing which threads to pay attention to can be overwhelming
a text based summary that highlights the diversity of opinions on a given topic can lighten this information overload
in this work we design a submodular function based framework for opinion summarization on community question answering and blog data
question what is the long term effect of piracy on the music and lm industry best answer rising costs for movies and music



if they sell less they need to raise the price to make up for what they lost
the other thing will be music and movies with less quality



other answers its bad


really bad
just watch this movie and you will nd out


piracy causes rappers to appear on your computer
by removing the protability of music lm companies piracy takes away their motivation to produce new music movies
if they ca nt protect their copyrights they ca nt continue to do business



it is forcing them to rework their business model which is a good thing
in short i do nt think the music industry in particular will ever enjoy the huge prots of the s



please people in those businesses make millions of dollars as it is i do nt think piracy hurts them at all figure example discussion on yahoo answers
besides the best answer other answers also contain relevant information in italics
for example the sentence in blue has a contrasting viewpoint compared to the other answers
opinion summarization has previously been applied to restricted domains such as product reviews hu and liu lerman et al
and news stoyanov and cardie where the output summary is either presented in a structured way with respect to each aspect of the product or organized along contrastive viewpoints
unlike those works we address user generated online data community qa and blogs
these forums use a substantially less formal language than news articles and at the same time address a much broader spectrum of topics than product reviews
as a result they present new challenges for automatic summarization
for example figure illustrates a sample question from yahoo along with the answers from different users
the question receives more than one answer and one of them is selected as the best answer by the asker or other participants
in general answers from other users also provide relevant information
while community qa successfully pools rich knowledge from the wisdom of the crowd users might need to seine through numerous posts to extract the information this work is licensed under a creative commons attribution
international licence
page numbers and proceedings footer are added by the organisers
licence details
org licenses

yahoo
they need
hence it would be benecial to summarize answers automatically and present the summaries to users who ask similar questions in the future
in this work we aim to return a summary that lates different perspectives for a given opinion question and a set of relevant answers or documents
in our work we assume that there is a central topic or query on which a user is seeking diverse ions
we predict query relevance through automatically learned statistical rankers
our ranking function not only aims to nd sentences that are on the topic of the query but also ones that are opinionated through the use of several features that indicate subjectivity and sentiment
the relevance score is coded in a submodular function
diversity is accounted for by a dispersion function that maximizes the pairwise distance between the pairs of sentences selected
our chief contributions are we develop a submodular function based framework for query focused opinion summarization
to the best of our knowledge this is the rst time that submodular functions have been used to support opinion summarization
we test our framework on two tasks summarizing opinionated sentences in community qa yahoo answers and blogs corpus
human evaluation using amazon chanical turk shows that our system generates the best summary
of the time
on the other hand the best answer picked by yahoo users is chosen only
of the time
we also obtain signicant higher pyramid score on the blog task as compared to the system of lin and bilmes
within our summarization framework the statistically learned sentence relevance is included as part of our objective function whereas previous work on submodular summarization lin and bilmes only uses ngram overlap for query relevance
additionally we use latent dirichlet allocation blei et al
to model the topic structure of the sentences and induce clusterings according to the learned topics
therefore our system is capable of generating summaries with broader topic coverage
furthermore we are the rst to study how different metrics for computing text similarity or similarity affect the quality of submodularity based summarization methods
we show empirically that lexical representation based similarity such as tfidf scores uniformly outperforms semantic ity computed with wordnet
moreover when measuring the summary diversity topical representation is marginally better than lexical representation and both of them beats semantic representation
related work our work falls in the realm of query focused summarization where a user asks a question and the tem generates a summary of the answers containing pertinent and diverse information
a wide range of methods have been investigated where relevance is often estimated through tf idf similarity bonell and goldstein topic signature words lin and hovy or by learning a bayesian model over queries and documents daume and marcu
most work only implicitly penalizes summary redundancy e

by downweighting the importance of words that are already selected
encouraging diversity of a summary has recently been addressed through submodular functions which have been applied for multi document summarization in newswire lin and bilmes sipos et al
and comments summarization dasgupta et al

however these works either ignore the query information when available or else use simple ngram matching between the query and sentences
in contrast we propose to optimize an objective function that addresses both relevance and diversity
previous work on generating opinion summaries mainly considers product reviews hu and liu lerman et al
and formal texts such as news articles stoyanov and cardie or als paul et al

mostly there is no query information and summaries are formulated in a tured way based on product features or contrastive standpoints
our work is more related to opinion summarization on user generated content such as community qa
liu et al
manually construct taxonomies for questions in community qa
summaries are generated by clustering sentences according to their polarity based on a small dictionary
tomasoni and huang introduce coverage and quality constraints on the sentences and utilize an integer linear programming framework to select sentences
submodular opinion summarization in this section we describe how query focused opinion summarization can be addressed by submodular functions combined with dispersion functions
we rst dene our problem
then we introduce the basic features answer position in all answers sentence position in blog length of the answer sentence length is less than words query sentence overlap features unigram bigram tf tfidf similarity with query number of key phrases in the query that appear in the sentence
a model similar to that described in luo et al
was applied to detect key phrases
sentiment features number portion of sentiment words from a lexicon section
if contains sentiment words with the same polarity as sentiment words in query query independent features unigram bigram tfidf similarity with cluster centroid sumbasic score nenkova and vanderwende number of topic signature words lin and hovy js divergence with cluster table features used for candidate ranking
we use them for ranking answers in both community qa and blogs
components of our objective function sections


the full objective function is presented in section

lastly we describe a greedy algorithm with constant factor approximation to the optimal solution for generating summaries section

a set of documents or answers to be summarized are rst split into a set of individual sentences v sn
our problem is to select a subset s v that maximizes a given objective function r within a length constraint s arg max s subject to s c
s is the length of sv the summary s and c is the length limit
denition a function r is submodular iff for all s v and every s s v it satises s s s s s s
previous submodularity based summarization work assumes this diminishing return property makes submodular functions a natural t for summarization and achieves state of the art results on various datasets
in this paper we follow the same assumption and work with non decreasing submodular tions
nevertheless they have limitations one of which is that functions well suited to modeling diversity are not submodular
recently dasgupta et al
proved that diversity can nonetheless be encoded in well designed dispersion functions which still maintain a constant factor approximation when solved by a greedy algorithm
based on these considerations we propose an objective function s mainly considering three pects relevance section
coverage section
and non redundancy section

relevance and coverage are encoded in a non decreasing submodular function and non redundancy is enforced by maximizing the dispersion function

relevance function we rst utilize statistical rankers to produce a preference ordering of the candidate answers or sentences
we choose listnet cao et al
which has been shown to be effective in many information retrieval tasks as our ranker
we use the implementation from ranklib dang
features used in the ranking algorithm are summarized in table
all features are normalized by standardization
due to the length limit we can not provide the full results on feature evaluation
theless we nd that ranking candidates by tfidf similarity or key phrases overlapping with the query can produce comparable results with using the full feature set see section
we take the ranks output by the ranker and dene the relevance of the current summary s as where ranki is the rank of sentence in v
for qa answer ranking sentences from the i p same answer have the same ranking
the function is our rst submodular function
i
coverage functions topic coverage
this function is designed to capture the idea that a comprehensive opinion mary should provide thoughts on distinct aspects
topic models such as latent dirichlet allocation lda blei et al
and its variants are able to discover hidden topics or aspects of document lections and thus afford a natural way to cluster texts according to their topics
recent work xie and xing shows the effectiveness of utilizing topic models for newsgroup document clustering
we rst learn an lda model from the data and treat each topic as a cluster
we estimate a sentence topic distribution for each sentence and assign the sentence to the cluster k corresponding to the mode of the distribution i
e
k arg maxi i
this naive approach produces comparable clustering performance to the state of the art according to xie and xing
t is dened as the clustering induced by our rithm on the set v
the topic coverage of the current summary s is dened as pt t t
from the concavity of the square root it follows that sets s with uniform coverages of topics are preferred to sets with skewed coverage
authorship coverage
this term encourages the summarization algorithm to select sentences from different authors
let a be the clustering induced by the sentence to author relation
in community qa sentences from the answers given by the same user belong to the same cluster
similarly sentences from blogs with the same author are in the same cluster
the authorship score is dened as paa
polarity coverage
the polarity score encourages the selection of summaries that cover both positive and negative opinions
we categorize each sentence simply by counting the number of polarized words given by our lexicon
a sentence belongs to a positive cluster if it has more positive words than negative ones and vice versa
if any negator co occurs with a sentiment word e

within a window of size the sentiment is reversed
the polarity clustering p thus have two clusters corresponding to positive and negative opinions
the score is dened as pp p s p
our lexicon consists of mpqa lexicon wilson et al
general inquirer stone et al
and sentiwordnet esuli and sebastiani
words with conicting sentiments from different lexicons are removed
content coverage
similarly to lin and bilmes and dasgupta et al
we use the following function to measure content coverage of the current summary s pvv s v where s pus u
we experiment with two types of similarity functions
one is a cosine tfidf similarity score
the other is a wordnet based semantic similarity score between pairwise dependency relations from two sentences dasgupta et al

specically u preliv relju w n ai aj w n bi bj where reli ai bi relj aj bj w n wi wj is the shortest path length
all scores are scaled onto

dispersion function summaries should contain as little redundant information as possible
we achieve this by adding an additional term to the objective function encoded by a dispersion function
given a set of sentences s a complete graph is constructed with each sentence in s as a node
the weight of each edge u v is their dissimilarity
then the distance between any pair of u and v v is dened as the total weight of the shortest path connecting u and v
we experiment with two forms of dispersion function dasgupta et al
hsum pu vv v v and hmin minu vv v
then we need to dene the dissimilarity function d
there are different ways to measure the dissimilarity between sentences mihalcea et al
agirre et al

in this work we experiment with three types of dissimilarity functions
lexical dissimilarity
this function is based on the well known cosine similarity score using tfidf weights
let simtf idf u v be the cosine similarity between u and v then we have d v simtf idf u v
semantic dissimilarity
this function is based on the semantic meaning embedded in the dependency relations
d v u where u is the semantic similarity used in content coverage measurement in section

topical dissimilarity
we propose a novel dissimilarity measure based on topic models
celikyilmaz et al
show that estimating the similarity between query and passages by using topic structures can help improve the retrieval performance
as discussed in the topic coverage in section
each sentence is represented by its sentence topic distributions estimated by lda
for candidate sentence u and v let their topic distributions be pu and pv
then the dissimilarity between u and v can be dened
as d t v where pv i pi pi
full objective function the objective function takes the interpolation of the submodular functions and dispersion function
exists a large amount of work on determining the polarity of a sentence pang and lee which can be employed for polarity clustering in this work
we decide to focus on summarization and estimate sentence polarity through sentiment word summation yu and hatzivassiloglou though we do not distinguish different sentiment words
denition of distance is used to produce theoretical guarantees for the greedy algorithm described in section

the coefcients are non negative real numbers and can be tuned on a development set
notice that each summand except is a non decreasing non negative and submodular function and summation preserves monotonicity non negativity and submodularity
dispersion function is either hsum or hmin as introduced previously

summary generation via greedy algorithm generating the summary that maximizes our objective function in equation is np hard chandra and halldorsson
we choose to use a greedy algorithm that guarantees to obtain a constant factor proximation to the optimal solution nemhauser et al
dasgupta et al

concretely starting with an empty set for each iteration we add a new sentence so that the current summary achieves the maximum value of the objective function
in addition to the theoretical guarantee existing work donald has empirically shown that classical greedy algorithms usually works near optimally
experimental setup
opinion question identication we rst build a classier to automatically detect opinion oriented questions in community qa questions in the blog dataset are all opinionated
our opinion question classier is trained on two opinion question datasets the rst from li et al
contains opinionated and objective questions the second dataset from amiri et al
consists of implicit opinion questions such as what can you do to help environment and objective questions
we train a rbf kernel based svm classier to identify opinion questions which achieves scores of
and
on the two datasets when evaluated using fold cross validation the best scores reported are
and


datasets community qa summarization yahoo answers
we use the yahoo answers dataset from yahoo webscopet m which contains questions
we rst run the opinion question classier to identify the opinion questions
for summarization purpose we require each question having at least answers with the average length of answers larger than words
this results in questions
to make a compelling task we reserve questions with an average length of answers larger than words as our test set for both ranking and summarization all the other questions are used for training
as a result we have questions in the training set for learning the statistical ranker and in the test set
the category distribution of training and test questions yahoo answers organizes the questions into predened categories are similar
questions from the training set are further reserved as the development set
each question in the yahoo answers dataset has a user voted best answer
these best answers are used to train the statistical ranker that predicts relevance
separate topic models are learned for each category where the category tag is provided by yahoo answer
blog summarization tac
we use the tac corpus dang which consists of topics
of them are provided with human labeled nuggets which tac used in human evaluation
tac also provides snippets i
e
sentences that are frequently retrieved by participant systems or identied as relevant by human annotators
we do not assume those snippets are known to any of our systems

comparisons for both opinion summarization tasks we compare with the approach by dasgupta et al
and the systems from lin and bilmes with and without query information
the sentence clustering process in lin and bilmes is done by using cluto karypis
for the implementation of systems in lin and bilmes and dasgupta et al
we always use the parameters reported to have the best performance in their work
for cqa summarization we use the best answer voted by the user as a baseline
note that this is a strong baseline since all the other systems are unaware of which answer is the best
for blog rization we have three additional baselines the best systems in tac kim et al
li et al
top sentences returned by our ranker a baseline produced by tfidf similarity and a lexicon values for the coefcients are




for respectively as tuned on the development set

yahoo
henceforth called
in sentences are ranked by the tfidf ity with the query and then sentences with sentiment words are selected in sequence
this baseline aims to show the performance when we only have access to lexicons without using a learning algorithm
results
evaluating the ranker we evaluate our ranker described in section
on the task of best answer prediction
table compares the average precision and mean reciprocal rank mrr of our method to those of three baselines where answers are ranked randomly baseline random by length baseline length and by jensen shannon divergence jsd with all answers
we expect that the best answer is the one that covers the most information which is likely to have a smaller jsd
therefore we use jsd to rank answers in the ascending order
table manifests that our ranker outperforms all the other methods
avg precision mrr baseline random

baseline length

jsd

ranker listnet

table performance for best answer prediction
our ranker outperforms the three baselines

community qa summarization automatic evaluation
since human written abstracts are not available for the yahoo answers dataset we adopt the jensen shannon divergence jsd to measure the summary quality
intuitively a smaller jsd implies that the summary covers more of the content in the answer set
louis and nenkova report that jsd has a strong negative correlation spearman correlation
with the overall summary quality for multi document summarization mds on news articles and blogs
our task is similar to mds
meanwhile the average jsd of the best answers in our test set is smaller than that of the other answers
vs

with an average length of words compared with words for the other answers
also on the blog task section
the top two systems by jsd also have the top two rouge scores a common metric for summarization evaluation when human constructed summaries are available
thus we conjecture that jsd is a good metric for community qa summaries
table left shows that our system using a content coverage function based on cosine using tfidf weights and a dispersion function hsum based on lexicon dissimilarity and topics outperforms all of the compared approaches paired t test p

the topic number is tuned on the development set and we nd that varying the number of topics does not impact performance too much
meanwhile both our system and dasgupta et al
produce better jsd scores than the two variants of the lin and bilmes system which implies the effectiveness of the dispersion function
we further examine the effectiveness of each component that contributes to the objective function section
and the results are shown in table right
best answer lin and bilmes lin and bilmes q dasgupta et al
our system length








rel rel aut tm topic models rel aut tm rel aut tm pol coverage rel aut tm pol cont











table left summaries evaluated by jensen shannon divergence jsd on yahoo answer for maries of words and words
the average length of the best answer is

right value addition of each component in the objective function
the jsd on each line is statistically signicantly lower than the jsd on the previous

human evaluation
human evaluation for yahoo answers is carried out on amazon mechanical with carefully designed tasks or hits
turkers are presented summaries from different systems in a random order and asked to provide two rankings one for overall quality and the other for information diversity
we indicate that informativeness and non redundancy are desirable for quality however ers are allowed to consider other desiderata such as coherence or responsiveness and write down those when they submit the answers
here we believe that ranking the summaries is easier than evaluating each summary in isolation lerman et al


mturk
com we randomly select questions from our test set each of which is evaluated by distinct turkers located in united states
hits are thus created each containing different questions
four system summaries best answer dasgupta et al
and our system with and words respectively are displayed along with one noisy summary i
e
irrelevant to the question per question in random order
we reject turkers hits if they rank the noisy summary higher than any other
two duplicate questions are added to test intra annotator agreement
we reject hits if turkers produced inconsistent rankings for both duplicate questions
a total of submissions of which hits pass the above quality lters
turkers of all accepted submissions report themselves as native english speakers
an inter rater ment of fleiss of
fair agreement landis and koch is computed for quality ranking and is
moderate agreement for diversity ranking
table shows the percentage of times a particular method is picked as the best summary and the average rank of a method for both overall quality and information diversity
macro average is computed by rst averaging the ranks per question and then averaging across all questions
for overall quality our system with a word limit is selected as the best in
of the evaluations
it outperforms the best answer
signicantly which suggests that our system summary covers evant information that is not contained in the best answer
our system with a length constraint of words is chosen as the best for quality
times while that of dasgupta et al
is chosen
of the time
our system is also voted as the best summary for diversity in
of the evaluations
more interestingly both of our systems with words and words outperform the best answer and gupta et al
for average ranking both overall quality and information diversity signicantly by using wilcoxon signed rank test p

when we check the reasons given by turkers we found that people usually prefer our summaries due to helpful suggestions that covered many options or being balanced with different opinions
when turks prefer the best answers they mostly stress on coherence and responsiveness
sample summaries from all the systems are displayed in figure
best answer dasgupta et al
our system our system
length of summary overall quality average rank macro micro best











information diversity average rank macro micro best











table human evaluation on yahoo answer data
boldface implies statistically signicance pared to other results in the same columns using paired t test
both of our systems are ranked higher i
e
numbers in bold with than the best answers voted by yahoo users and system summaries from dasgupta et al

question what is the long term effect of piracy on the music and lm industry dasgupta et al
qty
div

short i do nt think the music industry in particular will ever enjoy the huge prots of the s
please people in those businesses make millions of dollars as it is i do nt think piracy hurts them at all the other thing will be music and movies with less quality
a big gray area i do nt see anything wrong with burning a mix cd or a cd for a friend so long as you re not selling them for prot
by removing the protability of music lm companies piracy takes away their motivation to produce new music movies
our system words qty
div

rising costs for movies and music
the other thing will be music and movies with less quality
with piracy there is nt the willingness to take chances
it s also like the person put the effort into it and they are nt getting paid
it s a big gray area i do nt see anything wrong with burning a mix cd or a cd for a friend so long as you re not selling them for prot
it is forcing them to rework their business model which is a good thing
our system words qty

div
rising costs for movies and music
the other thing will be music and movies with less quality
with piracy there is nt the willingness to take chances
american idol is the result of this




the real problem here is that the mainstream music will become even tighter
record labels will not wo nt to go far from what is currently like by the majority
i hate when people who have billions of dollars whine about not having more money
but it s also like the person put the effort into it and they are nt getting paid


i do nt see anything wrong with burning a mix cd or a cd for a friend



it is forcing them to rework their business model which is a good thing
by removing the protability of music lm companies piracy takes away their motivation to produce new music movies
figure sample summaries from dasgupta et al
and our systems words and words
sentences from separate bullets are partial answers from different users
that we aim to compare results with the gold standard best answers of about words
the evaluation of the word summaries is provided only as an additional data point

blog summarization automatic evaluation
we use the rouge lin and hovy software with standard options to automatically evaluate summaries with reference to the human labeled nuggets as those are available for this task
measures bigram overlap and rouge measures the overlap of unigram and skip bigram separated by up to four words
we use the ranker trained on yahoo data to produce relevance ordering and adopt the system parameters from section

table left shows that our system outperforms the best system in with highest score kim et al
the two baselines and our ranker lin and bilmes and dasgupta et al

rouge best system in tfidf lexicon ranker listnet lin and bilmes lin and bilmes q dasgupta et al
our system jsd






table results on dataset
left our system has signicant better rouge scores than all the other systems except our ranker paired t test p

we also achieve the best js divergence
right human evaluation with pyramid f score
our system signicantly outperforms the others
pyramid f score


best system in lin and bilmes our system













human evaluation
for human evaluation we use the standard pyramid f score used in the opinion summarization track with dang
in the tac task systems are allowed to return up to non white characters for each question
since the tac metric favors recall we do not produce summaries shorter than characters
we ask two human judges to evaluate our system along with the one that got the highest pyramid f score in the and lin and bilmes
cohen s for inter annotator agreement is
substantial
while we did not explicitly evaluate non redundancy both of our judges report that our system summaries contain less redundant information

further discussion yahoo answer tac dispersionsum dispersionmin dissimi semantic topical lexical conttf idf


contsem conttf idf





contsem


dispersionsum dispersionmin dissimi semantic topical lexical conttf idf


contsem conttf idf





contsem


table effect of different dispersion functions content coverage and dissimilarity metrics on our system
left jsd values for different combinations on yahoo data using lda with topics
all systems are signicantly different from each other at signicance level

systems using summation of distances for dispersion function hsum uniformly outperform the ones using minimum distance hmin
rouge scores of different choices for tac data
all systems use lda with topics
the parameters of our systems are adopted from the ones tuned on yahoo answers
given that the text similarity metrics and dispersion functions play important roles in the framework we further study the effectiveness of different content coverage functions cosine using tfidf vs
mantic dispersion functions hsum vs
hmin and dissimilarity metrics used in dispersion functions semantic vs
topical vs
lexical
results on yahoo answer table left show that systems using summation of distances for dispersion functions hsum uniformly outperform the ones using minimum distance hmin
meanwhile cosine using tfidf is better at measuring content coverage than based semantic measurement and this may due to the limited coverage of wordnet on verbs
this is also true for dissimilarity metrics
results on blog data table right however show that using minimum distance for dispersion produces better results
this indicates that optimal dispersion function varies by genre
topical based dissimilarity also marginally outperforms the other two metrics in blog data
conclusion we propose a submodular function based opinion summarization framework
tested on community qa and blog summarization our approach outperforms state of the art methods that are also based on modularity in both automatic evaluation and human evaluation
our framework is capable of including statistically learned sentence relevance and encouraging the summary to cover diverse topics
we also study different metrics on text similarity estimation and their effect on summarization
references eneko agirre daniel cer mona diab and aitor gonzalez agirre

task a pilot on tic textual similarity
in volume proceedings of the sixth international workshop on semantic evaluation semeval pages montreal canada june
association for computational linguistics
hadi amiri zheng jun zha and tat seng chua

a pattern matching based model for implicit opinion question identication
in aaai
aaai press
david m
blei andrew y
ng and michael i
jordan

latent dirichlet allocation
j
mach
learn
res
march
zhe cao tao qin tie yan liu ming feng tsai and hang li

learning to rank from pairwise approach to listwise approach
in proceedings of the international conference on machine learning icml pages new york ny usa
acm
jaime carbonell and jade goldstein

the use of mmr diversity based reranking for reordering documents and producing summaries
in proceedings of the annual international acm sigir conference on research and development in information retrieval sigir pages new york ny usa
acm
asli celikyilmaz dilek hakkani tur and gokhan tur

lda based similarity modeling for question ing
in proceedings of the naacl hlt workshop on semantic search ss pages stroudsburg pa usa
association for computational linguistics
barun chandra and magnus m
halldorsson

facility dispersion and remote subgraphs
in proceedings of the scandinavian workshop on algorithm theory swat pages london uk uk
verlag
hoa tran dang

overview of the tac opinion question answering and summarization tasks
in proc
tac
van dang

ranklib

cs
umass
ranklib
html
anirban dasgupta ravi kumar and sujith ravi

summarization through submodularity and dispersion
in proceedings of the annual meeting of the association for computational linguistics volume long papers pages soa bulgaria august
association for computational linguistics
hal daume iii and daniel marcu

bayesian query focused summarization
in proceedings of the international conference on computational linguistics and the annual meeting of the association for computational linguistics pages stroudsburg pa usa
association for computational linguistics
andrea esuli and fabrizio sebastiani

sentiwordnet a publicly available lexical resource for opinion in in proceedings of the conference on language resources and evaluation pages mining

minqing hu and bing liu

mining and summarizing customer reviews
in proceedings of the tenth acm sigkdd international conference on knowledge discovery and data mining kdd pages new york ny usa
acm
george karypis

cluto a clustering toolkit
technical report november
hyun duk kim dae hoon park v
g
vinod vydiswaran and chengxiang zhai

opinion summarization using entity features and probabilistic sentence coherence optimization uiuc at tac opinion summarization pilot
in proc
tac
j r landis and g g koch

the measurement of observer agreement for categorical data
biometrics
kevin lerman sasha blair goldensohn and ryan mcdonald

sentiment summarization evaluating and learning user preferences
in proceedings of the conference of the european chapter of the association for computational linguistics eacl pages stroudsburg pa usa
association for computational linguistics
baoli li yandong liu and eugene agichtein

cocqa co training over questions and answers with an application to predicting question subjectivity orientation
in emnlp pages
wenjie li you ouyang yi hu and furu wei

polyu at tac
in proc
tac
hui lin and jeff bilmes

a class of submodular functions for document summarization
in proceedings of the annual meeting of the association for computational linguistics human language technologies volume hlt pages stroudsburg pa usa
association for computational linguistics
chin yew lin and eduard hovy

the automated acquisition of topic signatures for text summarization
coling pages stroudsburg pa usa
association for computational linguistics
chin yew lin and eduard hovy

automatic evaluation of summaries using n gram co occurrence statistics
in proceedings of the conference of the north american chapter of the association for computational linguistics on human language technology volume pages
yuanjie liu shasha li yunbo cao chin yew lin dingyi han and yong yu

understanding and marizing answers in community based question answering services
in proceedings of the international conference on computational linguistics volume coling pages stroudsburg pa usa
association for computational linguistics
annie louis and ani nenkova

automatically assessing machine summary content without a gold standard
comput
linguist
june
xiaoqiang luo hema raghavan vittorio castelli sameer maskey and radu florian

finding what matters in questions
in hlt naacl pages
ryan mcdonald

a study of global inference algorithms in multi document summarization
pages berlin heidelberg
springer verlag
rada mihalcea courtney corley and carlo strapparava

corpus based and knowledge based measures of text semantic similarity
in proceedings of the national conference on articial intelligence volume pages
aaai press
g
l
nemhauser l
a
wolsey and m
l
fisher

an analysis of approximations for maximizing ular set functionsi
mathematical programming december
ani nenkova and lucy vanderwende

the impact of frequency on summarization
microsoft research redmond washington tech
rep
msr
bo pang and lillian lee

opinion mining and sentiment analysis
found
trends inf
retr
january
michael j
paul chengxiang zhai and roxana girju

summarizing contrastive viewpoints in opinionated text
in proceedings of the conference on empirical methods in natural language processing emnlp pages stroudsburg pa usa
association for computational linguistics
ruben sipos pannaga shivaswamy and thorsten joachims

large margin learning of submodular rization models
eacl pages stroudsburg pa usa
association for computational linguistics
philip j
stone dexter c
dunphy marshall s
smith and daniel m
ogilvie

the general inquirer a computer approach to content analysis
mit press cambridge ma
veselin stoyanov and claire cardie

partially supervised coreference resolution for opinion summarization through structured rule learning
in proceedings of the conference on empirical methods in natural language processing emnlp pages stroudsburg pa usa
association for computational linguistics
mattia tomasoni and minlie huang

metadata aware measures for answer summarization in community question answering
in proceedings of the annual meeting of the association for computational tics acl pages stroudsburg pa usa
association for computational linguistics
theresa wilson janyce wiebe and paul hoffmann

recognizing contextual polarity in phrase level ment analysis
in proceedings of the conference on human language technology and empirical methods in natural language processing hlt pages stroudsburg pa usa
association for computational linguistics
pengtao xie and eric xing

integrating document clustering and topic modeling
in proceedings of the twenty ninth conference annual conference on uncertainty in articial intelligence pages corvallis oregon
auai press
hong yu and vasileios hatzivassiloglou

towards answering opinion questions separating facts from in proceedings of the conference on empirical opinions and identifying the polarity of opinion sentences
methods in natural language processing emnlp

