selective attention encoders by syntactic graph convolutional networks for document summarization haiyang yun kun baochang junwen xiangang chuxing beijing china research america mountain view ca usa university beijing china
com
edu
cn kunhan maobaochang chenjunwen
com r a m l c
s c v
v i x r a abstract abstractive text summarization is a challenging task and one need to design a mechanism to effectively extract salient formation from the source text and then generate a summary
a parsing process of the source text contains critical tic or semantic structures which is useful to generate more accurate summary
however modeling a parsing tree for text summarization is not trivial due to its non linear structure and it is harder to deal with a document that includes multiple sentences and their parsing trees
in this paper we propose to use a graph to connect the parsing trees from the sentences in a document and utilize the stacked graph convolutional works gcns to learn the syntactic representation for a ument
the selective attention mechanism is used to extract salient information in semantic and structural aspect and erate an abstractive summary
we evaluate our approach on the cnn daily mail text summarization dataset
the imental results show that the proposed gcns based selective attention approach outperforms the baselines and achieves the state of the art performance on the dataset
index terms document summarization sequence sequence dependency parsing tree graph convolutional networks
introduction document summarization aims at generating a short and uent summary consisting of the salient information of the source text
existing approaches for text summarization are divided into two major types extractive and abstractive
tractive methods produce summaries by extracting important sentences from the original document
abstractive methods produce the summaries by summarizing the salient tion of the source text and representing by arbitrary words
the end to end neural framework based on sequence sequence models have achieved tremendous success in many text generation tasks such as machine lation dialogue systems
the essence of method is an encoder decoder framework which rst encodes the input sentence to a low dimensional representation and then decodes the abstract representation based on attention
some researchers also apply neural model to stractive text summarization
although it is ward to adopt the approach to text summarization there is a signicant difference between it and machine lation the important component of text summarization is turing the salient information of the original document for generating summary instead of aligning between the input sentence and the summary
explicit information selection in text summary have proven to be more effective than plicit learning only via the approach
recent searchers has explored selective gate based on global ument and combining extractive method to capture salient information explicitly for decoding and achieved the current state of the art results
however these models ignore the syntactic structure of the source text which can help choose important words in structure to erate more accurate summary or feature based models represent dependency information by hand crafted features which face the problem of sparse feature spaces and not phasizing the explicit importance of structure
in this paper we explore syntactic graph convolutional networks to model non euclidean document structure and adopt attention information gate to select salient information for generating text summarization
specically we build a document level graph with heterogeneous types of nodes and edges which are formed by connecting the dependency parsing trees from the sentences in a document
we adopt stacked convolutional neural networks gcns to learn the local and non local syntactic representation for a document which have proved the effectiveness in other nlp
then we employ attention mechanism to acquire global document representation combining syntactic and semantic information and use explicit information tion gate based on global document representation to choose important words for generating better summary
we uate our model to the cnn daily mail text summarization datasets
the experimental results show that the proposed gcns encoders model outperforms the state of the art line models

related work existing approaches for document summarization are divided into two major categories extractive and abstractive
extractive use hierarchical recurrent neural works rnns to get the representations of the sentences and classify the importance of sentences
rank extracted sentences for summary generation through a reinforcement learning and extract salient sentences and propose a new policy gradient method to rewrite these sentences i
e
presses and paraphrases to generate a concise overall mary
propose a framework composed of a hierarchical document encoder based on cnns and an attention based tractor with attention over external information
present a new extractive framework by joint learning to score and selecting sentences
rstly apply neural networks for text summarization by using a local attention based model to erate word conditioned on the input sentence
applies framework with hierarchical attention for text marization
proposes graph based attention mechanism to summarize the salient information of document
however the above neural models all faces out of vocabulary oov lems since the vocabulary is xed at training stage
in order to solve this problem point network and copynet have been proposed to allow both copying words from the original text and generating arbitrary words from a xed cabulary
propose a unied model via inconsistency loss to combine the extractive and abstractive methods
adopt bottom up attention to alleviate the issue of point network tending to copy long sequences
recently more and more focus on explicit information selection in encoding step which lters unnecessary words and uses portant words for generating summary
the focus of our model is selecting salient syntactic and semantic information

algorithm details fig

overall architecture of the proposed model
the structural document colors denote the types of nodes and edges represented by gcns and the semantic document representation represented by bilstm are bined via attention mechanism to get document information
then selective gate lters unnecessary words for generating the summary


semantic and syntactic document encoder with gcn


semantic document encoder given a document


concatenating all sentences into a long sequence where wi is ith words in the document and n is the sequence length of document we ploy a bidirectional long short term memory bilstm as the encoder
the bilstm consists of forward lstm which reads the document d from to and backward lstm reads the document d from wn to xi wewi i

n he i he i lst m xi i

n lst m xi i

n where xi is the distributed representation of token ei by bedding matrix we
we concatenate every forward hidden he state j to get the original he word semantic representation he i
he j with the backward hidden state he i the architecture of our model is shown in figure
in this section we describe the proposed model specically which consists of semantic document encoder syntactic ment encoder with gcn and attention information gate



syntactic document encoder in order to build a document level graph we apply a parser to generate the dependency tree lk of every sentence sk by treating each syntactic dependency direction and label as a different edge type
directions and labels of edges can criminate the important words in structure and our comparing experimental results also demonstrate the essence of ing directions and labels of edges in text summarization task
then we link the syntactic root node serially with adjacent sentence type edges to build a complete document adjacency matrix
we apply gcns compute the structural sentation of every word on the constructed document graph which keeps separate parameters for each edge type
more we stack many gcns layers to make every node scious of more distant neighbors
after l layers every node representation can capture local and global syntactic tion
specically in the llayer gcn we denote hs hs l i as the ith input and output vector of node i at the lth layer the input vector is the word semantic representation he i
a graph convolution operation can be denoted as follows hs l i w l i i j jm i i j where w l i j are the trainable parameters mi is the set of neighbouring nodes of ith node
as we set the word semantic representation he i as the original inputs of gcns hs to alleviate the parsing error
furthermore we also prevent over parametrization by setting the same weighs of direction and label edges but separate weights for adjacent sentence edges and sole bias for all edge types
we use the output vector of node i at the lth layer as word structural representation hs l i
we concatenate the semantic and structural representation to get the informative i hs word representation hi he i


attention information gate we propose a novel global information gate based on tion mechanism to select the salient information from the put
concretely we adopt the attention gregate the representation of those informative words to form a document vector
then the gate network takes the document vector dv and the word representation as the input to compute the selective gate vector gi ai ui bw n aihi i uw i uw dv n gi ugdv bg where ww wg ug are the trainable weights and bw bg are the trainable bias
then each word can be ltered by the gate vector gi to get important words for decoding h i hi gi where h i is the representation of word wi after information ltration and used as the input word representation for the decoder to generate the summary
is element wise plication
in the decoding stage we apply pointer generator network to alleviate the oov problems and coverage network to vent as prior works
furthermore we also ploy bottom up to relieve the problem of ing very long sequences using pointer generator network
the nal loss consists of negative log likelihood and the coverage loss

evaluations in this section we introduce the expermental setup and present the experimental results


experiment setup we use cnn daily mail dataset to evaluate our model which consist of long text and has been widely used in text summarization task
we used scripts supplied by to produce the non anonymized version of the cnn daily mail summarization dataset which contains training pairs validation pairs and test pairs
for all ments we use words of the source vocabulary
to obtain the syntactic information of the sentences in the corpora we use stanford parser to get dependency trees with edge labels
our model takes dimensional hidden states and use dimensional word embeddings
we choose adagrad this was found to work best of stochastic gradient descent adadelta momentum adam and rmsprop with learning rate
and initialize the accumulator value with

for hyper parameter congurations we adjust them according to the performance on the validation set and examples randomly sampled from validation set in inference stage for bottom up
after tuning attention mask threshold is set to
the weight of length penalty is
and coverage loss weighted is
we use layers gcns in encoder and set beam size to



experimental results we adopt the widely used rouge by pyrouge it measures the similarity of the output evaluation metric

github
com abisee pointer generator
com abisee cnn dailymailr
stanford
edu software lex parser
shtml
com sebastiangehrmann bottom up summary
python
org pypi

methods extractive runner refresh rnn rl neusum abstractive intra bottom info our model r l
































table
results of abstractive summarizers on the dm dataset
the rst section shows extractive baselines
the second section describes abstractive approaches
the third section presents our model
all our rouge scores have a condence interval with at most

methods our model gate





r l


table
results of removing different components of our model on the cnn dataset
all our rouge scores have a condence interval with at most

summary and the standard reference by computing ping n gram such as unigram bigram and longest common subsequence lcs
in the following experiments we adopt unigram bigram and rouge l longest common subsequence for evaluation
it can be observed from table that the proposed proach achieves the best performance on the cnn daily datasets over state of the art extractive as well as abstractive baselines
comparing with the same architecture of erage our model has signicant improvement on
on
on
on rouge l which demonstrates the effectiveness of our model combining the syntactic and semantic information by information gate and modeling the heterogeneous document level graph via stacked gcns
furthermore to further study the effectiveness of each component of our model we conduct several ablation iments on the cnn dataset
gate denotes that we remove the attention gate and only use all encoded word to decode denotes that we remove the gcns and the attention gate which degrades into
table shows that each component of our model all improve the formance on the dataset evidently
gcns model the ture of document effectively and represent it in a dense dimension feature
attention gate adopt the attention anism to acquire informative document representation bining syntactic and semantic information then use selective gate to detect critical words for generating the summary

conclusions in this work we propose syntactic graph convolutional coders based on dependency trees for abstractive text rization which uses graph convolutional networks gcns to learn the representation of syntactic structure of the source text then adapt the attention mechanism combining semantic and structural information to select important words for coding
we evaluate our model to the cnn daily mail text summarization datasets
the experimental results show that the proposed gcns encoders model outperforms the state the art baseline models

references ilya sutskever oriol vinyals and quoc v le quence to sequence learning with neural networks in advances in neural information processing systems pp

minh thang luong hieu pham and christopher d to arxiv preprint manning based neural machine translation

approaches effective antoine bordes y lan boureau and jason weston arxiv learning end to end goal oriented dialog preprint

jiatao gu zhengdong lu hang li and victor ok li incorporating copying mechanism in sequence sequence learning arxiv preprint

abigail see peter j liu and christopher d manning get to the point summarization with pointer generator networks arxiv preprint

qingyu zhou nan yang furu wei and ming zhou selective encoding for abstractive sentence rization in meeting of the association for tional linguistics pp

cnn dataset is the subset of cnn daily dataset and is also widely used for text summarization
we also have achieved state of the art mance in this dataset wei li xinyan xiao yajuan lyu and yuanzhuo wang improving neural abstractive document summarization with explicit information selection modeling in ceedings of the conference on empirical methods in natural language processing pp

jiwei tan xiaojun wan and jianguo xiao tive document summarization with a graph based tional neural model in proceedings of the annual meeting of the association for computational tics volume long papers vol
pp

wan ting hsu chieh kai lin ming ying lee kerui min jing tang and min sun a unied model for tractive and abstractive summarization using tency loss arxiv preprint

kaiqiang song lin zhao and fei liu infused copy mechanisms for abstractive in proceedings of the international tion ference on computational linguistics pp

diego marcheggiani and ivan titov encoding tences with graph convolutional networks for semantic role labeling in proceedings of the conference on empirical methods in natural language processing pp

yuhao zhang peng qi and christopher d manning graph convolution over pruned dependency trees proves relation extraction in proceedings of the conference on empirical methods in natural language processing pp

joost bastings ivan titov wilker aziz diego graph marcheggiani and khalil simaan tional encoders for syntax aware neural machine lation in proceedings of the conference on pirical methods in natural language processing pp

ramesh nallapati feifei zhai and bowen zhou marunner a recurrent neural network based sequence model for extractive summarization of documents
in aaai pp

shashi narayan shay b cohen and mirella ranking sentences for extractive arxiv preprint learning ata tion with reinforcement

yen chun chen and mohit bansal fast abstractive summarization with reinforce selected sentence ing in proceedings of the annual meeting of the association for computational linguistics volume long papers pp

shashi narayan ronald cardenas nikos topoulos shay b cohen mirella lapata jiangsheng yu and yi chang document modeling with nal attention for sentence extraction in proceedings of the annual meeting of the association for putational linguistics volume long papers vol
pp

qingyu zhou nan yang furu wei shaohan huang ming zhou and tiejun zhao neural document marization by jointly learning to score and select tences in proceedings of the annual meeting of the association for computational linguistics volume long papers pp

alexander m rush sumit chopra and jason weston a neural attention model for abstractive sentence marization arxiv preprint

ramesh nallapati bowen zhou caglar gulcehre bing xiang al
abstractive text summarization using sequence to sequence rnns and beyond arxiv preprint

oriol vinyals meire fortunato and navdeep jaitly pointer networks in advances in neural information processing systems pp

sebastian gehrmann yuntian deng and alexander rush bottom up abstractive summarization in ceedings of the conference on empirical methods in natural language processing pp

alex graves and jurgen schmidhuber framewise phoneme classication with bidirectional lstm and other neural network architectures neural networks vol
no
pp

zichao yang diyi yang chris dyer xiaodong he alex smola and eduard hovy hierarchical attention networks for document classication in proceedings of the conference of the north american chapter of the association for computational linguistics man language technologies pp

danqi chen and christopher manning a fast and curate dependency parser using neural networks in proceedings of the conference on empirical ods in natural language processing emnlp pp

romain paulus caiming xiong and richard socher a deep reinforced model for abstractive tion arxiv preprint

chin yew lin rouge a package for automatic ation of summaries text summarization branches out

