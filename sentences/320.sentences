g u a l c
s c v
v i x r a fat albert finding answers in large texts using semantic similarity attention layer based on bert omar amgad anandharaju hari and zayed simon fraser university burnaby canada
ca abstract machine based text comprehension has always been a signicant research eld in natural language processing
once a full understanding of the text context and semantics is achieved a deep learning model can be trained to solve a large subset of tasks

text summarization classication and question answering
in this paper we focus on the question answering problem specically the multiple choice type of questions
we develop a model based on bert a state of the art transformer network
moreover we alleviate the ability of bert to support large text corpus by extracting the highest inuence sentences through a semantic larity model
evaluations of our proposed demonstrate that it outperforms the leading models in the movieqa challenge and we are currently ranked in the leader board with test accuracy of

finally we discuss the model shortcomings and suggest possible improvements to overcome these limitations
introduction one of the main challenges in natural language processing nlp is the ability of a machine to read and understand an unstructured text and then reason about answering some related questions
such question answering qa models can be applied to a wide range of applications such as nancial reports customer service and health care
a signicant number of datasets namely race and swag were created to provide a ground truth for training and evaluating a specic type of qa models that involve multiple choice questions mcqs
the movieqa dataset challenge has attracted a large number of promising solutions such as blohm et al
where they implement two attention models based on short term memory lstm and convolutional neural networks cnns
moreover they combine both model accuracies using an ensemble aggregation
however these models are prone to various systematic adversarial attacks like linguistic level attacks word vs
sentence level and the knowledge of the adversaries black box vs
white box
these models only learn to match patterns to select the right answer rather than performing plausible inferences as humans do
recently implemented bidirectional encoder representations from transformers bert which has since been used as a pre trained model to tackle a large subset of nlp tasks
bert s key technical source code is publicly available
com omossad fat albert leader board
cs
toronto
edu plot preprint
under review
table movieqa dataset description of movies of questions avg
q
of words avg
ca
of words avg
wa
of words train val






test


total





innovation is applying the bidirectional training of transformer a popular attention model to language modelling
the paper s results show that a language model which is bidirectionally trained can have a deeper sense of language context and ow than single direction language models
a plethora of deep learning models have ever since incorporated bert into several tasks ever improving the state of the art performances
a notable limitation of bert is that it is not able to support large texts that include more than the pre trained model s maximum number of words tokens
therefore when dealing with large texts the performance of bert is severely affected
in our work we aim to overcome the limitations of bert by analyzing and improving how accurate we predict the answer extracted from a large text in the movieqa mcq dataset
our approach relies on the concept of sentence attention to extract the most signicant sentences from a large corpus
we were able to accomplish this task using a pre trained semantic similarity model
the remainder of this paper is organized as follows section describes the datasets focusing on the semantics and the nature of the questions
next we highlight the proposed model and the intuition behind all the components we used in section
we evaluate the performance of our model in section
finally we conclude our report and suggest future modications for the model in section
movieqa dataset in this section we give an overview of the dataset used to evaluate our model
the movieqa dataset was created by generating a number of mcqs that can be solved from specic context extracted from the plots of real movies
an existing is still on going to nd the highest accuracy in solving these mcqs on movie topics
the models can be trained to use either video scenes subtitles scripts or movie plots extracted from wikipedia
the leader board for this challenge is divided according to the source of input selected for the model
the dataset consists of almost mcqs obtained from over movies and features high semantic diversity
each question comes with a set of highly plausible answers only one of which is correct
the dataset structure and semantics for the movie plots are described in table
on average each movie plot has
sentences and there are
words per sentence on average
all training and validation sets are labelled with the correct answer
however the test dataset is not labelled and can only be evaluated using the challenge s submission server
due to the large nature of the plot texts we have selected this dataset to demonstrate how we can incorporate bert in relatively large texts
fat albert model description existing bert mcq codes currently lack the support for large text documents since they are restricted to sequence length of at most tokens
furthermore due to our limited compute capabilities we are restricted to tokens
according to only tpus with gb memory are able to train models with number of tokens compared to gb gpus
therefore we have used another model on top of bert mcq to select the top sentences from the text that are highly similar to the question context
subsequently instead of processing the entire corpus bert mcq uses the top sentences only
the maximum span of a certain question i
e
the part from the text needed to answer this question is sentences and all plot alignments are consecutive i
e
all questions span a specic passage from the text not the entire text
challenge
cs
toronto
figure fat albert model overview figure semantic similarity network model our model consists of the following semantic similarity classier pre trained on sts and clinical datasets bert for mcq trained on movieqa dataset a description of the entire model is depicted in fig

the large movie plot text along with the concatenated question and answer sequences are fed to the similarity model to produce the top similar sentences for each question
we feed the questions answers and the attention made plot text top similar sentences to the bert for mcq model
the output of this model is an array of probabilities having the size of the number of possible choices
finally we select the choice having the highest probability

semantic similarity network we use two pre trained models based on sts and clinical datasets to nd the similarity measure between sentences
a number of variation for semantic similarity exists between our dataset and the aforementioned ones but the selected model has proven to be effective in our application
a detailed comparison between the performance of these models on the movieqa dataset is provided in the evaluation section
the model uses bert to extract the embeddings from both sentences
these embeddings undergo a number of similarity functions namely cosine similarity qgram distance levenshtein similarity
and the outputs are sent to a fully connected network followed by a softmax layer to provide the similarity index
we have selected to use a combination of sts and clinical bert models after normalizing each model probabilities
eq
describes how the cosine similarity index between two sentences and is calculated
fig
highlights the contextual structure of the bert similarity model

bert for mcq the bert mcq model uses a pre trained bert transformer network modied and ne tuned on the movieqa dataset
the embedding outputs of bert are passed to a fully connected layer to produce the predicted probabilities of each possible choice
we used the pre trained model which uses layer hidden heads and m parameters
the ne tuning is performed on the movieqa dataset after modifying bert outputs to support the variable number of choices
when running bert for mcq on the perfectly aligned plot sentences the model was able to achieve a validation accuracy of

although the model was initially developed to support sentence completion type of questions we modied the model to handle mcqs by changing the network structure to output probabilities for each choice instead of complete text tokens
evaluation results we evaluate the performance of our model on the movieqa dataset
in this section we indicate whether the results were obtained from our own implementation or as mentioned in the reference paper
some differences appear between our evaluation and the published results probably due to changes in parameters by the authors which were not mirrored in their source codes
we also provide a brief case study to highlight two cases from the same movie plot where the model succeeds and fails respectively

evaluation on movieqa dataset a properly trained bert for mcq can reach an accuracy in the range of on the movieqa dataset once it is aligned with exact sentences having the answer
however when performed without any sentence selection the accuracy drops to a random choice due to the fact that bert truncates the sentences with word count higher than the maximum sequence length words in our case
therefore using the semantic similarity model we captured the top similar sentences to the question at hand
we selected questions since the average word count of sentences over the entire dataset is in the range of which seems to be acceptable to truncate a few words from the least similar sentences
the similarity model performance is depicted in table
the combined model aggregates the output probabilities of web and clinical models after normalization
a possible improvement for the model accuracy can be done by further training it on the movieqa dataset
next we compare the accuracy of our model against the current movieqa challenge leader board models
we have included the validation accuracy and test evaluations we received from the movieqa authors after submission
another contribution is that we have created an ensemble model that aggregates the results from the top approaches in the challenge and performs a majority ruling to select the label with the highest probability
this model uses the cnn and lstm models previously described along with the bert model
the main incentive behind this ensemble is to allow different models to correct one another and collaboratively avoid making mistakes
table demonstrates the accuracy of our models compared to the leader board models
to highlight the effect of the number of tokens in bert we showcase in fig
the training loss for models with different number of tokens
the main observation is that in order to support larger number of tokens we had to reduce the batch size used during training
although the results indicate that larger tokens have lower losses in general it is clear that reducing the batch size has notably affected the model accuracy
for instance in fig
when using a number of tokens equivalent to table semantic similarity evaluations train
acc
val acc
web bert clinical bert combined





table movieqa dataset evaluations val
acc
test acc
fat albert ensemble fat albert lstm cnn word level cnn










as included in the paper leader board not obtained from our evaluations the model accuracy increases signicantly compared to a maximum number of tokens where the model truncates any input having tokens
due to the compute capability we have nt been able to run the model with tokens as the gpus were not able to allocate enough memory
therefore we evaluated the loss for larger number tokens using a smaller batch size in fig

the loss is gradually decreasing as the number of tokens becomes higher until it stabilizes when the inputs generally become smaller than the maximum number of tokens
hence the input sentences are padded with zeros to reach the required size of tokens

case study we demonstrate two cases extracted from the movie forrest gump
the movie plot is sentences and a brief extract is shown in fig

two sample questions are displayed in fig
the model selects the top similar sentences to the question and in that case the answer can be fully interpreted from one of these s sentences highlighted in bold
hence after passing these sentences instead of the entire plot to bert mcq it successfully selects the correct choice
on the other hand the similarity model was nt able to select the best sentences in the second example as shown in fig

despite nding one of the accurate sentences for this specic question the model missed the most informative one
therefore the qa model subsequently failed to select the correct answer
conclusions and future work in this paper we have created an attention model based on semantic similarity to overcome bert limitations
in order to solve an mcq we begin by extracting the most relevant sentences from a large text thereby reducing the complexity of the problem of answering mcq question
at the time of writing this report our latest submission is ranked rst in the movieqa challenge
as a future work we plan to extend our model to process other input signals provided by the movieqa dataset like subtitles
we could build more powerful models by incorporating other human like processing mechanisms such as referential relations entailment and answer by elimination
finally migrating the code to work on tpus with higher computational power instead of gpus may allow us to handle larger texts avoiding sentence truncation
batch size batch size figure bert mcq training loss on movieqa dataset figure forrest gump plot figure correct model prediction example figure wrong model prediction example references blohm m
jagfeld g
sood e
yu x
and vu n
t
comparing attention based convolutional and recurrent neural networks success and limitations in machine reading hension
in proceedings of the conference on computational natural language learning association for computational linguistics
devlin j
chang m
w
lee k
and toutanova k
bert pre training of deep bidirectional transformers for language understanding
corr

lai g
xie q
liu h
yang y
and hovy e
race large scale reading comprehension dataset from examinations
arxiv preprint

tapaswi m
zhu y
stiefelhagen r
torralba a
urtasun r
and fidler s
movieqa understanding stories in movies through question answering
in ieee conference on computer vision and pattern recognition cvpr jun ieee
zellers r
bisk y
schwartz r
and choi y
swag a large scale adversarial in proceedings of the conference on dataset for grounded commonsense inference
empirical methods in natural language processing association for computational linguistics

