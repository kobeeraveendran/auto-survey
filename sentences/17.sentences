automated text summarization base on lexicales chain and graph using of wordnet and wikipedia knowledge base mohsen pourvali and mohammad saniee abadeh department of electrical computer qazvin branch islamic azad university qazvin iran department of electrical and computer engineering at tarbiat modares university tehran iran abstract the is technology of automatic document summarization maturing and may provide a solution to the information overload problem
nowadays document summarization plays an important role in information retrieval
with a large volume of documents presenting the user with a summary of each document greatly facilitates the task of finding the desired documents
document is a process of automatically creating a summarization compressed version of a given document that provides useful information to users and multi document summarization is to produce a summary delivering the majority of information content from a set of documents about an explicit or implicit main topic
the lexical cohesion structure of the text can be exploited to determine the importance of a sentence phrase
lexical chains are useful tools to analyze the lexical cohesion structure in a text
in this paper we consider the effect of the use of lexical cohesion features in summarization and presenting a algorithm base on the knowledge base
ours algorithm at first find the correct sense of any word then constructs the lexical chains remove lexical chains that less score than other detects topics roughly from lexical chains segments the text with respect to the topics and selects the most important sentences
the experimental results on an open benchmark datasets from and show that our proposed approach can improve the performance compared to sate of the art summarization approaches
keywords text summarization data mining text mining word sense disambiguation
introduction the technology of automatic document summarization is maturing and may provide a solution to the information overload problem
nowadays document summarization plays an important role in information retrieval ir
with a large volume of documents presenting the user with a summary of each document greatly facilitates the task of finding the desired documents
text summarization is the process of automatically creating a compressed version of a given text that provides useful information to users and multi document summarization is to produce a summary delivering the majority of information content from a set of documents about an explicit or implicit main topic
authors of the paper provide the following definition for a summary a summary can be loosely defined as a text that is produced from one or more texts that conveys important information in the original and that is no longer than half of the original and usually significantly less than that
text here is used rather loosely and can refer to speech multimedia documents hypertext
the main goal of a summary is to present the main ideas in a document in less space
if all sentences in a text document were of equal importance producing a summary would not be very effective as any reduction in the size of a document would carry a proportional decrease in its in formativeness
luckily information content in a document appears in bursts and one can therefore distinguish between more and less informative segments
identifying the informative segments at the expense of the rest is the main challenge in summarization
assumes a tripartite processing model distinguishing three stages source text interpretation to obtain a source representation source representation transformation to summary representation and summary representation
a variety of document summarization methods have been developed recently
the paper reviews research on automatic summarizing over the last decade
this paper and reviews developments and seeks to assess the state of the art for this challenging natural language processing nlp task
the review shows that some useful summarizing for various purposes can already be done but also not surprisingly that there is a huge amount more to do
sentence based extractive summarization techniques are commonly used in automatic summarization to produce extractive extractive summarization are technique for sentence extraction and attempt to identify the set of sentences the overall understanding of a given document
in paper proposed typically based on text generation salient notions that are most summaries
important summary systems from the for for ijcsi international journal of computer science issues vol
issue no january issn online www
ijcsi
c international journal of computer science issues
all rights reserved
paragraph extraction from a document based on document links between paragraphs
it yields a text relationship map trm from intra links which indicate that the linked texts are semantically related
it proposes four strategies from the trm bushy path depth first path segmented bushy path augmented segmented bushy path
in our study we focus on sentence based extractive summarization
in this way we to express that the lexical cohesion structure of the text can be exploited to determine the importance of a sentence
eliminate the ambiguity of the word has a significant impact on the inference sentence
in this article we will show that the separation text into the inside issues by using the correct concept noticeable effect on the summary text is created
the experimental results on an open benchmark datasets from and show that our proposed approach can improve the performance compared to state of the art summarization approaches
the rest of this paper is organized as follows section introduces related works word sense disambiguation is presented in section clustering of the lexical chains is presented in section text segmentation base on the inner topics is presented in section the experiments and results are given in section
finally conclusion presents in section

related work generally speaking the methods can be either extractive summarization or abstractive summarization
extractive summarization involves assigning salience scores to some units e

sentences paragraphs of the document and the sentences with highest scores while extracting abstraction summarization e


cs
columbia
edu nlp usually needs information fusion sentence compression and reformulation
sentence extraction summarization systems take as input a collection of sentences one or more documents and select some subset for output into a summary
this is best treated as a sentence ranking problem which allows for varying length requirements
most commonly such ranking approaches use some kind of similarity or centrality metric to rank sentences for inclusion in the summary see for example
the centroid based method is one of the most popular extractive summarization methods
mead an
summarization
com implementation of the centroid based method for either single or multi document summarizing
it is based on sentence extraction
for each sentence in a cluster of related documents mead computes three features and uses a linear combination of the three to determine what sentences are most salient
the three features used are to meet varying summary thresholds is centroid score position and overlap with first sentence which may happen to be the title of a document
for single documents or given clusters it computes centroid topic characterizations using tf idf type data
it ranks candidate summary sentences by combining sentence scores against centroid text position value and tf idf title lead overlap
sentence selection is constrained by a summary length threshold and redundant new sentences avoided by checking cosine similarity against prior ones
in the past extractive summarizers have been mostly based on scoring sentences in the source document
in paper each document is considered as a sequence of sentences and the objective of extractive summarization is to label the sentences in the sequence with and where a label of indicates that a sentence is a summary sentence while denotes a non summary sentence
to accomplish this task is applied conditional random field which is a state of art sequence labeling method
in paper proposed a novel extractive approach based on manifold ranking of sentences to query based multi document summarization
the proposed approach first employs the manifold ranking process to compute the manifold ranking score for each sentence that denotes the biased information richness of the sentence and then uses greedy algorithm to penalize the sentences with highest overall scores which are deemed both informative and novel and highly biased to the given query
the summarization techniques can be classified into two groups supervised techniques that rely on pre existing document summary pairs and unsupervised techniques based on properties and heuristics derived from the text
supervised extractive summarization techniques treat the summarization task as a two class classification problem at the sentence level where the summary sentences are positive samples while the non summary sentences are negative samples
after representing each sentence by a vector of features the classification function can be trained in two different manners
one is in a discriminative way with well known algorithms such as support vector machine svm
many unsupervised methods have been developed for document summarization by exploiting different features and relationships of the sentences see for example and the references therein
on the other hand summarization task can also be categorized as either generic or query based
a query based summary presents the information that is most relevant to the given queries and while a generic summary gives an overall sense of the document s content
the qcs system query cluster and summarize performs the following tasks in response to a query retrieves relevant documents separates the retrieved documents into clusters by topic and creates a summary for each cluster
qcs is a tool for document retrieval that presents results in a format so that a user can quickly identify a set of documents of interest
in paper are developed a generic a query based and a hybrid summarizer each with ijcsi international journal of computer science issues vol
issue no january issn online www
ijcsi
c international journal of computer science issues
all rights reserved
differing amounts of document context
the generic summarizer used a blend of discourse information and information obtained traditional surface level through analysis
the query based summarizer used only term information and the hybrid summarizer used some discourse information along with query term information
the article presents a multi document multi lingual theme based summarization system based on modeling text cohesion story flow

word sense disambiguation for extracting lexical chains in a document all words and correct senses of these words should be known
humans disambiguate words by the current context
lexical chaining algorithms depend on an assumption and this assumption is that correct sense of words has stronger relations with other word senses
using this assumption lexical chaining algorithms first try to disambiguate all word occurrences
for sense disambiguation wsd is an immediate application of lexical chains and an extrinsic evaluation methodology
reason word this
generating and traversing the wordnet graph the algorithm presented in this paper is based on lexical chains therefore the system needs to deeply analyze the text
per word has a sense based on it s position in the sentence
for instance the word bank in the follow sentences has different bank of river and bank failures were a major disaster
in first sentence bank means river s coast but in the second sentence it means economic bank
the most appropriate sense must be chosen for the connectedness in a lexical chain
in the algorithm presented in this paper word sense are calculated locally
in this way the best word sense is extracted
we also use wordnet as an external source for disambiguation this word and increasing it cause fig
diagram of algorithm s steps let wi be a word in the document and wi have n senses
in this procedure for finding the meaning of two words related locally together and placed in the same sentence we assume all of the possible meanings and senses of per word as the first level of the traversing word tree then we process every sense in a returning algorithm
next we connect all the relations for that sense as it s descendants and these descendants are generated through relations that are hypernym



we do this process in a returning manner for n levels
next every first level sense of the one word compare with all the first level senses of the other word
afterwards the numbers of equalities are considered
the same in comparison is done for another word
if there is nt any equality for each word we choose first sense that is most common
integer digit fig
sample graph built on the words in the above figure we illustrate the relations of the tree
the root of the tree is considered as the target word and the first level nodes as the senses of the target words
the nodes of the second third


levels are senses related with the first level nodes with hypernym relations
this tree is generated using returning functions and traversing of the tree is in the returning manner
function node t int level string sp for i to do sp if
index tnew create new call tnew end if end for fig
algorithm for creation wordnet graph ijcsi international journal of computer science issues vol
issue no january issn online www
ijcsi
c international journal of computer science issues
all rights reserved
file is downloaded the above algorithm is one of the functions used for producing wordnet graph
this function is the part of the graph related with hypernym relation
we use the great encyclopedia of wikipedia because of the lack of special names in knowledge base of wordnet
this is done using from that the
g xml dumps
wikipedia
org site
we have created a for this file and then goal word abstract is extracted
extracted abstract is used same of the glosses of another sentence s word we use creating the graph and traversing of it just for the first middle and last sentences and it is useful because these sentences usually encompass concise expression of the concept of the paragraph in most of the documents
in this manner we decrease the space of interpretation and therefore the time of calculation and the space of memory because we just need to keep some highlight sentences related with each other
after clarifying the actual senses of the all words in the prominent sentences and with the similarities and relations between every pair of the words we put them in incessant lexical chains
for example in the tree of two words and through the traversing of the first word we put these two words in the same lexical chain as soon as we reach the first common sense between the subordinate graph of the first word and the first level nodes of the second word
for each symbolizes that this word occur in lexical chain lci the first sentence and the third sense of this word is chosen as the best sense
lexical chains created at first are generated from highlight sentences and we use different algorithm for putting other words of sentences in the relevant lexical chains
in this algorithm with some changes in lesk algorithm we use gloss concepts to represent similarities and differences of two words
let are two words in text
firstly we extract senses of per word in normal lesk algorithm from knowledge base then we find overlaps between gloss concepts and every two concepts that have more similarities are chosen as the target words
moreover we use not only gram sequence of one word overlaps but also bi gram sequence of two words overlaps
if there is one of the senses the first word in gloss concepts of the second word we give one special score to this two senses
we do this because two concepts may have common words that are not related with their similarities and it causes increasing in scores of that two senses and makes a mistake in choosing related word as a result
considering the word sense in gloss concept of the second word s sense we can award an additional chance to this sense to be chosen in process of choosing words for chains from words that are not semantically related in fact
gloss or gloss is an additional score and considering average existed words in sense s gloss concept and experimental tests we find that the best value for is
it is important in surveying gloss concepts to survey just existed names and existed verbs
at first there are lexical chains generated from highlight sentences with traversing the graph and with assuming lci as one of the lexical chains generated from last step and wj as one of the other sentence s words and with using the above algorithm wj is compared with members of lexical chain lci
if the similarity s score of wj with one of the members of lci is more than threshold t wj is added to lci and from now on other residual words are investigated based on their similarities with members of lci and wj too
function wordingloss for to for to for to for to if s n elseif s n else break if
end if end for end for if
or
wordingloss end if f h wordingloss ed new alledge
end for end for fig
compare algorithm for glosses
clustring lexical chains after lexical chains are constructed for the text there will be some weak lexical chains formed of single word senses
for each lexical chain lci a sentence occurrence vector vi is formed
where n is the ijcsi international journal of computer science issues vol
issue no january issn online www
ijcsi
c international journal of computer science issues
all rights reserved
number of sentences in the document
each is the number of lci members in the sentence
if sentence k has members of lci then is
two lexical chains lci and lcj go into the same cluster if their sentence occurrence vectors vi and vj are similar
our clustering algorithm starts from an initial cluster distribution where each lexical chain is in its own cluster
thus our clustering algorithm starts with n clusters where n is the number of lexical chains
iteratively the most similar cluster pair is found and they are merged to form a single cluster
clustering stops when the similarity between the most similar clusters is lower than a threshold value
for this purpose we used the well known formula from linear algebra in the equation represents the euclidean length for the vector

sequence extraction in our algorithm the text is segmented from the perspective of each lexical chain cluster finding the hot spots for each topic
for each cluster connected sequences of sentences are extracted as segments
sentences that are cohesively connected are usually talking about the same topic
for each lexical chain cluster clj we form sequences separately
for each sentence sk if sentence sk has a lexical chain member in clj a new sequence is started or the sentence is added to the sequence
if there is no cluster member in sk then the sequence is ended
by using this procedure text is segmented with respect to a cluster identifying topic concentration points
figure is an example of text segmentation

experiments and results in this section we conduct experiments to test our summarization method empirically

datasets for evaluation the performance of our methods we used two document datasets and and corresponding word summaries generated for each of documents
the and are an open benchmark datasets which contain and summary pairs from document understanding conference
nist
gov
we use them because they are for generic single document extraction that we are interested in and they are well preprocessed
these datasets and are clustered into and topics respectively
in those document datasets stop words were removed using the
cs
cornell
du pub smart english
stop and the terms were stemmed using porter s scheme which is a commonly used algorithm for word stemming in english
provided stop list in
evaluation metrics there are many measures that can calculate the topical similarities between two summaries
for evaluation the results we use two methods
the first one is by precision p recall r and measure which are widely used in information retrieval
for each document the manually the reference extracted sentences are considered as summary denoted by summref
this approach compares the candidate summary denoted by summcand with the reference summary and computes the p r and measure values as shown in formula
fig
example of text segmentation each sequence is scored using the formula in equation
where li is the number of sentences in the sequencei
slci is the number of lexical chains that starts in sequencei
plci is the number of lexical chains having a member in sequencei and f is the number of lexical chains in cluster
score of the cluster is the average score of the lexical chains in the cluster
our scoring function tries to model the connectedness of the segment using this cluster score
the second measure we use the rouge toolkit for evaluation which was adopted by duc for automatically summarization evaluation
it has been shown that rouge is very effective for measuring document summarization
it measures summary quality by counting overlapping units such as the n gram word sequences and word pairs between reference summary
the rouge n measure compares n grams of two summaries and counts the number of matches
the measure is defined by formula
the candidate summary and the ijcsi international journal of computer science issues vol
issue no january issn online www
ijcsi
c international journal of computer science issues
all rights reserved
rouge n where n stands for the length of the n gram countmatch gram is the maximum number of n grams co occurring in candidate summary and a set of reference summaries
gram is the number of n grams in the reference summaries
we use two of the rouge metrics in the experimental results unigram based and bigram based

simulation strategy and parameters the parameters of our method are set as follows depth of tree that is created for any word extra value for lesk algorithm finally we would like to point out that algorithm was developed from scratch in c
net platform on a pentium dual cpu
ghz pc with kb cache and gb of main memory in windows xp environment

performance evaluation and discussion we compared our method with four methods crf netsum manifold ranking and svm
tables and show the results of all the methods in terms and measure metrics on and datasets respectively
as shown in tables and on dataset the average values of and metrics of all the methods are better than on dataset
as seen from tables and manifold ranking is the worst method in the tables and highlighted the best italic entries performing methods in terms of average evaluation metrics
among the methods netsum crf svm and manifold ranking the best result shows netsum
we use relative improvement for comparison
compared with the best method netsum on dataset our method improves the performance by



and

in terms and respectively
represent bold table average values of evaluation metrics for summarization methods dataset
methods av
av
our method netsum crf svm manifold ranking









av
measure




table average values of evaluation metrics for summarization methods dataset
methods av
av
our method netsum crf svm manifold ranking









av
measure





conclusion to select sentences to to add we have attacked single document summarization
our that human algorithm is able summarizers prefer their summaries
our algorithm relies on wordnet which is theoretically domain independent and also we have used wikipedia for some of the words that do not exist in the wordnet
for summarization we aimed to use more cohesion clues than other lexical chain based summarization algorithms
our results were competitive with other summarization algorithms and achieved good results
using co occurrence of lexical chain members our algorithm tries to build the bond between subject terms and the object terms in the text
with implicit segmentation we tried to take advantage of lexical chains for text segmentation
it might be possible to use our algorithm as a text segmenter
references alguliev r
m
alyguliev r
m

summarization of text based documents with a determination of latent topical sections and information rich sentences
automatic control and computer sciences
dunlavy d
m
oleary d
p
conroy j
m
schlesinger j
d

qcs a system for querying information summarizing documents
clustering processing and management
and erkan g
radev d
r

lexrank graph based lexical centrality as salience in text summarization
journal of artificial intelligence research
jones k
s

automatic summarizing the state of the art
information processing and management
lin c


rouge a package for automatic evaluation summaries
in proceedings of the workshop on text summarization branches out pp

barcelona spain
lin c

hovy e
h

automatic evaluation of summaries using n gram co occurrence statistics
in proceedings of the conference of the american chapter of the association for computational linguistics on human language technology hlt naacl pp

edmonton canada
mihalcea r
ceylan h

explorations in automatic book summarization
in proceedings of the joint conference on empirical methods in natural language processing and computational natural language learning emnlp conll pp

prague czech republic
ijcsi international journal of computer science issues vol
issue no january issn online www
ijcsi
c international journal of computer science issues
all rights reserved
navigli r
lapata m

an experimental study of for unsupervised word sense graph connectivity disambiguation
ieee computer society
porter m

an algorithm for suffix stripping
program
radev d
hovy e
mckeown k

introduction issue on summarization
omputational the special to linguistics
salton g
singhal a
mitra m
buckley c

automatic text structuring and summarization
information processing and management
shen d
sun j

li h
yang q
chen z

document summarization using onditional random fields
in proceedings of the international joint conference on artificial pp

hyderabad india
jcai intelligence svore k
m
vanderwende l
burges c
j
c
enhancing single document summarization by combining ranknet and third party sources
in proceedings of the joint conference on empirical methods in natural language processing and computational natural language learning emnlp conll pp

prague czech republic
wan x

using only cross document relationships for topic focused multi document both summarizations
information retrieval
generic and topic focused multidocument summarization
wan x
yang j
xiao j

manifold ranking in based proceedings of the international joint conference on artificial intelligence ijcai pp

hyderabad india
yeh j y
ke h r
yang w p
meng i h

text summarization using a trainable summarizer and latent semantic analysis
information processing and management
mcdonald d
m
chen h

summary in context searching versus browsing
acm transactions on information systems
fung p
ngai g

one story one flow hidden markov story models for multilingual multi document summarization
acm transaction on speech and language processing
cilibrasi r
l
vitanyi p
m
b

the google similarity measure
ieee transaction on knowledge and data engineering
from received him b
s
degree the mohsen pourvali department of computer engineering at razi university in
currently he is pursuing his m
s
degree in the department of electrical computer qazvin university
his research areas include data mining and text mining
mohammad saniee abadeh received his b
s
degree in computer engineering from isfahan university of technology isfahan iran in the m
s
degree in artificial intelligence from iran university of science and technology tehran iran in and his ph
d
degree in artificial intelligence at the department of computer engineering in sharif university of technology tehran iran in february
dr
saniee abadeh is currently a faculty member at the faculty of electrical and computer engineering at tarbiat modares university
his focused on developing advanced meta heuristic algorithms for data mining and knowledge discovery purposes
his interests include data mining bio inspired computing computational intelligence evolutionary algorithms fuzzy genetic systems and memetic algorithms
research has ijcsi international journal of computer science issues vol
issue no january issn online www
ijcsi
c international journal of computer science issues
all rights reserved

