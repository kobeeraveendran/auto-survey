e f l c
s c v
v i x r a a more abstractive summarization model satyaki chakraborty
cmu
edu xinya li
cmu
edu sayak chakraborty sayak

com may introduction like pointer generator networks is an extremely popular method of text summarization
more recent works in this domain still build on top of the baseline pointer generator by augmenting a content selection phase or by decomposing the decoder into a contextual network and a language model
in our work we rst thoroughly investigate why the generator network is unable to generate novel words and then show that adding an out vocabulary oov penalty is able to improve the amount of novelty abstraction signicantly
we use normalized n gram novelty scores from as a metric for determining the level of straction
moreover we also report rouge scores of our model since most summarization models are evaluated with r l scores
related work research on text summarization can largely be divided into two approaches by methodology pervised learning trained with a cross entropy loss and rl based training which directly tries to optimize the rouge score

sequence to seqeuence models one of the pioneering work in the rst approach based on the original is a model encoder decoder attention rnn model by
its encoder is a bidirectional gru rnn to form the condensed representation and the the decoder is a unidirectional gru rnn which generates the summary
proposes an original way to duce the dimensionality of the softmax s output by thresholding the size of the vocabulary
also applies a switching pointer generator work to generate rare or out of vocab oov words during test time
proposes a model for extractive summarization that uniquely models extractive summarization as sequence tion and learns to generate extractive summaries even when the ground truths are abstract maries
the pointer generator model our project is based on by shares the idea of generating oov words by the network as in
sign their network such that they would output hybrid summaries of abstractive and extractive type and in the same time avoid phrase tion by incorporating a coverage vector into the attention distribution and dening an auxiliary coverage loss that penalizes phrase repetition

rl based methods the main papers on applying rl methods to text summarization include and
corporates the standard encoder decoder neural network with supervised word prediction and inforcement learning of policy using self critical gradient
poses an extractive summarization problem as a sentence ranking task and uses inforcement learning for training the network by optimizing the rouge metric
used an objective function that combines cross entropy loss with rewards obtained from policy gradient learning to optimize the rouge objective
proposes a hybrid summarization task that tracts sentences and rewrite them abstractively while using hybrid network architecture and icy based reinforcement learning
rouge l and gram ized novelty
besides the cnn dm dataset we also use the gigaword for survey material it contains multiple summaries for the same source text and thus provides ample examples for our survey on people s preferences over ferent types as we discuss below

methods to improve survey tion while most of the models focus on improving the rouge score of the generated summaries is one of the few which attempt to actually focus on improving the level of abstraction
they do so by i having a separate contextual network which encodes the current state of the decoder and a separate language model and adding a novelty reward optimized through policy dient to encourage novel word generation
on the contrary in our approach we do not use any external optimization to reward novel word instead we argue that this is a more eration
fundamental issue and that novelty can be proved without an external reward
datasets the most commonly used dataset for text marization is the cnn daily mail dataset by
we used its processed version by in our project which also has become popular and was used to train s original pointer generator work
the original cnn daily mail dataset contains online news articles tokens on age paired with multi sentence summaries
sentences or tokens on
the cessed version contains training pairs validation pairs and test
both the original pg network and our model are evaluated with full length scores of the very motivation of our research project is to provide a better abstractive summarization tool
although a vanilla pointer generator model aims to learn hybrid summaries between the stractive and extractive type its summaries still largely cling to the source text and contain atively few novel words and a low novelty score or abstractiveness
to demonstrate its tance we conducted a survey to investigate ple s preferences between abstractive and tive ground truth summaries from the document understanding conference and marization
each survey taker is presented with pieces source texts and their stractive and extractive summaries without ing told which one is which
they are asked to choose the summary they prefer
a total of takers completed the survey preferences in total
averaging the preferences over all choices with equal weight
are in favor of the stractive summaries
approach in this section we rstly briey discuss the tails of the original vanilla pointer generator work to understand the fundamentals and then shift to its limitations with our diagnosis
nally we show how to overcome those limitations with an additional loss and results of the ed algorithm
figure the network architecture of the vanilla pointer generator
vanilla model where h t is a context vector dened as our model is based on the pointer generator work
in the model a single layer bidirectional lstm layer encodes the source text and duces a sequence of encoder hidden states hi
at every time step t the unidirectional decoder receives hi and the current decoder state st to generate the attention distribution as the ability of distribution over the source words at it makes the generation of oov words inside source text possible and a vocabulary tion pvocab vt wsst battn et at sof where v wh ws and battn are learnable eters
this attention distribution pattn given by at is then subsequently used to generate a text vector h t
the probability distribution over the entire vocabulary or pvocab is also given as follows
pvocab sof v st h h t ai thi i the nal probability distribution of word eration is a weighted combination between the attention distribution and the vocabulary tribution where the respective out of domain words have zero values words not in source text and oov words respectively
such tion weights are learned end to end and as eration probability or pgen pgen h h t wt s st wt xt bptr where vectors wt h ws wx and scalar bptr are learnable parameters and is the sigmoid tion
pf inal pgen pvocab pgen pattn we can easily see from eq
that pgen urally acts like a control switch that decide whether the model will generate a new word from the vocabulary distribution or source word bution
by controlling the generation probability of out of source text words pgen thus control the amount of abstraction of its summaries

observations

novelty we rst calculate the unigram and bigram elties of summaries generated by the vanilla pointer generator and the reference summaries to evaluate their quality
the novelty score is dened by as below n xgen n where n denotes the the function that computes the set of unique n grams in a ment xgen denotes the generated summary xsrc denotes the source document and the number of words in a piece of text s
a novel word is a word that is not in the given source text
since abstractive summaries dier by the extractive ones exactly by its use of novel words a novelty score eectively reects on how stractive a summary is compared to the source text
the novelty comparison table is shown low
table novelty score comparison between pg and ground truth n gram unigram bigram pointer generator ground truth



as we can see from the table the vanilla pointer generator network hardly produces any novel words
we next explore why this happens


pgen distribution figure pgen distribution from gure we observe that for more than of the time pgen is less than
and for less than
of the time it is more than

this explains why novelty of the vanilla model is so less
since pgen is extremely low most of the time the nal probability distribution is nicantly biased towards the attention tion and thus the model ends up mostly copying words from the source text and the vocabulary distribution is mostly just ignored
but why does pgen show this trend to answer that question we do the following analysis


pvocab vs pattn of sampled words we plot the pvocab and the pattn of every word that is sampled from the nal distribution in a randomly selected summary
the scatter plot is shown as follows
figure pvocab vs pattn of vanilla model
time increases from left to right the pgen distribution over randomly lected generated summaries is shown as follows
we observe that even when a word exists in vocabulary the contribution of the vocab bution is non signicant
pvocab is hardly greater than
for all the words that have occurrence in the word vocabulary
since pvocab pattn and pattn pvocab from tion a higher value of pf inal is obtained when pgen is low
this explains the pattern shown in gure
but why does the network have such biases for the attention distribution we observe that in the initial phase of training when the network is randomly initialized both pattn and pvocab are low
as training proceeds pattn slowly increases but there is no ca nt change in pvocab
this could be caused by the fact that pattn is distributed over words our maximum encoder sequence length whereas pvocab is distributed over words
it might be easier for the network to distribute the probability mass over words compared to words and once the preference for pattn is set pgen will start to increase to favour pattn over pvocab and thus the vocabulary distribution is always ignored

modication our assumption stems from a simple tion named entities such as foreign people and place names often appear in the source text and are mostly out of words
the main reason why we have a separate copy tribution in the pointer generator network is cause we would want to have the option of ating such oov words in the summary by ply copying them from the source text
but for a non oov word we argue that if we prefer the vocabulary distribution over the attention tribution we would be able to increase the pability of the network to generate more novel words and thus result in more abstract maries
this means in our case when a target word to be generated is oov we want pgen to be low i
e
favour the attention distribution and when a target word to be generated is non oov we want pgen to be high
we formulate this straint as an auxiliary loss which computes the negative log likelihood between pgen and yoov where yoov if the target word is oov and otherwise
loov pgen figure loov
loov when target word is out of vocabulary a lower value of pgen is ferred
loov when target word is not oov a higher value of pgen is preferred
it is to be noted that even though we have penalized the model to have a preference for vocabulary distribution for a non oov word whether we should use the vocab distribution or the copy distribution is debatable
in order to prevent the model from solely relying on the vocabulary distribution for non oov words we train the network in three phases rst vanilla pointer generator without coverage loss second with coverage loss and third with the auxiliary loss in
this gives us a reasonable balance tween the distributions as is observed in the following gure
truth which unsurprisingly scores the highest
our much higher novelty scores together with the mixed pvocab and pattn demonstrate an crease of abstractiveness in our summaries
figure pvocab vs pattn of our model
time increases from left to right
results before we jump into our result report it is necessary to note that there has not yet been a robust metric to evaluate abstractive text summaries
metrics like rouge have serious metrics like rouge only sess content selection and do not account for other quality aspects such as factual accuracy and uency these metrics rely mostly on ical overlap to evaluate content selection which naturally puts an abstractive summary in advantage as what makes abstractive summary dierent is that it summarizes the source text without perfect lexical overlaps
to address this problem we choose to use normalized n gram novelty dened in by eq

having decided on an evaluation metric fective for abstractive summaries given the low pgen distribution of the original generator model we hypothesized that our ed model with penalty would score much higher in novelty by comparison
to assess our esis we plot the novelty score comparison tween the vanilla pointer generator model our model and the ground truth in order below
each single novelty score is calculated by normalizing over randomly chosen samples from each source
the plot matches our hypothesis that our summaries show a huge improvement on elty score over the original model
in both cases of unigram and bigram our score is more than midway between the original one and the ground figure normalized novelties of baseline vs ours vs ground truth we also report our rouge scores with the original model s below as rouge score is cluded by almost all other summarization tools as one of their evaluation metric albeit it is not ideal for ours
table rouge score comparison between pg and our modied pg pointer generator our modied pg r l





future directions our results of mixed pattn and pvocab strates a denitive improvement of summaries abstractiveness
it is worth mentioning that there is trade o between rouge score and novelty
as the novelty increases the rouge score should decrease
for this report due to time and resource constraints we were unable to measure the correlation between the two and alyze the number of ne tuning iterations needed to hit the sweet spot
for better understanding of the trade o we plan to plot of the pareto frontier between n gram novelty and rouge n for dierent ne tuning iterations
references a
see p
j
liu and c
d
manning get to the point summarization with pointer generator networks arxiv preprint

s
gehrmann y
deng and a
m
rush bottom up abstractive summarization arxiv preprint

w
kryscinski r
paulus c
xiong and r
socher improving abstraction in text summarization arxiv preprint

r
nallapati b
zhou c
gulcehre b
et al
abstractive text tion using sequence to sequence rnns and beyond arxiv preprint

d
bahdanau k
cho and y
bengio neural machine translation by jointly learning to align and translate arxiv preprint

r
nallapati f
zhai and b
zhou marunner a recurrent neural network based sequence model for extractive marization of documents in thirty first aaai conference on articial intelligence
r
paulus c
xiong and r
socher for preprint a deep reinforced model summarization tive

arxiv s
narayan s
b
cohen and m
lapata ranking sentences for extractive rization with reinforcement learning arxiv preprint

y

chen and m
bansal fast summarization with stractive selected sentence rewriting arxiv preprint

k
m
hermann t
kocisky e
stette l
espeholt w
kay m
suleyman and p
blunsom teaching machines to read and comprehend in advances in ral information processing systems pp

summarization
english summarization
html
annotated english gigaword catalog
ldc
upenn
edu
duc and documents measures
nist
gov tasks
html
tasks duc and documents measures
nist
gov tasks
html
tasks
