text generation with exemplar based adaptive decoding hao peng ankur p
parikh manaal faruqui bhuwan dhingra dipanjan das paul g
allen school of computer science engineering university of washington seattle wa google ai language new york ny school of computer science carnegie mellon university pittsburgh pa
washington
edu aparikh mfaruqui
com
cmu
edu r a l c
s c v
v i x r a abstract we propose a novel conditioned text ation model
it draws inspiration from ditional template based text generation niques where the source provides the tent i
e
what to say and the template ences how to say it
building on the ful encoder decoder paradigm it rst encodes the content representation from the given put text to produce the output it retrieves exemplar text from the training data as soft templates which are then used to construct an exemplar specic decoder
we evaluate the proposed model on abstractive text marization and data to text generation
pirical results show that this model achieves strong performance and outperforms ble baselines
introduction conditioned text generation is the essence of many natural language processing nlp tasks e

text summarization mani machine translation koehn and data to text ation kukich mckeown reiter and dale
in its common neural sequence sequence formulation sutskever et al
cho et al
an encoder decoder architecture is used
the decoder generates the text sively token by token conditioning on the feature representations encoded from the source typically with attention bahdanau et al
and copy mechanisms gu et al
see et al

this paradigm is capable of generating uent stractive text but in an uncontrolled and times unreliable way often producing degenerate outputs and favoring generic utterances vinyals and le li et al

the encoder decoder approach differs template based siderably from earlier work done during internship at google
ods becker foster and white iter et al
gatt and reiter inter alia where the source content is lled into the slots of a handcrafted template
these solutions fer higher generation precision compared to ral approaches wiseman et al
but tend to lack the naturalness of neural systems and are less scalable to open domain settings where the number of required templates can be prohibitively large
to sidestep the scalability problems with crafted templates it has been proposed to use lar training samples as exemplars to guide the coding process gu et al
guu et al
weston et al
pandey et al
cao et al
inter alia
in general existing methods accomplish this by a using traditional tion retrieval ir techniques for exemplar tion e

tf idf and then concatenating the exemplar to the source as additional inputs ing the decoder to attend over and copy from both
we propose a different strategy for using plars
for motivation figure shows a target pair together with its exemplar from the gaword dataset graff et al

the target is a summary of the source sentence and the exemplar is retrieved from the training set

there is word overlap between the exemplar and the sired output which would be easily captured by an attention copy mechanism e

norway and aid
despite this ideally the model should also exploit the structural and stylistic aspects to produce an output with a similar sentence structure even if the words are different
indeed in traditional templates the source term exemplar indicates a training instance used to help generation
we aim to distinguish from templates since here no explicit procedure is involved
use the training target as the exemplar whose source is most similar to the current input

describes the details
norway said friday it would source give zimbabwe million kroner
million dollars
million euros in aid to help the country deal with a lack of food and clean drinking water and a cholera outbreak
exemplar norway boosts earthquake aid to pakistan
target million euros to zimbabwe
norway grants aid of
figure a source target pair from gigaword training set along with its exemplar
is supposed to determine what to say while the templates aim to address how to say it reminiscent of the classical content selection and surface realization pipeline reiter and dale
for instance an ideal template for this example might look as follows grants aid of to formulation in the neural it aspect is primarily controlled by the decoder
the how to say inspired by the above intuition we propose exemplar based adaptive decoding where a tomized decoder is constructed for each plar
this is achieved by letting the exemplars to directly inuence decoder parameters through a reparameterization step

the adaptive coder can be used as a drop in replacement in the encoder decoder architecture
it offers the tial to better incorporate the exemplars structural and stylistic aspects into decoding without sive increase in the amount of parameters or putational overhead
we empirically evaluate our approach on stractive text summarization and data to text eration on which most of the recent efforts on exemplar guided text generation have been ied
on three benchmark datasets our approach outperforms comparable baselines and achieves performance competitive with the state of the art
the proposed method can be applicable in many other conditioned text generation tasks
our plementation is available at

washington

background this section lays out the necessary background and notations for further technical discussion
we begin with conditioned text generation and the encoder decoder framework sutskever et al
cho et al

in the interest of the tation clarity will use an elman network man as a running example for the decoder which is briey reviewed in
the proposed technique generalizes to other neural network chitectures

conditioned text generation and the decoder architecture
our discussion centers around conditioned text generation i
e
the model aims to output the target


yt given the source input


xs both of which are sequences of tokens
each token xi yi takes one value from a vocabulary v
and y could vary depending on the tasks

they will tively be articles and summaries for text rization and for data to text generation would be structured data which can sometimes be earized lebret et al
wiseman et al
inter alia and y is the output text
we aim to learn a parameterized conditional distribution of the target text y given the source y t t where t


is the prex of y up to the t token inclusive
the probability of each target token is usually estimated with a softmax function exp y exp p t
wy denotes a learned vector for token y v
depends on y t and x and is computed by a tion which we will describe soon
a typical implementation choice for computing ht is the encoder decoder architecture sutskever et al

more specically an encoder rst gathers the feature representations from the source then a decoder f is used to compute the ht feature vectors f yt
and are respectively the collections of rameters for the encoder and the decoder both of which can be implemented as recurrent ral networks rnns such as lstms iter and schmidhuber or grus cho et al
or the transformer vaswani et al

in sutskever et al
the dependence of f on g is made by using the last hidden state of the encoder as the initial state of the decoder
such dependence can be further supplemented with tention bahdanau et al
and copy nisms gu et al
see et al
as we will do in this work
introduces how we use exemplars to inform decoding by dynamically constructing the coder s parameters
for the notation clarity we will use the elman network as a running example reviewed below
elman networks
given input sequence an elman network elman computes the den state at time step t from the previous one and the current input token by ht qvt where p and q are learned d d parameter trices with being the hidden dimension and vt is the embedding vector for token
we omit the bias term for clarity
method this section introduces the proposed method in detail
our aim is to use exemplars to inform the decoding procedure i
e
how to say it
to complish this we reparameterize the decoder s rameters with weighted linear sums where the efcients are determined by an exemplar
the coder is adaptive in the sense that its parameters vary according to the exemplars
the adaptive coder can be used as a drop in replacement in the encoder decoder architecture
before going into details let us rst overview the high level tion procedure of our model
given source text the model generates an output as follows
run a standard encoder to gather the content representations from the source

retrieve its exemplar zx and compute exemplar specic coefcients


construct the adaptive decoder parameters
using the coefcients computed at step
then the output is generated by applying the adaptive decoder followed by a softmax just as in any other decoder architecture
aiming for a smoother transition we will rst scribe step in
and then go back to discuss step in

for clarity we shall assume that the decoder is implemented as an elman network man equation
the proposed technique generalizes to other neural network architectures as we will discuss later in


reparameterizing the rnn decoder at its core the exemplar specic adaptive decoder involves a reparameterization step which we now describe
we focus on the parameters of the elman network decoder i
e
p and q in equation
parameter construction with linear sums
we aim to reparameterize the pair of matrices p q in a way that they are inuenced by the exemplars
let us rst consider an extreme case where one assigns a different pair of parameter matrices to each exemplar without any sharing
this leads to an unreasonably large amount of parameters which are difcult to estimate reliably
we instead construct p and q from a set of dened parameters matrices
take p for example it is computed as the weighted sum of pi matrices r p ipi where pi rdd with being the size of the hidden states
r is a hyperparameter determining the number of pi matrices to use
the tion is weighted by the coefcients i which are computed from the exemplar zx
for clarity the dependence of both p and i on zx is suppressed when the context is clear
equation constructs the decoder s parameter matrix p using a linear combination of
the exemplar informs this procedure through the coefcients i s the detailed computation of which is deferred to

the other matrix q can be similarly constructed by q i iqi
constraints
in the above formulation the number of parameters is still r times more than a standard elman network which can lead to tting with a limited amount of training data
sides it would be more interesting to compare the adaptive decoder to a standard rnn under a parable parameter budget
therefore we want to further limit the amount of parameters
this can be achieved by forcing the ranks of pi and qi to be since it then takes parameters to form each of them instead of
more formally we the amount of parameters grows linearly with the ber of possible exemplars which as we will soon discuss in
can be as large as the training set
of choosing r empirically we set it equal to in the experiments
please see the end of
for a related discussion
bound their ranks by construction i pi i
a denotes the outer product of two vectors are learned dimensional vectors
each qi can be similarly constructed by a separate set of vectors qi and i
i i i let up vp denote the stack of i i vectors i
e
up vp


r


r
equations and can be compactly written as p up
where is the diagonal matrix built from the dimensional coefcient vector






r the construction of q is similar but with a ent set of parameters matrices uq and q uq q
note that despite their similarities to svd at a rst glance equations and are not ing matrix factorization
rather we are ing up vp uq vq directly p q pi and qi are never explicitly instantiated peng et al

to summarize we reparameterize p and q as interpolations of matrices
by the fact that b the ranks of p and q are upper bounded by r
as pointed out by krueger and memisevic the parameter matrices of a trained rnn tend to have full rank
therefore in the experiments we set r equal to the hidden size d aiming to allow the adaptive decoder to use full rank matrices in the recurrent computation
yet if one holds a priori beliefs that the matrices should have lower ranks using r could be desirable
when r d an adaptive rnn constructed by the above approach has parameters which is comparable to the parameters in a standard elman network
bias term in the elman network can be constructed as b b with b being a learned d r matrix
does not include the bias term which contributes additional parameters to the former and to the latter

incorporating exemplars we now discuss the computation of coefcients through which the exemplars inform the decoder construction equations and
before ing the neural network architecture we begin by describing the exemplar retrieval procedure
retrieving exemplars zx
intuitively similar source texts should hold similar targets
fore given source input we use the training target as its exemplar zx whose source is most similar to
to compute the similarities between source texts we use bag of words bow features and cosine similarity
we extract the plar for each instance
this step is part of the processing and we do not change the exemplars as the training proceeds
there are of course many other strategies to get the exemplars e

using handcrafted or heuristically created hard templates reiter et al
becker foster and white ter alia randomly sampling multiple training stances guu et al
or learning a neural reranker cao et al

using more cally extracted exemplars is denitelly interesting to explore which we defer to future work
computing coefcients
next we describe the computation of the r dimensional coefcient vector which is used to construct the adaptive coder equations and
intuitively the matrices pi and qi s in equation and thereafter can be seen as turing different aspects of the generated text
and determines how much each of them contributes to the adaptive decoder construction
a natural choice to calculate is to use the similarities tween the exemplar and each of the aspects
to accomplish this we run a rnn encoder over zx and use the last hidden state as its vector resentation a
we further associate each pi qi pair with a learned vector ci and then i is puted as the similarity between a and ci using an inner product i
more compactly ca with c



the source of an exemplar is only used in the retrieval and never fed into the encoder decoder model
for a training instance we additionally disallow using its own target as the exemplar
clarity the dependence of a on the exemplar zx is suppressed just as
algorithm adaptive decoder construction
procedure retrieve the exemplar zx compute zx s representation a compute coefcients construct the decoder f

eq
eqs
end procedure nyt giga wikibio inst
avg
len
train dev
test src
tgt
k
m k k



k k k n a
closing this section algorithm summarizes the procedure to construct an adaptive decoder

discussion
although we ve based our discussion on elman networks so far it is straightforward to apply this method to its gated variants hochreiter and schmidhuber cho et al
inter alia and other recurrent neural tures bradbury et al
vaswani et al
peng et al
inter alia
throughout the periments we will be using an adaptive lstm decoder
as a drop in replacement in the encoder decoder architecture it introduces a sonable amount of additional parameters and putational overhead especially when one uses a the sizes small encoder for the exemplar i
e
of the ci vectors in equation are small
it can benet from the highly optimized gpu mentations e

since it uses the same recurrent computation as a standard nonadaptive rnn
in addition to the neural networks the tive decoder requires access to the full training set due to the retrieval step
in this sense it is semi parametric
the idea to dynamically struct the parameters is inspired by works ha et al
and earlier works therein
it proves successful in tasks such as tion jia et al
liu et al
and machine translation platanios et al

many recent template based generation models include the emplars as content in addition to the source and allow the decoder to attend over and copy from both gu et al
guu et al
weston et al
pandey et al
cao et al
inter alia
we compare to this approach in the experiments and show that our model offers nothing prohibits adaptively constructing other nents of the model e

the encoder g
yet our motivation is to use exemplars to inform how to say it which is ily determined by the decoder in contrast the encoder relates more to selecting the content
table number of instances and average text lengths for the datasets used in the experiments
the lengths are averaged over training instances
vorable performance and that they can potentially be combined to achieve further improvements
experiments this section empirically evaluates the proposed model on two sets of text generation tasks stractive summarization
and data to text generation

before heading into the imental details we rst describe the architectures of the compared models in


compared models in addition to previous works we compare to the following baselines aiming to control for founding factors due to detailed implementation choices

the encoder decoder ture enhanced with attention and copy anisms
the encoder is implemented with a bi directional lstm bilstm hochreiter and schmidhuber schuster and wal graves and the decoder a uni directional one
we tie the input dings of both the encoder and the decoder as well as the softmax weights press and wolf
we use beam search during tion with length penalty wu et al

attexp
it is based on
it codes attends over and copies from the emplars in addition to the source inputs
our model using the adaptive decoder adadec closely builds upon
it uses a cally constructed lstm decoder and does not use attention or copy mechanisms over the encoded exemplars
the extracted exemplars are the same as those used by attexp
to ensure fair parisons we use comparable training procedures and regularization techniques for the above els
the readers are referred to the appendix for further details such as hyperparameters

text summarization datasets
we empirically evaluate our model on two benchmark text summarization datasets annotated gigaword corpus gigaword graff et al
napoles et al

gaword contains news articles sourced from various news services over the last two decades
to produce the dataset we follow the split and preprocessing by rush et al
and pair the rst sentences and the headlines in the news articles
it results in a
train dev
split
the average lengths of the source and target texts are
and
respectively
new york times annotated corpus it contains news nyt sandaus
articles published between and by new york times
we use the split and preprocessing by durrett et al

following their effort we evaluate on a smaller portion of the test set where the gold summaries are longer than tokens
we further randomly sample instances from the training data for validation ing in a train dev
split
compared to gigaword the inputs and targets in nyt are much longer averaging
and
respectively
table summarizes some statistics of the datasets
we note that some recent works use a ent split of the nyt corpus paulus et al
gehrmann et al
and thus are not ble to the models in table
we decide to use the one by durrett et al
because their cessing script is publicly available
for both datasets we apply byte paired ing bpe sennrich et al
which proves to improve the generation of proper nouns fan et al

by using adaptive decoders empirical results
table compares the models on gigaword test set in rouge lin
our model adadec improves over by more than
rouge scores
cao et al
and the full model by cao et al
hold the best published results
the former uses extensive handcrafted features and relies on external mation extraction and syntactic parsing systems
com berkeley doc summarizer


of the ofcial script
model rg l open nmt cao et al
basic cao et al
full et al
this work this work attexp this work adadec




















text summarization performance table in rouge scores dubbed as rg x on gigaword test set

denotes the models using retrieved exemplars while uses handcrafted features
bold font indicates best performance
open nmt numbers are taken from cao et al

while the latter uses additional encoding tion and copy mechanisms over the exemplars extracted using a novel neural reranker
adadec achieves better or comparable performance to the state of the art models without using any handcrafted features or reranking techniques
the basic model by cao et al
ablates the reranking component from their full model and uses the top exemplar retrieved by the ir system
therefore it is a more comparable baseline to ours
adadec outperforms it by more than
rouge scores
surprisingly we do not observe interesting improvements by attexp over the sequence to sequence baseline
we believe that our model can benet from better extracted exemplars by e

applying a reranking system
such exploration is deferred to future work
the nyt experimental results are summarized in table
we follow previous works and port limited length rouge recall values
rett et al
is an extractive model and paulus et al
an abstractive approach based on forcement learning
our adadec model forms both
we observe similar trends when paring adadec to the and attexp baselines with the exception that attexp does improve over

data to text generation data to text generation aims to generate textual descriptions of structured data which can be following durrett et al
and paulus et al
we truncate the predictions to the lengths of the gold maries and evaluate rouge recall instead of on length predictions
model durrett et al
paulus et al
this work this work attexp this work adadec









table nyt text summarization test performance in rouge recall values
this is a smaller portion of the original test data after ltering out instances with summaries shorter than tokens
durrett et al

denotes the models using retrieved exemplars and bold font indicates best performance
seen as a table consisting of a collection of records liang et al

for a given entity each record is an attribute value tuple
figure shows an example for entity jacques louis david
the table species the entity s properties with ples born august nationality french and so forth
the table is paired with a description which the model is supposed to generate using the table as input
we refer the readers to lebret et al
for more details about the task
dataset and implementation details
we use the wikibio dataset lebret et al

it is tomatically constructed by pairing the tables and the opening sentences of biography articles from english wikipedia
we follow the split and processing provided along with the dataset with around k train dev
instances
following lebret et al
we linearize the tables such that we can conveniently train the sequence to sequence style models described in

table summarizes some statistics of the dataset
in contrast to the text summarization ment
we do not apply bpe here
ther the word embeddings are initialized with glove pennington et al
xed during training and not tied with the softmax weights
in addition to the models introduced in
we additionally compare to ing to study whether the adaptive decoder can ther benet from attention and copy mechanisms over the exemplars
empirical results
following liu et al
we report and bleu scores papineni jacques louis david august december was a french painter in the neoclassical style
figure a training instance from the wikibio dataset
it consists of a collections of records for jacques louis david top and a piece of textual description bottom
et al

table summarizes the to text generation results on the wikibio test set
overall we observe similar trends to those in the summarization experiment
by ing over and copying from the exemplars texp improves upon the baseline by around
absolute scores
also utilizing emplar information our adadec model performs by a larger margin
for and
for bleu
we further study whether we can get further improvements by bining both
achieves around
absolute improvements over adadec less than those by attexp over
this vides evidence that to some extend the ways texp and adadec incorporate exemplar mation might be complementary
wiseman et al
is a template motivated model based on a semi markov model
liu et al
hold the current state of the art results
they encode the table structures by using a position and led beddings and structure aware attention and gating techniques
these techniques are beyond the scope of this work which focuses mainly on the decoding end
use the script by lin to calculate the rouge score and the mteval script for bleu github
com moses smt mosesdecoder master scripts generic mteval

model bleu wiseman et al
liu et al


this work this work attexp this work adadec
this work








table data to text generation performance in and bleu on the wikibio test set

indicates the models using retrieved exemplars
analysis we now qualitatively evaluate our model by studying how its outputs are affected by using different exemplars
figure shows two domly sampled gigaword development instances
it compares the outputs by adadec i
e
without attention copy over exemplars
when ing different exemplars controlling for the same in each example exemplar source inputs
is retrieved by the system i
e
a training target
while the remaining ones are produced by the authors by modifying the rst one in styles and sometimes introducing distractions in the tent
in the top example the model includes ple into the subject three vs
three people der the inuence by exemplar exemplar changes the tense and adds some distraction by changing the place from britain to canada
the model follows the tense switch but gets fused by the distraction and decides to let a train in southern europe collide into north america which it should not
looking at the bottom ple the model in general follows the exemplar in using noun adjuncts or prepositional phrases e

new home sales vs
sales of new homes except the rst one
perhaps confused by the distraction in exemplar the model makes a judgment on the specic amount of growth but gets it wrong
related work exemplar based generation
partly inspired by traditional template based generation kukich reiter and dale inter alia many cent efforts have been devoted to augmenting text generation models with retrieved exemplars dosh et al
mason and charniak song et al
lin et al
inter alia
a portuguese train derailed source in the northern region of oporto on wednesday killing three people


exemplar train collision
output train derailment
two die in a britain three killed in portuguese two people were exemplar killed in britain train collision
output portuguese train derailment
three people killed in a train collision in exemplar canada killed two people
output in northern mexico killing three
portuguese train derails sales of new homes in the source u
s
increased by
percent in may the biggest gain in years


u
s
sales of new homes exemplar up strongly in march
output
percent in may
us new home sales rise the sales of new homes exemplar in the u
s
grow strongly
output sales of new homes in us rise in may
u
s
economic new home sales grow by
exemplar tics percent
output us new home sales grow percent in may
figure two randomly sampled gigaword opment instances used for qualitative evaluation
exemplar s are retrieved by the system
while the remaining ones are produced by the authors
notable exemplars changes are highlighted in bold purple and output changes in italic yellow
without committing to an explicit cess a typical method is to include exemplars as additional inputs to the sequence to sequence models gu et al
pandey et al
guu et al
inter alia
wiseman et al
took a different approach and used a semi markov model to learn templates
dynamic parameter construction
the idea of using a smaller network to generate weights for a larger one dues back to stanley et al
and koutnik et al
mainly under the evolution computing context
it is later revisited with resentation learning moczulski et al
nando et al
al shedivat et al
ter alia and successfully applied to tion jia et al
liu et al
and machine translation platanios et al

it also relates to the meta learning set up thrun and pratt
schwenk and yoshua bengio

learning phrase representations using rnn encoder decoder in proc
of for statistical machine translation
emnlp
conclusion we presented a text generation model using exemplar informed adaptive decoding
it rameterizes the decoder using the information gathered from retrieved exemplars
we mented with text summarization and data to text generation and showed that the proposed model achieves strong performance and outperforms comparable baselines on both
the proposed model can be applicable in other conditioned text generation tasks
we release our tion at
cs
washington

acknowledgments we thank antonios anastasopoulos ming wei chang michael collins jacob devlin yichen gong luheng he kenton lee dianqi li zhouhan lin slav petrov oscar tackstrom kristina toutanova and other members of the google ai language team for the helpful sion and the anonymous reviewers for their able feedback
references maruan al shedivat avinava dubey and eric p
contextual explanation networks
xing



dzmitry bahdanau kyunghyun cho and yoshua gio

neural machine translation by jointly learning to align and translate
in proc
of iclr
tilman becker

practical template based ral language generation with tag
in proceedings of the sixth international workshop on tree adjoining grammar and related frameworks
james bradbury stephen merity caiming xiong and richard socher

quasi recurrent neural work
in proc
of iclr
greg durrett taylor berg kirkpatrick and dan klein

learning based single document tion with compression and anaphoricity constraints
in proc
of acl
jeffrey l
elman

finding structure in time
cognitive science
angela fan david grangier and michael auli

in controllable abstractive summarization
ceedings of the workshop on neural machine translation and generation
chrisantha fernando dylan banarse malcolm reynolds frederic besse david pfau max berg marc lanctot and daan wierstra

convolution by evolution differentiable pattern producing networks
in proceedings of the genetic and evolutionary computation conference
mary ellen foster and michael white

niques for text planning with xslt
in proceeedings of the workshop on nlp and xml rdf rdfs and owl in language technology
albert gatt and ehud reiter

simplenlg a in alisation engine for practical applications
ceedings of the european workshop on natural language generation
sebastian gehrmann yuntian deng and alexander m rush

bottom up abstractive summarization
in proc
of emnlp
david graff junbo kong ke chen and kazuaki maeda

english gigaword second edition
alex graves

supervised sequence labelling with recurrent neural networks volume of studies in computational intelligence
springer
jiatao gu zhengdong lu hang li and victor ok incorporating copying mechanism in li

sequence to sequence learning
in proc
of acl
jiatao gu yong wang kyunghyun cho and search engine guided in proc
of tor ok li

parametric neural machine translation
aaai
ziqiang cao wenjie li sujian li and furu wei

retrieve rerank and rewrite soft template based neural summarization
in proc
of acl
kelvin guu tatsunori b hashimoto yonatan oren and percy liang

generating sentences by editing prototypes
tacl
ziqiang cao furu wei wenjie li and sujian li

faithful to the original fact aware neural abstractive summarization
in proc
of aaai
kyunghyun cho bart van merrienboer caglar cehre dzmitry bahdanau fethi bougares holger david ha andrew dai and quoc v le

networks
in proc
of iclr
kaiming he xiangyu zhang shaoqing ren and jian sun

deep residual learning for image nition
in proc
of cvpr
sepp hochreiter and jurgen schmidhuber

neural computation long short term memory

micah hodosh peter young and julia hockenmaier

framing image description as a ranking task data models and evaluation metrics
jair
xu jia bert de brabandere tinne tuytelaars and in luc v gool

dynamic lter networks
proc
of neurips
diederik kingma and jimmy ba

adam a in proc
of method for stochastic optimization
iclr
durk p kingma tim salimans and max welling

variational dropout and the local terization trick
in proc
of neurips
philipp koehn

statistical machine translation
cambridge university press
jan koutnik faustino gomez and jurgen ber

evolving neural networks in compressed weight space
in proceedings of the annual ence on genetic and evolutionary computation
david krueger and roland memisevic

larizing rnns by stabilizing activations
in proc
of iclr
karen kukich

design of a knowledge based port generator
in proc
of acl
remi lebret david grangier and michael auli

neural text generation from structured data with in proc
of application to the biography domain
emnlp
jiwei li michel galley chris brockett jianfeng gao and bill dolan

a diversity promoting jective function for neural conversation models
in proc
of naacl
percy liang michael i
jordan and dan klein

learning semantic correspondences with less vision
in proc
of acl
chin yew lin

rouge a package for automatic in text summarization evaluation of summaries
branches out proceedings of the shop
kevin lin dianqi li xiaodong he zhengyou zhang and ming ting sun

adversarial ranking for language generation
in proc
of neurips
minh thang luong hieu pham and christopher d
manning

effective approaches to in proc
of based neural machine translation
emnlp
inderjeet mani

advances in automatic text marization
mit press
rebecca mason and eugene charniak

specic image captioning
in proc
of conll
kathleen mckeown

text generation
bridge university press
marcin moczulski misha denil jeremy appleyard and nando de freitas

acdc a structured efcient linear layer


courtney napoles matthew gormley and benjamin in van durme

annotated gigaword
ceedings of the joint workshop on automatic edge base construction and web scale knowledge extraction
gaurav pandey danish contractor vineet kumar and sachindra joshi

exemplar encoder decoder for neural conversation generation
in proc
of acl
kishore papineni salim roukos todd ward and wei jing zhu

bleu a method for automatic uation of machine translation
in proc
of acl
romain paulus caiming xiong and richard socher

a deep reinforced model for abstractive marization
in proc
of iclr
hao peng roy schwartz sam thomson and noah a
smith

rational recurrences
in in proc
of emnlp
hao peng sam thomson and noah a
smith

deep multitask learning for semantic dependency parsing
in proc
of acl
hao peng sam thomson and noah a
smith

backpropagating through structured argmax using a spigot
in proc
of acl
hao peng sam thomson swabha swayamdipta and noah a
smith

learning joint semantic parsers from disjoint data
in proc
of naacl
jeffrey pennington richard socher and pher d
manning

glove global vectors for word representation
in proc
of emnlp
emmanouil antonios platanios mrinmaya sachan graham neubig and tom mitchell

tual parameter generation for universal neural chine translation
in proc
of emnlp
pengfei liu xipeng qiu and xuanjing huang

dynamic compositional neural networks over tree structure
in proc
of ijcai
or press and lior wolf

using the output in proc
of bedding to improve language models
eacl
tianyu liu kexiang wang lei sha baobao chang and zhifang sui

table to text generation by structure aware learning
in proc
of aaai
ehud reiter and robert dale

building applied natural language generation systems
natural guage engineering
yonghui wu mike schuster zhifeng chen quoc v
le mohammad norouzi wolfgang macherey maxim krikun yuan cao qin gao klaus macherey jeff klingner apurva shah melvin johnson xiaobing liu ukasz kaiser stephan gouws yoshikiyo kato taku kudo hideto kazawa keith stevens george kurian nishant patil wei wang cliff young jason smith son riesa alex rudnick oriol vinyals greg rado macduff hughes and jeffrey dean

google s neural machine translation system ing the gap between human and machine translation


ehud reiter somayajulu sripada jim hunter jin yu and ian davy

choosing words in generated weather forecasts
articial intelligence
alexander m
rush sumit chopra and jason weston

a neural attention model for abstractive tence summarization
in proc
of emnlp
evan sandaus

the new york times annotated corpus
ldc corpora
linguistic data consortium
m
schuster and k
k
paliwal

bidirectional recurrent neural networks
transactions on signal proccesing
abigail see peter j
liu and christopher d
manning

get to the point summarization with generator networks
in proc
of acl
rico sennrich barry haddow and alexandra birch

neural machine translation of rare words with subword units
in proc
of acl
yiping song rui yan xiang li dongyan zhao and ming zhang

two are better than one an ensemble of and generation based dialog systems


nitish srivastava geoffrey hinton alex krizhevsky ilya sutskever and ruslan salakhutdinov

dropout a simple way to prevent neural networks from overtting
jmlr
k
o
stanley d
b
dambrosio and j
gauci

a hypercube based encoding for evolving scale neural networks
articial life
ilya sutskever oriol vinyals and quoc v le

sequence to sequence learning with neural works
in proc
of neurips
sebastian thrun and lorien pratt editors

learning to learn
kluwer academic publishers
ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan n gomez ukasz kaiser and illia polosukhin

attention is all you need
in proc
of neurips
oriol vinyals and quoc le

a neural tional model
in proc
of icml
jason weston emily dinan and alexander miller

retrieve and rene improved sequence eration models for dialogue
in proceedings of the international workshop on search oriented versational ai
sam wiseman stuart shieber and alexander rush

challenges in data to document generation
in proc
of emnlp
sam wiseman stuart m shieber and alexander m rush

learning neural templates for text eration
in proc
of emnlp
and further truncate the source to the rst units
during evaluation we apply beam search of width with a
length penalty wu et al

a
data to text generation we in general follow the implementation details in the summarization experiment with the following modications we do not apply byte paired encoding here and use a vocabulary of size k
the word embeddings are initialized using version dimensional glove et al
and xed during ing
further the softmax weights are not tied to the embeddings
three layer dimensional bilstm coders are used with residual connections the exemplar encoder uses an one layer dimensional bilstm
early stopping is applied based on ment set performance
a maximum decoding length of tokens is used
appendices a implementation details our implementation is based on tensorflow
for both experiments we use the similar plementation strategies for the baselines and our model aiming for a fair comparison
a
text summarization we train the models using adam kingma and ba with a batch size of
we use the default values in tensorflow adam implementation for initial learning rate and
the models are trained for up to epochs with the learning rate annealed at a rate of
every epochs
a weight decay of
is applied to all ters with being the current learning rate
the norms of gradients are clipped to

early ping is applied based on rouge l performance on the development set
for the weights of the output softmax function are tied with the word embeddings which are the encoders we randomly initialized
use dimensional layer bilstms in the gigaword experiment and dimensional layer bilstms in the nyt experiment both with residual connections he et al
the decoders are one layer adaptive lstms and have the same size as the encoders and so do the word embeddings
we apply variational dropout kingma et al
in the encoder rnns and dropout srivastava et al
in the embeddings and the softmax layer the rates of which are empirically selected from



the last hidden state at the top layer of the encoder is fed through an one layer tanh mlp and then used to as the decoder s tial state
we use the attention function by luong et al
and copy mechanism by see et al

the exemplar encoder
uses layer bilstm for gigaword and nyt periments respectively
for numerical stability equation is scaled to have norms of d with being the hidden size of the adaptive coder peng et al

in the gigaword experiment we use k bpe types and limit the maximum decoding length to be subword units while for nyt we use k types with a maximum decoding length of
tensorflow

