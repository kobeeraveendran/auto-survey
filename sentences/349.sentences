topic aware abstractive text summarization chujie zheng
edu university of delaware usa harry jiannan wang
edu university of delaware usa kunpeng zhang
edu university of maryland usa ling fan
edu
cn tongji university china t c o l c
s c v
v i x r a abstract automatic text summarization aims at condensing a document to a shorter version while preserving the key information
ent from extractive summarization which simply selects text ments from the document abstractive summarization generates the summary in a word by word manner
most current state the art sota abstractive summarization methods are based on the transformer based encoder decoder architecture and focus on novel self supervised objectives in pre training
while these models well capture the contextual information among words in documents little attention has been paid to incorporating global semantics to better fine tune for the downstream abstractive summarization task
in this study we propose a topic aware abstractive tion taas framework by leveraging the underlying semantic structure of documents represented by their latent topics
cally taas seamlessly incorporates a neural topic modeling into an encoder decoder based sequence generation procedure via tention for summarization
this design is able to learn and preserve global semantics of documents and thus makes summarization fective which has been proved by our experiments on real world datasets
as compared to several cutting edge baseline methods we show that taas outperforms bart a well recognized sota model by and regarding the f measure of and rouge l respectively
taas also achieves rable performance to pegasus and prophetnet which is difficult to accomplish given that training pegasus and prophetnet quires enormous computing capacity beyond what we used in this study
keywords abstractive text summarization topic aware attention semantics introduction in today s digital economy we are facing a tremendous amount of information every day which often leads to information overload and poses great challenges to efficient information consumption
as shown in figure summarization enables a quick and condensed overview of the content and has been used in various applications to help users navigate in the ocean of content abundance
marization has been a widely studied topic in natural language processing nlp area where a short and coherent snippet is tomatically generated from a longer text
an accurate and concise summarization is very critical to many downstream tasks such as figure an example of text information retrieval and recommender systems
as illustrated by automatic summarization by algorithms can reduce reading time make users selection process easier improve the effectiveness of indexing be less biased than human summaries and increase the number of texts consumer are able to process
researchers have been developing various summarization niques that primarily fall into two categories extractive rization and abstractive summarization
extractive summarization involves the selection of phrases and sentences from the source document to generate the new summary
it involves ranking the relevance of phrases in order to choose only those most relevant to the meaning of the source
it does not modify any words
on the contrary abstractive summarization generates entirely new phrases and sentences in a word by word manner to capture the meaning of the source text
this is a more challenging direction but consistent with what humans do in summarization which holds the hope of more general solutions to this task
thus the present study focuses on abstractive summarization
recently deep learning models have shown promising results in many domains
inspired by the successful application of deep ing methods for machine translation abstractive text tion is specifically framed as a sequence to sequence learning task
therefore various sequence models especially the game changing transformer based encoder decoder framework can come to help
transformer has been used in a wide range of downstream cations such as reading comprehension question answering and
espn
com natural language inference and has achieved astonishing mance
for example the recent transformer based is ered the largest language model so far with a whopping billion parameters and can produce amazing results in various tasks with zero or few shots learning
similarly most current state of the art sota abstractive summarization methods also focus on novel self supervised objectives in pre training
within the contribution from attention mechanism transformer based models can well capture the syntactical and contextual information among words in documents
however little attention has been paid to incorporating semantic information at a global level to better fine tune for the specific abstractive summarization task
in ticular the latent topics in documents should play a role in text summarization since the generated summary is expected to capture the key information of the source text
many topic modeling ods have been proposed to discover the latent semantic structures of a collection of documents
each topic is distributed over words from the documents with their corresponding ities of belonging to a specific topic
the intuition behind our study in this paper is that by leveraging the topic association information of each word in the document our model is able to assign more weights to words that are more likely to represent the key topics of the documents and thus generate better summarization
like prior studies we also adopt a sequence to sequence model to generate summaries
a key component in a model is how to represent and encode a source text
current approaches include a sum up approach and a self attention approach
the sum up proach summarizes all latent representations of an input sequence into one latent representation for decoding
recent years have witnessed the prosperity of this approach in sequence modeling
however this approach has three shortcomings that need to be addressed it is easy to generate fake dependencies due to the overly strong assumption that any adjacent interactions in a quence must be dependent which may not be true in the real world because there might exist some noisy information in a sequence it is likely to capture point wise dependencies only while the tive group dependencies are greatly ignored the importance of each individual input in a sequence is likely not to be equal which inspires many research efforts on attention based sequence ing
among which self attention in transformer style architecture is the most common and well developed one
the short term and long term dependencies among inputs are well captured
please refer to for details
they seem to focus on capturing contextual information at a syntactical level while the semantics are looked which might significantly reduce the sequence modeling performance especially for the summarization task
motivated by this we intend to take the semantic structure of an input document into consideration in particular the latent topics of a document
this can overcome the limitation that we only focus on local contextual information while all high level semantics are neglected
therefore in our paper we bring this idea to the rization task and propose a new framework named topic aware abstractive summarization taas
we believe that this design can help us find informative words for a more comprehensive tation of the input sequence which leads to a better summary
our empirical experiments also demonstrate its effectiveness as compared to existing methods
overall the main contributions of this paper are three fold we propose a new framework for abstractive summarization with topic information incorporated which helps to ture the semantic information and provides guidance during generation to preserve the key information
this generic framework opens a new perspective in nlp and can be tended to other language tasks
we implement topic aware attention using topic level tures through neural topic modeling and transformer based encoder decoder which efficiently extracts salient topics and understands the long term dependency and informativeness of words in the input sequence
we conduct experiments on a real world dataset and pare the performance of our model with several state of art approaches to demonstrate the effectiveness of our model on the summarization task
we also discuss the impact of important hyperparameters in the model and different types of data on the performance
related work three lines of research are closely related to our paper attention mechanism text summarization and topic modeling
attention mechanism brings nlp to a new stage since its inception in
at present various models based on the attention mechanism have achieved breakthrough performance in many tasks of nlp
the idea of attention mechanism is inspired by the human visual attention which allows us to focus on a certain region with high resolution
the attention can be interpreted as a vector of importance weight
to predict or infer one element such as a pixel in an image or a word in a sentence we usually use an attention vector to estimate how strongly it is correlated with other elements
the element highly relevant to the target should be assigned with a higher weight while the irrelevant ones should be associated with lower weights
using the language generation task as an example the attention mechanism provides guidance on which how words contribute to the sequence generation which has been commonly seen in many nlp applications
text summarization is a widely studied topic in nlp
it aims to provide a high level view of the input document to a short and cise summary
there are two types of summarization approaches extractive and abstractive summarization
early extractive methods formulate the problem as selecting a subset of sentences to capture the main idea of the input document using handcrafted features and graph based structural information
with the advancement of models the encoder decoder network has been shown promising ability to generate the abstractive summary
in this framework the encoder obtains a comprehensive sentation for the input sequence and the decoder generates the output summary based on the latent representation
recurrent ral network rnn and transformer are commonly adopted in the encoder decoder network
in particular the transformer along with the attention mechanism has become a state of the art standard in both academia and industry
several variants have achieved ing results in the summarization task
for example bart is such a model and very effective for text generation tasks where it symbol representation input sequence h output from hidden state for x y output sequence s latent representation for r topic word distribution p r topic embedding topic attention for topic attention of token under topic table notations implements the bidirectional encoder and the left to right gressive decoder
pegasus and prophetnet introduce different pre training objectives for text summarization
pegasus masks important sentences and generates those gap sentences from the rest of the document as an additional pre training objective
prophetnet predicts the next tokens simultaneously at each time step which encourages the model to plan for the future tokens
one common drawback in these transformer based summarization models is that higher level global semantic structure in the text is usually ignored such as latent topics which motivates our study that designs topic aware attention for summarization
topic model is an important component in the taas
it covers semantically relevant terms that form coherent topics via probabilistic generative models in an unsupervised manner
one basic assumption among various topic models is that a ment is a mixture of topics and each topic is distributed over words in the vocabulary of the corpus
to learn these distributions latent dirichlet allocation lda is introduced by imposing latent ables with dirichlet prior
recently the development of deep generative networks and stochastic variational inference enables neural network based topic modeling that is proven to be effective
among which auto encoding variational bayes provides a generic framework for deep generative topic modeling especially the variational auto encoder vae that consists of a generative network and an inference network
this framework severs as an important foundation for many studies in this field
for ample the neural variational document model nvdm applies vae for unsupervised document modeling with bag of words ment representation
gaussian softmax model gsm extends nvdm by constructing the topic distribution with a softmax tion which is applied to the projection of the gaussian random vectors
in recent years topic modeling has also been extended to other nlp tasks including text summarization
they are different from ours in that we develop a topic attention via neural topic modeling and for text summarization
preliminaries in this section we first formally define the summarization task with key notations and concepts explained
we then briefly review sequence to sequence transformer based architecture which our taas is built upon
notations throughout the paper are listed in table

problem definition automatic text summarization aims at condensing a document to a shorter version while preserving the key information
let


be an input document with tokens and is the word embedding for the token
given our taas model learns a function that maps to another sequence of tokens where y is the summary with tokens
this automatic generation process is achieved by ing the probability via the beam search algorithm
is usually implemented by neural networks or transformer parameterized by

transformer architecture the key idea behind the sequence to sequence model is to represent an input sequence as a low dimensional vector while preserving the contextual information in the sequence as much as possible upon which a new task specific sequence with an arbitrary length can be automatically generated
in practice it usually consists of an encoder and a decoder where the encoder encodes key information from the input sequence and generates a contextualized tation s which is the input to the decoder
taking a single encoder layer as an example given an input sequence we can obtain h where each is the hidden state from that encoder layer for the input
given is a learned representation for input token one of the typical approach to obtain a sequence level representation s is to sum up all word level representations as shown in eq

note that this strategy ignores the complicated relationships among all token level latent representations
h s different approaches to obtain a sequence level representation have been proposed
for example adds a special token cls located at the beginning of a sequence
the final hidden state responding to this token by the encoder as is used as the aggregate sequence representation
we adopt this approach in this paper
to transform to h we leverage the sequence to sequence transformer network
given an input sequence transformer calculates multi head self attention for mapping one variable length sequence of symbol representation to another sequence of equal length h with r where is the hidden size
using single head attention as an ample transformer first multiplies w w and w to the input sequence to find the query matrix q the key matrix k and the value matrix v like eq

q w k w v w then the attention is calculated according to eq

k v softmax qk v thus the latent representation of an input sequence s can be written as follows which is also the output from the hidden state for the first token cls
be written as eq
s topic aware abstractive summarization taas in this section we describe the details of our proposed model taas which considers both syntactical and semantic structures of text for abstractive summarization
as illustrated in fig
taas consists of three major components a neural topic modeling
it is a deep learning based topic model where the variational autoencoder vae is implemented to learn latent topic vectors t document topic distribution and topic word distribution via two networks i
e
encoder and decoder in a generative manner
b topic aware attention
to incorporate the latent structure of documents at a semantic level into a subsequent based summarization model we introduce topic aware attention to derstand the impact of words on the summarization
we believe that such a design can help our model capture global information by leveraging the topic features learned from the neural topic eling
c encoder decoder based sequence modeling
the based encoder decoder framework is employed to understand plicated syntactical features in the text
the hidden state generated by the encoder along with the topic attention is used to calculate a latent representation where the contextual and global information is captured
this representation is the input to the decoder for the output summary generation

neural topic model our topic weighted attention is built on extracting the latent topic information through neural topic model ntm
ntm is based on variational auto encoder vae involved with a tinuous latent variable z as latent topics
given an input sequence document d contains tokens the latent topic variable z r corresponds to the topic proportion of document d
here denotes the number of topics
is the topic assignment for the observed word
ntm implements a vae to learn latent topic vectors via two networks a generative network and an inference network
the generative network encoder is a compressor that transforms the input text data into a latent resentation i
e
a latent topic vector while the inference network decoder is a reverter that reconstructs the latent representation back to the original input
such a design of using neural networks to parameterize the multinomial topic distribution can eliminate the need to predefine distributions to guide the generative process
it only requires specifying a simple prior e

a diagonal gaussian
therefore the overall generative process for document d can n z here we pass a diagonal gaussian distribution with mean and variance to parameterize the multinomial document topic distribution and build an unbiased gradient estimator for the variable distribution
w and b are trainable parameters
all parameters involved in this generative process are denoted by
the inference network is to approximate the true tion using a diagonal gaussian d d that is parametrized by
we use three fully connected networks and to represent d and d as d d and d d
overall we use variational inference to approximate the posterior distribution over z
the loss function is defined as the negative of variational lower bound elbo
in eq
z and are probabilities for encoding and decoding processes
is a standard normal prior i
z e z
topic aware attention to incorporate the document level semantics embedded in latent topics into the encoder decoder model taas introduces a topic aware attention mechanism from which two components are connected to enrich the representation of the input sequence for better summarization
as we mentioned above it is very ferent from prior studies where they either develop a sum up or a self attention approach to representing the input document
these past research emphasize contextual information at a syntactical level while the semantics are neglected which can deteriorate the sequence modeling performance especially for the summarization
this motivates us to design a topic aware attention based approach see figure
specifically it is designed as follows
from ntm we obtain a topic word distribution r where and denote the number of topics and the vocabulary size respectively
given a topic embedding and the hidden state of an input sequence we calculate the attention weight a as a where is the attention weight for the token under the topic



is the output of the last hidden layer of the encoder network h r and h r
however this simple attention design has two drawbacks the trained model is not able to generalize to unseen documents that have some words that never appear in the training vocabulary the dimensionality of and h mismatches because the is usually much larger than the hidden size
to overcome these limitations we add a transformation component i
e
mapping to realized by a fully connected feed forward network with a residual connection followed by a layer normalization p figure overview of our proposed topic aware abstractive summarization model taas to the decoder
then the decoder outputs a summary in a sequential manner
specifically the input sequence is sent to the first layer of the encoder where a hidden representation is generated
for the rest of the layers in the encoder the output from the previous layer serves as the input of the current layer
the final state of the encoder along with the topic attention is used to generate the latent h representation which serves as the initial hidden state for the decoder
in previous works recurrent neural network rnn and attention in transformer are two most widely used architectures for the encoder decoder network
one major weakness of this based approach lies in that the contextualized representation s has a short term impact on the generated sequence because it is only used at the beginning of the generation process
to address this challenge the attention mechanism is introduced to take the entire encoder context into account
as illustrated in eq
s is available during decoding by conditioning the current decoder state on it
is a stand in for self attention calculation and is the word embedding for the output sampled from the softmax at the previous step
h h h z h y s motivated by the great success of the transformer based model like bert in recent years we employ this sequence to sequence transformer architecture for our abstractive summarization task
given the topic attention and the hidden state h i
e
h with the superscript omitted the latent representation s is calculated as s h figure architecture of topic attention p r saves us from being confined to pre defined lary where is the hidden size of the encoder
p is considered as the topic embedding which carries latent features and information for the topic
now the attention weight a can be rewritten as a h further for every token we average the attention weight over topics to get a topic aware attention as this weight is then normalized via softmax to obtain our final attention as we denote this final topic attention as

encoder decoder sequence modeling our final sequence to sequence model uses a standard decoder framework where the encoder generates a contextualized latent representation s of the input sequence which is the input parameter estimation taas consists of two objectives from ntm and encoder decoder sequence modeling that need to be jointly optimized
the objective function of taas is defined as eq

is the negative of elbo defined in eq
and is the hidden states from encoder the following are extractive summarization methods
cross entropy loss between the predicted output of decoder and the true summary
is a hyper parameter that balances the importance between ntm and the encoder decoder
the overall process of taas is sketched in algorithm
algorithm taas topic aware abstractive summarization input input sequence d output summary y training phase for all do for all do topic word distribution each batch p h a hp calculate using eq
and eq
obtain latent representation s h update parameters of taas learning rate objective function see eq
sequence generation summary topic attention end for end for test phase y repeat steps for and the learned parameters
experiments in this section we first describe the dataset evaluation metrics and parameter settings
then we conduct several experiments to compare taas against the state of the art text summarization models
parameter sensitivity and model ablation analysis are also discussed
codes are publicly available

experimental settings dataset the data we use in this study is the cnn daily mail cnn dm dataset which contains news articles from cnn and articles from daily mail
the text in the dataset has tokens on average paired with multi sentence summaries
sentences or tokens on average which serve as the ground truth for our summarization task
the training validation and test sets include and data pairs respectively
evaluation metrics following existing works we use rouge for summarization performance evaluation
rouge measures the overlapping between the generated summary and the ground truth summary
rouge n and rouge l are the most commonly used rouge metrics in practice which stand for rouge n gram and longest common subsequence lcs
recall in the context of rouge means how much the generated summary covers the ground truth summary whereas precision measures how much of the generated
com taas taas summary was in fact included in the ground truth summary
in this study we report the score of and rouge l for every experiment for performance comparison
parameter setting our experiments are conducted on a machine with two geforce rtx ti gpus
due to the memory limitation the batch size for training and testing is set to
following the suggestion by we freeze parameters in the encoder and the token embedding while only fine tuning the decoder
benchmark methods to evaluate whether the topic aware based design is effective in text summarization we compare our taas model with the following methods which do not incorporate the semantic structure of the documents and can be grouped into rule based extractive and abstractive categories
is a simple rule based method that chooses the first three sentences from a document as its summary
summarunner is a two layer bi directional rnn based sequence model for extractive summarization
it formulates the summarization problem as a sequence sification problem
for each sentence a binary classifier is learned to decide if it is included
refersh formulates the extractive summarization problem as a ranking task among all sentences of an input document
it uses lstm to select sentences from the input documents
the following are abstractive summarization methods
drm introduces a neural network model with a novel intra attention that attends over the input and continuously generates output separately
the model reads the input quence with a bi directional lstm encoder and a single lstm decoder to generate the summary
pointer generator network pgn constructs a generator network for summarization which copies words from the source text to aid accurate reproduction of tion and retains the ability to produce novel words through the generator
this novel framework can be viewed as a balance between extractive and abstractive approach
is a unified framework that converts every language problem into a text to text format
it is pretrained with a large language corpus and the framework can be adjusted for different language tasks including summarization
bart employs the bidirectional encoder to enhance the sequence understanding and the left to right decoder to generate the summary
prophetnet predicts the next tokens simultaneously based on previous context tokens at each time step
this sign encourages the model to plan for the future generation process
pegasus introduces a new pre train objective to not only mask tokens but also mask some important sentences which enables the model to capture global information among their implementation at
com huggingface transformers master examples rule based extractive abstractive model refresh summarunner drm pgn bart our method taas industry sota prophetnet pegasus rouge l
































table performance comparison on cnn dm dataset regarding score of rouge
sentences and thus be able to generate candidate sentences using the surrounding sentence context
the main goal of this study is to leverage topic level semantics to help with the summarization task
thus our training is built on top of the sshleifer distilbart checkpoint released by huggingface for the bart model
taas uses a similar ture to bart a layer encoder with a bi directional transformer layer and a layer decoder with a uni directional transformer layer
we add one topic aware attention layer between encoder and decoder to incorporate the semantic structure
during our experiment we set in eq
to to only focus on the loss from the sequence modeling part while the parameters in ntm are fixed
note that could be tuned to balance the loss from both ntm and sequence modeling which we leave as future research
due to the limitation of our computing capacity we calculate the topic word distribution using samples within each batch
we use adam optimizer with learning rate of

note that the here are the hyperparameters of adam optimizer which are not related to the topic word distribution
we use a dropout rate of
across all layers

experiment results performance comparison table shows the performance parison between our taas model and aforementioned benchmark models on the cnn dm dataset
we separate prophetnet and gasus into the industry sota category given their top positions on the cnn dm leaderboard
note that we can not reproduce their performance using the hyperparameter settings from the original papers given our limited computing capability and thus report their scores using the same hyperparameter settings of our taas model
we show taas performance denoted using bold text and the improvement percentage over the second best performing models denoted as underlined text except for the industry sota els
we have the following observations
taas outperforms the naive rule based method
this is expected where only looking at the first sentences of an article is not able to capture all key information because different reporters have different ing styles
some prefer to summarize all important topics at the
co sshleifer distilbart figure a qualitative evaluation of taas and bart on tention and summary beginning while others like to use an inductive approach to leave the important information at the end
using for the latter case will make the summary inaccurate
taas has a superior performance over recent state of the art academic models both tractive and abstractive
since our model uses a similar architecture to bart we particularly make a comparison against bart and find that it improves the performance by

and regrading the measure of and rouge l respectively
this confirms that incorporating global semantic structures such as latent topics of text indeed generates better summarization
the improvements of all models over the simple rule based model in terms of the f scores are not amazingly large
this may due to the fact that the writing style of news articles could have a pattern that the first few sentences kind of summarize the whole article
more experiments should be conducted on other rization datasets to study the effects of our model on other types of documents
truth bob barker returned to host the price is right on wednesday
barker had retired as host in
generated summary bart prophetnet the price is right returned to hosting for the first time in eight years
despite being away from the show for most of the past years a television legend did nt seem to miss a beat
bob barker hosted the game show for years before stepping down in
bob barker hosted the tv game show for years before stepping down in
barker handled the first price guessing game of the show before turning hosting duties over to drew carey
despite being away from the show for most of the past eight years barker did nt seem to miss a beat
a tv legend returned to doing what he does best
contestants told to come on down on the april edition of the price is right encountered not host drew xarey
pegasus barker hosted the price is right for years
he stepped down in
taas bob barker returns to hosting the price is right for the first time in eight years
the year old tv legend stepped down from the show in after years on the show which he hosted for years
table example summaries generated by various models qualitative evaluation we conduct a qualitative evaluation to intuitively further demonstrate word attention weights are deed changed by our topic aware attention mechanism and our taas model can generate more novel sentences that are more lated to the document topics
as shown in figure we highlight yellow color the top five phases with the highest attention values using topic aware attention in taas and self attention in bart to illustrate the word level attention differences
we also provide the generated summary for comparison
important phases by the self attention focus on the leading part of articles which fails to capture the important information in the text
the generated result is also less meaningful as compared to that generated by taas
although summaries by both approaches have successfully covered the keywords in the news taas generates more clear and coherent results and especially it summarizes the key ideas correctly
since both models target an abstractive summarization task we further assess the extent to which models are able to perform rewriting by generating an abstractive summary
the result from bart is less informative which uses several original sentences from the news e

a whole sentence highlighted in purple in figure
taas not only captures the key information of the article well but also demonstrates a novel sentence structure
we also observe some duplicated information in the summary generated by taas
for example after years on the show conveys the same tion as which he hosted for years which indicates additional room for improvements in our future research
as an additional comparison we also present the summaries of the same article by other baseline models shown in table
discussion
effect of different number of topics taas achieves relatively superior performance over several lines demonstrating the effectiveness of incorporating the aware attention into summarization
like many deep learning els taas involves many hyperparameters to which the mance might be sensitive
among which the number of topics is a critical one that needs more exploration
to do so we vary the value of ranging from to to obtain f scores of rouge l
here we follow prior literature to only report f score of rouge l figure the impact of the number of topics on model formance
note that every reported score here is calculated on the test set
the result is shown in figure from which we have the ing observations
taas achieves the best performance when
this is reasonable given the fact that each batch during training has articles where the number of latent topics should not be too many or too few
the rouge l f scores for different s do nt vary too much which might be due to several reasons i the balancing ntm and encoder decoder sequence modeling part is set to as we focus on summarization each generated summary includes top attended words that are likely to be similar even varying given the nature of the dataset where it is unlikely to exhibit diverse topics for articles in a batch

effect of different document lengths to test whether taas performs equally well on different lengths of articles we separate the entire cnn dm dataset into three subsets based on the number of sentences which are denoted as cnn short sentences cnn dm medium sentences and cnn dm long sentences with and articles figure performance comparison score of rouge of taas and bart on different lengths of articles respectively
as shown in figure taas achieves better mance for longer articles up to
improvement of rouge l f score over bart as compared to
and
for short and medium articles
longer articles are likely to have more diverse topics which make the topic attention in the summarization model more salient
this further indicates that adding topic level mation can improve the model in the summarization task
conclusion in this work we study the abstractive text summarization problem by proposing a topic aware attention model to incorporate global semantic structures of text
in particular we combine neural topic modeling and encoder decoder like sequence to sequence model via topic attention for summarization
we conduct extensive iments on a real world dataset to compare our proposed approach with several cutting edge methods
the results demonstrate the perior performance over some well recognized models in academia and comparable performance to industry sota
we also shed light on how the model performance is affected by important parameters and the characteristics of textual data
although our current study in this paper incorporates topic word distribution into the framework the other important output from ntm i
e
document topic distribution is currently neglected which might be worth exploring in our future work
furthermore training our model with more powerful computing resources to improve the summarization performance and test the robustness of the model is always interesting to pursue
references jimmy lei ba jamie ryan kiros and geoffrey e hinton

layer tion
arxiv preprint

david m blei alp kucukelbir and jon d mcauliffe

variational inference a review for statisticians
journal of the american statistical association
chaitanya chemudugunta padhraic smyth and mark steyvers

combining concept hierarchies and statistical topic models
in proceedings of the acm conference on information and knowledge management

jacob devlin ming wei chang kenton lee and kristina toutanova

bert pre training of deep bidirectional transformers for language understanding
arxiv preprint

thomas l griffiths michael i jordan joshua b tenenbaum and david m blei

hierarchical topic models and the nested chinese restaurant process
in advances in neural information processing systems

xiaotao gu yuning mao jiawei han jialu liu you wu cong yu daniel finnie hongkun yu jiaqi zhai and nicholas zukoski

generating representative headlines for news stories
in proceedings of the web conference

kaiming he xiangyu zhang shaoqing ren and jian sun

deep residual learning for image recognition
in proceedings of the ieee conference on computer vision and pattern recognition

dan jurafsky and james h martin

speech and language processing
vol


diederik p kingma and max welling

auto encoding variational bayes
arxiv preprint

mike lewis yinhan liu naman goyal marjan ghazvininejad abdelrahman mohamed omer levy ves stoyanov and luke zettlemoyer

bart denoising sequence to sequence pre training for natural language generation translation and comprehension
arxiv preprint

wei li and andrew mccallum

pachinko allocation dag structured mixture models of topic correlations
in proceedings of the international conference on machine learning

chin yew lin

rouge a package for automatic evaluation of summaries
in text summarization branches out

yang liu and mirella lapata

text summarization with pretrained encoders
in proceedings of the conference on empirical methods in natural language processing and the international joint conference on natural language cessing emnlp ijcnlp

yuning mao liyuan liu qi zhu xiang ren and jiawei han

facet aware evaluation for extractive text summarization
arxiv
yu meng jiaxin huang guangyuan wang zihan wang chao zhang yu zhang and jiawei han

discriminative topic mining via category name guided text embedding
in proceedings of the web conference

yishu miao edward grefenstette and phil blunsom

discovering discrete latent topics with neural variational inference
arxiv preprint

yishu miao lei yu and phil blunsom

neural variational inference for text processing
in international conference on machine learning

ramesh nallapati feifei zhai and bowen zhou

summarunner a recurrent neural network based sequence model for extractive summarization of documents
arxiv preprint

shashi narayan shay b cohen and mirella lapata

ranking sentences for extractive summarization with reinforcement learning
in proceedings of the conference of the north american chapter of the association for computational linguistics human language technologies volume long papers

romain paulus caiming xiong and richard socher

a deep reinforced model for abstractive summarization
arxiv preprint

alec radford jeffrey wu rewon child david luan dario amodei and ilya sutskever

language models are unsupervised multitask learners
openai blog
colin raffel noam shazeer adam roberts katherine lee sharan narang michael matena yanqi zhou wei li and peter j liu

exploring the limits of transfer learning with a unified text to text transformer
journal of machine learning research
abigail see peter j liu and christopher d manning

get to the point summarization with pointer generator networks
arxiv preprint

akash srivastava and charles sutton

autoencoding variational inference for topic models
arxiv preprint

ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan n gomez ukasz kaiser and illia polosukhin

attention is all you need
in advances in neural information processing systems

yu yan weizhen qi yeyun gong dayiheng liu nan duan jiusheng chen ruofei zhang and ming zhou

prophetnet predicting future n gram for sequence to sequence pre training
arxiv preprint

liang yang fan wu junhua gu chuan wang xiaochun cao di jin and fang guo

graph attention topic modeling network
in proceedings of the web conference

yi yang and kunpeng zhang

sdtm a supervised bayesian deep topic model for text analytics
available at ssrn
jingqing zhang yao zhao mohammad saleh and peter j liu

pegasus pre training with extracted gap sentences for abstractive summarization
arxiv preprint

qingyu zhou nan yang furu wei shaohan huang ming zhou and tiejun zhao

neural document summarization by jointly learning to score and select sentences
in proceedings of the annual meeting of the association for computational linguistics volume long papers


