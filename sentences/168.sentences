indosum a new benchmark dataset for indonesian text summarization kemal kurniawan kata research team kata
ai jakarta indonesia
ai samuel louvan fondazione bruno kessler university of trento trento italy
eu r a m l c
s c v
v i x r a abstract automatic text summarization is generally sidered as a challenging task in the nlp community
one of the challenges is the publicly available and large dataset that is relatively rare and difcult to construct
the problem is even worse for low resource languages such as indonesian
in this paper we present indosum a new benchmark dataset for indonesian text summarization
the dataset sists of news articles and manually constructed summaries
notably the dataset is almost larger than the previous indonesian summarization dataset of the same domain
we evaluated various extractive summarization approaches and obtained encouraging results which demonstrate the usefulness of the dataset and provide baselines for future research
the code and the dataset are available online under permissive licenses
keywords extractive summarization dataset indonesian i
introduction the goal of text summarization task is to produce a summary from a set of documents
the summary should retain important information and be reasonably shorter than the original documents
when the set of documents contains only a single document the task is usually referred to as single document summarization
there are two kinds of summarization characterized by how the summary is produced extractive and abstractive
extractive summarization attempts to extract few tant sentences verbatim from the original document
in contrast abstractive summarization tries to produce an abstract which may contain sentences that do not exist in or are paraphrased from the original document
despite quite a few number of research on indonesian text summarization none of them were trained nor ated on a large publicly available dataset
also although rouge is the standard intrinsic evaluation metric for english text summarization for indonesian it does not seem so
previous works rarely state explicitly that their evaluation was performed with rouge
the lack of a benchmark dataset and the different evaluation metrics make comparing among indonesian text summarization research difcult
in this work we introduce indosum a new benchmark dataset for indonesian text summarization and evaluated several well known extractive single document rization methods on the dataset
the dataset consists of online news articles and has almost times more uments than the next largest one of the same domain
to encourage further research in this area we make our dataset publicly available
in short the contribution of this work is two fold indosum a large dataset for text summarization in indonesian that is compiled from online news articles and publicly available
evaluation of state of the art extractive tion methods on the dataset using rouge as the standard metric for text summarization
the state of the art result on the dataset although sive is still signicantly lower than the maximum possible rouge score
this result suggests that the dataset is ciently challenging to be used as evaluation benchmark for future research on indonesian text summarization
ii
related work fachrurrozi et al
proposed some scoring methods and used them with tf idf to rank and summarize news articles
another work used latent dirichlet allocation coupled with genetic algorithm to produce summaries for online news articles
simple methods like naive bayes has also been used for indonesian news summarization although for english naive bayes has been used almost two decades earlier
a more recent work employed a summarization algorithm called textteaser with some predened features for news articles as well
slamet et al
used tf idf to convert sentences into vectors and their similarities are then computed against another vector obtained from some keywords
they used these similarity scores to extract important sentences as the summary
unfortunately all these work do not seem to be evaluated using rouge despite being the standard metric for text summarization research
an example of indonesian text summarization research which used rouge is
they employed the best method on tac competition for news dataset and achieved scores that are close to that of mans
however their dataset consists of only articles which is very small and the dataset is not available publicly
an attempt to make a public summarization dataset has been done in
they compiled a chat dataset along with its summary which has both the extractive and abstractive versions
this work is a good step toward standardizing summarization research for indonesian
however to the ieee
personal use of this material is permitted
permission from ieee must be obtained for all other uses in any current or future media including reprinting republishing this material for advertising or promotional purposes creating new collective works for resale or redistribution to servers or lists or reuse of any copyrighted component or this work in other works
the nal version of this article is available at

ialp


best of our knowledge for news dataset there has not been a publicly available dataset let alone a standard
iii
methodology a
indosum a new benchmark dataset we used a dataset provided by an indonesian news aggregator and summarizer company
the dataset contains roughly k news articles
each article has the title category source e

cnn indonesia kumparan url to the original article and an abstractive summary which was created manually by a total of native speakers of indonesian
there are categories in total ment inspiration sport showbiz headline and tech
a sample article summary pair is shown in fig

note that k articles are actually quite small if we compare to english cnn dailymail dataset used in which has k articles
therefore we used fold validation to split the dataset into folds of training development and testing set
we preprocessed the dataset by tokenizing lowercasing removing punctuations and replacing digits with zeros
we used nltk and for sentence and word tokenization respectively
in our exploratory analysis we discovered that some articles have a very long text and some summaries have too many sentences
articles with a long text are mostly articles containing a list e

list of songs played in a concert list of award nominations and so on
since such a list is never included in the summary we truncated such articles so that the number of paragraphs are at most two standard deviations away from the mean
for each fold the mean and standard deviation were estimated from the training set
we discarded articles whose summary is too long since we do not want lengthy summaries anyway
the cutoff length is dened by the upper limit of the tukey s boxplot where for each fold the quartiles were estimated from the training set
after removing such articles we ended up with roughly k articles in total
the complete statistics of the corpus is shown in table i
since the gold summaries provided by shortir are abstractive we needed to label the sentences in the article for training the supervised extractive summarizers
we lowed nallapati et al
to make these labeled sentences called oracles hereinafter using their greedy algorithm
the idea is to maximize the rouge score between the labeled sentences and the abstractive gold summary
although the provided gold summaries are abstractive in this work we focused on extractive summarization because we think research on this area are more mature especially for indonesian and thus starting with extractive summarization is a logical rst step toward standardizing indonesian text summarization research
since there can be many valid summaries for a given article having only a single abstractive summary for an ticle is a limitation of our dataset which we acknowledge
nevertheless we feel that the existence of such dataset is a crucial step toward a fair benchmark for indonesian text summarization research
therefore we make the dataset publicly available for others to use
b
evaluation for evaluation we used rouge a standard metric for text summarization
we used the implementation vided by pythonrouge
following we report the score of and r l
intuitively and measure informativeness and r l measures uency
we report the score instead of just the recall score because although we extract a xed number of sentences as the summary the number of words are not limited
so reporting only recall benets models which extract long sentences
c
compared methods we compared several summarization methods which can be categorized into three groups unsupervised neural supervised and neural supervised methods
for the unsupervised methods we tested sumbasic which uses word frequency to rank sentences and selects top sentences as the mary
lsa which uses latent semantic analysis lsa to decompose the term by sentence matrix of a document and extracts sentences based on the result
we experimented with the two approaches proposed in and respectively
lexrank which constructs a graph representation of a document where nodes are sentences and edges represent similarity between two sentences and runs pagerank algorithm on that graph and extracts tences based on the resulting pagerank values
in the original implementation sentences shorter than a certain threshold are removed
our mentation does not do this removal to reduce the number of tunable hyperparameters
also it nally uses cross sentence informational subsumption csis during sentence selection stage but the paper does not explain it well
instead we used an imation to csis called cross sentence word overlap described in by the same authors
textrank which is very similar to lexrank but computes sentence similarity based on the number of common tokens
for the non neural supervised methods we compared bayes which represents each sentence as a feature vector and uses naive bayes to classify them
the original paper computes tf idf score on word tokens that are identied automatically using mutual information
we did not do this tion so our tf idf computation operates on word tokens
hmm which uses hidden markov model where states correspond to whether the sentence should be
com
io assume the number of paragraphs exhibits a gaussian distribution

com kata ai indosum
com tagucci pythonrouge sekuel dengan menikah berhenti mata mata menjadi rahasia sumber yang ini inggris demi terbaru terlibat dalam bocor james bond produksi lm ini agen suara
com cerita menurut dicintainya
perempuan yang bond berhenti menjadi agen rahasia karena jatuh cinta dan menikah dengan perempuan yang dicintai tutur seorang sumber yang dekat dengan produksi seperti dikutip laman pagesix
com
dalam lm tersebut bond diduga menikahi madeleine swann yang diperankan oleh lea seydoux
lea diketahui bermain sebagai gadis bond sekuel spectre pada silam
jika pernikahan merupakan sekuel on her majesty di terbunuh
di lm itu draco plot sekuel lm james bond bocor tak lama setelah daniel craig mengumumkan bakal kembali memerankan tokoh agen
cerita sekuel terbaru james bond bocor
menurut sumber yang terlibat dalam produksi lm ini agen rahasia berhenti menjadi mata mata inggris demi menikah dengan perempuan yang dicintainya
jika benar merupakan satu satunya sekuel yang bercerita pernikahan james bond sejak
sebelumnya sekuel on her majesty james bond menikahi tracy draco
namun lm itu draco terbunuh
bercerita menikahi tracy draco yang sekuel yang james bond james bond sejak
diperankan diana rigg
satu satunya figure
a sample article top and its abstractive summary bottom
underlined sentences are the extractive summary obtained by following the greedy algorithm in
table i corpus statistics
of articles avg of paras article avg of sents para avg of words sent avg of sents summ avg of words summ sent fold




train




dev test train test train test train dev test train fold dev


































fold dev




fold




fold dev




test














extracted
the original work uses qr sition for sentence selection but our implementation does not
we simply ranked the sentences by their scores and picked the top as the summary
used the stopword list provided in
hyperparameters were tuned to the development set of each fold optimizing for as it correlates best with human judgment
for neuralsum we tried several scenarios maxent which represents each sentence as a ture vector and leverages maximum entropy model to compute the probability of a sentence should be extracted
the original approach puts a prior distribution over the labels but we put the prior on the weights instead
our implementation still agrees with the original because we employed a bias feature which should be able to learn the prior label distribution
as for the neural supervised method we evaluated neuralsum using the original implementation by the authors
we modied their implementation slightly to allow for evaluating the model with rouge
note that all the methods are extractive
our implementation code for all the methods above is available online
as a baseline we used lead n which selects n leading sentences as the summary
for all methods we extracted sentences as the summary since it is the median number of sentences in the gold summaries that we found in our exploratory analysis
d
experiment setup some of these approaches optionally require puted term frequency tf or inverse document frequency idf table and a stopword list
we precomputed the tf and idf tables from indonesian wikipedia dump data and
com neuralsum
com kata ai indosum tuning the dropout rate while keeping other increasing the word embedding size from the default parameters xed to initializing the word embedding with fasttext pre trained embedding
scenario is necessary to determine whether any ment in scenario is due to the larger embedding size or the pre trained embedding
in scenario and we used the default hyperparameter setting from the authors implementation
in addition for every scenario we picked the model saved at an epoch that yields the best score on the development set
iv
results and discussion a
overall results table ii shows the test score of and rouge l of all the tested models described previously
the mean and standard deviation bracketed of the scores are computed over the folds
we put the score obtained by an oracle summarizer as oracle
its summaries are obtained by using the true labels
this oracle summarizer acts as the upper bound of an extractive summarizer on our dataset
as we can see in general every scenario of neuralsum consistently outperforms the other models signicantly
the best scenario is ralsum with word embedding size of although its rouge scores are still within one standard deviation table ii test score of and rouge l averaged over folds
unsupervised non neural supervised oracle sumbasic lsa lexrank textrank bayes hmm maxent neural supervised neuralsum neuralsum emb
size neuralsum fasttext r l







































































table iii test score of for the out of domain experiment
source dom
entertainment inspiration sport showbiz headline tech target dom
oracle lexrank neuralsum entertainment inspiration sport showbiz headline tech





















































of neuralsum with the default word embedding size
baseline performs really well and outperforms most all the other models which is not surprising and even consistent with other work that for news summarization lead n baseline is surprisingly hard to beat
slightly lower than are lexrank and bayes but their scores are still within one standard deviation of each other so their performance are on par
this result suggests that a non neural supervised summarizer is not better than an unsupervised one and thus if labeled data are available it might be best to opt for a neural summarizer right away
we also want to note that despite its high rouge every neuralsum scenario scores are still considerably it can be improved lower than oracle hinting that further
moreover initializing with fasttext pre trained embedding slightly lowers the scores although they are still within one standard deviation
this nding suggests that the effect of fasttext pre trained embedding is unclear for our case
b
out of domain results since indonesian is a low resource language collecting in domain dataset for any task including summarization can be difcult
therefore we experimented with out domain scenario to see if neuralsum can be used easily for a new use case for which the dataset is scarce or existent
concretely we trained the best neuralsum with word embedding size of on articles belonging to category and evaluated its performance on articles belonging to category for all categories and
as we have a total of categories we have domain pairs to experiment on
to reduce computational cost we used only the articles from the rst fold and did not tune any hyperparameters
we note that this decision might undermine the generalizability of conclusions drawn from these out of domain experiments
nonetheless we feel that the results can still be a useful guidance for future work
as comparisons we also evaluated oracle and the best unsupervised method lexrank
for lexrank we used the best hyperparameter that we found in the previous experiment for the rst fold
we only report the scores
table iii shows the result of this experiment
we see that almost all the results outperform the baseline which means that for out of domain cases neuralsum can summarize not just by selecting some leading sentences from the original text
almost all neuralsum results also outperform lexrank gesting that when there is no in domain training data training neuralsum on out of domain data may yield better performance than using an unsupervised model like lexrank
looking at the best results we observe that they all are the out of domain cases
in other words training on out of domain data is surprisingly better than on in domain data
for example for sport as the target domain the best model is trained on headline as the source domain
in fact using headline as the source domain yields the best result in out of target domains
we suspect that this phenomenon is because of the larity between the corpus of the two domain
specically training on headline yields the best result most of the time because news from any domain can be headlines
further investigation on this issue might leverage domain similarity metrics proposed in
next comparing the best neuralsum performance on each target domain to oracle we still see quite a large gap
this gap hints that neuralsum can still be improved further probably by lifting the limitations of our experiment setup e

tuning the hyperparameters for each domain pair
v
conclusion and future work we present indosum a new benchmark dataset for indonesian text summarization and evaluated state of art extractive summarization methods on the dataset
we tested unsupervised non neural supervised and neural supervised summarization methods
we used rouge as the evaluation metric because it is the standard intrinsic evaluation metric for text summarization evaluation
our results show that neural models outperform non neural ones and in absence of in domain corpus training on of domain one seems to yield better performance instead of using an unsupervised summarizer
also we found that the best performing model achieves rouge scores that are still signicantly lower than the maximum possible scores which suggests that is sufciently challenging for future work
the dataset which consists of k article summary pairs is publicly available
we hope that the dataset and the evaluation results can serve as a benchmark for future research on indonesian text summarization
the dataset future work in this area may focus on improving the summarizer performance by employing newer neural models such as summarunner or incorporating side information
since the gold summaries are abstractive abstractive summarization techniques such as attention based neural models models pointer networks or reinforcement learning based approach can also be interesting directions for future avenue
other tasks such as further investigation on the out of domain issue human evaluation or even extending the corpus to include more than one summary per article are worth exploring as well
references d
das and a
f
martins a survey on automatic text summarization literature survey for the language and statistics ii course at cmu vol
pp

c

lin rouge a package for automatic evaluation of summaries in text summarization branches out ings of the workshop vol

barcelona spain
a
najibullah indonesian text summarization based on nave bayes method in proceeding of the tional seminar and conference the golden triangle indonesia india tiongkok interrelations in religion ence culture and economic semarang indonesia

m
fachrurrozi n
yusliani and r
u
yoanita frequent term based text summarization for bahasa indonesia in proceedings of the international conference on innovations in engineering and technology bangkok thailand

silvia p
rukmana v
aprilia d
suhartono r
wongso and meiliana summarizing text for indonesian guage by using latent dirichlet allocation and genetic algorithm in proceeding of international conference on electrical engineering computer science and informatics eecsi yogyakarta indonesia p

c
aone m
e
okurowski and j
gorlinsky trainable scalable summarization using robust nlp and machine learning in proceedings of the international ence on computational linguistics volume
association for computational linguistics pp

d
gunawan a
pasaribu r
f
rahmat and r
budiarto automatic text summarization for indonesian language using textteaser iop conference series materials ence and engineering vol
no
p

c
slamet a
r
atmadja d
s
maylawati r
s
lestari w
darmalaksana and m
a
ramdhani automated text summarization for indonesian article using vector space model iop conference series materials science and engineering vol
p
jan

d
t
massandy and m
l
khodra guided tion for indonesian news articles in international conference of advanced informatics concept theory and application icaicta aug
pp

f
koto a publicly available indonesian corpora for tomatic abstractive and extractive chat summarization in proceedings of the tenth international conference on language resources and evaluation lrec
toroz slovenia european language resources association elra p

r
nallapati f
zhai and b
zhou summarunner a recurrent neural network based sequence model for tractive summarization of documents in proceedings of the thirty first aaai conference on articial intelligence february san francisco california usa
pp

j
cheng and m
lapata neural summarization by tracting sentences and words in proceedings of the annual meeting of the association for computational guistics
berlin germany association for computational linguistics aug
pp

s
bird e
loper and e
klein natural language cessing with python
oreilly media inc

a
nenkova and l
vanderwende the impact of frequency on summarization microsoft research redmond ington tech
rep
msr vol

l
vanderwende h
suzuki c
brockett and a
nenkova beyond sumbasic task focused summarization with tence simplication and lexical expansion information processing management vol
no
pp

a
see p
j
liu and c
d
manning get to the point summarization with pointer generator networks in ceedings of the annual meeting of the association for computational linguistics volume long papers
canada association for computational linguistics pp

r
paulus c
xiong and r
socher a deep reinforced model for abstractive summarization
may
y
gong and x
liu generic text summarization using relevance measure and latent semantic analysis in ceedings of the annual international acm sigir conference on research and development in information retrieval
acm pp

j
steinberger and k
jezek using latent semantic analysis in text summarization and summary evaluation in proc
pp

g
erkan and d
r
radev lexrank graph based lexical centrality as salience in text summarization journal of articial intelligence research vol
pp

d
r
radev h
jing and m
budzikowska based summarization of multiple documents sentence extraction utility based evaluation and user studies in proceedings of the naacl anlp workshop on tomatic summarization
association for computational linguistics pp

r
mihalcea and p
tarau textrank bringing order into text in proceedings of the conference on empirical methods in natural language processing
j
conroy and d
oleary text summarization via hidden markov model and pivoted qr matrix decomposition
m
osborne using maximum entropy for sentence traction in proceedings of the workshop on automatic summarization including duc
philadelphia association for computational linguistics jul

f
tala j
kamps k
e
muller and r
de m the impact of stemming on information retrieval in bahasa indonesia for symbolic studia logica an international journal logic slogica jan

c

lin and e
hovy automatic evaluation of summaries using n gram co occurrence statistics in proceedings of the conference of the north american chapter of the association for computational linguistics on human language technology volume
association for tational linguistics pp

p
bojanowski e
grave a
joulin and t
mikolov riching word vectors with subword information arxiv preprint

s
ruder and b
plank learning to select data for transfer learning with bayesian optimization in proceedings of the conference on empirical methods in natural language processing
copenhagen denmark association for computational linguistics jul
pp

s
narayan n
papasarantopoulos m
lapata and s
b
cohen neural extractive summarization with side mation corr vol


a
m
rush s
chopra and j
weston a neural attention model for abstractive sentence summarization in ings of the conference on empirical methods in ural language processing
lisbon portugal association for computational linguistics pp

r
nallapati b
zhou c
dos santos c
gulcehre and b
xiang abstractive text summarization using sequence to sequence rnns and beyond in proceedings of the signll conference on computational natural language learning
berlin germany signll

