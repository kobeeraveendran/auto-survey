indosum a new benchmark dataset for indonesian text summarization kemal kurniawan kata research team

jakarta indonesia samuel louvan
fondazione bruno kessler university of trento
trento italy r a m l c
s
c
v
v
i x r a abstract automatic text summarization is generally sidered as a challenging task in the nlp community
one of
the challenges is the publicly available and large dataset that is relatively rare and difcult to construct
the problem is even worse for low resource languages such as indonesian
in this paper we present indosum a new benchmark dataset for indonesian text summarization
the dataset sists of news articles and manually constructed summaries

notably the dataset is almost larger than the previous indonesian summarization dataset of the same domain

we evaluated various extractive summarization approaches and obtained encouraging results which demonstrate the usefulness of the dataset and provide baselines for future research
the code and the dataset are available online under permissive licenses
keywords extractive summarization dataset indonesian introduction the goal of text summarization task is to produce a summary from a set of documents
the summary should retain important information and be reasonably shorter than the original documents
when the set of documents contains only a single document the task is usually referred to as single document summarization

there are two kinds of summarization characterized by how the summary is produced extractive and abstractive

extractive summarization attempts to extract few
tant sentences verbatim from the original document
in contrast abstractive summarization tries to produce an abstract which may contain sentences that do not exist in or are paraphrased from the original document

despite quite a few number of research on indonesian text summarization none of them were trained nor ated on a large publicly available dataset
also although
rouge
is the standard intrinsic evaluation metric for english text summarization for indonesian it does not seem so
previous works rarely state explicitly that their evaluation was performed with rouge
the lack of a benchmark dataset and the different evaluation metrics make comparing among indonesian text summarization research difcult

in this work we introduce indosum a new benchmark dataset for indonesian text summarization and evaluated several well known extractive single document rization methods on the dataset
the dataset consists of online news articles and has almost times more uments than the next largest one of the same domain
to encourage further research in this area we make our dataset publicly available
in short the contribution of this work is two fold indosum a large dataset for text summarization in indonesian that is compiled from online news articles and publicly available
evaluation of state of the art extractive
tion methods on the dataset using rouge as the standard metric for text summarization
the state of the art result on the dataset although sive is still signicantly lower than the maximum possible
rouge score
this result suggests that the dataset is ciently challenging to be used as evaluation benchmark for future research on indonesian text summarization
ii
related work fachrurrozi et al
proposed some scoring methods and used them with tf idf to rank and summarize news articles
another work used latent dirichlet allocation coupled with genetic algorithm to produce summaries for online news articles
simple methods like naive bayes has also been used for indonesian news summarization although for english naive bayes has been used almost two decades earlier
a more recent work employed a summarization algorithm called textteaser with some predened features for news articles as well
slamet et
al
used tf idf to convert sentences into vectors and their similarities are then computed against another vector obtained from some keywords
they used these similarity scores to extract important sentences as the summary

unfortunately all these work do not seem to be evaluated using rouge despite being the standard metric for text summarization research
an example of indonesian text summarization research which used rouge is
they employed the best method on tac competition for news dataset and achieved scores that are close to that of mans
however their dataset consists of only articles which is very small and the dataset is not available publicly
an attempt to make a public summarization dataset has been done in
they compiled a chat dataset along with its summary which has both the extractive and abstractive versions
this work is a good step toward standardizing summarization research for indonesian
however to the ieee
personal use of this material is permitted
permission from ieee must be obtained for all other uses in any current or future media including reprinting republishing this material for advertising or promotional purposes creating new collective works for resale or redistribution to servers or lists or reuse of any copyrighted component or this work in other works
the nal version of this article is available at
best of our knowledge for news dataset there has not been a publicly available dataset let alone a standard
iii
methodology indosum
a new benchmark dataset we used a dataset provided by an indonesian news aggregator and summarizer company
the dataset contains roughly k news articles
each article has the title category source cnn indonesia kumparan url to the original article and an abstractive summary which was created manually by a total of native speakers of indonesian
there are categories in total
ment inspiration sport showbiz headline and tech
a sample article summary pair is shown in fig

note that k articles are actually quite small if we compare to english cnn dailymail dataset used in which has k articles
therefore we used fold validation to split the dataset into folds of training development and testing set
we preprocessed the dataset by tokenizing lowercasing removing punctuations and replacing digits with zeros
we used nltk and
for sentence and word tokenization respectively

in our exploratory analysis we discovered that some articles have a very long text and some summaries have too many sentences
articles with a long text are mostly articles containing a list list of songs played in a concert list of award nominations and so on
since such a list is never included in the summary we truncated such articles so that the number of paragraphs are at most two standard deviations away from the for each fold the mean and standard deviation were estimated from the training set
we discarded articles whose summary is too long since we do not want lengthy summaries anyway
the cutoff length is dened by the upper limit of the tukey s boxplot where for each fold the quartiles were estimated from the training set
after removing such articles we ended up with roughly k articles in total
the complete statistics of the corpus is shown in table since the gold summaries provided by shortir are abstractive we needed to label the sentences in the article for training the supervised extractive summarizers
we lowed nallapati al
to make these labeled sentences called oracles hereinafter using their greedy algorithm

the idea is to maximize the rouge score between the labeled sentences and the abstractive gold summary

although the provided gold summaries are abstractive in this work we focused on extractive summarization because we think research on this area are more mature especially for indonesian and thus starting with extractive summarization is a logical rst step toward standardizing indonesian text summarization research
since there can be many valid summaries for a given article having only a single abstractive summary for an ticle is a limitation of our dataset which we acknowledge

nevertheless we feel that the existence of such dataset is a crucial step toward a fair benchmark for indonesian text summarization research
therefore we make the dataset publicly available for others to evaluation for evaluation we used rouge
a standard metric for text summarization
we used the implementation vided by following we report the score of and r intuitively and measure informativeness and r l measures uency

we report the score instead of
just the recall score
because although we extract a xed number of sentences as the summary the number of words are not limited
so reporting only recall benets models which extract long sentences
compared methods we compared several summarization methods which can be categorized into three groups unsupervised neural supervised and neural supervised methods
for the unsupervised methods we tested sumbasic which uses word frequency to rank sentences and selects top sentences as the mary
lsa which uses latent semantic analysis lsa to decompose the term by sentence matrix of a document and extracts sentences based on the result

we experimented with the two approaches proposed in and respectively
lexrank which constructs a graph representation of a document where nodes are sentences and edges represent similarity between two sentences and runs pagerank algorithm on that graph and extracts tences based on the resulting pagerank values

in the original implementation sentences shorter than a certain threshold are removed
our mentation does not do this removal to reduce the number of tunable hyperparameters
also it nally uses cross sentence informational subsumption
csis during sentence selection stage but the paper does not explain it well
instead we used an imation to csis called cross sentence word overlap described in by the same authors
textrank which is very similar to lexrank but computes sentence similarity based on the number of common tokens


for the non neural supervised methods we compared bayes which represents each sentence as a feature vector and uses naive bayes to classify them

the original paper computes tf idf score on word tokens that are identied automatically using mutual information
we did not do this tion so our tf idf computation operates on word tokens
hmm which uses hidden markov model where states correspond to whether the sentence should be
assume the number of paragraphs exhibits a gaussian distribution
sekuel dengan menikah berhenti mata mata menjadi rahasia sumber yang ini inggris demi terbaru terlibat dalam bocor
james bond produksi lm ini agen cerita
menurut dicintainya
perempuan yang bond berhenti menjadi agen rahasia karena jatuh cinta dan menikah dengan perempuan yang dicintai tutur seorang sumber yang dekat dengan produksi seperti dikutip laman

dalam lm tersebut bond diduga menikahi madeleine swann yang diperankan oleh lea seydoux

lea diketahui bermain sebagai gadis bond sekuel spectre pada silam
jika pernikahan merupakan sekuel on her majesty di terbunuh
di lm itu draco
plot sekuel lm james bond bocor tak lama setelah daniel craig mengumumkan bakal kembali memerankan tokoh agen

cerita sekuel terbaru james bond bocor

menurut sumber yang terlibat dalam produksi lm ini agen rahasia berhenti menjadi mata mata inggris demi menikah dengan perempuan yang dicintainya

jika benar merupakan satu satunya sekuel yang bercerita pernikahan james bond sejak

sebelumnya sekuel on her majesty james bond menikahi tracy draco

namun lm itu draco terbunuh
bercerita menikahi tracy draco yang sekuel yang
james bond james bond sejak
diperankan diana rigg
satu satunya figure
a sample article top and its abstractive summary bottom
underlined sentences are the extractive summary obtained by following the greedy algorithm in
table i corpus statistics
of articles avg of paras article avg of sents para avg
of words sent avg of sents summ avg
of words summ sent fold train dev test train test train test train dev test train fold dev fold
dev fold
fold dev
test extracted
the original work uses qr sition for sentence selection but our implementation does not
we simply ranked the sentences by their scores and picked the top as the summary
used the stopword list provided in
hyperparameters were tuned to the development set of each fold optimizing for as it correlates best with human judgment

for neuralsum we tried several scenarios maxent which represents each sentence as a ture vector and leverages maximum entropy model to compute the probability of a sentence should be extracted

the original approach puts a prior distribution over the labels but we put the prior on the weights instead
our implementation still agrees with the original because we employed a bias feature which should be able to learn the prior label distribution

as for the neural supervised method we evaluated neuralsum
using the original implementation by the we modied their implementation slightly to allow for evaluating the model with rouge
note that all the methods are extractive
our implementation code for all the methods above is available as a baseline we used lead n which selects n leading sentences as the summary
for all methods we extracted sentences as the summary since it is the median number of sentences in the gold summaries that we found in our exploratory analysis
experiment setup some of these approaches optionally require puted term frequency tf or inverse document frequency idf table and a stopword list
we precomputed the tf and idf tables from indonesian wikipedia dump data and
tuning the dropout rate while keeping other increasing the word embedding size from the default parameters xed to initializing the word embedding with fasttext pre trained embedding
scenario is necessary to determine whether any ment in scenario is due to the larger embedding size or the pre trained embedding
in scenario and we used the default hyperparameter setting from the authors implementation
in addition for every scenario we picked the model saved at an epoch that yields the best score on the development set
iv
results and discussion overall results table ii shows the test score of and rouge l of all the tested models described previously
the mean and standard deviation bracketed of the scores are computed over the folds
we put the score obtained by an oracle summarizer as oracle
its summaries are obtained by using the true labels
this oracle summarizer acts as the upper bound of an extractive summarizer on our dataset
as we can see in general every scenario of neuralsum consistently outperforms the other models signicantly
the best scenario is
ralsum with word embedding size of although its rouge scores are still within one standard deviation table ii test score of and rouge l averaged over folds

unsupervised non neural supervised oracle
sumbasic

lsa
lexrank textrank
bayes
hmm maxent
neural supervised neuralsum neuralsum emb
size neuralsum fasttext r l









table iii test score of for the out of domain experiment
source dom

entertainment inspiration sport showbiz headline tech target dom
oracle
lexrank neuralsum entertainment inspiration sport showbiz headline tech



of neuralsum with the default word embedding size

baseline performs really well and outperforms
most all the other models which is not surprising and even consistent with other work that for news summarization lead n baseline is surprisingly hard to beat
slightly lower than are lexrank and bayes but their scores are still within one standard deviation of each other
so their performance are on par
this result suggests that a non neural supervised summarizer is not better than an unsupervised one and thus if labeled data are available it might be best to opt for a neural summarizer right away
we also want to note that despite its high rouge every neuralsum scenario scores are still considerably
it can be improved lower than oracle hinting that further
moreover initializing with fasttext pre trained embedding slightly lowers the scores although they are still within one standard deviation
this nding suggests that the effect of fasttext pre trained embedding is unclear for our case

out of domain results since indonesian is a low resource language collecting in domain dataset for any task including summarization can be difcult
therefore we experimented with out domain scenario to see if neuralsum can be used easily for a new use case for which the dataset is scarce or existent
concretely we trained the best neuralsum
with word embedding size of on articles belonging to category and evaluated its performance on articles belonging to category for all categories and

as we have a total of categories we have domain pairs to experiment on
to reduce computational cost we used only the articles from the rst fold and did not tune any hyperparameters
we note that this decision might undermine the generalizability of conclusions drawn from these out of domain experiments
nonetheless we feel that the results can still be a useful guidance for future work
as comparisons we also evaluated oracle and the best unsupervised method lexrank

for lexrank we used the best hyperparameter that we found in the previous experiment for the rst fold
we only report the scores
table iii shows the result of this experiment

we see that almost all the results outperform the baseline which means that for out of domain cases neuralsum can summarize not just by selecting some leading sentences from the original text
almost all neuralsum results also outperform lexrank gesting that when there is no in domain training data training neuralsum on out of domain data may yield better performance than using an unsupervised model like lexrank
looking at the best results we observe that
they all are the out of domain cases
in other words training on out of domain data is surprisingly better than on in domain data
for example for sport as the target domain the best model is trained on headline as the source domain
in fact using headline as the source domain yields the best result in out of target domains
we suspect that this phenomenon is because of the larity between the corpus of the two domain
specically training on headline yields the best result most of the time because news from any domain can be headlines

further investigation on this issue might leverage domain similarity metrics proposed in
next comparing the best neuralsum performance on each target domain to oracle we still see quite a large gap
this gap hints that neuralsum can still be improved further probably by lifting the limitations of our experiment setup tuning the hyperparameters for each domain pair
conclusion and future work
we present indosum a new benchmark dataset for indonesian text summarization and evaluated state of art extractive summarization methods on the dataset
we tested unsupervised non neural supervised and neural supervised summarization methods
we used rouge as the evaluation metric because it is the standard intrinsic evaluation metric for text summarization evaluation
our results show that neural models outperform non neural ones and in absence of in domain corpus training on of domain one seems to yield better performance instead of using an unsupervised summarizer
also we found that the best performing model achieves rouge scores that are still signicantly lower than the maximum possible scores which suggests that is sufciently challenging for future work
the dataset which consists of k article summary pairs is publicly available
we hope that the dataset and the evaluation results can serve as a benchmark for future research on indonesian text summarization
the dataset future work in this area may focus on improving the summarizer performance by employing newer neural models such as summarunner or incorporating side information
since the gold summaries are abstractive abstractive summarization techniques such as attention based neural models models pointer networks or reinforcement learning based approach can also be interesting directions for future avenue
other tasks such as further investigation on the out of domain issue human evaluation or even extending the corpus to include more than one summary per article are worth exploring as well
references
das and martins a survey on automatic text summarization literature survey for the language and statistics ii course at cmu vol
pp


lin rouge a package for automatic evaluation of summaries in text summarization branches out ings of the workshop vol

barcelona spain
najibullah indonesian text summarization based on nave bayes method in proceeding of the tional seminar and conference
the golden triangle indonesia india tiongkok
interrelations in religion ence culture and economic semarang indonesia
fachrurrozi yusliani and yoanita frequent term based text summarization for bahasa indonesia in proceedings of the international conference on innovations in engineering and technology bangkok thailand

silvia rukmana aprilia suhartono wongso and meiliana summarizing text for indonesian guage by using latent dirichlet allocation and genetic algorithm in proceeding of international conference on electrical engineering computer science and informatics
eecsi yogyakarta indonesia
aone okurowski and gorlinsky trainable scalable summarization using robust nlp and machine learning in proceedings of the international ence on computational linguistics volume
association for computational linguistics pp

gunawan pasaribu rahmat and budiarto automatic text summarization for indonesian language
using textteaser iop conference series materials ence and engineering vol
no
slamet atmadja maylawati lestari darmalaksana and ramdhani automated text summarization for indonesian article using vector space model iop conference series materials science and engineering vol

massandy and khodra guided tion for indonesian news articles in international conference of advanced informatics concept theory and application icaicta pp

koto a publicly available indonesian corpora for tomatic abstractive and extractive chat summarization in proceedings of the tenth international conference on language resources and evaluation lrec
toroz slovenia european language resources association elra

nallapati zhai and zhou summarunner a recurrent neural network based sequence model for
tractive summarization of documents in proceedings of the thirty first aaai conference on articial intelligence february san francisco california usa pp


cheng and lapata neural summarization by
tracting sentences and words in proceedings of the
annual meeting of the association for computational guistics
berlin germany association for computational
linguistics pp

bird loper and klein natural language cessing with python
oreilly media

nenkova and vanderwende the impact of frequency on summarization microsoft research redmond ington tech
msr vol


vanderwende suzuki brockett and nenkova beyond sumbasic task focused summarization with tence simplication and lexical expansion information processing management vol
no pp

see liu and manning get to the point summarization with pointer generator networks in ceedings of the annual meeting of the association for computational linguistics volume long papers

canada association for computational linguistics pp


paulus xiong and socher a deep reinforced model for abstractive summarization
may

gong and liu generic text summarization using relevance measure and latent semantic analysis in ceedings of the annual international acm sigir conference on research and development in information retrieval
acm pp


steinberger and jezek using latent semantic analysis in text summarization and summary evaluation in proc
pp


erkan and radev lexrank graph based lexical centrality as salience in text summarization journal of articial intelligence research vol
pp

radev jing and budzikowska based summarization of multiple documents sentence extraction utility based evaluation and user studies in proceedings of the naacl anlp workshop on tomatic summarization
association for computational linguistics pp


mihalcea and tarau textrank
bringing order into text in proceedings of the conference on empirical methods in natural language processing
conroy and oleary text summarization via hidden markov model and pivoted qr matrix decomposition

osborne using maximum entropy for sentence traction in proceedings of the workshop on automatic summarization including duc

philadelphia association for computational linguistics

tala kamps muller and de m the impact of stemming on information retrieval in bahasa indonesia for symbolic
studia logica an international journal logic slogica
lin and hovy automatic evaluation of summaries using n gram co occurrence statistics in proceedings of the conference of the north american chapter of the association for computational linguistics on human language technology volume
association for tational linguistics pp


bojanowski grave joulin and mikolov riching word vectors with subword information arxiv preprint

ruder and plank learning to select data for transfer learning with bayesian optimization in proceedings of the conference on empirical methods in natural language processing
copenhagen denmark association for computational linguistics pp

narayan papasarantopoulos lapata and cohen neural extractive summarization with side mation corr vol

rush chopra and weston a neural attention model for abstractive sentence summarization in ings of the conference on empirical methods in
ural language processing
lisbon portugal association for computational linguistics pp

nallapati zhou dos santos gulcehre and xiang abstractive text summarization using sequence to sequence rnns and beyond in proceedings of the signll conference on computational natural language learning
berlin germany signll

