a large scale indonesian dataset for text summarization fajri koto jey han lau timothy baldwin school of computing and information systems the university of melbourne
unimelb
edu
au jeyhan

com
edu
au v o n l c
s c v
v i x r a abstract in this paper we introduce a large scale donesian summarization dataset
we vest articles from
com an online news portal and obtain document summary pairs
we leverage pre trained guage models to develop benchmark tive and abstractive summarization methods over the dataset with multilingual and lingual bert based models
we include a thorough error analysis by examining generated summaries that have low rouge scores and expose both issues with rouge self as well as with extractive and abstractive summarization models
introduction despite having the fourth largest speaker tion in the world with million native indonesian is under represented in nlp
one son is the scarcity of large datasets for different tasks such as parsing text classication and marization
in this paper we attempt to bridge this gap by introducing a large scale indonesian corpus for text summarization
neural models have driven remarkable progress in summarization in recent years particularly for abstractive summarization
one of the rst studies was rush et al
where the authors proposed an encoder decoder model with attention to erate headlines for english gigaword documents graff et al

subsequent studies introduced pointer networks nallapati et al
see et al
summarization with content selection hsu et al
gehrmann et al
graph based attentional models tan et al
and deep inforcement learning paulus et al

more recently we have seen the widespread adoption
visualcapitalist
most spoken
of pre trained neural language models for rization e

bert liu and lapata bart lewis et al
and pegasus zhang et al

progress in summarization research has been driven by the availability of large scale glish datasets including k cnn daily mail document summary pairs hermann et al
and nyt articles sandhaus which have been widely used in abstractive tion research see et al
gehrmann et al
paulus et al
lewis et al
zhang et al

news articles are a natural date for summarization datasets as they tend to be well structured and are available in large volumes
more recently english summarization datasets in other avours domains have been developed e

xsum has k documents with highly abstractive summaries narayan et al
bigpatent is a summarization dataset for the legal domain sharma et al
reddit tifu is sourced from social media kim et al
and cohan et al
proposed using scientic publications from arxiv and pubmed for abstract summarization
this paper introduces the rst large scale marization dataset for indonesian sourced from the
com online news portal over a year period
it covers various topics and events that happened primarily in indonesia from ber to october
below we present tails of the dataset propose benchmark extractive and abstractive summarization methods that age both multilingual and monolingual pre trained bert models
we further conduct error analysis to better understand the limitations of current models over the dataset as part of which we reveal not just modelling issues but also problems with rouge
to summarize our contributions are we release a large scale indonesian summarization pus with over k documents an order of figure example articles and summaries from
to the left is the original document and summary and to the right is an english translation for illustrative purposes
we additionally highlight sentences that the summary is based on noting that such highlighting is not available in the dataset
nitude larger than the current largest indonesian summarization dataset and one of the largest english summarization datasets in we present statistics to show that the summaries in the dataset are reasonably abstractive and vide two test partitions a standard test set and an extremely abstractive test set we develop benchmark extractive and abstractive tion models based on pre trained bert models and we conduct error analysis on the basis of which we share insights to drive future research on indonesian text summarization
data construction
com is an online indonesian news tal which has been running since august and provides news across a wide range of ics including politics business sport ogy health and entertainment
according to the alexa ranking of websites at the time of
com is ranked in indonesia and globally
the website produces daily articles along data can be accessed at

alexa
com topsites with a short description for its rss feed
the summary is encapsulated in the javascript able window
kmklabs
article and the key shortdescription while the article is in the main body of the associated html page
we vest this data over a year window from tober to october to create a scale summarization corpus comprising document summary pairs
in terms of ing we remove formatting and html entities e

quot and lowercase all words and segment sentences based on simple punctuation heuristics
we provide example articles and summaries with english translations for expository purposes ing that translations are not part of the dataset in figure
as a preliminary analysis of the document summary pairs over the year period we binned the pairs into chronologically ordered groups taining of the data each and computed the proportion of novel n grams order to in the summary relative to the source document
based on the results in figure we can see that the portion of novel n grams drops over time implying that the summaries of more recent articles are less dokumen
com jakarta gara gara berusaha kabur dimintamenunjukkan barang hasil curian rosihan bin usman tersangkapencurian tas wisatawan asing baru baru ini tersungkur ditembakaparat kepolisian resor denpasar barat bali
sebelumnya rosihanditangkap massa setelah mencuri tas nicholas dreyden wisatawanasing asal inggris
tas yang berisi dokumen keimigrasian dan suratpenting itu diambil rosihan setelah mengelabui korban
kalimat dengan kata setelahnya tidak seorang pencuri tas wisatawan asing ditembak polisi
ia berusahakabur diminta menunjukan hasil curian
karena itu polisimenembaknya

com jakarta because of trying to escape when asked toshow stolen goods rosihan bin usman a suspect of the theft of aforeign tourist bag recently fell down shot by the west denpasarresort police bali
previously rosihan was arrested by the mob afterstealing the bag of nicholas dreyden a foreign tourist from england
the bag containing immigration documents and important letters wastaken by rosihan after tricking the victim
sentences with words are abbreviated from a foreign tourist bag thief was shot by police
he tried to run awaywhen asked to show the loot
because of this the police shot him
dokumen
com jakarta organisasi negara negara pengeksporminyak opec mengakui mengalami kesulitan untuk harga minyak dunia
itu lantaran harga minyak terusmelonjak sepanjang tahun ini
hingga kini harga minyak mentahdunia masih mencapai tingkat tertinggi sejak pecah teluksepuluh tahun silam
kalimat dengan kata tidak sebelumnya opec telah produksi minyaksebanyak tiga kali dalam enam bulan terakhir
pertama april hinggajuni dengan kenaikan mencapai ribu barel dan terakhir september ini opec kembali menaikkan produksi sebesar ribubarel per hari
kalimat kata setelahnya tidak opec kesulitan menjaga stabilitas harga minyak dunia lantaran hargaminyak dipasaran terus melonjak
padahal opec telah tiga kalimenaikkan produksi dalam enam bulan terakhir

com jakarta the organization of petroleum exportingcountries opec has admitted that it is having difficulty stability of world oil prices
that because oil prices continue tosoar this year
until now world crude oil prices have still reached level since the gulf war broke out ten years ago
sentences with words are abbreviated from fact opec had previously revised oil production three times in thelast six months
first april to june with an increase of thousandbarrels and last this september opec has again increased productionby thousand barrels per day
sentences with words are abbreviated from opec is struggling to maintain the stability of world oil pricesbecause oil prices on the market continue to soar
in fact opec hasraised production three times in the past six months
variant doc train dev test of novel n grams canonical xtreme







table statistics for the canonical and xtreme ants of our data
the percentage of novel n grams is based on the combined dev and test set
figure proportion of novel n grams over time in the summaries
abstractive
for this reason we decide to use the earlier articles october to jan as the development and test documents to create a more challenging dataset
this setup also means there is less topic overlap between training and ment test documents allowing us to assess whether the summarization models are able to summarize unseen topics
for the training development and test partitions we use a splitting ratio of
in addition to this canonical partitioning of the data we provide an xtreme variant inspired by xsum narayan et al
whereby we discard development and test document summary pairs where the summary has fewer than novel grams leaving the training data unchanged creating a smaller more challenging data conguration
summary statistics for the canonical and xtreme variants are given in table
we next present a comparison of canonical partitioning and indosum the current largest indonesian summarization dataset as tailed in section kurniawan and louvan in terms of number of documents in table
is approximately times larger than indosum the current largest indonesian rization dataset although articles and summaries in are slightly shorter
to understand the abstractiveness of the maries in the two datasets in table we present rouge scores for the simple baseline of using the rst n sentences as an extractive summary lead n and the percentage of novel n grams in the summary
we use and for indosum and respectively based on the average number of sentences in the summaries table
we see that has consistently lower rouge scores and rl for n it also has a substantially higher proportion of novel n grams
this suggests that the summaries in are more abstractive than indosum
to create a ground truth for extractive rization we follow cheng and lapata and nallapati et al
in greedily selecting the subset of sentences in the article that maximizes the rouge score based on the reference summary
as a result each sentence in the article has a nary label to indicate whether they should be cluded as part of an extractive summary
extractive summaries created this way will be referred to as oracle to denote the upper bound performance of an extractive summarization system
summarization models we follow liu and lapata in building tive and abstractive summarization models using bert as an encoder to produce contextual sentations for the word tokens
the architecture of both models is presented in figure
we tokenize words with wordpiece and append prex and sep sufx tokens to each sentence
to ther distinguish the sentences we add even odd ment embeddings ta tb based on the order of the sentence to the word embeddings
for instance for a document with sentences the segment embeddings are ta tb ta tb
tion embeddings p are also used to denote the position of each token
the wordpiece segment and position embeddings are summed together and provided as input to bert
bert produces a series of contextual tations for the word tokens which we feed into a second transformer encoder decoder for the tractive abstractive summarization model
we tail the architecture of these two models in tions
and

note that this second transformer is initialized with random parameters i
e
it is not pre trained
for the pre trained bert encoder we use statistics are based on the entire dataset encompassing the training dev and test data
jul aug apr aug oct gram dataset doc article summary train dev test vocab vocab indosum



k k



k k table a comparison of indosum and
and denote the average number of words and sentences respectively
dataset lead n rl of novel n grams indosum













table abstractiveness of the summaries in indosum and
tilingual bert mbert and our own indobert koto et al
to appear
indobert is a base model we trained ourselves using sian documents from three sources sian wikipedia m words news articles m words from tempo tala et al
and and the indonesian web corpus m words medved and suchomel
in total the training data has m words
we implement indobert using the huggingface and follow the default conguration of bert base uncased hidden size hidden layers attention heads and feed forward
we train indobert with pieces vocabulary for million steps

extractive model after the document is processed by bert we have a contextualized embedding for every word token in the document
to learn inter sentential relationships we use the cls embeddings

xsm to represent the sentences to which we add a sentence level positional ding p and feed them to a transformer encoder figure
an mlp layer with sigmoid activation is applied to the output of the transformer encoder to predict whether a sentence should be extracted i
e
ys
we train the model with binary pre trained mbert is sourced from github
com google research bert

com
tempo
co we use only the articles from the training partition

figure architecture of the extractive and abstractive summarization models
cross entropy and update all model parameters including bert during training
note that the rameters in the transformer encoder and the mlp layer are initialized randomly and learned from scratch
the transformer encoder is congured as lows layers hidden size feed forward and heads
in terms of training parameters we train using the adam optimizer with learning rate lr
step
where warmup
we train for steps on gb gpus and form evaluation on the development set every steps
at test time we select sentences for the tractive summary according to two conditions the summary must consist of at least two sentences and at least words
these values were set based on the average number of sentences and the minimum number of words in a summary
we also apply trigram blocking to reduce redundancy paulus et al

henceforth we refer to this model as bertext
mbert modeltransformer layer











pre trained








transformer decodersoftmax layergeneration of yabstractive modellearned from






abstractive model similar to the extractive model we have a second transformer to process the contextualized dings from bert
in this case we use a transformer decoder instead i
e
an attention mask is used to prevent the decoder from attending to future time steps as we are learning to generate an abstractive summary
but unlike the extractive model we use the bert embeddings for all tokens as input to the transformer decoder as we do not need sentence representations
we add to these bert dings a second positional encoding before feeding them to the transformer decoder figure
the transformer decoder is initialized with random rameters i
e
no pre training
the transformer decoder is congured as lows layers hidden size feed forward and heads
following liu and pata we use a different learning rate for bert and the decoder when training the model
step
and

step
for bert and the transformer decoder respectively
both networks are trained with the adam optimizer for steps on gb gpus and ated every steps
for summary generation we use beam width trigram blocking and a length penalty wu et al
to generate at least two sentences and at least words similar to the extractive model
henceforth the abstractive model will be ferred to as bertabs
we additionally ment with a third variant bertextabs where we use the weights of the ne tuned bert in bertext for the encoder instead of off the shelf bert weights
experiment and results we use three rouge lin scores as evaluation metrics unigram overlap gram overlap and rl longest common quence overlap
in addition we also provide bertscore as has recently been used for machine translation evaluation zhang et al

we use the development set to select the best checkpoint during training and report the evaluation scores for the canonical and xtreme test sets in table
for both test sets the tion models are trained using the same training
com tiiiger set but they are tuned with a different ment set see section for details
in addition to the bert models we also include two generator models see et al
the base model ptgen and the model with coverage penalty
we rst look at the baseline lead n and cle results
is the best lead n baseline for
this is unsurprising given that in table the average summary length was tences
we also notice there is a substantial gap between oracle and points for and points for bertscore depending on the test set
this suggests that the baseline of using the rst few sentences as an extractive summary is ineffective
comparing the performance between the canonical and xtreme test sets we see a stantial drop in performance for both lead n and oracle highlighting the difculty of the xtreme test set due to its increased abstractiveness
for the pointer generator models we see little improvement when including the coverage anism vs
ptgen implying that there is minimal repetition in the output of ptgen
we suspect this is due to the summaries being relatively short sentences with words on average
a similar observation is reported by narayan et al
for xsum where the maries are similarly short a single sentence with words on average
next we look at the bert models
overall they perform very well with both the mbert and dobert models outperforming the lead n lines and ptgen models by a comfortable margin
indobert is better than mbert approximately rouge point better on average over most metrics showing that a monolingually trained bert is a more effective pre trained model than the lingual variant
the best performance is achieved by indobert s bertextabs
in the canonical test set the improvement over is


and
bertscore points
in the xtreme test set bertextabs suffers a stantial drop compared to the canonical test set rouge and bertscore points although the performance gap between it and is about the same
use the default hyper parameter conguration ommended by the original authors for the pointer generator models
model oracle ptgen bertext mbert bertabs mbert bertextabs mbert bertext indobert bertabs indobert bertextabs indobert canonical test set xtreme test set rl bs rl bs































































































table rouge results for the canonical and xtreme test sets
all rouge and rl scores have a condence interval of at most
as reported by the ofcial rouge script
bs is berscore computed with bert base multilingual cased layer as suggested by zhang et al

error analysis extractive in this section we analyze errors made by the and abstractive bertext bertextabs models to better understand their behaviour
we use the mbert version of these models in our analysis

error analysis of extractive summaries we hypothesized that the disparity between cle and bertext
point difference for in the canonical test set was due to the number of extracted sentences
to test this when extracting sentences with bertext we set the total number of extracted sentences to be the same as the ber of sentences in the oracle summary
ever we found minimal benet using this approach suggesting that the disparity is not a result of the number of extracted sentences
to investigate this further we present the quency of sentence positions that are used in the summary in oracle and bertext for the ical test set in figure
we can see that bertext tends to over select the rst two sentences as the in terms of proportion
of summary
error analysis is based on mbert rather than dobert simply because this was the best performing model at the time the error analysis was performed
while indobert ultimately performed slightly better given that the two models are structurally identical we would expect to see a similar pattern of results
bertext summaries involve the rst two tences
in comparison only
of oracle summaries use sentences in these positions
one may argue that this is because the training and test data have different distributions under our logical partitioning strategy recall that the test set is sampled from the earliest articles but that does not appear to be the case as figure shows the distribution of sentence positions in the training data is very similar to the test data
of oracle summaries involve the rst two sentences

error analysis of abstractive summaries to perform error analysis for bertextabs we randomly sample documents with an score
in the canonical test set which accounts for nearly of the test documents
two native indonesian speakers examined these samples to manually assess the quality of the summaries and score them on a point ordinal scale bad average and good
each annotator is presented with the source document the reference summary and the summary generated by bertextabs
in addition to the overall quality evaluation we also asked the annotators to analyze a number of grained attributes in the summaries abbreviations the system summary uses breviations that are different to the reference summary
distribution of sentence positions for oracle and bertext in the canonical test set
distribution of sentence positions for oracle in the ing set
figure position of oracle predicted extractive summaries category bad avg
good samples abbreviation morphology paraphrasing lack of coverage wrong focus un
details from doc un
details not from doc




















table error analysis for samples with

morphology the system summary uses phological variants of the same lemmas tained in the reference summary
synonyms paraphrasing the system mary contains paraphrases of the reference summary
lack of coverage the system summary lacks coverage of certain details that are present in the reference summary
wrong focus the system summarizes a ent aspect focus of the document to the ence summary
unnecessary details from document the tem summary includes unimportant but ally correct information
unnecessary details not from document the system summary includes unimportant and factually incorrect information tions
average good are resolved as follows bad average bad and good average good
we only have four examples with bad good agreement which we resolved through discussion
interestingly more than half of our samples were found to have good summaries
the primary reasons why these summaries have low rouge scores are paraphrasing
and the inclusion of additional but valid details

ations and morphological differences also appear to be important factors
these results underline a problem with the rouge metric in that it is able to detect good summaries that use a different set of words to the reference summary
one way forward is to explore metrics that consider sentence semantics beyond word overlap such as meteor banerjee and lavie and and question answering system based evaluation such as apes eyal et al
and qags wang et al

another way is to create more ence summaries which will help with the issue of the system summaries including validly different details to the single reference
looking at the results for average summaries middle column bertextabs occasionally fails to capture salient information of the maries have coverage issues and
contain unnecessary but valid details
they also tend to use paraphrases
which further impacts on a lower rouge score
finally the bad system summaries have similar coverage issues and also tend to have a very different focus compared to the we present a breakdown of the different error types in table
inter annotator agreement for the overall quality assessment is high pearson s r

disagreements in the quality label bad we suggest that bertscore should be used as the canonical evaluation metric for the dataset but leave ical validation of its superiority for indonesian summarization evaluation to future work
figure two examples to highlight error categories used in our error analysis
reference summary

in figure we show two representative ples from bertextabs
the rst example is sidered good by our annotators but due to viations morphological differences paraphrasing and additional details compared to the reference summary the rouge score is

in this ample the gold summary uses the abbreviation kepmenakertrans while bertextabs generates the full phrase keputusan menteri tenaga kerja dan transmigrasi which is correct
the example also uses paraphrases invites strong criticism to explain dissatisfaction and there are morphological ences in words such as tuntutan noun vs
tut verb
the low rouge score here highlights the fact that the bigger issue is with rouge itself rather than the summary
the second example is considered to be bad with the following issues lack of coverage wrong focus and contains unnecessary details that are not from the article
the rst sentence president abdurrahman wahid was absent has nothing to do with the original article creating a different focus and confusion in the overall summary
to summarize coverage focus and the sion of other details are the main causes of low quality summaries
our analysis reveals that breviations and paraphrases are another cause of summaries with low rouge scores but that is an issue with rouge rather than the summaries
couragingly hallucination generating details not in the original document is not a major issue for these models notwithstanding that almost of bad samples contain hallucinations
related datasets previous studies on indonesian text summarization have largely been extractive and used small scale datasets
gunawan et al
developed an supervised summarization model over k news ticles using heuristics such as sentence length word frequency and title features
in a similar vein najibullah trained a naive bayes model to extract summary sentences in a article dataset
dokumen
com jakarta langkah reshuffle yang dilakukan presidenabdurrahman wahid agaknya tak mendapat restu
buktinya wakilpresiden megawati sukarnoputri kembali tidak hadir dalampelantikan tiga menteri bidang ekonomi rabu
kalimat dengan kata setelahnya tidak manusia wapres sukarnoputri kembali tidak hadir dalam pelantikantiga menteri baru
dalam reshufle juni megawati juga tak munculdalam pelantikan karena merasa tak dilibatkan dalam reshufflekabinet
ringkasan sistem abdurrahman wahid kembali tidak hadir dalam pelantikantiga menteri bidang ekonomi
ketidaksepakatan soal perombakankabinet itu juga terjadi juni silam
presiden meminta mereka lebihmenjaga koordinasi antarmenteri
of error analysis lack of coverage wrong focus and details that are not from the
com jakarta the reshuffle step was taken by presidentabdurrahman wahid apparently did not get the blessing
the proof vice president megawati sukarnoputri was again not present at theinauguration of three ministers in the economic sector
sentences with words are abbreviated from summary vice president megawati sukarnoputri is not present at theinauguration of three new ministers again
in the reshuffle on june also did not appear in the inauguration because she felt notinvolved in the cabinet reshuffle
system summary abdurrahman wahid was again absent from theinauguration of three ministers in the economic sector
disagreementabout the cabinet reshuffle also occurred june ago
the presidentasked them to maintain more coordination between ministries
dokumen
com jakarta protes masih menyambutkeputusan menteri tenaga kerja dan transmigrasi nomor
kebijakan yang sengaja dikeluarkan sebagai wujud perubahankeputusan sebelumnya sampai sekarang masih mengundangkecaman dari pekerja indonesia
itulah sebabnya merekamenuntut kepmenakertrans baru dicabut karena dinilai merugikanpekerja
kalimat dengan kata tidak itu spsi secara tegas menolak segala bentuk negosiasi
kalimat dengan kata setelahnya tidak manusia pemberlakuan kepmenakertrans masih mengundang rasatidak puas dada sejumlah pekerja indonesia
maka lahirlahtuntutan agar peraturan yang dinilai merugikan dicabut
ringkasan sistem keputusan menteri tenaga kerja dan transmigrasi nomor mengundang kecaman keras dari pekerja indonesia
merekamenuntut kepmenakertrans dicabut karena dinilai merugikan pekerja
spsi menolak negosiasi
of error analysis abbreviation morphoplogy synonyms paraphrashing and details from the
com jakarta protests still resonate with welcomingminister of manpower and transmigration decree no

thispolicy which was deliberately issued as an amendment to theprevious decision until now still invites harsh criticism from workersin indonesia
that is why they demand to revoke the newkepmenakertrans because it is considered detrimental to workers
sentences with words are abbreviated from spsi firmly rejected all forms of negotiation
sentences with words are abbreviated from summary the enactment of kepmenakertrans still invites thedissatisfaction of indonesian workers
hence demands to revoke theregulation arose as it was considered to be detrimental
system summary of manpower and transmigration decree number of strong criticism from workers in indonesia
they demand torevoke kepmenakertrans because it is considered detrimental toworkers
spsi rejects negotiations
aristoteles et al
and silvia et al
ply genetic algorithms to a summarization dataset with less than articles
these studies do not use rouge for evaluation and the datasets are not publicly available
koto released a dataset for chat rization by manually annotating chat logs from whatsapp
however this dataset contains only documents
the largest summarization data to date is indosum kurniawan and louvan which has approximately k news articles with manually written summaries
based on our sis however the summaries of indosum are highly extractive
beyond indonesian there is only a handful of non english summarization datasets that are of cient size to train modern deep learning rization methods over including lcsts hu et al
which contains million chinese short texts constructed from the sina weibo croblogging website and es news gonzalez et al
which comprises spanish news articles with summaries
lcsts documents are relatively short less than chinese characters while es news is not publicly available
our goal is to create a benchmark corpus for indonesian text summarization that is both large scale and publicly available
conclusion we release a large scale summarization corpus for indonesian
our dataset comes with two test sets a canonical test set and an xtreme variant that is more abstractive
we present results for several benchmark summarization models in part based on indobert a new pre trained bert model for indonesian
we further conducted sive error analysis as part of which we identied a number of issues with rouge based evaluation for indonesian
acknowledgments we are grateful to the anonymous reviewers for their helpful feedback and suggestions
in this search fajri koto is supported by the australia awards scholarship aas funded by the ment of foreign affairs and trade dfat tralia
this research was undertaken using the lief hpc gpgpu facility hosted at the university of
whatsapp

melbourne
this facility was established with the assistance of lief grant
references aristoteles aristoteles yeni herdiyeni ahmad ridha and julio adisantoso

text feature weighting for summarization of document bahasa indonesia ijcsi international using genetic algorithm
nal of computer science issues
satanjeev banerjee and alon lavie

meteor an automatic metric for mt evaluation with proved correlation with human judgments
in ceedings of the acl workshop on intrinsic and trinsic evaluation measures for machine tion summarization pages
jianpeng cheng and mirella lapata

neural marization by extracting sentences and words
in proceedings of the annual meeting of the sociation for computational linguistics volume long papers volume pages
arman cohan franck dernoncourt doo soon kim trung bui seokhwan kim walter chang and nazli goharian

a discourse aware attention model for abstractive summarization of long documents
in naacl hlt annual conference of the north american chapter of the association for putational linguistics human language gies volume pages
matan eyal tal baumel and michael elhadad

question answering as an automatic evaluation ric for news article summarization
in naacl hlt annual conference of the north american chapter of the association for computational guistics pages
sebastian gehrmann yuntian deng and alexander m rush

bottom up abstractive summarization
in proceedings of empirical methods in natural language processing pages
j

gonzalez l

hurtado e
segarra f
granada and e
sanchis

summarization of spanish talk shows with siamese hierarchical tion networks
applied sciences
david graff junbo kong ke chen and kazuaki maeda

english gigaword
linguistic data consortium
d gunawan a pasaribu r f rahmat and r budiarto

automatic text summarization for indonesian language using textteaser
iop conference series materials science and engineering
karl moritz hermann tomas kocisky edward stette lasse espeholt will kay mustafa suleyman and phil blunsom

teaching machines to read and comprehend
neural information processing systems pages
wan ting hsu chieh kai lin ming ying lee kerui min jing tang and min sun

a unied model for extractive and abstractive summarization in proceedings of the using inconsistency loss
annual meeting of the association for tational linguistics pages
baotian hu qingcai chen and fangze zhu

sts a large scale chinese short text summarization dataset
in proceedings of the conference on empirical methods in natural language processing pages
byeongchang kim hyunwoo kim and gunhee kim

abstractive summarization of reddit posts with multi level memory networks
in naacl hlt annual conference of the north american chapter of the association for computational guistics pages
fajri koto

a publicly available indonesian pora for automatic abstractive and extractive chat summarization
in proceedings of the tional conference on language resources and uation lrec
fajri koto afshin rahimi jey han lau and timothy baldwin
to appear
indolem and indobert a benchmark dataset and pre trained language model for indonesian nlp
in proceedings of the ternational conference on computational tics coling
kemal kurniawan and samuel louvan

sum a new benchmark dataset for indonesian text in international conference summarization
on asian language processing ialp pages
mike lewis yinhan liu naman goyal jan ghazvininejad abdelrahman mohamed omer levy veselin stoyanov and luke zettlemoyer

bart denoising sequence to sequence training for natural language generation translation and comprehension
in proceedings of the nual meeting of the association for computational linguistics pages
chin yew lin

rouge a package for in text matic evaluation of summaries
rization branches out proceedings of the workshop pages
yang liu and mirella lapata

text tion with pretrained encoders
in conference on empirical methods in natural language ing pages
marek medved and vt suchomel

indonesian web corpus idwac
in lindat clarin digital brary at the institute of formal and applied tics ufal faculty of mathematics and physics charles university
ahmad najibullah

indonesian text tion based on naive bayes method
proceeding of the international seminar and conference
ramesh nallapati feifei zhai and bowen zhou

summarunner a recurrent neural network based sequence model for extractive tion of documents
in proceedings of the thirtieth aaai conference on articial intelligence pages
ramesh nallapati bowen zhou cicero nogueira dos santos caglar gulcehre and bing xiang

abstractive text summarization using sequence sequence rnns and beyond
in proceedings of the signll conference on computational natural language learning pages
shashi narayan shay b
cohen and mirella lapata

do nt give me the details just the summary topic aware convolutional neural networks for treme summarization
in emnlp ference on empirical methods in natural language processing pages
romain paulus caiming xiong and richard socher

a deep reinforced model for abstractive marization
in proceedings of the international conference on learning representations
alexander m
rush sumit chopra and jason weston

a neural attention model for abstractive in proceedings of tence summarization
cal methods in natural language processing pages
evan sandhaus

the new york times annotated corpus
linguistic data consortium
abigail see peter j
liu and christopher d
manning

get to the point summarization with generator networks
in proceedings of the nual meeting of the association for computational linguistics pages
eva sharma chen li and lu wang

patent a large scale dataset for abstractive and coherent summarization
in acl the nual meeting of the association for computational linguistics pages
silvia pitri rukmana vivi regina aprilia derwin suhartono rini wongso and meiliana

marizing text for indonesian language by using tent dirichlet allocation and genetic algorithm
in international conference on electrical ing computer science and informatics pages
f
tala j
kamps k
e
muller and m
rijke

the impact of stemming on information retrieval in bahasa indonesia
in the meeting of tional linguistics in the netherlands
jiwei tan xiaojun wan and jianguo xiao

abstractive document summarization with a in proceedings of based attentional neural model
the annual meeting of the association for putational linguistics volume long papers ume pages
alex wang kyunghyun cho and mike lewis

asking and answering questions to evaluate the in proceedings of tual consistency of summaries
the annual meeting of the association for putational linguistics pages
yonghui wu mike schuster zhifeng chen quoc v
le mohammad norouzi wolfgang macherey maxim krikun yuan cao qin gao klaus macherey jeff klingner apurva shah melvin johnson xiaobing liu ukasz kaiser stephan gouws yoshikiyo kato taku kudo hideto kazawa keith stevens george kurian nishant patil wei wang cliff young jason smith jason riesa alex rudnick oriol vinyals greg corrado macduff hughes and jeffrey dean

google s neural machine translation system bridging the gap between human and machine translation
arxiv preprint

jingqing zhang yao zhao mohammad saleh and ter liu

pegasus pre training with tracted gap sentences for abstractive summarization
in icml international conference on machine learning
tianyi zhang varsha kishore felix wu kilian q
weinberger and yoav artzi

bertscore in iclr evaluating text generation with bert
eighth international conference on learning representations

