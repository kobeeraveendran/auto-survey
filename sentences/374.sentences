few shot text generation with pattern exploiting training timo schick and hinrich schtze center for information and language processing lmu munich germany
lmu
c e d l c
s c v
v i x r a abstract patterns generated texts providing pretrained language models with simple task descriptions or prompts in ral language yields impressive few shot results for a wide range of text classication tasks when combined with gradient based learning from examples
in this paper we show that the underlying idea can also be applied to text generation tasks we adapt pattern exploiting training pet a recently proposed few shot approach for netuning generative language models on text generation tasks
on several text summarization and headline generation datasets our proposed variant of pet gives consistent improvements over a strong line in few shot settings
introduction pretraining large neural networks with a language modeling objective has led to signicant ments throughout nlp peters et al
howard and ruder radford et al
devlin et al

further improvements are often possible by choosing a different pretraining objective that more closely matches the downstream task of est
examples include casing prediction for named entity recognition mayhew et al
gap tence generation for summarization zhang et al
and sentence unshufing for discourse resentations lee et al

while such approaches often reduce the amount of training data required they still do not perform well if only a handful of examples is available for the downstream task which is common for word uses of nlp
in such few shot settings ever signicant gains are possible by proceeding the other way around instead of making ing more similar to a downstream task we can reformulate the task itself to make it more similar to the pretraining objective
for masked language implementation is publicly available at github
com timoschick pet
short summary please contact us if you have any questions
dear john i have been having trouble with my internet banking account
your internet banking accounts are now setup again for accessing
e mail title internet banking password reset figure texts generated by pegasus large with ferent patterns for input dear john your internet banking accounts are now setup again for accessing
the login i d is still your main account with the password being reset to the last six digits of your ssn
models e

devlin et al
lewis et al
one such reformulation technique is to convert puts to cloze questions by adding a text snippet that contains some form of task description often in the form of a short prompt radford et al
schick and schtze
besides making pretraining and netuning more similar this approach has the compelling benet of enabling users to explain a task to a pretrained model making it much easier for the model to understand the task
examples in figure demonstrate that pretrained language models can make use of such text snippets to adapt their output for a generation task
while this idea even works in an unsupervised setting radford et al
or when examples are simply provided as additional context brown et al
it only unfolds its full potential when bined with gradient based training on a handful of labeled examples schick and schtze
in particular pattern exploiting training pet an approach proposed by schick and schtze that combines task descriptions with learning from examples performs strongly for various few shot text classication datasets
however it can only be applied to classication tasks and is therefore not applicable to any problems that require the tion of text sequences
in this paper we adapt pet to train generative models on text generation tasks
in particular we propose several modications that enable us to tune a pretrained pegasus model zhang et al
with pet
we evaluate our approach on a diverse set of six english headline generation and text summarization tasks both in zero shot and shot settings and show that pegasus trained with our adapted version of pet clearly outperforms regular netuning
in summary our contributions are as follows we describe how pet can be modied for netuning generative language models for quence generation tasks
we show that training pegasus with pet outperforms regular netuning across a large set of tasks and training set sizes
we analyze the factors contributing to pet s strong performance
related work masked language modeling was proposed as a training objective by devlin et al

several variants of this objective have been proposed that involve generating sequences of text including raffel et al
bart lewis et al
and pegasus zhang et al
on which our approach is based
the idea to rephrase tasks as cloze questions is commonly used to probe the knowledge contained within masked language models e

petroni et al
wang et al
schick and schtze ettinger
schick and schtze propose pet which combines this idea with gradient based learning for few shot text cation
jiang et al
and schick et al
consider the problem of nding the best way to rephrase a given task as a cloze question
closely related to our work schick and schtze propose a modication to pet that enables the eration of multiple tokens but their approach still requires a text classication objective and does not scale to long output sequences
other approaches for few shot learning in nlp commonly require large sets of examples from lated tasks gu et al
dou et al
qian and yu parallel data for consistency ing et al
chen et al
or highly specialized methods tailored towards a specic task laban et al

in contrast pet requires no additional labeled data and provides an intuitive terface to leverage task specic human knowledge
finally our proposed variant of pet is closely related to concurrent work by he et al
who use prompts and keywords for controllable text eration but do so only in high resource settings and to prex constrained decoding knowles and koehn wuebker et al
keskar et al

pegasus pretraining pegasus zhang et al
is a standard former encoder decoder architecture vaswani et al
that is pretrained using gap sentences generation an objective tailored to text rization tasks
this pretraining objective requires a set of documents consisting of multiple sentences
the key idea is to preprocess each document by i picking a subset of sentences that are important to the document replacing each of these tences by a mask token and concatenating all removed sentences into a pseudo summary
the transformer model is then trained to generate this pseudo summary given the partially masked ument
similar to prior work e

raffel et al
lewis et al
this is done by ing the entire masked document using the encoder and generating the output in an autoregressive ion using the decoder
for picking informative sentences the informativeness of each sentence is measured as the score lin tween the sentence and the remaining document and the top m sentences according to this measure are selected
zhang et al
train two variants of gasus pegasus base a layer model with approximately m parameters and large a layer model with m parameters
as only the latter version is publicly available all of our experiments are based on pegasus large
pattern exploiting training we rst discuss pattern exploiting training pet for text classication tasks i
e
for problems where some textual input x must be mapped to a single output y from a nite set y
let m be a masked language model mlm t its set of tokens and t the mask token we denote the set of all token sequences as t
pet requires a pattern p x t that maps inputs to cloze questions containing exactly one mask a verbalizer v y t that maps each output to a single token representing its meaning
the probability of y given is then derived from the score that m assigns to at the masked position in p
as shown in jiang et al
schick and schtze some pairs p v work much ter than others
however in the absence of a large development set pairs that work well are often hard to distinguish from those that perform poorly
pet alleviates this issue by enabling the neous usage of multiple pattern verbalizer pairs and combining them using a mechanism similar to knowledge distillation hinton et al

for each pair p v a separate mlm is tuned on all available training data

the ensemble of mlms is used to annotate a set of unlabeled examples with soft labels

a single mlm with a sequence classication head is netuned on the resulting soft labeled dataset and serves as the nal classier
pet for text generation there are several differences to consider when signing a form of pet appropriate for the ative setting first we do not require a izer as the output space already consists of natural language sentences i
e
y t
second the encoder decoder architecture of most generative language models supports some subtle adjustments with regard to the application of patterns
finally we need a novel strategy for combining multiple patterns as we can not simply average the text quences produced by different models in a ingful way
throughout this section let p be a pattern x y y and p z for some t
furthermore let y


z


zm and let the single mask token in z be at some position h m
we denote the concatenation of y and z by yz and the subsequence yi


yj by yi
we consider an encoder decoder model m pretrained using a masked language modeling objective
that is the model must be able to compute a ity pm y that measures to what extent y is a plausible substitute for the mask in z
we further assume that this is done by decomposing the joint probability of y as follows pm y pm yi z n where pm yi z is obtained from m by processing z using the encoder and using the decoder
if we happen to already know some prex of y the probability of the remaining sequence yk can be expressed as pm yk n z pm yi z n i as m is an encoder decoder language model we have several options of how to compute the probability of y given using pattern p we may process the entire sequence p with the encoder as above but we may also choose some index j h and process n using the coder and zj using the decoder
for example if summary text we can process the x summary using the encoder or the decoder that is we may either compute pm z or pm text summary
in preliminary experiments we found tokens that belong to the partially generated output quence i
e
tokens that are processed using the decoder to have a stronger impact on the model s predictions than regular input tokens
this applies all the more to pegasus which is pretrained to always generate full sentences if some part of the used pattern is supposed to be a prex of the sentence to be generated e

a short prompt gasus tends to simply ignore this part when it is processed using the encoder
based on this observation we supplement each pattern p with a decoder prex d t that is given to the model as part of the generated quence rather than the observed input
accordingly we dene the probability of y given as pm y p in the example discussed above corresponds to using p summary text with an empty are several recent architectures that fulll this quirement including bart lewis et al
raffel et al
and pegasus zhang et al

decoder prex d whereas corresponds to ing the pattern p text with a decoder prex d summary
we netune m on a set of training examples y simply by minimizing the cross entropy tween and y using teacher forcing
combining patterns if we have multiple pairs


pk dk of patterns and decoder xes we rst netune an individual model mi for each pi as in regular pet
to combine their knowledge and distill it into a single model m we again require a set of unlabeled examples u
however we need a different strategy than schick and schtze for assigning target sequences y y to each unlabeled example u because we can not simply average all sequences generated by the individual models
computing a single put sequence that is most likely according to all models is also infeasible as doing so efciently would require us to keep k models in memory at the same time
we instead resort to the following approach for each u we rst generate one output sequence di per pi using greedy decoding as in zhang et al
resulting in a set of candidate outputs cx i k
we then assign a score to each candidate y cx
to this end we rst compute the normalized log likelihood of y for each pi as log where we divide by the length of y to correct for length bias boulanger lewandowski et al
jean et al

we obtain the total score of y from the average of its scores according to ual patterns as exp
the nal model m is then trained on pairs y where u and y is drawn from cx with bility proportional to
while we could train the nal model to simply maximize p m y we note that this creates a strong discrepancy between pretraining and ing during pretraining pegasus only processes sequences that contain at least one mask token
in the spirit of our intention to make pretraining and netuning as similar as possible we therefore train using a trivial pattern p that just prepends a single mask token to the input and use an empty decoder prex
task decoder prexes e mail subject aeslc headline gigaword cnn dailymail highlights others e mail topic article headline article highlights short summary brief summary table decoder prexes we use for the tion and headline generation tasks experiments tasks we evaluate pegasus with and without pet on a subset of the tasks used by zhang et al

as we have limited compute available we only choose those tasks for which the maximum output length in zhang et al
is at most tokens
specically we consider the following datasets aeslc zhang and tetreault given an email body the title of the email must be predicted
gigaword rush et al
given the rst sentence of a news article its headline has to be generated
xsum narayan et al
articles ning a wide range of different topics have to be summarized
reddit tifu kim et al
summaries have to be generated for posts from the tifu community in reddit
newsroom grusky et al
maries must be generated for articles from various major publications
cnn dailymail hermann et al
for articles from cnn and the daily mail a list of highlights has to be generated
for each task we use the entire test set for ation
we create two types of training sets taining either or training examples in dition we provide unlabeled examples per task
both unlabeled and training examples are randomly sampled from the original training set
only exception to this is newsroom which tains more than examples
to enable a more friendly evaluation we only consider a random subset of examples for this dataset
do not use the same few shot datasets as zhang et al
as they did not use a xed random seed and thus their exact training data is not recoverable
t model aeslc gigaword xsum reddit tifu newsroom cnn dailymail avg




















pegasus




















pegasus m pegasus pet









































pegasus pegasus m




















pegasus pet









































pegasus pegasus m




















pegasus pet




















table rl scores for all tasks and training set sizes considered all results are averaged across three different seed dependent training sets
the last column shows average performance across all tasks
as previous work schick and schtze has shown that the particular set of training ples can have a huge impact on a model s mance we create three distinct training sets per size and and task using different random seeds
scores reported in this section are always average scores across all three sets of training examples
patterns we use the same set of patterns across all tasks but we combine them with different coder prexes
the patterns we use are text all decoder prexes are shown in table
we combine each pattern with each decoder prex resulting in four pairs and per task
setup for all of our experiments with pet we use pegasus large zhang et al
as lying language model our implementation is based on the transformers library wolf et al

unless stated differently all experiments are formed using the same setup as schick and schtze using a single gpu
for optimizing hyperparameters much previous work uses development sets that are larger than the training sets by multiple orders of magnitude e

xie et al
zhang et al
chen et al
however assuming the existence of such large development sets is highly inconsistent with real world few shot settings
in contrast schick and schtze assume no development data at all and determine hyperparameters solely based on previous work and practical considerations
we choose a middle way and create a small ment set of examples for only one of the six tasks xsum
we use this development set in bination with a single training set containing amples to determine hyperparameters for all tasks and training sets
we do so only for ters for which no consistent value can be derived from previous work
following zhang et al
we use a mum input length of tokens the adafactor timizer shazeer and stern with square root learning rate decay a dropout rate of
and label smoothing setting
szegedy et al
we also adopt zhang et al
s maximum put lengths for each task
as recommended by schick and schtze we train all models for steps using a batch size of
we also tried training for and steps on our velopment set but found no major differences in performance
for the learning rate we tried values of with as schick and schtze use and zhang et al
use we found to perform best for all models
for evaluation we follow zhang et al
and report and rougel rl scores lin after stemming using the porter algorithm porter
results on all six tasks we compare the ing three approaches for netuning a pretrained pegasus model pegasus the regular netuning procedure described in zhang et al

pegasus m finetuning with a single trivial pattern that inserts a mask token before the rst word
pegasus pet finetuning using pet with all patterns described above
table shows results both for zero shot learning and for few shot learning with and training examples
in the few shot settings pet tently outperforms both baselines across all tasks model aeslc xsum newsroom model reddit tifu newsroom cnn dailymail








pegasus








pegasus m








pegasus pet








worst only best only








no dec
prex








pegasus








pegasus pet








pegasus








pegasus pet








table rl scores for several baselines and variants of pet given training examples table rl scores with maximum output lengths of and given training examples resulting in an average improvement in over pegasus of

vs
and

vs

while pegasus m gives consistent provements over regular netuning it still performs clearly worse than pet demonstrating that the pet model is indeed able to make use of the task scriptions provided
in the zero shot setting pet also outperforms both baselines on average but falls short on individual tasks
analysis we now look at the factors contributing to pet s performance in detail
to this end ble compares the performance of the best best only and the worst worst only performing tern and decoder prex to that of pet in a setting with training examples
we see some ence in performance between using only the best and worst patterns but this difference is not as nounced as in previous work schick and schtze
notably our simple strategy for bining patterns performs even better than using just the best pattern across all tasks and metrics
table also shows results for using no decoder prex no dec
prex and instead processing the entire input using the encoder
that is given p d with p


zn and zh we compute pm y





zn rather than pm y


zn
while this variant overall forms better than pegasus m results clearly show that pegasus makes less use of task descriptions if they are processed using the encoder
finally we look at the performance of pet as a function of the maximum output length
we hypothesize that the inuence of the decoder prex on generated tokens may decrease with distance
this would mean that diminishing gains are to be expected from pet for tasks that require longer text sequences to be generated
to investigate this assumption table shows the performance of both pegasus and pegasus pet for three tasks using maximum output lengths of and
for both values of we compute the gains from using pet as the difference in mance between pegasus pet and pegasus
on average increasing to tokens reduces the gains from pet over regular netuning by just
points
this shows that task descriptions provided using pet have a strong impact on generated tokens even if there are dozens of other tokens in between and accordingly our proposed approach is also benecial for generating long text sequences
conclusion we have shown how pattern exploiting training pet can be transferred to text generation tasks by i introducing the concept of decoder prexes and combining patterns through knowledge tion where target sequences are generated from domly chosen patterns
with these modications a pretrained pegasus model netuned with pet clearly outperforms regular netuning in few shot settings
for future work it would be interesting to see whether it is also possible to make pretrained language models understand more complex task descriptions than the simple prompts we have used especially as some concurrent work weller et al
efrat and levy suggests this might not be the case
references nicolas boulanger lewandowski yoshua bengio and pascal vincent

audio chord recognition with recurrent neural networks
in ismir
tom b
brown benjamin mann nick ryder melanie subbiah jared kaplan prafulla dhariwal arvind neelakantan pranav shyam girish sastry amanda askell sandhini agarwal ariel herbert voss gretchen krueger tom henighan rewon child aditya ramesh daniel m
ziegler jeffrey wu clemens winter christopher hesse mark chen eric sigler mateusz litwin scott gray jamin chess jack clark christopher berner sam mccandlish alec radford ilya sutskever and dario amodei

language models are computing research repository shot learners


jiaao chen zichao yang and diyi yang

text linguistically informed interpolation of den space for semi supervised text classication
in proceedings of the annual meeting of the ciation for computational linguistics pages online
association for computational guistics
jacob devlin ming wei chang kenton lee and kristina toutanova

bert pre training of deep bidirectional transformers for language in proceedings of the conference standing
of the north american chapter of the association for computational linguistics human language technologies volume long and short papers pages minneapolis minnesota
ation for computational linguistics
zi yi dou keyi yu and antonios anastasopoulos
investigating meta learning algorithms for
low resource natural language understanding tasks
in proceedings of the conference on empirical methods in natural language processing and the international joint conference on natural guage processing emnlp ijcnlp pages hong kong china
association for tional linguistics
avia efrat and omer levy

the turking test can language models understand instructions ing research repository

allyson ettinger

what bert is not lessons from a new suite of psycholinguistic diagnostics for language models
transactions of the association for computational linguistics
max grusky mor naaman and yoav artzi

newsroom a dataset of
million summaries with diverse extractive strategies
proceedings of the conference of the north american chapter of the association for computational linguistics man language technologies volume long pers
jiatao gu yong wang yun chen victor o
k
li and kyunghyun cho

meta learning for in resource neural machine translation
ings of the conference on empirical methods in natural language processing pages brussels belgium
association for computational linguistics
junxian he wojciech kryscinski bryan mccann nazneen rajani and caiming xiong

towards generic controllable text sum computing research repository marization


karl moritz hermann tomas kocisky edward stette lasse espeholt will kay mustafa suleyman and phil blunsom

teaching machines to read in advances in neural and comprehend
tion processing systems volume pages
curran associates inc
geoffrey hinton oriol vinyals and jeff dean

distilling the knowledge in a neural network
puting research repository

jeremy howard and sebastian ruder

universal language model ne tuning for text classication
in proceedings of the annual meeting of the sociation for computational linguistics volume long papers pages melbourne australia
association for computational linguistics
sbastien jean orhan firat kyunghyun cho roland memisevic and yoshua bengio

montreal neural machine translation systems for
in proceedings of the tenth workshop on statistical machine translation pages lisbon tugal
association for computational linguistics
zhengbao jiang frank f
xu jun araki and graham neubig

how can we know what language models know transactions of the association for computational linguistics
nitish shirish keskar bryan mccann lav r varshney caiming xiong and richard socher

ctrl a conditional transformer language model for trollable generation
computing research tory

byeongchang kim hyunwoo kim and gunhee kim

abstractive summarization of reddit posts in with multi level memory networks
ings of the conference of the north american chapter of the association for computational guistics human language technologies volume long and short papers pages neapolis minnesota
association for computational linguistics
rebecca knowles and philipp koehn

neural interactive translation prediction
in proceedings of the association for machine translation in the icas pages
philippe laban andrew hsi john canny and marti a
hearst

the summary loop learning to write in abstractive summaries without examples
ceedings of the annual meeting of the ciation for computational linguistics pages online
association for computational guistics
haejun lee drew a
hudson kangwook lee and christopher d
manning

slm ing a discourse language representation with tence unshufing
computing research repository

mike lewis yinhan liu naman goyal jan ghazvininejad abdelrahman mohamed omer levy veselin stoyanov and luke zettlemoyer

bart denoising sequence to sequence training for natural language generation translation and comprehension
in proceedings of the nual meeting of the association for computational linguistics pages online
association for computational linguistics
chin yew lin

rouge a package for matic evaluation of summaries
in text tion branches out pages barcelona spain
association for computational linguistics
stephen mayhew gupta nitish and dan roth

robust named entity recognition with truecasing training
proceedings of the aaai conference on articial intelligence
shashi narayan shay b
cohen and mirella lapata

do nt give me the details just the summary topic aware convolutional neural networks for in proceedings of the treme summarization
conference on empirical methods in natural guage processing pages brussels gium
association for computational linguistics
matthew peters mark neumann mohit iyyer matt gardner christopher clark kenton lee and luke zettlemoyer

deep contextualized word in proceedings of the resentations
ence of the north american chapter of the ation for computational linguistics human guage technologies volume long papers pages new orleans louisiana
association for computational linguistics
fabio petroni tim rocktschel sebastian riedel patrick lewis anton bakhtin yuxiang wu and alexander miller

language models as edge bases proceedings of the conference on empirical methods in natural language processing and the international joint conference on ral language processing emnlp ijcnlp
martin f
porter

an algorithm for sufx ping page
morgan kaufmann publishers inc
san francisco ca usa
kun qian and zhou yu

domain adaptive log generation via meta learning
in proceedings of the annual meeting of the association for putational linguistics pages florence italy
association for computational linguistics
alec radford karthik narasimhan tim salimans and improving language ilya sutskever

standing by generative pre training
alec radford jeff wu rewon child david luan dario amodei and ilya sutskever

language models are unsupervised multitask learners
cal report
colin raffel noam shazeer adam roberts ine lee sharan narang michael matena yanqi zhou wei li and peter j
liu

exploring the limits of transfer learning with a unied text text transformer
journal of machine learning search
alexander m
rush sumit chopra and jason weston

a neural attention model for abstractive in proceedings of the tence summarization
conference on empirical methods in natural guage processing pages lisbon portugal
association for computational linguistics
timo schick helmut schmid and hinrich schtze

automatically identifying words that can serve as labels for few shot text classication
in proceedings of the international conference on computational linguistics pages barcelona spain online
international committee on computational linguistics
timo schick and hinrich schtze

exploiting cloze questions for few shot text classication and natural language inference
computing research repository

timo schick and hinrich schtze

it s not just size that matters small language models are also few shot learners
computing research repository

timo schick and hinrich schtze

rare words a major problem for contextualized embeddings and how to x it by attentive mimicking
in proceedings of the thirty fourth aaai conference on articial intelligence
noam shazeer and mitchell stern

adafactor adaptive learning rates with sublinear memory cost
computing research repository

c
szegedy v
vanhoucke s
ioffe j
shlens and z
wojna

rethinking the inception in ieee ture for computer vision
ence on computer vision and pattern recognition cvpr pages
ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan n gomez lukasz kaiser and illia polosukhin

attention is all you need
in advances in neural information cessing systems pages
curran ciates inc
cunxiang wang shuailong liang yue zhang nan li and tian gao

does it make sense and why a pilot study for sense making and planation
in proceedings of the annual ing of the association for computational linguistics pages florence italy
association for computational linguistics
orion weller nicholas lourie matt gardner and matthew peters

learning from task tions
proceedings of the conference on pirical methods in natural language processing emnlp
thomas wolf lysandre debut victor sanh julien chaumond clement delangue anthony moi ric cistac tim rault remi louf morgan icz joe davison sam shleifer patrick von platen clara ma yacine jernite julien plu canwen xu teven le scao sylvain gugger mariama drame quentin lhoest and alexander rush

formers state of the art natural language ing
in proceedings of the conference on pirical methods in natural language processing system demonstrations pages online
ciation for computational linguistics
joern wuebker spence green john denero saa hasan and minh thang luong

models and inference for prex constrained machine translation
in proceedings of the annual meeting of the association for computational linguistics volume long papers pages berlin germany
sociation for computational linguistics
qizhe xie zihang dai eduard hovy minh thang ong and quoc v
le

unsupervised data mentation for consistency training
computing search repository

jingqing zhang yao zhao mohammad saleh and peter liu

pegasus pre training with tracted gap sentences for abstractive summarization
in proceedings of the international conference on machine learning volume of proceedings of machine learning research pages virtual
pmlr
rui zhang and joel tetreault

this email could save your life introducing the task of email subject line generation
in proceedings of the annual meeting of the association for computational guistics pages florence italy
association for computational linguistics

