framerank a text processing approach to video summarization zhuo chao qian guoping doctoral innovation center of computer science the university of nottingham ningbo china of information engineering and guangdong key lab for intellignet information processing shenzhen university china of computer science the university of nottingham uk zhuo
lei chao zhang qian
edu
cn guoping

ac
uk r a l c
s c v
v i x r a abstract video summarization has been extensively studied in the past decades
however user generated video summarization is much less explored since there lack large scale video datasets within which human generated video summaries are biguously dened and annotated
toward this end we pose a user generated video summarization dataset that consists of videos minutes
in structing the dataset because of the subjectivity of generated video summarization we manually annotate summaries for each video which are in total summaries
to the best of our knowledge it is currently the largest dataset for user generated video summarization
based on this dataset we present framerank an pervised video summarization method that employs a to frame level afnity graph to identify coherent and mative frames to summarize a video
we use the based graph to rank temporal ments according to the amount of semantic information tained in their frames
we illustrate the effectiveness of our method by applying it to three datasets summe tvsum and and show it achieves state of the art results
index terms video summarization unsupervised learning framerank kl divergence graph
introduction user generated video is growing exponentially
hence the demand for efcient ways of searching and retrieving desired content will cost huge amounts of resources like time man resources and machine congurations
however users always think little of time spending cutting content and view selection
thus user generated videos consist of long including illumination shakiness dynamic background and so on and unedited contents
in this text video summarization plays an important role in assisting users to quickly browse through important events contained in it
recently video summarization techniques have drawn a lot of attention especially for user generated videos
the fig

overview of the proposed framework
essential of user generated video summarization is to identify important parts of original videos and dene their importance
however the insufciency of publicly available datasets has limited this important line of research
due to the tivity human generated summaries are the most needed to meet the purpose of training and evaluation for different generated video summarization methods
to help alleviate this we introduce a new dataset which contains videos covering various user generated contents and human generated video summaries for each one
more another challenge is no standard criteria for ing importance even humans can not agree on a universal sis for generating video summary
in this paper we consider frame importance as the ones can most substitute others
based on dataset we propose a novel pervised framework for user generated video summarization which identies coherent and informative video frames to summarize a video
as shown in figure we rst divide an original video into disjoint segments with a dense based clustering method
we then develop a graph based ranking method framerank to score and rank these ments according to the amount of semantic information
table
comparison between existing datasets
summe ours tvsum dataset ute static video dynamic video egocentric video total frame avg
frame total video length s avg
video length s avg
mary per video f score avg
cronb
n a n a n a





nally we sample video segments with high scores to ate video summaries
through systematic experiments and evaluation we show the proposed novel video summarization method is effective and outperforms state of the art methods on the new dataset and the other two existing datasets summe and tvsum
our main tions are as the followings we introduce a new dataset for user generated video summarization
to the best of our knowledge it is the largest user generated video tion dataset able to meet the purpose of training and ation for different methods we develop a new method framerank to assess the importance of video frames we proposed a novel approach for video temporal segmentation in which segments are semantically consistent and ate to produce good video summaries

related work

datasets to facilitate the comparison between these datasets and our dataset we show in table more dataset ics
to be noted the annotation information on ute is not available since it does not apply human generated video summaries to conduct the evaluation while asking question to participants
generally speaking previous datasets have greatly boosted the researches in user generated video marization but still have several drawbacks
first video types are still insufcient
second multiple human generated video summaries for each video are necessary due to their tivity
third these datasets do not cover abundant categories


user generated video summarization most works attempt to assess importance or interestingness of video frames with supervised methods
trained a model according to signicant interaction between people and jects to learn the saliency in egocentric videos
built links between objects to create story driven summaries grounded on s egocentric feature
combined multiple tures to train a regressor to predict interestingness
devised a two steam deep convolutional neural network cnn chitecture by fusing spatial and temporal information tively on each steam for video highlight detection
learned non parametrically to transfer summary structures from ing videos to the test ones and improved long term memory lstm to model the variable range ral dependency among video frames
in general supervised methods require a large amount of training data which is what is lacking in this eld
existing datasets are not able to cover variety of user generated videos because what users are interested in can not be exactly dened
while human generated summaries are labor intensive and time consuming and it requires multiple summaries for each single video due to the subjectivity
therefore the learned models may be not portable to work on user generated videos
some unsupervised methods are put forward in generated video summarization
in detail these methods use various types of intuitive criteria or pre trained models from other elds to facilitate the assessment of importance or estingness of video frames
with video s title or keywords as query obtained canonical viewpoints collected from website to predict important frames to achieve tion
employed an auto encoder to train with internet videos of the same topic and then assessed importance cording to how well it can reconstruct input video s feature
utilized a linear svm classier to obtain the condence of event type as importance scores
however although tant frames can be effectively predicted these methods can only work on domain specic videos or require metadata of videos
moreover retrieving these images or videos is sive even collected metadata may not be relevant or correct

benchmark dataset user generated video summarization is a relatively plored domain and there are few public datasets with ple human generated summaries available
we therefore lected a new dataset that contains videos and each has human annotated summaries


video collection we collected videos captured either by ourselves or from youtube which are recorded in multiple ways including static dynamic and egocentric views
the duration ranges from to minutes
they are all raw or minimally edited
to group video frames into disjoint segments of semantically consistent frames
second we rank segments ing to the amount of semantic information contained in their frames using the graph based framerank
finally we apply a greedy selection strategy to generate nal summaries


video temporal segmentation comparing to we divide videos with a temporal tation method based on the clustering of deep semantic ity graph
in general connection between frames can be sidered as a graph where vertexes refers to the video frames and edges are the pairwise similarities
the initial idea was to transfer a video to text and form text summarization
however image tagging and image caption are still open areas
even with correctly classied bels it is still difcult to precisely represent semantic content of video frames
besides frame contents may be different even though they have the same labels
to convey tic information effectively we decide to use the probability in detail distribution of a set of labels to denote a frame
we feed video frames to a deep cnn pre trained on ages of object categories from imagenet dataset to compute the probabilities of frames containing objects
this representation enjoys the advantage of capturing information of the presence of a variety of object categories
we choose the kl divergence to measure how well a frame can represent another which is a measurement of the difference between two probability distributions
it is the amount of information loss when a distribution is used to proximate another distribution which can be interpreted how much a frame contains semantic information of another
fig

comparison between segments with and without using temporal constraint factor
vertical axis is segment indexes where the negative represent generated segment indexes with temporal order factor and the positive represent the ones out temporal order factor
horizontal axis is temporal order
we construct a graph w where v fi are texes and w are edges between vertex fi and fj
edge wij is the kl divergence computed as follows fig

we show these videos represented by their thumbnails
we collected videos encompassing various user generated contents like holidays events and sports
compared with other datasets ours has more videos categories and generated summaries
figure shows the thumbnails


video annotation due to the subjectivity of user generated video tion it is almost impossible to obtain absolute ground truth bels thus evaluation is often carried out with multiple human judgment
we asked participants to collect human generated summaries
videos were shown to participants in a random order playing at the speed of
second per frame
participants were asked to watch entire video in a single take and provide time slots to generate video summary
we muted audio to ensure scores are based solely on visual stimuli
we have different annotations for each video taking more than hours
following we calculated average wise score among collected human ground truth
the score is
which is approximately close to that of summe and tvsum dataset
and

meanwhile we puted cronbach which is a standard measure to assess the reliability of a psychometric test
the dataset has a mean of

the minimum value is
and the mum value is

the cronbach of summe and tvsum datasets are
and
respectively
ideally is around
while
is considered acceptable in exploratory searches
thus with dataset video summarization experiments can be carried out with condence
we provide a full list of the videos including name camera type frame number video length average summary length average score and cronbach in supplementary material

proposed framework the proposed framework consists of three main steps as shown in figure which inputs a user generated video and output a video summary
we construct a graph where vertex corresponding to a frame and edge between two vertexes is the kl divergence of two frames semantic probability butions
first we use a bundling center clustering algorithm log pfj k where i and j are frame indexes and means element wise multiplication
represents the probability of label of frame fi
we negate values to transfer the difference into ilarity and normalize matrix g
in addition w is a constrained graph with a gaussian function to maintain poral order and smooth frame difference where w w ij are edges between frames
each vertex can be represented as w ij e where is a control parameter to modify temporal tion and smoothness level
hence a temporally constrained graph g tc can be represented as g tc g g furthermore cluster center can be multiple similar frames rather than a single one which is denoted as bundling center
with a dense neighbor based clustering method we can identify local clusters based on the edge connectivity on g tc
to be noted elements of a local cluster are locally similar to all of the other elements inside neighborhood instead of being close to a single element
more details can be referred to
we show examples of the comparison between results with or without temporal constraint factor in figure


segment selection with framerank the difculty in video summarization is how to dene tant frames or segments to compose summary
there are no standard criteria for measuring the importance of video ments even human subjects can not agree on a universal sis
a good summary should be concise and retain the most informative and signicant contents
in other words selected frames or segments that compose the summary should be able to represent the unselected ones as much as possible
in this paper we dene important frames as the ones can substitute others with least information loss
as described in section
with w we develop the framerank method which works similarly to the trank text ranking method in natural language ing
we build a graph with its vertices corresponding to video frames and edges measure the similarity between the frames
we then implement a graph ranking technique to measure ative importance of each video frame as well as segment
we calculate importance score of the vertex fi as d fj wji wjl where is a damping factor and which plays the role of integrating the model into the probability of ing from a given vertex to another random vertex in the graph
in detail damping factor d can be interpreted as a dom suffer changing of visual contents which may be caused by a sudden camera moving in the case of user generated videos
following we set d

the running of the algorithm starts with arbitrary ues assigned to each vertex in the graph and iterates until in our implementation we stop the iteration convergence
when importance scores between two consecutive iterations is below a given threshold
let i be the score of tex fi at iteration k the iteration stops at kth iteration if i where is a pre set threshold
after the algorithm converges each vertex has a score resenting the importance of video frames associated with the vertex
the nal importance scores of the vertices of erank are not dependent on initial values only the number of iterations to converge may be different
other related work tried to estimate the score of ment by summing up all frame importance scores
however it may result in longer segments getting larger importance scores
thus we compute relative importance score of the segment sn with the average importance tstart tstart tend where tstart and tend are start and end frames of the segment


video summary generation we generate a video summary by selecting video segments that can substitute the others with the least information loss
given the set of importance scores we want to nd a subset of segments with their total length below a pre dened imum l while the total importance scores is maximized
in other words we want to solve the optimization problem max s
t
l where and xn indicates the segment is selected
under the assumption of independence between scores this maximization is a standard knapsack problem with a greedy selection strategy
furthermore generated videos rarely contain redundant interesting events hence we do not account for redundancy and diversity

evaluation and discussion to demonstrate the effectiveness of our proposed video marization approach we evaluated and compared it with state of the art methods
we carried on experiments on three video datasets summe interestingness modular dpp video mmr tvsum web image prior livelight dpplstm and random form
following we compute f socre against man summaries for evaluation according to temporal overlap comparing to computed ones
table
we compare our approach with state of the art methods on summe tvsum and ugsum datasets
dataset method f score fig

quantitative results with different features and larity measurement
interestingness submodular dpp vslstm dpplstm video mmr framerank ours livelight web image prior tvsum vslstm dpplstm framerank ours random uniform framerank framerank ours















summe tvsum ugsum

results table summarizes the performance of our methods and trasts to those attained by prior work
the highlighted bers indicate framerank obtains the best performance in the corresponding setting
we achieve the highest overall f score of
on summe dataset and
on tvsum dataset the previous state of the art published was
and
spectively
furthermore we also carry on experiments on our dataset
ours is superior to the other two methods where f score is

it shows not only the fectiveness of the proposed framerank method but also the reliability of the proposed temporal segmentation method by comparing framerank and uniform framerank methods
it proves our method is able to nd important segments to produce an informative summary
the results demonstrate the proposed method can create video summaries closer to human level performance than other methods
meanwhile it is interesting to see our result is better than all the supervised methods
we analyze there are no standard rules to dene what important content is to summarize video
thus human generated summaries may be quite different from each other due to different human perception and personal experience
we believe training data for user generated video tion is not sufcient for supervised methods and the ated model is not able to characterize the property to marize videos
moreover since tvsum is slightly different from summe and where it only contains egories of videos thus in theory the characteristic of sum should be suitable for supervised methods to learn video structure while our framerank still performs better
fore we have reasons to believe a good unsupervised method is more appropriate for user generated video summarization


analysis and discussion temporal video segmentation
we analyze the performance gained by different temporal segmentation methods figure
we compare our kl divergence based temporal tation approach with the following methods joint temporal segmentation jts uniform segmentation based temporal segmentation kts superframe motion based temporal segmentation keyframe marization highest scored frames disregarding the temporal segment process
we employ the same segment selection method framerank and summary generation method
figure shows our temporal segmentation method yields a better performance
it demonstrates the signicance of structural analysis in video summarization
first based summarization is better than keyframes
we lieve summarization annotation is segment based rather than frame based because it is quite expensive and difcult for participants to give scores to individual frames
in fact ticipants were not required to select a segment during tation
moreover we nd our approach has a greater vantage on summe and datasets and close sults for tvsum may be caused by similar videos
thus it demonstrates segment based summary is in better agreement with human perception and produces more reasonable maries because segments contain motion information paring to keyframes
furthermore it also demonstrates our approach to cluster semantically similar frames matches ter with human perception
it shows using such a grouping is indeed more semantically logical
therefore the tal results prove the superiority of our segmentation approach which is capable of generating meaningful summaries
feature and similarity metrics
we investigate the portance and reliability of different features and similarity metrics in the framerank approach
in figure we show the performance gained by using different features deep visual feature deep semantic feature label embedding and label and different similarity metrics euclidean cosine kl vergence and label overlap
label overlap is the textrank
as could be expected deep semantic features with the kl y
j
lee j
ghosh and k
grauman discovering portant people and objects for egocentric video rization in cvpr june pp

z
lu and k
grauman story driven summarization for egocentric video in cvpr
m
gygli h
grabner and l
v
gool video rization by learning submodular mixtures of objectives in cvpr
t
yao t
mei and y
rui highlight detection with pairwise deep ranking for rst person video tion in cvpr june
k
zhang w
l
chao f
sha and k
grauman mary transfer exemplar based subset selection for video summarization cvpr
k
zhang w
l
chao f
sha and k
grauman video summarization with long short term memory in eccv pp

a
khosla r
hamid c
j
lin and n
sundaresan large scale video summarization using web image ors in cvpr june pp

g
kim l
sigal and e
p
xing joint summarization of large scale collections of web images and videos for storyline reconstruction in cvpr
d
potapov m
douze z
harchaoui and c
schmid in eccv category specic video summarization pp

z
lei k
sun q
zhang and g
qiu user video marization based on joint visual and semantic afnity graph in acm multimedia workshop on vision and language integration meets multimedia fusion
k
simonyan and a
zisserman very deep tional networks for large scale image recognition in iclr
o
russakovsky j
deng h
su j
krause s
satheesh s
ma z
huang a
karpathy a
khosla m
stein a
c
berg and f
li imagenet large scale visual recognition challenge
s
kullback and r
a
leibler on information and sufciency ann
math
statist
pp

q
zhang and g
qiu bundling centre for landmark image discovery in acm icmr
r
mihalcea and p
tarau textrank bringing order into text in conference on empirical methods in ural language processing pp

y
li and b
merialdo multi video summarization based on video mmr in wiamis april
fig

quantitative results of different temporal segmentation methods
kts and jts are short for kernel and joint based temporal segmentation methods respectively
divergence metric performs the best
first in most cases tures with semantic information are better than deep visual feature alone in summarizing video
due to similar contents in tvsum deep visual feature achieves relatively good sults
furthermore it is interesting to observe label ding feature has a comparable result to others except
furthermore the kl divergence has a better bility in measuring information loss when using one frame to represent others
hence it also proves our denition of portance of frames is reasonable and effective

conclusion we introduce a new benchmark for generated video summarization
we have proposed a new unsupervised method for video summarization
with a novel dense neighbor based clustering method our approach rst partitions video into segments based on the deep semantic similarity of frames
we then develop a graph based ing method framerank to rank these segments
finally we sample segments with high information scores to generate video summary
we show our framerank method achieved results which are superiority to state of the art methods

acknowledgement the author acknowledges the nancial support from the national doctoral innovation centre ningbo education reau ningbo science and technology bureau and the versity of nottingham
this work was also supported by the uk engineering and physical sciences research cil grant number ep

references m
gygli h
grabner h
riemenschneider and l
v
in creating summaries from user videos gool eccv pp

y
song j
vallmitjana a
stent and a
jaimes sum summarizing web videos using titles in cvpr pp


