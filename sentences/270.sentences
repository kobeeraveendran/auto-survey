felix flexible text editing through tagging and insertion jonathan mallinson university of edinburgh j

ac
aliaksei severyn google research eric malmi google research severyn emalmi
com guillermo garrido google research r a m l c
s c v
v i x r a abstract we present felix a exible text editing approach for generation designed to derive the maximum benet from the ideas of coding with bi directional contexts and supervised pre training
in contrast to tional sequence to sequence models felix is efcient in low resource settings and fast at inference time while being capable of modeling exible input output tions
we achieve this by decomposing the text editing task into two sub tasks tagging to decide on the subset of input tokens and their order in the output text and insertion to the missing tokens in the output not present in the input
the tagging model employs a novel pointer mechanism while the insertion model is based on a masked language model
both of these models are chosen to be autoregressive to guarantee faster inference
felix performs favourably when compared to recent text editing methods and strong baselines when evaluated on four nlg tasks sentence fusion machine translation matic post editing summarization and text simplication
introduction the idea of text when coupled with the self supervised pre training of deep transformer networks on large text corpora have dramatically changed the landscape in natural language derstanding
bert devlin et al
and its successive renements roberta liu et al
albert lan et al
implement this recipe and have signicantly pushed the state of the art on multiple nlu benchmarks such as glue wang et al
and squad rajpurkar et al

more recently the idea of using masked or lling style objective for model pretraining has equal contribution
work completed during internship at google research
figure felix transforms the source the big very loud cat into the target text the very big old cat
also been applied to sequence to sequence tasks and has signicantly pushed the state of the art on a number of text generation tasks
g mit chan et al
mass song et al
rothe et al
bart lewis et al
and raffel et al

while sequence to sequence frameworks offer a generic tool for modeling almost any kind of text to text transduction there are still many world tasks where generating target texts pletely from scratch as it is done with the approaches can be unnecessary
this is especially true for monolingual settings where input and put texts have relatively high degrees of overlap
in such cases a natural approach is to cast the task of conditional text generation into a text editing task where the model learns to reconstruct target texts by applying a set of edit operations to the inputs
typically the set of edit operations is xed and dened ahead of time which on one hand limits the exibility of the model to reconstruct arbitrary output texts from their inputs but on the other leads taggingkeep keepins keep del keepthe very big repl loud mask cattaggerthe big very loud catpointerinsertionmasked language modelthe very big repl loud old cat to higher sample efciency as the limited set of lowed operations signicantly reduces the search space
based on this observation text editing proaches have recently re gained signicant est gu et al
dong et al
awasthi et al
malmi et al

in this paper we present a novel text editing framework felix which is heavily inspired by the ideas of bi directional decoding slot and self supervised pre training
in particular we have designed felix with the following requirements in mind sample efciency
training a high precision text generation model typically requires large amounts of high quality supervised data
supervised techniques based on text have been shown to provide a crucial advantage in resource settings
hence we focus on approaches able to benet from already existing pre trained language models such as bert where the nal model is directly ne tuned on the down stream task
fast inference time
achieving low latencies when serving text generation models typically quires specialized hardware and nding a trade off between model size and accuracy
one of the jor reasons for slow inference times is that text generation models typically employ an sive decoder i
e
output texts are generated in a sequential non parallel fashion
to ensure faster inference times we opt for keeping felix fully autoregressive
even though it is well known that autoregressive decoding leads to higher accuracy scores fast inference was one of our top priority features for felix
flexible text editing
while simplifying the learning task text editing models are not as erful as general purpose sequence to sequence proaches when it comes to modeling arbitrary output text transductions
hence we strive to strike a balance between the complexity of learned edit operations and the percentage of input output formations the model can capture
ordering
the target words not present in the source are represented by the generic slot predictions to be by the insertion model
to benet from self supervised pre training we chose our insertion model to be fully compatible with the bert tecture such that we can easily re use the publicly available pre trained checkpoints
by decomposing text editing tasks in this way we redistribute the complexity load of generating an output text between the two models the source text already provides most of the building blocks required to reconstruct the target which is dled by the tagging model
the missing pieces are then by the insertion model whose job becomes much easier as most of the output text is already in place
moreover such a two step proach is the key for being able to use completely non autoregressive decoding for both models and still achieve competitive results compared to fully autoregressive approaches
we evaluate felix on four distinct text eration tasks sentence fusion text tion summarization and automatic post editing for machine translation and compare it to recent text editing and approaches
each task is unique in the editing operations required and the amount of training data available which helps to better quantify the value of solutions we have integrated into
model description felix decomposes the conditional probability of generating an output sequence y from an input as follows where the two terms correspond to the tagging and the insertion model
term ym which denotes an intermediate sequence with masked spans ym fed into the insertion model is constructed from yt a sequence of tags assigned to each input token and a permutation which reorders the input tokens
given this factorization both models can be trained independently
felix
to meet the aforementioned tum we propose to tackle text editing by ing it into two sub problems tagging and insertion see fig

our tagger is a transformer based network that implements a novel pointing nism vinyals et al

it decides which source tokens to preserve and in which order they appear in the output thus allowing for arbitrary word
tagging the tag sequence yt is constructed as follows source tokens that must be copied are assigned the keep tag tokens not present in the output are marked by the delete tag token spans present in the output but missing from the input are modeled code is publicly available at url to be added src the keep ym the pred the keep ym the pred the cat big del very loud del delins keep repl big very loud mask mask noisy large del delins keep mask large repl big very loud mask noisy cat cat del mask inll mask mask cat pad pad cat figure an example of two ways to model inputs to the insertion model via token masking mask or inlling inll
in the former case the tagging model predicts the number of masked tokens ins while in the latter it is delegated to the insertion model which replaces the generic ins tag with a xed length span length
note that the insertion model predicts a special pad symbol to mark the end of the predicted span
replacements are modeled by keeping the deleted spans between the repl tags
this transforms the source text the big very loud cat into the target the noisy large cat
note that for simplicity this example does not include reordering
root the big very loud cat figure pointing mechanism to transform the big very loud cat into the very big cat
by the insert ins
this tag is then converted into masked token spans by the insertion model
word reordering is handled by a specialized pointing mechanism

pointing felix explicitly models word reordering to allow for larger global edits as well as smaller local changes such as swapping nearby words john and mary mary and john
without word ordering step a vanilla editing model based on just tagging such as malmi et al
dong et al
would rst need to delete a span and mary and then insert mary and before john
felix is able to model this without the need for deletions or insertions
i given a sequence and the predicted tags the re ordering model generates a permutation so that from and yt we can reconstruct the tion model input ym
thus we have
we highlight that each is predicted independently in a non regressive fashion
the output of this model is a ries of predicted pointers source token next get token
ym can easily be constructed by daisy chaining the pointers together as seen in fig

as highlighted by this gure felix s reordering process is similar to non projective dependency parsing dozat and manning where head relationships are non autoregressively predicted to form a tree
similarly felix predicts next word relationship and instead forms a sequence
our implementation is based on a pointer work vinyals et al
where an attention mechanism points to the next token
unlike vious approaches where a decoder state attends over an encoder sequence our setup applies attention where source tokens attend to all other source tokens
when constructing the training data there are many possible combinations of and which could produce ym as trivially all source tokens can be deleted and then target tokens inserted
hence we construct the dataset using a greedy method to maximize the number of kept tokens minimize the number of inserted token and minimize the amount of reordering keeping source tokens in continuous sequences where possible
since each token can only point to one other token loops will be formed if the same token is pointed to multiple times
when constructing the dataset we ensure that each token is only pointed to at most once
at inference time a constrained beam search is used to ensure no loops are created

insertion an input to the insertion model ym contains a set of the input tokens in the order determined by the tagging model as well as masked token spans that it needs to
to represent masked token spans we consider two options masking and inlling see fig

in the former case the tagging model predicts how many tokens need to be inserted by specializing the insert tag into ins k where k translates the span into k mask tokens
for the inlling case the tagging model predicts a generic ins tag which signals the insertion model to inll it with a span of tokens of an arbitrary length
if we were to use an autoregressive tion model the natural way to model it would be to run the decoder until it decides to stop by producing a special stop symbol e

eos
since by design we opted for using a non autoregressive model to represent variable length insertions we use a pad symbol to pad all insertions to a xed length quence of mask tokens
note that we preserve the deleted span in the input to the insertion model by enclosing it between and tags
even though this introduces an undesired discrepancy between the pretraining and ne tuning data that the insertion model observes we found that making the model aware of the text it needs to replace signicantly boosts the accuracy of the insertion model

felix as insertion transformer another intuitive way to picture how felix works is to draw a connection with the insertion former stern et al

in the latter the decoder starts with a blank output text canvas and atively inlls it by deciding which token and in which position it should appear in the output
tiple tokens can be inserted at a time thus ing sub linear decoding times
in contrast felix trains a separate tagger model to the output canvas with the input tokens in a single step
as the second and nal step felix does the insertion into the slots predicted by the tagger
this is equivalent to a single decoding step of the insertion former
hence felix requires signicantly fewer namely two decoding steps than insertion former and through the tagging insertion position of the task it is straightforward to directly take advantage of existing pre trained masked guage models
all our experiments the maximum lengths of was sufcient to represent over of insertion spans from the training set
the text edit tasks reported in this paper this sponds to more than of the output tokens
model implementation
tagging model tagger
our tagger is a layer bert base model
tags are predicted by applying a single feed forward layer to the output of the encoder hl as such t argmaxf hl
pointer
the input to the pointer layer at position i is a combination of the encoder hidden state hl i the embedding of the predicted tag and the positional embedding as follows hl i
i next token prediction uses a pointer network attending over all hidden states as such i i attention between hidden states is calculated ing a query key network with a scaled dot product k softmax qkt dk where k and q linear projections of and dk is the hidden dimension
we found the optional inclusion of an additional transformer layer prior to the query projection increased the performance on movement heavy datasets
when realizing the pointers we use a strained beam search where we ensure no loops are created
we note that loops only form in of the

insertion model similar to the tagger our insertion model is also based on a layer bert base and is initialized from a public pretrained checkpoint
when using the masking approach the insertion model is essentially solving a masked language modeling task and hence we can directly take vantage of the bert style pretrained checkpoints
this is a considerable advantage especially in the low resource settings as we do not waste training data on learning a language model component of the text editing model
with the task tion where tagging and insertion can be trained et al
have shown that models trained with masked language modeling objectives lose positional tion a property we consider important for reordering
x the beam size to
for a batch size of and mum sequence length of beam search incurs an additional penalty of about when run on a xeon

disjointly it essentially comes for
switching from masking approach to inlling shifts the complexity of modeling the length of the inserted token spans from the tagging model to the insertion model
depending on the amount of training data available it provides interesting trade offs between the accuracy of the tagging and insertion models
we explore this more in detail in sec


experiments we evaluate felix on four distinct text editing tasks sentence fusion text simplication marization and automatic post editing for chine translation
in addition to reporting ously published results for each task we also pare to a recent text editing approach ger malmi et al

we follow their setup and set the phrase vocabulary size to and run all experiments using their most accurate gressive model
for all tasks we run an ablation study examining the effect of an open vocabulary with no reordering felixinsert and a xed with reordering model felixpoint
task analysis
the chosen tasks cover a diverse set of edit operations and a wide range of dataset sizes varying from under data points to over million
table provides dataset statistics including the size sentence length and the tion error rate ter snover et al
between the source and target sentences
we use ter to highlight unique properties of each task
the marization dataset is a deletion heavy dataset with the highest number of deletion edits and the largest reduction in sentence length
it contains moderate amounts of substitutions and large number of shift edits caused by sentence re ordering
both the simplication and post editing datasets contain a large number of insertions and substitutions while simplication contains a greater number of tion edits
post editing however is a much larger dataset covering multiple languages
sentence sion has the lowest ter indicating that obtaining the fused targets requires only a limited number of local edits
however these edits require modeling still ne tune the insertion model to accommodate for the additional token spans between the repl and such that it learns to condition the prediction of masked tokens on those spans
simplicity we use the lasertagger phrase lary
the discourse relation between the two input tences since a common edit type is predicting the correct discourse connective geva et al

additionally we provide coverage statistics and the percentage of training instances for which an editing model can fully reconstruct the output from the input of our proposed model in table trasting it against lasertagger
as both felix and felixinsert use an open vocabulary they cover of the test data whereas felixpoint and lasertagger often cover less than half
for every dataset felixpoint covers a signicantly higher percentage than lasertagger with the noticeable case being summarization where there is a increase in coverage
this can be explained by the high number of shift edits within tion table something felixpoint is explicitly designed to model
we found that the difference in coverage between felixpoint and ger correlates strongly correlation

with the number of shift edits
comparing the erage percentage of masks inserted we see that felix always inserts less masks than felixinsert since no word reordering requires more deletions and insertions for the latter

sentence fusion sentence fusion is the problem of fusing dent sentences into a coherent output
data
we use the balanced wikipedia portion of the discofuse dataset geva et al
and also study the effect of the training data size by ing four increasingly smaller subsets of discofuse
and
data points
metrics
following geva et al
we report two metrics exact score which is the percentage of exactly correctly predicted fusions and sari xu et al
which computes the average scores of the added kept and deleted n grams
results
table includes additional bert based baselines from rothe et al
and from malmi et al

for all felix variants we further break down the scores based on how the insertion is modelled via token masking mask or inlling inll
ditionally to better understand the contribution of tagging and insertion models to the nal accuracy we report scores assuming oracle insertion and ging predictions respectively highlighted rows
dataset size lsrc ltgt ter ins del sub shft post editing simplication summarization sentence fusion m
k
k

m
























table statistics across tasks size of the dataset size source length in tokens lsrc target length in tokens ltgt and ter score snover et al
along with its components including number of insertions ins deletions del substitutions sub and shifts shft
dataset coverage mask lasertagger felixpoint felixinsert felix postediting simplication summarization sentence fusion















table coverage and mask statistics
coverage is the percentage of training examples that the models are able to generate
both felixinsert and felix have full coverage of all test sets
mask is the ratio of masked tokens to target tokens
the results show that felix and its variants nicantly outperform the baselines lasertagger and across all data conditions
der the condition achieves the highest sari and exact score however for all other data conditions felix outperforms
the results highlights that both models form poorly with less than
datapoints whereas all editing models achieve relatively good performance
when comparing felix variants we see that in the case felixinsert outperforms felix however we note that for felixinsert we lowed malmi et al
and used an additional sentence re ordering tag a hand crafted feature lored to discofuse which swaps the sentence order
it was included in malmi et al
and resulted in a signicant exact increase
however in the low resource setting felix outperforms lixinsert suggesting that felix is more data efcient than felixinsert
ablation
we rst contrast the impact of the sertion model and the tagging model noticing that for all models inll achieves better tagging scores and worse insertion scores than mask
secondly felix achieves worse tagging scores but better insertion scores than felixinsert
this lights the amount of pressure each model is ing by making the tagging task harder such as the inclusion of reordering the insertion task comes easier
finally the insertion models even under very low data conditions achieve impressive performance
this suggests that under low data conditions most pressure should be applied to the insertion model

simplication sentence simplication is the problem of fying sentences such that they are easier to stand
simplication can be both lexical replacing or deleting complex words or syntactic replacing complex syntactic constructions
data
training is performed on wikilarge zhang and lapata a large tion corpus which consists of a mixture of of three wikipedia simplication datasets collected by kauchak woodsend and lapata zhu et al

the test set was created by xu et al
and consists of source sentences taken from wikipedia and then simplied using amazon mechanical turkers to create eight references per source sentence
metrics
we report sari as well as breaking it down into each component keep delete and add as we found the scores were uneven across these metrics
we include a readability metrics fkgl and the percentage of unchanged source sentences copy
results
in table we compare against three state of the art smt based simplication systems pbmt r wubben et al
a phrase based machine translation model
hybrid narayan and gardent a model which performs tence spiting and deletions and then simplies with pbmt r
sbmt sari xu et al
a syntax based translation model trained on ppdb and which is then tuned using sari
four neural approaches dress zhang and model insertion mask inll oracle tag ins sari exact

lasertagger felixpoint felixinsert felix























































































table sentence fusion results on discofuse using the full and subsets
and
of the training set
we report three model variants felixpoint felixinsert and felix using either mask or inll insertion modes
rows in gray background report scores assuming oracle tagging tag or insertion ins predictions
wikilarge sari add del keep fkgl copy
summarization




sbmt sari




pbmt r hybrid nts dress dress ls editnts lasertagger











































felixpoint felixinsert felix

















table sentence simplication results on wikilarge
ata an lstm based trained with reinforcement learning dress ls a variant of dress which has an additional lexical tion component nts nisioi et al
and model
dmass zhao et al
a transformer based model enhanced with cation rules from ppdb
and two neural editing models lasertagger and editnts
felix achieves the highest overall sari score and the highest sari keep score
in addition all ablated models achieve higher sari scores than lasertagger
while felixinsert achieves a higher sari score than editnts felixpoint does not this can in part be explained by the large number of substitutions and insertions within this dataset with felixpoint achieving a low add score
data
we use the dataset from toutanova et al
which contains short input texts one or two sentences and one or more human written summaries resulting in total training pairs
the human experts were not restricted to just ing words when generating a summary but were allowed to also insert new words and reorder parts of the sentence which makes this dataset larly suited for abstractive summarization models
metrics
in addition to sari we include rouge l and as these metrics are monly used in the summarization literature
results
the results in table show that felix achieves the highest sari rouge and bleu score
all ablated models achieve higher sari scores than all other models
interestingly the ference between felixpoint and lasertagger is modest even though felixpoint covers twice as much data as lasertagger
with ger being trained on data points and point trained on
in table we see that lasertagger and felixpoint perform larly under such low data conditions

post editing automatic post editing ape is the task of matically correcting common and repetitive errors found in machine translation mt outputs
sari add del keep rouge bleu lasertagger








felixpoint felixinsert felix

















table summarization
copy is not included as all models copied less than of the time
copy transformer lasertagger levt sota lee et al
felixpoint felixinsert felix ter bleu















data
ape approaches are trained on triples the source sentence the machine translation output and the target translation
we experiment on the en de it post editing where the goal is to improve the output of an mt system that translates from english to german and is applied to documents from the it domain
we follow the procedures introduced in junczys dowmunt and grundkiewicz and train our models using two synthetic corpora of m and k examples merged with a corpus of k real examples sampled times
the models that we study pect a single input string
to obtain this and to give the models a possibility to attend to the english source text we append the source text to the man translation separated by a special token
since the model input consists of two different languages we use the multilingual bert for the proposed methods and for lasertagger
metrics
we follow the evaluation procedure of ape task and report translation error rate ter snover et al
as the primary metric and bleu as a secondary metric
results
we consider the following baselines copy which is a competitive baseline given that the required edits are typically very limited lasertagger malmi et al
shtein transformer levt gu et al
which is a partially autoregressive model that also employs a deletion and an insertion mechanisms a standard transformer evaluated by gu et al
and a state of the art method by lee et al

unlike the other methods the last baseline is tailored specically for the ape task by ing the source separately and conditioning the mt output encoding on the source encoding lee et al


org ape task
html
googleapis
com bert
zip table ende post editing results
the results are shown in table
first we can see that using a custom method lee et al
brings signicant improvements over generic text transduction methods
second felix performs very competitively yielding comparative results to levenshtein transformer gu et al
which is a partially autoregressive model and performing the other generic models in terms of ter
third felixinsert performs considerably worse than felix and felixpoint suggesting that the pointing mechanism is important for the ape task
this observation is further backed by ble which shows that without the pointing anism the average proportion of masked tokens in a target is
whereas with pointing it is only

therefore removing the pointing nism shifts the responsibility too heavily from the tagging model to the insertion model
related work models sutskever et al
have been applied to many text generation tasks that can be cast as monolingual translation but they fer from well known drawbacks wiseman et al
they require large amounts of training data and their outputs are difcult to control
ever input and output sequences have a large lap it is reasonable to cast the problem as a text editing task rather than sequence to sequence generation
ribeiro et al
argued that the general problem of string transduction can be reduced to sequence labeling
their approach applied only to character deletion and insertion and was based on simple patterns
lasertagger malmi et al
is a general approach that has been shown to perform well on a number of text editing tasks but it has two limitations it does not allow for arbitrary reordering of the input tokens and sertions are restricted to a xed phrase vocabulary that is derived from the training data
similarly itnts dong et al
and pie awasthi et al
are two other text editing models that predict tokens to keep delete and add which are oped specically for the tasks of text simplication and grammatical error correction respectively
in contrast to the aforementioned models felix lows more exible rewriting using a pointer work that points into the source to decide which tokens should be preserved in the output and in which order
pointer networks have been previously proposed as a way to copy parts of the input in hybrid sequence to sequence models
gulcehre et al
and nallapati et al
trained a pointer network to specically deal with out of vocabulary words or named entities
see et al
hybrid proach learns when to use the pointer to copy parts of the input
chen and bansal proposed a summarization model that rst selects salient tences and then rewrites them abstractively using a pointer mechanism to directly copy some out vocabulary words
these methods still typically require large amounts of training data and they are inherently slow at inference time due to sive decoding
previous approaches have proposed alternatives to autoregressive decoding gu et al
lee et al
chan et al
wang and cho
instead of the left to right autoregressive decoding insertion transformer stern et al
and blm shen et al
generate the output quence through insertion operations whereas enshtein transformer levt gu et al
ditionally incorporates a deletion operation
these methods produce the output iteratively while felix requires only two steps tagging and insertion
the differences between the proposed model felix its ablated variants and a selection of lated works is summarized in table
conclusions and future work we have introduced felix a novel approach to text editing by decomposing the task into tagging and insertion which are trained independently
such separation allows us to take maximal benet from the already existing pretrained masked lm models
felix works extremely well in low resource tings and it is fully non autoregressive which favors faster inference
our empirical results demonstrate that it delivers highly competitive performance type non gressive pretrained reordering open vocab text edit transformer copying levt pie editnts lasertagger felixinsert felixpoint felix table model comparison along ve dimensions model type whether the decoder is non autoregressive levt is partially autoregressive whether the model uses a pretrained checkpoint a word reordering anism uses a reordering pretraining task but it does not have a dedicated copying mechanism for ing reordering and whether the model can generate any possible output open vocab
when compared to strong baselines and other recent text editing approaches
in the future work we plan to investigate the following ideas how to effectively share senations between the tagging and insertion models using a single shared encoder how to perform joint training of insertion and tagging models stead of training them separately iii strategies for unsupervised pre training of the tagging model which appears to be the bottleneck in highly resource settings and iv distillations recipes
acknowledgments we thank aleksandr chuklin daniil mirylenka ryan mcdonald and sebastian krause for useful discussions running early experiments and paper suggestions
references abhijeet awasthi sunita sarawagi rasna goyal sabyasachi ghosh and vihari piratla

allel iterative edit models for local sequence duction
in proceedings of the conference on empirical methods in natural language processing and the international joint conference on ral language processing emnlp ijcnlp pages
william chan nikita kitaev kelvin guu mitchell stern and jakob uszkoreit

kermit ative insertion based modeling for sequences
yen chun chen and mohit bansal

fast tive summarization with reinforce selected sentence rewriting
in proceedings of the annual ing of the association for computational linguistics volume long papers pages
jacob devlin ming wei chang kenton lee and kristina toutanova

bert pre training of deep bidirectional transformers for language ing
arxiv preprint

yue dong zichao li mehdi rezagholizadeh and jackie chi kit cheung

editnts an ral programmer interpreter model for sentence plication through explicit editing
arxiv preprint

timothy dozat and christopher d
manning

deep biafne attention for neural dependency in international conference on learning ing
representations iclr toulon france april conference track proceedings
review
net
mor geva eric malmi idan szpektor and jonathan berant

discofuse a large scale dataset for discourse based sentence fusion
arxiv preprint

jiatao gu james bradbury caiming xiong tor o
k
li and richard socher

autoregressive neural machine translation
in national conference on learning representations
jiatao gu changhan wang and junbo zhao
in h
wallach
levenshtein transformer
h
larochelle a
beygelzimer f
buc e
fox and r
garnett editors advances in neural information processing systems pages
curran associates inc
caglar gulcehre sungjin ahn ramesh nallapati bowen zhou and yoshua bengio

in annual meeting ing the unknown words
of the association for computational linguistics acl pages
association for tational linguistics acl
marcin junczys dowmunt and roman grundkiewicz

log linear combinations of monolingual and bilingual neural machine translation models for in proceedings of the first tomatic post editing
conference on machine translation pages berlin germany
association for computational linguistics
david kauchak

improving text simplication language modeling using unsimplied text data
in proceedings of the annual meeting of the ation for computational linguistics volume long papers pages
zhenzhong lan mingda chen sebastian goodman kevin gimpel piyush sharma and radu soricut

albert a lite bert for self supervised ing of language representations
jason lee elman mansimov and kyunghyun cho

deterministic non autoregressive neural in quence modeling by iterative renement
ceedings of the conference on empirical ods in natural language processing pages
wonkee lee junsu park byung hyun go and jong hyeok lee

transformer based matic post editing with a context aware encoding approach for multi source inputs
arxiv preprint

mike lewis yinhan liu naman goyal jan ghazvininejad abdelrahman mohamed omer levy ves stoyanov and luke zettlemoyer

bart denoising sequence to sequence pre training for natural language generation translation and comprehension
yinhan liu myle ott naman goyal jingfei du dar joshi danqi chen omer levy mike lewis luke zettlemoyer and veselin stoyanov

roberta a robustly optimized bert pretraining proach
eric malmi sebastian krause sascha rothe daniil mirylenka and aliaksei severyn

encode tag realize high precision text editing
in ings of the conference on empirical methods in natural language processing and the national joint conference on natural language cessing emnlp ijcnlp pages
ramesh nallapati bowen zhou cicero dos santos caglar gulcehre and bing xiang

tive text summarization using sequence to sequence in proceedings of the rnns and beyond
signll conference on computational natural guage learning pages
shashi narayan and claire gardent

hybrid plication using deep semantics and machine lation
sergiu nisioi sanja stajner simone paolo ponzetto and liviu p dinu

exploring neural text in proceedings of the plication models
nual meeting of the association for computational linguistics volume short papers pages
colin raffel noam shazeer adam roberts katherine lee sharan narang michael matena yanqi zhou wei li and peter j
liu

exploring the limits of transfer learning with a unied text to text former
pranav rajpurkar jian zhang konstantin lopyrev and percy liang

squad questions for machine comprehension of text
joana ribeiro shashi narayan shay b cohen and xavier carreras

local string transduction as sequence labeling
in proceedings of the national conference on computational linguistics pages
kristian woodsend and mirella lapata

ing to simplify sentences with quasi synchronous grammar and integer programming
in proceedings of the conference on empirical methods in natural language processing pages
association for computational linguistics
sander wubben antal van den bosch and emiel mer

sentence simplication by monolingual machine translation
in proceedings of the nual meeting of the association for computational linguistics volume long papers pages jeju island korea
association for tional linguistics
wei xu courtney napoles ellie pavlick quanze chen and chris callison burch

optimizing statistical machine translation for text simplication
transactions of the association for computational linguistics
xingxing zhang and mirella lapata

tence simplication with deep reinforcement ing
arxiv preprint

xingxing zhang and mirella lapata

tence simplication with deep reinforcement ing
in proceedings of the conference on pirical methods in natural language processing pages
association for computational guistics
sanqiang zhao rui meng daqing he saptono andi and parmanto bambang

integrating former and paraphrase rules for sentence tion
arxiv preprint

zhemin zhu delphine bernhard and iryna gurevych

a monolingual tree based translation model in proceedings of the for sentence simplication
international conference on computational guistics pages
association for tational linguistics
sascha rothe shashi narayan and aliaksei leveraging pre trained checkpoints arxiv preprint eryn

for sequence generation tasks


abigail see peter j liu and christopher d manning

get to the point summarization with generator networks
in proceedings of the nual meeting of the association for computational linguistics volume long papers pages
tianxiao shen victor quach regina barzilay and tommi jaakkola

blank language models
matthew snover bonnie dorr richard schwartz nea micciulla and john makhoul

a study of translation edit rate with targeted human annotation
in proceedings of association for machine tion in the americas volume
kaitao song xu tan tao qin jianfeng lu and yan liu

mass masked sequence to sequence pre training for language generation
mitchell stern william chan jamie kiros and jakob uszkoreit

insertion transformer flexible quence generation via insertion operations
arxiv preprint

i sutskever o vinyals and qv le

sequence to sequence learning with neural networks
advances in nips
kristina toutanova chris brockett ke m tran and saleema amershi

a dataset and evaluation metrics for abstractive compression of sentences and short paragraphs
oriol vinyals meire fortunato and navdeep jaitly

pointer networks
in advances in neural formation processing systems pages
elena voita rico sennrich and ivan titov

the bottom up evolution of representations in the transformer a study with machine translation and language modeling objectives
arxiv preprint

alex wang and kyunghyun cho

bert has a mouth and it must speak bert as a markov arxiv preprint random field language model


alex wang amanpreet singh julian michael felix hill omer levy and samuel r
bowman

glue a multi task benchmark and analysis platform for natural language understanding
sam wiseman stuart shieber and alexander rush

learning neural templates for text generation
in proceedings of the conference on cal methods in natural language processing pages

