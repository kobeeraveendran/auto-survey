dissecting the components and factors of neural text generation khyathi raghavi chandu alan w black language technologies institute carnegie mellon university kchandu
cmu
edu t c o l c
s c v
v i x r a abstract neural text generation metamorphosed into several critical natural language applications ranging from text completion to free form rative generation
generating natural language has fundamentally been a human attribute and the advent of ubiquitous nlp applications and virtual agents marks the need to impart this skill to machines
there has been a colossal research effort in various frontiers of neural text generation including machine translation summarization image captioning storytelling
we believe that this is an excellent ture to retrospect on the directions of the eld
specically this paper surveys the tal factors and components relaying task nostic impacts across various generation tasks such as storytelling summarization tion
in specic we present an tion of the imperative techniques with respect to learning paradigms pretraining modeling approaches decoding and the key challenges
thereby we hope to deliver a one stop nation for researchers in the eld to facilitate a perspective on where to situate their work and how it impacts other closely related tasks
introduction text generation is the task of producing written or spoken narrative from structured or unstructured data
the overarching goal is the seamless machine communication by presenting a wealth of data in a way we can comprehend
with spect to the modeling approaches there are three main paradigms in generating text based on the schema of input and output i text to text ii data to text none to text
table presents the categorization of different tasks based on this paradigm
these several tasks deserve undivided attention and accordingly they have been ily dissected studied and surveyed in the recent past
for instance independent and exclusive generation paradigm text to text data to text none to text task dialog machine translation style transfer summarization image captioning visual storytelling speech recognition table to text knowledge bases to text knowledge bases language modeling input conversation history source language style text single multiple documents image images audio table null output next response target language style text summary descriptive text descriptive text text text text sequence of text table paradigms of tasks in text generation
for the purposes of compactness we include to text paradigm within data to text
veys are periodically conducted on summarization lin and ng allahyari et al
nenkova and mckeown tas and kiyani knowledge to text generation dblp conf inlg dblp conf naacl koncel machine translation chu and wang dabre et al
chand slocum dialog sponse generation liu et al
montenegro et al
ramesh et al
chen et al
storytelling narrative generation tong et al
togelius et al
image captioning hossain et al

to dig deeper into task specic approaches that are foundational as well as in the bleeding edge of research
while these are tremely necessary often the focus on techniques that are benecial to other tightly coupled tasks are overlooked
the goal of this survey is to cus on these key components that are task agnostic to improve the ensemble of tasks in neural text generation
there have been several studies conducted on surveying text generation
perera and nand present a detailed overview of information ory based approaches
iqbal and qureshi primarily focus on core modeling approaches pecially vaes kingma and welling and gans goodfellow et al

gatt and mer elaborated on tasks such as captioning style trasfer
with a primary focus on data figure components and factors in approaches for text generation
components are laid out in grey boxes and factors are laid out on the left in dotted lines
text tasks
controllability aspect is explored by prabhumoye et al

the workclosest to this is by lu et al
who perform an empirical study on the core more modeling approaches only
in contrast to these this paper focuses on task nostic components and factors capable of pushing the ensemble of tasks forward
figure presents the various components and factors that are tant to study in neural text generation which are elaborated in this paper
modeling approaches
core modeling paradigms supervised learning most generation proaches in this setting use maximum likelihood objective for training sequence generation with a sequential multi label cross entropy
log y t t however there is an inherent inconsistency in posure to ground truth text between training and inference stages when using teacher forcing during training
this leads to the problem of exposure bias ranzato et al

during training the token in the current time step is predicted conditioned on ground truth prex correcting the course tive of what word is predicted by the model
ever during inference the same is conditioned on generated prex in the absence of ground truth x
this problem becomes severe with the ing length of the output
a solution to address this issue is scheduled sampling bengio et al
which mixes teacher forced embeddings and model predictions from previous time step
reinforcement learning the main issue with the supervised learning approach for text ation is the mismatch between maximum hood objective that is optimized and metrics for text quality
reinforcement learning addresses this mismatch by directly optimizing end metrics which could be non differentiable
typically policy ent algorithms are used to optimize for bleu score directly via reinforce
the objective is shown low which is the sum of log probabilities multiplied by the reward score
the reward score itself is puted as the expected bleu score
rt logp t x t however computing bleu before every update is not computationally efcient to incorporate in the training procedure
another problem is the inherent inefciency of the metric itself i
e bleu is not the best measure to evaluate text quality
in practice usually the policy network is usually trained with maximum likelihood objective before optimizing for bleu score
adversarial learning the third paradigm is adversarial learning comprising of competing jectives
the mismatch in training and inference stages is addressed using professor forcing lamb et al
with adversarial domain adaptation to bring the behavior of the training and sampling close to each other
this is done by sharing the parameters between teacher forcing network and evaluation humanautomaticemulatedfluency coverage length content planning speedinference decoding autoregressivenon autoregressivesamplingsearchsupervisedcore modeling encoder decoder adversarialvariationalreinforcementgenerative pretrainingsupervised the free running network
apart from this the two main components are a generator and a nator
discriminator here is optimized to correctly classify the sequence as belonging to free running behavior or teacher forced behavior
the tor has two goals i maximize the likelihood of the data fool the discriminator
there are two options with respect to keeping one xed and ing the other closer to the rst
this can be done with respect to either of teacher forcing network or free running network
empirically professor forcing also plays the role of a regularizer
erative adversarial networks gan also gained popularity with respect to this in the recent times
the core idea is that the gradient of the tor guides how to alter the generated data and by what margin in order to make it more realistic
this slight change is apparent in continuous values in comparison to language which is a discrete space
there are several variants adopted to address cic problems such as seqgan to assess partially generated sequence yu et al
maskgan to improve sample quality using text lling fedus et al
and leakgan to model long term pendencies by leaking discriminator information to generator guo et al

the three main challenges researched in this area are discrete sampling the sampling step selecting argmax in language is a non differentiable function
one solution is to replace it with a continuous proximation by adding gumbel noise which is ative log of negative log of a sample from uniform distribution also known as gumbel softmax
mode collapse gans typically face the issue of sampling from specic tokens to cheat nator known as mode collapse
in this way only a subspace of target distribution is learnt by the erator
dp gan addresses this using an explicit diversity promoting reward xu et al

power dynamics between generator and criminator another problem arises when the criminator is trained faster than the generator
this is most often the case the gradient from tor vanishes leading to no real update to generator

pre training recent couple of years have seen a major surge in interest for pre training techniques
while they are primarily focused on language understanding tasks there has been some work targeted for training for generation as well
unilm unied pre trained language model dong et al
is proposed as a pre training mechanism for both natural language understanding and natural guage generation tasks
fundamentally the viously widely used elmo peters et al
constitutes a language model that is left to right and right to left
while gpt radford et al
has an autoregressive left to right language model bert devlin et al
has a bidirectional guage model
unilm is optimized jointly for all of the above objectives along with an additional new lm which is bidirectional encoding followed by unidirectional decoding
depending on the use case unilm can be adopted to use directional lm left to right bidirectional lm attention on all tokens and lm attention on all tokens in previous segment and left context in the current segment
with a similar goal in mind mass song et al
modied ing patterns in input to achieve this
bert and xlnet yang et al
pre train an encoder and gpt pretrains a decoder
this is a framework introduced to pretrain encoder attention decoder gether
encoder masks a sequence of length and the decoder predicts the same sequence of length and every other token is masked
while the idea of jointly training the encoder attention decoder remains the same as in unilm the interesting tribution here is the way masking is utilized to i the bring out the following advantages
kens masked in decoder are the tokens that are not masked in encoder
this complementary ing encourages joint training of encoder decoder
ii encoder supports decoder by extracting ful information from the masked fragments which improves the understanding or nlu capabilities of the model
since a sequence of length is decoded consecutively nlg capability is proved as well
note that when k is the model is closer to bert which is biased to an encoder and when k is the length of sentence the model is closer to gpt which is biased to decoder
similar to unilm bart lewis et al
has a tional encoder and an autoregressive decoder
the underlying model is standard transformer vaswani et al
based neural mt framework
the main difference of bart from mass is that the tokens masked here are not necessarily consecutive
the main idea and the second difference is to corrupt text with arbitrary noise and reconstruct original text
the input is corrupted with the following transformations token masking token deletion ken inlling sentence permutation and document rotation
following this raffel et al
posed as a unifying framework that ties all nlp problems as text generation tasks with a text in and text out paradigm
recently dathathri et al
introduced plug and play language models capable of efciently training fewer parameters to control a huge underlying pretrained model
finetuning these vast models for generative tasks has been studied in style transformers sudhakar et al
and conversational agents dinan et al


decoding strategies the natural next step after pre training and training is decoding
the distinguishing characteristic of generation is the absence of one to one dence between time steps of input and the output thereby introducing a crucial component which is decoding
primarily they can be categorized as i autoregressive and non autoregressive
autoregressive decoding traditional models with this strategy correspond well to the true tributions of words
this mainly comes from specting the conditional dependence property from left to right
the autoregressive techniques can be further viewed as sampling and search techniques
the main disadvantage of this strategy is throttling transformer based models that fall short in cating their training advantages as training can be non sequential and inference holds to be sequential with autoregressive decoding
non autoregressive decoding this line of work primarily addresses two problems that are associated with autoregressive decoding
first by denition there is a conditional independence erty that holds
this leads to the multimodality problem where each time step considers different variants with respect to the entire sequence and these conditions compete with each other
ond the main advantage is the reduction in latency during real time generation
guo et al
dressed this problem in the context of neural chine translation using transformers by copying each of the source inputs to the decoder either formly or repeatedly based on their fertility counts
this is done to address varying sequence lengths between source and target texts
these fertilities are predicted using a dedicated neural network to reduce the unsupervised problem to a supervised one and thereby enabling it to be used as a latent variable
this invariable replications based on tilities may lead to duplication of words
closely followed by this van den oord et al
took a different approach by introducing probability sity distillation by modifying a convolutional ral network using a pre trained teacher network to score a student network attempting to minimize the kl divergence between itself and the teacher work
both these works set the trend of using latent variables to capture the interdependence between different time steps in the decoder
following this work lee et al
use iterative renement by denoising the latent variables at each of the ment steps
this idea of iterative decoding inspired way to more avenues by combining the benets of cloze style mask prediction objectives from bert devlin et al

some of them include tion based techniques gu et al
repeated masking and regenerating ghazvininejad et al
and providing model predictions to the input ghazvininejad et al

wang et al
proposed an alternative proach to address repetition observed in guo et al
and completeness using regularization terms for each
repetition is handled by izing similarity between consecutive words
pleteness is addressed by enabling reconstruction of source sentence from hidden states of the coder based on the duality of translation tasks tween source to target and target to source
rently guo et al
also address these issues by improving the inputs to decoder using additional phrase table information and sentence level ment between source and target word embeddings
sampling and search techniques
random sampling the words are sampled randomly based on the probability from the entire distribution without pruning any of the mass
p yt t jv
greedy decoding this technique simply boils down to selecting argmax of the probability distribution
as you keep selecting argmax where the problem is that it limits the diversity of generation
note that this may not result in the best output as there may be an alternate hypothesis comprising of a path that does not have to select the most probable word at each time step
yt argmaxyt jv a major disadvantage of greedy decoding is that there is no mechanism to correct the course if a mistake is made
this accumulates errors for the following time steps
it is monotonous with more predictable texts
this is alleviated by the next niques and beam search
this is also worked out for discrete settings using gumbel greedy decoding gu et al

variants of this were also studied by zarrie and schlangen
beam search beam search introduces a course correction mechanism in approximation of the argmax by selecting a beam size number of beams at each time step
when beam size is this is the same as greedy decoding and when beam size is the size of the vocabulary it it computationally very expensive
it has been relatively well ied in task agnostic objectives wang et al
for instance including social media text wang and ng error correction dahlmeier and ng
small beam sizes may lead to matical sentences they get more grammatical with increasing beam size
similarly small beam sizes may be less relevant with respect to content but get more generic with increasing beam size
there are several varieties within beam search noisy parallel approximate decoding this method cho introduces some noise in each hidden state to non deterministically make it slightly deviate from argmax
beam blocking repetition is one of the lems we see in nlg and this technique paulus et al
combats this problem by blocking the repeated n grams
it essentially adjusts the bility of any repeated n gram to
c iterative beam search in order to search a more diverse search space another technique likov et al
was introduced to iteratively perform beam search several times
and for each current time step we avoid all of the partial potheses encountered until that time step in the previous iterations based on soft or hard decisions on how to include or exclude these beams
diverse beam search one problem with beam search is that most times the decoded sequence still tends to come from a few highly signicant beams thereby suppressing diversity
the moderation by vijayakumar et al
adds a diversity penalty computed for example using hamming distance between the current hypothesis and the hypotheses in the groups to readjust the scores for predicting the next word
e clustered beam search the goal is prune unnecessary beams
at each time step tam get the top candidates and embed them by using averaged glove representations
cluster them ing k means to get k clusters
and then they pick the top b k candidates from each cluster to get b candidates in total for that time step
clustering post decoding the above proaches modify decoding step itself
this nique kriz et al
clusters after decoding is done
sentence representations from any of the diversity promoting beam search variants are tained
these are then clustered and the sentence with high log likelihood is selected from the cluster

top k sampling this technique by fan et al
randomly samples from the most ble candidates from this distribution
this means that we are conning the model to select from a truncated probability mass
p t p t v k p t ifyt v k otherwise if k is the size of vocabulary then it is random sampling and if k is then it is greedy decoding
high valued k results in dicey words but are monotonous and low valued k results in safe puts which are monotonous
the problem however is that k is limited to the same value in all scenarios

top p sampling the aforementioned lem of a xed value of k is addressed by top p sampling
this is also known as nucleus sampling holtzman et al
which instead of getting rid of the unspecied probability mass in top k sampling importance is shifted to the amount of probability mass preserved
this addresses ios where there could be broader set of reasonable options and sometimes a narrow set of options
it is achieved by selecting a dynamic k number of words from a cumulative probability distribution of words until a threshold probability value is attained
ytv p p t p key challenges for each of the challenges this section provides a list of solutions
the pitfalls of these solutions are also described there by encouraging research to address these key challenges

fluency there are a couple of detrimental factors that affect the uency of text generation which are repetition and coherence
solution beam blocking blocking beams taining previously generated n grams from quent generation combats repetition and ages diversity
there are multiple options to form this including cutting the beam stream or lect from the rest of the n grams klein et al
paulus et al

problem however sometimes beams with natural kind of repetition done for instance in order to emphasize something that is naturally done by humans are also blocked
selecting the number of beams is often a problem since it is natural for a function word to repeat more often
solution to problem massarelli et al
tensively studied the variants of introducing beam blocking which is also referred to as n gram ing by applying delays in beam search
solution unlikelihood objective welleck et al
argue that there is a fundamental aw in the objective of likelihood
the main idea is to decrease the probability of unlikely or negative candidates
the negative candidates are selected from the previous contexts either at token or at sequence levels which are essentially n grams
this way we are simultaneously optimizing for both likelihood with unlikelihood by discouraging the repetition of previous outputs
problem this may not seem a major issue ever selecting negative contexts is tricky and needs to be beyond selection of simple n gram sequences that occurred previously
solution coverage penalty this discourages the attention mechanism to attend the same word repeatedly see et al

navigating through each of the time step in the source if across ferent time steps of the decoded output the tion weights are higher for that particular source timestep then that timestep is covered and hence the coverage penalty would be which is
otherwise coverage penalty would be the attention probability mass on that source time step
solution static and dynamic planning this addresses coherence in terms of layout or tural organization of the text yao et al

a schema of static or dynamic plans are used to form an abstract ow of the text from which the actual text is realized
problem however underlying language models are capable of taking over leading to hallucinations and thereby compromising the delity of text

length of decoding one factor that guishes generation from rest of the family of tasks is the variability in the length of the ated output
the main problem here is that as the length of the sequence increases the sum of the log probability scores decrease
this means that models prefer shorter hypotheses
some solutions to combat this problem are the following
solution length normalization or penalty the generated output is scored by normalizing or viding with length
wu et al
explore a different variation of the normalization constant
this is pretty standard when the dataset has high variance in lengths
solution probability boosting this technique multiplies the probability with a xed constant at every time step
this alleviates the diminishing score problem
solution bias incorporate bias in the model based on empirical relations on lengths in source and target sentences in the training data

content selection certain tasks demand copying over the details in the input such as rare proper nouns for instance in news articles
this is especially needed in tasks like summarization which can demand a combination of extractive and abstractive techniques
solution copy mechanism copy mechanism can take various forms such as pointing to unknown words gulcehre et al
based on attention see et al
or a joint or a conditional copy mechanism gu et al
puduppully et al

it maybe based on attention that copies segments from input into the output
the problem is that sometimes this technique boils down from a combination of being extractive and abstractive to sort of an extractive system
solution hierarchical modeling this nique maintains a global account of the content
this is often modeled using hierarchical techniques or dual stage models martin et al
xu et al
gehrmann et al
where the rst stage pre selects relevant keywords for generation in the following stage
problem such models possibly take a hit on uency while connecting dots between selected content and generation
this means that can be good because the right words are extracted but may decrease as it affects the uency

optimization objective similar to the servation earlier in section there is an inherent mismatch in the between the objective function which is maximum likelihood and the end metrics which are bleu rouge solution reinforcement learning a common solution for this problem is using reinforcement learning to optimize end metrics such as rouge
often a combination of mle and rl objectives are used hu et al
wang et al

problem however this is still a problem since these end metrics do not directly correlate to human judgements
hence optimizing for bleu or rouge does not ensure human quality text
solution maximum mutual information the idea is to incorporate pairwise information of source and target instead of only one direction which is usually target given source li et al

the target probability is subtracted from target given source probability to diminish the probability of generic sentences
a viable extension to this is conditioning on personality for consistency
solution distinguishability hallucinations in abstractive generation are unwanted byproducts of optimizing log loss
to combat this several searchers explored optimizing for minimized guishability with human generated text hashimoto et al
theis et al

following similar path kang and hashimoto proposed cating loss to get rid of unwanted samples

speed practical applications call for ating text in real time without time lag in ing in addition to chasing the state of the art sults
model compression plays a crucial part in demonstrating an increase in the speed of ation
cheng et al
exhaustively surveyed the different techniques to perform model sion
while there are techniques in the hardware side there are certain modeling approaches that can handle this problem as well gonzalvo et al

most of this work is studied in the context of real time interpretation of speech fugen et al
yarmohammadi et al
grissom ii et al

recently deng and rush proposed a cascaded decoding approach introducing markov transformers to demonstrating high speed and curacy
quantization quantizing roy et al
gray the weights i
sharing the same weight value when they belong to a bin also proved helpful in improving the speed
this also facilitates the computations of gradients only once per bin
distillation it can be performed with a teacher and a smaller student network that tries to replicate the performance of the teacher with fewer ters chen et al

pruning this technique thresholds and prunes all the connections that have weights lesser than the predetermined threshold and then we can retrain the network in order to adjust the weights of the remaining connections
real time gu et al
trained an agent that learns to decide between the actions of reading by discarding a candidate or writing by accepting a candidate
the policy network is optimized with a combination of quality evaluated with bleu and delay evaluated by number of consecutive words in reading stage which increases wait time
caching another trick is to cache some of the previous computations to avoid repetition
evaluation similar to other generative modeling text ation also faces crucial challenges in evaluation reiter and belz reiter
van der lee et al
present some of the best practices of evaluating automatically generated text
the main hindrance to standardize or evaluate nlg like other standard tasks is that it is often a sub component of other tasks
this means that the input can be in varied forms such as tables images and text
in tain settings such as diverse image captioning we would need more objects or entities
sometimes in dialog we would need pronouns to have a natural coherence instead of repeating nouns
desiderata of text it is crucial to dene the tors contributing to the quality of good text
some of the factors include relevant content appropriate structure in terms of coherence and suitable surface forms
in addition uency grammaticality ability and novelty in some scenarios are crucial factors
intrinsic and extrinsic evaluation in subjective scopes such as text generation can be performed intrinsically or extrinsically
intrinsic evaluation is performed internally with respect to the eration itself and extrinsic evaluation is typically performed on the metric used to evaluate a stream task in which this generation is used
the quality can also be judged using automatic metrics and human evaluation
automatic metrics here we outline the broad categories of metrics along with their vantages and disadvantages
these metrics can be classied into the following categories word overlap based metrics these are based on the extent of word overlap which means that they capture replication of words
the problem with such measures is that they do not focus on semantics but rather just the surface form of words and alone
this includes precision for papineni et al
improved weighting for rare n grams nist doddington recall for n grams rouge lin and hovy equivalent of n grams meteor banerjee and lavie tf idf based cosine similarity for n grams cider vedantam et al

in extension to this we also have specic metrics to evaluate content selection by ing summarization content units using pyramid nenkova and passonneau and parsed scene graphs with objects and relations using spice derson et al

stanojevic and simaan proposed beer to address this as a ranking lem with character n grams along with words
language model based metrics this includes perplexity brown et al

such metrics are good in commenting about the language model self
it sort of gives the average number of choices each random variable has
however it does not directly evaluate the generation itself for instance a decrease in perplexity does not imply a decrease in the word error rate
it just means that cally the lm is good enough to select the right next word for that corpus
the human likeness is also measured by training a model to discriminate between human and machine generated text such as an automatic turing test lowe et al
cui et al
hashimoto et al

embedding based metrics this has the tage of being able to capture semantics
meant
lo and lo et al
putes structural similarity with shallow semantic parses being denitely and discretionarily used spectively along with word embeddings
recently contextulaized embeddings have been extensively used to capture this such as bertscore zhang et al
and bleurt sellam et al

metrics based on a combination of different embeddings are also proposed shimanaka et al
ma et al

however the problem of not correlating to human judgements still persists
emulated automatic metrics these rics check for the intended behavior in generation based on the sub problem the modeling approach is addressing
to check correctness or delity or alty with respect to source document we can apply inference
diversity can be evaluated by computing corpus based distributions on number of distinct entities fan et al
dong et al
clark et al
and so on
recently wang et al
worked on identifying factual cies generated summaries
the idea is that when a question is posed the source document and the summary should result in same or similar answers
human evaluation there are broadly two mechanisms in conducting subjective evaluations which is a challenging component of text tion
the rst is preference testing and the second is scoring
some studies have shown that ence based testing is prone to less variance pared to absolute scoring
here are some important points to keep in mind during conducting human i they are very expensive to evaluation
duct and hence not feasible to check the model by repeated examination
there are no standard universally agreed upon guidelines to setup such tasks
in other words conducting subjective ation itself is subjective in nature
scores tend to vary based on the nature of scales whether the judgements are binary discrete integer values or continuous
it is observed that human ences are inconsistent
they are biased with sonal and demographic conditions
in such cases it is important to measure inter annotator ment as well
some people might be lenient and others more strict which is not scaled across people
vi framing the task in an unambiguous way to elicit the right information and maintain reproducibility
having critically discussed human evaluation this is still really the best we got
it is absolutely crucial to perform human evaluation in most nlg tasks
so these problems need to be taken merely as cautions to develop more rational and systematic testing conditions
comparisons between automatic and human evaluation systems belz and reiter are also studied actively in order to bring human evaluation closer to automatic metrics
conclusion the past decade witnessed text generation dribbling from niche scenarios into several mainstream nlp applications
this urges the need for a snapshot to retrospect the progress of varied text tion tasks in unison
this paper is written with the goal of presenting a one stop destination for task agnostic components and factors in text ation for researchers foraging to situate their work and guage their impact in this vast eld
moving forward we envision that there are some of the crucial directions to focus for impactful innovation in text generation
these include i generation in real time non autoregressive decoding iii sistency with situated contexts in real and virtual environments and games iv consistency with sonality with opinions especially for virtual agents v conditioning on multiple modalities together with text and data vi investigation is still ing on nding better metrics to evaluate nlg with better correlated human judgements vii creative text generation
we believe this is the right time to extend advancements in any particular task to other tightly coupled tasks to revamp improvements in text generation as a holistic task
references mehdi allahyari seyedamin pouriyeh mehdi asse saeid safaei elizabeth d trippe juan b text rez and krys kochut

arxiv preprint tion techniques a brief survey


peter anderson basura fernando mark johnson and stephen gould

spice semantic tional image caption evaluation
in computer vision eccv european conference dam the netherlands october ceedings part v volume of lecture notes in computer science pages
springer
satanjeev banerjee and alon lavie

meteor an automatic metric for mt evaluation with proved correlation with human judgments
in ceedings of the workshop on intrinsic and trinsic evaluation measures for machine tion ann arbor michigan usa june pages
ciation for computational linguistics
anja belz and ehud reiter

comparing matic and human evaluation of nlg systems
in eacl conference of the european ter of the association for computational tics proceedings of the conference april trento italy
the association for computer tics
samy bengio oriol vinyals navdeep jaitly and noam shazeer

scheduled sampling for quence prediction with recurrent neural networks
in advances in neural information processing tems annual conference on neural tion processing systems december montreal quebec canada pages
peter f
brown stephen della pietra vincent j
della pietra jennifer c
lai and robert l
mercer

an estimate of an upper bound for the entropy of english
comput
linguistics
sunita chand

empirical survey of machine translation tools
in second international ference on research in computational intelligence and communication networks icrcicn pages
ieee
hongshen chen xiaorui liu dawei yin and jiliang tang

a survey on dialogue systems recent advances and new frontiers
sigkdd explorations
yen chun chen zhe gan yu cheng jingzhou liu and jingjing liu

distilling the knowledge of bert for text generation
corr

yu cheng duo wang pan zhou and tao zhang

a survey of model compression and eration for deep neural networks
arxiv preprint

kyunghyun cho

noisy parallel approximate decoding for conditional recurrent language model
corr

in international conference on agents
ing representations iclr new orleans la usa may
openreview
net
chenhui chu and rui wang

a survey of main adaptation for neural machine translation
in proceedings of the international conference on computational linguistics pages
elizabeth clark yangfeng ji and noah a
smith

neural text generation in stories using entity in proceedings of the sentations as context
conference of the north american chapter of the association for computational linguistics human language technologies naacl hlt new orleans louisiana usa june volume long papers pages
association for computational linguistics
yin cui guandao yang andreas veit xun huang and serge j
belongie

learning to in ieee conference ate image captioning
on computer vision and pattern recognition cvpr salt lake city ut usa june pages
ieee computer society
raj dabre chenhui chu and anoop kunchukuttan

a survey of multilingual neural machine translation
arxiv preprint

daniel dahlmeier and hwee tou ng

a search decoder for grammatical error correction
in proceedings of the joint conference on ical methods in natural language processing and computational natural language learning pages
association for computational tics
sumanth dathathri andrea madotto janice lan jane hung eric frank piero molino jason yosinski and rosanne liu

plug and play language models a simple approach to controlled text generation
in international conference on learning tations iclr addis ababa ethiopia april
openreview
net
yuntian deng and alexander m
rush

cascaded text generation with markov transformers
corr

jacob devlin ming wei chang kenton lee and kristina toutanova

bert pre training of deep bidirectional transformers for language in proceedings of the conference standing
of the north american chapter of the association for computational linguistics human language technologies naacl hlt minneapolis mn usa june volume long and short pers pages
association for tional linguistics
emily dinan stephen roller kurt shuster angela fan michael auli and jason weston

wizard of wikipedia knowledge powered conversational george doddington

automatic evaluation of machine translation quality using n gram occurrence statistics
in proceedings of the second international conference on human language nology research pages
li dong nan yang wenhui wang furu wei aodong liu yu wang jianfeng gao ming zhou and hsiao wuen hon

unied language model pre training for natural language ing and generation
in advances in neural tion processing systems pages
ruo ping dong khyathi raghavi chandu and alan w
black

induction and reference of entities in a visual story
corr

angela fan mike lewis and yann dauphin

in proceedings erarchical neural story generation
of the annual meeting of the association for computational linguistics volume long papers pages
angela fan mike lewis and yann n
dauphin

in strategies for structuring story generation
ceedings of the conference of the association for computational linguistics acl florence italy july august volume long pers pages
association for tional linguistics
william fedus ian j
goodfellow and andrew m
dai

maskgan better text generation via lling in international conference on in the learning representations iclr vancouver bc canada april may conference track proceedings
openreview
net

christian fugen alex waibel and muntsin kolss
simultaneous translation of lectures and
speeches
machine translation
albert gatt and emiel krahmer

survey of the state of the art in natural language generation core tasks applications and evaluation
j
artif
intell
res

sebastian gehrmann yuntian deng and alexander m rush

bottom up abstractive summarization
in proceedings of the conference on cal methods in natural language processing pages
marjan ghazvininejad omer levy yinhan liu and luke zettlemoyer

mask predict parallel decoding of conditional masked language models
in proceedings of the conference on cal methods in natural language processing and the international joint conference on natural language processing emnlp ijcnlp hong kong china november pages
association for computational linguistics
marjan ghazvininejad omer levy and luke semi autoregressive training arxiv preprint moyer

proves mask predict decoding


and the aaai symposium on educational vances in articial intelligence new leans louisiana usa february pages
aaai press
xavi gonzalvo siamak tazari chun an chan markus becker alexander gutkin and hanna silen

recent advances in google real time hmm driven unit selection synthesizer
ian goodfellow jean pouget abadie mehdi mirza bing xu david warde farley sherjil ozair aaron courville and yoshua bengio

generative in advances in neural information versarial nets
processing systems pages
robert gray

vector quantization
ieee assp magazine
alvin grissom ii he he jordan boyd graber john morgan and hal daume iii

do nt until the nal verb wait reinforcement learning for taneous machine translation
in proceedings of the conference on empirical methods in natural language processing emnlp pages
jiatao gu daniel jiwoong i m and victor ok li

neural machine translation with gumbel greedy coding
in thirty second aaai conference on cial intelligence
jiatao gu qi liu and kyunghyun cho

insertion based decoding with automatically ferred generation order
trans
assoc
comput
guistics
jiatao gu zhengdong lu hang li and victor ok incorporating copying mechanism in li

in proceedings of sequence to sequence learning
the annual meeting of the association for putational linguistics volume long papers pages
jiatao gu graham neubig kyunghyun cho and tor o
k
li

learning to translate in in time with neural machine translation
ings of the conference of the european ter of the association for computational linguistics eacl valencia spain april ume long papers pages
association for computational linguistics
caglar gulcehre sungjin ahn ramesh nallapati bowen zhou and yoshua bengio

pointing the unknown words
in proceedings of the nual meeting of the association for computational linguistics volume long papers pages
jiaxian guo sidi lu han cai weinan zhang yong yu and jun wang

long text generation via adversarial training with leaked information
in ceedings of the thirty second aaai conference on articial intelligence the tive applications of articial intelligence junliang guo xu tan di he tao qin linli xu and tie yan liu

non autoregressive ral machine translation with enhanced decoder put
in the thirty third aaai conference on cial intelligence aaai the thirty first vative applications of articial intelligence ence iaai the ninth aaai symposium on ucational advances in articial intelligence eaai honolulu hawaii usa january ary pages
aaai press
junliang guo xu tan linli xu tao qin enhong chen and tie yan liu

fine tuning by riculum learning for non autoregressive neural chine translation
in the thirty fourth aaai ference on articial intelligence aaai the thirty second innovative applications of articial intelligence conference iaai the tenth aaai symposium on educational advances in articial telligence eaai new york ny usa ary pages
aaai press
tatsunori b
hashimoto hugh zhang and percy liang

unifying human and statistical evaluation for natural language generation
in proceedings of the conference of the north american ter of the association for computational linguistics human language technologies naacl hlt minneapolis mn usa june volume long and short papers pages
ation for computational linguistics
ari holtzman jan buys li du maxwell forbes and yejin choi

the curious case of neural text in international conference on degeneration
learning representations iclr addis ababa ethiopia april
openreview
net
md
zakir hossain ferdous sohel mohd fairuz ratuddin and hamid laga

a comprehensive survey of deep learning for image captioning
corr

junjie hu yu cheng zhe gan jingjing liu jianfeng gao and graham neubig

what makes a good story designing composite rewards for visual storytelling
in the thirty fourth aaai conference on articial intelligence aaai the second innovative applications of articial gence conference iaai the tenth aaai posium on educational advances in articial ligence eaai new york ny usa february pages
aaai press
touseef iqbal and shaima qureshi

the survey text generation models in deep learning
journal of king saud university computer and information sciences
daniel kang and tatsunori hashimoto

proved natural language generation via loss tion
corr

diederik p
kingma and max welling

in international encoding variational bayes
iclr conference on learning representations banff ab canada april ference track proceedings
guillaume klein yoon kim yuntian deng jean lart and alexander m rush

opennmt source toolkit for neural machine translation
in proceedings of acl system demonstrations pages
reno kriz joao sedoc marianna apidianaki carolina zheng gaurav kumar eleni miltsakaki and chris callison burch

complexity weighted loss and diverse reranking for sentence simplication
in proceedings of the conference of the north american chapter of the association for tional linguistics human language technologies volume long and short papers pages
ilia kulikov alexander miller kyunghyun cho and jason weston

importance of search and uation strategies in neural dialogue modeling
in proceedings of the international conference on natural language generation pages
alex m lamb anirudh goyal alias parth goyal ying zhang saizheng zhang aaron c courville and yoshua bengio

professor forcing a new in algorithm for training recurrent networks
vances in neural information processing systems pages
chris van der lee albert gatt emiel van miltenburg sander wubben and emiel krahmer

best practices for the human evaluation of automatically generated text
in proceedings of the tional conference on natural language generation inlg tokyo japan october november pages
association for tional linguistics
jason lee elman mansimov and kyunghyun cho

deterministic non autoregressive neural in quence modeling by iterative renement
ceedings of the conference on empirical ods in natural language processing brussels gium october november pages
association for computational linguistics
mike lewis yinhan liu naman goyal jan ghazvininejad abdelrahman mohamed omer levy ves stoyanov and luke zettlemoyer

bart denoising sequence to sequence pre training for natural language generation translation and comprehension
arxiv preprint

jiwei li michel galley chris brockett jianfeng gao and bill dolan

a diversity promoting tive function for neural conversation models
in ceedings of the conference of the north ican chapter of the association for computational linguistics human language technologies pages
chin yew lin and eduard hovy

manual and tomatic evaluation of summaries
in proceedings of the workshop on automatic summarization pages
hui lin and vincent ng

abstractive in rization a survey of the state of the art
ceedings of the aaai conference on articial ligence volume pages
chia wei liu ryan lowe iulian vlad serban mike noseworthy laurent charlin and joelle pineau

how not to evaluate your dialogue system an empirical study of unsupervised evaluation in rics for dialogue response generation
ings of the conference on empirical methods in natural language processing pages
chi kiu lo

meant
accurate semantic mt evaluation for any output language
in ings of the second conference on machine tion wmt copenhagen denmark september pages
association for tional linguistics
chi kiu lo michel simard darlene a
stewart samuel larkin cyril goutte and patrick littell

accurate semantic textual similarity for ing noisy parallel corpora using semantic machine translation evaluation metric the nrc supervised submissions to the parallel ltering task
in proceedings of the third conference on machine translation shared task papers wmt gium brussels october november pages
association for computational guistics
ryan lowe michael noseworthy iulian vlad ban nicolas angelard gontier yoshua bengio and joelle pineau

towards an automatic turing test learning to evaluate dialogue responses
in proceedings of the annual meeting of the sociation for computational linguistics acl vancouver canada july august volume long papers pages
association for computational linguistics
sidi lu yaoming zhu weinan zhang jun wang and yong yu

neural text generation past present and beyond
corr

qingsong ma yvette graham shugen wang and qun liu

blend a novel combined mt metric based on direct assessment casict dcu sion to metrics task
in proceedings of the second conference on machine translation wmt copenhagen denmark september pages
association for computational guistics
lara j martin prithviraj ammanabrolu xinyu wang william hancock shruti singh brent harrison and mark o riedl

event representations for tomated story generation with deep neural nets
in thirty second aaai conference on articial gence
luca massarelli fabio petroni aleksandra piktus myle ott tim rocktaschel vassilis plachouras fabrizio silvestri and sebastian riedel

how decoding strategies affect the veriability of ated text
arxiv preprint

joao luis zeni montenegro cristiano andre da costa and rodrigo da rosa righi

survey of sational agents in health
expert systems with cations
ani nenkova and kathleen mckeown

a in mining vey of text summarization techniques
text data pages
springer
ani nenkova and rebecca j
passonneau

ating content selection in summarization the mid method
in human language technology ference of the north american chapter of the ciation for computational linguistics hlt naacl boston massachusetts usa may pages
the association for computational linguistics
van den driessche aaron van den oord yazhe li igor babuschkin karen simonyan oriol vinyals koray kavukcuoglu edward george hart luis c
cobo florian stimberg norman casagrande dominik grewe seb noury sander dieleman erich elsen nal kalchbrenner heiga zen alex graves helen king tom walters dan belov and demis hassabis

parallel wavenet fast speech synthesis
in proceedings of the international conference on machine learning icml stockholmsmassan holm sweden july volume of proceedings of machine learning research pages
pmlr
kishore papineni salim roukos todd ward and jing zhu

bleu a method for automatic uation of machine translation
in proceedings of the annual meeting of the association for tational linguistics july philadelphia pa usa pages
acl
romain paulus caiming xiong and richard socher

a deep reinforced model for abstractive in international conference on marization
learning representations iclr vancouver bc canada april may conference track proceedings
openreview
net
rivindu perera and parma nand

recent vances in natural language generation a survey and classication of the empirical literature
comput
formatics
matthew peters mark neumann mohit iyyer matt gardner christopher clark kenton lee and luke zettlemoyer

deep contextualized word sentations
in proceedings of the conference of the north american chapter of the association for computational linguistics human language technologies volume long papers pages
shrimai prabhumoye alan w black and exploring arxiv preprint lan salakhutdinov

lable text generation techniques


ratish puduppully li dong and mirella lapata

data to text generation with content selection and planning
in proceedings of the aaai conference on articial intelligence volume pages
alec radford karthik narasimhan tim salimans and ilya sutskever
improving language understanding by generative pre training
colin raffel noam shazeer adam roberts katherine lee sharan narang michael matena yanqi zhou wei li and peter j
liu

exploring the limits of transfer learning with a unied text to text former
corr

kiran ramesh surya ravishankaran abhishek joshi and k chandrasekaran

a survey of in sign techniques for conversational agents
ternational conference on information cation and computing technology pages
springer
marcaurelio ranzato sumit chopra michael auli and wojciech zaremba

sequence level in ing with recurrent neural networks
national conference on learning representations iclr san juan puerto rico may conference track proceedings
ehud reiter

a structured review of the validity of bleu
comput
linguistics
ehud reiter and anja belz

an investigation into the validity of some metrics for automatically ating natural language generation systems
tational linguistics
aurko roy ashish vaswani arvind neelakantan and niki parmar

theory and experiments on tor quantized autoencoders
corr

abigail see peter j liu and christopher d manning

get to the point summarization with generator networks
in proceedings of the nual meeting of the association for computational linguistics volume long papers pages
thibault sellam dipanjan das and ankur p
parikh

bleurt learning robust metrics for text eration
corr

hiroki shimanaka tomoyuki kajiwara and mamoru komachi

ruse regressor using sentence embeddings for automatic machine translation uation
in proceedings of the third conference on machine translation shared task papers wmt belgium brussels october november pages
association for tional linguistics
jonathan slocum

a survey of machine tion its history current status and future prospects
computational linguistics
kaitao song xu tan tao qin jianfeng lu and yan liu

mass masked sequence to sequence pre training for language generation
arxiv preprint

milos stanojevic and khalil simaan

beer in proceedings of better evaluation as ranking
the ninth workshop on statistical machine lation june more maryland usa pages
the ation for computer linguistics
akhilesh sudhakar bhargav upadhyay and arjun heswaran

transforming delete retrieve generate approach for controlled text style transfer
in proceedings of the conference on cal methods in natural language processing and the international joint conference on natural language processing emnlp ijcnlp hong kong china november pages
association for computational linguistics
yik cheung tam

cluster based beam search for pointer generator chatbot grounded by knowledge
computer speech language page
oguzhan tas and farzad kiyani
a survey matic text summarization
pressacademia procedia
lucas theis aaron van den oord and matthias bethge

a note on the evaluation of generative models
in international conference on ing representations iclr san juan puerto rico may conference track ings
julian togelius georgios n
yannakakis kenneth o
stanley and cameron browne

search based procedural content generation a taxonomy and ieee trans
comput
intell
ai games survey

chao tong richard c
roberts rita borgo sean p
walton robert s
laramee kodzo wegba aidong lu yun wang huamin qu qiong luo and juan ma

storytelling and visualization an extended survey
information
ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan n
gomez lukasz kaiser and illia polosukhin

attention is all you need
in advances in neural information cessing systems annual conference on neural information processing systems ber long beach ca usa pages
ramakrishna vedantam c
lawrence zitnick and devi parikh

cider consensus based image description evaluation
in ieee conference on puter vision and pattern recognition cvpr boston ma usa june pages
ieee computer society
ashwin k vijayakumar michael cogswell prasath r selvaraju qing sun stefan lee david crandall and dhruv batra

diverse beam search decoding diverse solutions from neural quence models
arxiv preprint

alex wang kyunghyun cho and mike lewis

asking and answering questions to evaluate the factual consistency of summaries
arxiv preprint

pidong wang and hwee tou ng

a beam search decoder for normalization of social media text with application to machine translation
in proceedings of the conference of the north american ter of the association for computational linguistics human language technologies pages
xin wang wenhu chen yuan fang wang and william yang wang

no metrics are perfect adversarial reward learning for visual storytelling
in proceedings of the annual meeting of the association for computational linguistics volume long papers pages
xuancong wang hwee tou ng and khe chai sim

a beam search decoder for disuency in proceedings of coling the tion
international conference on computational tics technical papers pages
yiren wang fei tian di he tao qin chengxiang zhai and tie yan liu

non autoregressive machine translation with auxiliary regularization
in the thirty third aaai conference on articial telligence aaai the thirty first innovative applications of articial intelligence conference iaai the ninth aaai symposium on cational advances in articial intelligence eaai honolulu hawaii usa january ary pages
aaai press
sean welleck ilia kulikov stephen roller emily nan kyunghyun cho and jason weston

ral text generation with unlikelihood training
in international conference on learning tations iclr addis ababa ethiopia april
openreview
net
yonghui wu mike schuster zhifeng chen quoc v le mohammad norouzi wolfgang macherey maxim krikun yuan cao qin gao klaus macherey et al

google s neural machine translation system bridging the gap between arxiv preprint man and machine translation


jingjing xu xuancheng ren yi zhang qi zeng aoyan cai and xu sun

a skeleton based model for promoting coherence among sentences in in proceedings of the narrative story generation
conference on empirical methods in natural language processing pages
jingjing xu xu sun xuancheng ren junyang lin bingzhen wei and wei li

gan diversity promoting generative adversarial network for generating informative and diversied text
corr

zhilin yang zihang dai yiming yang jaime bonell russ r salakhutdinov and quoc v le

xlnet generalized autoregressive pretraining for language understanding
in advances in neural formation processing systems pages
lili yao nanyun peng ralph weischedel kevin knight dongyan zhao and rui yan

and write towards better automatic storytelling
in proceedings of the aaai conference on articial telligence volume pages
mahsa yarmohammadi vivek kumar rangarajan har srinivas bangalore and baskaran sankaran

incremental segmentation and decoding strategies for simultaneous translation
in ings of the sixth international joint conference on natural language processing pages
lantao yu weinan zhang jun wang and yong yu
seqgan sequence generative adversarial
in proceedings of the nets with policy gradient
thirty first aaai conference on articial gence february san francisco nia usa pages
aaai press
sina zarrie and david schlangen

decoding strategies for neural referring expression generation
proceedings of inlg
tianyi zhang varsha kishore felix wu kilian q
weinberger and yoav artzi

bertscore in uating text generation with bert
national conference on learning representations iclr addis ababa ethiopia april
openreview
net

