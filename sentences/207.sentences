multimodal abstractive summarization for videos shruti jindrich spandana florian of computer science carnegie mellon university of mathematics and physics charles university amazon ai
cmu
edu
mff
cuni
cz
com
cmu
edu n u j l c
s c v
v i x r a abstract in this paper we study abstractive tion for open domain videos
unlike the ditional text news summarization the goal is less to compress text information but rather to provide a uent textual summary of mation that has been collected and fused from different source modalities in our case video and audio transcripts or text
we show how a multi source sequence to sequence model with hierarchical attention can integrate mation from different modalities into a ent output compare various models trained with different modalities and present pilot periments on the corpus of instructional videos
we also propose a new evaluation ric content for abstractive summarization task that measures semantic adequacy rather than uency of the summaries which is ered by metrics like rouge and bleu
introduction in recent years with the growing popularity of video sharing platforms there has been a steep rise in the number of user generated instructional videos shared online
with the abundance of videos online there has been an increase in demand for efcient ways to search and retrieve relevant videos song et al
wang et al
otani et al
torabi et al

many cross modal search applications rely on text associated with the video such as description or title to nd relevant content
however often videos do not have text meta data associated with them or the existing ones do not provide clear information of the video tent and fail to capture subtle differences between related videos wang et al

we address this by aiming to generate a short text summary of the video that describes the most salient content of the done while sg was at university of edinburgh video
our work benets users through better textual information and user experience and video sharing platforms with increased user engagement by retrieving or suggesting relevant videos to users and capturing their attention
summarization is a task of producing a shorter version of the content in the document while serving its information and has been studied for both textual documents automatic text tion and visual documents such as images and videos video summarization
automatic text marization is a widely studied topic in natural guage processing luhn kupiec et al
mani given a text document the task is to generate a textual summary for applications that can assist users to understand large documents
most of the work on text summarization has cused on single document summarization for mains such as news rush et al
nallapati et al
see et al
narayan et al
and some on multi document summarization stein et al
lin and hovy woodsend and lapata cao et al
yasunaga et al

video summarization is the task of producing a compact version of the video visual summary by encapsulating the most informative parts money and agius lu and grauman gygli et al
song et al
sah et al

multimodal summarization is the combination of textual and visual modalities by summarizing a video document with a text summary that rizes the content of the video
multimodal rization is a more recent challenge with no marking datasets yet
li et al
collected a multimodal corpus of english news videos and articles paired with manually annotated summaries
the dataset is small scale and has news articles with audio video and text summaries but there are no human annotated audio transcripts
figure dataset example with different modalities
cuban breakfast and free cooking video is not mentioned in the transcript and has to be derived from other sources
related tasks include image or video captioning and description generation video story generation procedure learning from instructional videos and title generation which focus on events or activities in the video and generating descriptions at ous levels of granularity from single sentence to multiple sentences das et al
regneri et al
rohrbach et al
zeng et al
zhou et al
zhang et al
gella et al

a closely related task to ours is video title generation where the task is to describe the most salient event in the video in a compact title that is aimed at capturing users attention zeng et al

zhou et al
present the youcookii dataset containing instructional videos specically cooking recipes with temporally localized tations for the procedure which could be viewed as a summarization task as well although localized with time alignments between video segments and procedures
in this work we study multimodal tion with various methods to summarize the intent of open domain instructional videos stating the clusive and unique features of the video tive of modality
we study this task in detail using the new sanabria et al
which contains human annotated video summaries for a varied range of topics
our models generate ral language descriptions for video content using the transcriptions both user generated and output of automatic speech recognition systems as well as visual features extracted from the video
we also introduce a new evaluation metric content that suits this task and present detailed results to understand the task better
multimodal abstractive summarization the sanabria et al
contains about hours of short instructional videos spanning different domains such as cooking sports indoor outdoor activities music
each video is accompanied by a human generated transcript and a to sentence summary is available for ery video written to generate interest in a potential viewer
the example in figure shows the transcript scribes instructions in detail while the summary is a high level overview of the entire video ing that the peppers are being cut and that this is a cuban breakfast recipe which is not tioned in the transcript
we observe that text and vision modalities both contain complementary formation thereby when fused helps in generating richer and more uent summaries
additionally we can also leverage the speech modality by using the output of a speech recognizer as input to a marization model instead of a human annotated transcript
the contains videos for training for validation and for testing
the average length of transcripts is words and of summaries is words
a more general parison of the dataset for summarization as compared with certain common datasets is given in sanabria et al

video based summarization
we represent videos by features extracted from a pre trained action recognition model a convolutional neural network hara et al
today we are going to show you how to make spanish omelet
i m going to dice a little bit of peppers here
i m not going to use a lot i m going to use very very little
a little bit more then this maybe
you can use red peppers if you like to get a little bit color in your omelet
some people do and some people do
t is the way they make there spanish omelets that is what she says
i loved it it actually tasted really good
you are going to take the onion also and dice it really small
you do want big chunks of onion in there cause it is just pops out of the omelet
so we are going to dice the up also very very small
so we have small pieces of onions and peppers ready to go
how to cut peppers to make a spanish omelette get expert tips and advice on making cuban breakfast recipes in this free cooking video
summarytranscriptvideo trained to recognize different human actions in the kinetics dataset kay et al

these features are dimensional extracted for every non overlapping frames in the video
this results in a sequence of feature vectors per video rather than a single global one
we use these sequential features in our models described in section
dimensional feature vector representing all text a single video
speech based summarization
we leverage the speech modality by using the outputs from a trained speech recognizer that is trained with other data as inputs to a text summarization model
we use the state of the art models for microphone conversational speech recognition pire peddinti et al
and eesen miao et al
le franc et al

the word error rate of these models on the test data is

this high error mostly stems from normalization issues in the data
for example recognizing and labeling as twenty
handling these tively will reduce the word error rates signicantly
we accept these as is for this task
transfer learning
our parallel work sanabria et al
demonstrates the use of summarization models trained in this paper for a transfer learning based summarization task on the charades dataset sigurdsson et al
that has audio video and text summary caption and question answer pairs modalities similar to the dataset
sanabria et al
observe that pre training and transfer learning with the dataset led to signicant improvements in unimodal and multimodal tion tasks on the charades dataset
summarization models we study various summarization models
first we use a recurrent neural network rnn to sequence model sutskever et al
consisting of an encoder rnn to encode text or video features with the attention mechanism bahdanau et al
and a decoder rnn to generate summaries
our second model is a pointer generator pg model vinyals et al
glehre et al
that has shown strong formance for abstractive summarization nallapati et al
see et al

as our third model we use hierarchical attention approach of libovick and helcl originally proposed for multimodal machine translation to combine textual and visual video frames resnext features rnn rnn attention attention hier
attn



w rnn over transcript rnn decoder figure building blocks of the sequence to sequence models gray numbers in brackets indicate which ponents are utilized in which experiments
modalities to generate text
the model rst putes the context vector independently for each of the input modalities text and video
in the next step the context vectors are treated as states of another encoder and a new vector is computed
when using a sequence of action features instead of a single averaged vector for a video the rnn layer helps capture context
in figure we present the building block of our models
evaluation we evaluate the summaries using the standard ric for abstractive summarization rouge l lin and och that measures the longest common sequence between the reference and the generated summary
additionally we introduce the content metric that ts the template like structure of the summaries
we analyze the most frequently occurring words in the transcription and summary
the words in transcript reect the conversational and spontaneous speech while the words in the summaries reect their descriptive nature
for amples see table in appendix a

content
this metric is the score of the content words in the summaries based over a lingual alignment similar to metrics used to ate quality of monolingual alignment sultan et al

we use the meteor toolkit banerjee and lavie denkowski and lavie to obtain the alignment
then we remove function words and task specic stop words that appear in most of the summaries see appendix a
from the ence and the hypothesis
the stop words are easy to predict and thus increase the rouge score
we treat remaining content words from the reference v i e o r a m e s r e s n e x t e a t u r e s w r n n w o r n n a t t e n t i o n r n n o v e r t r a n s c r i t a t t e n t i o n h i e r
a t t n
w


r n n d e c o e r model no
description rouge l content random baseline using language model rule based extractive summary next neighbor summary using extracted sentence from only text only first tokens text only complete transcript text only tokens pg complete transcript text only asr output complete transcript text only action features only video action features rnn video ground truth transcript action with hierarchical attn asr output action with hierarchical attn























table rouge l and content for different summarization models random baseline rule based extracted summary nearest neighbor summary different text only pointer generator asr output transcript video only and text and video models
model no
inf rel coh flu text only video only text and video











table human evaluation scores on different sures of informativeness inf relevance rel herence coh fluency flu
and the hypothesis as two bags of words and pute the score over the alignment
note that the score ignores the uency of output
human evaluation
in addition to automatic evaluation we perform a human evaluation to derstand the outputs of this task better
following the abstractive summarization human annotation work of grusky et al
we ask our annotators to label the generated output on a scale of on informativeness relevance coherence and uency
we perform this on randomly sampled videos from the test set
we evaluate three models two unimodal only video only and one multimodal text and video
three workers annotated each video on amazon mechanical turk
more details about human evaluation are in the appendix a

experiments and results as a baseline we train an rnn language model sutskever et al
on all the summaries and randomly sample tokens from it
the output tained is uent in english leading to a high rouge score but the content is unrelated which leads to a low content score in table
as another baseline we replace the target summary with a rule based extracted summary from the tion itself
we used the sentence containing words how to with predicates learn tell show discuss or explain usually the second sentence in the script
our nal baseline was a model trained with the summary of the nearest neighbor of each video in the latent dirichlet allocation lda blei et al
based topic space as a target
this model achieves a similar content score as the based model which shows the similarity of content and further demonstrates the utility of the content score
we use the transcript either ground truth script or speech recognition output and the video action features to train various models with ferent combinations of modalities
the text only model performs best when using the complete script in the input tokens
this is in contrast to prior work with news domain summarization nallapati et al

we also observe that pg networks do not perform better than models on this data which could be attributed to the tive nature of our summaries and also the lack of common n gram overlap between input and output which is the important feature of pg networks
we also use the automatic transcriptions obtained from a pretrained automatic speech recognizer as input to the summarization model
this model achieves difference in content rather than length
example presented in table section a
shows how the outputs vary
conclusions we present several baseline models for ing abstractive text summaries for the open domain videos in data
our presented models include a video only summarization model that performs competitively with a text only model
in the future we would like to extend this work to generate document multi video summaries and also build end to end models directly from audio in the video instead of text based output from pretrained asr
we dene and show the quality of a new metric content for evaluation of the video summaries that are designed as teasers or highlights for ers instead of a condensed version of the input like traditional text summaries
acknowledgements this work was mostly conducted at the erick jelinek memorial summer workshop on speech and language hosted and sponsored by johns hopkins university
shruti palaskar received funding from facebook and zon grants
jindrich libovick received funding from the czech science foundation grant no

this work used the extreme science and engineering discovery environment xsede ported by nsf grant and the bridges system supported by nsf award at the pittsburgh supercomputing center
references dzmitry bahdanau kyunghyun cho and yoshua neural machine translation by corr bengio

jointly learning to align and translate


satanjeev banerjee and alon lavie

meteor an automatic metric for mt evaluation with improved correlation with human judgments
in proceedings of the acl workshop on intrinsic and extrinsic ation measures for machine translation marization pages
david m blei andrew y ng and michael i jordan

latent dirichlet allocation
journal of chine learning research

clsp
jhu
edu figure word distribution in comparison with the man summaries for different unimodal and multimodal models
density curves show the length distributions of human annotated and system produced summaries
competitive performance with the video only els described below but degrades noticeably than ground truth transcription summarization model
this is as expected due to the large margin of asr errors in distant microphone open domain speech recognition
we trained two video only models the rst one uses a single mean pooled feature vector tation for the entire video while the second one applies a single layer rnn over the vectors in time
note that using only the action features in input reaches almost competitive rouge and content scores compared to the text only model ing the importance of both modalities in this task
finally the hierarchical attention model that bines both modalities obtains the highest score
in table we report human evaluation scores on our best text only video only and multimodal els
in three evaluation measures the multimodal models with the hierarchical attention reach the best scores
model hyperparameter settings tion analysis and example outputs for the models described above are available in the appendix
in figure we analyze the word distributions of different system generated summaries with the man annotated reference
the density curves show that most model outputs are shorter than human notations with the action only model being the shortest as expected
interestingly the two different uni modal and multimodal systems with truth text and asr output text features are very similar in length showing that the improvements in rouge l and content scores stem from the of








feat

truth transcript

feat

output



only

ozan caglayan mercedes garca martnez adrien bardet walid aransa fethi bougares and loc rault

nmtpy a exible toolkit for advanced neural machine translation systems
the prague bulletin of mathematical linguistics
chloe hillier will kay joao carreira karen simonyan brian sudheendra zhang narasimhan fabio viola tim green trevor back paul natsev al

the kinetics human action video dataset
corr
ziqiang cao furu wei li dong sujian li and ming zhou

ranking with recursive neural works and its application to multi document rization
in twenty ninth aaai conference on cial intelligence
kyunghyun cho bart van merrienboer caglar cehre dzmitry bahdanau fethi bougares holger schwenk and yoshua bengio

learning phrase representations using rnn encoder decoder for statistical machine translation
in proceedings of the conference on empirical methods in ral language processing emnlp
p
das c
xu r
f
doell and j
j
corso

a thousand frames in just a few words lingual scription of videos through latent topics and sparse object stitching
in proceedings of ieee conference on computer vision and pattern recognition
michael denkowski and alon lavie

meteor universal language specic translation evaluation for any target language
in proceedings of the ninth workshop on statistical machine translation pages
association for computational tics
spandana gella mike lewis and marcus rohrbach

a dataset for telling the stories of social media videos
in proceedings of the conference on empirical methods in natural language processing pages
jade goldstein vibhu mittal jaime carbonell and mark kantrowitz

multi document in proceedings rization by sentence extraction
of the naacl anlp workshop on automatic summarization pages
association for putational linguistics
max grusky mor naaman and yoav artzi

newsroom a dataset of
million summaries with diverse extractive strategies
corr
aglar glehre sungjin ahn ramesh nallapati bowen zhou and yoshua bengio

pointing the unknown words
in proceedings of the nual meeting of the association for computational linguistics acl volume long papers
michael gygli helmut grabner hayko der and luc van gool

creating summaries from user videos
in european conference on puter vision pages
springer
kensho hara hirokatsu kataoka and yutaka satoh

can spatiotemporal cnns retrace the in proceedings of tory of cnns and imagenet the ieee conference on computer vision and tern recognition cvpr pages
diederik p
kingma and jimmy ba

adam corr a method for stochastic optimization


julian kupiec jan pedersen and francine chen

a trainable document summarizer
in proceedings of the annual international acm sigir ence on research and development in information retrieval pages
acm
adrien le franc eric riebling julien karadayi w yun camila scaff florian metze and rina cristia

the aclew divime an easy in interspeech pages use diarization tool

interspeech isca
haoran li junnan zhu cong ma jiajun zhang and chengqing zong

multi modal tion for asynchronous collection of text image in proceedings of the dio and video
ference on empirical methods in natural language processing pages
jindrich libovick and jindrich helcl

attention strategies for multi source sequence to sequence learning
in proceedings of the annual ing of the association for computational linguistics volume short papers pages
chin yew lin and eduard hovy

from single to multi document summarization
in proceedings of annual meeting of the association for tational linguistics pages
chin yew lin and franz josef och

matic evaluation of machine translation quality ing longest common subsequence and skip bigram statistics
in proceedings of the meeting of the association for computational linguistics pages
association for computational tics
zheng lu and kristen grauman

story driven summarization for egocentric video
in proceedings of the ieee conference on computer vision and tern recognition pages
hans peter luhn

the automatic creation of erature abstracts
ibm journal of research and velopment
inderjeet mani

advances in automatic text marization
mit press
yajie miao mohammad gowayyed and florian metze

eesen end to end speech recognition in ing deep rnn models and wfst based decoding
automatic speech recognition and understanding asru ieee workshop on pages
ieee
arthur g money and harry agius

video marisation a conceptual framework and survey of the state of the art
journal of visual tion and image representation
ramesh nallapati bowen zhou cicero dos santos a glar gulehre and bing xiang

tive text summarization using sequence to sequence rnns and beyond
conll page
shashi narayan shay b cohen and mirella lapata

ranking sentences for extractive tion with reinforcement learning
corr
mayu otani yuta nakashima esa rahtu janne heikkil and naokazu yokoya

learning joint representations of videos and sentences with web image search
in european conference on puter vision pages
springer
vijayaditya peddinti guoguo chen vimal manohar tom ko daniel povey and sanjeev khudanpur

jhu aspire system robust lvcsr with tdnns ivector adaptation and rnn lms
in automatic speech recognition and understanding asru ieee workshop on pages
ieee
michaela regneri marcus rohrbach dominikus zel stefan thater bernt schiele and manfred pinkal

grounding action descriptions in videos
tacl
anna rohrbach marcus rohrbach wei qiu nemarie friedrich manfred pinkal and bernt schiele

coherent multi sentence video in scription with variable level of detail
tern recognition german conference gcpr pages
alexander m
rush sumit chopra and jason weston

a neural attention model for abstractive in proceedings of the tence summarization
conference on empirical methods in natural guage processing pages
association for computational linguistics
shagan sah sourabh kulhare allison gray hashini venugopalan emily prudhommeaux and raymond ptucha

semantic text in applications of computer tion of long videos
vision wacv ieee winter conference on pages
ieee
ramon sanabria ozan caglayan shruti palaskar desmond elliott loc barrault lucia specia and florian metze

a large scale dataset for multimodal language understanding
in ings of the workshop on visually grounded tion and language vigil
nips
ramon sanabria shruti palaskar and florian metze

cmu sinbad s submission for the avsd challenge
in proc
dialog system technology challenges workshop at aaai honolulu hawaii usa
abigail see peter j
liu and christopher d
manning

get to the point summarization with generator networks
in proceedings of the nual meeting of the association for computational linguistics volume long papers pages
rico sennrich orhan firat kyunghyun cho dra birch barry haddow julian hitschler marcin junczys dowmunt samuel lubli antonio rio miceli barone jozef mokry and maria nadejde

nematus a toolkit for neural machine lation
in proceedings of the software tions of the conference of the european ter of the association for computational tics pages
association for computational linguistics
gunnar a
sigurdsson gl varol xiaolong wang ali farhadi ivan laptev and abhinav gupta

hollywood in homes crowdsourcing data in european tion for activity understanding
ference on computer vision
jingkuan song yi yang zi huang heng tao shen and richang hong

multiple feature ing for real time large scale near duplicate video in proceedings of the acm retrieval
national conference on multimedia pages
acm
yale song jordi vallmitjana amanda stent and jandro jaimes

tvsum summarizing web videos using titles
in proceedings of the ieee ference on computer vision and pattern recognition pages
md arafat sultan steven bethard and tamara sumner

back to basics for monolingual alignment exploiting word similarity and contextual evidence
transactions of the association for computational linguistics
ilya sutskever james martens and geoffrey e ton

generating text with recurrent neural networks
in proceedings of the international conference on machine learning pages
jmlr
org
ilya sutskever oriol vinyals and quoc v le

sequence to sequence learning with neural networks
in advances in neural information processing tems pages
curran associates inc
atousa torabi niket tandon and leonid sigal
learning language visual embedding for
movie understanding with natural language
corr

oriol vinyals meire fortunato and navdeep jaitly
in advances in neural
pointer networks
information processing systems pages
curran associates inc
meng wang richang hong guangda li zheng jun zha shuicheng yan and tat seng chua

event driven web video summarization by tag ieee ization and key shot identication
tions on multimedia
kristian woodsend and mirella lapata

multiple aspect summarization using integer linear ming
in proceedings of the joint conference on empirical methods in natural language ing and computational natural language learning pages
michihiro yasunaga rui zhang kshitijh meelu ayush pareek krishnan srinivasan and dragomir r
radev

graph based neural multi document summarization
in proceedings of the ence on computational natural language learning conll pages
kuo hao zeng tseng hung chen juan carlos niebles and min sun

generation for user generated videos
in european conference on puter vision pages
springer
jianguo zhang pengcheng zou zhao li yao wan ye liu xiuming pan yu gong and philip s yu
product title renement via multi modal
arxiv preprint generative adversarial

learning
luowei zhou chenliang xu and jason j
corso

towards automatic learning of procedures from web in proceedings of the instructional videos
second aaai conference on articial intelligence pages
a appendix a
experimental setup in all our experiments the text encoder consists of bidirectional layers of the encoder with gated recurrent units gru cho et al
and layers of the decoder with conditional gated recurrent units cgru sennrich et al

we optimize the models with the adam optimizer kingma and ba with learning rate halved after each epoch when the validation mance does not increase for maximum epochs
we restrict the input length to tokens for all experiments except the best text only model in the section experiments and results
we use ulary the most frequently occurring words which showed best results in our experiments largely outperforming models using subword based vocabularies
we ran all experiments with the nmtpytorch toolkit caglayan et al

set words transcript the to and you a it that of is i going we in your this s so on summary in a this to free the video and learn from on with how tips for of expert an table most frequently occurring words in script and summaries
a
frequent words in transcripts and summaries table shows the frequent words in transcripts input and summaries output
the words in transcripts reect conversational and spontaneous speech while words in the summary reect their descriptive nature
a
output examples from different models table shows example outputs from our different text only and text and video models
the text only model produces a uent output which is close to the reference
the action features with the rnn model which sees no text in the input produces an in domain y tying and shing abstractive summary that involves more details like ment which is missing from the text based models but is relevant
the action features without rnn model belongs to the relevant domain but contains fewer details
the nearest neighbor model is lated to knot tying but not related to shing
the scores for each of these models reect their respective properties
the random baseline output shows the output of sampling from the random guage model based baseline
although it is a uent output the content is incorrect
observing other outputs of the model we noticed that although dictions were usually uent leading to high scores there is scope to improve them by predicting all details from the ground truth summary like the subtle selling point phrases or by using the visual features in a different adaptation model
a
attention analysis figure shows an analysis of the attention tributions using the hierarchical attention model in an example video of painting
the vertical axis denotes the output summary of the model and the horizontal axis denotes the input time steps from the transcript
we observe less attention in the rst no
model r l c output reference watch and learn how to tie thread to a hook to help with y tying as explained by out expert in this free how to video on y tying tips and techniques
ground truth text action feat
text only truth asr output tion feat






learn from our expert how to attach thread to y shing for y shing in this free how to video on y tying tips and techniques
learn from our expert how to tie a thread for y shing in this free how to video on y tying tips and techniques
learn how to tie a y knot for y shing in this free how to video on y tying tips and techniques
asr output

learn tips and techniques for y shing in this free shing video on techniques for and making y shing nymphs
action features rnn

learn about the equipment needed for y tying as well as other y shing tips from our expert in this free how to video on y tying tips and techniques
action only features

learn from our expert how to do a double half hitch knot in this free video clip about how to use y shing
next neighbor

use a sheep shank knot to shorten a long piece of rope
learn how to tie sheep shank knots for shortening rope in this free knot tying video from an eagle scout
random baseline

learn tips on how to play the bass drum beat variation on the guitar in this free video clip on music theory and guitar lesson
table example outputs of ground truth text and video with hierarchical attention text only with truth text only with asr output asr output text andv video with hierarchical attention action features with rnn and action features only models compared with the reference the topic based next neighbor and random baseline
arranged in the order of best to worst summary in this table
figure visualizing attention over video features
part of the video where the speaker is introducing the task and preparing the brush
in the middle half the camera focuses on the close up of brush strokes with hand to which the model pays higher attention over consecutive frames
towards the end the close up does not contain the hand but only the paper and brush where the model again pays less attention which could be due to unrecognized cutcuttalking and preparing the brushclose up of brushstrokes handblack frames at the end close up of brushstrokes no hand tions in the close up
there are black frames in the very end of the video where the model learns not to pay any attention
in the middle of the video there are two places with a cut in the video when the camera shifts angle
the model has learned to identify these areas and uses it effectively
from this particular example we see the model using both modalities very effectively in this task of the summarization of open domain videos
a
human evaluation details to understand the outputs generated for this task better we ask workers on amazon mechanical turk to compare outputs of unimodal and modal models with the ground truth summary and assign a score between lowest and highest for four metrics informativeness relevance herence and uency of generated summary
the annotators were shown the ground truth summary and a candidate summary without knowledge of the type of modality used to generate it
each ample was annotated by three workers
annotation was restricted to english speaking countries
annotators participated in this task

