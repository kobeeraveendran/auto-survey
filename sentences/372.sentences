c e d l c
s c v
v i x r a leveraging parsbert and pretrained for persian abstractive text summarization preprint compiled december mehrdad mohammad and mohammad
of computer engineering islamic azad university north tehran branch tehran iran m
tnb
ac
ir
of electrical engineering and robotics queensland university of technology brisbane australia mohammad

qut
edu
au
of electrical and electronic engineering shahed univerisity tehran iran
ac
ir abstract text summarization is one of the most critical natural language processing nlp tasks
more and more researches are conducted in this eld every day
pre trained transformer based encoder decoder models have begun to gain popularity for these tasks
this paper proposes two methods to address this task and introduces a novel dataset named pn summary for persian abstractive text summarization
the models employed in this paper are and an encoder decoder version of the parsbert model i
e
a monolingual bert model for persian
these models are ne tuned on the pn summary dataset
the current work is the rst of its kind and by achieving promising results can serve as a baseline for any future work
keywords text summarization abstractive summarization pre trained based bert introduction with the emergence of the digital age a vast amount of textual information has become digitally available
dierent natural language processing nlp tasks focus on dierent aspects of this information
automatic text summarization is one of these tasks and concerns about compressing texts into shorter formats such that the most important information of the content is served
this is crucial in many applications since ing summaries by humans however precise can become quite a time consuming and cumbersome
such applications include text retrieval systems used in search engines to display a marized version of the search results
text summarization can be viewed from dierent perspectives including single document vs
multi document and monolingual vs
multi lingual
however an important pect of this task is the approach which is either extractive or abstractive
in extractive summarization a few sentences are selected from the context to represent the whole text
these sentences are selected based on their scores or ranks
these scores are determined by computing certain features such as the ordinal position of sentences concerning one another length of the sentence a ratio of nouns
after sentences are ranked the top n sentences are selected to represent the whole text
abstractive summarization techniques create a short version of the original text by generating new sentences with words that are not necessarily found in the original text
compared to extractive summarization abstractive techniques are more daunting yet more attractive and exible
therefore more and more attention is given to abstractive techniques in dierent languages
however to the best of our knowledge too few works have been dedicated to text summarization in the persian language of which almost all are extractive
this is partly due to the lack of proper persian text datasets available for this task
this is the primary motivation behind the current work to ate an abstractive text summarization framework for the persian language and compose a new properly formatted dataset for this task
there are dierent approaches towards abstractive text rization especially for the english language of which many are based on sequence to sequence structures as text summarization can be viewed as a task
in a encoder decoder model in which a deep current generative decoder is used to improve the tion quality is presented
the model presented in is an attentional encoder decoder recurrent neural network rnn used for abstractive text summarization
in a new ing method is introduced that combines reinforcement learning with supervised word prediction
an augmented version of a model is presented in
similarly an extended sion of encoder decoder architecture that benets from an formation selection layer for abstractive summarization is preprint leveraging parsbert and pretrained for persian abstractive text summarization
sequence to sequence parsbert parsbert is a monolingual version of bert language model for the persian language that adopts the base guration of the bert model i
e
hidden layers hidden size of with attention heads
bert is a based language model with an encoder only architecture that is shown in gure
in this architecture the input sequence


xn is mapped to a contextualized encoded sequence by going through a series of bi directional attention blocks with two feed forward layers in each block
the output sequence can then be mapped to a task specic put class by adding a classication layer to the last hidden layer



n sented in
many of the works mentioned above benet from pre trained language models as these models have started to gain dous popularity over the past few years
this is because they simplify each nlp task to a lightweight ne tuning phase by employing transfer learning benets
therefore an approach to pre train a structure for text summarization can be quite promising
bert and are amongst widely used pre trained language modeling techniques
bert uses a masked language model mlm and an encoder decoder stack to perform conditioning on the left and right context
on the other hand is a unied framework that employs text text format to address nlp text based problems
a multilingual variation of the model is called that covers dierent languages and is trained on a common crawl based dataset
due to its multilingual property the model is a suitable option for languages other than english
the bert model also has a multilingual version
however there are numerous monolingual variations of this model that have shown to outperform the multilingual version on various nlp tasks
for the persian language the parsbert model has shown state of the art on many persian nlp tasks such as named entity recognition ner and sentiment analysis
although pre trained language models have been quite ful in terms of natural language understanding nlu tasks they have shown less eciency regarding tasks
as a result in the current paper we seek to address the mentioned shortcomings for the persian language regarding text rization by making the following contributions introducing a novel dataset for the persian text marization task
this dataset is publicly available for anyone who wishes to use it for any future work
investigating two dierent approaches towards stractive text summarization for persian texts
one is to use the parsbert model in a structure as presented in
the other one is to use the model
both models are ne tuned on the proposed dataset
the rest of this paper is structured as follows
section lines the parsbert encoder decoder model as well as
in section an overview of the ne tuning and text generation congurations for both approaches is provided
the composition of the dataset and its statistical features are duced in section
this section also outlines the metrics used to measure the performance of the models
section presents the results obtained from ne tuning the dataset mentioned in earlier models
finally section concludes the paper
models in this section an overview of sequence to sequence bert and architecture is provided

com hooshvare pn summary figure the encoder only architecture of bert
other tions of bert such as parsbert have the same architecture
bert model achieves state of the art performance on nlu tasks by mapping input sequences to output sequences with a priori known output lengths
however since the output quence dimension does not rely on the input it is impractical to use bert for text generation summarization
in other words any bert based model corresponds to the architecture of only the encoder part of transformer based encoder decoder models which are mostly used for text generation
on the other hand decoder only models such as preprint leveraging parsbert and pretrained for persian abstractive text summarization can be used as a means of text generation
however it has been shown that encoder decoder structures can perform better for such a task
as a result we used parsbert to warm start both encoder and decoder from an encoder only checkpoint as mentioned in to achieve a pre trained encoder decoder model or which can be ne tuned for text summarization using the dataset introduced in section
in this architecture the encoder layer is the same as the bert transformer layers
the decoder layers are also the same as that of parsbert with a few changes
first cross attention layers are added between self attention and feed forward ers in order to condition the decoder on the contextualized coded sequence e

the output of the parsbert model
ond the bi directional self attention layers are changed into uni directional layers to be compatible with the auto regressive generation
all in all while warm starting the decoder only the cross attention layer weights are initialized randomly and all other weights are parsbert s pre trained weights
the proposed gure illustrates the building blocks of model warm started with the parsbert model along with an example text and its summarized version ated by the proposed model
the persian language in various situations e

forming plural in the example text shown in gure the word nouns
is actually composed of three tokens noun pluralizing token where the token represents the half space token that connect the noun to the pluralizing token
after that the text is fed into the encoder block the result of the encoder block is fed to the decoder block which in turn generates the output summary
the half character tokens are then converted to actual half characters by the particular token decoder block

stands for multilingual text to text transfer transformer multilingual and is a multilingual version of the model
is an encoder decoder transformer architecture that closely reects the primary building block of the original transformer model and covers the following objectives language modeling to predict the next word
de shuing to redene the original text
corrupting spans to predict masked words
network architecture inherits and transforms the previous unifying frameworks for down stream nlp tasks into a text text format
in other words the architecture allows for employing the encoder decoder procedure to aggregate every possible nlp task into one network
thus the same parameters and loss function are used for every task
this is shown in gure
figure architecture along with an ple persian text and its summarized version generated by the model
in this gure the input text is rst fed to a special token encoder that handles half space character unicode and moves unwanted tokens
half space character is widely used in figure as a unied framework for down stream nlp tasks
the diagram shows each down stream task in a to text format including translation red linguistic ity blue sentence similarity yellow and text summarization green
inherits all capabilities of the model
was trained on an extended version of the dataset that contains more than and web page contents in languages including persian over monthly scrapes to date
compared to other multilingual models like multilingual bert xlm r and multilingual bert no support for persian reaches state of the art on all the tasks especially on the summarization task
gure illustrates the architecture after ne tuning along with an example text
in this schema the hfs token represents preprint leveraging parsbert and pretrained for persian abstractive text summarization the half space character in persian and summarize serves as a text to text ag for summarization task
sequences as compared to a greedy search
one drawback is that beam search tends to generate sequences with some words repeated
to overcome this issue we utilize n grams penalties
this way if a next word causes the generation of an already seen n grams the probability of that word will be set to manually thus preventing that n gram from being repeated
another parameter used in beam search is early stopping which can be either active or inactive
if active text generation is stopped when all beam hypotheses reach the eos token
the number of beams the n grams penalty sizes the length penalty and early stopping values used for and models in the current work are presented in table
table beam search conguration for and models for auto regressive text summarization after ne tuning
beams repetitive n gram size length penalty

early stoping status active active evaluation for evaluating the performance of the two architectures duced in this paper we composed a new dataset by crawling numerous articles along with their summaries from dierent news agency websites hereafter denoted as pn summary
both models are ne tuned on this dataset
therefore this is the rst time this dataset is being proposed to be used as a benchmark for persian abstractive summarization
this dataset includes a total of documents and covers a range of categories from economy to tourism
the frequency distribution of the article categories and the number of articles from each news agency can be seen from gures and respectively
it should be noted that the number of tokens in article maries is varying
this can be viewed in gure
as shown from this gure most of the articles summaries have a length of around tokens
to determine the performance of the models we use oriented understudy for gisting evaluation rouge metric package
this package is widely used for automatic marization and machine translation evaluation
the metrics cluded in this package compare an automated summary against a reference summary for each document
there are ve ent metrics included in this package
we calculate the score for three of these metrics to show the overall performance of both models on the proposed dataset unigram scoring which computes the overlap of uni grams between the generated and the reference summaries
bigram scoring which computes the overlap of bigrams between the generated and the erence summaries
figure architecture solution and an example persian text and its summarized version generated by the model
configurations
fine tuning conguration to fine tune both models presented in section on the summery dataset introduced in section we have used the adam optimizer with warm up steps a batch size of and training epochs
the learning rate for parsbert and are and respectively

text generation conguration the text generation process refers to the decoding strategy for auto regressive language generation after the ne tuned model
in essence the auto regressive generation is centered around the assumption that the probability distribution of any word quence can be decomposed into a product of conditional next word distributions as denoted by equation where is the initial context word and t is the length of the word sequence
t the objective here is to maximize the sequence probability by choosing the optimal tokens words
one method is greedy search in which the next word selected is simply the word with the highest probability
this method however neglects words with high probabilities if they are hidden behind some low probability words
to address this problem we use beam search method that keeps nbeams number of most likely sequences i
e
beams at each time step and eventually chooses the one with the highest overall probability
beam search generates higher probability preprint leveraging parsbert and pretrained for persian abstractive text summarization rouge l scoring in which the scores are lated at sentence level
in this metric new lines are ignored and longest common subsequence lcs is computed between two text pieces
results and discussion this section presents the results obtained from ne tuned and parsbert based structure on the proposed pn summary dataset
the scores on three dierent rouge metrics discussed in section are reported in table
it can be seen that the parsbert structure achieves higher scores as compared to the model
this could be due to the fact that encoder decoder weights i
e
parsbert weights in this architecture are concretely tuned on a massive persian corpus making it a tter architecture for persian only tasks
table depicts rouge scores on the test set
the jective of models and baselines is abstractive
the two models are ne tuned on the persian news summarization dataset summary
rouge model r l





since no other pre trained abstractive summarization methods have been proposed for persian language and since this is the rst time the pn summary dataset is being introduced and leased it is impossible to compare the results of the present work with any other baseline
as a result the outcomes sented in this work can serve as a baseline for any future stractive methods for the persian language that seeks to train their model on the proposed pn summary dataset presented and released with the current work
to further illustrate these two models performance we have included two examples from the dataset in table
the main text the actual summary and the summaries generated by the and models are shown in this table
based on this table the summary given by the model in both examples is relatively closer to the actual summary in terms of both meaning and lexical choices
conclusion limited work has been dedicated to text summarization for the persian language of which none are abstractive based on trained models
in this paper we presented two pre trained methods and designed to address text summarization in sian with an abstract approach one is based on a multilingual model and the other is a warm started from the parsbert language model
we have also composed and leased a new dataset called pn summary for text summarization since there is an apparent lack of such datasets for the persian language
the results of ne tuning the proposed methods on the mentioned dataset are promising
due to a lack of works in this area our work could not be compared to any earlier work figure the frequency of article categories in the proposed dataset
figure the number of articles extracted from each of the news agency website
figure token length distribution of articles summaries
preprint leveraging parsbert and pretrained for persian abstractive text summarization table examples of highly abstractive reference summaries from persian news network using and models
each example consists of the trim article the true summary and the generated summaries by both models
references example and can now serve as a baseline for any future works in this eld
ani nenkova and kathleen mckeown
a survey of text summarization techniques
in mining text data pages
springer
harold p edmundson
new methods in automatic ing
journal of the acm jacm
andrew turpin yohannes tsegay david hawking and hugh e williams
fast generation of result snippets in web search
in proceedings of the annual tional acm sigir conference on research and ment in information retrieval pages
aarti patil komal pharande dipali nale and roshani international agrawal
automatic text summarization
journal of computer applications
janara christensen stephen soderland oren etzioni et al
towards coherent multi document summarization
in proceedings of the conference of the north ican chapter of the association for computational tics human language technologies pages
ani nenkova lucy vanderwende and kathleen own
a compositional context sensitive multi document summarizer exploring the factors that inuence rization
in proceedings of the annual international acm sigir conference on research and development in information retrieval pages
mahak gambhir and vishal gupta
recent automatic text summarization techniques a survey
articial gence review
vishal gupta and gurpreet singh lehal
a survey of text summarization extractive techniques
journal of emerging technologies in web intelligence
piji li wai lam lidong bing and z
wang
deep current generative decoder for abstractive text tion
arxiv

ramesh nallapati bowen zhou c
d
santos c aglar gulcehre and b
xiang
abstractive text summarization using sequence to sequence rnns and beyond
in conll
romain paulus caiming xiong and r
socher
a deep reinforced model for abstractive summarization
arxiv

a
see peter j
liu and christopher d
manning
get to the point summarization with pointer generator works
arxiv

wei li x
xiao yajuan lyu and yuanzhuo wang
proving neural abstractive document summarization with in emnlp explicit information selection modeling

jacob devlin ming wei chang kenton lee and kristina toutanova
bert pre training of deep tional transformers for language understanding
arxiv

preprint leveraging parsbert and pretrained for persian abstractive text summarization colin rael noam shazeer adam roberts katherine lee sharan narang m
matena yanqi zhou w
li and peter j
liu
exploring the limits of transfer learning with a unied text to text transformer
j
mach
learn
res

linting xue noah constant a
roberts mihir kale rami al rfou aditya siddhant a
barua and colin fel
a massively multilingual pre trained text to text transformer
arxiv

wissam antoun fady baly and hazem m
hajj
arabert transformer based model for arabic language ing
arxiv

louis martin benjamin muller pedro javier ortiz suarez yoann dupont laurent romary eric de la clergerie djame seddah and benot sagot
camembert a tasty french language model
arxiv

mehrdad farahani mohammad gharachorloo marzieh farahani and m
manthouri
parsbert based model for persian language understanding
arxiv

sascha rothe shashi narayan and a
severyn
ing pre trained checkpoints for sequence generation tasks
transactions of the association for computational guistics
ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan n
gomez lukasz kaiser and illia polosukhin
attention is all you need
arxiv

a
radford jerey wu r
child david luan dario amodei and ilya sutskever
language models are supervised multitask learners

colin rael noam shazeer adam roberts katherine lee sharan narang m
matena yanqi zhou w
li and peter j
liu
exploring the limits of transfer learning with a unied text to text transformer
j
mach
learn
res

ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan n
gomez lukasz kaiser and illia polosukhin
attention is all you need
alexis conneau kartikay khandelwal naman goyal vishrav chaudhary guillaume wenzek francisco guzman edouard grave myle ott luke zettlemoyer and veselin stoyanov
unsupervised cross lingual resentation learning at scale
yinhan liu jiatao gu naman goyal xian li sergey edunov marjan ghazvininejad mike lewis and luke zettlemoyer
multilingual denoising pre training for ral machine translation
g
klein yoon kim y
deng jean senellart and der m
rush
opennmt open source toolkit for neural machine translation
arxiv

chin yew lin
rouge a package for automatic tion of summaries
in acl

