supmmd a sentence importance model for extractive summarization using maximum mean discrepancy umanga alexander patrick aditya krishna menon lexing national university canberra act australia google research new york ny united states
bista alex
mathews lexing

edu
au
com t c o l c
s c v
v i x r a abstract most work on multi document summarization has focused on generic summarization of mation present in each individual document set
however the under explored setting of update summarization where the goal is to identify the new information present in each set is of equal practical interest e

presenting readers with updates on an evolving news topic
in this work we present supmmd a novel technique for generic and update summarization based on the maximum mean discrepancy from kernel two sample testing
supmmd combines both supervised learning for salience and vised learning for coverage and diversity
ther we adapt multiple kernel learning to make use of similarity across multiple information sources e

text features and knowledge based concepts
we show the efcacy of supmmd in both generic and update summarization tasks by meeting or exceeding the current state of the art on the and datasets
introduction multi document summarization is the problem of producing condensed digests of salient information from multiple sources such as articles
concretely suppose we are given two sets of articles denoted set a and set b on a related topic e

climate change the pandemic separated by publication timestamp or geographic region
we may then identify three possible instantiations of multi document summarization see figure i generic summarization where the goal is to summarize a set a or b individually
comparative summarization where the goal is to summarize a set b against another set a while highlighting the differences
iii update summarization where the goal is both generic summarization of set a and comparative summarization of set b versus a
most existing work on this topic has focused on the generic summarization task
however figure different summarization tasks generic comparative c update
two sets of articles set a and b are denoted by red and blue circles respectively
summary prototypes are bold circles and information coverage of each tasks is lled with respective colors
update summarization is of equal practical interest
intuitively the comparative aspect of this setting aims to inform a user of new information on a topic they are already familiar with
extractive multi document summarization methods can be unsupervised or supervised
pervised methods typically dene salience or erage using a global model of sentence sentence similarity
methods based on retrieval goldstein et al
centroids radev et al
graph centrality erkan and radev or utility maximization lin and bilmes gillick and favre have been well explored
however sentence salience also depends on surface features e

position length presence of cue words effectively capturing these requires supervised models specic to the dataset and task
a body of work has incorporated such information through supervised learning for example based on point processes kulesza and taskar learning important words hong and nenkova graph neural networks yasunaga et al
and support vector regression varma et al

these supervised methods have either a separate model for learning and inference leading to a disconnect between learning sentence salience and sentence selection varma et al
yasunaga et al
hong and nenkova or are designed specically for generic summarization kulesza and taskar
in this work we propose supmmd which has a single model of learning salience and inference and can be applied to generic and comparative summarization
we make the following contributions we present supmmd a novel technique for both generic and update summarization that combines supervised learning for salience and unsupervised learning for coverage and diversity
supmmd has a single model for learning and inference
we adapt multiple kernel learning cortes et al
into our model which allows similarity across multiple information sources e

text features and knowledge based concepts to be used
we show that supmmd meets or exceeds the state of the art in generic and update tion on the and datasets
literature review multi document summarization can be extractive where salient pieces of the original text such as sentences are selected to form the summary or abstractive where a new text is generated by paraphrasing important information
the former is popular as it often creates semantically and matically correct summaries nallapati et al

in this work we focus on generic and update document summarization in the extractive setting
most extractive summarizers have two nents sentence scoring and selection
a variety of unsupervised and supervised methods have been developed for the former
unsupervised sentence scorers are based on centroids radev et al
graph centrality erkan and radev retrieval relevance goldstein et al
word statistics nenkova and vanderwende topic models haghighi and vanderwende or concept coverage gillick and favre lin and bilmes
supervised techniques include using a graph based neural network yasunaga et al
learning sentence quality from point processes kulesza and taskar combining word importances hong and nenkova combining sentence and phrase importances cao et al
or employing a mixture of submodular functions lin and bilmes
sentence selection methods can be broadly egorized as greedy methods goldstein et al
radev et al
erkan and radev nenkova and vanderwende cao et al
haghighi and vanderwende hong and nenkova kulesza and taskar cao et al
varma et al
which produce approximate solutions by iteratively selecting the sentences with the maximal score or exact integer linear programming ilp based methods gillick and favre cao et al

some greedy methods use an objective which belongs to a special class of set functions called submodular functions lin and bilmes kulesza and taskar which have good approximation guarantees under greedy optimization nemhauser et al

there has been limited research into update and comparative summarization
notable prior work includes maximizing concept coverage using ilp gillick et al
learning sentence scores using a support vector regressor varma et al
and temporal content ltering zhang et al

bista et al
cast the comparative summarization problem as classication and use mmd gretton et al

in this work we adapt their method to learn sentence importances driven by surface features
summarization as classication we review a perspective introduced by bista et al
where summarization is viewed as classication and provide a brief introduction to maximum mean discrepancy mmd
both these ideas form the basis of our subsequent method

generic summarization as classication let v be t topics of articles that we wish to summarize
for a topic t we wish to select summary sentences st
bista et al
lated summarization as selecting prototypes that minimize the accuracy of a powerful classier between sentences in the input and summary
the intuition is that a powerful classier should not be able to distinguish between the sentences from articles and summary sentences
formally we pick st argmax t s
sst where st l comprise subsets of v t with upto l words and y is the accuracy of the best possible classier that distinguishes between elements in sets x and y we shall shortly realize this using mmd

comparative summarization as competing binary classication for comparative summarization between two sets a and b bista et al
introduced an additional term into
giving rise to competing goals for the classier it should not be able to distinguish between the summaries and sentences from set b but should be able to distinguish between the maries and sentences from set a
formally let v t b be the set of sentences in set b v t a be the sentences in set to compare set a
then for suitable we seek st the summary sentences of set b st argmax t b t a

sst the hyperparameter controls the relative tance of accurately representing articles in set b versus not representing the articles in set a

maximum mean discrepancy mmd the mmd is a kernel based measure of the distance between two distributions
more formally denition

let h be a reproducing kernel hilbert space rkhs with associated kernel
let f be the set of functions h x r in the unit ball of h where x is a topological space
then the mmd between distributions p q is the maximal difference in expectations of functions from f under p q gretton et al
q sup hf e xp e yq

a small mmd value indicates that p q are similar
given nite samples x pn and y qm an empirical estimate of the mmd denoted as y can be computed as y nm y y


mmd for summarization the mmd corresponds to the minimal achievable loss of a centroi based kernel classier rumbudur et al

consequently we use s to approximate the s in
and
using a suitable kernel k that measures the similarity of two sentences
intuitively this selects summaries s which best represent the distribution of original sentences v
note that if we expand s as per
and later in
the rst term is irrelevant for optimization
the second and third term capture the coverage and diversity of the summary tences without any supervision
hence this is an unsupervised summarization
the supmmd method we start by developing a technique for incorporating sentence importance into mmd for the purpose of generic multi document extractive summarization
we then extend this method to comparative summarization and incorporate multiple different kernels to use a diverse sets of features

from mmd to weighted mmd that cover unsupervised mmd bista et al
selects relevant representative sentences concepts while retaining diversity
the notion of representativeness is based on a global model of sentence sentence similarity however this notion of representativeness is not necessarily well matched to the selection of salient information
salience of a sentence may be determined by surface features such as position in the article or number of words
for example news articles are often written such that sentences at the start of a article have the characteristics of a summary kedzie et al

learning a notion of salience that is specic to the summarization task and dataset requires supervised training
thus we extend the mmd model by porating supervised sentence importance weighting
let v s x be independent samples drawn from the distributions of article sentences p and summary sentences q on the space of all sentences x
we dene non negative importance functions q parameterized by learnable parameters
we restrict these functions so that epf p v and eqf q s
equipped with f we may modify mmd such that the importance of sentences which are good summary candidates is increased
denition

the weighted mmd q between p q is q
sup hf p e p q note that classic mmd
is a special case of
where f
in practice the supremum over all h is impossible to compute directly
we thus derive an alternative form for equation

lemma

for
is equivalently p q

in the above x f is a canonical feature mapping of sentences and summaries from x to rkhs
the derivation which mirrors a similar derivation for mmd gretton et al
is given in the appendix

importance function we use log linear models as importance functions as they are a common choice of sentence tance kulesza and taskar and easy to t when training data is scarce
formally the log linear importance function is where is the surface features of sentence v
we can dene the empirical estimates nt s of the importance functions p v as v and q nt v s nt
where nt is the number of sentences and mt is the number of summary sentences in topic t

training generic summarization the parameters of the log linear importance function must be learned from data so we dene a loss function based on weighted mmd
let v t be the t training tuples
then the loss of topic t is the square of importance weighted empirical mmd between sentences and summary sentences from within the topic st t st
where the weighted trick to equation
gives see appendix t st is an empirical estimate of q
applying the kernel lt nt nt t v s v nt t s nt mt s mt
equation
is the loss for a single topic but during training we will instead minimize the average loss over all topics in the training set i
e
t st
intuitively we learn min the parameters by minimizing an importance weighted distance between sentences and ground truth summary sentences over all topics
t
training comparative summarization we now extend the learning task to comparative marization using the competing binary classiers idea of bista et al



specically we replace the accuracy terms in equation
with the square of weighted mmd
given the t comparative training tuples v t then the objective is to minimize a b v t min b a t t t b st t a st
note there are two sets of importance parameters b a one for each of the two document sets

multiple kernel learning we employ multiple kernel learning mkl to make use of data from multiple sources in our mmd summarization framework
we adapt two stage kernel learning cortes et al
where different kernels are linearly combined to maximize the alignment with the target kernel of the classication problem
since mmd can be interpreted as classiability sriperumbudur et al
mkl ts neatly into our mmd based summarization objective
intuitively mkl should identify a good combination of kernels for building a classier that separates summary and non summary sentences
untkt let be p kernel functions
for topic t let kt i be the kernel matrix according to kernel function ki and kt iunt be the centered kernel matrix with unt i nt
let yt be the ground truth summary labels with yt i iff i st
the target kernel represents the ideal notion of similarity between sentences
the non negative kernel weights w which lead to the optimal alignment with the target kernel are given by cortes et al

min where mt rpp has mt i
has ai rs and at rp the kernel function must be characteristic for mmd to be a valid metric muandet et al

most popular kernels used for bag of words like text features including tf idf the linear kernel y and the cosine kernel y are not characteristic budur et al

fortunately the exponential kernel y y is characteristic for any kernel steinwart
hence we use the normalized exponential kernel combined with the cosine kernel y

inference given a learned importance function f we may nd the best set of summary sentences st for generic summarization via st argmax t st
sst similarly for the comparative task with learned importance functions we seek st as to extract relevant text and then perform sentence and word tokenization
for duc we clean the text using various regular expressions the details of which are provided in our code release
we train punktsentencetokenizer to detect sentence boundaries and use the standard nltk bird word tokenizer
for the tac dataset we use the preprocessing pipeline employed by gillick et al

this enables a cleaner comparison with the state of the art icsi gillick et al
method on the tac dataset
for all datasets we keep the sentences between and words per yasunaga et al

argmax sst t b st t a st a

feature representations both these inference problems are budgeted maximization problems which are often solved by greedy algorithms lin and bilmes
the generic unsupervised summarization task is submodular and monotone under certain tions kim et al
so greedy algorithms have good theoretical guarantees nemhauser et al

while our supervised variants do not have these guarantees we nd that greedy optimization nonetheless leads to good solutions
experimental setup we include guidance on applying supmmd and the details required to reproduce our experiments

datasets we use four standard multi document tion benchmark datasets and dataset statistics are provided in table
each of these datasets has multiple topics where each topic in turn has multiple news articles and four human written summaries
in one setting we use as the training set and as test set and in another setting we use as the training set and as the test set both settings are common in the literature
the duc datasets can be used for generic summarization while tac being an update summarization task can be used for both generic set a and comparative summarization set b

data preprocessing and preparation the duc and tac datasets are provided as collections of xml documents so it is necessary
nist
gov data
html our method requires two different sets of sentence features text features which are used to compute the sentence sentence similarity as part of the kernel and surface features which are used in learning the sentence importance model


text features each sentence has three different feature sentations unigrams bigrams and entities
the unigrams are stemmed words with stop words from the nltk english list removed
the bigrams are a combination of stemmed unigrams and bigrams
the entities are dbpedia concepts extracted using dbpedia spotlight mendes et al

we use a term frequency inverse sentence frequency tf isf neto et al
tation for all text features
tf isf has been used extensively in multi document summarization dias et al
alguliev et al
wan et al



surface features we use surface features for the duc dataset and for the tac dataset position there are ve position features
four indicators denote the or a later position of the sentence in the article
the nal feature gives the position relative to the length of the article
counts there are two count features the number of words and number of nouns
we use the spacy part of speech tagging to nd nouns
tsf this is the sum of the ts isf scores for unigrams composing the sentence
for sentence this is ws s where is the inverse sentence frequency of unigram w and s is the term frequency of w in s

com benob icsisumm
io dataset topics sents avg summ sents avg summ sents oracle ours oracle liu and lapata a b a b



































table dataset statistics and oracle performance
we report the number of topics in each dataset along with the number of sentences after preprocessing
we show the rouge scores of our oracle method and the one by liu and lapata with average number of sentence in summary from each method
btsf the boosted sum of ts isf scores for unigrams composing the sentence
specically we compute ws s where we boost the score of unigrams w that appear in the rst sentence of the article as
in the generic summarization for comparative summarization as used by gillick et al

unigrams that do not appear in the rst sentence of the article have
lexrank the lexrank score erkan and radev computed on the bigrams cosine similarity
for the tac datasets we additionally use an indicator whether the sentence begins a paragraph
this is provided by the cessing pipeline from icsi gillick et al

qsim the fraction of topic description unigrams present in each sentence these topic descriptions are only available for tac

oracle extraction both duc and tac provide four human written summaries for each topic
since our goal is tive summarization with supervised training we need to know which sentences in the articles could be used to construct the summaries in the training set
the article sentences that best match the abstractive summaries are called the oracles st
algorithm oracle extraction function v t h t r l st while s argmax sv st st return st l do h t extraction score h t against the human maries h t until a word budget l is reached
we only include sentences between to words as gested by yasunaga et al
and set a budget of words to ensure our oracle summaries are within words consistent with the evaluation

in contrast to liu and lapata which uses only recall score lin our method balances both and recall scores using the harmonic mean and explicitly accounts for sentence length
grid search on the validation sets shows that the optimal value for r is
across different datasets and summarization tasks
as reported in table on average our method produces oracles consisting of more sentences and with higher and scores compared to oracles from liu and lapata
this is consistent across all datasets

implementation details supervised variants use an regularized log linear model of importance
trained using the oracles
as ground truth
we selected the number of training epochs using fold cross validation
we then tune the other hyperparameters on the training set
the hyperparameters of the generic tion task are a parameter of the kernel the regularization weight for the log linear importance function and r which denes the length dependent scaling factor in greedy selection lin and bilmes
the comparative objective
has an additional hyperparameter which controls the comparativeness
more implementation details are provided in the appendix
we will make implementation publicly

evaluation settings our extraction algorithm algorithm is spired by liu and lapata
we greedily select sentences s which provide the maximum gain in to evaluate our methods we use the rouge lin metric the choice for evaluating both
com computationalmedia supmmd generic summarization hong and nenkova cho et al
yasunaga et al
kulesza and taskar and update summarization varma et al
gillick and favre zhang et al
li et al

rouge metrics have been shown to correlate with human judgments lin in generic summarization task
our recent work bista et al
shows that human judgments are consistent with the automatic metrics for evaluating comparative summaries
both duc and tac evaluations use the rst words of the generated summary
our evaluation setup mirrors hong et al

this allows us to compare performance with the state of the art methods they reported and other works also evaluated using this
as is standard for the datasets we report and recall scores
for datasets both set a and b we adopt the evaluation settings from the so we can compare against the three best performing systems in the
as is standard for the dataset we report and rouge recall scores

baselines we select the top performing methods from a recent benchmark paper hong et al
to serve as baselines and report rouge scores from the benchmark paper
they are icsi an integer linear programming method that maximizes coverage gillick et al
dpp a determinantal point process method that learns sentence quality and maximizes diversity kulesza and taskar submodular a method based on a learned mixture of submodular functions lin and bilmes a method base on topic ing conroy et al
regsum a method that focuses on learning word importance hong and nenkova lexrank a popular graph based sentence scoring method erkan and radev
we also include recent deep learning methods evaluated using the same setup as hong et al
and report rouge scores from the individual papers dppsim an extension to the dpp model which learns the sentence sentence similarity using a rouge

with args a

nist
summarization args
a
capsule network cho et al
himap a recurrent neural model that employs a modied pointer generator component fabbri et al
and a model that uses a graph convolution network combined with a recurrent neural network to learn sentence saliency yasunaga et al

as baselines for the dataset we use the top three systems in the competition for each task resulting in four systems altogether
to the best of our knowledge these systems are the current state of the art
we report the rouge scores from the competition
the systems are icsi with two variants sys
uses integer linear programming to maximize coverage of concepts gillick et al
and sys
which additionally uses sentence compression to generate new candidate sentences iit uses a support vector regressor to predict sentence rouge scores varma et al
ictcas temporal method zhang et al
and icl a manifold ranking based method li et al

ltering content a experimental results we compare our methods with the baselines on the a and b datasets
we present several variants of our method to analyze the effects of different components and modeling choices
we report the performance of unsupervised mmd unsupmmd which does not explicitly consider sentence importance
for our supervised method supmmd we report the performance with a bigram kernel supmmd and combined kernels supmmd mkl
we also evaluated the impact of our oracle extraction method by replacing it with the extraction method suggested by liu and lapata in supmmd alt oracles
meanwhile supmmd mkl compress presents the result of applying sentence compression gillick et al
to our model

generic summarization the performance of our methods on the generic summarization task are shown in table
on the dataset all supmmd variants exceed the state of the art when evaluated with and perform similarly to the best existing methods when evaluated with
a icsi gillick et al


dpp kulesza and taskar

submodular lin and bilmes

conroy et al


regsum hong and nenkova

lexrank erkan and radev

dpp sim cho et al


himap fabbri et al


yasunaga et al


unsupmmd supmmd alt oracle supmmd supmmd mkl compress supmmd mkl









table results on generic multi document summarization task
our best system supmmd mkl outperforms the previous best system icsi on score by

while the dpp baseline achieves the highest score on it has a relatively low score which suggests it is optimized for unigram performance at the cost of bigram performance
supmmd mkl strikes a better balance scoring the best in and second best in
on the generic summarization task in table our supmmd mkl model outperforms the state of the art icsi model on both and rouge
specically supmmd mkl scores
in while the best icsi variant scores
in
supervised modeling models using supervised training to identify important sentences tially outperform the unsupervised method supmmd
in fact unsupmmd is the lowest ing method across all metrics and datasets
this strongly indicates that a degree of supervision is essential to perform well in this task and that the importance function is a suitable way to adapt the unsupmmd model to supervised training
over we observe a strong correlation between the the relative position of a sentence and the score given by supmmd
this observation is consistent with previous works kedzie et al
and strates that supmmd has learned to use the surface features to capture salience
further details of ture correlations are provided in the appendix
oracle extraction our oracle extraction technique for transforming abstractive training data to extractive training data helps supmmd methods achieve higher rouge performance
an alternative technique developed by liu and lapata
gillick et al



gillick et al

varma et al


zhang et al

unsupmmd supmmd alt oracle supmmd supmmd mkl compress supmmd mkl













table results on generic multi document summarization task set a
and implemented in supmmd alt oracle gives lower performance than our technique
for example on supmmd alt oracle has a of
and of
while supmmd has a of
and a of

thus the advantages of our proposed cle extraction method are substantial and consistent across multiple datasets and evaluation metrics
multiple kernel learning we observe that combining multiple kernels helps the performance of supmmd models on the generic summarization task
supmmd mkl which combines both bigram and entity kernels has a of
on while supmmd only uses the bigrams kernel and scores
in
multiple kernels show even clearer gains in the a dataset
sentence compression incorporated into the post processing steps of supmmd mkl compress does not clearly improve the results over supmmd mkl
on a compression clearly reduces performance and on supmmd mkl compress has a higher score but a lower score than supmmd mkl
incorporating compression into the summarization pipeline is an appealing direction for future work

comparative summarization the results for the comparative summarization task on the b dataset are shown in table
our supervised mmd variants supmmd and mmd mkl both outperform the state of the art baseline icsi in rouge but fall short in
it would be hard to claim that either method is superior in this instance however it does show that supmmd which uses a substantially different approach to that of icsi provides an alternative state of the art
thus supmmd further b conclusion icsi sys
gillick et al

icsi sys
gillick et al

iiit sys
varma et al

icl sys
li et al

unsupmmd supmmd alt oracle supmmd supmmd mkl compress supmmd mkl















table results on comparative document summarization task set b
maps out the set of techniques that are useful for comparative summarization
as per the generic summarization task both our supervised training method and oracle extraction method are essential for achieving good performance in and rouge
we also identify sentence position and btsf as important features for sentence salience see the appendix
multiple kernels as in supmmd mkl has relatively little effect reducing the score to
from the slightly higher
achieved by supmmd
a similar small decrease is seen for rouge
manual inspection shows that the summaries from supmmd and supmmd mkl methods are largely identical with differences primarily on topic which covers political movements in nepal
the key entities in this topic are not resolved accurately by dbpedia spotlight contributing additional noise and affecting the mkl approach
model variants we have tested an additional variant of our model for comparative summarization which denes two different importance functions one for each of the two document sets a and b see
for details
in contrast supmmd has a single importance function shared between document sets i
e
in equation
a b
performed substantially worse than supmmd in both metrics for example supmmd has a of
while has a of

we conjecture that a single importance function performs better when training data is relatively scarce because it reduces the number of parameters and simplies the learning problem
techniques for tying together the eters for both importance functions such as with a hierarchical bayesian model are left as future work
in this work we present supmmd a novel technique for update summarization based on the maximum mean discrepancy
supmmd combines supervised learning for salience and unsupervised learning for coverage and diversity
further we adapt multiple kernel learning to exploit multiple sources of similarity e

text features and knowledge based concepts
we show the efcacy of supmmd in both generic and update tion tasks on two standard datasets when compared to the existing approaches
we also show that the portance model we introduce on top of our existing unsupervised mmd bista et al
improves the summarization performance substantially on both generic and comparative summarization tasks
for future work we leave the task of rating embeddings features such as bert devlin et al
and evaluating with large generic multi document summarization dataset news fabbri et al

acknowledgments this work is supported in part by data to decisions crc and arc discovery project
this research is also supported by use of the nectar research cloud a collaborative tralian research platform supported by the national collaborative research infrastructure strategy
we thank minjeong shin for helpful feedback and suggestions
references rasim m
alguliev ramiz m
aliguliyev makrufa s
hajirahimova and chingiz a
mehdiyev

mcmr maximum coverage and minimum redundant text summarization model
expert systems with applications
steven bird

nltk the natural language toolkit
in proceedings of the coling acl teractive presentation sessions pages sydney australia
association for computational linguistics
umanga bista alexander patrick mathews minjeong shin aditya krishna menon and lexing xie

comparative document summarisation via classication
in the thirty third aaai conference on articial intelligence aaai the thirty first innovative applications of articial intelligence conference iaai the ninth aaai symposium on educational advances in articial intelligence eaai honolulu hawaii usa january february pages
aaai press
ziqiang cao furu wei li dong sujian li and ming zhou

ranking with recursive neural networks and its application to multi document summarization
in proceedings of the twenty ninth aaai conference on articial intelligence page
aaai press
natural language processing pages boulder colorado
association for computational linguistics
daniel gillick benoit favre dilek hakkani tr bernd bohnet yang liu and shasha xie

the icsi utd summarization system at tac
in tac
sangwoo cho logan lebanoff hassan foroosh and fei liu

improving the similarity measure of determinantal point processes for extractive multi document summarization
in proceedings of the annual meeting of the association for putational linguistics pages florence italy
association for computational linguistics
jade goldstein mark kantrowitz vibhu mittal and jaime carbonell

summarizing text documents sentence selection and evaluation metrics
in ings of the annual international acm sigir conference on research and development in tion retrieval sigir page new york ny usa
association for computing machinery
john conroy sashka t
davis jeff kubina yi kai liu dianne p
oleary and judith d
schlesinger

multilingual summarization dimensionality reduction and a step towards optimal term coverage
in proceedings of the multiling workshop on multilingual multi document summarization pages soa bulgaria
association for computational linguistics
j
b
conway

a course in functional analysis second edition volume of graduate texts in mathematics
springer verlag new york
corinna cortes mehryar mohri and afshin tamizadeh

two stage learning kernel rithms
in proceedings of the annual tional conference on machine learning icml
jacob devlin ming wei chang kenton lee and bert pre training kristina toutanova

transformers for language of deep bidirectional in proceedings of the understanding
ference of the association for computational linguistics human language technologies volume long and short papers pages minneapolis minnesota
association for computational linguistics
the north american chapter of gal dias elsa alves and jos gabriel pereira lopes

topic segmentation algorithms for text summarization and passage retrieval an exhaustive in proceedings of the national evaluation
conference on articial intelligence volume page
aaai press
gnes erkan and dragomir r
radev

lexrank graph based lexical centrality as salience in text summarization
j
artif
int
res

alexander fabbri irene li tianwei she suyi li and dragomir radev

multi news a large scale multi document summarization dataset and tive hierarchical model
in proceedings of the annual meeting of the association for computational linguistics pages florence italy
association for computational linguistics
dan gillick and benoit favre

a scalable global model for summarization
in proceedings of the workshop on integer linear programming for arthur gretton karsten m
borgwardt malte j
rasch bernhard schlkopf and alexander smola

a kernel two sample test

aria haghighi and lucy vanderwende

exploring content models for multi document summarization
in proceedings of human language technologies the annual conference of the north american chapter of the association for computational linguistics pages boulder colorado
association for computational linguistics
kai hong john conroy benoit favre alex kulesza hui lin and ani nenkova

a repository of state of the art and competitive baseline summaries in proceedings for generic news summarization
of the ninth international conference on language resources and evaluation pages reykjavik iceland
european languages resources association elra
kai hong and ani nenkova

improving the estimation of word importance for news in proceedings of the document summarization
conference of the european chapter of the association for computational linguistics pages gothenburg sweden
association for computational linguistics
chris kedzie kathleen mckeown and hal daum iii

content selection in deep learning models in proceedings of the of summarization
conference on empirical methods in natural language processing pages brussels belgium
association for computational linguistics
been kim rajiv khanna and oluwasanmi koyejo

examples are not enough learn to criticize criticism for interpretability
in proceedings of the international conference on neural information processing systems page red hook ny usa
curran associates inc
alex kulesza and ben taskar

determinantal now point processes for machine learning
publishers inc
hanover ma usa
sujian li wei wang and yongwei zhang

tac update summarization of icl
in tac
yujia li kevin swersky and richard zemel

in generative moment matching networks
ceedings of the international conference on international conference on machine learning volume page
jmlr
org
chin yew lin

rouge a package for automatic in text summarization evaluation of summaries
branches out pages barcelona spain
association for computational linguistics
hui lin and jeff bilmes

multi document marization via budgeted maximization of submodular functions
in human language technologies the annual conference of the north american the association for computational chapter of linguistics pages los angeles california
association for computational linguistics
hui lin and jeff bilmes

a class of submodular functions for document summarization
in ings of the annual meeting of the association for computational linguistics human language technologies pages portland oregon usa
association for computational linguistics
hui lin and jeff bilmes

learning mixtures of submodular shells with application to document summarization
page arlington virginia usa
auai press
dong c liu and jorge nocedal

on the limited memory bfgs method for large scale optimization
mathematical programming
yang liu and mirella lapata

text summarization in proceedings of the with pretrained encoders
conference on empirical methods in natural language processing and the international joint conference on natural language processing emnlp ijcnlp pages hong kong china
association for computational linguistics
pablo n
mendes max jakob andrs garca silva and christian bizer

dbpedia spotlight shedding light on the web of documents
in proceedings of the international conference on semantic systems i semantics page new york ny usa
association for computing machinery
krikamol muandet kenji fukumizu bharath budur bernhard schlkopf al

kernel mean embedding of distributions a review and beyond
foundations and trends in machine learning
ramesh nallapati feifei zhai and bowen zhou

summarunner a recurrent neural network based sequence model for extractive summarization of documents
in proceedings of the thirty first aaai conference on articial intelligence page
aaai press
george l nemhauser laurence a wolsey and marshall l fisher

an analysis of tions for maximizing submodular set functions i
mathematical programming
ani nenkova and lucy vanderwende

the impact of frequency on summarization
microsoft research redmond washington tech
rep
msr
joel larocca neto alexandre d santos celso aa kaestner neto alexandre d santos al

document clustering and text summarization
ganapati p patil and calyampudi r rao

weighted distributions and size biased sampling with applications to wildlife populations and human families
biometrics pages
dragomir r radev hongyan jing magorzata stys and daniel tam

centroid based summarization of multiple documents
information processing management
bharath k
sriperumbudur kenji fukumizu arthur gretton gert r
g
lanckriet and bernhard schlkopf

kernel choice and classiability for rkhs embeddings of probability distributions
in proceedings of the international conference on neural information processing systems page red hook ny usa
curran associates inc
bharath k sriperumbudur arthur gretton kenji fukumizu bernhard schlkopf and gert rg lanckriet

hilbert space embeddings and metrics on probability measures
journal of machine learning research
ingo steinwart

on the inuence of the kernel on the consistency of support vector machines
journal of machine learning research
vasudeva varma prasad pingali rahul katragadda sai krishna surya ganesh kiran sarvabhotla harish garapati hareen gopisetty vijay bharath reddy kranthi reddy al

iiit hyderabad at tac
in tac
xiaojun wan jianwu yang and jianguo xiao

towards an iterative reinforcement approach for simultaneous document summarization and keyword extraction
in proceedings of the annual meeting of the association of computational linguistics pages prague czech republic
association for computational linguistics
michihiro yasunaga rui zhang kshitijh meelu ayush pareek krishnan srinivasan and dragomir radev

graph based neural multi document marization
in proceedings of the conference on computational natural language learning conll pages vancouver canada
association for computational linguistics
manzil zaheer sashank reddi devendra sachan satyen kale and sanjiv kumar

adaptive methods for nonconvex optimization
in s
bengio h
wallach h
larochelle k
grauman n
bianchi and r
garnett editors advances in neural information processing systems pages
curran associates inc
jin zhang pan du hongbo xu and xueqi cheng
ictgrasper at temporal preferred
update summarization
in proceedings of the second text analysis conference tac gaithersburg maryland usa november
a background theory on kernels and mmd in this section we provide a brief overview of kernels and maximum mean discrepancy mmd
for a detailed overview we refer readers to to muandet et al
and gretton et al
from which this brief overview is taken
a
positive denite kernels and kernel trick denition a

a function k x x r is called positive denite kernel if it is symmetric i
e
x yx and gram trix is positive denite i
e
nn

cnr i xj
theorem a

if a kernel is positive denite there exists a feature map x h such that x yx
this is known as the kernel trick in machine learning
the feature space h is called a ing kernel hilbert space rkhs and the kernel k is also known as reproducing kernel
a
reproducing kernel hilbert space denition a

an rkhs is a hilbert space of functions where all function evaluations are bounded i
e
xx hh
in an rkhs function evaluation where x h are canonical feature map associated with rkhs h and

a rkhs is fully characterized by its reproducing kernel k or a positive denite nel k uniquely determines a rkhs and vice versa
hence which is known as the riesz representer theorem conway
a
more on mmd recall that f is a class of rkhs functions within the unit ball i
e
h h
suppose h admits a feature map x h
then per gretton et al
we may solve the supremum in equation
as q
a
hence mmd is computed as the distance tween the mean feature embeddings under each distribution for a suitable kernel based feature space gretton et al

eq
a
involves explicitly evaluating the arbitrarily high dimensional features
instead the kernel trick allows efcient computation of q by evaluating just pairwise kernels
supposing h has induced kernel k we have q e e y e xp yq y
a
a
characteristic kernel for a distribution p and kernel with feature map x h the kernel mean map is p
a kernel k is characteristic if the map p is injective
a characteristic kernel ensures mmd is if and only if p q i
e
no information is lost in mapping the distribution into the rkhs muandet et al

examples of characteristic kernels for rd y include the gaussian kernel and laplace kernel y
mmd with the gaussian kernel is equivalent to comparing all moments between two distributions li et al

b proof of lemma
the weighted mmd q
where f contains functions h x r within unit ball rkhs h is dened as sup hf e vp p e sq q recall f is a non negative importance weighting function
then according to patil and rao the weighted probability density p of p is p v v and q and similarly q for q
p the weighted mmd is and q since we restrict s we have
thus sup hf e vp e sq sup e vp e sq since in an rkhs this simplies to h e vp h e sq q e sq p e sq sup e vp e vp where the penultimate step follows from the dual norm
the proof is similar to mmd in gretton et al

c empirical estimate of q first q can be expanded as e vp e h p sq q e p p p q q v e vp sq q e s applying the kernel trick a
e p p p q v e vp sq q e s q our loss of generic summarization t st is t st
recalling nt and mt lt nt nt t v s v nt t s nt mt s mt mt d training details we train generic summarization model with full batch lbfgs liu and nocedal with learning rate

we train comparative rization model using yogi optimizer zaheer et al

wikipedia
org wiki with a mini batch size of topics learning rate
and decreasing the learning rate by half every epochs
we choose the number of training epochs by validating across folds with early stopping
we set the patience to epochs for early stopping with lbfgs optimizer and epochs with yogi optimizer
we tune the other hyperparameters on the training set and the optimal hyperparameters of best model supmmd mkl and searched space are shown in table
the kernel combination weights w
are also shown in table
the kernel combination weights w are written in order unigrams bigrams and entities
hyp
a b r epoch




















w








table optimal hyperparameters their search space and mkl combination weights on each dataset
e additional results in this section we provide some additional results
e
correlation with rouge score dataset supmmd lexrank supmmd lexrank







table correlation of sentence importance scores with normalized sentence rouge scores
we analyze the correlation between normalized rouge recall scores of the sentences and sentence scores from supmmd and lexrank
the ized rouge score of each sentence is dened as
as shown in table we nd that supmmd has a slightly high correlation with sentence rouge scores
this suggests that supmmd is better in capturing sentence importance for summarization
e
feature correlations we analyze the correlation between various surface features and sentence importance scores from supmmd and lexrank erkan and radev
as a b feature supmmd lexrank supmmd lexrank supmmd lexrank position tsf btsf words nouns





























table correlation of some features with sentence scores from supmmd and lexrank eigenvector centrality
method set a set b icsi a fourth day of thrashing thunderstorms began to take a heavier toll on southern california with at least three deaths blamed on the rain as ooding and mudslides forced road closures and emergency crews carried out harrowing rescue operations
downtown los angeles has had more than inches of rain since jan
more than its average rainfall for an entire year including
inches a record
meteorologists say southern fornia has not been hit by this much rain in nearly years
the disaster was the latest caused by rain and snow that has battered california since dec

supmmd downtown los angeles has had more than inches of rain since jan
more than its average rainfall for an entire year including
inches a record
a fourth day of thrashing thunderstorms began to take a heavier toll on southern california with at least three deaths blamed on the rain as ooding and mudslides forced road closures and emergency crews carried out harrowing rescue operations
the roads in los angeles county were equally frustrating
part of a rain saturated hillside gave way sending a mississippi like torrent of earth and trees onto four blocks of this oceanfront town and killing two men
californians braced for even more rain as they gled to recover from storms that have left at least nine people dead triggered mudslides and tornadoes and washed away roads and runways
the record
inches
centimeters was set in
mudslides forced amtrak to suspend train service tween los angeles and santa barbara through at least thursday
a winter storm pummeled southern fornia for the third straight day claiming the lives of three people and raising fears of mudslides even as homes around the region were evacuated
staff writers rick orlov and lisa mascaro contributed to this story
storms have caused
million
million in damage to los angeles county roads and facilities since the beginning of the year
multi million dollar homes collapsed and mudslides trapped residents in their homes as a heavy rains that have claimed three lives pelted los angeles for the fth straight day
in scenes reminiscent of the aftermath of the northridge earthquake years ago this month los angeles area residents faced gridlocked freeways and roads day while cleanup crews cleared mud rubble and bris left from a two week siege of rain
a shattering storm slammed southern california for a sixth straight day tuesday triggering mudslides and tornadoes and forcing more road closures but ers predicted it would wane wednesday before a new storm moves in sunday night
table example summaries of topic containing articles about rains and mudslides in southern california

we highlight few phrases in bold which could help us to identify the difference between set a and b
summaries from icsi and supmmd methods suggest that set a contains articles describing events from earlier days of the disaster and set b contains articles from later stage of the disaster
shown in table supmmd has higher correlation with relative position signifying the importance of position of sentence in summary sentences
lexrank has a higher correlations with the number of words number of nouns and tfisf scores of the sentences which is expected as lexrank is an eigenvector centrality of sentence sentence similarity matrix
this suggest supmmd is able to learn that rst few sentences are important in news summarization
similar result is reported by kedzie et al
where they show that the rst few sentences are important in creating summary of news articles
e
example summary we present the update summaries set a and b of topic which contains articles about rains and mudslides in southern california in table
