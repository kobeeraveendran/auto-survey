hybrid memnet for extractive summarization abhishek kumar singh iiit hyderabad abhishek

iiit
ac
in manish gupta iiit hyderabad manish

ac
in vasudeva varma iiit hyderabad
ac
in c e d l c
s c v
v i x r a abstract extractive text summarization has been an extensive research lem in the field of natural language understanding
while the conventional approaches rely mostly on manually compiled tures to generate the summary few attempts have been made in developing data driven systems for extractive summarization
to this end we present a fully data driven end to end deep network which we call as hybrid memnet for single document tion task
the network learns the continuous unified representation of a document before generating its summary
it jointly captures local and global sentential information along with the notion of summary worthy sentences
experimental results on two different corpora confirm that our model shows significant performance gains compared with the state of the art baselines
ccs concepts information systems summarization information retrieval keywords summarization deep learning natural language acm reference format abhishek kumar singh manish gupta and vasudeva varma

brid memnet for extractive summarization
in proceedings of singapore singapore november pages
doi

introduction the tremendous growth of the data over the web has increased the need to retrieve analyze and understand a large amount of information which often can be time consuming
motivation to make a concise representation of large text while retaining the core meaning of the original text has led to the development of various summarization systems
summarization methods can be broadly classified into two categories extractive and abstractive
tive methods aim to select salient phrases sentences or elements from the text while abstractive techniques focus on generating summaries from scratch without the constraint of reusing phrases from the original text
the author is also a principal applied researcher at microsoft
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page
copyrights for components of this work owned by others than acm must be honored
abstracting with credit is permitted
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission a fee
request permissions from
org
singapore singapore acm




doi

most successful summarization systems use extractive methods
sentence extraction is a crucial step in such systems
the idea is to find a representative subset of sentences which contains the information of the entire set
traditional approaches to extractive summarization identify sentences based on human crafted features such as sentence position and length the words in the title the presence of proper nouns content features like term frequency and event features like action nouns
generally sentences are assigned a saliency score indicating the strength of presence of these features
kupiec et al
use binary classifiers to select summary worthy sentences
conroy and oleary gated the use of hidden markov models while introduced graph based algorithms for selecting salient sentences
recently interest has shifted towards neural network based proaches for modeling the extractive summarization task
kageback et al
employed the recursive autoencoder to summarize documents
yin and pei exploit convolutional neural works to project sentences to a continuous vector space and select sentences based on their prestige and diversity cost for the document extractive summarization task
very recently cheng and lapata introduced attention based neural encoder decoder model for extractive single document summarization task trained on a large corpus of news articles collected from daily mail
lar to cheng and lapta our work is focused on sentential extractive summaries of single document using deep neural works
however we propose the use of memory networks and convolutional bidirectional long short term memory networks for capturing better document representation
in this work we propose a data driven end to end enhanced encoder decoder based deep network that summarizes a news cle by extracting salient sentences
figure shows the architecture of the proposed hybrid memnet model
the model consists of ment reader encoder and a sentence extractor decoder
contrary to cheng and lapata s model where they used an tion based decoder our model uses attention for both encoder and decoder
our focus is to learn a better document representation that incorporates local as well as global document features along with attention to sentences to capture the notion of saliency of a sentence
contrary to the orthodox method of computing tential features our model uses neural networks and is a purely data driven approach
zhang et al
and kim have shown the successful use of convolution neural networks cnn in obtaining latent feature representation
hence our network applies cnn with multiple filters to automatically capture latent semantic features
then a long short term memory lstm work is applied to obtain a comprehensive set of features known as thought vector
this vector captures the overall abstract tation of a document
we obtain the final document representation by concatenating the document embeddings obtained from lutional lstm conv lstm and the document embeddings from november singapore singapore figure the architecture of the hybrid memnet model memory network
the final unified document embedding along with the embeddings of the sentences are used by the decoder to select salient sentences in a document
we experiment with conv lstm encoder as well as convolutional bidirectional lstm conv blstm encoder
we summarize our primary contributions below we propose a novel architecture to learn better unified document representation combining the features from the memory network as well as the features from convolutional lstm blstm network
we investigate the application of memory network porates attention to sentences and conv blstm porates n gram features and sentence level information for learning better thought vector with rich semantics
we experimentally show that the proposed method forms the basic systems and several competitive baselines
our model achieves significant performance gain on the duc generic single document summarization dataset
we begin by describing our network architecture in section lowed by experimental details including corpus details in section
we analyze our system against various benchmarks in section and finally conclude our work in section
hybrid memnet model the primary building blocks of our model are document encoder captures local n grams level mation global sentence level information and the notion of summary worthy sentences decoder attention based sequence to sequence decoder

document encoder the idea is to learn a unified document representation that not only incorporates n gram features and sentence level information but also includes the notion of salience and redundancy of sentences
for this purpose we sum the document representations vectors learned from convolutional lstm conv lstm for hierarchical coding and memnet for capturing salience and redundancy
since the unified document embedding is learned from the joint interaction of the above mentioned two models we refer to this network as hybrid memnet
sentence encoder convolution neural networks are used to encode sentences as they have been shown to successfully work for multiple sentence level classification tasks
conventional convolution neural network uses convolution operation over various word embeddings which is then followed by a max pooling operation
suppose dimensional word embedding of the ith word in the sentence is wi and wi is the concatenation of word embeddings wi



then volution operation over a window of c words using a filter of t rmcd yields new features with m dimensions
here t is the filter index
convolution operation is written as i c here b is the bias term
we obtain a feature map f c by applying filter over all possible window of c words in the sentence of t length n
t wi f c c


c n our intention is to capture the most prominent features in the feature map hence we use max over time pooling operation to acquire set of features for a filter of fixed window size
single feature vector s can be represented as max f c s c t we use multiple convolution nets with different filter sizes to compute a list of embeddings which are summed to obtain the final sentence vector
conv blstm document encoder since recurrent neural network rnn suffers from vanishing gradient problem over long sequences we use long term memory lstm network
to obtain hierarchical document encoding sentence vectors obtained from convolutional sentence encoder are fed to the lstm
this new representation intuitively captures both local as well as global sentential information
we plore lstm network as well as bidirectional lstm network for our experiments
experiments show that combination of convolution network and bidirectional lstm blstm performs better in our case
blstm exploits future context in the sequence as well which is done by processing the data in both directions
the final unified document encoding and sentences vectors from convolutional sentence encoder are fed to the decoder model
in this section we discuss details of the encoder and decoder modules
memnet based document encoder we leverage a memory network encoder inspired from the rent attention model to solve question answering and language convolution max pooling sentence encoder memory encoder decoder document encoder sum d d sentence embedding mi pi ci b sentences words in sentence df hybrid memnet for extractive summarization november singapore singapore modeling task
the model uses an attention mechanism and has been shown to capture temporal context
in our case it learns the document representation which captures the notion of salience and redundancy of sentences
we first describe the model that implements a single memory hop operation single layer then we extend it to multiple hops in memory
consider an input set of sentence vectors


obtained from the sentence encoder for a document d
let d be the document representation of d obtained from conv lstm model and d is the document embedding from the memnet model
the entire set of are transformed into memory vectors mi of dimension d in continuous space using a learned weight matrix a of size d v where v is the embedding size of a sentence
similarly an input document embedding d is transformed via a learned weight matrix b with the same dimension as a to obtain internal state u
we then compute the match between u and each memory mi by taking inner product followed by softmax as follows
pi so mi where so ezj and p is the probability vector over the inputs
each si also has a corresponding output vector ci using another embedding matrix c
the output vector from memory o is computed as the sum over the transformed inputs ci weighted by the probability vector from the input as follows
o pici i in the case of multiple layer model to handle k in our case hop operation the memory layers are stacked and the input to layer k is computed as follows
uk ok let d be the output obtained from the last memory unit ok
final unified document representation df is obtained by summing up the output from the conv blstm d and the output from the memnet d
d d df intuitively df as well as the notion of worthiness of a sentence
captures the hierarchical information of a document
decoder the decoder uses an lstm to label sentences sequentially keeping in mind the individual relevance and mutual redundancy
taking into account both the encoded document and the previously labeled sentences labeling of the next sentence is done
if encoder hidden states are denoted by


hm and decoder hidden states are denoted by


hm at time step t then for t sentence the decoder equations are as follows
ht lst ht mlp where pt is the degree to which the decoder assumes the previous sentence should be a part of summary and is memorized
pt is if system is certain
yl is sentence s label
concatenation of ht and ht is given as input to an mlp multi layer perceptron
ht ht experimental results in this section of the paper we present experimental setup for assessing the performance of the proposed system
we present the details of the corpora used for training evaluation and give implementation details of our approach

datasets for the purpose of training the model we use the daily mail pus which was also used for the task of single document marization by cheng and lapata
overall this corpus tains training documents validation documents and test documents
to evaluate our model we use standard single document summarization dataset which consists of documents
we also evaluate our system on articles from the dailymail test set with human written highlights as the gold standard
the average byte count for each document is and article highlight pairs are sampled such that the highlights include a minimum of sentences

implementation details we use top three high scored sentences subject to the standard word limit of words to generate summaries
the size of the beddings for word sentence and document are set to and respectively
a list of kernel sizes is used for convolutional sentence encoder
two hop operation is performed in the case of memnet encoder
all lstm parameters were domly initialized over a uniform distribution within


we use batch size of documents with learning rate
and the two momentum parameters as
and

we use adam as optimizer

evaluation metrics we evaluate the quality of system summaries using rouge unigram overlap bigram overlap as means of assessing informativeness and rouge l as means of assessing fluency

baseline methods we evaluate our system against several state of the art baselines
we select best systems having state of the art summarization sults on duc corpus for single document summarization task which are ilp tgraph urank se summarunner and deep classifier
ilp is a phrase based extraction model that selects salient phrases and recombines them subject to length and grammar constraints via integer linear programming ilp
tgraph is a graph based tence extraction model
urank uses a unified ranking for as well as multi document summarization
we also use lead as a standard baseline of simply selecting the leading three sentences from the document as the summary
nn se is a neural network based sentence extractor
deep classifier uses gru rnn to tially accept or reject each sentence in the document for being in summary
summarunner is an rnn based extractive summarizer
november singapore singapore table rouge evaluation on the corpus and samples from the daily mail corpus duc rouge l lead ilp tgraph urank nn se deep classifier summarunner hybrid memnet hybrid memnet






























dailymail rouge l lead nn se deep classifier summarunner hybrid memnet hybrid memnet























results and analysis in this section we compare the performance of our system against summarization baselines mentioned in section

table shows our results on the duc test dataset and on the samples from the daily mail corpus
hybrid memnet represents our system with conv lstm encoder and memnet encoder while hybrid memnet uses conv blstm encoder and memnet encoder
it is evident from the results that our system hybrid hybrid memnet outperforms the lead and ilp baselines with a large margin which is an encouraging result as our system does not have access to manually crafted features syntactic information and sophisticated linguistic constraints as in the case of ilp
results also show that our system performs better without the sentence ranking mechanism urank
it also achieves significant performance gain against nn se deep classifier and summarunner
to explore the contribution of the memnet encoder towards the performance of our system we compare results of nn se with hybrid memnet
note that there is significant performance gain of about in the results
post hoc tukey tests showed that the proposed hybrid memnet model is significantly p
better than nn se
this is due to the fact that memnet learns document representation which captures salience estimation of a sentence using the attention mechanism prior to the summary generation
we also notice that replacing lstm with blstm in the encoder improves the performance of the system
this may be because blstm in our setting is able to learn a richer set of semantics as they exploit some notion of future context as well by processing the sequential data in both directions while lstm is only able to make use of the previous context
conclusions in this work we proposed a data driven end to end deep neural network approach for extractive summarization of a document
our system makes use of a combination of memory network and convolutional bidirectional long short term memory network to learn better unified document representation which jointly tures n gram features sentence level information and the notion of the summary worthiness of sentences eventually leading to ter summary generation
experimental results on duc and daily mail datasets confirm that our system outperforms several state of the art baselines
references ronan collobert jianpeng cheng and mirella lapata

neural summarization by extracting sentences and words
in proc
of the annual meeting of the association for computational linguistics
the association for computer linguistics
jason weston leon bottou michael karlen koray kavukcuoglu and pavel kuksa

natural language processing almost from scratch
journal of machine learning research aug
john m conroy and dianne p oleary

text summarization via hidden markov models
in proc
of the annual intl
acm sigir conf
on research and development in information retrieval
acm
gunes erkan and dragomir r radev

lexrank graph based lexical ity as salience in text summarization
journal of artificial intelligence research
elena filatova and vasileios hatzivassiloglou

event based extractive summarization
in proc
of acl workshop on summarization
barcelona spain
mikael kageback olof mogren nina tahmasebi and devdatt dubhashi

extractive summarization using continuous vector space models
in proc
of the workshop on continuous vector space models and their compositionality eacl
citeseer
yoon kim

convolutional neural networks for sentence classification
in emnlp
acl
diederik kingma and jimmy ba

adam a method for stochastic tion
arxiv preprint

julian kupiec jan pedersen and francine chen

a trainable document summarizer
in proc
of the annual intl
acm sigir conf
on research and development in information retrieval
acm
chin yew lin and eduard hovy

automatic evaluation of summaries using n gram co occurrence statistics
in proc
of the conf
of the north american chapter of the association for computational linguistics on human language technology volume
association for computational linguistics
rada mihalcea

language independent extractive summarization
in proc
of the acl on interactive poster and demonstration sessions
acl
ramesh nallapati feifei zhai and bowen zhou

summarunner a recurrent neural network based sequence model for extractive summarization of documents
in proc
of the thirty first aaai conf
on artificial intelligence

ramesh nallapati bowen zhou and mingbo ma

classify or select neural architectures for extractive document summarization
corr

ani nenkova lucy vanderwende and kathleen mckeown

a tional context sensitive multi document summarizer exploring the factors that influence summarization
in proc
of the annual intl
acm sigir conf
on research and development in information retrieval
acm
daraksha parveen hans martin ramsl and michael strube

topical coherence for graph based extractive summarization

hava t siegelmann and eduardo d sontag

on the computational power of neural nets
in proc
of the fifth annual workshop on computational learning theory
acm
richard socher eric h huang jeffrey pennington andrew y ng and pher d manning

dynamic pooling and unfolding recursive autoencoders for paraphrase detection
in nips vol


sainbayar sukhbaatar jason weston rob fergus al

end to end memory networks
in advances in neural information processing systems

xiaojun wan

towards a unified approach to simultaneous document and multi document summarizations
in proc
of the intl
conf
on computational linguistics
acl
kristian woodsend and mirella lapata

automatic generation of story highlights
in proc
of the annual meeting of the association for computational linguistics
association for computational linguistics
wenpeng yin and yulong pei

optimizing sentence modeling and selection for document summarization
in ijcai

xingxing zhang and mirella lapata

chinese poetry generation with recurrent neural networks
in emnlp


