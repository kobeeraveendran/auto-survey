r a l c
s c v
v i x r a reader aware multi document summarization via sparse coding piji li lidong bing wai lam hang li and yi liao department of systems engineering and engineering management the chinese university of hong kong hong kong machine learning department carnegie mellon university pittsburgh pa usa noah s ark lab huawei technologies hong kong wlam
cuhk
edu
hk
cmu
edu hangli

com abstract we propose a new mds paradigm called aware multi document summarization ra mds
specically a set of reader comments associated with the news reports are also collected
the erated summaries from the reports for the event should be salient according to not only the reports but also the reader comments
to tackle this mds problem we propose a sparse coding based method that is able to calculate the salience of the text units by jointly considering news reports and reader comments
another reader aware teristic of our framework is to improve linguistic quality via entity rewriting
the rewriting eration is jointly assessed together with other marization requirements under a unied tion model
to support the generation of sive summaries via optimization we explore a ner in this syntactic unit namely noun verb phrase
work we also generate a data set for conducting ra mds
extensive experiments on this data set and some classical data sets demonstrate the tiveness of our proposed approach
it therefore introduction in the typical multi document summarization mds ting the input is a set of documents reports about the same topic event
the reports on the same event normally cover many aspects and the continuous follow up reports bring in more information of it
is very lenging to generate a short and salient summary for an event
mds has drawn some attention and some method have been proposed
for example wan et al
posed an extraction based approach that employs a fold ranking method to calculate the salience of each tence
filatova and hatzivassiloglou modeled the mds task as an instance of the maximum coverage set lem
gillick and favre developed an exact solution the work described in this paper is substantially supported by grants from the research and development grant of huawei nologies co
ltd and the research grant council of the hong kong special administrative region china project codes and
for a model similar to filatova and hatzivassiloglou based on the weighted sum of the concepts approximated by bigrams
li et al
proposed a guided sentence pression framework to generate compressive summaries by training a conditional random eld crf based on a notated corpus
li et al
considered linguistic ity in their framework
ng et al
exploited timelines to enhance mds
moreover many works liu et al
kageback et al
denil et al
cao et al
utilized deep learning techniques to tackle summarization tasks
as more and more user generated content is available one natural extension of the setting is to incorporate such content regarding the event so as to directly or indirectly improve the generated summaries with greater user satisfaction
in this paper we investigate a new setting in this direction
ically a set of reader comments associated with the news ports are also collected
the generated summaries from the reports for the event should be salient according to not only the reports but also the reader comments
we name such a paradigm of extension as reader aware multi document marization ra mds
we give a real example taken from a data set collected by us to illustrate the importance of ra mds
one hot event in is malaysia airlines jet disappeared
after the outbreak of this event lots of reports are posted on ferent news media
most existing summarization systems can only create summaries with general information e

flight carrying passengers and crew members ished early saturday after departing kuala lumpur for jing due to the fact that they extract information solely from the report content
however after analyzing the reader ments we nd that many readers are interested in more cic aspects such as military radar indicated that the plane may have turned from its ight route before losing contact and two passengers who appear to have used stolen pean passports to board
under the ra mds setting one should jointly consider news and comments when generating the summary so that the summary content can cover not only important aspects of the event but also aspects that attract reader interests as reected in the reader comments
no previous work has investigated how to incorporate the comments in mds problem
one challenge is how to duct salience calculation by jointly considering the focus of news reports and the reader interests revealed by comments
meanwhile the model should not be sensitive to the ability of diverse aspects of reader comments
another lenge is that reader comments are very noisy grammatically and informatively
some previous works explore the effect of comments or social contexts in single document marization such as blog summarization et al
yang et al

however the problem setting of mds is more challenging because the considered comments are about an event with multiple reports spanning a time riod resulting in diverse and noisy comments
to tackle the above challenges we propose a coding based method that is able to calculate the salience of the text units by jointly considering news reports and reader comments
intuitively the nature of summarization is to lect a small number of semantic units to reconstruct the inal semantic space of the whole topic
in our ra mds setting the semantic space incorporates both the news and reader comments
the selected semantic units are sparse and hold the semantic diversity property
then one issue is how to nd these sparse and diverse semantic units ciently without supervised training data
sparse coding is a suitable method for learning sets of over complete bases to represent data efciently and it has been demonstrated to be very useful in computer vision mairal et al

over sparse coding can jointly consider news and comments to select semantic units in a very simple and elegant way by just adding a comments reconstruction error item into the original loss function
currently there are only a few works employing sparse coding for the summarization task
dsdr he et al
represents each sentence as a non negative linear combination of summary sentences
but this method does not consider the sparsity
mds sparse liu et al
proposed a two level sparse representation model ing coverage sparsity and diversity
but their results do not show a signicant improvement
in this paper we propose a more efcient and direct sparse model to tackle these lems and achieve encouraging results on different data sets
another reader aware characteristic of our framework is to improve linguistic quality via entity rewriting
maries may contain phrases that are not understandable out of context since the sentences compiled from different uments might contain too little too much or repeated mation about the referent
a human summary writer only uses the full form mention e

president barack obama of an entity one time and uses the short form mention e

obama in the other places
analogously for a lar entity our framework requires that the full form tion of the entity should only appear one time in the mary and its other appearances should use the most cise form
some early works perform rewriting along with the greedy selection of individual sentence nenkova
some other works perform summary rewriting as a processing step siddharthan et al

in contrast with such works the rewriting consideration in our framework is jointly assessed together with other summarization ments under a unied optimization model
this brings in two advantages
first the assessment of rewriting tion is jointly considered with the generation of the news sentences news reconstruction comment sentences nps vps uo summary generation and compression comments partial reconstruction figure our ra mds framework
sive summary so that it has a global view to generate better rewriting results
second we can make full use of the length limit because the effect of rewriting operation on summary length is simultaneously considered with other constraints in the model
to support the generation of compressive maries via optimization we explore a ner syntactic unit namely noun verb phrase
precisely we rst decompose the sentences into noun verb phrases and the salience of each phrase is calculated by jointly considering its importance in reports and comments
in this work we also generate a data set for conducting ra mds
extensive experiments on our data set and some benchmark data sets have been conducted to examine the cacy of our framework
description of the proposed framework
overview to tackle the ra mds problem we propose an vised compressive summarization framework
the overview of our framework is depicted in fig

a sparse coding based method is proposed to reconstruct the semantic space of a topic revealed by both the news sentences i
e
xi s and the comment sentences i
e
zi s on the news sentences
thus an expressiveness score ai is designed for each news sentence
the dashed boxes of comment sentences indicate that a cial treatment is applied on comments to avoid noise in the reconstruction
the details will be introduced in section

the compression is carried out by deleting the unimportant constituents i
e
phrases of the input sentence
we rst decompose each sentence into noun phrases nps and verb phrases vps
the salience of a phrase depends on two ria namely the expressiveness score inherited from the tence and the concept score of the phrase
the extraction of phrases and the calculation of phrase salience will be troduced in section

our framework carries out mention rewriting for entities to improve the linguistic quality of our summary
specically we rewrite the mentions of three types of named entities namely person location and organization
we will discuss the details of mention detection mention cluster merging short form and full form mention nding in section

after the above preparation steps we will troduce our summarization model in section

our model simultaneously performs sentence compression and mention rewriting via a unied optimization method
meanwhile a variety of summarization requirements are considered via mulating them as the constraints

reader aware sentence expressiveness intuitively the nature of summarization is to select semantic units which can be used to reconstruct the original semantic space of the topic
the expressiveness score of a sentence in the news is dened as its contribution in constructing the semantic space of the topic from both the news content and the reader comments
therefore the expressiveness conveys the attention that a sentence attracts from both the news ers and the readers
we propose a sparse coding model to compute such expressiveness scores
in typical sparse coding the aim is to nd a set of basis vectors i which can be used to reconstruct m target input vectors xi as a linear combination of them so as to minimize the following loss function m min a kxi aijj where s
is a sparsity cost function which penalizes ai for being far from zero
in our summarization task each topic contains a set of news reports and a set of reader comments
after ming and stop word removal we build a dictionary for the topic by using unigrams and bigrams from the news
then each sentence of news and comments is represented as a weighted term frequency vector
let x


xm and z


denote the vectors of sentences from news and comments respectively where rd and zi rd are term frequency vectors
there are d terms in dictionary m sentences in news and n sentences in ments for each topic
we take semantic units as sentences here and assume that for each sentence xi there is a cient variable ai named expressiveness score to represent the contribution of this sentence in the reconstruction
based on the spirit of sparse coding we directly regard each news sentence xi as a candidate basis vector and all xi s are employed to reconstruct the semantic space of the topic including x and z
thus we propose a preliminary error formulation as expressed in eq
for which we aim at minimizing m m m n m kxi kzi where the coefcient aj s are the expressiveness scores and all the target vectors share the same coefcient vector a here
to harness the characteristics of the summarization lem setting more effectively we rene the preliminary ror formulation as given in eq
along three directions
as mentioned before the original sentence vector space can be constructed by a subset of them i
e
the number of summary sentences is sparse so we put a sparsity straint on the coefcient vector a using norm in eq
with the weight as a scaling constant to determine its relative importance
moreover we just consider negative linear reconstruction in our framework so we add non negative constraints on the coefcients
as previous work ng et al
mentioned some prior knowledge can benet the sentence expressiveness detection performance e

sentence position
so we add a variable i to weight each news sentence reconstruction error
here we employ the position information to generate cp if cp otherwise where p is the paragraph id in each document starting from and c is a positive constant which smaller than
sides those useful information comments usually introduce lots of noise data
to tackle this problem our rst step is to eliminate terms only appear in comments another step is to add a parameter i to control the comment sentence struction error
due to the fact that the semantic units of erated summaries are all from news intuitively a sentence will introduce more information if it is more similar with news
therefore we employ the mean cosine similarity between comment sentence zi with all the news sentences x as the weight variable i
after the above considerations we have the global loss function as follows ikxi ajxj k j min a m m x n x m x m x ikzi ajxjk s
t
aj for j


m for the optimization problem of sparse coding there are ready many classical algorithms mairal et al

in this paper we utilize coordinate descent method as shown in gorithm
under the iterative updating rule as in eq
the objective function j is non increasing and the convergence of the iteration is guaranteed
our sparse coding model introduces several advantages
first sparse coding is a class of unsupervised methods so no manual annotations for training data are needed
second the optimization procedure is modular leading to easily plug in different loss functions
third our model incorporates mantic diversity naturally as mentioned in he et al

last but not the least it helps the subsequent unied timization component which generates compressive maries
in particular it reduces the number of variables cause the sparsity constraint can generate sparse ness scores i
e
most of the sentences get a score

phrase extraction and salience calculation we employ stanford parser klein and manning to tain a constituency tree for each input sentence
after that we extract nps and vps from the tree as follows the nps and vps that are the direct children of the s node are tracted
vps nps in a path on which all the nodes are all vps nps are also recursively extracted and regarded as ing the same parent node s
recursive operation in the second step will only be carried out in two levels since the phrases in the lower levels may not be able to convey a complete fact
algorithm coordinate descent algorithm for sentence pressiveness detection input news sentences x rdm comments sentences z rdn news reconstruction weight i comments reconstruction weight i penalty parameter and ping criterion t and
root s vp vp np dt jj nn vp cc vp an armed man vbd pp vbd np advp and vp cc vp walked in np sent dt nns rb vbn prt and vpn np into dt nnp nn the boys outside tied rp shot dt nns an amish school up the girls vbg np

s vp killing np pp cd in np three of prp them take partial derivatives for reconstruction error items figure the constituency tree of a sentence
output salience vector a rm
initialize a t while t t and j t do reconstructing x at j m m n m n j ak x txk x txk select the coordinate with maximum partial derivative k arg max


m by update donoho and johnstone coordinate the j ak soft thresholding k j ak where s ai
j t jat t t end while return a a
take the tree in fig
as an example the corresponding tence is decomposed into phrases an armed man walked into an amish school sent the boys outside and tied up and shot the girls killing three of them walked into an amish school sent the boys outside and tied up and shot the girls killing three of them
the salience of a phrase depends on two criteria
the rst criterion is the expressiveness score which is inherited from the corresponding sentence in the output of our sparse ing model
the second criterion is the concept score that conveys the overall importance of the individual concepts in the phrase
let tf t be the frequency of the term t igram bigram in the whole topic
the salience si of the because of the recursive operation the extracted phrases may have overlaps
later we will show how to avoid such overlapping in phrase extraction
we only consider the recursive operation for a vp with more than one parallel sub vps such as the highest vp in fig

the sub vps following modal link or auxiliary verbs are not extracted as individual vps
in addition we also extract the clauses functioning as subjects of sentences as nps such as that clause
note that we also mention such clauses as noun phrase although their labels in the tree could be sbar or s
phrase pi is dened as tf t ai tf t si ptpi opic where ai is the expressiveness of the sentence containing pi
resolution
preparation of entity mentions for rewriting we rst conduct co reference resolution for each ument using stanford co reference age lee et al

we adopt those resolution rules that are able to achieve high quality and address our need for summarization
in particular sieve and in the package are employed
a set of clusters are obtained and each cluster contains the mentions corresponding to the same entity in a document
the clusters from different documents in the same topic are merged by matching the named entities
three types of entities are considered namely person location and organization
let m denote the mention cluster of an entity
the form mention is determined as arg max tf mm xtm where tf is calculated in m
we do not simply select the longest one since it could be too verbose
the short form mention ms is determined as ms arg max mm xtm tf where m contains the mentions that are the shortest and meanwhile are not pronouns

unied optimization framework the objective function of our optimization formulation is ned as i isi x i j ij si sj rij where i is the selection indicator for the phrase pi si is the salience scores of pi ij and rij is co occurrence indicator and the similarity a pair of phrases pi pj respectively
the similarity is calculated with the jaccard index based method
specically this objective maximizes the salience score of the selected phrases as indicated by the rst term and nalizes the selection of similar phrase pairs
the constraints that govern the selected phrases are able to form compressive sentences and the constraints for entity rewriting are given below
note that the rewriting consideration is conducted for different candidates for the purpose of the assessment of the effects on summarization in the optimization framework
consequently no actual permanent rewriting operations are conducted during the optimization process
the actual ing operations will be carried out on the selected phrases put from the optimization component in the post processing stage
compressive sentence generation
let k note the selection indicator of sentence xk
if any phrase from xk is selected k
otherwise k
for generating a compressed summary sentence it is required that if at least one np and at lease one vp of the sentence should be selected
it is expressed as pi xk pi is an n p i k i k xi xi pi xk pi is a v p i k i
entity rewriting
let pm be the phrases that contain the entity corresponding to the cluster m
for each pi pm two indicators f indicates that the entity in pi is rewritten by the full form while s i indicates that the entity in pi is rewritten by the short form
to adopt our rewriting strategy we design the following constraints i are dened
f i and s i if pi pm i f j xpj pm
i s if pi pm i f note that if a phrase contains several mentions of the same entity we can safely rewrite the latter appearances with the short form mention and we only need to decide the rewriting strategy for the rst appearance
not i within i
two phrases in the same path of the constituency tree can not be selected at the same time if pk pj then k j for example walked into an amish school sent the boys outside and tied up and shot the girls killing three of them and walked into an amish school can not be both selected
phrase co occurrence
these constraints control the co occurrence relation of two phrases ij i ij j i j ij the rst two constraints state that if the summary includes both the units pi and pj then we have to include them vidually
the third constraint is the inverse of the rst two
short sentence avoidance
we do not select the vps from the sentences shorter than a threshold because a short sentence normally can not convey a complete key fact pronoun avoidance
observed woodsend and lapata normally not used by human summary writers
we exclude the nps that are pronouns from being selected
previously are as pronouns length constraint
the overall length of the selected nps and vps is no larger than a limit l
note that the length calculation considers the effect of rewriting operations via the rewriting indicators
the objective function and constraints are linear so that the optimization can be solved by existing integer linear programming ilp solvers such as simplex rithm dantzig and thapa
in the implementation we use a package called lp

postprocessing the timestamp of a summary sentence is dened as the tamp of the corresponding document
the sentences are dered based on their pseudo timestamps
the sentences from the same document are ordered according to their original der
finally we conduct the appropriate entity rewriting as indicated from the optimization output
experiments
experimental setting our data set
our data set contains topics
each topic contains related news reports and at least reader ments
for each topic we employ summary writers with nalist background to write model summaries
when writing summaries they take into account the interest of readers by digesting the reader comments of the event
model maries are written for each topic
we also have a separate development tuning set containing topics and each topic has one model summary
duc
in order to show that our sparse coding based work can also work well on traditional mds task we employ the benchmark data sets duc and duc for uation
duc and duc contain and topics respectively
each topic has news documents and model summaries
the length of the model summary is limited by words
evaluation metric
we use rouge score as our ation metric lin and the f measures of and rouge are reported
parameter settings
we set c
and p in the position weight function
for the sparse coding model we set the stopping criteria t and the learning rate
for the sparsity item penalty we set


results on our data set we compare our system with three summarization baselines
random baseline selects sentences randomly for each topic
lead baseline wasson ranks the news cally and extracts the leading sentences one by one
mead radev et al
generates summaries using cluster troids produced by a topic detection and tracking system
as shown in table our system reports the best results on all of rouge metrics
the reasons are as follows our sparse coding model directly assigns coefcient values
sourceforge


berouge
com
summarization
com system random lead mead ours rouge











table results on our data set
as expressiveness scores to the news sentences which are tained by minimizing the global semantic space tion error and are able to precisely represent the importance of sentences
the model can jointly consider news tent and reader comments taking into account of more aware information
in our sparse coding model we weight the reconstruction error by a prior knowledge i
e
paragraph position which can improve the summarization performance signicantly
our unied optimization framework can ther lter the unimportant nps and vps and generate the pressed summaries
we conduct entity rewriting in the unied optimization framework in order to improve the guistic quality

results on duc in order to illustrate the performance of our framework on traditional mds task we compare it with several state the art systems on standard data set duc
our framework can still be used for mds task without reader comments by ignoring those components for comments
besides random and lead methods we compare our system with two other unsupervised sparse coding based methods namely dsdr he et al
and mds sparse liu et al
mds and mds sparse div
because both data set and evaluation metrics are standard we directly retrieve the results in their papers
the results are given in tables and
our system can signicantly form the comparison methods for the reasons mentioned in section

rouge system random lead dsdr non mds mds sparse div ours system random lead dsdr non mds mds sparse div ours























table results on duc
table results on duc










rouge
case study based on the news and comments of the topic bitcoin change mt
gox goes ofine we generate two summaries with our model considering comments ours and ing comments ours noc respectively
the summaries and rouge evaluation are given in table
all the rouge values of our model considering comments are better than those ignoring comments with large gaps
the sentences in italic bold of the two summaries are different
by ing the comments of this topic we nd that many comments are talking about the company had lost bitcoins


and anonymity prevents reversal of transactions
which are well identied by our model



rouge system ours noc mt
gox went ofine today as trading on the tokyo based site came to a screeching halt
a withdrawal ban imposed at the exchange earlier this month
deposits are insured by the ernment
the sudden closure of the mt
gox bitcoin exchange sent the virtual currency to a three month low on monday the currency s value has fallen to about from in the past few hours
the statement from the bitcoin companies on day night which was not signed by mr
silbert are committed to the future of bitcoin and the security of all customer funds
ours mt
gox went ofine today as trading on the tokyo based site came to a screeching halt
the company had lost coins in a theft that had gone unnoticed for years
the sudden closure of the mt
gox bitcoin exchange sent the virtual rency to a three month low on monday
the currency s value has fallen to about from in the past few hours
anonymity prevents reversal of transactions
the statement from the coin companies on monday night which was not signed by mr
silbert are committed to the future of bitcoin and the security of all customer funds



table generated summaries for the topic bitcoin change mt
gox goes ofine
we also present an entity rewriting case study
for son name dong nguyen in the topic flappy bird the summary without entity rewriting contains different tion forms such as dong nguyen dong and nguyen
after rewriting dong is replaced by nguyen which makes the co reference mentions clearer
as expected there is only one full form mention such as nguyen ha dong a hanoi based game developer shuhei yoshida president of sony computer entertainment worldwide studios and the australian maritime safety authority s rescue nation centre which is overseeing the rescue in each mary
conclusion we propose a new mds paradigm called reader aware document summarization ra mds
to tackle this mds problem we propose a sparse coding based method jointly considering news reports and reader comments
we propose a compression based unied optimization work which explores a ner syntactic unit namely noun verb phrase to generate compressive summaries and meanwhile it conducts entity rewriting aiming at better linguistic quality
in this work we also generate a data set for ra mds task
the experimental results show that our framework can achieve good performance and outperform state of the art vised systems
references cao et al
ziqiang cao furu wei li dong sujian li and ming zhou
ranking with recursive neural works and its application to multi document tion
in aaai
dantzig and thapa george mukund n
thapa
duction
springer verlag new york inc

dantzig b
linear programming and denil et al
misha denil alban demiraj nal brenner phil blunsom and nando de freitas
elling visualising and summarising documents with a arxiv preprint single convolutional neural network


donoho and johnstone david l donoho and jain m johnstone
ideal spatial adaptation by wavelet shrinkage
biometrika
filatova and hatzivassiloglou elena filatova and vasileios hatzivassiloglou
a formal model for mation selection in multi sentence text extraction
in coling
gillick and favre dan gillick and benoit favre
a scalable global model for summarization
in workshop on ilp for nlp pages
he et al
zhanying he chun chen jiajun bu can wang lijun zhang deng cai and xiaofei he
document summarization based on data reconstruction
in aaai
et al
meishan hu aixin sun and ee peng lim
comments oriented document summarization standing documents with readers feedback
in sigir pages
kageback et al
mikael kageback olof mogren nina tahmasebi and devdatt dubhashi
extractive marization using continuous vector space models
in pages
klein and manning dan klein and christopher d
manning
accurate unlexicalized parsing
in acl pages
lee et al
heeyoung lee angel chang yves man nathanael chambers mihai surdeanu and dan rafsky
deterministic coreference resolution based on entity centric precision ranked rules
comput
linguist

et al
chen li fei liu fuliang weng and yang liu
document summarization via guided sentence pression
in emnlp pages
et al
chen li yang liu fei liu lin zhao and fuliang weng
improving multi documents tion by sentence compression based on expanded stituent parse trees
in emnlp pages
lin chin yew lin
rouge a package for automatic evaluation of summaries
in text summarization branches out proceedings of the workshop pages
liu et al
yan liu sheng hua zhong and wenjie li
query oriented multi document summarization via pervised deep learning
in aaai
liu et al
he liu hongliang yu and zhi hong deng
multi document summarization based on two level sparse representation model
in aaai
et al
julien mairal francis bach and jean ponce
sparse modeling for image and vision processing
foundations and trends in computer graphics and sion
nenkova ani nenkova
entity driven rewrite for multi document summarization
in third international joint conference on natural language processing nlp pages
et al
jun ping ng praveen bysani ziheng lin min yen kan and chew lim tan
swing exploiting category specic information for guided summarization
in proceedings of tac
et al
jun ping ng yan chen min yen kan and zhoujun li
exploiting timelines to enhance document summarization
in acl pages
radev et al
dragomir radev timothy allison sasha blair goldensohn john blitzer arda celebi stanko dimitrov elliott drabek ali hakim wai lam danyu liu al
mead a platform for multidocument tilingual text summarization

siddharthan et al
advaith ani information nenkova and kathleen mckeown
tus distinctions and referring expressions an empirical study of references to people in news summaries
comput
linguist

siddharthan wan et al
xiaojun wan jianwu yang and guo xiao
manifold ranking based topic focused document summarization
in ijcai pages
wasson mark wasson
using leading text for news summaries evaluation results and implications for mercial summarization applications
in acl pages
association for computational linguistics
woodsend and lapata kristian woodsend and mirella lapata
multiple aspect summarization using integer linear programming
in emnlp conll pages
yang et al
zi yang keke cai jie tang li zhang zhong su and juanzi li
social context summarization
in sigir pages

