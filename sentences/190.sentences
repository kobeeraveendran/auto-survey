automatic text summarization what has been done and what has to be done abdelkrime aries djamel eddine zegour walid khaled hidouci ecole nationale superieure dinformatique esi
ini algiers algeria emails aries zegour w
dz abstract summaries are important when it comes to process huge amounts of information
their most important benet is saving time which we do not have much nowadays
therefore a summary must be short representative and readable
generating summaries automatically can be benecial for humans since it can save time and help selecting relevant documents
automatic summarization and in particular automatic text summarization ats is not a new research eld it was known since the
since then researchers have been active to nd the perfect summarization method
in this article we will discuss dierent works in automatic summarization especially the recent ones
we will present some problems and limits which prevent works to move forward
most of these challenges are much more related to the nature of processed languages
these challenges are interesting for academics and developers as a path to follow in this eld
keywords text summarization summary evaluation summarization methods introduction a summary as dened in is a brief statement that presents the main points in a concise form
one of its benets is identifying the content of a given document which will help readers peaking the right documents to their interests
according to borko and bernier a summary has many benets such as saving time easing selection and search improving indexing eciency
summaries of the same document can be dierent from person to person
this can occur due to dierent points of interest or due to each individual s understanding
automatic text summarization is not a new eld of research
since the work of luhn many works have been conducted to improve this eld of research
even if it was an old eld of research ats still gains a lot of attention from research communities
this eld is motivated by the growth of electronic data turning the selection of relevant information into a more dicult task
information can be found in many sources such as electronic newspapers with many variations
the redundant information makes their selection and processing very dicult
our goal is to aord a concise report about dierent research works in ats
this can help academics and professionals having an overview on what has been already done in this wide eld
also we indicate some challenges to lead future research
most of them are considered as related topics to this eld since they are associated with natural language processing nlp domain
though these challenges are well known they are still holding this eld from going forward
the remainder of this article is organized as follows
section covers the dierent methods in ats classied using criteria of input document purpose and output document
section is an enumeration of some major approaches for ats
section presents the evaluation methods workshops and campaigns
these three sections are intended to present what has been done in ats
section addresses some challenges a researcher may nd in this eld
this late section is intended to present what has to be done in order to advance this eld forward
finally section sums up what has been discussed in all of these sections

thefreedictionary
com outline february r a l c
s c v
v i x r a summarization methods dierent summarization methods have been proposed since the rst work of luhn
each method is intended to solve a problem in a certain context
a summarization system can be classied using many criteria it can belong to many classes at once
these criteria are regrouped into three categories input document criteria purpose criteria and output document criteria hovy and lin sparck jones
based on the criteria aorded by hovy and lin and some other changes to t current systems the dierent classications can be illustrated as in figure
figure classication of summarization systems using dierent criteria

input document examining the input document we can classify a summarization system using three criteria source size specicity and form
the source size refers to how many documents a system can have as input
the specicity refers to the domains which this system can handle is it designed just for a specic domain or is it a general purpose system the form of the input species if they are structured or not have low scale tweets or large one novel are textual or multimedia documents and if they are of the same genre or not


source size a summarization system can have one input document or many
mono document summarizers process just one input document even if the summarizer used some other resources for learning
this kind of summarization includes the rst works of ats luhn baxendale edmundson
in the other hand multi document summarizers take as input many documents of the same topic for example news about a car crush from many newspapers
the earliest work we can nd in document summarization is the work of mckeown and radev
the benet of this type of marization is to diversify information sources
but the problem is the redundancy documents talking about the same topic have much information in common
this classication mono vs
document summarization is very popular among researchers in the eld of ats


specicity an automatic summary can be generated from domain specic documents or from general domains
when we want to summarize documents of the same domain it is more appropriate to use a summarization system specic to this domain
this can reduce terms ambiguities the usage of grammar and formating schemes
some works use domain specic concepts derived from some resources to enhance relevancy detection
in biomedical text summarization the work of reeve et al
uses two methods one to identify salient sentences and the other to remove redundant information
the two methods are based summarizationoutput documentinput documentpurposesource sizesingle documentmulti documentspecificitydomain specificgeneralformaudiencegenericquery orientedusageexpansivenessindicativeinformativederivationconventionalitybackgroundupdateextractabstractpartialityneutralevaluativefixedfloatingstructure scale medium genrevsvsvsvsvsvsvsvs on concepts extracted from biomedical resources and are both used to generate the nal summary
in medical text summarization sarkar uses domain specic cue phrases combined with other features to decide sentence relevancy
letsumm system farzindar and lapalme is also an example of domain specic summarization systems designed for juridic texts


form input documents can have many forms based on their structures scales mediums and genres
the structure refers to the explicit organization found in the document eg
sections in papers introduction related works method experiments and conclusion
the structure can be used to increase summarization performance where every section can be treated dierently
for instance letsumm system farzindar and lapalme uses thematic structures in legal judgments decision data introduction context juridical analysis and conclusion to generate summaries
for each thematic some linguistic markers are aected to be used as cue phrases paice
the resulted summary is the combination of those thematics summaries with a percentage of contribution
pembe and gungor use html documents as input document
they score the sentences extract the rst ones and reorder them using their initial sections the sections are detected using html tags
the scale is another aspect which can aect the summarizing method an input document can be a paragraph an article a book
many known summarization systems use term frequency as relevancy measure therefore the input document must be large important terms are repeated a lot
in cases of summarizing low scale documents such as tweets and micro blogs we ca nt use usual techniques
the work of shari et al
revolves around the summarization of tweets
their algorithm takes as input a trending phrase or a phrase specied by user
this phrase is used to collect a large number of tweets which are served to generate the summary
in this case the summary is the most commonly used phrase covering the topic phrase
in duan et al
given a hashtag as a topic we can select a set of tweets
the medium is the support which carries the information
most summarization systems are based on textual support while there are some researches on summarizing images videos and audios
the work of kim et al
addresses the problem of jointly summarizing large scale flickr images and youtube user videos
the summarization of videos uses similarity with images to delete redundant information
in the other hand summarizing photos consists of creating a photo storyline with the interactive assistance of videos
lately researchers show some interest in multimedia summarization football summarization moon summarizing important events in a football video zawbaa et al
video summarization using web images khosla et al
video summarization using a given category potapov et al
video summarization by learning gygli et al

summarization systems can be classied using the genre of their input documents news interviews reviews novels
there are some research works which adjust their scoring methods according to the document s genre
in goldstein et al
the authors propose a machine learning based method which detects the genre of the input document and estimates the adequate features
a similar work can be found in yatsko et al
where the summary is generated based on the genre
we can nd some systems designed for a specic genre such as reviews zhan et al


purpose

audience sometimes the user needs a summary which focuses on some aspects rather than the main idea of the input document
a summarization system which takes in consideration the user s preferences is called query oriented while the one which does not is known as generic system
a generic summarization system tries to preserve important information out of an input document
so we can say it tries to focus on the topic presented by the author and present what the input document is all about
this type of systems exists widely as an example the systems in vanetik and litvak aries et al
thomas et al
vicente et al

query oriented systems take in consideration the users preferences
for instance if a user wants a summary focusing on someone in a story it must contain events around this person without loosing the main interest of the story
bhaskar and bandyopadhyay describe a method to summarize multiple documents based on user queries
the correlation measure between sentences from each document are calculated to create clusters of similar sentences
then the sentences are scored according to the query
this score is accumulated to the cluster score to extract the highest scored sentences
texts are not the only thing to summarize concepts can be summarized too
in cebiric et al
the authors propose a method for query oriented summarization using graphs
in their method they try to summarize an rdf graph to get another one which focuses on a given query
recent techniques such as deep learning have been used to generate query driven summaries hua zhong et al



usage the usage of a summary searches whether it is meant to help users decide their interests or as a tative replacement for the original
informative summaries contain the essential information of the original after reading them you can tell what are the main ideas
this type of in a research article the author must maries can be found in journals thesis research articles
present the essential ideas and ndings using an abstract which is considered as an informative summary
the majority of systems and methods focus on this type of summarization hence we can nd examples of this type anywhere
indicative summaries does not contain informative content they only contain a global description of the original document
that includes the purpose scope and research methodology
this can be helpful to decide whether or not to consult the original document
the abstracts on the title page s verso of books and reports are an example of indicative summaries
works have been conducted to investigate such type of summaries
kan et al
propose an indicative multi document summarization system centrifuser based on the problem of content planning
figure represents an example of a centrifuser summary on the health care topic of angina where the generated indicative summary is in the bottom half using the dierence in topic distribution of documents to regroup them
the method figure an example of centrifuser summarizer output kan et al

uses an ir system to search in a collection of documents which gives a selected set of documents
then some features are extracted from the collection and from the individual chosen documents to be used along with the query to generate the indicative summary


expansiveness a generated summary can focus on the background of the original document or aords the news compared to some past documents
this property is referred to as expansiveness
a background summary according to mani assumes that the reader has poor prior knowledge of general setting of the input and hence includes explanatory material such as circumstances of place time and actors
in the other hand just the news summary is containing just novel or principal themes assuming that the reader knows enough background to interpret them in context
nowadays a lot of summarizers tend to aord the most relevant information found in input texts without verifying if the resulted summary incorporates some explanatory materials
following the previous two denitions these summarizers are neither background nor just the news
that is a summarizer of this kind can generate both background summaries and the news summaries
so classifying systems based on whether they are designed to incorporate news is more appropriate
resource description framework proceedings of european workshop on natural language generation toulouse france july
pp
yenkanandkathleenr
mckeowndepartmentofcomputersciencecolumbiauniversitynewyork kathy
columbia

klavanscolumbiauniversitycenterforresearchoninformationaccessnewyork
columbia
eduabstractthetaskofcreatingindicativesum mariesthathelpasearcherdecidewhethertoreadaparticulardocumentisadifculttask
thispaperexam inestheindicativesummarizationtaskfromagenerationperspective byrstanalyzingitsrequiredcontentviapub lishedguidelinesandcorpusanalysis
weshowhowthesesummariescanbefactoredintoasetofdocumentfeatures andhowanimplementedcontentplan nerusesthetopicalitydocumentfea turetocreateindicativemultidocumentquery basedsummaries
whichcharacterizeswhatthedocumentsareabout
thisisincontrasttotheinformativesummary whichservesasasurrogateforthedocument
indicativemultidocumentsummariesareanimportantwayofhelpingauserdiscriminatebetweenseveraldocumentsreturnedbyasearchengine
traditionalsummarizationsystemsareprimar ilybasedontextextractiontechniques
foranin dicativesummary whichtypicallydescribesthetopicsandstructuralfeaturesofthesummarizeddocuments theseapproachescanproducesum mariesthataretoospecic
inthispaper wepro umentsummaries
ourmodelisbasedontheval uesofhigh leveldocumentfeatures suchasitsdistributionoftopicsandmediatypes
highlighted differences between the documents the topics include definition and what are the information on additional topics which are american medical assocation family medical physicians and surgeons complete home medical guide
this file minute emergency medicine consult is close in content to the extract
included in the extract is available in these filesthe merck manual of medical information containsextensive information on the topic
guide and the columbia university college ofwe found documents on angina summary of the disease anginaget information on variant angina treatment diag


navigational aidstreatment is designed to prevent or reduce ischemia andextract minimize symptoms
angina that can not be controlled by drugs


extracted summarygenerated acentrifusersummaryonthehealthcaretopicofangina
thegeneratedin dicativesummaryinthebottomhalfcategorizesdocumentsbytheirdifferenceintopicdistribu tion
specically wefocusontheproblemofcon tentplanninginindicativemultidocumentsum marygeneration
startingfromasingledocumentcon textandgeneralizingtoamultidocument query basedcontext
thisyieldstworules of
wehaveimplementedtheserulesaspartofthecontentplanningmoduleofourcentrifusersummarizationsystem
thesummarizersarchi culationandcontentplanning
wefollowthegenerationofasampleindicativemultidocumentquery basedsummary showninthebottomhalf lets redene what just the news summaries and call them update summaries since this term is more used
recently the emerging interest in systems that automatically monitor streams of social media posts such as twitter or news to keep users up to date on topics they care about led to the appearance of update summaries
the idea is to generate summaries from some recent documents which do not contain information from previous documents
it means the system must have prior knowledge of what have been seen before
update summaries were promoted in duc s update task where the participants must generate multi document update summaries from some chronologically ordered sets of news documents see section

for more detail
more recent evaluation campaigns for update summaries are trec s temporal summarization and real time summarization tasks
we can then dene a background summarization system as a system which generates summaries based on the input content and without excluding information from prior documents on the same subject

output document three criteria of the summary can be used to classify a summarization system derivation partiality and format
the derivation is the way used to produce a summary from the original document either by extracting relevant units or by creating a new text
partiality is how a summary handles the opinions found in the original document which can be either neutral or evaluative
as for format the summary format can be x or oating


derivation it refers to the way used to obtain a summary
it can be by extracting pertinent units or by understanding and generating a new summary
extractive summaries are produced by as the name says extracting units from the original documents
usually these units are sentences because it is easy to keep the correctness of the grammatical structure
the rst researches in the eld of ats are extractive they use some features to estimate the pertinence of a sentence
some of these features are term frequency luhn position in the text baxendale edmundson and keywords edmundson
till nowadays this type of summarization is the most used vanetik and litvak aries et al
vicente et al
thomas et al

what makes extractive methods so famous is their simplicity compared to the abstractive ones
abstraction in the other hand is the generation of new text based on the input
stractive systems are dicult to be designed due to their heavy dependence on linguistic techniques
specifying the domain of the system can simplify the creation of this type of systems mitkov


partiality partiality by denition is the bias in favor of one thing over another
following partiality a summarization system can be neutral or evaluative
a neutral system produces summaries which reect the content of the input without judgment or evaluation
they are not designed to specically include opinion into the summary even if the input document contains judgments
most works fall into this class luhn baxendale edmundson vanetik and litvak aries et al
vicente et al
thomas et al
hua zhong et al

in contrast an evaluative system includes automatic judgments either implicitly or explicitly
an explicit judgment can be seen as some statements of opinion included
the implicit one uses bias to clude some material and omit another
a lot of examples can be aorded for this type of summarization especially with the growth of interest towards users opinions
for instance the work of othman et al
is based on summarizing customer opinions through twitter
given a conversation of tweets the method tries to eectively extract the dierent product features as well as the polarity of the sation messages
the year knew the debut of a new task within tac conference called opinion summarization task which was meant to generate summaries from answers on opinion questions see section



format each system has a format used to present the resulted summaries
some systems use a xed format while others present the summaries based on user preferences or based on their goals
once again most systems generate a xed format mostly by joining sentences together
most of them are research systems which focus on the informational part of the summary rather than its format presentation to the user
floating situation summarization systems try to display the content of summaries using variable settings to a variety of readers for diverse purposes
the ontosum system of bontcheva is one of these systems which uses device prole e

mobile phone web browser to adjust the summary formatting and length
approaches in this section we will discuss dierent approaches of ats
according to nenkova and mckeown topic representation approach contains topic words frequency driven latent semantic analysis
we are rather interested in the nature of used resources are they dependent on a specic language or domain do the methods need a lot of resources this is why we follow their other taxonomy nenkova and mckeown called semantics and discourse which we present as linguistic approach since its methods as highly connected to the language being processed
the taxonomy of lloret and palomar seems to approach our vision
the dierence is that we consider topic based and discourse based approaches as sub approaches of linguistic one since the two are based on linguistic properties of the input text

statistical statistical approach has been used in ats since its rst days
it is based on some features which are used to score the relevancy of text units generally sentences to the main topic or users requests
these features can be combined to score the units using many aspects and get the highest scored ones
but combining some features does not always improve the quality of a summary
as follows we will present the most used amongst statistical features


term frequency it is the oldest feature luhn and the most famous one
it supposes that a term repeated many times in a text is important
this feature has a problem when it comes to domain relative words for example in documents talking about computer science certain words such as computer will have great frequencies even if they do not represent the main topic
to address this problem luhn uses two thresholds to ensure that the term is important yet not specic to the document s domain
a more advanced solution is to use tf idf dened by salton and yang
the measure idf inverse document frequency is calculated as in equation
idf t log t where is the number of documents in d t is the number of documents containing t


position in the text the position of words luhn and sentences baxendale edmundson in the text has good potential to capture their importance
in luhn the position of words in a sentence is used to create some sort of groups where each one is the set of signicant words separated by at most to non signicant words
then the group which has the most signicant words is used to score the sentence as shown in figure
the position of sentences in the text is used as indicator of its importance the rst and the last sentences tend to be more informative than the others edmundson
for instance in scientic articles sentences from the introduction and the conclusion contain more information about the subject than the other sentences
in baxendale it is established that the rst and the last sentences in paragraphs are more important
position feature can be used dierently to score the sentences
nobata and sekine dene three methods to score a sentence based on its position in the input document only the rst sentences less than a dened position are important a sentence s importance is inversely proportional to its position and the rst and the last sentences are more important
according to the authors the second method gives the best results among the three
fattah and ren use the position of sentences in paragraphs figure luhn score using word position luhn
instead of the whole text
they suppose that the rst sentences in a paragraph are the most important and thus the others should have a score of zero
like luhn ouyang et al
use word position based on the hypothesis that a word is more informative if it appears earlier in the text
therefore the position of a word can be calculated according to its other occurrences in the whole text and not just according to other words in the sentence
for that four dierent functions are dened direct proportion dp inverse proportion ip geometric sequence gs and binary function bf
dp attributes a score of of the rst appearance and n to the last one where n is the count of words in the sentence
ip is calculated as i where the degree decreases quickly at smaller positions
this gives advantage to leading sentences
gs function scores an appearance of a word as the sum of the scores of all its following appearances as
bf aords more importance to the rst appearance of a word and the others an equally less importance
therefore the rst one will get a score of and the others a score of a given
the nal score is calculated as shown in equation wis where is one of the four functions shown previously is the frequency of the word wi is the length of the sentence


title and subtitle words the title carries the topic of the document this hypothesis was rst introduced by edmundson
when we divide a document into sections and subsections we choose representative titles for each
so any sentence containing some words of a title is considered as important
this feature can be seen as if the title was a request salton and buckley
ishikawa et al
fuse this feature with term frequency in a way that the frequencies of the title words are weighted more than regular words
in equation a sentence is scored by the frequencies of its words w if the word belongs to the title its frequency is multiplied by a number a the authors used a
tf w where a if w title otherwise to score sentences based on title s words nobata and sekine propose two methods
according to the authors the second method shown in equation gives better results
et et tf e tf tf e tf where e are named entities and tf is term frequency


sentence length this feature was used to penalize too short sentences kupiec et al

given a threshold for example words the feature is true if it exceeds this threshold and false otherwise
sentences which are very small in number of words are unlikely to be important so it is better to omit them
a more complex formula to score a sentence is expressed in the two methods proposed by nobata and sekine
the rst method scores a sentence based on its length and a predened maximum value lmax
the second which gives better results aords a negative score to penalize sentences shorter than a predened minimum value lmin
a more recent formula proposed by fattah and ren is indicated in equation
it scores a sentence si using its words number the document s words number and the number of sentences in this document
s

centroid a centroid as dened by radev et al
is a set of words that are statistically important to a cluster of documents
since it is the most important in the cluster the documents and sentences containing it are also important
one famous summarization system using clusters centroid to extract summaries is mead radev et al

mead is a multi document summarizer where similar documents to the centroid are clustered together
then a set of parameters centroid value positional value and first sentence overlap are used to rank each sentence from the resulting cluster
the centroid value ci for a sentence si is computed as ci cw i
where cw i is the centroid value of a word w in sentence si
the positional value pi for a sentence si is computed according to its position in the document with n sentences as follows pi cmax where cmax is the maximum centroid score in that document
the first sentence overlap value fi of a sentence si is given as fi si where is the rst sentence
these three scores are combined into one score along with a redundancy score to extract the most scored sentences
wsi n

frequent itemsets frequent itemsets are common sets of items which have at least a minimum amount of times
it was proposed by agrawal and srikant to solve the problem of discovering association rules between items in a large database of sales transactions
in ats the itemsets are considered as sets of terms extracted from sentences where those which co occur in many sentences are considered as frequent itemsets
baralis et al
apply an entropy based strategy to generate compact itemset based models where each document sentence is seen as a separate transaction
they formalize the problem of selecting sentences as a set covering optimization problem which is solved by a greedy strategy where sentences covering the maximum number of itemsets
this method is ameliorated in mwi sum system baralis et al
by replacing traditional itemsets with weighted ones in order to increase item relevance in the mining process
also term frequency document frequency tf df is used as relevance score
this approach is used in litvak and vanetik with the minimum description length mdl principle that employs krimp compression algorithm vreeken et al
for query based ats
the key idea is to use the query to select related frequent itemsets word sets


latent semantics latent semantic analysis lsa seeks to analyze relationships between a set of documents and the terms they contain
it was used in steinberger and jezek for text summarization
the algorithm starts by creating a matrix a of m rows representing the document terms and n columns representing the sentences where ai j a represents the frequency of the term i in the sentence j
then the singular value decomposition svd of the matrix a is represented as in equation
where a u v t u ui is an m n column orthonormal matrix whose columns are called left singular vectors



n is an n n diagonal matrix whose diagonal elements are non negative singular values sorted in descending order v vi is an n n orthonormal matrix whose columns are called right singular vectors
then the salience of a sentence k is given in equation
sk k i
n

combination of features usually to score a unit sentence not just one feature but many are used as shown in equation
f scoref f where f is the weight of a feature in the features set f
an early work which combine many features to score sentences is the work of edmundson
the author used four features cue words c cue term frequency k key title words t title and position l location
he used all the combinations of these features which gives dierent combinations where the weights are equal to
most systems use dierent features combined together linear combination generally to get a unique score
the features weights can be xed manually or using machine learning techniques described in a later section
also other methods such as optimization can be used
for instance feigenblat et al
propose an unsupervised query focused multi document summarization method based on cross entropy ce method rubinstein and kroese
it seeks to select a subset of sentences that maximizes a given quality target function using some statistical features

graphs inter cohesion between text units sentences is an important property a summary which contains linked units has the chance to be more pertinent to the input document s main topic
graph based approach is based on transforming documents units sentences paragraphs
into a graph using a similarity measure then this structure is used to calculate the importance of each unit
we divide graph based works into two categories those using graph properties such as the number of neighbors and those iterating over the graph and changing nodes scores till reaching a stable graph representation


graph properties in salton et al
document paragraphs are used to construct a graph of similarities where each paragraph represents a node which is connected to another when their similarity is above a given threshold
the authors dene a feature called bushiness which is the number of a node s connections
the most scored paragraphs in term of bushiness are extracted to form a summary
node s bushiness can be used to score sentences alongside other statistical features fattah and ren
in a graph of similarities between sentences the importance of a sentence is the number of arcs connected to it
equation represents the score based on number of arcs where g s a is the graph of similarities between the sentences s is the set of sentences and a is the set of arcs
sj a sj s beside bushy path of the node ferreira et al
dene another property called aggregate similarity
instead of counting the number of arcs connected to a node their weights are summed to represent this node s importance
in their work the authors investigate the fusion of multiple features either graph based or sentence features


iterative graph lexrank erkan and radev and textrank mihalcea and tarau are the most popular ods using graph based summarization approach
the two methods use a modied version of pagerank brin and page in order to score sentences
lexrank method erkan and radev uses cosine similarity to construct a weighted graph where the nodes with a weight similarity less than a given threshold are omitted
the continuous version follows almost the same equation as of textrank but it uses a tf idf based cosine similarity instead
in textrank an indirected graph is constructed from the input text where each sentence represents a node and the arc between two nodes is weighted by their similarity
equation scores each sentence i based on its neighbors and it is executed recursively till reaching convergence where is the damping factor usually around

w d wji w wjk vj where wij wk si and wk li et al
use pagerank to rank document events rather than sentences then extract those sentences containing more important events
they dene an event as either a named entity ne person organization location or date or an event term et verb or action noun
the relevance between two events is used to weight the arcs relating them
it is calculated as the number of association in case of a pair of et and ne
in case of an et pair it is calculated as the number of ne associated to both of them
similarly in case of an ne pair it is the number of et associated to both of them
in case of multi document summarization where there are some documents more recent than others the temporal information matters
recent documents contain novel information in an evolving topic therefore their sentences may be given more chance to be included into the summary
wan proposes a method called timedtextrank to incorporate this information into textrank method
the informativeness score w is time weighted multiplying it by
where y is the current time and tj is publication time of the document containing a sentence j
textrank and lexrank exploit sentence to sentence relationships to score them under the tion that they are indistinguishable
but in multi document summarization a document may be more important than others and therefore its sentences must be favored over others
wan proposes adding a sentence to document relationship into the graph based ranking process
in addition to ments impact on sentences the author argues that even sentences in the same document must not be treated uniformly
the position of a sentence and its distance to the document s centroid are two factors to be included in sentence score
most graph based summarization methods mihalcea and tarau erkan and radev li et al
wan are based on ranking algorithms developed for web pages analyze such as pagerank brin and page and hits kleinberg
in their method called ispreadrank yeh et al
exploit activation theory quillian which explains the cognitive process of human prehension
the idea is to construct a graph of similarities between sentences score each sentence using some features centroid position and first sentence overlap then spread the scores to the neighbors iteratively until reaching equilibrium
some works tried to introduce machine learning into graph based ats
in liu et al
a prior probability is incorporated into pagerank algorithm to introduce query relevance into graph based proach
the prior probability q is estimated using naive bayes where the relevance of a sentence is estimated using four features paragraph feature position in paragraph feature mood type ture and length feature and the relevance of query having some sentences s is estimated using shared named entities between a query and the sentences in the training corpus
then this probability is introduced to the past lexrank equation see equation as in equation
w q d wji w wjk vj this model can select sentences with high relevance to the query without loosing the ability to select those with high information novelty
statistical approach uses some primary nlp techniques such as word segmentation stop word elimination and stemming which are used by information retrieval systems
in the contrary linguistic approach uses more profound nlp techniques part of speech rhetoric relations semantic
to generate summaries either by extraction or abstraction

linguistic

topic words the presence of some words such as signicant impossible
can be a strong indicator of the relevance of a sentence to the main topic edmundson
a dictionary can be prepared from a corpus to save three types of cue words bonus words which are positively relevant stigma words which are negatively relevant and null words which are irrelevant
the score of a sentence based on this feature is the sum of the weight of every word w according to the dictionary as indicated in equation
where wsi if w bonus if w stigma otherwise in fattah and ren the cue words are divided into two groups positive keywords and negative keywords
positive keywords are dened as the keywords frequently included in the summary
the score of a sentence si using positive keywords is given by equation
tf p wsi where p is the probability that a sentence si belongs to a summary s given the occurrence of the word w which can be estimated using machine learning
accordingly negative keywords are the words unlikely to be in a summary thus p


indicators paice denes them as commonly occurring structures which explicitly state that the sentences taining them have something important to say about the subject matter or the message of the document for example the principal aim of this paper is to investigate



the identication of such indicators is not an easy task in his work the author followed these steps we ca nt list all the indicators due to their variations
for example these expressions have the same structure this article is concerned with


our paper deals with


the present report concerns


and the following discussion is about



so the solution is to use some templates
using templates some sequences of words may show up which are not part of the indicators selves
the solution is to use skip limits between the paradigms of a given template
there exists optional words but can add a weight when be used such as the word here in the expression the purpose here is to



this can be addressed by dening multiple paths in the template
to handle the variations of words their stems can be used in the templates instead
figure represents a template where the words or stems are paradigms
the skip limits are shown thus
weight increments are shown thus
a query denotes an optional paradigm
figure a slightly simplied template paice


co reference information some works use statistical approach to calculate the score but they use linguistic techniques for that so we can consider them as linguistic ats systems
in the work of orasan and st the authors try to use anaphora resolution to improve the informativeness of summaries
sentences usually contain pronouns rather than words which lead to incorrect score calculation
anaphora resolution will increase the frequencies of words referred by these pronouns and produces more accurate frequency counts
the authors use a simple term frequency algorithm to score sentences and six anaphora resolution methods
the average informativeness was improved using anaphora resolution
semantic representations of terms are often used to generate summaries
using ontologies and lexicons like wordnet miller the semantic relationship between sentences words can be exploited to enhance summary generation
hennig et al
train an svm classier to identify salient sentences using ontology based sentence features
this classier is used to map the sentences to a taxonomy created by the authors where each sentence is assigned a set of tags
for each sentence the tags are used to calculate the similarity with the document tags to determine how well a sentence represents the information content of its document in the ontology space


rhetorical structure the structure of discourse can be exploited through rhetorical relations between sentences to generate summaries
ono et al
use a penalty score dened over dierent rhetorical relations to exclude non important sentences
in marcu a discourse tree is built to reect the rhetorical relations between the text s sentences as illustrated in figure where the leafs are sentences
figure an example of a rhetorical tree marcu
the author uses seven metrics based on the rhetorical tree to nd the best discourse interpretation similar to those of summaries the clustering based metric the marker based metric the clustering based metric the shape based metric the title based metric the position based metric and the connectedness based metric
kikuchi et al
propose a single document ats based on nested tree structure
the authors exploit words dependency and rhetorical dependency by constructing a nested tree composed of a ment tree and a sentence tree
the document tree has sentences as nodes and head modier relationships between sentences obtained by rst as edges
the sentence tree has words as nodes connected by head modier relationships between them obtained by the dependency parser
the summarization is lated as combinatorial optimization problem in order to trim the nested tree
a more recent work using rst is the work of goyal and eisenstein to x the problems of local inference techniques which do not capture document level structural regularities and annotated training data
so the authors investigated the use samplerank wick et al
structure learning algorithm as a potential solution to both problems

machine learning ml usually machine learning approach is coupled with other approaches to improve and estimate their generation rules instead of xing them manually
it can solve the problem of combining features in statistical approach
many works have been conducted to generate summaries using machine learning some focus on the choice of appropriate classication methods kupiec et al
osborne yeh et al
some try to solve the problems related to training phase such as the absence of labeled corpora amini and gallinari others use heterogeneous features in their algorithms and try to combine them using machine learning wong et al
yatsko et al



as a tuning function ml can be used with other approaches to tune certain parameters
it is mostly accompanied with tical approach to solve the problem of xing features weights where these features scores are combined linearly into one score
yeh et al
propose a method using genetic algorithm to estimate the tures weights sentence position keywords centroid and title similarity
the learning makes the system dependent to the corpus s genre
a solution is to tune the features according to the input document s genre as suggested in yatsko et al

features are used to train the system on three genres scientic newspaper and artistic texts
the system can detect the genre of the input document and execute the adequate model of scoring


as a decision function given a corpus of documents with their extractive summaries a machine learning algorithm can be used to decide if a unit sentence mostly belongs to the summary or not
kupiec et al
propose an ats system based on bayes classication in order to calculate the probability that a summary may contain a given sentence
so for each sentence si the probability that it belongs to a summary s using a features vector is given in equation
p p s p s p fj where p s is a constant so it can be emitted since the probability is used for reordering p s and p fj can be estimated from a corpus
choosing the right classication algorithm is another issue for instance bayes classication supposes features independence which is not always the case
from this point of view osborne uses a maximum entropy based classier which unfortunately performs lower than bayes because it tends to reject a lot of sentences but when he adds a prior probability the method performs well and surpasses bayes based method
his method though is not used to reorder the sentences using the probability but to classify them into two classes summary sentences and other sentences


bayesian topic models topic models are based on identifying the main concepts from the input and nd the relationship between them to construct a hierarchy
words of each input document are assigned to a given number of topics where a document is a mixture of dierent topics and each topic is a distribution of words
bayesian topic models are quite sophisticated for multi document summarization since they make dierence between documents in contrast to most ats methods which consider them as one giant document nenkova and mckeown
one of bayesian topic models advantages is their simplicity to to incorporate dierent features such as cue phrases used in eisenstein and barzilay for unsupervised topic segmentation
daume iii and marcu propose a query driven muli document summarization method using bayesian topic models which is reported in nenkova and mckeown as one of the rst works in this direction
in their method the authors train their system to have three probability distributions general english learning model p g background document language model p d for each one of the k available documents with s number of sentences and n number of words w in a sentence and query language model p q for each query q over a set of j queries
using this trained model they estimate using expectation propagation minka
figure represents the graphical model of their method where r is the relevance judgment z is the word level indicator variables is the sentence level degrees and is a dirichlet distribution as prior over
celikyilmaz and hakkani tur introduces two tiered topic model ttm which models topics as two levels low level topics which are distributions over words and top level topics which are correlations between these lower level topics given sentences
one problem with ttm is its incapability to dierentiate general words from specic ones given a query
a sentence containing general words which are more frequent in document clusters must have more chance to be included into the summary
the authors present an enriched ttm ettm generative process to sample words from high level topics leading to three words distributions low level topics over specic words high level topics over general words and background word distributions
figure bayesian query focused summarization model proposed in iii and marcu
yang et al
argue that using expert summaries to indicate only two levels of topics limits the practical applications of celikyilmaz and hakkani tur s model because we can draw many latent topics from multiple documents
the availability of modal summaries and their quality as golden standards are two other challenges to this method
furthermore ttm does not take word dependency in consideration
to address these limits they propose a new method called contextual topic model ctm which is based on bayesian topic model ttm hlda model blei et al
topical n grams wang et al
with some concepts of graph models


reinforcement learning rl ryang and abekawa see ats extractive approach as a search problem
they model the problem of constructing a summary as a rl and attempt to optimize the summary score function based on some feature representation where the score is a trade o between relevance and redundancy
the summary is represented as a state with a history of actions and an indicator showing if it is terminal or not
an action inserting a sentence into the summary or nishing is selected based on a probability calculated using boltzman selection
as for the reward the summary is rewarded only when it is in nish state it is rewarded by the summary score if it does not violate the size limit and it is penalized otherwise
hen et al
follow the same method but with some changes
they use a dierent reward function based on reference summaries during training
instead of using temporal dierence learning they use q learning to determine the value of the partial summary and the value of adding a new sentence to a summary state
their method learns one global policy for a specic summarization task instead of one policy for each document cluster
similarly rioux et al
base their work on the method of ryang and abekawa
they use an improved version of td called sarsa which in addition to modeling state space models actions space too
their reward function is immediate at every action to help the learner get immediate feedback


deep learning deep learning has gained a lot of attention recently even in ats research
it is based on large neural networks nn which eventually needs a huge amount of data to be trained
rush et al
are the rst to successfully apply deep learning to an abstractive ats
the method uses a local attention based model to generate each word of the summary conditioned on the input sentence
their system called performs well on shared task despite being tested using rouge metric which mostly encourages extractive summaries
their method shows some limitations when it comes to input document and summary sizes
it processes only documents with a size of about words and produces a very short summary about characters
following their lead nallapati et al
also use an attention model in the encoder decoder
when they decode they use only the words that appear in the source document following a method code
com facebookarchive namas october tion butnolongerrequiredtosumtoone



igivenby


j anddoc k



j



cepttheonecorrespondingtokisnon zero orifanyquerycomponentofexceptthosequeriestowhichdocumentkisdeemedrelevantisnon umentorquerycomponents
wehavefurtheras respondstogeneralenglish k


pq therelevancejudg levelin dicatorvariableszandthesentenceleveldegrees
theroundedplatesdenotereplication therearejqueriesandkdocuments containingssen tencesinagivendocumentandnwordsinagivensentence
graphicalmodelforthebayesianquery focusedsummarizationmodel
j r
inthecaseofthevariables accordingtothepriordistributiongivenby
inthecaseofthezvariables
pdandpq
computingthisexpressionandndingoptimalmodelparametersisintractableduetothecouplingofthevariablesundertheintegral
tractableintegrals andalargevarietyoftech niqueshavebeenproposedtodealwiththis
mationandthevariationalapproximation
athird lesscommon butveryeffectivetechnique espe ciallyfordealingwithmixturemodels isexpec
inthispaper called large vocabulary trick lvt jean et al

then to introduce new words they add a layer of nearest neighbor in the input
the decision whether to use a word from the input or a new word based on the context is guaranteed by another layer they call switching generator pointer layer luong et al
vinyals et al

ling and rush try to x the problem of speed when long source sequences document marization are processed using sequence to sequence models with attention
so they propose to use a two layer hierarchical attention where the rst layer s function is to select one or more important chunks from the input document using hard attention then feed it them into the second layer using sequence to sequence model
they use reinforcement learning to train the hard attention model
the method shows promise to scale up existing methods into large inputs but fails to beat the standard sequence to sequence model
deep learning was not used just for abstractive ats but also for extractive one
in liu et al
hua zhong et al
the authors propose a query oriented multi document ats method based on a deep learning model with three layers concepts extraction summary generation and tion validation
concentrated information is used with dynamic programming to seek most informative sentences
denil et al
use a hierarchical convnet architecture cnn divided into a sentence level and a document level
the sentence level learns to represent sentences using their words and the document level learns to represent the input document using the rst level
then these representations are used to score how pertinent a sentence can be towards the entire document
in cao et al
a recursive neural networks is used to rank sentences for multi document summarization
they use two types of hand crafted features as inputs word features and sentence features
this enables the system to learn how to score sentences based on a hierarchical regression process which uses a parsing tree to score sentence constituents such as phrases
like hua zhong et al
and hamey propose an extractive query oriented summarization method based on deep learning but for single document ats
the main dierence is the use of an unsupervised approach with deep auto encoder ae which was used in hua zhong et al
as a word lter rather than a sentence ranker
the ae in this case can learn dierent features rather than manually engineering them
furthermore the authors investigate the eect of adding random noise to the local word representation vector on the summary
in ren et al
a neural network of two levels using contextual relations among sentences is proposed to improve sentence regression s performance
the rst level captures sentence representations using a word level attentive pooling convolutional neural network
then the second one constructs context representations using sentence level attentive pooling recurrent neural network
these learning representations and the similarity of a sentence with its context are used to extract useful contextual features
the authors train the system to learn how to score a sentence to t the ground truth of

compression the compression or reduction of sentences is the elimination of non informative parts especially when the sentence is too long
jing and mckeown conrm that the compression is often used by professional summarizers
in fact they found out that of summaries sentences are borrowed from the original document and that half of them have been compressed
in knight and marcu two statistical compression methods have been proposed using the noisy channel model and using decision trees
the rst method supposes that the input sentence was short but has been bloated with additional words
for a given long sentence a syntactic tree is produced to which a quality score is attributed
it is calculated using probabilistic context free grammar pcfg scores and the probabilities of next sentence calculated with bi gram language model
they measure the likelihood of transforming a wide tree to a reduced one using statistics gathered from a corpus of documents and their summaries
the second method is based on decision trees used to learn the dierent rules to reduce sentences
likewise in their statistical compression method zajic et al
see the sentence as if it was a headline distorted by adding other words to it
they use hidden markov models hmm to represent the problem and nding the headline which maximizes the likelihood of this headline generating the sentence
this likelihood is estimated using probabilities that a word is followed by another in a corpus of english headlines taken from tipster corpus
the authors propose another rule based method where they use a parser to get the syntactic representation of the sentence used to remove some components such as temporal expressions modal verbs complimentizer that
the compression is executed iteratively by removing one component at a time outputting the compressed sentence into the extraction module
other methods have been proposed to improve sentences compression using not only one sentence but using similar sentences
in cohn and lapata instead of shortening the sentence by removing words or components the authors introduce additional steps as substitution reordering and insertion
in filippova a method is proposed to summarize a group of related sentences to a reduced one
in this method the sentences are represented by one graph which is used to generate a reduced summary by following the shortest path
deep learning has been applied in order to compress sentences
filippova et al
use long short term memory models lstm in order to perform a deletion based sentence compression
the method performs very well either in term of readability or informativeness even without aording syntactic information pos ne tags and dependencies
the same method is used in hasegawa et al
to compress japanese sentences but with some changes
these modications are based on three japanese language characteristics frequent verbs are nominalized and nouns are abbreviated non verbs can be the root node and easily estimated subjects and objects are omitted

approaches discussion we based our taxonomy on resources dependency is a method based on some corpora does it depend on some language toolkits how much calculation power it needs our intension is to classify ats methods based on resource availability and the eort spent to be implemented
also we try to show the capacity of each approach to support multilingualism
statistical features based methods were the rst introduced in ats
features like term frequency position and length are language independent and are good indicators of sentence relevance
a statistical approach does not need much language dependent tools just some basic nlp tasks such as sentence segmentation tokenizaion stop word elimination and stemming
mostly it is easy to be implemented and does not need a lot of processing power
but mostly these features are combined together in hope to increase sentence relevance
this leads to a more complicated problem which is how to combine them and how much amount of inuence of each one
the problem can be solved by combining them linearly and x their weights through experiments using a corpus or estimate them using machine learning
either way we will have another problem which is language dependency
graph based approach seeks to exploit the shared information between sentences
it is a bottom up method which discusses the similarity problem from the perspective of content structure yang et al

it can be language independent when using just lexical similarities mostly cosine similarity and statistical features
but some room for improvement can be done by considering more language dependent similarities like semantic similarity
also the modal can be improved using machine learning by introducing prior probabilities into the equation as in liu et al

processing a great amount of text using iterative graphs can consume more processing prower this should be investigated to better understand how far this approach can go
linguistic approach is more powerful than the statistic one because it integrates richer processing of the input text
also it can be used either for extractive or abstractive summarization
the problem with this approach is the lack of appropriate nlp tools for certain languages
it can be as simple as using sentence components verbs nouns
as statistical features or as hard as using sentence structure and its relationships with others to generate a new text
mostly it is harder to be implemented and takes more time to generate a summary
nenkova and mckeown suggests using linguistic methods as a post processing task to improve linguistic quality of the generated summary rather than a processing one
according to authors it is unclear how much this approach can improve content selection compared to the methods using no linguistic relations
all approaches discussed previously can be ameliorated using machine learning ml
but in the other hand they can loose multilingual property unless the system is trained on as many languages as possible
still the problem of domain training your system on news articles does not mean it can handle ction as good as it does with news
also collecting labeled corpora for one language can be a very hard work let alone many languages
some works seeks to use unlabeled data and some propose auto supervised methods such as in amini and gallinari
sentence compression seeks to get rid of non informative parts allowing us to have shorter sentences
it can be used as a post processing task after generating a summary which will eliminate more redundancy and allows more space for other sentences to be included
it is a language dependent task which ca nt be used alone to have a summary but in conjunction of other approaches
we tried to summarize all the above discussion in table
the table shows some advantages and disadvantages of each approach some encountered problems and the eventual xes
table comparison between dierent ats approaches
approach pros cons problems xes statistical graph less resources simple fast readability less resources simple coherence processing power more accurate abstractive ats resources toolkits complex processing power features combination relevancy redundancy in sentences machine learning linguistic features compression sentences similarity sentences documents properties tf idf semantic statistical tic similarity document temporal property features linguistic generation rules machine learning machine learning deducing rules matically lack of corpora labeled corpora corpora insuciency reinforcement learning corpora creation compression less redundancy sentence level sentence level fuse with other as proaches processing task a evaluation evaluating summarization systems is one of the most dicult tasks
it is not clear which is the perfect summary since humans produce dierent summaries and yet we can consider them all good
also the evaluation must be accurate objective and fast
this led to the appearance of many evaluation methods which can be automatic manual or semi automatic
many workshops have been organized to evaluate the performance of summarization methods and test how eective evaluation methods are

evaluation methods ats evaluation methods can be considered as intrinsic or extrinsic mani
intrinsic evaluation is meant to evaluate the system in of itself
it is based on two criteria summary coherence and summary informativeness
in the other hand extrinsic evaluation seeks to determine the eect of summarization on other tasks
figure represents the dierent categories of summarization evaluation as described in mani


intrinsic evaluation the evaluation can be done by comparing the generated summary to the original text or to a reference summary
when the system s summary is compared to the original text actually we look for the quantity of information recovered more like information retrieval systems
comparing it to a reference summary will allow us to quantify how good the system can be against humans
also the intrinsic methods can be seen as text quality evaluation methods or content evaluation methods steinberger and jezek
text quality evaluation methods attempt to verify linguistic aspects of the generated summary such as grammaticality reference clarity and coherence
the content evaluation methods can be divided into two sub classes co selection methods which evaluate the system based on selecting the right sentences and content based methods which go deeper by using smaller units such words and grams
figure classication of evaluation methods
given a summarizer which takes an input document e and generates a summary s and given a reference model summary m which is some preselected sentences from e we can dene the following relations tp true positive tn true negative fp false positive and fn false negative
the recall is the quantity of right information recovered by a system comparing to what the it should recover see equation
the precision is the quantity of right information recovered by a system comparing to what it has recovered see equation
the f score is a mix between the recall and the precision using harmonic mean see equation
if is more than one the recall is advantaged
if it is less than one the precision is advantaged
the most used f score is score which is a trade o between recall and precision
r t p t p f n m s s p t p t p f p m s m f r r we have to say this type of evaluation does not give systems credit if they choose other sentences similar to the preselected ones
it may be useful to use it when the summary must contain a precise information from the input text
but in general to evaluate a summarization system it must be avoided especially for abstractive methods
recall oriented understudy for gisting evaluation proposed by lin and hovy is a method inspired from another used for automatic translation evaluation called bilingual evaluation understudy bleu papineni et al

the objective is to automatically measure the quality of the generated summary comparing to a reference one
the idea is to compute the number of units n grams in both the system s summary and the reference one and calculate the recall
since a text can have many summaries this method allows the use of many reference summaries
many variances of rouge have been proposed lin rouge n rouge l rouge w rouge s rouge su
equation describes how rouge n is calculated
rou ge n ssummref ssummref n grams gram n grams gram where n is the size of the n gramme gram is the number of n grammes found in both the candidate and the reference summaries
gram is the number of n grammes in the reference summary
using similarity measure between automatic summary and human made one can present some itations nenkova et al
human variation analyze granularity semantic equivalence semantic equivalence and the comparison between extractive and abstractive summaries
pyramid nenkova et al
comes to solve these limitations using a semi automatic approach
given a number of reference summaries annotators are asked to dene summary content units scus where the weight of each scu can be calculated as the number of relative reference summaries
the process of dening the scus is not
berouge
com pages default
aspx november summarization evaluationintrinsic methodsextrinsic methodscomparison againstreference outputcomparison againstsummarization inputrelevanceassessmentinformationretrievalquestionanswering


given they can be as small as modiers of a noun phrase or as large as a clause
the scus are regrouped in a pyramid of n tiers where n is the maximum weight of scus
so an scu which belongs to the tier ti will have a score of i where is the number of scus in this tier
for a summary with x scus equation shows the optimal content score where scus which do not belong to the pyramid will have a zero weight
max i j x n i n n where j max x i i t i basic elements bes are minimal semantic units which can be extracted from a sentence hovy et al

their goal is to automate summaries evaluation in contrast of pyramids which needs human involvement
this later can introduce some problems such as human variability and evaluation expensiveness in time and cost
to evaluate a summary three dierent modules are proposed be breakers be matchers and be scorers
the rst one is used to break the text into bes which are dened by hovy et al
as the head of a major syntactic constituent noun verb adjective or adverbial phrases expressed as a single item or a relation between a head be and a single dependent expressed as a triple head modier relation
the matching of bes is either lexical using lemmas using synonyms from wordnet miller using paraphrases or using semantic generalization
each be gets a weight of point for each related reference summary
automatic summary evaluation based on n gram graphs autosummeng giannakopoulos et al
attempts to be language neutral fully automatic and context sensitive
the method is based on n grams so to be language neutral no preprocessing must be done this is why the n grams are extracted over the characters
then a graph is constructed where the n grams represent its nodes and each arc connecting two n grams is the number of times these n grams are judged as neighbors given a distance window dw
given two summaries si and sj with a set of graphs and respectively and given two graphs gi and gj with the same rank of n grams the value similarity is given in equation
v gj egi gj e wj e e wj e with as the membership function which returns when e gj and otherwise
wi e are the weights of the edges of the element e in the graphs gi gj respectively
the overall similarity of and is the weighted sum of the v s over all ranks equation
wj v lmax r v sr lmax r where r is the rank of n grams lmin lmax are the minimum and the maximum rank of n grams v sr is the value similarity using the graphs with n gram of rank r
the grade of a summary is the average of all its similarities with the reference summaries
merged model graph memog is somehow a variant of autosummeng where all the graphs of reference summaries are merged into one graph giannakopoulos and karkaletsis
n gram graph powered evaluation via regression npower giannakopoulos and karkaletsis is a combination of many of the past metrics via optimization
the idea is to create a method which stands as an independent judge to combine the dierent metrics grades into a better estimate
machine learning linear regression is used to estimate the nal grade
given a vector of features x here rouge be
and a target numeric feature y r y where is an unknown function human scores or pyramid scores the idea is to estimate a combination function f as shown in equation
f x r f x x looking to the expansiveness of the summarization systems evaluated by these past metrics we can say that they are all background systems
but how about just the news systems in case we want to evaluate automatic summaries in term of new information which they contain
nouveau rouge conroy et al
is a method based on rouge metrics to evaluate update summaries
the idea is to calculate rouge score between an original summary and an update one then a high rouge score indicates high redundancy
in trec temporal summarization task aslam et al
many metrics are proposed to address this issue
given an event wikipedia event pages gold standard updates called nuggets are extracted and annotated to form a set of nuggets n such as a nugget n n has the properties the time stamp of revision history n
t and the importance provided by assessors no importance to high importance n
i
the evaluation metrics are designed to measure the degree to which a system can generate these nuggets in a timely manner
each system must generate a summary s containing some updates where an update u is a sentence length text u
string having a time stamp u
t
the earliest update u that match a nugget n is dened as m n s argmin us
t
then the inverse function is dened as m s n n m n s u
the rst metric is expected gain metric eg which is similar to traditional notions of precision in ir evaluation see equation
when using latency discount the chosen latency step was hours
where s us n discounting factor nm s graded binary discount free gain en
i
i emax if n
i otherwise discounting factor latency discounted gain
t u
arctan u
tn
t another metric is comprehensiveness metric which is similar to traditional notions of recall in ir evaluation see equation
nn us s

extrinsic evaluation ats is often used to complete other tasks like executing instructions information retrieval question answering relevance assessment
to evaluate how well a summarization system is doing leads to evaluate its eectiveness towards its related task
in the relevance assessment task the assessors try to score how well a summary is related to a given subject
another example is reading comprehension task where a human judge is asked to answer some questions based on the original document then on its summary
the correct answers number is considered as the score of this summary
in tipster summac evaluation mani et al
two tasks are proposed to evaluate the impact of ats on real world tasks ad hoc task and categorization task
in ad hoc task the focus is to test the pertinence of indicative summaries towards a particular topic
given a document summary or source text a human subject is asked to determine its relevance to a given topic description ignoring whether it was a full text or a summary
the accuracy of the subjects is measured on how well they can indicate the relevance between the subjects and their relative full texts
then the recall precision and score in categorization task the purpose is to determine if a are calculated for the participants systems
generic summary contains enough information to allow an analyst categorizing a document as quickly and correctly as possible
the evaluation is proceeded as in ad hoc task s
but the human subject after reading the document has to choose one category out of ve or choose none of the above
then the three measures recall precision and score are calculated
to assess the usefulness of a summary according to dorr et al
the decision made by a human judge subject based on the summary must be compared to its own decision made on the full text rather than to a gold standard
this is motivated by the fact that the users judgments based on the original texts are more reliable than basing on gold standard judgments
given a summary document pair s d the function d equals to if the subjects have the same judgment on both s and
if the subjects change their judgment between s and
equation calculates the relevance prediction score for a set of summary document pairs dsi in association with an event i
relevance s d another relevance prediction example is trec real time summarization track
the purpose evaluation
github
io rts guidelines
html december of summaries is to aord the users with tweets that are relevant to their proles and are novel
given a prole and a set of retrieved tweets the gain of each tweet t is if the tweet is not relevant
if it is relevant or
if it is highly relevant
this gain is attributed by some users having the target prole
based on this three metrics are dened expected gain eg normalized cumulative gain ncg and gain minus pain gmp which are described in equation
eg n ncg z gm p ncg p where z is the maximum possible gain given the ten tweet per day limit n is the number of tweets returned p pain is the number of non relevant tweets that are pushed and controls the balance between the gain and the pain

and
are used

workshops and evaluation campaigns

tipster summac it was launched in may by the us government in order to evaluate ats systems in a large scale
three evaluation tasks were dened two extrinsic adhoc and categorization tasks and one intrinsic question answering mani et al
the ad hoc task it is intended for indicative summaries based on a specic topic
given a document the evaluator do not know if it is a summary or a full text and a description of a topic the evaluator is asked to determine if this document is pertinent to the topic
the categorization task its aim is to measure the eectiveness of a generic summary ignoring the topic to aord enough information allowing an analyst to categorize a document as quickly and correctly as possible
given a document the evaluator do not know if it is a summary or a full text the evaluator must choose from ve categories the one which is pertinent to the document otherwise he choose no category
question answering task this task seeks to evaluate the summaries in term of their ness
this later is calculated using the number of correct answers which can be found in a summary for some questions generated from the source text
each automatic summary is compared manually to some answer keys for each input document to decide if the answer is correct partially correct or incorrect
ars answer recall strict and arl answer recall lenient metrics were dened to measure accuracy see equation
ars arl
where and are the numbers of correct and partially correct answers in the summary and is the number of questions answered in the key


duc tac in document understanding was launched as evaluation series in the area of text summarization
the aim of this workshop is to move forward the summarization research and enable researchers to test their methods in large scale experiments
duc knew evaluation tasks task very short single document summaries for each english document out of a very short summary must be generated bytes
task short multi document summaries focused by tdt events for each set of english documents out of sets a short summary must be generated bytes
task very short cross lingual single document summaries for each english translation tomatic and manual of arabic documents a very short summary must be generated bytes

nist
november tasks
nist
gov tasks
html february task short cross lingual multi document summaries focused by tdt events for each english translation automatic and manual of arabic document sets a short summary must be generated bytes
task short summaries focused by questions for each set out of english documents sets a short summary must be generated bytes to answer the question in form who is x where x is a name of a person or a group of persons
the summaries which exceed the limit size are truncated and no bonus is attributed to the summaries shorter than this
the evaluation of tasks to uses rouge as a metric and rouge l
in task the summaries are evaluated in term of quality and coverage using summary evaluation environment
as for the pertinence to the question who is x human evaluators have been used
duc used aquaint corpus which contains news articles from associated press new york times and xinhua news agency
there have been two tasks principal task and update task
principal task for each topic of documents the contestants must generate a words summary to answer one or more questions
update task the goal is to produce a words multi document summary as update supposing that the user has already read the previous articles
in this task there are three clusters cluster a with documents for which the generated summary is not an update cluster b with documents for which the summaries must assume the user has already read those of cluster a and cluster c with documents which are more recent than those of cluster b
the principal task is evaluated using many criteria linguistic form of each summary is evaluated manually using some criteria grammar non dancy references clarity focus structure and coherence
for each criterion the evaluator must give a score between not good and very good
the pertinence to a given topic is evaluated manually for each summary the evaluator must give a score between not good and very good
automatic evaluation is used too rouge and be
as for update task each summary is evaluated automatically using rouge be and pyramid
since year duc has been included into the tac conference as summarization track
the aim of this track is to develop ats systems that aord short coherent summaries of document
this task is meant to promote a deep linguistic analysis for ats
it contains two tasks the former duc s update task dang and owczarzak and a new one called opinion summarization task
in the opinion summarization task each system must generate well organized uent summaries of opinions about specied targets as found in a set of blog documents
the questions are not simple hence the answer can not be a named entity
the evaluation is conducted manually using a nugget pyramid created during the evaluation of submissions to the qa task
in tac s summarization track a new task called guided summarization replaced update summarization one
in this task each system has to generate a words summary from news articles for each topic where the topic belongs to a predened category
there are categories accidents and natural catastrophes crises health and safety endangered resources investigations and trials
for a given topic the contestants have to generate summaries for sets a and b one for the set a which is guided by a request
the second for set b is the same as set a but the summary must take in consideration that the user has already seen the documents of set a
each category has some aspects which have to be covered by the summary for example what why when where
to evaluate the content of summaries pyramid method is used
readability and global sensibility are evaluated manually giving a score between not good and very good

isi
edu licensed sw november

ntcir the rst nii testbeds and community for information access research workshop was held in tokyo
it was originally designed to enhance research in japanese text retrieval
the second edition had a text summarization task which aims to collect data for text summarization and evaluate ats systems
the data was gathered from newspapers articles which were summarized by hand to be used for research purposes
two types of summaries were produced extractive summaries which are the important sentences in the text and abstractive summaries
two tasks were proposed intrinsic evaluation which contains two subtasks extractive and abstractive and extrinsic evaluation
the process of each task is as follows fukusima and okumura task the aim is to extract pertinent sentences where the number of the extracted sentences is of the original texts
task the aim is to generate simple abstractive summaries
the generated summaries must have a number of characters of et comparing to the original text
task b in this task the summaries are produced based on some requests
for each request the system has to search for one relevant document and use it to produce a summary
the length of the summary is not limited but it has to be simple
as to evaluate each task the following metrics are used task for each summary the correct sentences are selected
then the metrics recall precision and score are calculated using the number of sentences as a unit
the nal score of each system is the average of all summaries scores
task two ways are used an evaluation based on the content and a subjective evaluation
in the rst one the distance between the two terms frequencies vectors representing the system s summary and the human summary is considered as the score
in the second one human evaluators are asked to evaluate the summaries based on two criteria coverage and readability and give a score of very good to very bad
each one of them is given summaries human summaries the system s summary and a summary produced using lead method
task b in this task human evaluators are given the requests and the generated summaries
for each summary they have to judge if it is relevant or not to the request
recall precision and score measures are calculated for each system based on the number of pertinent summaries
one other measure is the time taken for each system to complete this task


multiling the multiling workshop began as a task of tac conference in which aims to evaluate independent summarization systems on many languages giannakopoulos et al

in this task at least two languages out of seven must be processed by participant systems arabic czech english french greek hebrew and hindi
for each it has to generate a summary of to words
to create the test corpus topics were selected where every topic contains news articles from wikinews
then these articles were translated to the other languages sentence by sentence
to evaluate the generated summaries the two types of evaluation has been used automatic evaluation it aims to calculate the performance of the systems using some model maries created by uent speakers of each language
three methods have been used rouge rouge memog and autosummeng
manual evaluation overall responsiveness of a text was used where each summary was given a score of to based on the content and the quality of the language
when it covers all the important aspects of the original text and remain uent it will be attributed the score
if it is unreadable nonsensical or containing just trivial information it will be attributed the score
the topline system uses the model summaries thus cheating to select relevant sentences as summaries from original texts
the baseline system uses the centroid extracted from a bag of words of same topic documents to extract sentences using cosine similarity

nii
ac
jp november in multiling went from a simple task of tac to a workshop which aims to test and promote multilingual summarization methods
there were three tasks multi document multilingual rization mms giannakopoulos multilingual single document summarization mss kubina et al
and multilingual summary evaluation
the past languages used in mms were used again along with three new languages chinese romanian and spanish
the test corpus contains topics for french chinese and hindi and topics for the remaining languages
the evaluation methodology was the same as the s plus two automatic metrics and npower
the single document task introduces languages with a corpus of documents for each language created from wikipedia s featured articles
to evaluate the summaries automatic methods are used and memog
multiling had two more tasks call center conversation summarization cccs and online forum summarization onforums
in cccs task favre et al
every system must generate abstractive summaries from call center conversations between a caller and an agent
the summaries must contain the caller s problem and the solution aorded by the agent
a corpus of french and italian conversations was used along with english translations of these two which makes them languages
the two submitted systems had a hard time beating the three proposed baselines using as a metric
onforums task kabadjov et al
seeks to bring automatic summarization argumentation mining and sentiment analysis all together
the data is a collection of english and italian news articles along with the corresponding top comments
each system must generate some links between the article sentences and the comment sentences
four research group submitted their systems which have been evaluated using crowd sourcing which is evaluating the linked sentences by a human judge based on relation agreement and sentiment between them


trec text retrieval is a metrics based evaluation of tipster text program which started in
its purpose is to provide the necessary infrastructure for large scale evaluation which can support research within the information retrieval community
temporal summarization is a task of trec which started in and took place over years
the goal of this track is to develop systems for eciently monitoring the information associated with a news event such as protests accidents or natural disasters over time
the track has the following four main aims aslam et al
developing low latency algorithms to detect sub events modeling information reliability with a dynamic corpus understanding and addressing the sensitivity of text summarization algorithms in an on line understanding and addressing the sensitivity of information extraction algorithms in dynamic quential setting and tings
the track includes two tasks sequential update summarization a system should emit relevant and novel sentences to an event
a simulator is given as arguments the system to be evaluated a time ordered corpus an event keyword query the event start time and its end time
for each document in the time ordered corpus the system may choose the sentences relevant to the keyword query and novel comparing to the earlier timestamps
value tracking a system should emit accurate attribute value estimates for an event
a simulator is given as arguments the system to be evaluated a time ordered corpus an event keyword query the event start time the event end time and the event attribute
first the system is initialized with the query and the attribute to generate an initial estimated value
then with every document which is in the time line the system generate a new value along with the supporting sentence s id if there is a change
to evaluate the tasks some metrics have been proposed for detailed description and formulas see aslam et al

nist
gov november the novelty and the relevancy of the updated summary to the event topic
the metric used to measure this is called the normalized expected gain metric
the coverage of the essential information for the event by the summary
the metric used to measure this is called comprehensiveness metric
the degree to which the information contained within the updates is outdated
this is measured by the expected latency metric
since the temporal summarization track was merged with microblog track to form a new track called real time summarization track
rts is meant to explore novel and evolving information needed by users in streams of social media posts such as twitter
to achieve this goal the track includes two scenarios push notications in this scenario scenario a the system must send relevant posts to the user s mobile phone as soon as they are identied
these posts must be relevant to the user s interest in time and novel
email digest in this scenario scenario b the system must identify a batch of tweets according to a specic topic in a daily frequency
the summaries must be relevant to the user s interest and novel
the results are uploaded to nist server after the end of the evaluation
the evaluation takes days where all systems must listen to the twitter sample stream using twitter streaming api
a list of interest proles will be provided to systems
to evaluate the systems mances two approaches are used live user in the loop assessments which is used only for the rst scenario and post hoc batch evaluations used for both scenarios
live user in the loop assessments each system must push a notication to the trec rts uation broker as soon as it identies relevant content to the users interest
these notications are immediately delivered to the mobile phones of a group of assessors who judge them using interleaved approach qian et al

the assessor can judge the tweets in time or later and send back the results to the evaluation broker using a mobile application
each tweet is attributed a mention as relevant relevant but redundant or irrelevant
post hoc batch evaluations a common pool is constructed based on the scenario s submissions where the depth of the pool is determined using the number of submissions and available resources
tweets are assessed based on relevance not relevant get a gain of
relevant get a gain of
or highly relevant get a gain of

then relevant tweets are semantically clustered into groups with substantively similar information the same as trec microblog evaluation
for the rst scenario and gmp scores are used besides the latency which is the mean and the median dierence between the time the tweet was pushed and the rst tweet in the semantic cluster to which the tweet belongs
for the second scenario and are used
challenges there exists a lot of challenges which harden the advance of ats
one of these problems is the absence of a precise denition of what should be included in a summary
over time several works were conducted to generate the perfect summary which must be informative in one hand and not redundant in the other
the rst challenge is the denition of a good summary or more precisely how can a summary be generated
our needs for a summary are good indicators of what it should be extractive or abstractive generic or query driven
even if we get the idea how humans usually summarize implementing it will not be easy
designing a powerful automatic text summarizer needs a lot of resources either tools or corpora
another type of challenges is the summary informativeness how can a system imitate human beings in summarizing task the coherence of the summary is one of the challenges that stood for years
mostly ats methods aim to generate an informative summary but when it comes to the summary s readability it is a matter of future work

github
io november
denition a good summary can be dened as the shortest and more informative grammatically correct one without redundancy
this denition seems simple but when we want to implement such idea we will come across a lot of choices
a text usually contains a main topic and some satellite topics
for instance if we talk about pcs we may also include some commercial notions to express the market shares
the choice of what topic to favor raises the question what is a good summary if the summarization system is query based it is settled that the most similar the topics are to the request the more they are relevant
nevertheless some may argue that adding relevancy to the main topic along with the request can be helpful for the user
when it comes to generic summarization systems it is more dicult to decide what is better for the reader
some researches try to capture the most relevant sentences from each topic to form a summary eg
song et al

the summary can cover all the topics in the text but sometimes it got far from the point
others take only the most relevant sentences to the main topic and ignore the secondary topics
this leads to a summary more focusing on the main topic and ignoring the others which can be as important as the main topic
sure we have to give more importance to the main topics but the satellite ones count too
this is what some researches are trying to achieve a balance between the main zhang et al
aries et al
topic and other topics that may contain some useful information eg


summary informativeness the main goal of a summary is to aord a representative text of the original document
it must cover the essential information using few words
which means the summary must retain more information with less redundancy


summary coverage a summary must cover the most important content of the original document
when we read a summary we must get an idea on what is its original document about or what the document can tell about a specic request
information quantity has been the main focus of intrinsic evaluation methods and evaluation workshops as well
a lot of advance has been made so it can be considered as a minor problem
for example in multiling the systems had recall scores close to the top line system which is a system that cheats in order to generate the summaries


summary conciseness a good summary must not contain redundant information leaving the space for more useful information to be included
most systems reorder sentences using their scores so they can highly score two similar sentences and include them to the summary especially in case of multi document summarization
when generating summaries a system must exclude similar sentences even if they score more than others
a very known work which takes redundancy in consideration is mmr maximal marginal relevance carbonell and goldstein
the authors score each candidate sentence si of a document d which is not already included to the summary s using two scoring similarities sim which calculates the similarity of to a query q and sim which calculates the similarity of to another sentence sj s
the idea is to maximize mmr score given in equation
m m r arg max sim q max sj s sim sj where is a parameter that balances between the relevance and the diversity
the authors use cosine similarity to calculate sim and sim
more complex methods are proposed to nd a trade o between the coverage and the conciseness of the generated summary
for instance nishikawa et al
model text summarization as a knapsack problem to generate a summary with a maximum coverage selecting sentences with as many information units unigrams and bigrams as possible
then they add a constraint to prevent adding a unit that already exists in the summary
the two previous methods try to score the sentences based on their coverage and conciseness in the same time
a more simpler method is to score the sentences using just the coverage then control redundancy while selecting summary ones
in aries et al
cosine similarity and a threshold are used to decide two sentences similarity
the most scored sentence is selected into the summary and each time the next one to be added is tested whether it is similar to the last added one if not it will be included to the summary
redundancy is not just a property of texts it can be found even in videos
bhaumik et al
divide a video into multiple shots from which key frames are extracted
then they remove intra and inter shots redundant frames to form the nal video summary

summary readability extractive summarization methods are based on extracting pertinent units sentences often from the source
the extracted units are used to form the summary which can contain dangling anaphora references and sentences which are ordered badly
abstractive methods seem to handle the anaphora problem and the order of sentences since the generation is from a semantic representation
but the problem is can the generator aord a good grammar with a uent language summary readability has been in future works section for years a future that has never come


reference clarity when we extract summaries some references remain unsolved such as personal pronouns
that way the reader ca nt nd out who or what is involved in the sentence
so to generate a good summary we have to replace the rst mention of a reference in the summary
in bayomi et al
it is found that the introduction of anaphoric resolution can slightly improve the readability of the summary
also the authors conrm that the impact of anaphora resolution ar varies from one domain to another which means it improves some domains summaries more than others
the resulted summary s readability can be enhanced by resolving anaphora but it can even improve the process of summarization itself
orasan and st try to test the eect of ar on the summarization process using the for scoring and three dierent ar methods
the informativeness of generated summaries improves when ar is used before summarization
in the other hand it has no correlation with the performance of the anaphora resolver used to improve the frequency counts
steinberger et al
incorporate ar into a more complex scoring method lsa representation gong and liu to improve its performance even with an imperfect anaphoric resolver
to improve the coherence they propose an algorithm to replace incorrect anaphoric relations in the summary by their respective original expressions in the text which leads to a precision of


coherence most summarization methods are based on extracting relevant sentences and present them as they are
it would be a good idea if these sentences are reordered since they are extracted from dierent parts of a text
when reading the summary the reader has to feel the ow of ideas without just jumping from idea to another
in single document summarization it seems right to present them as ordered in the original text
that is reordering them using their original positions mckeown et al
radev et al
lin and hovy
but sometimes it will be better to move a sentence towards another due to their similarities
methods which use rhetorical relations such as marcu do not suer this problem since they keep the relations between sentences
in multi document summarization sentence reordering is important since the sentences come from dierent sources
in this type of summarization the main topic is the same but the secondary topics are not always the same in each document
moreover sentences in each source document are not ordered in the same ow of ideas
some works tried to solve this problem dierently
one solution is majority ordering barzilay et al
where similar sentences are grouped together into themes
then a graph representing the local order is created where each node represents a theme
two nodes are connected with an arc if in a document one sentence of the rst node s theme is followed by another of the second node s theme
the weight of this arc is the number of documents where this order is observed
to infer the global order of the graph an approximation algorithm is used
other variations of this method can be found in the works of bollegala et al
ji and nie

resources one of the most problematic challenges in ats is the lack of resources
nowadays there are many powerful tools for stemming parsing
compared to the past
nevertheless there is the challenge of nding the adequate ones for a certain problem of summarization
besides annotated corpus for ats can be seen as a challenge as well
in this section we will try to present some freely available resources which can be helpful to researchers


toolkits generally when we create a new method we have to compare it to other existing methods
in marization we can generate summaries with the dierent methods including ours and evaluate them using the earlier presented evaluation metrics
a well known summarizer is which includes baseline summarization algorithms besides others
semantic liu et al
is another system which uses semantic representations to generate abstractive summaries
as example of neural networks in ats we can cite rush et al

aries et al
is a cal extractive summarizer designed for multilingual summarization
palmonari et al
produces a summary of linked data sets which make use of ontologies to describe the semantics of their data
berkeley doc durrett et al
uses machine learning compression and anaphora resolution to generate single document summaries
this list comprises a little sample of the wide ats systems which proves the emergence of openness support among ats systems
summarization methods can be hard to be implemented especially when they are heavily based on complex nlp functionality
nlp methods are more advancing by time such as tokenizers stemmers syntactic parsers
unfortunately many original systems used to test these methods are not available either openly or commercially
there are some languages that lack basic nlp tools such as tokenizers and stemmers
recently there seems to be a more interest on aording open source toolkits to support research among nlp community


corpora many research methods use machine learning to train the system on a labeled corpus and then test it on another one
the problem in many cases is the absence of adequate corpora for ats
text summarization task is aected by the genre of training corpus news novels
which imply the creation of diverse corpora based on the purpose of the system
some researches tried to create methods with semi supervised learning to solve this problem amini and gallinari
to adjust this problem some workshops like multiling propose tasks for summarization corpora creation
also they aord training and evaluation corpora for automatic text summarization

evaluation comparing a generated summary automatically to some man made summaries is good but not enough
the summarization system still can generate a good summary which is not even close to the reference summaries
also using reference summaries is more likely to work with extractive systems better than abstractive ones
this is a brief summary of some challenges people write summaries dierently therefore we can not say this is the best summary for sure
hiring professionals to evaluate automatic summaries can be very expensive in term of coast and time
also a summary can be judged dierently from a person to another due to human variation
to face the problems with human evaluation especially the evaluation time automatic methods can be used
most automatic methods evaluate just the content without the quality aspect
also they are more adequate for extractive summarization methods than the abstractive ones
using words or n grams in a reference summary and comparing them to those of the automatic summary can favorize certain systems over other better ones
automating quality evaluation such as grammar reference clarity and coherence can be very lenging
the challenge is the same as in ats one discussed earlier
this needs more profound linguistic analysis of the summary with accurate and powerful tools
also the methods that can
summarization
com january summ
com summarization january
com facebook namas january
com kariminf allsummarizer january
com siti disco unimib abstat january doc summarizer
com gregdurrett berkeley doc summarizer january detect such aws can be used to x them in the rst place
for example if we can detect matically that a sentence must come earlier than another this can be added to the summarization systems to x the problem
conclusion automatic text summarization continues to gain more importance due to large amount of information nowadays
in this article we reviewed dierent ats classications which can benecial to know in order to create new methods
then we discussed some approaches used to generate the nal summary
after presenting the summarization techniques we must present the evaluation part which is a domain in its own
there are dierent evaluation methods which estimate the capacity of a system to imitate human summarization task
these methods have been used in dierent workshops intended for ats systems evaluation
we tried to capture and discuss some of the challenges which can be the denition of the summary itself informativeness readability availability of resources and evaluation
in general before summarizing we must dene what is the summary for
by classifying our intended method is it request based is it extractive or abstractive
we can dene what could be a good summary in our case
a good summarization method for a specic genre like news could be of no use for novels
as for the approach many factors can dene if we use statistical linguistic centroid
statistical methods are more fast and simple to be designed since they are based on simple techniques such as tf idf
to be more accurate deep linguistic methods have to be used for example tf idf can be boosted by introducing words synonyms
linguistic methods can capture more aspects in the original text such as rhetorical relations semantic relations
but it can be dicult to implement such methods without adequate resources
the absence of resources either tools or corpora is a challenging problem not just for linguistic methods but also for statistical ones
machine learning is often used with both statistical and linguistic approaches to train the system using a labeled corpus
evaluating dierent methods is challenging too since a summary has to be evaluated using many aspects
informativeness is mostly covered by the dierent workshops but what is mostly ignored is the summaries readability
it is an aspect to which recent methods may pay more attention
references harold borko and charles l
bernier
abstracting concepts and methods
academic press london
h
p
luhn
the automatic creation of literature abstracts
ibm j
res
dev
april
issn
doi
rd


url


rd


eduard hovy and chin yew lin
automated text summarization and the summarist system
in proceedings of a shop on held at baltimore maryland october pages
association for computational linguistics
karen sparck jones
automatic summarising factors and directions
in advances in automatic text summarisation
cambridge ma mit press
p
b
baxendale
machine made index for technical literature an experiment
ibm j
res
dev
october
issn

rd


url


rd


h
p
edmundson
new methods in automatic extracting
j
acm april
issn
doi


url
acm



kathleen mckeown and dragomir r
radev
generating summaries of multiple news articles
in proceedings of the annual international acm sigir conference on research and development in information retrieval sigir pages new york ny usa
acm
isbn



url
acm



lawrence h
reeve hyoil han and ari d
brooks
the use of domain specic concepts in biomedical text summarization
information processing management
issn
doi


j
ipm



url
sciencedirect
com science article pii
text summarization
kamal sarkar
using domain knowledge for text summarization in medical domain

atefeh farzindar and guy lapalme
legal text summarization by exploration of the thematic structure and argumentative roles
in stan szpakowicz marie francine moens editor text summarization branches out proceedings of the workshop pages barcelona spain july
association for computational linguistics
c
d
paice
the automatic generation of literature abstracts an approach based on the identication of self indicating phrases
in proceedings of the annual acm conference on research and development in information retrieval sigir pages kent uk uk
butterworth co
isbn
url
acm
org citation


f
canan pembe and tunga gungor
automated query biased and structure preserving text summarization on web ments
in proceedings of the international symposium on innovations in intelligent systems and applications
beaux shari mark anthony hutton and jugal kalita
summarizing microblogs automatically
in human language technologies the annual conference of the north american chapter of the association for computational linguistics pages los angeles california june
association for computational linguistics
url http
aclweb
org anthology
yajuan duan zhumin chen furu wei ming zhou and heung yeung shum
twitter topic summarization by ranking in proceedings of coling pages mumbai india tweets using social inuence and content quality
december
the coling organizing committee
url
aclweb
org anthology
gunhee kim leonid sigal and eric p xing
joint summarization of large scale collections of web images and videos for storyline reconstruction
in ieee conference on computer vision and pattern recognition cvpr
brandon b moon
interactive football summarization

hossam m zawbaa nashwa el bendary aboul ella hassanien and th kim
event detection based approach for soccer video summarization using machine learning
int j multimed ubiquitous eng
aditya khosla raay hamid chih jen lin and neel sundaresan
large scale video summarization using web image priors
in proceedings of the ieee conference on computer vision and pattern recognition pages
danila potapov matthijs douze zaid harchaoui and cordelia schmid
category specic video summarization
in puter vision eccv pages
springer
michael gygli helmut grabner and luc van gool
video summarization by learning submodular mixtures of objectives
in proceedings of the ieee conference on computer vision and pattern recognition pages
jade goldstein gary m
ciany and jaime g
carbonell
genre identication and goal focused summarization
in ceedings of the sixteenth acm conference on conference on information and knowledge management cikm pages new york ny usa
acm
isbn



url
acm



v
a
yatsko m
s
starikov and a
v
butakov
automatic genre recognition and adaptive text summarization
volume pages secaucus nj usa june
springer verlag new york inc


url



jiaming zhan han tong loh and ying liu
gather customer concerns from online product reviews a text summarization approach
expert syst
appl
march
issn

j
eswa



url


j
eswa



natalia vanetik and marina litvak
proceedings of the annual meeting of the special interest group on discourse and dialogue chapter multilingual summarization with polytope model pages
association for computational linguistics
url
org anthology
abdelkrime aries eddine djamel zegour and walid khaled hidouci
proceedings of the annual meeting of the special interest group on discourse and dialogue chapter allsummarizer system at multiling multilingual single and multi document summarization pages
association for computational linguistics
url
org anthology
stefan thomas christian beutenmuller xose de la puente robert remus and stefan bordag
proceedings of the annual meeting of the special interest group on discourse and dialogue chapter exb text summarizer pages
association for computational linguistics
url
org anthology
marta vicente oscar alcon and elena lloret
proceedings of the annual meeting of the special interest group on discourse and dialogue chapter the university of alicante at multiling approach results and further insights pages
association for computational linguistics
url
org anthology
pinaki bhaskar and sivaji bandyopadhyay
a query focused multi document automatic summarization
in paclic pages
sejla cebiric francois goasdoue and ioana manolescu
query oriented summarization of rdf graphs
proc
vldb endow
august
issn



url




sheng hua zhong yan liu bin li and jing long
query oriented unsupervised multi document summarization via deep learning model
expert systems with applications
issn
doi


j
eswa



url
sciencedirect
com science article pii
min yen kan kathleen r
mckeown and judith l
klavans
applying natural language generation to indicative marization
in proceedings of the european workshop on natural language generation volume ewnlg pages stroudsburg pa usa
association for computational linguistics



url




inderjeet mani
summarization evaluation an overview

ruslan mitkov
automatic abstracting in a limited domain
pages
url
handle

rania othman rami belkaroui and rim faiz
customer opinion summarization based on twitter conversations
in proceedings of the international conference on web intelligence mining and semantics wims pages new york ny usa
acm
isbn



url
acm



kalina bontcheva
generating tailored textual summaries from ontologies pages
springer berlin heidelberg isbn


url


berlin heidelberg

ani nenkova and kathleen mckeown
a survey of text summarization techniques pages
springer us boston ma
isbn


url


ani nenkova and kathleen mckeown
automatic summarization
foundations and trends in information retrieval

elena lloret and manuel palomar
text summarisation in progress a literature review
artif
intell
rev
january
issn

z
url


z
g
salton and c
s
yang
on the specication of term values in automatic indexing
journal of documentation
chikashi nobata and satoshi sekine
crl nyu summarization system at
in duc
mohamed abdel fattah and fuji ren
ga mr nn pnn and gmm based models for automatic text summarization
issn

j
csl



url http comput
speech lang
january



j
csl



you ouyang wenjie li qin lu and renxian zhang
a study on position information in document summarization
in proceedings of the international conference on computational linguistics posters number in pages
association for computational linguistics
gerard salton and christopher buckley
term weighting approaches in automatic text retrieval
inf
process
manage
august
issn


url



kai ishikawa shinichi ando and akitoshi okumura
hybrid text summarization method based on the tf method and the lead method
in proceedings of the second ntcir workshop meeting on evaluation of chinese japanese text retrieval and text summarization
julian kupiec jan pedersen and francine chen
a trainable document summarizer
in proceedings of the annual international acm sigir conference on research and development in information retrieval sigir pages new york ny usa
acm
isbn



url
acm



dragomir r
radev hongyan jing stys and daniel tam
centroid based summarization of multiple documents
information processing management
issn
doi


j
ipm



url
sciencedirect
com science article pii
dragomir r
radev hongyan jing and malgorzata budzikowska
centroid based summarization of multiple documents sentence extraction utility based evaluation and user studies
in proceedings of the naacl anlpworkshop on automatic summarization volume naacl anlp autosum pages stroudsburg pa usa
association for computational linguistics



url




rakesh agrawal and ramakrishnan srikant
fast algorithms for mining association rules in large databases
in proceedings of the international conference on very large data bases vldb pages san francisco ca usa
morgan kaufmann publishers inc
isbn
url
acm
org citation


elena baralis luca cagliero saima jabeen and alessandro fiori
multi document summarization exploiting frequent itemsets
in proceedings of the annual acm symposium on applied computing sac pages new york ny usa
acm
isbn



url
acm



elena baralis luca cagliero alessandro fiori and paolo garza
mwi sum a multilingual summarizer based on frequent weighted itemsets
acm trans
inf
syst
september
issn


url
acm


marina litvak and natalia vanetik
query based summarization using mdl principle
in proceedings of the multiling workshop on summarization and summary evaluation across source types and genres pages valencia spain april
association for computational linguistics
url
aclweb
org anthology
jilles vreeken matthijs van leeuwen and arno siebes
krimp mining itemsets that compress
data mining and knowledge discovery jul
issn


url


josef steinberger and karel jezek
using latent semantic analysis in text summarization and summary evaluation
in proc
pages
guy feigenblat haggai roitman odellia boni and david konopnicki
unsupervised query focused multi document marization using the cross entropy method
in proceedings of the international acm sigir conference on research and development in information retrieval sigir pages new york ny usa
acm
isbn



url
acm



reuven y
rubinstein and dirk p
kroese
the cross entropy method a unied approach to combinatorial optimization monte carlo simulation information science and statistics
springer verlag new york inc
secaucus nj usa
isbn
gerard salton amit singhal mandar mitra and chris buckley
automatic text structuring and summarization
inf
issn


url http process
manage
march




r
ferreira f
freitas l
d
s
cabral r
d
lins r
lima g
frana s
j
simske and l
favaro
a context based text summarization system
in iapr international workshop on document analysis systems pages april

das


gunes erkan and dragomir r
radev
lexrank graph based lexical centrality as salience in text summarization
journal of articial intelligence research december
issn
url
acm
org citation


rada mihalcea and paul tarau
textrank bringing order into texts
in dekang lin and dekai wu editors proceedings of emnlp pages barcelona spain july
association for computational linguistics
url http
aclweb
org anthology
s
brin and l
page
the anatomy of a large scale hypertextual web search engine
in seventh international world wide web conference www
url
stanford

wenjie li mingli wu qin lu wei xu and chunfa yuan
extractive summarization using and event relevance
in proceedings of the international conference on computational linguistics and the annual meeting of the association for computational linguistics pages stroudsburg pa usa
association for computational linguistics



url



xiaojun wan
timedtextrank adding the temporal dimension to multi document summarization
in proceedings of the annual international acm sigir conference on research and development in information retrieval sigir pages new york ny usa
acm
isbn



url
acm



xiaojun wan
an exploration of document impact on graph based multi document summarization
in proceedings of the conference on empirical methods in natural language processing emnlp pages stroudsburg pa usa
association for computational linguistics
url
acm
org citation


jon m
kleinberg
authoritative sources in a hyperlinked environment
j
acm september
issn



url
acm



jen yuan yeh hao ren ke and wei pang yang
ispreadrank ranking sentences for extraction based summarization using feature weight propagation in the sentence similarity network
expert systems with applications
issn


j
eswa



url
sciencedirect
science article pii
m
ross quillian
semantic memory pages
the mit press
y
liu x
wang j
zhang and h
xu
personalized pagerank based multi document summarization
in ieee international workshop on semantic computing and systems pages july

wscs


constantin orasan and staord st
pronominal anaphora resolution for text summarisation
of proceedings of the recent advances in natural language processing pages
george a
miller
wordnet a lexical database for english
commun
acm november
issn
doi


url
acm



leonhard hennig winfried umbrath and robert wetzker
an ontology based approach to text summarization
in ceedings of the ieee wic acm international conference on web intelligence and intelligent agent technology volume wi iat pages washington dc usa
ieee computer society
isbn

wiiat


url


wiiat


kenji ono kazuo sumita and seiji miike
abstract generation based on rhetorical structure extraction
in proceedings of the conference on computational linguistics volume coling pages stroudsburg pa usa
association for computational linguistics



url




daniel marcu
improving summarization through rhetorical parsing tuning
in in the sixth workshop on very large corpora pages
yuta kikuchi tsutomu hirao hiroya takamura manabu okumura and masaaki nagata
single document summarization in proceedings of the annual meeting of the association for computational based on nested tree structure
linguistics volume short papers pages baltimore maryland june
association for computational linguistics
url
aclweb
org anthology
naman goyal and jacob eisenstein
a joint model of rhetorical discourse structure and summarization
in proceedings of the workshop on structured prediction for nlp pages austin tx november
association for computational linguistics
url
org anthology
michael l
wick khashayar rohanimanesh kedar bellare aron culotta and andrew mccallum
samplerank training factor graphs with atomic gradients
in proceedings of the international conference on machine learning icml bellevue washington usa june july pages
miles osborne
using maximum entropy for sentence extraction
in proceedings of the workshop on automatic summarization volume as pages stroudsburg pa usa
association for computational linguistics



url




jen yuan yeh hao ren ke wei pang yang and i heng meng
text summarization using a trainable summarizer and latent semantic analysis
inf
process
manage
january
issn

j
ipm



url


j
ipm



massih reza amini and patrick gallinari
the use of unlabeled data to improve supervised learning for text summarization
in proceedings of the annual international acm sigir conference on research and development in information retrieval sigir pages new york ny usa
acm
isbn



url
acm



kam fai wong mingli wu and wenjie li
extractive summarization using supervised and semi supervised learning
in proceedings of the international conference on computational linguistics volume coling pages isbn
url stroudsburg pa usa
association for computational linguistics

acm
org citation


jacob eisenstein and regina barzilay
bayesian unsupervised topic segmentation
in proceedings of the conference on pirical methods in natural language processing emnlp pages stroudsburg pa usa
association for computational linguistics
url
acm
org citation


hal daume iii and daniel marcu
bayesian query focused summarization
in proceedings of the international ference on computational linguistics and annual meeting of the association for computational linguistics pages sydney australia july
association for computational linguistics



url
aclweb
org anthology
thomas minka
a family of algorithms for approximate bayesian inference
asli celikyilmaz and dilek hakkani tur
discovery of topically coherent sentences for extractive summarization
in proceedings of the annual meeting of the association for computational linguistics human language technologies volume hlt pages stroudsburg pa usa
association for computational linguistics
isbn
url
acm
org citation


guangbing yang dunwei wen kinshuk nian shing chen and erkki sutinen
a novel contextual topic model for document summarization
expert syst
appl
february
issn

j
eswa



url


j
eswa



david m
blei thomas l
griths and michael i
jordan
the nested chinese restaurant process and bayesian ric inference of topic hierarchies
j
acm february
issn



url
acm



xuerui wang andrew mccallum and xing wei
topical n grams phrase and topic discovery with an application to information retrieval
in proceedings of the seventh ieee international conference on data mining icdm pages washington dc usa
ieee computer society
isbn

icdm


url

icdm


seonggi ryang and takeshi abekawa
framework of automatic text summarization using reinforcement learning
in ceedings of the joint conference on empirical methods in natural language processing and computational natural language learning emnlp conll pages stroudsburg pa usa
association for computational linguistics
url
acm
org citation


stefan hen margot mieskes and iryna gurevych
a reinforcement learning approach for adaptive single and document summarization
in gscl pages
cody rioux sadid a
hasan and yllias chali
fear the reaper a system for automatic multi document summarization with reinforcement learning
in proceedings of the conference on empirical methods in natural language processing emnlp pages doha qatar october
association for computational linguistics
url
aclweb
org anthology
alexander m
rush sumit chopra and jason weston
a neural attention model for abstractive sentence summarization
in proceedings of the conference on empirical methods in natural language processing pages lisbon portugal september
association for computational linguistics
url
org anthology
ramesh nallapati bowen zhou cicero dos santos caglar gulcehre and bing xiang
abstractive text summarization using in proceedings of the signll conference on computational natural sequence to sequence rnns and beyond
language learning pages berlin germany august
association for computational linguistics
url
aclweb
org anthology
sebastien jean kyunghyun cho roland memisevic and yoshua bengio
on using very large target vocabulary for neural machine translation
in proceedings of the annual meeting of the association for computational linguistics and the international joint conference on natural language processing volume long papers pages beijing china july
association for computational linguistics
url
aclweb
org anthology
thang luong ilya sutskever quoc le oriol vinyals and wojciech zaremba
addressing the rare word problem in neural machine translation
in proceedings of the annual meeting of the association for computational linguistics and the international joint conference on natural language processing volume long papers pages beijing china july
association for computational linguistics
url
aclweb
org anthology
oriol vinyals meire fortunato and navdeep jaitly
pointer networks
in advances in neural information processing systems pages
jerey ling and alexander rush
coarse attention models for document summarization
in proceedings of the workshop on new frontiers in summarization pages copenhagen denmark september
association for computational linguistics
url
aclweb
org anthology
yan liu sheng hua zhong and wenjie li
query oriented multi document summarization via unsupervised deep learning
in proceedings of the twenty sixth aaai conference on articial intelligence july toronto ontario canada
aaai press
url
aaai
org ocs index
php aaai paper
misha denil alban demiraj and nando de freitas
extraction of salient sentences from labelled documents
corr

url
org

ziqiang cao furu wei li dong sujian li and ming zhou
ranking with recursive neural networks and its tion to multi document summarization
in proceedings of the twenty ninth aaai conference on articial gence pages
aaai press
isbn
url
acm
org citation


mahmood and len hamey
text summarization using unsupervised deep learning
expert syst
appl
february
issn

j
eswa



url

j
eswa



pengjie ren zhumin chen zhaochun ren furu wei jun ma and maarten de rijke
leveraging contextual sentence relations for extractive summarization using a neural attention model
in proceedings of the international acm sigir conference on research and development in information retrieval sigir pages new york ny usa
acm
isbn



url
acm



hongyan jing and kathleen r
mckeown
the decomposition of human written summary sentences
in proceedings of the annual international acm sigir conference on research and development in information retrieval sigir pages new york ny usa
acm
isbn



url http
acm



kevin knight and daniel marcu
summarization beyond sentence extraction a probabilistic approach to sentence issn


url compression
artif
intell
july




david zajic bonnie j
dorr jimmy lin and richard schwartz
multi candidate reduction sentence compression as a tool for document summarization tasks
inf
process
manage
november
issn
doi
j
ipm



url


j
ipm



trevor cohn and mirella lapata
sentence compression beyond word deletion
in proceedings of the international ference on computational linguistics volume coling pages stroudsburg pa usa
association for computational linguistics
isbn
url
acm
org citation


katja filippova
multi sentence compression nding shortest paths in word graphs
in proceedings of the international conference on computational linguistics coling pages stroudsburg pa usa
association for computational linguistics
url
acm
org citation


katja filippova enrique alfonseca carlos a
colmenares lukasz kaiser and oriol vinyals
sentence compression by deletion with lstms
in proceedings of the conference on empirical methods in natural language processing pages lisbon portugal september
association for computational linguistics
url
anthology
shun hasegawa yuta kikuchi hiroya takamura and manabu okumura
japanese sentence compression with a large training dataset
in proceedings of the annual meeting of the association for computational linguistics volume short papers pages vancouver canada july
association for computational linguistics
url
org anthology
josef steinberger and karel jezek
evaluation measures for text summarization
computing and informatics
url
cai
sk ojs index
php cai article
chin yew lin and eduard hovy
automatic evaluation of summaries using n gram co occurrence statistics
in proceedings of the conference of the north american chapter of the association for computational linguistics on human language technology volume naacl pages stroudsburg pa usa
association for computational linguistics



url




kishore papineni salim roukos todd ward and wei jing zhu
bleu a method for automatic evaluation of machine translation
in proceedings of the annual meeting on association for computational linguistics acl pages stroudsburg pa usa
association for computational linguistics



url




chin yew lin
rouge a package for automatic evaluation of summaries
in proc
acl workshop on text summarization branches out page
url
microsoft
download papers
pdf
ani nenkova rebecca passonneau and kathleen mckeown
the pyramid method incorporating human content selection issn
doi variation in summarization evaluation
acm trans
speech lang
process
may



url
acm



eduard hovy chin yew lin liang zhou and junichi fukumoto
automated summarization evaluation with basic elements
in proceedings of the fifth conference on language resources and evaluation lrec pages
george a
miller
wordnet a lexical database for english
commun
acm november
issn
doi


url
acm



george giannakopoulos vangelis karkaletsis george vouros and panagiotis stamatopoulos
summarization system uation revisited n gram graphs
acm trans
speech lang
process
october
issn
doi


url
acm



george giannakopoulos and vangelis karkaletsis
summarization system evaluation variations based on n gram graphs
in proceedings of the text analysis conference tac
george giannakopoulos and vangelis karkaletsis
summary evaluation together we stand npower ed pages
springer berlin heidelberg berlin heidelberg
isbn


url



john m conroy judith d schlesinger and dianne p oleary
nouveau rouge a novelty metric for update summarization
computational linguistics
javed a aslam matthew ekstrand abueg virgil pavlu fernando diaz and tetsuya sakai
trec temporal rization
in trec
inderjeet mani david house gary klein lynette hirschman therese firmin and beth sundheim
the tipster summac text summarization evaluation
in proceedings of the ninth conference on european chapter of the association for putational linguistics eacl pages stroudsburg pa usa
association for computational linguistics



url




bonnie dorr christof monz stacy president richard schwartz and david zajic
a methodology for extrinsic evaluation of text summarization does rouge correlate in in proceedings of the acl ann arbor pages
hoa trang dang and karolina owczarzak
overview of the tac update summarization task
in proceedings of text analysis conference tac
takahiro fukusima and manabu okumura
text summarization challenge text summarization evaluation at ntcir
in proceedings of ntcir workshop pages
g
giannakopoulos m
el haj b
favre m
litvak j
steinberger and v
varma
tac multiling pilot overview
in proceedings of the fourth text analysis conference tac pilot track
gaithersburg maryland usa
george giannakopoulos
multi document multilingual summarization and evaluation tracks in acl multiling workshop
in proceedings of the multiling workshop on multilingual multi document summarization pages soa bulgaria august
association for computational linguistics
url
aclweb
org anthology
je kubina john conroy and judith schlesinger
acl multiling pilot overview
in proceedings of the multiling workshop on multilingual multi document summarization pages soa bulgaria august
association for computational linguistics
url
aclweb
org anthology
benoit favre evgeny stepanov jeremy trione bechet and giuseppe riccardi
proceedings of the annual meeting of the special interest group on discourse and dialogue chapter call centre conversation summarization a pilot task at multiling pages
association for computational linguistics
mijail kabadjov josef steinberger emma barker udo kruschwitz and massimo poesio
onforums the shared task on online forum summarisation at
in proceedings of the forum for information retrieval evaluation pages
acm
xin qian jimmy lin and adam roegiest
interleaved evaluation for retrospective summarization and prospective tication on document streams
in proceedings of the international acm sigir conference on research and development in information retrieval sigir pages new york ny usa
acm
isbn



url
acm



wei song lim cheon choi soon cheol park and xiao feng ding
fuzzy evolutionary optimization modeling and its applications to unsupervised categorization and extractive summarization
expert syst
appl
august
issn

j
eswa



url


j
eswa



zhengchen zhang shuzhi sam ge and hongsheng he
mutual reinforcement document summarization using embedded graph based sentence clustering for storytelling
inf
process
manage
july
issn
doi
j
ipm



url


j
ipm



abdelkrime aries houda oufaida and omar nouali
using clustering and a modied classication algorithm for automatic text summarization
volume of proc
spie pages



url http




jaime carbonell and jade goldstein
the use of mmr diversity based reranking for reordering documents and producing summaries
in proceedings of the annual international acm sigir conference on research and development in information retrieval pages
acm
hitoshi nishikawa tsutomu hirao toshiro makino and yoshihiro matsuo
text summarization model based on in proceedings of coling posters pages mumbai redundancy constrained knapsack problem
dia december
the coling organizing committee
url
aclweb
org anthology
hrishikesh bhaumik siddhartha bhattacharyya and susanta chakraborty
redundancy elimination in video isbn

marization pages
springer international publishing cham

url



mostafa bayomi killian levacher m
rami ghorab peter lavin alexander oconnor and seamus lawless
towards evaluating the impact of anaphora resolution on text summarisation from a human perspective pages
isbn


url springer international publishing cham




josef steinberger massimo poesio mijail a
kabadjov and karel jezek
two uses of anaphora resolution in summarization
information processing management
issn
doi


j
ipm



url
sciencedirect
com science article pii
text summarization
yihong gong and xin liu
generic text summarization using relevance measure and latent semantic analysis
in proceedings of the annual international acm sigir conference on research and development in information retrieval sigir pages new york ny usa
acm
isbn



url
acm



kathleen r
mckeown judith l
klavans vasileios hatzivassiloglou regina barzilay and eleazar eskin
towards document summarization by reformulation progress and prospects
in proceedings of the sixteenth national conference on articial intelligence and the eleventh innovative applications of articial intelligence conference innovative cations of articial intelligence aaai iaai pages menlo park ca usa
american association for articial intelligence
isbn
url
acm
org citation


chin yew lin and eduard hovy
from single to multi document summarization a prototype system and its evaluation
in proceedings of the annual meeting on association for computational linguistics acl pages stroudsburg pa usa
association for computational linguistics



url http




regina barzilay noemie elhadad and kathleen r
mckeown
inferring strategies for sentence ordering in multidocument news summarization
j
artif
int
res
august
issn
url
acm
org citation


danushka bollegala naoaki okazaki and mitsuru ishizuka
a machine learning approach to sentence ordering for tidocument summarization and its evaluation
in proceedings of the second international joint conference on natural language processing pages berlin heidelberg
springer verlag
isbn


url



donghong ji and yu nie
sentence ordering based on cluster adjacency in multi document summarization
in proceedings of the third international joint conference on natural language processing volume
fei liu jerey flanigan sam thomson norman m
sadeh and noah a
smith
toward abstractive summarization using semantic representations
in rada mihalcea joyce yue chai and anoop sarkar editors hlt naacl pages
the association for computational linguistics
isbn
url
uni trier
conf naacl

matteo palmonari anisa rula riccardo porrini andrea maurino blerina spahiu and vincenzo ferme
abstat linked data summaries with abstraction and statistics pages
springer international publishing cham
isbn


url



greg durrett taylor berg kirkpatrick and dan klein
learning based single document summarization with compression and anaphoricity constraints
in proceedings of the annual meeting of the association for computational linguistics volume long papers berlin germany august
association for computational linguistics

