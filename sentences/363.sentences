c e d l c
s c v
v i x r a on device tag generation for unstructured text manish chugani on device ai samsung institute bangalore india m

com shubham vatsal on device ai samsung institute bangalore india shubham

com gopi ramena on device ai samsung institute bangalore india gopi

com sukumar moharana on device ai samsung institute bangalore india
com naresh purre on device ai samsung institute bangalore india naresh

com abstract with the overwhelming transition to smart phones storing important information in the form of unstructured text has become habitual to users of mobile devices
from grocery lists to drafts of emails and important speeches users store a lot of data in the form of unstructured text for eg in the notes application on their devices leading to cluttering of data
this not only prevents users from efcient navigation in the applications but also precludes them from perceiving the relations that could be present across data in those applications
this paper proposes a novel pipeline to generate a set of tags using world knowledge based on the keywords and concepts present in unstructured textual data
these tags can then be used to summarize categorize or search for the desired information thus enhancing user experience by allowing them to have a holistic outlook of the kind of information stored in the form of unstructured text
in the proposed system we use an on device mobile phone efcient cnn model with pruned conceptnet resource to achieve our goal
the architecture also presents a novel ranking algorithm to extract the top n tags from any given text
index terms abstractive summary keyword extraction text analysis deep tag ranking device concept extraction learning i
introduction data can be broadly categorized into two categories i
e
structured data and unstructured data
structured data can be dened as text which consists of certain patterns and is highly organized
since structured data has a dened outline and framework machines can search and navigate through it with ease
example of such data would be nance account number date formats
unstructured data as the name suggests although present in abundance is very difcult to process as it does not conform to given set of rules
examples of such data would be product reviews on e commerce emails
structured data analysis has become a mature industry today
analysis of unstructured data which comprises of of enterprise data is where the actual challenge lies and the latest trend concentrates on exploiting this resource
unstructured text contains huge amounts of unrelated and diverse tion with no framework or outline for machines to be able to identify any patterns or structure in order to locate the said information
as far as unstructured text on mobile devices is concerned it turns out that users store even more random information in the form of such text for e

passwords otps blog texts to do lists emails drafts for speeches
this results in data of manifold nature with varied forms and lengths of text
our proposed system draws on knowledge of concepts encoded in a hierarchical common sense knowledge database known as to provide enhanced tag extraction capabilities
our approach uses deep learning to provide abstractive extraction of concepts by using knowledge graph embeddings to extract tags from keywords while ensuring device efciency by keeping the entire pipeline ally inexpensive
before using the knowledge graph cnn we also use part of speech pos to extract words which are nouns and proper nouns which are further fed as input to our model
apart from these we have also proposed a custom ranking algorithm to extract the top n tags generated from the given data
the remaining part of the paper is organized in the following manner section ii talks about the related works and how our work differs from them section iii describes the overall pipeline model and the techniques employed section iv talks about the datasets used to either evaluate the performance of this pipeline or used as a part of this pipeline section v provides the experiments conducted section vi talks about the methods with which our pipeline has been compared to section vii shows the results obtained after experimentation section viii talks about the applications of this pipeline in real world scenarios and section ix nally concludes the paper and lists down some improvements which could be researched in future

fig

proposed system ii
related work keyword extraction is an important task in the area of text mining
extracting a small set of keywords from a text or document can help in various tasks in understanding the document
none of the prior works to the best of our knowledge have shared results on user notes application which is one of the most prominent sources of unstructured text on device
tionally our work targets predicting results with an entirely on device pipeline
this we considered necessary so that the user privacy is maintained by not uploading his personal data on any server
several previous works have approached keyword extraction from short text using various statistical approaches such as idf or bag of words on features extracted from text
many of these works focus in methods of selecting alternative input features
these approaches mostly rely on word frequencies and the keywords extracted are not always relevant to the user
furnkranz et al
uses all noun phrases matching any of a number of syntactic heuristics as features
aizawa extracts pos entities by matching pre dened patterns
their representation shows a small improvement in results
in these works it is unclear how many keywords are extracted
witten et al
use a key phrase extraction algorithm called kea based on naive bayes algorithm
their algorithm learns a model for identifying extracted keywords during training which is then applied to nding keywords from new documents
tang et al
also apply bayesian decision theory for keyword extraction using word linkage information and thus using context features
however these methods limit themselves to extracting keywords present in the text and can not extract keywords or tags based on the present in the text
another interesting approach is depicted in sahlgren and coster where they compute a concept based sentation from word co occurrence data which is combined with full text representation
they show that this combination improved performance for their task of text categorization
some other approaches also make use of text tion methods to nd sentences containing relevant keywords
fig

bi lstm crf with character and word glove embeddings then they use a scoring mechanism to give these sentences higher weight in their feature vectors
in this paper we propose a fast novel system for on device extraction of keywords and generation of tags for unstructured text which generates tags from entities and concepts present in the text and ranks those in order to enhance user experience
iii
proposed system fig
shows the pipeline of the proposed system
as we can see an unstructured text is sent as input to a pos tagger from which a set of entities are extracted
depending on whether those set of entities are present in the knowledge base or not a set of similar entities is obtained
finally these set of entities are passed to a graph cnn model to extract the relevant tags in the form of keywords and concepts
once these tags have been extracted it is passed to a custom ranking method which reorganizes these set of tags on the basis of their priority
the in depth details of each component of the pipeline are mentioned in the coming sub sections
a
part of speech tagging for building a pos model a model similar to lample al
and ma and hovy is used
firstly a bi lstm layer is trained to get character embeddings from the train data
this gives a character based representation of each word
next this is concatenated with standard glove dimension vectors trained on billion corpus of wikipedia and word vector representation
this gives us the contextual representation of each word
then a bi lstm is run on each sentence represented by the above contextual representation
this nal output from the model is decoded with a linear chain crf using viterbi algorithm
for on device inference the viterbi decode algorithm is implemented in java to be run on android devices and get the nal output
the model is quantized to reduce its size and make it feasible for on device requirements
we used the tagged dataset from the shared task for training of the above neural network
the model uses word embeddings of dimension character embeddings of dimension and has lstm units
the nal pos model used on device had an accuracy of
on the test dataset
when an input text is passed to the pos model the extracted proper nouns are added to the nal set of tags
the verbs are lemmatized and passed alongside the nouns to the neural network for inferencing concepts from the commonsense based knowledge graph
fig
shows the architecture of the pos model used
b
cnn based knowledge graph learning our approach uses a cnn based knowledge graph model as explained in feipeng zhao et al
in this architecture both embedding and cnn based score function are unknown
the model develops the entity and relation embeddings whilst also learning the knowledge graph structure using triplets h r t where h and t are the head and tail entities and r is the relationship between them
given any h r t the three embedding vectors are stacked over one another and kernels are used for convolution over the combined matrix of size embedding dimension
cnns when applied on images go through rows and columns on the image pixels but in our case they go over the locally connected structure of the head relation and tail together
the cnn output is then passed to the max pooling layer to get subsamples
the max pooling lter size is set to with stride as
dropout is added for regularization
the dropout probability is set to
during training
the nal layer of the network is a logistic regression layer
positive correct triplets have score and negative incorrect triplets have score
the nal negative score is a tanh activation of the regression layer
the loss function is given by the formula h r h r t r t r where h is the head entity r is the relation t is the tail entity h is the corrupted head entity t is the corrupted tail entity s is the set of golden triplets s is the set of corrupted triplets is the margin hyperparameter of the network r t is the score of a golden triplet and t is the score of a corrupted triplet
mini batch stochastic gradient descent is used as an mizer for the loss function
also we require negative sampling in order to calculate the score for each positive triplet
the embedding and cnn parameters are initialized with random values
training is xed at a certain number of epochs based on the size of dataset used
the architecture is shown in fig
the training data knowledge graph provided to this model is ltered from the vast conceptnet dataset as explained in section iv
our knowledge graph contains triplets of a summarizing nature and is specically ltered for this task of generating concepts from unstructured text
other methods use standard datasets for training and validation sets but this task required the creation of a hierarchical knowledge graph which we split in a ratio during the model training phase
the purpose of using a knowledge graph to generate tags is to ensure that the approach is not conned to the input text
the knowledge graph facilitates real world knowledge being applied to the extraction process to emulate human behaviour when trying to understand the same input text
another reason for using a cnn based learning method is that this pipeline was designed for on device inference where such models are feasible and efcient
c
entity similarity module due to the architecture being deployed on device the constraints of model size and inference time are strict
this results in restrictions on how deep the cnn architecture can be since heavy model sizes prevent on device deployment
this necessitated the use of glove embeddings to nd similar words to entities outside the knowledge graph in order to be able to incorporate such entities
if an entity outside the vocabulary of the knowledge graph is encountered we extract words similar to the entity in question using cosine similarity
table ii in section v c shows the various on device metrics of the models that have been experimented with and lishes the need for this alternative approach to incorporating large knowledge graphs on device
d
concept selection module when a word from a given text is passed through net it gives a number of concepts corresponding to that word
for example when we pass the word car through conceptnet we get concepts such as artifact tool vehicle item machine
most of these concepts are generally irrelevant with respect to the general context of the text
hence in order to choose the most appropriate concept we calculate context factor
if ci represents a concept from set of extracted concepts for a word wj represents an output of pos represents term frequency of word wj and n represents the length of text then context factor can be dened as tf wj ci n j where cosinesim is cosine similarity between word wj and concept ci is calulcated using glove embeddings
we choose the concept ci with maximum lcontx value as the most appropriate concept for a given word wi
this context factor helps us in analysing the general context of the entire text while selecting a concept for a word
for example consider the text typically the driver is responsible for all damage to the car during the tenure of the lease even if they fig

architecture of the knowledge graph cnn are not at fault
in this text is maximum with value
for the word car
but for the sentence machine was a very popular term in my family but car was the rst machine that actually caught my imagination and i can safely say that it is my favorite machine till date
is maximum with value
and hence becomes the extracted concept for the word car
e
ranking of tags there can be a scenario that for considerably long tured text we may end up extracting large number of tags say up to tags which can in turn prove to be another form of clutter for the user
hence in order to enhance user experience it is utterly important to rank and select only a handful of extracted tags for any given text
in this section we present a custom ranking algorithm and later we also present evidence in the form of results obtained on various datasets as a justication for the hypothesis on which this algorithm is based on
the hypothesis on which the algorithm is based is that if the tag generating word is found in the vicinity of a large number of other tag generating words for an input text it will be given a higher priority while ranking the tags
a tag generating word is simply a word from which a tag is extracted
on the basis of our hypothesis we calculate a ranking factor rf given by the equation rftj ti j wj f wi f wj where wj is the co occurrence of words from which tags ti and have been extracted for each sentence and are the frequencies of words wi and wj in the unstructured text and is the average number of words occurring in the unstructured text between wi and wj plus
rftj ti is the ranking factor of tag ti with respect to tag tj
the tags are then ranked in descending order of rf values
table i table for ranking matrix change vehicle contract responsibility payment change




vehicle




contract




responsibility



payment



in our custom ranking method the co occurrence value in the equation is determined by calculating the number of sentences in which both words wi and wj are found
the frequency for words wi and wj are calculated by taking the complete unstructured text into consideration
another factor is present which adds extra weightage to the extracted tags
this factor accounts for the distance between the words that generate tags ti and tj from the knowledge graph embeddings
the distance measure can be dened as the number of words between the words in the unstructured text from which tags ti and tj are generated
since our hypothesis is based on giving highest priority to a tag which occurs in the neighborhood of most of the other tags this factor helps in achieving the same
here is a small example explaining the working of this gorithm
consider the note typically the driver is responsible for all damage to the car during the tenure of the lease even if they are not at fault
your own insurance may apply to pay for the damage
also the credit card you used to pay for the lease may have supplemental insurance for damage to the car
after this text is passed through our pipeline the tags extracted are responsibility contract payment vehicle and change
for visualization we construct a ranking matrix calculating the relatedness of these extracted tags as shown in table i
finally considering the values in the ranking matrix the pairs wi ti are ranked as leasecontract carvehicle damagechange paypayment faultresponsibility
here wi is the word in the input text and ti is the extracted tag
iv
datasets the dataset used for training the convolutional neural knowledge graph learning model is conceptnet
the conceptnet knowledge graph contains r t from various languages with a huge variety of concepts
due to on device constraints the entire conceptnet dataset is too vast to be inferred from
as a result of which we created our own pruned conceptnet dataset
we used a set of rules in order to nally arrive at our ltered conceptnet the rst lter we added to select a smaller set of data was to only select those triplets that were in the english language
another selection technique we used was to select relationships r in h r t that were such that the head entity is a superset or parent of the tail entity
in order to ensure that the tags extracted from unstructured textual data are of a summarizing nature we added this constraint
the relations we used to extract the triplets were isa derivedfrom instanceof and partof
other relations in the knowledge graph that were of a slightly less summarizing nature were ambiguous and were dropped
the conceptnet knowledge graph also incorporated some dbpedia relations that were ltered out since they were not that relevant with respect to our work
this narrows down the dataset to a few hundred thousand triplets
but this is still too vast to be inferred from an on device perspective due to the model being around mb after quantization and compression
therefore we decided to manually select a smaller dataset of most commonly used and relevant concepts from the knowledge graph
this results in a dataset of around k triplets which reduced the model size to mb after quantization and compression
apart from the dataset used for training the graph cnn we have used open source datasets of amazon and enron for benchmarking our proposed system
we also used a dataset of user notes application to evaluate the feasibility of the proposed pipeline
the amazon review dataset consists of short and long texts of user reviews on various shopping categories
the enron email dataset contains emails generated by employees of the enron corporation
the notes application dataset consists of notes of variable lengths ranging from short to do lists to lengthy email drafts
v
experiments a
evaluation metric for quality of tags since the tags extracted from our text contain mostly of concepts which are not exact same words present in our text we can not use gold standard datasets to compare our method
another comparison method involves annotators judging the most appropriate tags for a given piece of text but this ends up incorporating a bias towards the authors own methodology and we clearly wanted to avoid that
inspired by bellaachia et al
we introduce a new way to compare the quality of tags generated by various methods
we use volume of google search query to get an idea about the popularity of a tag extracted
the rationale behind using this approach is that if a keyword is more frequently used by the masses it must have more signicance while representing a piece of text
on an average our method generated tags per test sample in the datasets mentioned in the above section
we randomly selected tags extracted by our method
we then sorted them according to their popularity and compared their search volumes one on one with that of random tags extracted from top tags generated by the given methods
for comparison purposes we made sure we were not comparing proper nouns which would be nothing but some entity names
we use word to get the volume of extracted keywords and thereafter go ahead with comparisons
let tcorrect be the number of keywords for a given method which has more popularity then keywords extracted by other methods and textracted be the total number of keywords extracted which in our case is for each sample text precision can be dened as p recision tcorrect textracted the comparison results of our pipeline with respect to methods discussed in the above section are shown in table iv b
evaluation metric for quality of tags we again use the volume of google search query for the extracted tags as a measure to rank them
if a keyword or tag is more widely searched on the internet it s word occurrence factor on which most of the ranking algorithms are based must be of high signicance for any given piece of text
as discussed in bellaachia et al
we use binary preference for calculating rank of extracted keywords
the binary preference measure or bpm can be calculated as bp m tt ranked higher than where t is the set of correct tags within the set m of tags extracted by a method and t is a correct tag and n is an incorrect tag
c
model parameters our graph cnn model uses adam to optimize and learn all the parameters
in our model we can set the width of convolutional kernels with different size for simplicity we xed the kernel size as
when using pairwise ranking loss to learn cnn we xed the margin value as
the learning rate in our model is xed as

epoch number is set as for conceptnet dataset of k triplets
we use the negative sampling method as explained in section iii b
the batch size
github
io amazon index

kaggle
com wcukierski enron email dataset
wordtracker
table ii graph cnn model metrics no
of entities in knowledge graph no
of triplets model size parameters mb mb mb number of nodes in final layer number of entities in knowledge graph size of fully connected layer half the size of pooling layer two convolutional layers table iii entity similarity module impact metrics dataset out of vocabulary entities per test sample amazon reviews enron emails notes


average length of each sample no
of words of triplets for mini batch stochastic gradient descent is set to
the embedding dimension is set to
the dissimilarity distance measure used is the norm
the evaluation triplet size is set to
the number of lters used for convolution is set to
the dropout keep probability is set to

the on device metrics for different graph cnn models while experimenting in terms of number of triplets are listed in table ii
model size and vocabulary length are essential metrics that need to be taken into consideration when ing the model on mobile devices
as we can clearly see from table ii the size of graph cnn model trained with entities is around mb which is not at all feasible from on device perspectives
this is the reason we went ahead with lightweight model along the entity similarity module
as mentioned in section iii c for developing a set of similar entities in order to deal with entities outside the knowledge graph we optimally chose a similarity score threshold of
based on trial and error
table iii showcases the effectiveness of our entity similarity module
it shows the average number of entities detected outside knowledge graph across all our chosen datasets
vi
methods for comparison we used the following methods for comparison with our proposed system a
topic modelling using latent dirichlet allocation latent dirichlet is a generative tistical model used in natural language processing
in topic modelling it explains topics by using unobserved clusters of words which explain reasons behind some parts of data being similar
it is an unsupervised learning model that clusters similar groups of observations
it posits that each document is a mixture of a small number of topics concepts and that each observation s presence is attributable to one of topics of that specic document
for our comparisons we set the number of topics to and extract the top relevant keywords representing that topic
b
automatic summarization using text rank algorithm automatic summarization is the process of computational reduction shortening of data in order to create a synopsis containing highly relevant and important information whilst abstracting the unnecessary aspects of the larger data
for example nding the most informative sentences from a news article the most representative images from a collection of images or even the most important frames in a video fall under the umbrella of automatic summarization
text is an unsupervised approach to tomatic summarization of text
it is a graph based ranking algorithm used in natural language processing
we use the default parameters for candidate parts of speech and case of input text and a window size of
c
rapid automatic keyword language processing
it rake is a popular keyword extraction technique in natural involves using lists of stopwords and phrase delimiters to extract the most relevant keywords in textual data
python implementation of rake in the rake nltk library was used with default parameters for comparison experiments
common methods such as tf idf term frequency inverse document frequency or bag of word models have not been compared with due to the length of the input texts being relatively shorter
generating an appropriate idf score or vocabulary for comparison would require a substantial amount of relevant text
therefore taking into account the average length of input texts in our specic case we choose to not compare with such methods
vii
results tags are extracted by the model on a set of test samples across different datasets and the evaluation metrics mentioned in section iv are used to calculate results
the precision and bpm of the conducted experiments are shown in table iv and the on device inference times and model sizes are shown in table v
the on device metrics have been calculated using samsung s galaxy with gb ram and a
ghz octa core processor
the results clearly show an improvement in both precision and bpm on the serving data and give a quantitative tive to the outcomes of our proposed approach
apart from these results our proposed system demonstrates efciency with respect to device based computational tions
our entire pipeline s size is restricted to just around mb with inference time being as low as ms
an important thing to note here is that the overall pipeline s size and ence timing is more than the sum of components mentioned in table v because of presence of additional resources like glove embeddings which are used across multiple components
fig

application content presentation table iv results across the three datasets enron email precision methods
lda
tr rake
proposed system
amazon reviews notes bpm precision







bpm precision







bpm



table v on device inference times and model sizes component size mb pos graph cnn proposed system inference time per sample ms viii
applications an arbitrary search for on device note taking applications on the web will list down to such applications thus providing strong evidence about the utility and signicance of notes application in the modern world smartphones
from do lists to email drafts key conversations and blog snapshots everything can be stored as a note
thus notes are a form of text that can not be expected to have any kind of structure soever and are bound to have enormous variations depending on multiple factors associated with the user
unstructured text in notes may or may not have punctuation marks correct sentence formation or correct grammar
recently there have been many developments in the eld of notes applications ranging from automatic detection of list type of notes to notes containing images but none of these new features actually address the problem of cluttering of data
in this section we show one of the ways in which this cluttering of data in notes application can be handled using our proposed work
fig
shows on device screenshots which can signicantly enhance user experience while navigating through notes application
initially in step the user uses one of the querying keywords to search for the desired note
then in step all the notes which are indexed by the search made by the user are displayed with their set of tags extracted using our pipeline in a summarized manner
all the notes which are indexed in step are selected if there is a match between the querying keyword in step and tags being displayed for each note in step
finally in step user can select its desired note out of all the indexed notes in step
this is just one of the ways in which content presentation with our pipeline running in the background can be done but there can be other better ways to render the content as well
ix
conclusion and future work unstructured text is a special type of text having no dened format or pattern
generating relevant tags in the form of concepts and keywords from unstructured text therefore not involve the use of contextual semantics usually associated across entire text
thus our proposed pipeline uses word level dynamics to extract concepts and keywords from tured textual data
also because of disorganized nature of unstructured data extracted tags can prove really helpful while navigating through such text
the most popular application targeted for on device usage of the proposed pipeline is notes application
with recent developments in device based note taking applications our proposed pipeline with on device feasibility can play a vital role in enhancing user experience as we have seen in section viii
one of the areas where we can signicantly improve is by analysing multiple input data formats such as images texts audio
at the same time
multiple input data formats can give us a better context and thus extracting a more subtle set of tags
analysing these multiple input formats would require techniques such as ocr or speech recognition depending on the input provided by the user
x
acknowledgement the authors would like to thank all the users who tributed in notes application data collection
the authors would like to express their gratitude towards all the reviewers who have given constructive feedback to improve the paper
references a
hulth and b
b
megyesi a study on automatically extracted keywords in text categorization in proceedings of the international conference on computational linguistics and the annual meeting of the association for computational linguistics
association for computational linguistics pp

j
furnkranz t
mitchell e
riloff et al
a case study in using linguistic phrases for text categorization on the www in working notes of the aaai icml workshop on learning for text categorization pp

a
n
aizawa linguistic techniques to improve the performance of automatic text categorization
in nlprs vol

citeseer pp

i
h
witten g
w
paynter e
frank c
gutwin and c
g
manning kea practical automated keyphrase extraction in design and usability of digital libraries case studies in the asia pacic
igi global pp

j
tang j

li k

wang and y

cai loss minimization based keyword distillation in asia pacic web conference
springer pp

m
sahlgren and r
coster using bag of concepts to improve the performance of support vector machines in text categorization
y
ko j
park and j
seo improving text categorization using the portance of sentences information processing management vol
no
pp

g
lample m
ballesteros s
subramanian k
kawakami and c
dyer neural architectures for named entity recognition arxiv preprint

x
ma and e
hovy end to end sequence labeling via bi directional lstm cnns crf arxiv preprint

f
zhao m
r
min c
shen and a
chakraborty convolutional neural knowledge graph learning arxiv preprint

a
bellaachia and m
al dhelaan ne rank a novel graph based keyphrase extraction in twitter in ieee wic acm international conferences on web intelligence and intelligent agent technology vol

ieee pp

d
p
kingma and j
ba adam a method for stochastic optimization arxiv preprint

d
m
blei a
y
ng and m
i
jordan latent dirichlet allocation journal of machine learning research vol
no
jan pp

r
mihalcea and p
tarau textrank bringing order into text in proceedings of the conference on empirical methods in natural language processing pp

s
rose d
engel n
cramer and w
cowley automatic keyword extraction from individual documents text mining applications and theory vol
pp


