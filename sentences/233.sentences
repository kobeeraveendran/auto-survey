summae zero shot abstractive text summarization using length agnostic auto encoders peter j
liu google brain
com yu an chung mit csail
edu jie ren google brain
com t c o l c
s c v
v i x r a abstract we propose an end to end neural model for zero shot abstractive text summarization of paragraphs and introduce a benchmark task rocsumm based on rocstories a subset for which we collected human summaries
in this task ve sentence stories paragraphs are summarized with one sentence using human summaries only for evaluation
we show sults for extractive and human baselines to demonstrate a large abstractive gap in formance
our model summae consists of a denoising auto encoder that embeds tences and paragraphs in a common space from which either can be decoded
summaries for paragraphs are generated by decoding a sentence from the paragraph representations
we nd that traditional sequence to sequence auto encoders fail to produce good summaries and describe how specic architectural choices and pre training techniques can signicantly improve performance outperforming tive baselines
the data training tion code and best model weights are sourced
story paragraph to summarize jason listened to the weather and heard it was going to be sunny
he thought the kids might like to go swimming
he gathered up the swimsuits towels and sunscreen
jason and the kids got into the truck and drove to the beach
they spent the next hours playing and splashing in the surf
three human summaries jason saw a nice weather forecast and went to the beach with his kids for hours
jason took the kids swimming at the beach on a sunny day
jason decided to take the kids to the beach since it was a sunny day
best extractive sentence jason listened to the weather and heard it was going to be sunny
unsupervised abstractive summary ours jason listened to the weather to be the coolest and fun day and he went to the beach and got ready
an example story to summarize from figure the validation set for which we collected human maries
human summaries from independent raters
c the extractive sentence with the highest
summary generated from our model without seeing any summaries
introduction extractive summarization has been studied sively over the past several decades gupta and lehal ferreira et al

however humans typically summarize abstractively phrasing and performing non trivial compression of details that are difcult to encode in cal summarization algorithms
recent progress in neural language models sutskever et al
jozefowicz et al
radford et al
has enabled models to generate near uent language that are not mere regurgitations of training data
with large datasets of document summary pairs equal contribution work done while interning at google brain
primarily from the news domain abstractive marization has been approached as a supervised neural sequence transduction problem rush et al
nallapati et al
narayan et al
fabbri et al

outside of news however such large parallel datasets are rare due to the cost prohibitive labeling process i
e
reading long documents and writing summaries
much more commonly available are large corpora of documents without summaries
it is therefore sirable to have models capable of automatically summarizing documents abstractively with little to no supervision
in contrast to abstractive methods many tractive approaches do not rely on example maries
inspired by that we study the extreme case of no exposure to summaries during training or unsupervised zero shot abstractive tion uas
recently there has been some thus far limited work on uas for both the multi document chu and liu and single document isonuma in this work we focus on et al
cases
uas of paragraphs with a sentence which is haps the most basic form of multi sentence document summarization
in particular we marize the ve sentence stories from ries mostafazadeh et al
and show that there is a non trivial abstractive gap between man and extractive performance making it a able benchmark for measuring progress in uas
our approach is based on training a ing auto encoder vincent et al
that codes sentences and paragraphs in a shared space
the decoder input is pre pended with a special token to signal whether to decode a sentence or a paragraph and a summarizing sentence is erated by decoding a sentence from an encoded paragraph
however we found that traditional approaches to training such an auto encoder sulted in non overlapping sentence and paragraph latent sub spaces which we call segregation resulting in long multi sentence summaries
we describe architectural modications and supervised pre training objectives to prevent regation and improve performance signicantly beyond sentence extractive baselines
while the goal of human performance is still far we believe the techniques presented here are a major step in that direction
in summary our contributions are as follows

we introduce a new benchmark task summ for measuring progress toward human performance on uas

we propose a novel end to end differentiable neural model for uas of graphs

we describe novel self supervised that training signicantly improve performance beyond sentence extractive baselines
and denoising objectives
we conduct ablation experiments showing the importance of architectural choices and model objectives
a new task for unsupervised abstractive summarization our new task re purposes and augments an isting dataset rocstories mostafazadeh et al
originally designed for the story cloze test sct where a model must choose the correct fth sentence of two candidates given the rst four
the stories are self contained verse realistic non technical high quality and have a coherent story arch
the human mance on the sct task is close to
our proposed uas task involves ing the ve sentence training rocstories with a single sentence without summaries at ing i
e
perform zero shot summarization
we found summaries by independent human raters have high similarity in this task suggesting it is well dened and relatively unambiguous in trast to other summarization tasks where the sired length or the topic of the summary is unclear
the simplicity of the task is conducive to iterating quickly and making rapid progress in uas
ing only ve sentences and a low bound on total number of words avoids engineering issues that often arise with very long sequences
due to the constraints it is simple to calculate the maximum sentence extractive summarization performance which is far from the human performance see ble suggesting a need for abstractive models
in contrast it is unclear for example what human performance is on the popular cnn dailymail supervised summarization task see et al
and whether abstractive models provide much of a benet over extractive ones on it kryscinski et al


collection of reference summaries for evaluation to evaluate summarization models we collected multiple summaries from independent enced and highly reputable amazon mechanical turk amt workers
the full worker selection criteria and amt template can be found in pendix a

we split the original training stories into train valid and test amples and collect human summaries each for validation examples and test examples
collecting multiple summaries allowed us to mate human performance as well as treat multiple right answers more fairly by averaging metrics across the summaries for a given example
an ample story with human summaries the best tractive sentence and one of our model summaries can be found in figure
related work chu and liu proposed a model for shot multi document abstractive summarization where the mean of the representations from an auto encoder for input documents is used to code a summary
isonuma et al
proposed to summarize a product review by describing it as a discourse tree where the summary is the root and the child sentences explain their parent
baziotis et al
performed sentence pression by chaining two sequence to sequence models as an auto encoder
a through gumbel softmax estimator jang et al
was used to sample an output sequence from the rst which was encouraged to be guage via a pre trained language model loss
it was also encouraged to be related to the original sentence by using it as input to the second model which was trained to reconstruct the inal sentence
fevry and phang used a noising auto encoder for sentence compression as well
an input sentence was articially extended and word shufed encouraging the model to learn to exclude and compress producing shorter tences
wang and lee trained a cycle gan model zhu et al
to learn a to summary mapping given large datasets of paired documents and summaries
however due to the model being exposed to summaries during training it is not zero shot summarization
ther unlike the original cycle gan model on ages it is non differentiable since the nator must distinguish real from generated in the discrete language domain and relies on force williams
radford et al
trained a large language model on a large web text dataset and found that the model could produce zero shot summaries if prompted with a document followed by though they considered them rudimentary and usable
historically there have been strong parallels in the development of neural sequence transduction models for translation and summarization relying on some avor of sequence to sequence learning
we depart signicantly from recent unsupervised figure model architecture of summae whose backbone is a denoising auto encoder
the encoder enc maps paragraphs and sentences into a common space from which either can be decoded via the coder dec by conditioning on two different of sequence tokens
during training we add noise to text sequences before mapping to age enc to learn more robust text representations
translation work lample et al
artetxe et al
where models are exposed to both source and target sequences though unpaired
in our work models must learn to produce target quences i
e
summarize having only been posed to source sequences documents during training
model and methods
architecture our summarization model summae is depicted in figure and consists of a denoising encoder enc dec capable of auto encoding text sequences t that can be sentences or in particular the encoder enc t graphs
rzdim is a deterministic function parameterized by e mapping text to a latent vector tion rzdim
for an input sequence t we add random noise to t described in tion
t t
we consider two encoder implementations
a bidirectional rnn schuster and paliwal where z is derived from the rnn s nal hidden state h followed by an afne transformation h w h
a transformer stacked with n cal transformer encoder blocks denoted as trfenc from vaswani et al

z is derived from the output sentation of the rst token as in devlin et al
followed by an afne transformation where x is the standard cross entropy loss between the input sequence and its reconstruction
we optimize it using gradient descent and forcing williams
t hl n h hn w h
the decoder parameterized by g is an regressive generative model dening a ity distribution over t conditioned on z
we also condition the decoder on whether to decode a tence or paragraph using two different of sequence tokens indicated by
the reconstructed input t is obtained by sampling one token at a time until a special end of sequence ken is obtained t i



we consider two decoder implementations
a unidirectional rnn that conditions on z by concatenating the decoder input embeddings with z at each time step
a transformer with causal masked attention that conditions by adding z to each input embedding
this is similar to the former decoder in vaswani et al
out decoder encoder attention
in both cases we avoid decoder encoder attention to encourage all semantic information to be sulated in z
in our dataset a single example is a paragraph consisting of sentences p


and the auto encoder contributes two tion loss terms one for the sentences and one for the paragraph weighted by s and p
e g si e g p e g our approach to summarize paragraphs with a sentence is to prompt the decoder to generate a sentence using conditioned on the latent vector z of a paragraph
however simply training an rnn or transformer auto encoder as described generally fails as we see in section
we pothesize that the encoder can learn to map tences and paragraphs to separate regions in the tent space and the decoder can recognize whether to decode a sentence or paragraph in tion based solely on the location of z and ignore
we nd this can result in the decoder ing a paragraph even if prompted for a sentence
we call this phenomenon segregation
ideally the auto encoder learns a higher level latent concept conveyed in paragraphs and tences disentangled from their original sion as paragraphs and sentences
to explicitly encourage this we investigated adding an sarial discriminator critic d which is trained to classify whether a latent vector z is an encoded sentence or a paragraph
in other words it learns where is if t is a sentence and if a paragraph while the auto encoding loss is augmented to encourage fooling the inator into classifying paragraphs as sentences
similar approaches have been used in style fer hu et al
shen et al
romanov et al
and unsupervised machine tion lample et al
although not for stractive summarization
details of our tation can be found in appendix a

in our experiments we found adding the critic was very effective for generating one sentence short summaries
however for some decoder congurations the critic was found to be unnecessary and even harmed performance which we discuss in section


adding noise to text we use a denoising auto encoder rather than a standard auto encoder by reconstructing a text quence from a noisy version of it
denoising can be seen as a useful self supervised objective for improving representations as seen in devlin et al
it also serves as a form of data tion effectively increasing the number of training examples nally it discourages merely learning the identity function without having to reduce the information bottleneck zdim to a very small value
we employ two techniques for adding noise randomly masking tokens similar to devlin et al
and song et al
we randomly mask the input sequence tokens at training before feeding it to the encoder
however instead of only predicting masked tokens we generate the full noised sequence
we apply the masking with the following procedure
select sequences to mask with probability ps
so that some of the sequences are unpermuted as they are during test time

for selected sequences replace each token with a mask token with probability pm
permuting order of sentences within graphs even with token masking we observed a failure mode where the latent representation of a paragraph overly focuses on the rst sentence of the paragraph and memorizes it
indeed it is the best sentence to memorize for the purpose of constructing a paragraph
however to encourage learning the structure and coherence of paragraphs beyond the rst sentence with probability pperm we permute the order of sentences in a paragraph and train the auto encoder to recover the original paragraph

pre training of encoder and decoder motivated by the recent success in self supervised language representation learning peters et al
howard and ruder radford et al
devlin et al
song et al
zhang et al
yang et al
we propose eral strategies that pre train the encoder and coder before optimizing them jointly with the auto encoding objective equation
the gies are applied jointly in the pre training phase by adding the corresponding losses
although we adopt the paradigm of pre training followed by ne tuning there are two signicant in past work differences with other work
beled data are available for the downstream tasks and ne tuning is supervised in our work ever both pre training and ne tuning are supervised
additionally most previous work pre trained their models on extremely large pora different from their downstream datasets whereas our model learns everything from the same
encoder pre training corrupted graph prediction summarizing a paragraph quires understanding how its sentences follow each other to form a coherent narrative
to age good paragraph representations we propose a novel pre training task classifying whether a graph has been corrupted by swapping the order of the i th sentence with its next sentence
we mented this by adding a logistic regression layer to the encoder paragraph output z and minimizing cross entropy error with of paragraphs rupted and unmodied
we refer to this task as corrupted paragraph prediction cpp
encoder pre training next sentence diction we propose another pre training tive encouraging the encoder to understand how sentences follow within a paragraph
the jective referred to as next sentence or same paragraph nssp shares an idea with bert s next sentence prediction nsp objective devlin et al
classify whether two sentences are adjacent but is modied to be more difcult
as in bert we sample the sentence pairs a and b such that of the time b follows a otherwise it does not negatives
however negative pairs are sampled from within the same paragraph instead of from the whole corpus
this is more ing as sentences from the same paragraph usually are more similar and about the same topic
we served this harder negative sampling leads to ter downstream summarization performance
further our implementation differs in the lowing way from bert s nsp in bert each input sequence is constructed by concatenating a and b separated by a sep token
to better differentiate a and b bert further employs segment dings
in contrast we encode a and b pendently avoiding the need for the separator token and segment embeddings
in bert a binary multi layer perceptron is instead of added during pre training
troducing these extra parameters we directly take the dot product of the two sentence resentations followed by a sigmoid function a b t
tried pre training on large corpora but it did nt help
this also restricts the capacity of the er forcing all relevant features into z
this implementation is simpler and more generic since it is not tied to a specic encoder e

bert transformer architecture
in contrast to cpp which learns to encode the tence relationships in paragraph representations nssp is taught to encode sentence relationships in individual sentence representations
decoder pre training auto regressive guage modeling we pre train the decoder with a standard auto regressive language modeling lm objective similar to ramachandran et al

we implement this by setting z in equation regardless of the input sequences in pre training so that the decoder receives no tioning signal during teacher forcing making the process equivalent to a standard lm task
experiments
metrics we found human summaries which are quite stractive to be unfairly punished on and rouge x precision variants and thus only report and recall scores lin
we truncated generated summaries to their rst sentence and to a word limit of to prevent ferring models with overly long summaries ilar to rush et al
although they used a byte limit
the limitations of rouge are known and in particular favor extractive ods since paraphrasing and synonyms are not warded fairly
however at the time of writing rouge remains a dominant metric in tion research kryscinski et al

we treated the summaries per evaluation example equally and report average rouge scores over the test examples using the validation examples for hyper parameter tuning
we observed little ference in averaged validation and test scores

baselines extractive because the documents consist of only ve sentences for each example there are only ve possible sentence extractive summaries
thus we computed the performance of selecting sentence i as the summary for i


and denote these as extract i
we also computed imum achievable sentence extractive score by lecting the best performing sentence per example evaluated based on recall against the human summaries which we refer to as extract oracle
although this method cheats by looking at the test data and may not actually be achievable in practice it is useful to have an estimate of the ceiling of extractive methods
human we estimated human performance by computing a the maximum and the average rouge between all pairs of human summaries for each evaluation example and took the mean across examples
meansum single although designed for document summarization we adapted sum chu and liu for the single document case by treating each sentence as a document similar to isonuma et al
where it is called meansum single
since meansum erates summaries of the same shape as the put documents this adaptation generates summaries

training details and hyper parameters text tokenization we used a data driven word tokenizer sennrich et al
with a cabulary size of to convert text into integer id tokens followed by a dim embedding layer
we shared the input and output embedding layers as in press and wolf
architecture we experimented with three base encoder decoder congurations for summae rnn rnn trf trf and trf rnn where both rnnenc trf stands for transformer
and rnndec were single layer grus chung et al
with hdim while rnnenc was bidirectional and rnndec was unidirectional
trfenc was a stack of transformer encoder ers with each consisting of a self attention layer with attention heads and a point wise forward network with hidden units
trfdec had the same hyper parameter setting as trfenc but was composed of transformer decoder layers
zdim was set to
all model weights ing embeddings were initialized randomly
coding at each time step was performed greedily using arg max
optimization we performed gradient descent using the adam optimizer kingma and ba with a learning rate of
and batch size of
the models were trained with early stopping ing maximum validation recall
the rnn rnn without critic
rnn rnn with critic
c trf rnn with lm pre training and without critic
figure d t sne visualizations for summae latent space
each blue circle corresponds to a sentence and each red triangle corresponds to a paragraph best viewed in color
number of pre training steps was set to when pre training was employed
was added the model was able to generate sentence short summaries
when critic discriminator was used it was plemented as a multi layer perceptron with hidden units
when token mask was added we set ps
and pm

for permuting tences in paragraphs we set pperm

the critic and noise were only added during the tuning phase
we experimented with parameters for constructing summae but found the reported setting worked the best empirically
different
results and discussion table shows rouge scores and summary lengths for human and extractive baselines and for summae with enhancements described in tion
for the base encoder decoder tions rnn rnn trf trf and trf rnn ken masking and paragraph shufing were added as noise but no pre training was done and no critic was added
human and extractive baseline performance the best extractive sentences were unsurprisingly the rst and last as the sentences introducing the subject and revealing the ending
extractive acle is by denition the best and its performance was considerably higher than any xed index tence to extract
estimated human performance was much higher than even the extractive oracle suggesting an abstractive gap
effectively restricts summary length critic all three base encoder decoder congurations tended to generate overly long invalid summaries as shown by the number of words close to and sentences exceeding indicating the in equation was likely ignored
once the critic to validate our hypothesis that segregation in the latent space was the underlying problem ing long summaries we visualized the latent space of summae rnn rnn similar plots for trf and trf rnn in d using t sne maaten and hinton with and without the critic
in figure where there was no critic sentence and paragraph representations indeed were mapped to completely separate regions while in figure the adversarial feedback from the critic effectively merged the two clusters supporting our sis
effect of lm pre training as can be seen from the table lm pre training always improved the model performance while keeping the summaries short for all of three encoder decoder tions it boosted their scores from
to

to
and
to
tively
qualitatively we found models with lm pre training generated more uent summaries as well
best encoder decoder conguration ingly we observed that trf rnn outperformed the other two congurations that use the same quential architecture for both encoder and decoder
similar results were reported in chen et al
where they found their hybrid model composed of a transformer encoder and rnn decoder worked the best in machine translation
another possible reason is that the decoder here does not need to attend to long term dependencies which is where transformers have major advantages over rnns
surprisingly we found that the trf rnn ant enhanced with lm pre training did not quire a critic to generate one sentence summaries table rouge recall scores and generated summary lengths number of words and sentences for estimated human performance extractive baselines meansum single and variants of summae
by default all summae models incorporate token masking and paragraph shufing without critic and any pre training
trf stands for transformer
numbers in bold denote the best performed models in each category based on rouge numbers in italics denote the models that are not qualied for generating one sentence summaries
superscript letters a b c d denote model pairs that are compared in human evaluations in table
model rouge l num words num sentences e v i t c a r t e e v i t c a r t s b a human average human maximumd extract extract extract extract extract extract oracle summae rnn rnn critica lm pre trainingb c summae trf trf critic lm pre training summae trf rnn critic lm pre training lm pre training token masking paragraph shufing cpp pre training nssp pre trainingc meansum singlea token masking































































































and prevented segregation as the other decoder congurations did
figure visualizes its corresponding latent space where the paragraph and sentence representations were mapped to the same region suggesting that lm pre training has the same effect of critic in this specic ration
it also outperformed the model with both critic and lm pre training
vs


effect of token masking and paragraph ing by default both token masking and graph shufing introduced in section
were added as noise to base summae models during auto encoding
removing either one degraded the performance score decreased from
to
when token masking was removed and to
when paragraph shufing was moved
during experimentation we also found adding this noise produced more stable results across different runs
effect of nssp cpp pre training summae trf rnn with only decoder pre training
already surpassed the best extractive sentence tract

with encoder pre training with ther nssp or cpp we observed a further ment from
to
and to
respectively
our best summae model constructed by a transformer encoder and an rnn decoder was pre trained with nssp and lm objectives lowed by denoising auto encoding of masked and shufed input sequences
it achieved rouge l

that signicantly outperformed the xed index extractive sentences and was comparable with extract oracle which looks at the test data
human evaluation of model summaries to validate that making progress on the rouge rics correlates to making real progress as judged by humans we conducted several side by side model comparisons covering the range of rouge scores from low to high end using amazon chanical turk workers
workers were presented with the sentence story paragraph along with two model summaries and asked to rate each on two dimensions uency and information vance
to minimize inter rater noise scores by table human evaluation results comparing model summary pairs side by side on uency and information relevance
superscript letters a b c d correspond to models in table
the denotes statistical signicance at
using a binomial two tailed test null hypothesis the models are equally good
model preference a rnn rnn critic meansum single noise b rnn rnn critic lm rnn rnn critic c trf rnn lm nssp rnn rnn critic lm d human trf rnn lm nssp fluency information distinct workers were collected for each example and averaged
we aggregated results across random examples from the test set
results ing the average preference of the workers on the two dimensions are presented in table
we observed that the uency improved icantly from the meansum single model through the rnn models while the formation aspect continued to improve through our best model the trf rnn with nssp and language model pre training
the human formance was still far better on both dimensions when compared side by side
additional model samples can be viewed in appendix figures and
in addition to decoding a sentence from a graph representation we found it informative to look at reconstructed paragraphs from the encoder which are also included in figure
the paragraph reconstructions show some coherence although with some disuencies and factual curacies that are common with neural generative models
since the summaries are decoded from the same latent vector as the reconstructions proving them could lead to more accurate maries
conclusions we introduce rocsumm a new benchmark task for zero shot unsupervised abstractive rization uas that is useful for iterating and measuring progress on this challenging lem
as one of the very rst works ing single document uas we propose a novel neural model based on a denoising auto encoder along with several self supervised pre training techniques for enhancing the model
while performance is still far behind humans summae outperforms extractive baselines and is a major step toward uas
code and data release at provided the code to reproduce our experimental setup
is google research google tree master summae
we include the into code to process the rocstories dataset rocsumm our train test splits the human summaries used in validation and test evaluation amazon mechanical turk templates used for data collection and evaluation
we also include model training and evaluation code and the model weights for our best model
references mikel artetxe gorka labaka eneko agirre and kyunghyun cho

unsupervised neural chine translation
in iclr
christos baziotis ion androutsopoulos ioannis stas and alexandros potamianos

seq differentiable sequence to sequence to sequence autoencoder for unsupervised abstractive sentence compression
in naacl hlt
mia xu chen orhan firat ankur bapna melvin johnson wolfgang macherey george foster llion jones mike schuster noam shazeer niki parmar ashish vaswani jakob uszkoreit lukasz kaiser zhifeng chen yonghui wu and macduff hughes

the best of both worlds combining recent advances in neural machine translation
in acl
eric chu and peter liu

meansum a neural model for unsupervised multi document abstractive summarization
in icml
junyoung chung caglar gulcehre kyunghyun cho and yoshua bengio

empirical evaluation of gated recurrent neural networks on sequence eling
in nips deep learning and representation learning workshop
jacob devlin ming wei chang kenton lee and kristina toutanova

bert pre training of deep bidirectional transformers for language standing
in naacl hlt
alexander fabbri irene li tianwei she suyi li and dragomir radev

multi news a large scale multi document summarization dataset and tive hierarchical model
in proceedings of the annual meeting of the association for tional linguistics pages florence italy
association for computational linguistics
rafael luciano ferreira souza cabral rafael dueire lins gabriel pereira e silva fred freitas george cavalcanti rinaldo lima steven simske and luciano favaro

ing sentence scoring techniques for extractive text summarization
expert systems with applications
thibault fevry and jason phang

vised sentence compression using denoising encoders
in conll
ian goodfellow jean pouget abadie mehdi mirza bing xu david warde farley sherjil ozair aaron courville and yoshua bengio

generative versarial nets
in nips
vishal gupta and gurpreet singh lehal

a survey of text summarization extractive techniques
journal of emerging technologies in web gence
jeremy howard and sebastian ruder

universal language model ne tuning for text classication
in acl
zhiting hu zichao yang xiaodan liang ruslan salakhutdinov and eric xing

toward trolled generation of text
in icml
masaru isonuma junichiro mori and ichiro sakata

unsupervised neural single document marization of reviews via learning latent discourse structure and its ranking
in acl
eric jang shixiang gu and ben poole

gorical reparameterization with gumbel softmax
in iclr
rafal jozefowicz oriol vinyals mike schuster noam exploring arxiv preprint shazeer and yonghui wu

the limits of language modeling


diederik kingma and jimmy ba

adam a method for stochastic optimization
in iclr
wojciech kryscinski nitish shirish keskar mccann bryan caiming xiong and richard socher

neural text summarization a critical evaluation
in emnlp
guillaume ludovic denoyer and lample marcaurelio ranzato

unsupervised machine translation using monolingual corpora only
in iclr
guillaume lample myle ott alexis conneau dovic denoyer al

phrase based in ral unsupervised machine translation
ings of the conference on empirical methods in natural language processing pages
chin yew lin

rouge a package for automatic evaluation of summaries
in workshop on text marization branches out
laurens van der maaten and geoffrey hinton

visualizing data using t sne
journal of machine learning research
nasrin mostafazadeh nathanael chambers xiaodong he devi parikh dhruv batra lucy vanderwende pushmeet kohli and james allen

a corpus and evaluation framework for deeper understanding of commonsense stories
in naacl hlt
nasrin mostafazadeh michael roth annie louis nathanael chambers and james allen

sem shared task the story cloze test
in eacl workshop on linking models of lexical sentential and discourse level semantics
ramesh nallapati bowen zhou cicero dos santos c a glar and bing xiang

tive text summarization using sequence to sequence rnns and beyond
in conll
shashi narayan shay cohen and mirella lapata

do nt give me the details just the summary topic aware convolutional neural networks for treme summarization
in emnlp
matthew peters mark neumann mohit iyyer matt gardner christopher clark kenton lee and luke zettlemoyer

deep contextualized word resentations
in naacl hlt
or press and lior wolf

using the output bedding to improve language models
in eacl
alec radford karthik narasimhan tim salimans and ilya sutskever

improving language standing by generative pre training
technical port openai
alec radford jeffrey wu rewon child david luan dario amodei and ilya sutskever

language models are unsupervised multitask learners
cal report openai
prajit ramachandran peter liu and quoc le

unsupervised pretraining for sequence to sequence learning
in emnlp
alexey romanov anna rumshisky anna rogers and david donahue

adversarial decomposition of text representation
in naacl hlt
alexander rush sumit chopra and jason weston

a neural attention model for abstractive tence summarization
in emnlp
mike schuster and kuldip paliwal

bidirectional ieee transactions on recurrent neural networks
signal processing
abigail see peter liu and christopher manning

get to the point summarization with generator networks
in acl
rico sennrich barry haddow and alexandra birch

neural machine translation of rare words with subword units
in acl
tianxiao shen tao lei regina barzilay and tommi jaakkola

style transfer from non parallel text by cross alignment
in nips
kaitao song xu tan tao qin jianfeng lu and yan liu

mass masked sequence to quence pre training for language generation
in icml
ilya sutskever oriol vinyals and quoc le

quence to sequence learning with neural networks
in nips
ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan gomez ukasz kaiser and illia polosukhin

attention is all you need
in nips
pascal vincent hugo larochelle yoshua bengio and pierre antoine manzagol

extracting and composing robust features with denoising coders
in icml
yaushian wang and hung yi lee

learning to encode text as human readable summaries using generative adversarial networks
in emnlp
ronald williams

simple statistical following algorithms for connectionist ment learning
machine learning
zhilin yang zihang dai yiming yang jaime bonell ruslan salakhutdinov and quoc le

xlnet generalized autoregressive pretraining for language understanding
in neurips
zhengyan zhang xu han zhiyuan liu xin jiang maosong sun and qun liu

ernie hanced language representation with informative tities
in acl
jun yan zhu taesung park phillip isola and alexei efros

unpaired image to image tion using cycle consistent adversarial networks
in iccv
a appendix a
amazon mechanical turk settings figure shows the detailed instructions for the rocstories summarization task and the task user interface
we rewarded
per summary and restricted worker requirements to the following hit approval rate location united states number of hits approved we used the same worker requirements for the human evaluation experiment discussed in section

the templates used are available at our
code repository google research google tree master summae
a
training the adversarially regularized auto encoder the model architecture augmented with a critic is depicted in figure
the critic was implemented as a multi layer perceptron mlp with one den layer parameterized by d and trained with a standard classication loss d d where t is p or a randomly sampled sentence
the auto encoder loss was augmented with aging the encoder to fool d into classifying graph vectors as sentences e e g e g e as with generative adversarial networks fellow et al
this is a two player max game and in practice we alternate training the autoencoder with respect to encoder and coder parameters e and g and the tor with respect to d
however unlike gans we do not generate data from the training domain in this case textual sequences and instead operate in the continuous latent space which allowed us to maintain differentiability
algorithm our model training loop with critic a collection of paragraphs p input for p in p do step step if step npretrain then update e g by gradient descent on e g eq
else if step then update d by gradient descent on d eq
update e g by gradient descent e g eq
else end end output auto encoder parameters e g figure amt task instructions
figure the auto encoder model augmented with critic
story paragraph to summarize ben decided to go for a walk
but he stepped outside and shivered
the weather had turned cold overnight ben went back in and put on another sweater
now he was ready for his walk human from different workers ben put on more layers after noticing the cold and went on a walk
ben went outside and realized it was cold so he put a sweater on
ben put on more layers after noticing the cold and went on a walk
meansum single noise ben decided back in to also take him
rnn rnn critic ben decided to go for an clean glass
rnn rnn critic lm ben decided to go outside and he was in a dark trf rnn nssp lm best model ben decided to go outside for awhile and he was ready for a ride back on and shivering figure for the same story we show summaries produced by different models used in human evaluation table
sample story bill was always bullied
his bully was a star athlete
one day bill pushed him back
the bully fell wrong and broke his ankle
it ruined his athletic career
summary bill was always bullied by a bully and he broke his ankle but it broke
reconstruction bill was always bullied
one day a bully was bullied
bill beat his opponent
the ball broke down
bill was crushed his ankle laced
sample story antonio was happy
his new pizza place was doing great business
his pizza place had an edge on all the others
he only used the freshest and best ingredients
his pizza place would be there for a long time
summary antonio was happy to get his best pizza place and had a great time at the place
reconstruction antonio was happy to do the best pizza place
is favorite place had a great time
his best friend had an extra place
his place was the best pizza place
his place had a good time with the pizza
sample story charles was sure he would nt qualify for an auto loan
he was years old and did nt have a car
on a lunch break he met a nissan car salesman
the salesman invited charles to apply for a car loan
charles was approved and the salesman sold him a nissan
summary charles was a salesman and told him he needed a loan from a dealership to buy a car and was approved
reconstruction charles was a salesman and wanted to quit
charles applied for a job and wanted a car
charles applied for a loan and asked him to pay a loan
charles was offered a loan and asked him to quit
charles was a salesman and told him he needed a loan
sample story sue was nt feeling well
she had stayed in bed all day
sue had nt eaten all day
she barely drank anything
sue got dehydrated
summary sue was nt feeling well in bed because she had barely drank after all
reconstruction sue was nt feeling well
she had never drank before
she had all night
sue got all cold
sue felt very refreshed
sample story my boss asked me to nd the mean value of a data set
i told him the mean was not appropriate for the data set
he asked me to explain why it was not appropriate
i told him it was because the data was not symmetrical
he thanked me for my brilliant insight into the problem
summary my boss asked me to make the mistake of the data entry for me so i was pleased to nd it
reconstruction my boss asked me to make the mistake of the data entry
he told me what the expectation was making him a special name
he told me it was the wrong size
he told me not to make the decision to be careful
i told me it was the perfect decision for me
sample story bob just got a new puppy
bob s cat did not like the new puppy
they fought with each other constantly
eventually bob s pets became more familiar with each other
bob s new puppy and cat eventually became best friends
summary bob just got a new puppy to the other cat and was very happy with their new puppy
reconstruction bob just got a new puppy
they were very cute and a little girl
they got the best cat they d met with other kids
they eventually got a new puppy
the other person became very lonely now
sample story nathan wanted to be a computer programmer
he would code everyday after school
eventually he got a degree in computer science
he was hired by microsoft to help program their new operating system
nathan was a big contribution to microsoft s development team
summary nathan wanted to be a great job at all his company s computer and was able to improve his new role
reconstruction nathan wanted to be a programmer
he worked hard to get a new company
he was hired to improve his computer
he was hired to sponsor a month and was able to own a company
he was able to improve his own company
sample story fred has a fever
he was not able to go to work
he had to call out
his work boss was angry
he got red
summary fred has a fever and was not able to go to work out of work and got red
reconstruction fred has a fever
he was not able to go to work
he got out of work
he was mad
he was able to get out
figure some summaries generated by our best summae model along with the reconstructed stories

