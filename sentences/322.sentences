noname manuscript no
will be inserted by the editor image conditioned keyframe based video summarization using object detection neeraj baghel suresh c
raikwar charul bhatnagar p e s v c
s c v
v i x r a received date accepted date abstract video summarization plays an important role in selecting keyframe for understanding a video
tionally it aims to nd the most representative and diverse contents or frames in a video for short maries
recently query conditioned video tion has been introduced which considers user queries to learn more user oriented summaries and its ence
however there are obstacles in text queries for user subjectivity and nding similarity between the user query and input frames
in this work i image is troduced as a query for user preference a ematical model is proposed to minimize redundancy based on the loss function summary variance and iii the similarity score between the query image and input video to obtain the summarized video
more the object based query image oqi dataset has been introduced which contains the query images
the proposed method has been validated using ut tric ute dataset
the proposed model successfully resolved the issues of i user preference ii recognize important frames and selecting that keyframe in daily life videos with dierent illumination conditions
the proposed method achieved
average score for ute dataset and outperforms the existing state of art by

the process time is
times faster than actual time of video experiments on a recently proposed ute dataset show the eciency of the posed method
keywords image key frame object detection query video summarization neeraj baghel suresh c
raikwar charul bhatnagar department of computer engineering and applications gla university mathura india e mail
com introduction the video capable mobile devices becoming increasingly ubiquitous
there is an analogous increase in the amount of video data that is captured and stored
additionally as the diculty of capturing video the cost of storage decreases and tends to see a corresponding increase in the quality of captured videos
as a result of this it becomes very dicult to watch or discover interesting video clips among the vast amount of data
one tion to this problem is to lies in the development of a video summarization system which can automatically locate these interesting clips and generate a nal rated video summary
in current years interest in video summarization is increased due to a large amount of video data
tainly professionals and consumers both have access to video retrieval nowadays
the image contains a large amount of data whereas the video is a collection of ages which contains huge information and knowledge
thus it is very hard for users to watch or discover the incidents in it
quick video summarization methods able us to speedily scan a long video by removing evant and redundant frames
the video summarization is a tool for creating a compact summary of a video
it can either be an der of stable images keyframes or motion pictures video skims
the video can be summarized using i keyframes and video skims
the keyframes are used to represent important formation contained in video
these are also named as r frames representative frames still image synopses and a collection of prominent images obtained from the video data
the challenges in selecting keyframes are as follows
i redundant frames are selected as a keyframe
dicult to make a cluster when content is neeraj baghel et al

dened the loss function for obtaining a selection score for frames in a video

the object based query image oqi dataset is prepared for the selection of a query image

an adaptive method to compute threshold is ned for the selection of keyframes using standard deviation
in addition to this primary contribution parts of the proposed work also serve as relevant contributions in isolation
these include perform video summarization on commodity hardware
the remaining structure of this paperwork is as lows provides the related work of video marization
describes the datasets used and its setting
describes the proposed ical model of the problem and its solution by calculating loss function
also flowchart and its working consist of video segmentation feature extraction frame scoring and summary generation
describes the plementation details evaluation metrics experimental results obtained by the proposed model
cuss about gui tool for summary generation
in the last discussion about the conclusion of the proposed model
related work although video summarization has been an active search topic in computer vision nowadays
the high computational capabilities of modern hardware allow us to process a video in a fraction of the time if quired which when combined with the evolution of modern vision techniques such as deep neural networks has resulted in a signicant increase in the breadth of techniques which are viable to apply to the topic of video summarization
combined with the vast quantity of prior work involving video summarization many teresting research prospects are available to pursue
although the primary focus is video summarization some of the steps performed during the proposed work need a signicant amount of prior research
among these query focused video summarizing which simply deals with taking a video and dividing it up into a ber of segments and image and video feature extraction related to the query which deals with extracting vant and useful features from images and videos
in author has proposed a new technique on video summarization which is query focused that includes user perspective based on text queries related to video into the summarization method
this is a promising way to personalize video summary
the author lects user annotations and meets the issues of good fig
video summarization workow video and image are selected from their respective datasets
further various techniques are applied to generate the selection matrix
on behalf of the selection matrix summarised video is formed
non identical
the video skims represents moving story borad of a video
the video is split into many tions that are video clips with a smaller length
every portion follows a part of a video or a regular result
the trailer of a movie is an example of video skimming
finally dierent viewers will have dierent ences on what they nd interesting in a video
with traditional hand editing viewers only see the segments deemed interesting by the editor and the time cost for a human editor to create multiple edits of a single video is signicant
the ability to generate multiple possible summaries rather than just a single summary would be a very useful feature for a video summarization system to have as would be the ability to learn a specic users preferences over time
the proposed framework has taken an image query for user preference and uses both global local tures to learn user preference from that image query to generate video summary eciently
object detection is used major user preference based on objects as local features while a salient region is used for focused area colours in an image as global features
the objective of the proposed work is to resolve the issue of the wrong selection of keyframe with a ematical model to minimize redundancy based on ilarity score between query image and frames of the input video to obtain the summarized video
hence the proposed method has used the following features to measure the degree of similarity between an input query image and video
i local features based on ject detection and its details
global features based on salient regions
the primary contributions of the presented work are
a mathematical method to calculate summary ance to reduce redundancy between frames
image conditioned keyframe based video summarization using object detection table related work to video summarization table dataset used for experiments evaluation measures for system generated summaries to user labeled summaries
authors contribution i collect dense tags for the dataset a memory work using a sequential determinantal point process query focused video summarization
the author uses the memory network to take user queries for a video within each shot onto dierent frames
in query conditioned summarization is introduced using a three player generative adversarial network
where generator is used to learn the joint representation of query and video
the discriminator takes three dierent summaries as input and discriminate the real summary from the other two summaries which are randomly erated
generic video summarization has been studied for global keyframes and ecient analysis of video
for shot level summarization song et al
nding important shots using learning of sual concepts shared between videos and images
in distinguish highlight segments from non highlight ones by using a pair wise deep ranking model
in frame level video summarization khosla et al
use web based images before video summarization
in model is used for learning of quential structures and generating further summaries
object level video summarization extracts jects to perform summarization
also there are two gan based networks that include adversarial training
however user preferences is not considered so summaries may not generalize and robust for ferent users
therefore the video summarization based on query conditioned came into focus and provide more personalized summary to user
query conditioned video summarization takes queries given by user into consideration which are in the form of texts to learn and generate user oriented summaries
to tackle this challenge trained a quential and hierarchical dpp sh dpp
in to pick relevant and representative frames adopt a aware relevance model
specically oosterhuis et al
generate visual trailers based on graph based method for selecting most relevant frames to a given user query
ji et al
mulate incorporating web images task obtained from user query searches
recently sharghi et al
instead of using generic task datasets for query conditioned task they propose a new dataset its evaluation metric and a technique based on this new dataset
they propose an adversarial network that summarize videos based on user queries and does not rely on external source web images
datasets and settings in the proposed methodology experiments are plished on the prevailing ut egocentric ute dataset
the dataset contains four videos each
hours long completely dierent uncontrolled daily life situations
a lexicon is provided for user queries consists of ous set of forty eight ideas based on daily life
as for the queries four completely dierent situations videos are enclosed to formalize comprehensive queries
also follow three scenarios which are i queries wherever all ideas seem along within the same video shot ii queries where all ideas seem however not within the same shot and iii queries wherever just one of the concepts seem
also introduced conjointly created a tiny based query image oqi dataset of pictures for the testing image as a question and object detection and feature extraction
during this dataset various pictures of totally dierent eighty categories
every category has quite ve pictures images will have quite one totally dierent objects
the proposed dataset got impressed by the coco dataset and conjointly contained tures are taken by us
neeraj baghel et al
fig
mathematical model where represents a feature of that frame and z represents the selection value of that frame
sv where sv is summary variance sq is distance score tr is trace function and are parameters to control the importance of each term
so variance sv for the summary of a video is dened as n n thus by using trace of can be written as t zixt i xi zt i xixt zi by placing all n frames together using stacked variable it can be written as t z t p where z


p t x and q xx t distance matrix d is dened as d


dn v q where d is the cumulative distance function dened as q where q is a feature of the query image f v is a feature of video frames and is distance function m is feature no
therefore distance score sq for the summary of a video can be dened as


q m m v sq ddt z thus its trace can be written as t rz where r d t by putting the trace values of summary variance t and distance score t in the loss function
z t z z t for solving used method same given the loss function represented by where and g are convex functions by utilized a well known cccp concave convex procedure algorithm to solve it
where is z t z and is z t
in this the loss function can be decomposed into the dierence of two convex functions
in tth tion it will converge the values of z which lies between and as denoted as zm
in zm values lies between and where values close to are for important frames while close to represent unrelated frame
then threshold is required for selecting important frames calculated as the dard deviation as the values are gaussian distributed
convert zm to z on the basis of adaptive threshold proposed method video summarization using object detection is a great challenge in the eld of articial intelligence for the chines as well as the programmers to train machines in such a way that it recognizes the keyframes cally
the proposed method facilitates query conditioned video summarization by extracting an object s detail local feature and salient region global feature ture extraction
it takes the image as a query into sideration with its features recognizes that same feature into the video frame
the framework of the proposed approach is shown in fig

the rst part extracts jects and its visual feature in order to provide hensive summary for the given image
additionally introduced a small object based query image dataset of images for the testing image query
in this dataset there are images of dierent classes
each class has more than images image can have more than one dierent objects
this dataset has spired by common objects in context coco dataset and also contained some other images which are taken by us
the proposed method has also tested these on dierent videos and images that show this proposed model not restricted query image and videos to these datasets

mathematical model consider x


be a feature matrix for a video with n segment frame features
the feature representation of the summary for video dened as vs x t z where z is selection matrix z


and zn is the selection variable
the model is based on view point in multiple videos
loss function is dened by diverse content same group videos and dierent group videos whereas proposed method used dierent distance functions based on similarity between image query and input video
objective to nd the values of zn in order to imize the loss function t t image conditioned keyframe based video summarization using object detection fig
video segmentation frames are extracted from lected video and various pre processing techniques are plied


feature extraction feature extraction methods should be video summary oriented
the features are rst extracted from the query image and each frame of input video then aggregated within each segment xn to obtain a feature vector x
the proposed framework has taken both global and cal features to generate video summary eectively
for local features the proposed work utilized object tection
the objective of object detection is to nize all occurrences of objects from a recognized sication similar to people vehicles or faces in an age
an item extraction module is intended to provide the thing division strategy that furthermore gives a pendable contribution to the pursuit instrument
the proposed method has utilized you only look once yolo method with dierent classes for nition and trained on coco dataset
the classes considered in the proposed method are person umbrella tie backpack handbag suitcase cycle motorcycle bus truck car airplane train boat trac light stop sign bench re hydrant parking meter bird dog sheep elephant zebra cat horse cow bear girae frisbee snowboard kite baseball glove board skis sports ball baseball bat skateboard nis racket bottle cup knife bowl wine glass fork spoon banana sandwich broccoli hot dog donut apple ange carrot pizza cake chair potted plant dining table couch bed tv toilet mouse keyboard laptop remote cell phone toaster microwave refrigerator oven sink book vase teddy bear toothbrush clock scissors hair drier the yolo is one of the faster object detection methods
the proposed method has used the trained model on the coco dataset
the output for every frame using yolo consists of three lists each sentation are dimensional vectors makes predictions at three scales by down sampling the size of the input image into blocks of size and blocks
however the proposed method has used only and blocks because very small objects are not require
if the prediction condence score is generated if more than
then the object is extracted from that frame
then the output of these two feature vector and object category is matched with other frames of a given video
with condence threshold
and non maximal fig
framework for video summarization contains four major steps video segmentation feature extraction frame scoring and summary generation
value
values having greater then threshold values will be considered as while rest are
therefore selection matrix z is dened as z zm where is a standard deviation from zm
at last generated summarized video by accumulated keyframe dened by selection matrix which can be ned as vs x t where x is feature matrix and z is selection matrix

further the owchart of the proposed model the owchart of the proposed model of video rization is shown in fig

the proposition is broadly classied into these phases a video segmentation feature extraction c frame scoring and summary generation


video segmentation video segmentation is the signicant step in almost all video processing systems where for a target input video v a segmentation vn is generated
this phase includes a pre processing step to eliminate undesirable frames
in the pre processing step i frames are extracted from the input video ii histogram equalization is applied iii resizing of images to a default size iv ing an unnecessary part from all images extract frames per second and vi total no of frames from the input video
each video of ute dataset v is partitioned into n frames seconds long for fair comparison with related work where n is the total number of frames
casings second long is utilized outline extraction technique to compare with existing work
neeraj baghel et al
fig
global feature extraction a processed query image b salient region on query image c processed video frame d salient region on video frame
is calculating cumulative distance between the query image and input video
a cumulative distance is a lection of various distance calculated between features of the query image and frames of the input video
the proposed work have four distance function for i ferent objects ii location of the objects iii size of the objects iv salient region
v v q v v denoted as the cumulative distance of ith frame distance function as a feature of query image as q and feature of the input video frame as f v
also proposed the cumulative distance function as q q q v where q is an absolute value of the dierence tween the number of dierent objects in the query age and video frame q v is a summation of the dierence between location of a similar object in the query image and video frame q is a tion of the dierence between size of a similar object in the query image and video frame and then divide by total number of pixel and q is a summation of dierence between a salient region of the query image and video frame and then divide by total no of pixel
v f v the second step is to generate a distance matrix with the collection of cumulative distance function with respect to every frame of the input video
distance trix d is described as d


dn where d is the cumulative distance between the query image and input video
the third step is to calculate the values of p q and r as described p t x q xx t and r ddt where diag is a diagonal matrix x is a feature matrix d is a distance matrix
the fourth step is to nd values of selection score which lies between and with the help of gence of concave convex procedure by minimizing the optimized equation of loss function in tth iteration
fig
local feature extraction a processed query image b object detection on query image c object extracted from query image d processed video frames e object detection on video frames object extracted from video frame
suppression threshold

finally the proposed method uses these various object properties as local features
for global features the proposed method is using the salient region
the objective of the salient region is to nd important regions in the video frames color bination and depth of that region
recognize the portant region of image from the hsv color model
author has used hsv color model to nd salient region in stereo images by using subtraction of left and right images with saturation and value components of hsv color space
to obtain a salient region in mono age proposed methord used the hsv color model using functions
sr s where v is value plane s is the saturation plane is parameter threshold to select the region exp is nential function and sr is the salient region
the value of is always lie between to and the value is as much close to function select more salient pixel from the age
however it may skip some pixel which may salient but due to some noise its value is less
the value of empirically by testing this on various images of gqi dataset to be



frame scoring in frame scoring various steps are used to generate the similarity score of the input video frame
first the step image conditioned keyframe based video summarization using object detection fig
salient region dierence a salient region on query image b salient region on video frame c dierence of salient region between query image and video frames
fig
keyframe selection a video frames selection trix c selected keyframe

key frame selection keyframe selection is the last step in video tion where the important frame is selected from input video and generates the output video of those frames
a keyframe is selected on the basis of similarity score
in the proposed method rst selection matrix is ated which takes the decision of selection of keyframe
the selection matrix is generated on the basis of the selection score where a threshold is applied to the lection score
if the value is greater than the threshold then considered as keyframe otherwise that frame is discarded
the threshold is calculated as the standard deviation as the values are gaussian distributed
fore selection matrix z is dened as z zm where is a standard deviation from zm
at last summarized video is generated by lated keyframe dened by selection matrix which can be dened as vs x t z where x is a feature matrix and z is a selection matrix

implementation details the proposed video summarization system is mented in python
at gtx ti gb card with gb on a single server work station
in the rst part object detection is used
the output for a frame consists of three lists each representation are dimensional vectors
yolo makes predictions at scales that square measure exactly given by sampling the size of the input image by thirty two teen and eight severally
in the second module if the prediction condence score is generated if more than
then the object is extracted from that frame
then output of these three feature vectors and object category is matched with other frames of a given video
with condence old
and non maximal suppression threshold

in the third module accumulate the frame sequence and generate summarized video
during the testing phase the proposed method obtain the feats consist of frameid indices class classid condence of predicted shot and score for each video shot
experimental results and discussion in this chapter the proposed method has been dated the approach using ut egocentric ute dataset
the proposed method successfully resolved the issues of i user preference ii recognize important objects in frames according to user preference and selecting keyframe in daily life videos with dierent illumination conditions
experiments is performed on ut egocentric dataset shows the eciency of the proposed method

evaluation metrics in qc dpp the mapping between the predicted summary and the ground truths is proposed with partite graph bipartite using weight matching
also gives a similarity function between two video shots by using intersection over union iou on corresponding ideas to calculate the performance
the iou is dened using edge weights therefore the predicted summary neeraj baghel et al
fig
result analysis score of the proposed work with seq dpp sh dpp qc dpp and ground truth belongs to dierent sides of tite graph
precision pre recall rec and score area unit computed as follows
total no of frames in summary
total no of frames in ground truth
dist distance summary ground truth pre rec pre rec
quantitative results the proposed approach is compared with all other works which have been applied to ute dataset
sion recall and score parameters are taken for parison for all four videos are shown in table tively
the proposed method achieved
average score for ute dataset
it can be observed that the proposed approach outperforms the existing approach by

such substantial improvement in the mance indicates the superiority of the proposed method by using an object detection method with other visual information and the image query
the rest four works are based on architecture which can take a long time to learn temporal relations among video shots and queries
however the proposed work facilitates key short tion using relation between video and image query
to obtain a result comparison fast with ground truth values the proposed method has generated an vector list along with every summarized video
so that it can compare this vector list with ground truth values
the results and analysis of query conditioned video summarization proposed method using parameters pi precision ri recall and score
the process time is also calculated for all four videos and compared with actual timing of videos
proposed method is
times less than actual time of video
hence by using this method we can summarize a video in very less time as compared to manually by watching
fig
the average processing time on ute dataset
actual time is the time length of videos
process time is the time taken by proposed method for generating video summary
that compact and representative summaries can be nd by the proposed method
conclusion
qualitative results the visual results obtained by the proposed method are shown in fig

the proposed method uses an image query that contains two dierent objects person and car using or operation
the axis represents the shot to video
ground truth is denoted by blue lines for the given user query while the green lines in the bottom represents predicted key shots of the proposed method
note that predicted summaries can be related to one or more details given a user query
it can observe in the proposed work a mathematical model is sented to minimize redundancy based on the similarity score between the query image and input video to tain the summarized video
the mathematical model contains a method to calculate the summary variance to reduce redundancy between frames
a mathematical formula to calculate distance score between a query age and video frames is also presented
the presented work dened a loss function for obtaining a selection score for frames in a video
the proposed method sumes that the distribution of the keyframes is based image conditioned keyframe based video summarization using object detection table comparison of proposed work with previous work and a memory network based approach
in proceedings of the ieee conference on computer vision and pattern recognition pages

ke zhang wei lun chao fei sha and kristen man
summary transfer exemplar based subset tion for video summarization
in proceedings of the ieee conference on computer vision and pattern recognition pages

yen liang lin vlad i morariu and winston hsu
marizing while recording context based highlight tion for egocentric videos
in proceedings of the ieee ternational conference on computer vision workshops pages

yujia zhang michael kampmeyer xiaodan liang min tan and eric p xing
query conditioned three player arxiv adversarial network for video summarization
preprint


aditya khosla raay hamid chih jen lin and neel sundaresan
large scale video summarization using image priors
in proceedings of the ieee conference on computer vision and pattern recognition pages

gunhee kim leonid sigal and eric p xing
joint marization of large scale collections of web images and videos for storyline reconstruction
in proceedings of the ieee conference on computer vision and pattern recognition pages

ke zhang wei lun chao fei sha and kristen man
video summarization with long short term ory
in european conference on computer vision pages
springer

zheng lu and kristen grauman
story driven rization for egocentric video
in proceedings of the ieee conference on computer vision and pattern tion pages

yale song jordi vallmitjana amanda stent and jandro jaimes
tvsum summarizing web videos using titles
in proceedings of the ieee conference on puter vision and pattern recognition pages

ting yao tao mei and yong rui
highlight detection with pairwise deep ranking for rst person video marization
in proceedings of the ieee conference on computer vision and pattern recognition pages

boqing gong wei lun chao kristen grauman and fei sha
diverse sequential subset selection for supervised video summarization
in advances in neural information processing systems pages

yujia zhang michael kampmeyer xiaodan liang min tan and eric p xing
query conditioned three player adversarial network for video summarization
arxiv preprint

fig
visualization results for the proposed method with shot number on the axis in video
ground truths and predicted key shots are shown in blue and green lines respectively
these results are for the query person or car
on gaussian distribution
thus an adaptive method to compute threshold is dened for the selection of keyframes using standard deviation
the object based query image oqi dataset is prepared for a selection of query images
the proposed model successfully solved the issues of i user preference ii recognize important frames and selecting that keyframe in daily life videos with dierent illumination conditions
the proposed method achieved a
average score for the ute dataset
a video pre processing process which makes use of very low level features to eciently locate undesirable frames then uses these to compute optimal segments
the processing time is
times less than the actual time of video future work will be a cus on increasing local and global features to improve user subjectivity
conicts of interest the authors declare no ict of interest
references
ravi kansagara darshak thakore and mahasweta joshi
a study on video summarization techniques
ternational journal of innovative research in computer and communication engi neering

ba tu truong and svetha venkatesh
video tion a systematic review and classication
acm actions on multimedia computing communications and applications tomm

aidean sharghi jacob s laurel and boqing gong
query focused video summarization dataset evaluation neeraj baghel et al

jingjing meng hongxing wang junsong yuan and peng tan
from keyframes to key objects video marization by representative object proposal selection
in proceedings of the ieee conference on computer vision and pattern recognition pages

yujia zhang xiaodan liang dingwen zhang min tan and eric p xing
unsupervised object level video marization with online motion auto encoder
pattern recognition letters

behrooz mahasseni michael lam and sinisa todorovic
unsupervised video summarization with adversarial lstm networks
in proceedings of the ieee conference on computer vision and pattern recognition pages

zhong ji yaru ma yanwei pang and xuelong li
query aware sparse coding for multi video tion
arxiv preprint


harrie oosterhuis sujith ravi and michael arxiv preprint semantic video trailers
dersky



aidean sharghi boqing gong and mubarak shah
query focused extractive video summarization
in ropean conference on computer vision pages
springer

arun balajee vasudevan michael gygli anna volokitin and luc van gool
query adaptive video summarization via quality aware relevance estimation
in proceedings of the acm international conference on multimedia pages
acm

tsung yi lin michael maire serge belongie james hays pietro perona deva ramanan piotr dollar and c lawrence zitnick
microsoft coco common objects in context
in european conference on computer vision pages
springer

a kanehira l van gool y ushiku and t harada
viewpoint aware video summarization
in proceedings of the ieee conference on computer vision and tern recognition salt lake city ut usa pages

alan l yuille and anand rangarajan
the in advances in neural convex procedure cccp
mation processing systems pages

alan l yuille and anand rangarajan
the convex procedure
neural computation

joseph redmon and ali farhadi
an incremental improvement
arxiv preprint


zhong liu weihai chen yuhua zou and cun hu
gions of interest extraction based on hsv color space
in ieee international conference on industrial matics pages
ieee

