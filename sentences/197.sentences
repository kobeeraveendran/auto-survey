r a l c
s c v
v i x r a textkd gan text generation using knowledge distillation and generative adversarial networks md
akmal haidar and mehdi rezagholizadeh
akmal
haidar mehdi

com huawei noah s ark lab montreal research center montreal canada abstract
text generation is of particular interest in many nlp plications such as machine translation language modeling and text summarization
generative adversarial networks gans achieved a markable success in high quality image generation in computer vision and recently gans have gained lots of interest from the nlp nity as well
however achieving similar success in nlp would be more challenging due to the discrete nature of text
in this work we duce a method using knowledge distillation to eectively exploit gan setup for text generation
we demonstrate how autoencoders aes can be used for providing a continuous representation of sentences which is a smooth representation that assign non zero probabilities to more than one word
we distill this representation to train the generator to synthesize similar smooth representations
we perform a number of periments to validate our idea using dierent datasets and show that our proposed approach yields better performance in terms of the bleu score and jensen shannon distance jsd measure compared to traditional gan based text generation approaches without pre training
keywords text generation generative adversarial networks edge distillation
introduction recurrent neural network rnn based techniques such as language models are the most popular approaches for text generation
these rnn based text generators rely on maximum likelihood estimation mle solutions such as teacher forcing i
e
the model is trained to predict the next item given all previous observations however it is well known in the literature that mle is a simplistic objective for this complex nlp task
mle based methods suer from exposure bias which means that at training time the model is exposed to gold data only but at test time it observes its own predictions
however gans which are based on the adversarial loss function and have the generator and the discriminator networks suers less from the mentioned problems
gans could provide a better image generation framework comparing to the traditional mle based methods and achieved substantial success in the eld of computer vision for generating realistic and sharp images
this great success motivated researchers to apply its framework to nlp applications as well
authors suppressed due to excessive length gans have been exploited recently in various nlp applications such as machine translation dialogue models question answering and natural language generation
however applying gan in nlp is challenging due to the discrete nature of the text
consequently back propagation would not be feasible for discrete outputs and it is not straightforward to pass the gradients through the discrete output words of the generator
the existing gan based solutions can be categorized according to the technique that they leveraged for handling the problem of the discrete nature of text reinforcement learning rl based methods latent space based solutions and approaches based on continuous approximation of discrete sampling
several versions of the rl based techniques have been introduced in the literature including gan maskgan and leakgan
however they often need training and are computationally more expensive compared to the methods of the other two categories
latent space based solutions derive a latent space representation of the text using an ae and attempt to learn data manifold of that space
another approach for generating text with gans is to nd a continuous approximation of the discrete sampling by using the gumbel softmax technique or approximating the non dierentiable argmax operator with a continuous function
in this work we introduce textkd gan as a new solution for the main tleneck of using gan for text generation with knowledge distillation a technique that transfer the knowledge of softened output of a teacher model to a student model
our solution is based on an ae teacher to derive a smooth tation of the real text
this smooth representation is fed to the textkd gan discriminator instead of the conventional one hot representation
the generator student tries to learn the manifold of the softened smooth representation of the ae
we show that textkd gan outperforms the conventional gan based text generators that do not need pre training
the remainder of the paper is organized as follows
in the next two sections some preliminary background on generative adversarial networks and related work in the literature will be reviewed
the proposed method will be presented in section
in section the experimental details will be discussed
finally section will conclude the paper
background generative adversarial networks include two separate deep networks a generator and a discriminator
the generator takes in a random variable following a distribution and attempt to map it to the data distribution
the output distribution of the generator is expected to converge to the data distribution during the training
on the other hand the discriminator is expected to discern real samples from generated ones by outputting zeros and ones respectively
during training the generator and discriminator generate samples and classify them respectively by adversarially aecting the performance of each other
in this regard an adversarial loss function is employed for training min g max d v d g title suppressed due to excessive length this is a two player minimax game for which a nash equilibrium point should be derived
finding the solution of this game is non trivial and there has been a great extent of literature dedicated in this regard
fig
simplistic text generator with gan as stated using gans for text generation is challenging because of the discrete nature of text
to clarify the issue figure depicts a simplistic ture for gan based text generation
the main bottleneck of the design is the argmax operator which is not dierentiable and blocks the gradient ow from the discriminator to the generator
min g
knowledge distillation knowledge distillation has been studied in model compression where knowledge of a large cumbersome model is transferred to a small model for easy deployment
several studies have been studied on the knowledge transfer technique
it starts by training a big teacher model or ensemble model and then train a small student model which tries to mimic the characteristics of the teacher model such as hidden representations it s output probabilities or directly on the generated sentences by the teacher model in neural machine translation
the rst teacher student framework for knowledge distillation was proposed in by introducing the softened teacher s output
in this paper we propose a gan framework for text generation where the generator student tries to mimic the reconstructed output representation of an auto encoder teacher instead of mapping to a conventional one hot representations

improved wgan generating text with pure gans is inspired by improved wasserstein gan iwgan work
in iwgan a character level language model is developed authors suppressed due to excessive length based on adversarial training of a generator and a discriminator without using any extra element such as policy gradient reinforcement learning
the generator produces a softmax vector over the entire vocabulary
the discriminator is responsible for distinguishing between the one hot representations of the real text and the softmax vector of the generated text
the iwgan method is described in figure
a disadvantage of this technique is that the discriminator is able to tell apart the one hot input from the softmax input very easily
hence the generator will have a hard time fooling the discriminator and vanishing gradient problem is highly probable
fig
improved wgan for text generation related work a new version of wasserstein gan for text generation using gradient penalty for discriminator was proposed in
their generator is a cnn network generating xed length texts
the discriminator is another cnn receiving tensors as input sentences
it determines whether the tensor is coming from the generator or sampled from the real data
the real sentences and the generated ones are represented using one hot and softmax representations respectively
a similar approach was proposed in with an rnn based generator
they used a curriculum learning strategy to produce sequences of gradually increasing lengths as training progresses
in rnn is trained to generate text with gan using curriculum learning
the authors proposed a procedure called teacher helping which helps the generator to produce long sequences by conditioning on shorter ground truth sequences
all these approaches use a discriminator to discriminate the generated softmax output from one hot real data as in figure which is a clear downside for them
the reason is the discriminator receives inputs of dierent representations a one hot vector for real data and a probabilistic vector output from the generator
it makes the discrimination rather trivial
aes have been exploited along with gans in dierent architectures for computer vision application such as aae ali and hali
similarly title suppressed due to excessive length aes can be used with gans for generating text
for instance an adversarially regularized ae arae was proposed in
the generator is trained in parallel to an ae to learn a continuous version of the code space produced by ae encoder
then a discriminator will be responsible for distinguishing between the encoded hidden code and the continuous code of the generator
basically in this approach a continuous distribution is generated corresponding to an encoded code of text
methodology aes can be useful in denoising text and transferring it to a code space encoding and then reconstructing back to the original text from the code
aes can be combined with gans in order to improve the generated text
in this section we introduce a technique using aes to replace the conventional one hot representation with a continuous softmax representation of real data for discrimination

distilling output probabilities of ae to textkd gan generator as stated in conventional text based discrimination approach the real and generated input of the discriminator will have dierent types one hot and softmax and it can simply tell them apart
one way to avoid this issue is to derive a continuous smooth representation of words rather than their one hot and train the discriminator to dierentiate between the continuous representations
in this work we use a conventional ae teacher to replace the one hot representation with softmax reconstructed output which is a smooth representation that yields smaller variance in gradients
the proposed model is depicted in figure
as seen instead of the one hot representation of the real words we feed the softened reconstructed output of the ae to the discriminator
this technique would makes the discrimination much harder for the discriminator
the gan generator student with softmax output tries to mimic the ae output distribution instead of conventional one hot representations used in the literature
fig
textkd gan model for text generation authors suppressed due to excessive length
why textkd gan should work better than iwgan suppose we apply iwgan to a language vocabulary of size two words and
the one hot representation of these two words as two points in the cartesian coordinates and the span of the generated softmax outputs as a line segment connecting them is depicted in the left panel of figure
as evident graphically the task of the discriminator is to discriminate the points from the line connecting them which is a rather simple very easy task
now let s consider the textkd gan idea using the two word language example
as depicted in figure right panel the output locus of the gan decoder would be two red line segments instead of two points in the one hot case
the two line segments lie on the output locus of the generator which will make the generator more successful in fooling the discriminator
fig
locus of the input vectors to the discriminator for a two word language model left panel iwgan right panel textkd gan

model training we train the ae and textkd gan simultaneously
in order to do so we break down the objective function into three terms a reconstruction term for the ae a discriminator loss function with gradient penalty an adversarial cost for the generator
mathematically min min min ww min ww min expx min ezpz
title suppressed due to excessive length these losses are trained alternately to optimize dierent parts of the model
we employ the gradient penalty approach of iwgan for training the discriminator
in the gradient penalty term we need to calculate the gradient norm of random samples x px
according to the proposal in these random samples can be obtained by sampling uniformly along the line connecting pairs of generated and real data samples x px px xgen the complete training algorithm is described in
algorithm textkd gan for text generation
require the adam hyperparameters the batch size m
initial ae parameters encoder decoder discriminator parameters and initial generator parameters for number of training iterations do px and compute code vectors ci ae training sample and reconstructed text backpropagate reconstruction loss
update with
train the discriminator for k times do
px and sample sample compute generated text backpropagate discriminator loss
update with w
n i
end for train the generator sample compute generated text backpropagate generator loss
update with
px and sample end for n i
experiments
dataset and experimental setup we carried out our experiments on two dierent datasets google billion benchmark language modeling and the stanford natural language inference snli
our text generation is performed at character level with a sentence
statmt
org lm
stanford
edu projects authors suppressed due to excessive length length of
for the google dataset we used the rst million sentences and extract the most frequent characters to build our vocabulary
for the snli dataset we used the entire preprocessed training data which contains sentences in total and the built vocabulary has characters
we train the ae using one layer with lstm cells for both the encoder and the decoder
we train the autoencoder using adam optimizer with learning rate

and

for decoding the output from the previous time step is used as the input to the next time step
the hidden code c is also used as an additional input at each time step of decoding
the greedy search approach is applied to get the best output
we keep the same cnn based generator and discriminator with residual blocks as in
the discriminator is trained for times for gan generator iteration
we train the generator and the discriminator using adam optimizer with learning rate

and

we use the bleu n score to evaluate our techniques
bleu n score is calculated according to the following equation bleu n bp exp n where pn is the probability of n gram and wn n
we calculate bleu n scores for n grams without a brevity penalty
we train all the models for iterations and the results with the best bleu n scores in the generated texts are reported
to calculate the bleu n scores we generate ten batches of sentences as candidate texts i
e
sentences character sentences and use the entire test set as reference texts

experimental results the results of the experiments are depicted in table and
as seen in these tables the proposed textkd gan approach yields signicant improvements in terms of and scores over the iwgan and the arae approaches
therefore softened smooth output of the decoder can be more useful to learn better discriminator than the traditional one hot representation
moreover we can see the lower bleu and less improvement for the google dataset compared to the snli dataset
the reason might be the sentences in the google dataset are more diverse and complicated
finally note that the text based one hot discrimination in iwgan and our proposed method are better than the traditional code based arae technique
some examples of generated text from the snli experiment are listed in table
as seen the generated text by the proposed textkd gan approach is more meaningful and contains more correct words compared to that of iwgan
we also provide the training curves of jensen shannon distances jsd tween the n grams of the generated sentences and that of the training real ones
com aboev arae tf tree master title suppressed due to excessive length table results of the bleu n scores using million sentences from billion google dataset model iwgan arae

textkd gan






table results of the bleu n scores using snli dataset model iwgan arae

textkd gan






table example generated sentences with model trained using snli dataset textkd gan iwgan two people are standing on the s the people are laying in angold a man is walting on the beach a woman is standing on a bench
a man is looking af tre walk aud people have a ride with the comp a woman is sleeping at the brick a man standing on the beach the man is standing is standing four people eating food
a man is looking af tre walk aud the dog is in the main near the the man is in a party
two members are walking in a hal these people are looking at the the people are running at some l a boy is playing sitting
a black man is going to down the in figure
the distances are derived from snli experiments and calculated as in
that is by calculating the log probabilities of the n grams of the generated and the real sentences
as depicted in the gure the textkd gan approach further minimizes the jsd compared to the literature methods
in sion our approach learns a more powerful discriminator which in turn generates the data distribution close to the real data distribution

discussion the results of our experiment shows the superiority of our textkd gan method over other conventional gan based techniques
we compared our technique with those gan based generators which does not need pre training
this explains why we have not included the rl based techniques in the results
we showed the power of the continuous smooth representations over the well known tricks to work around the discontinuity of text for gans
using aes in textkd gan adds another important dimension to our technique which is the latent space which authors suppressed due to excessive length c d fig
jensen shannon distance jsd between the generated and training tences n grams derived from snli experiments
a b c and represent the jsd for and grams respectively can be modeled and exploited as a separate signal for discriminating the generated text from the real data
it is worth mentioning that our observations during the experiments show training text based generators is much easier than training the code based techniques such as arae
moreover we observed that the gradient penalty term plays a signicant part in terms of reducing the mode collapse from the generated text of gan
furthermore in this work we focused on based techniques however textkd gan is applicable to the word based settings as well
bear in mind that pure gan based text generation techniques are still in a newborn stage and they are not very powerful in terms of learning semantics of complex datasets and large sentences
this might be because of lack of capacity of capturing the long term information using cnn networks
to address this problem rl can be employed to empower these pure gan based techniques such as textkd gan as a next step
conclusion and future work in this work we introduced textkd gan as a new solution using knowledge distillation for the main bottleneck of using gan for generating text which is title suppressed due to excessive length the discontinuity of text
our solution is based on an ae teacher to derive a continuous smooth representation of the real text
this smooth representation is distilled to the gan discriminator instead of the conventional one hot resentation
we demonstrated the rationale behind this approach which is to make the discrimination task of the discriminator between the real and generated texts more dicult and consequently providing a richer signal to the generator
at the time of training the textkd gan generator student would try to learn the manifold of the smooth representation which can later on be mapped to the real data distribution by applying the argmax operator
we evaluated textkd gan over two benchmark datasets using the bleu n scores jsd measures and quality of the output generated text
the results showed that the proposed textkd gan approach outperforms the traditional gan based text generation methods which does not need pre training such as iwgan and arae
finally we summarize our plan for future work in the following
we evaluated textkd gan in a character based level
however the mance of our approach in word based level needs to be investigated

current textkd gan is implemented with a cnn based generator
we might be able to improve textkd gan by using rnn based generators

textkd gan is a core technique for text generation and similar to other pure gan based techniques it is not very powerful in generating long sentences
rl can be used as a tool to accommodate this weakness
references
belghazi m
i
rajeswar s
mastropietro o
rostamzadeh n
mitrovic j
courville a
hierarchical adversarially learned inference
arxiv preprint

bengio y
louradour j
collobert r
weston j
curriculum learning
in proceedings of the annual international conference on machine learning
pp

acm
cer d
manning c
d
jurafsky d
the best lexical metric for phrase based statistical mt system optimization
in human language technologies the annual conference of the north american chapter of the association for putational linguistics
pp

association for computational linguistics
dumoulin v
belghazi i
poole b
mastropietro o
lamb a
arjovsky m
courville a
adversarially learned inference
arxiv preprint

fedus w
goodfellow i
dai a
m
maskgan better text generation via lling in the
arxiv preprint

goodfellow i
pouget abadie j
mirza m
xu b
warde farley d
ozair s
courville a
bengio y
generative adversarial nets
in advances in neural information processing systems
pp

gulrajani i
ahmed f
arjovsky m
dumoulin v
courville a
improved training of wasserstein gans
arxiv preprint

guo j
lu s
cai h
zhang w
yu y
wang j
long text generation via adversarial training with leaked information
arxiv preprint
authors suppressed due to excessive length
hinton g
vinyals o
dean j
distilling the knowledge in a neural network
arxiv preprint

hochreiter s
schmidhuber j
long short term memory
neural computation
j w
r
david z
a learning algorithm for continually running fully recurrent neural networks
neural computation
kim y
rush a
m
sequence level knowledge distillation
in emnlp
pp

kim y
zhang k
rush a
m
lecun y
al
adversarially regularized autoencoders for generating discrete structures
arxiv preprint

kusner m
j
hernndez lobato j
m
gans for sequences of discrete elements with the gumbel softmax distribution
arxiv preprint

li j
monroe w
shi t
ritter a
jurafsky d
adversarial learning for neural dialogue generation
arxiv preprint

liu c
w
lowe r
serban i
v
noseworthy m
charlin l
pineau j
how not to evaluate your dialogue system an empirical study of unsupervised evaluation metrics for dialogue response generation
arxiv preprint

makhzani a
shlens j
jaitly n
goodfellow i
frey b
adversarial coders
arxiv preprint

papineni k
roukos s
ward t
zhu w
bleu a method for automatic evaluation of machine translation
in acl
pp

press o
bar a
bogin b
berant j
wolf l
language generation with recurrent generative adversarial networks without pre training
arxiv preprint

rajeswar s
subramanian s
dutil f
pal c
courville a
adversarial tion of natural language
arxiv preprint

romero a
ballas n
kahou s
e
chassang a
gatta c
bengio y
fitnets hints for thin deep nets
in iclr
salimans t
goodfellow i
zaremba w
cheung v
radford a
chen x
proved techniques for training gans
in advances in neural information processing systems
pp

sutton r
s
mcallester d
a
singh s
p
mansour y
policy gradient methods for reinforcement learning with function approximation
in nips
pp

wu l
xia y
zhao l
tian f
qin t
lai j
liu t
y
adversarial neural machine translation
arxiv preprint

yang z
chen w
wang f
xu b
improving neural machine translation with conditional sequence generative adversarial nets
arxiv preprint

yang z
hu j
salakhutdinov r
cohen w
w
semi supervised qa with ative domain adaptive nets
arxiv preprint

yu l
zhang w
wang j
yu y
seqgan sequence generative adversarial nets with policy gradient
in aaai
pp

zhang y
gan z
fan k
chen z
henao r
shen d
carin l
adversarial feature matching for text generation
arxiv preprint

zhu y
lu s
zheng l
jiaxian g
weinan z
jun w
yong y
texygen a benchmarking platform for text generation models
arxiv preprint


