cite this paper

cite copy and paste a formatted citation or use one of the links to import into a bibliography manager
mla yadav chandra shekhar and aditi sharan
hybrid approach for single text document summarization using statistical and sentiment features
international journal of information retrieval research ijirr

apa yadav c
s
sharan a

hybrid approach for single text document summarization using statistical and sentiment features
international journal of information retrieval research ijirr
chicago yadav chandra shekhar and aditi sharan
hybrid approach for single text document summarization using statistical and sentiment features
international journal of information retrieval research ijirr no

harvard yadav c
s
and sharan a

hybrid approach for single text document summarization using statistical and sentiment features
international journal of information retrieval research ijirr pp

vancouver yadav cs sharan a
hybrid approach for single text document summarization using statistical and sentiment features
international journal of information retrieval research ijirr
oct
hybrid approach for single text document summarization using statistical and sentiment features abstract
summarization is a way to represent same information in concise way with equal sense
this can be categorized in two type abstractive and extractive type
our work is focused around extractive summarization
a generic approach to extractive summarization is to consider sentence as an entity score each sentence based on some indicative features to ascertain the quality of sentence for inclusion in summary
sort the sentences on the score and consider top n sentences for summarization
mostly statistical features have been used for scoring the sentences
we are proposing a hybrid model for a single text document summarization
this hybrid model is an extraction based approach which is combination of statistical and semantic technique
the hybrid model depends on the linear combination of statistical measures sentence position tf idf aggregate similarity centroid and semantic measure
our idea to include sentiment analysis for salient sentence extraction is derived from the concept that emotion plays an important role in communication to effectively convey any message hence it can play a vital role in text document summarization
for comparison we have generated five system summaries proposed work mead system microsoft system opinosis system and human generated summary and evaluation is done using rouge score
keywords summarization single document summarization sentiment analysis hybrid model

introduction text document summarization playing an important role in ir information retrieval because it condense a large pool of information into a concise form through selecting the salient sentences and discards redundant sentences or information and we termed it as summarization process
radev et al
in has defined a text summary as a text that is produced from one or more texts that convey important information in the original texts and that is no longer than half of the original text and usually significant less than that
as explained in automatic text document summarization is an interdisciplinary research area of computer science that includes ai artificial intelligence data mining statistics as well as psychology
we can classify text summarization in two ways by techniques abstractive summarization and extractive summarization
abstractive summarization is more human like a summary which is the actual goal of text document summarization
as defined by abstractive summarization needs three things as information fusion
sentences compression and reformation
abstractive summarization may contain new sentences phrases words even which are not present in the source document
although till now a lot of research in happened in the last decades in the area of nlp natural language processing nlg natural language generation so much computing power increased but still we are not near for abstractive summarization
the actual challenge is a generation of new sentences new phases along with produced summary must retain the same meaning as the same source document has
extractive summarization based on extractive entities entities may be sentence sub part of sentence phrase or a word
our work is focused on extractive based technique
in this paper we are proposing a hybrid method for single text document summarization which is linear combination of statistical features as used in and new kind of semantic feature i
e
sentiment analysis
the idea which is used in this paper has been derived from different papers like for statistical features and their collective sum obtained from centroid measure are taken from
to include sentiment analysis is derived from the concept that emotion plays an important role in communication to effectively convey any message hence it can play a vital role in text document summarization
outline of paper looks like in section we are presenting categorized literature work done in recent years section contains features used for summarization purpose section contain summarization algorithm and detail approach in section we are presenting description with statistical and linguistic statistic section showing some experiments and results in section is about conclusion

related work according to m
ramiz summarization in is defined as a three steps process analysis of text
as summary representation and produce an appropriate summary
eduard hovy and chin yew lin introduced summarist system to create a robust text summarization system system that works on three phases which can describe in form of an equation like summarization topic identification interpretation generation
a lot of research done in the direction of extraction based approaches
in extractive summarization the important the task is to find informative sentences a subpart of sentence or phrase and include these extractive elements into the summary
here we are presenting work done in two categories early work done and work done in recent years
in our views these are three works done initially that provides direction of text document summarization extractive explained below the early work in document summarization started on single document summarization by h
p
luhn he proposed a frequency based model in which frequency of word play crucial role to decide importance of any sentence in the given document
another work of p
baxendale had been introduced a statistical model based on sentence position
in his research he found that starting and ending sentences became salient sentences for summarization but this is not better for every document like scientific research paper but good for newspapers summarization
h
p
edmondson also proposed an effective technique for document summarization
at first edmonson designed some rules for manual extraction then rules were applied to technical documents
edmondson considers four features sentences position frequency of word presence of cue words and the skeleton of the document
the work was done almost manual
after these early work lot of work done in this discipline some are available in here in the next section we are presenting only work done in recent years
query focused summarization is a special case of document summarization in which summary purposely demands to be biased according to the user query
you ouyang et
al used svr support vector regression to calculate the importance of the sentences in a given document
another query focused summarization multi document summarization done by carbonell j
goldstein in
researcher done a lot of work in the multi document summarization field and many more like in considering this is as a global multi optimization problem which requires simultaneous optimization of more than one objective function
rasim m
alguliev in which the objective function is a weighted combination of content coverage and for redundancy objectives
in another work they proposed cdds based summarization with two objectives diversity and coverage
in summarization similarity evaluation among of sentences is a laborious task because of complex sentence structure and lack of extra information so ming che lee had been proposed transformed vector space model based on word net
due to improper ordering of information there is possibility that it can confuse the readers as well as can degrade the readability of the summary
to maintain the association and order of sentences danushka bollegala et
al defined four criteria chronology topical closeness precedence succession
these all four criteria are combined into a single criteria by using a supervised learning approach
redundancy can be defined as a multiplicity of sentences sub sentences or information
coverage and redundancy are reciprocal to each other
summarization objective is maximum coverage and minimum redundancy
kamal sarkar gave a simple approach to include a sentence in summary one by one based on modified cosine similarity threshold
to include sentences in the summary system first selects the most top rank sentence include it in the summary and this process is repeated for remaining sentences
next sentence is included in the summary if similarity between sentences and summary is less than some threshold otherwise the sentence is not included in the summary and algorithm stops when required summary length is reached
we are using this model in our implementation to handle redundancy
another model mmr is popularly used especially in with a given query to reduce redundancy
the mmr maximal marginal relevance criteria strives to reduce redundancy while maintaining query relevance in re ranking retrieved documents and in selecting appropriate passages for text summarization
this technique gives better result for multi document summarization
rasim m
alguliev et
al proposed an unsupervised text summarization model which can be used for single as well as multi document summarization
in simple terms the problem is treated as a multi objective problem where the objective is to optimize three objectives relevance redundancy and length
to find a good summary lot of work done but to decide the quality of the summary still a challenging task
research is done by goldstein in he conclusion that even human judgment of the quality of a summary varies from person to person only little overlap among the sentences picked by people human judgment usually does find concurrence on the quality of a given summary
hence it is sometimes difficult to judge the quality of the summary
for evaluation most researcher use the recall oriented understudy for gisting evaluation rouge introduced by lin and this has been officially adopted by duc for summarizer evaluation
rouge compares system generated summary with different model summaries
it has been considered that rouge is an effective approach to measure document summarizes so widely accept
rouge measures overlap words between the system summary and standard summary gold summary human summary
overlapping words are measured based on n gram co occurrence statistics where n gram can be defined as the continuous sequence of n words
multiple rouge metrics has been defined for different value of n and different models like lcs weighted
standard rouge n is defined by here n stands for the length of the n gram gram is the number of n grams present in the reference summaries and the maximum number of n grams co occurring in the system summary the set of reference summaries is countmatch n gram rouge measures generally gives three basic score precision recall and f score
since score is not a sufficient indicator of summarizer performance so another variation of rouge is rouge n rouge l rouge w rouge s rouge su
in our evaluation we are using fourteen rouge measure l w s and su
for other variation kindly follow

proposed model for text document summarization in this section we are presenting features used in our sentence selection approach in section
and detailed approach in section

we are proposing a hybrid model for salient sentence extraction for single text document summarization based on two types of features statistical based features i
e
location frequency tf idf aggregate similarity centroid and semantic based feature sentiment


features statistical features used a
the location feature
p
baxendale in introduced a feature based on sentence position
although his work was almost manual but later on this measure used widely in sentence scoring he proposed that leading sentences of an article are important
model which we are using given below where n is total number of sentences
the used model is where i n and b
the aggregation similarity feature
kim et al
defined aggregate similarity as the score of a sentence is as the sum of similarities with other all sentence vectors in document vector space model
it is given by sim si sj wik
wjk score si sim si sj where wik is defined as the binary weight ok kth word in ith sentence
similarity measure plays an important role in text document summarization even studied different similarity measure affects the outcome
in our implementation using cosine similarity that is widely used
the cosine measure between two sentences

wim and
wjm
standard cosine similarity measure gives by following formula which is used in our implementation is below


i to n c
frequency feature
the early work in document summarization started on single document summarization by h
p
luhn at ibm in the
luhn proposed a frequency based model frequency of word play a crucial role to decide the importance of any word or sentence in a given document
in our method we are using the traditional method of tf idf measure is defined as below i
e
tf stands for term frequency idf for inverse document frequency
log where tfi is the term frequency of ith word in the document nd represents total number of documents and idfi is the document frequency of ith word in the whole data set
in our implementation to calculate importance of word wi for tf we considering the sentence as a document and for idf entire document as a data set
d
centroid feature
radev et
al defined centroid as a centroid is a set of words that are statistically important to a cluster of documents
as such centroids can be used both to identify salient sentences in a cluster and classify relevant documents
the centroid score ci for sentence si is computed as the sum of the centroid scores cw i of all words appeared in the particular sentence
sentiment feature or semantic feature used e
sentiment feature
in previous sections we mentioned statistical measures used by us
we are calling this feature as a semantic feature because in this a set of things are related to one other
semantic summary generation may be done using shallow level analysis and deep level analysis as defined by
in shallow approach to most analysis done on sentence level is syntactic but important to note that word level analysis may be semantic level and in deep analysis at least a sentential semantic level of representation is done
so our approach i
e
sentiment feature is semantic and low level analysis because at the entity level
for finding sentiment score for a sentence fist we find all entities present in a sentence then find sentiment score of each entity and then do sum of all entity sentiment score i
e
sentiment strength
if sentiment of entity is neutral then we scorning it as if entity sentiment is positive then considering as same and adding to find the total score of a sentence but if sentiment score is negative we multiplying it by to covert in positive score then adding this score to find total score
reason for considering negative score to positive score is that we are interested only in sentiment strength which may be positive or negative i
e
if sentiment score of an entity if
it means sentiment of entity is negative and strength is

detail procedure are explained in section
s fifth feature
here a representing mode a i
e
a a

summarization procedure our summarization approach is based on salient sentence extraction
the importance of any sentence is decided by the combined score given by the sum of statistical measures and semantic measure
in next step we are explaining our approach algorithm used in this paper
basically work of summarization can be dived in three pass sentence scoring sentence extraction and evaluation


algorithm sentence scoring according linear combinations of different measures
salient sentence extraction summary generation
pass evaluation of summary
pass sentence scoring input documents output scored sentences step score the sentence given with different measures
outcome is m n matrix m no
of sentences n no
of measures aggregate cosine similarity position sentiment of sentence centroid score of sentence tfidf
step normalized columns of matrix step add all the features for every sentence we calling this sum as a score of the sentence
step sort according to score a highest score representing most significant sentence
pass algorithm for redundancy input number of sentences descending according to total score output extracted sentences parameter initialization empty summary similarity threshold l required step summary scored sentence step for to number of sentences step rearrange summary sentences as given in source document for maintain cohesiveness
if ith sentence and length summary l then summary ith sentence length summary
human and our proposed algorithm
pass evaluation of summary input different summaries as standard summaries and peer summaries output precision recall and f score step generate different summary different length using mead microsoft opinosis human for experiment model summary human generated summary peer summary mead microsoft opinosis our proposed method step used rouge n to rouge l rouge w we set
rouge s rouge su measure model summary mead microsoft opinosis peer summary our proposed algorithm for experiment to find precision recall and f measure


detailed approach description here we are describing detail approach used as the procedure described above
pass
sentence scoring and extraction in algorithm defined in the previous section most things are covered and gives the main idea of algorithm still some micro points are needs to specify
pass is the sum of the linear combination of five different measures four are statistical dependent i
e
aggregate similarity position tf idf centroid and fifth measure is semantic dependent i
e
sentiment
first feature is position of sentences position is an important indicator for important sentence and it has been analyzed that first or leading sentences mostly contains important information
the n
position score for some sentence index to are given in s second column
second feature tf idf approach we are using the standard formula as defined in the previous section
normalized tf idf score are given in third columns
third feature is an aggregate similarity cosine score of a sentence vector can be calculated as the sum of similarities with other all sentence vectors in document vector space model
the significance of this is to find a sentences which are highly similar to all other sentences
after representing all sentences in a vector space and then find vector cosine similarity with all other sentences as defined standard formula in the section
normalized aggregate cosine similarity in column four
since other scores centroid position sentiment are between so we need to normalized score
normalization of values means adjusting to values measured on different scales to one notionally common that removes chance to be bias w

t
some values
in our implementation we are just using column normalization instead of matrix normalization
normalization of a column vector
xn is done using equation
where xi is the ith element in the column and n is the size of the column



b


let a is a given matrix which size is and column one and two has does have values between then we are doing normalization of only column one and two but not third column and b is the give normalized matrix in our case
fourth feature is centroid based d
r
radev defined as centroid as a set of words that are statistical important to a cluster of documents
in our approach using mead centroid score output as our input
the centroid value of a sentence is given by summation of each word centroid score present in the sentence
fifth feature is sentiment score this is a novelty in our work to find this feature we depends on alchemy api which is available at
alchemyapi

our consideration is that it is finding sentiment score is a semantic approach and fall under shallow level approach as defined in section

for any sentence or words we can define three kind of sentiment neutral negative and positive
neutral sentiment value mean that words or that sentence sentiment score is zero most important to note that it is easy to find sentiment score based on cue word like good bad pleasant
but still due to so much complexity in text words limitation of nlp
it is not possible to find correct sentiment score sometime even it is also not possible to detect sentiment due to hidden sentiments document naac accredited jnu with the cgpa of
on four point scale of a grade highest grade by naac and sentiment of this is neutral
document jnu ranked in top in times higher education asia and brics top ranking and sentiment of this document is positive and score is

naac stands for national assessment and accreditation council naac and brics stands for five nations brazil russia india china and south africa here document and document both representing positive news about jnu
but the sentiment of document is neutral and sentiment of document is positive with
score
we still have to discover approach which can find correct sentiment hidden sentiment
some results are displayed in table with sentiment results
in our implementation to find sentiment score of a sentence we are using alchemy api first finding all entities present in the sentence and their sentiment score then we add all entity sentiment score
for example consider document number meanwhile bjp spokesperson prakash javadekar has said that party president


has five entities as follow prakash javadekar person name
rajnath singh person name
uttarakhand state county
bjp company
president jobtitle
triplet x y z representing x is entity name y is entity type z sentiment score
and to give sentiment score of sentence we add all so sentiment score
for sentence
but if we see the table result in row and column sentiment score the value is

here we are considering only positive sentiment scores if entity sentiment score is negative then by multiplying to convert it into positive score
the obvious goal of this procedure is to give equal importance if magnitude is same
let consider one childhood story the fox and the grapes in figure
figure the fox and the graps story if here we consider two sentences given below document and as document both are important in the story and about same things grapes
document just these sweat and juicy grapes to quench my thirst and document they re probably sour anyway
with alchemy system if we find a sentiment of both these sentences then the sentiment of document is positive and the score is
where the sentiment of document is negative with

for us only magnitude is important reasons to consider as value are are interested to find sentiment strength it may be negative or positive and both are important for us and if we will add negative score to find total score then value will reduce
in next step we are finding the total score of a sentence by adding all scores
the total score can be represented by equation given below
in our implementation
detail result of individual score is given in table and last column is total score of all scores

table different features scores and total score for sentences pass
redundancy to remove redundancy we used same model as proposed by sarkar in which the top most sentence according to total score defined in equation is add in summary we add next sentence in the summary if similarity is less than threshold
algorithm is described in section input in this pass is a number of sentences which are sorted according to descending total score
we need to initialize some parameter to get the desired summary parameter like summary initially empty given similarity threshold and l for desired length summary
even in our system l means maximum length of desired summary but due to limitation here length of sentences we can guaranteed minimum maximum length summary
we will add the next sentence in the summary if still summary length is less than l and similarity new sentence summary
the output of this step is a summary with minimal redundancy and length nearly equal to l but the position of the sentence is zigzag that lost the sequence and cohesiveness
to maintain the sequence we need to rearrange the sentences according to given in initial index
in table we are representing the summary generated by our system in which similarity threshold is
and desired summary length is
we can define arbitrary l in a number of words or percentage of summary required
here we chosen small
if we put large like
or
then the sentences which are in coming in summary will depend only on the total score as in table
in other words the summary is only depended on totalscores as shown in table but our objective is also to minimize redundancy
note before calculating sentence summary we are eliminating stopwords stopwords play a big role to increase the similarity between two sentences
with different stopwords list we will get different similarity score
mead microsoft and our model generated summary with different length are shown below in table
the truth is we do have the detail of microsoft generated a summary
this summarizer is inbuilt inside microsoft office package
when we observed then find little unhappy that microsoft summarizer is not reducing redundancy sentence and are almost similar sentences see table
pass
evaluation goldstein in he concluded two things even human judgment of the quality of a summary varies from person to person human judgment usually does find concurrence on the quality of a given summary hence it is difficult to judge the quality of a summary
for evaluation of any summary we need two summary first one is a system generated summary and other summary is user generated model summary or standard summary
to generate different model summary we used three approaches we given our text data set to person and tell them to write a summary in about to to words we generate summary by mead tool in score with and third model summary is generated by summary given in figure microsoft system
combination approach centroid position taking linear and this we of sent no
position scoretf idfaggregate cosine sim
centroid scoresentiment scoresum of





















































































to evaluate summary we are using rouge evaluation package
rouge is adopted by duc for official evaluation metric for both single text document summarization and multi document summarization
rouge finds recall precision and f score for evaluation results
based on n gram co occurrence statistic it rouge measures how much the system generated summary machine summary overlaps with the standard summary human summaries model summary
where an n gram is a contiguous sequence of n words
in our evaluation we are adopting different measures of rouge as rouge n to rouge w rouge l rouge s and rouge su

corpus description in june was a multi day cloudburst centered on the north indian state of uttarakhand caused devastating floods along with landslides and became the country worst natural disaster
though some parts of western nepal tibet himachal pradesh haryana delhi and uttar pradesh in india experienced the flood over of the casualties occurred only in uttarakhand
as of july according to figures provided by the uttarakhand government more than people were presumed
corpus is self designed taken from various newspapers ex
the hindu times of india
this dataset is also published in paper c
s
yadav
here we are showing some statistically and linguistic statics about our data set used


statistical statistics total no
of sentences in document length of document after stop word removed total number of distinct words minimum sentence length words maximum sentence length words average sentence length is

in our experiment we used sql stopword list which is available at
by seeing the figure we can interpret that sentences are between length and and sentences are between length and
figure sentence length y axis vs sentence number x axis

linguistic statistics nn nnp nns dt jj jjr jjs vb vbn vbd vbz vbg vbp where different abbreviation stands for nn noun singular mass nns nounplural nnp proper noun singular nnps proper noun plural vb verb vbd verb past tense vbg verb gerund vbn verb past participle vbp verb non person singular vbz verb person singular jj adjective jjr adjective comparative jjs adjective superlative dt determinant
note means x is entity type and is its count

experiment and results in this section we are presenting two experiment done on mentioned dataset


experiment as explained in section s pass we created types of model summary human summary via we gave data set to persons to summarize it based on their experience with instruction to summarize it within to words length
due to limitations and user experiences the generated summary varies from to words length mead microsoft and opinosis system
different system generated summary are given in table
since opinosis summarizer is abstractive type in figure we are giving summarization result length summary generated by opinosis system
table presenting different summaries generated by different systems
table our system generated summary using proposed algorithm
table microsoft system generated summary
table mead system generated summary
figure opinosis generated summary in the first experiment we took our own summary generated by algorithm discussed in section as system generate summary and another summary as model summary
in next step we find different rouge scores to rouge l rouge w where
rouge s and rouge as defined by
rouge scores is given by formula defined in section
it measures things recall precision and f score for any system generated summary and model summary or reference summary
recall and precision are given in a slightly different way as defined by
we are comparing our system generated summary with other as model summary same length summary
result of this is given is table table and table for length respectively due to limitation of space we providing only three tables
figure showing f measure with different model summaries of length of nearly and our summary length is nearly
in simple term we can define high precision means that an algorithm retrieved substantially more relevant than irrelevant and high recall means that an algorithm return most of the relevant
from figure and for summary length it is clear that we are getting high precision f score w

t
mead reference summary and high recall w

microsoft generated summary
table summary generated by our algorithm considered as system summary another summary as a model summary







































































































table summary generated by our algorithm as system summary another summary as model summary table summary generated by our algorithm as system summary another summary as model summary summary






















































































































































































































































































figure precision curve figure recall curve figure showing f score

experiment in this experiment we comparing different system generated summary w

t
human summary or in other words here model reference summary is human generated summary and other summaries are system generated summary i
e
mead microsoft opinosis our algo are system generated summary
result are shown in table and table
table is representing different rouge scores for the summary length of
table summary generated by different as system summary human generated summary as model summary
table summary generated by different system considered as system summary human generated summary as model summary from table we can say that
we are getting high f score comparison to mead microsoft system and opinosis system
except rouge w in mead s and opinosis s rouge w
f score is represent in

we are getting high precision compare to mead and opinosis but microsoft system leading in rouge l rouge w rouge s rouge su only

we are getting high recall comparison to mead in to and higher recall comparison to opinosis and microsoft in all measures except opinosis getting higher than our system
measureuser







































































































































































method measureuser summary







































































































































































in figure we are representing comparison of different system generated summary length using measure and figure comparison of length summary and we representing here only f score
from table we can say that
mead system and microsoft performing better in term of recall but our system is performing better compare to opinosis

our method getting higher precision compare to opinsis all rouge score p and higher precision achieved compare to mead except to

we are getting low f score compare to mead and microsoft system but higher w

opinosis
figure f score summary figure f score summary

experiment in this experiment we are showing the significance of sentiment feature the purpose of this experiment to show is really sentiment score performing a significant role in salient sentence extraction
to generate a good quality summary of some words like words is a tedious task
in our experiment we are taken five different features
we tried all combination of all five features and using this combination we trying to prove this feature is playing a significant role in summarization
if number of features are n then the total number of combination so in our experiment we are trying all combination calling approaches
here we generate summary using single stand alone feature based summary and summary in which sentiment score is playing a role
here first we generate approximate words summary to evaluate this summary we took three human generated summary as reference summary
motivated by duc task we are evaluating only first words of the summary
stands for tf idf feature stands for aggregate similarity score stands for position based stands for centroid based feature stands for sentiment based score feature showing collective score of tf idf aggregate similarity position based features
table different rouge score for summary generated using different approaches
here we are presenting different features combination to find a summary
by seeing table we can see that position based feature approach highlighted in green is performing best among all but this is due to that in all three human reference summary out of summary are extractive type extractive type summary is available at address which are used for evaluation contains top sentences and which is almost words and the model which we are using for score position giving higher preference for leading sentences but as we know position based score ca perform well in all cases like in scientific article so we need some more features
from table this is clear when we are taking sentiment feature i d along with other features we are getting improved summary
more rouge score means more accurate summary
the conclusion of this experiment is that out of approaches when sentiment feature added times we are getting improved summary by adding sentiment feature
for example if we take collective features and by adding sentiment feature we are getting improved results highlighted in red color
here position based feature performing best among all approaches the reason is given above and we ca depend only on position based feature so we need more features
in approach i
e
aggregate and position when we add sentiment score done in approach i
e
highlighted in blue color the performance is reduced this be due to position based score not preferred i
e
position based feature is not dominating here as in approach
results obtain from three human summary as reference summary and summary obtain from different approaches consider as system summary along with document are available at address
to remove biasness and evaluate first world summary we use and to evaluate rouge w we have taken












































































































































conclusion and future work in this work we are taken dataset designed by us
we presented a hybrid text document summarization algorithm based on linear combination of different statistical measures and semantic measures
in our hybrid approach we taken statistical measures like sentence position centroid tf idf as well as semantic approach doing sentiment analysis that is based on word level analysis
sentiment score of a sentence is given as the sum of sentiment score of every entities present in a sentence
we are getting three polarity for any entity like neutral negative and positive
if entity sentiment is negative then we multiplying every score by to treat it as positive score the reason for doing this we wants to select a sentence in which strong sentiment is present it may be either negative or positive and both have same importance for us
the significance contribution of sentiment score is presented in section

we are not using any learning or brute force approach to deciding how much importance to given different measures or in other words we giving equal importance for each and every feature
to calculate the score for a sentence we just add all the scores for every sentence and pick up a sentence based on highest score
in next step we select next sentence based on next highest score and add it to the summary if the similarity between summary and sentence is lower that threshold to maintain redundancy and coverage
we stop our algorithm when the desired length summary is found
to generate several summaries of different length we used methods like mead microsoft opinosis and human and for evaluation different rouge score is used
we done two experiment in first experiment we take our summary generate from algorithm described in section as system summary and all other as model summary and it have showed that we getting high precision almost every time that denotes we covered most relevant results
in the second experiment we compare different system generated summary mead microsoft opinosis our algorithm to model summary human generated
in this we find that our explained algorithm performed well for generated summary for almost every time but in mead system generate summary leading in some way but here also we getting higher recall compare to mead
in third experiment section
we have shown that when we are adding sentiment score as a feature we are getting improved results compare to without sentiment score
third experiment is showing that sentiment score has contribution in extraction of more appropriate sentences limitation of our work that in step i
e
salient sentence extraction we are initializing two parameter desired summary length and similarity threshold we need to set very less

if we set as
or
the sentences which came in summary only depends on the output of step which are not following one important property of summary that is coverage
since we are using long stop word list and wants to follow coverage property we need to choose with precaution but we can put high if not using any stop word list
in future we can use soft computing technique to learn different weights for different feature scores we can extend our approach to multi document summarization we can extend this experiment for benchmark dataset

acknowledgement thanks to ugc for funding me to iskandar keskes miracl laboratory anlp research group tunisia to give me description about how to evaluate summary
thanks to mr
prem ayadav mr
sarad ms
payal biswas to generate extractive type summary from given document and ashish kumar all from sc ss ir lab jnu delhi to help me at several stages

references radev d
r
hovy e
mckeown k

introduction to the special issue on summarization
computational linguistics
alguliev r
m
aliguliyev r
m
mehdiyev c
a

sentence selection for generic document summarization using an adaptive differential evolution algorithm
swarm and evolutionary computation
mani i
maybury m
t
eds


advances in automatic text summarization vol

cambridge it press
wan x
wan using only cross document relationships for both generic and topic focused document summarizations information retrieval pp
ko y
seo j

an effective sentence extraction technique using contextual information and statistical approaches for text summarization
pattern recognition letters
radev d
r
blair goldensohn s
zhang z

experiments in single and multi document summarization using mead
ann arbor
radev d
r
jing h
stys m
tam d

centroid based summarization of multiple documents
information processing management
yeh j
y
ke h
r
yang w
p
meng i
h

text summarization using a trainable summarizer and latent semantic analysis
information processing management
aliguliyev r
m

automatic document summarization by sentence extraction

hovy e
lin c
y
october
automated text summarization and the summarist system
in proceedings of a workshop on held at baltimore maryland october pp

association for computational linguistics
luhn h
p

the automatic creation of literature abstracts
ibm journal of research and development baxendale p
b

machine made index for technical literature an experiment
ibm journal of research
and development
edmundson h
p

new methods in automatic extracting
journal of the acm jacm
das d
martins a
f

a survey on automatic text summarization
literature survey for the language and statistics ii course at cmu
ganapathiraju m
k

relevance of cluster size in mmr based summarizer a report
advisors carbonell j
and yang y
ouyang y
li w
li s
lu q

applying regression models to query focused multi document summarization
information processing management
carbonell j
goldstein j
august
the use of mmr diversity based reranking for reordering documents and producing summaries
in proceedings of the annual international acm sigir conference on research and development in information retrieval pp

acm
alguliev r
m
aliguliyev r
m
hajirahimova m
s

mclr generic document summarization based on maximum coverage and less redundancy
expert systems with applications
alguliev r
m
aliguliyev r
m
isazade n
r

cdds constraint driven document summarization models
expert systems with applications
lee m
c

a novel sentence similarity measure for semantic based expert systems
expert systems with applications
bollegala d
okazaki n
ishizuka m

a bottom up approach to sentence ordering for document summarization
information processing management
sarkar

syntactic trimming of extracted sentences for improving extractive multi document summarization journal of computing
vol
pp

goldstein j
mittal v
carbonell j
callan j
november
creating and evaluating multi document sentence extract summaries
in proceedings of the ninth international conference on information and knowledge management pp

acm
alguliev r
m
aliguliyev r
m
hajirahimova m
s
mehdiyev c
a

mcmr maximum coverage and minimum redundant text summarization model
expert systems with applications
lin c
y
july
rouge a package for automatic evaluation of summaries
in text summarization branches out proceedings of the workshop pp

kim j
h
kim j
h
hwang d
november
korean text summarization using an aggregate similarity
in proceedings of the fifth international workshop on information retrieval with asian languages pp

acm
mani i
maybury m
t
eds


advances in automatic text summarization vol

cambridge normalization
n


in wikipedia retrieved jan from
wikipedia
org mit press
ganesan k
zhai c
han j
august
opinosis a graph based approach to abstractive summarization of highly redundant opinions
in proceedings of the international conference on computational linguistics pp

association for computational linguistics
uttrakhand flood
n


in wikipedia
retrieved jan from
wikipedia
org yadav c
s
sharan a
joshi m
l
semantic graph based approach for text mining
in issues and challenges in intelligent computing techniques icict international conference on pp

ieee
yadav c
s
sharan a
kumar r
biswas p
july
a new approach for single text document summarization
in international conference on computer and communication technology t aisc series accepted
springer
oracle n


full text stopwords
retrieved from
mysql
com doc
en sankarasubramaniam y
ramanathan k
ghosh s

text summarization using wikipedia
information processing management
precision and recall
n


in wikipedia retrieved date jan from
wikipedia
org stopwords
html
chandra shekhar april in google drive retrieve april
google
com
