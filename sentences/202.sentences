n u j l c
s c v
v i x r a efcient adaptation of pretrained transformers for abstractive summarization andrew hoang antoine bosselut asli celikyilmaz yejin choi allen school of computer science engineering university of washington seattle wa allen institute for articial intelligence seattle wa microsoft research redmond wa antoineb
washington
edu
org abstract large scale learning of transformer language models has yielded improvements on a variety of natural language understanding tasks
whether they can be tively adapted for summarization however has been less explored as the learned representations are less seamlessly integrated into existing neural text production architectures
in this work we propose two solutions for efciently adapting pretrained transformer language models as text summarizers source embeddings and domain adaptive training
we test these solutions on three abstractive marization datasets achieving new state of the art performance on two of them
finally we show that these improvements are achieved by producing more focused summaries with fewer superuous and that performance improvements are more pronounced on more abstractive datasets
introduction recent work in large scale language models has allowed pretrained contextual sentations to be easily adapted for a variety of downstream tasks yielding improvements on many benchmarks evaluating natural language understanding
less explored however has been the effect of these pretrained representations on text production tasks such as abstractive summarization where state of the art performance is still achieved with sequence to sequence models
these sequence to sequence methods typically use an encoder and decoder model with separate parameters to represent the input article and produce the output summary and the most successful solutions use attention mechanisms that learn an alignment between encoder and decoder states
pretrained language models however do not learn the parameters for such a task specic alignment making it challenging to integrate their learned representations into a summarization architecture at a higher level of abstraction than the word embedding
in this work we adapt full transformer language models for abstractive summarization
building off the work of liu et al
who rst proposed concatenating input and output text to a joint sequence and using a common transformer to encode both we use a language model as a summarizer rather than an encoder decoder
with this approach representations from a pretrained transformer language model in this case gpt can be used to fully initialize the parameters of the summarization model allowing it to leverage the representational power of a model trained at much larger scale
to accomplish this effectively we outline two strategies for adapting pretrained representations for abstractive summarization
in the rst we augment the input representation of the summarization model by instantiating source embeddings that encode the token type of the text being read
this change allows the model to recognize whether a given token belongs to the input article or the output summary thereby learning how to distinguish both types of text when encoding
in the second we preprint
under review
figure the embedding process for inputs to the transformer sm model
introduce a domain adaptive training procedure that ne tunes the transformer toward understanding general newswire text before training on the summarization end task directly allowing the model to learn the general structure and language distribution of newswire text before being ne tuned to produce summaries
a comprehensive empirical study across three datasets cnn dailymail xsum and room shows that transformer language models can be used to train abstractive summarizers producing summaries that are more concise and focused than state of the art baselines
our tion also empirically validates several observations about the abstractive summarization task
first echoing the results of sun et al
the most common summarization evaluation metric rouge is highly sensitive to summary length providing an advantage to methods that produce longer summaries either through learning or with minimum summary length constraints
second achieving higher rouge scores is not strongly consistent with human assessments of abstractive summary quality
finally despite being conceived as abstractive summarizers most current state of the art models are highly extractive copying phrases and even sentences verbatim from the document
model in this paper we focus on a variant of the transformer that has been pretrained on a large corpus of natural language stories the gpt model
as our architecture is practically identical to the one proposed in radford et al
we point readers to that work for background on the architecture of the model and focus below on the enhancements to the input representation made in our approach

input representation


xa each article is represented as a sequence of m tokens xa m and its corresponding summary is a sequence of n tokens xs n
as outlined in figure the input structure of the training set is a pair of article and corresponding summary concatenated into two sequences similar to xa


xs xs where t m n and d and e are special tokens identifying the delimitation and end of the sequence
below we dene the process of encoding these sequences as inputs to the transformer
xa d xs e x word embedding first each token xt in the concatenated sequence x indexes a word embedding rh from a joint vocabulary for the article and summary and special tokens
position embedding second since the transformer a self attention model has no concept of ordering of tokens a position embedding pt rh is initialized for each absolute position in the sequence
the embedding for each position in the sequence is added to the word embedding of the token occupying that position augmenting the nal representation of the input
for example each token in the article would be represented as wa m m pm
once the delimitation token d is




















dadads


dsdsdssourceembeddings reached the position counter is reset
for example the rst token of the article xa of the summary xs both receive as a positional embedding to augment their representations
and the rst token source embedding finally because the transformer must recognize pragmatic differences between the text of the article it reads and the text of the summary it learns to produce an additional specic embedding is initialized rh
the source embedding encodes whether a token is from the article portion da of the concatenated input or the summary portion ds
for any article token eq
or summary token eq
then the nal encoding is wa m m pm ws n n pn ds in contrast to the other embeddings in the model the source embeddings are not pretrained ducing the potential that they could dominate pretrained representations for the word and position embeddings when summed eq

to avoid this we normalize the random initialization of the source embeddings to have norm equal to half of the average norm of the word embeddings
training the model is initialized with pretrained parameters from the gpt model that was trained on the bookscorpus
following this initialization we pursue two additional training procedures domain adaptive training and end task training

domain adapative training despite the benet of using pretrained representations from the gpt model to initialize a summarizer there is a language shift between the storybooks data on which the gpt model was trained and the type of language found in newswire summarization datasets
additionally there are structural differences between how articles are written usually expressing salient points early on followed by details later and how stories unfold less front loading of key information
to address this discrepancy we propose domain adaptive training dat to adapt the transformer summarization model to the language distribution of newswire text by maximizing the conditional loglikelihood of the article tokens and summary tokens given all previous tokens in their concatenated input representation see figure ldat log p xa log p xs m n where m is length of the article n is the length of the summary is the set of all tokens in the article that precede xa n and is the set of all article tokens
in this framework the model is adapted to produce newswire like language before being trained on the summarization end task which only focuses on learning for summary production
m is the set of all tokens in the summary that precede xs
end task training during end task training ett the model is trained specically to be able to produce a summary given a document constraining the loss function toward maximizing the conditional loglikelihood of producing only the correct summary tokens given the set of article tokens lett log p xs where is the set of tokens in the summary that precede xs n
n table comparison of summarization datasets with respect to dataset size proportion of unique n grams mean article length in words and mean summary length in words
dataset newsroom xsum cnn dailymail split size train validation test novel n grams in gold summary mean words unigrams


bigrams


trigrams


grams article





summary


experimental setup datasets the cnn daily mail dataset consists of articles from cnn and daily mail
each article is associated with several descriptive bullet point highlights
similar to previous work we concatenate the highlights to create a target summary for each article in the dataset and use the same dataset splits
the extreme summarization xsum dataset consists of article summary pairs taken from the bbc
each summary is a single sentence long and is professionally written usually by the author making the dataset exhibit more abstractive content than typical summarization datasets such as cnn dailymail
the newsroom dataset consists of
m article summary pairs scraped from the internet archive
the articles come from a set of publishers and cover diverse topics
we provide statistics about each dataset in table
data preprocessing we used a bytepair encoding bpe for tokenization
for each summarization dataset we use the bpe to tokenize each article and summary and then truncate the articles to a maximum length of tokens and each summary to a maximum length of tokens
we then format each article summary pair into the format outlined in figure
model specications we used a transformer decoder with n blocks and h masked self attention heads in each block
we set the dimensionality of each self attention head to be dmodel
unless stated otherwise we use the pretrained weights of radford et al
to initialize the parameters of the model
special tokens that are added to the vocabulary i
e
the end token start token and delimiter token are initialized by sampling from the standard normal distribution
our full model with source embeddings
is denoted as as transformer sm and we also train an ablation transformer lm that does not use source embeddings
training details all models were trained with a learning rate of
and a minibatch size of
when domain adaptive training dat is used we train for epochs using dat and then for an additional epochs using end task training ett
without dat we train on the end task for epochs
unless specied otherwise the nal model trained for each dataset uses both domain adaptive training and end task training
we did not tune hyperparameters
all models were trained using the pytorch and the huggingface implementation of gpt
we trained each model on tesla
training for a total of epochs took approximately day of clock time for the xsum and cnn daily mail datasets and days for the newsroom dataset
our source code is publicly available
generation we perform generation by using beam search with a beam size of
we use the trigram trick during beam search
each summary token is generated by decoding from the distribution yielded by the model from processing an input tensor that is the concatenation of the article tokens the delimiter token and any previously generated summary tokens
evaluation we evaluate our system with common summarization metrics a measure of unigram recall between the summary and document a similar measure of bigram recall and rouge l r l a measure of the longest common subsequence between the summary and document
we also report the length of the summary in terms of tokens produced
for each dataset for evaluation on the test set we selected models with the largest score on a subset of samples from the validation set


com huggingface pytorch openai transformer lm
com transformer abstractive summarization experiments
cnn daily mail baselines we report the results from various models previously trained and evaluated on the cnn daily mail dataset
the pgen and pgen coverage models consist of attentive rnn encoder decoders that integrate the ability to directly copy from the article when generating tokens
pasunuru and bansal extend this work by adding policy gradient training with a mixture of rewards that promote saliency and entailment
bottom up summarization and the copy transformer also extend see et al
by using the copy mechanism to compress the article to only relevant content before summarizing it
chen and bansal also look at performing content selection but extract full sentences from the document with a novel extractor model
finally the dca model uses multiple separate communicating encoders over different parts of the document to produce representations that are more focused on salient details
table rouge results on the test set of cnn daily mail
best model results are bolded
model pgen pgen coverage rougesal ent rl bottom up summ copytransformer rnn ext rl dca transformer lm transformer sm

















r l length l















automatic metrics we report our results using automatic metrics in table
on this dataset our main model transformer sm performs slightly worse than other state of the art models
we note that our model tends to generate shorter summaries than the gold summaries shorter which could lower rouge recall performance
in figure we investigate the correlation of rouge l scores with summary length and note that a minimum decoding length used by state of the art algorithms places baseline generated summaries in length bins of higher average rouge l performance
when transformer sm produces summaries in these same length bins i
e
more than tokens its performance is only consistently beaten by the dca model which was ne tuned with rl
figure average rouge l for summaries in different length bins
scatter plots correspond to rouge l scores for each bin while solid lines correspond to the number of summaries in each bin table head to head comparison between test set outputs of left dca and sm right and transformer sm
analyses done on summaries for cnn dailymail
model non redundancy coherence focus dca same t sm same t sm overall human evaluation while rouge scores are negatively inuenced by the shorter average length of the summaries produced by our model it is not clear that shorter summaries are correlated with worse quality
to evaluate this hypothesis we perform a human evaluation on article summary pairs randomly sampled from the test set
the article and model generated summaries were presented to three workers from amazon mechanical turk amt
each worker was presented two model generated summaries one produced by the sm model and one from the dca model or the model
workers were asked to select the better summary for four different quality metrics from celikyilmaz et al
redundancy fewer of the same ideas are repeated coherence ideas are expressed clearly focus the main ideas of the document are shared while avoiding superuous details and overall
the results are presented in table
interestingly the summaries from transformer sm are tently preferred by humans across all evaluations dimensions compared to those from the dca and models indicating that the transformer sm s lower rouge scores observed in table are not necessarily correlated with human judgments of quality
table rouge l precision r l p recall l r and r l scores computed between generated summaries and input cnn dailymail articles after removing stop words table ablation study of training schedules on cnn dailymail
pt model initialized with pretrained weights dat model uses adaptive training ett trained on end task
r l p r l r model name

pgen



bottom up

rnn ext rl
dca

transformer lm

transformer sm
l






gold summary


model t lm ett t lm t lm t lm t sm ett t sm t sm t sm















r l







efciency due to the large improvements over the baseline models in the human evaluation gories of non redundancy and focus and the generally shorter summaries produced by sm we investigate whether transformer sm is able to more efciently express key ideas of the document
to evaluate the efciency of each model we remove non content words from the generated summaries and articles and compute the rouge score between them
this measure serves as a proxy for the rate at which ideas expressed in the summary can be found in the document
we report these results in table and observe that transformer sm reports comparable l recall scores to other baselines when evaluated with respect to the article despite producing summaries that on average shorter
meanwhile rouge l precision is also very similar to the baseline models indicating that the summaries of all models indicate a similar degree of information relevance
combined with the results from table we conjecture that transformer sm is able high precision scores across all models conrm that despite being conceived as abstractive generators these models display highly extractive behavior
to more efciently express key ideas from the document
while other models may be producing longer summaries that yield higher rouge performance table the additional tokens may reect redundant and unsalient information which human evaluators penalize
analysis of domain adaptive training and source embeddings our approach involved two gies for efciently using transformer language models for abstractive summarization domain adaptive training and source embeddings
to assess their individual impact we evaluate multiple training schedule permutations e

various combinations of using pretrained representations from the gpt model and using domain adaptive training as well as the impact of source embeddings
our results in table yield multiple interesting conclusions
first in general domain adaptive training dat in table provides a clear improvement over training directly on the end task irrespective of whether pretrained representations are used
similarly using source embeddings t sm in table provides a repeated improvement over the t lm ablation
surprisingly when pretrained initializations dat and source embeddings are used in tandem performance drops slightly compared to not using dat or not using source embeddings
we note however that this observation does not hold true for the xsum dataset
and conjecture that the extractive nature of the cnn dailymail dataset may make these approaches have redundant effects in this setting

xsum a study on the quality of abstractive summaries is best performed on the xsum dataset which is specically designed with gold summaries that are less extractive than the other datasets table
baselines we report the performance of transformer sm on this dataset in comparison to baselines originally reported in narayan et al
an attention based sequence to sequence model a pointer generator model capable of generating words and copying directly from the input pgen a second pointer generator model with a coverage mechanism to prevent repetition and the top performing variant of the topic aware convolutional sequence to sequence model t in which the encoder and decoder are provided with word topic and document topic distributions obtained using lda as additional inputs
our nal baseline is the multi level memory network mmn which applies attention over multiple memory layers for varying levels of abstraction
results we report our results in table
our els signicantly outperform the comparison lines across all three variants of the rouge metric
interestingly the transformer sm achieves able improvement over the transformer lm model suggesting that both source embeddings and domain adaptive training are helpful when target summaries are more abstractive
examples of model generated summaries from the xsum dataset illustrate the provement over baselines qualitatively in table
in support of results presented earlier the model duces abstractive summaries that provide focused information about the main points of the articles

newsroom table comparison results on the xsum test set using the variants of rouge




model pgen t mmn




r l




transformer lm
transformer sm




finally we report the performance of our model on the newsroom dataset the largest of the evaluation datasets
due to the large cost of training only the transformer sm model was evaluated
baselines as baselines we report the performance of models released by the authors of the newsroom dataset
these models included an attentive encoder decoder attn and a generator network pgen
we also compared against a complex encoder decoder that uses lstms encoder attention intra decoder attention and pointer generation to produce summaries
we also compare against the multi level memory network mmn mentioned earlier
the authors of this baseline only evaluated on the abstractive subset of the newsroom dataset
table xsum samples for the baseline t model and transformer sm along with the gold summary
articles are shortened for brevity
capitalization was manually added for ease of reading
source source text article snippet ofcials said the attack happened at the europa shopping centre in the capital minsk



police later arrested the year old suspect



he cut one woman with the chainsaw and hit her with a hammer
she died
he also attacked others
the injured woman was taken to a local hospital
the attacker had brought the chainsaw and the axe to the shopping centre


t transformer sm a man has been arrested on suspicion of attempted murder by after a knife attack on a shopping centre in central london
a teenage girl has been killed by a chainsaw attack at a shopping centre in central russia police say
gold a young man has attacked people with a chainsaw and an axe at a shopping centre in belarus killing one woman and injuring another
article snippet the year old sweden striker s contract with the french champions expires in the summer and he has been linked with manchester united la galaxy and ac milan



psg said ibrahimovic leaves as the greatest striker and one of the very best players in the club s history



t transformer sm paris st germain have completed the signing of zlatan ibrahimovic from paris st germain for an undisclosed fee
zlatan ibrahimovic says he will leave paris st germain at the end of the season to return to the club
gold zlatan ibrahimovic will leave paris st germain at the end of the season
article snippet


the animal was taken from lathom pets and aquatics in ormskirk on tuesday afternoon lancashire police said
the shop s owner said cctv showed a man taking the tortoise which needs calcium supplements out of the tank



t transformer sm a tortoise has been stolen from a pet shop
a puppy s pet shop has been stolen from a shop in lancashire
gold a baby tortoise has been stolen from a pet shop in lancashire
table rouge results on validation subsets and full validation set for newsroom model name
pgen mmn transformer sm
extractive

r l



mixed

abstractive newsroom d r l







r l






r l

table comparison results on the room test set using rouge model attn pgen





r l


t sm


results we report our results with rouge style tomatic metrics in table showing that sm outperforms the previous best model across all metrics
interestingly our model achieves its highest performance increase over baseline models on rouge l the metric usually considered as being most strongly correlated with strong summaries
thermore an analysis of different validation subsets of the newsroom dataset in table split on the level of extractiveness of the gold summaries shows that transformer sm performs better than baselines proaches on all varieties of summary types
related work abstractive summarization there has been a large variety of work exploring different methods for neural abstractive document summarization
attention mechanisms have been shown to improve a variety of models and is one of the motivating factors for this work
pointer generator networks introduced in see et al
have been shown to increase summary veracity and inspired the tangential usage of copy mechanisms in transformers for document summarization gehrmann et al

other works have also explored the use of reinforcement learning to directly optimize summarization models on the rouge metric
contextualized representations our approach is also relevant to recent work on contextualized language representations that are pretrained on large scale language corpora
these representations can then be simply integrated or ne tuned for improved performance on many downstream tasks
ssl cove and elmo all learned contextualized representations through training rnn language models and encoder decoders
follow up work extended these ideas but replaced the rnn with a deep transformer that was trained to learn language patterns on a large story dataset
bert more clearly extended the idea of using transformers for language modeling by making the encoded representations bidirectional and adding two new loss functions a masked token loss and next sentence prediction loss for more accurate discourse representations
more recently expanded the scale of pretrained language models and showed promising results on zero shot tasks
conclusion in this work we introduce two approaches for effectively adapting pretrained language model sentations to abstractive summarization domain adaptive training and source embeddings
we uate the effect of both approaches across three abstractive summarization testbeds cnn dailymail xsum and newsroom and achieve state of the art rouge l results on two of them while showing superior human evaluation performance on the third
in the process we show that the rouge l metric often used for abstractive summarization evaluation is quite sensitive to summary length allowing it to be exploitable by approaches that use heuristics to control summary length
references asli celikyilmaz antoine bosselut xiaodong he and yejin choi

deep communicating agents for abstractive summarization
in naacl
yen chun chen and mohit bansal

fast abstractive summarization with reinforce selected sentence rewriting
in acl
jianpeng cheng and mirella lapata

neural summarization by extracting sentences and words
arxiv preprint

andrew m
dai and quoc v
le

semi supervised sequence learning
in nips
jacob devlin ming wei chang kenton lee and kristina toutanova

bert pre training of deep bidirectional transformers for language understanding
in naacl
sebastian gehrmann yuntian deng and alexander m rush

bottom up abstractive summarization
in emnlp
max grusky mor naaman and yoav artzi

newsroom a dataset of
million summaries with diverse extractive strategies
in naacl
karl moritz hermann tomas kocisky edward grefenstette lasse espeholt will kay mustafa suleyman and phil blunsom

teaching machines to read and comprehend
in advances in neural information processing systems pages
byeongchang kim hyunwoo kim and gunhee kim

abstractive summarization of reddit posts with multi level memory networks
arxiv preprint

chin yew lin

looking for a few good metrics automatic summarization evaluation how many samples are enough in ntcir
chin yew lin

rouge a package for automatic evaluation of summaries
text rization branches out
peter j
liu mohammad saleh etienne pot ben goodrich ryan sepassi lukasz kaiser and noam shazeer

generating wikipedia by summarizing long sequences
in iclr
bryan mccann james bradbury caiming xiong and richard socher

learned in translation contextualized word vectors
in nips
ramesh nallapati feifei zhai and bowen zhou

summarunner a recurrent neural network based sequence model for extractive summarization of documents
in thirty first aaai conference on articial intelligence
ramesh nallapati bowen zhou ccero nogueira dos santos aglar glehre and bing xiang

abstractive text summarization using sequence to sequence rnns and beyond
in conll
shashi narayan shay b cohen and mirella lapata

do nt give me the details just the summary topic aware convolutional neural networks for extreme summarization
in emnlp
ramakanth pasunuru and mohit bansal

multi reward reinforced summarization with saliency and entailment
in acl
romain paulus caiming xiong and richard socher

a deep reinforced model for abstractive summarization
in iclr
matthew e
peters mark neumann mohit iyyer matt gardner christopher clark kenton lee and luke zettlemoyer

deep contextualized word representations
in proc
of naacl
alec radford karthik narasimhan tim salimans and ilya sutskever

improving language understanding by generative pre training

url us
amazonaws
com openai assets research covers language unsupervised
pdf
alec radford jeff wu rewon child david luan dario amodei and ilya sutskever

language models are unsupervised multitask learners
abigail see peter j liu and christopher d manning

get to the point summarization with pointer generator networks
in acl
tian shi yaser keneshloo naren ramakrishnan and chandan k reddy

neural tive text summarization with sequence to sequence models
arxiv preprint

simeng sun ori shapira ido dagan and ani nenkova

how to compare summarizers without target length pitfalls solutions and re examination of the neural summarization literature
in proceedings of naacl workshop on optimizing and evaluating neural language generation neuralgen
jiwei tan xiaojun wan and jianguo xiao

abstractive document summarization with a graph based attentional neural model
in proceedings of the annual meeting of the association for computational linguistics volume long papers volume pages
ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan n gomez ukasz kaiser and illia polosukhin

attention is all you need
in advances in neural information processing systems pages
alex wang amanpreet singh julian michael felix hill omer levy and samuel r
bowman

glue a multi task benchmark and analysis platform for natural language understanding
arxiv preprint

yukun zhu ryan kiros richard s
zemel ruslan r
salakhutdinov raquel urtasun antonio torralba and sanja fidler

aligning books and movies towards story like visual explanations by watching movies and reading books
ieee international conference on computer vision iccv pages
a reproducibility we provide additional details relevant to the experimental environment here
data sources the cnn daily mail dataset consists of articles from cnn and daily mail
each article is associated with several descriptive bullet point highlights
similar to previous work we concatenate the highlights to create a target summary for each article in the dataset
the newsroom dataset consists of
m article summary pairs scraped from the internet archive
the articles come from a set of publishers and cover diverse topics
finally the extreme summarization xsum dataset consists of article summary pairs taken from the bbc
each summary is a single sentence long and is professionally written usually by the author
for all datasets we use the splits dened in the original works that proposed them
because the datasets are too large to provide as supplementary material we provide pointers in the source code readme for acquiring them
hyperparameters details about important hyperparameters can be found in section of the paper
additional training hyperparameters can be found as the default parameters in the training script of the source code
most hyperparameter values selected were the same ones suggested by previous work on transformer language models
the only hyperparameter we varied that is not measured as an ablation i
e
training schedules and whether to include source embeddings was the initialization of source embeddings if they were included
for this hyperparameter we explored three different initializations initializing both source embeddings with zero vectors initializing both source embeddings with values sampled from the standard normal distribution and initializing both source embeddings with values sampled from a normal distribution with mean and standard deviation equal to half the norm of the average norm among pretrained embeddings from the gpt language model
this last one is the one we report in all experiments
experimental process each experiment was run as follows for any given model and dataset
first we trained the model as described in the paper
after every minibatches we compute rouge for a random but persistent example subset of the validation set
when the score of the model stopped rising we used the previous checkpoint as a model to generate summaries for all articles in the test set
we used beam search to decode summaries using a beam with of
we ran exactly one evaluation run for each result we include in our paper

cnn
com
dailymail
co


bbc

com transformer abstractive summarization
