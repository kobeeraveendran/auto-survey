c e d l c
s c v
v i x r a cross lingual approach to abstractive summarization ales zagar marko sikonja university of ljubljana faculty of computer and information science vecna pot ljubljana slovenia ales
zagar marko

uni lj
abstract automatic text summarization extracts important information from texts and presents the information in the form of a summary
abstractive marization approaches progressed signicantly by switching to deep neural networks but results are not yet satisfactory especially for languages where large training sets do not exist
in several natural language processing tasks cross lingual model transfers are successfully applied in low resource guages
for summarization such cross lingual model transfer was so far not attempted due to a non reusable decoder side of neural models
in our work we used a pretrained english summarization model based on deep neural networks and sequence to sequence architecture to summarize slovene news articles
we solved the problem of inadequate decoder by using an additional language model for target language evaluation
we developed several models with different proportions of target language data for ne tuning
the results were assessed with automatic evaluation measures and with small scale man evaluation
the results show that summaries of cross lingual models ne tuned with relatively small amount of target language data are useful and of similar quality to an abstractive summarizer trained with much more data in the target language
introduction summarization is a process of extracting or collecting important information from texts and presenting that information in the form of a summary
according to the output of the process it can be broadly divided into extractive and abstractive summarization
the extractive approach is non productive in a sense that it just copies important sentences and the resulting summary does not include any new words or sentences
the abstractive approach is creative and produces summaries that rephrase the given content in compressed sentences that can contain originally unused words
the abstractive neural summarization approaches use similar deep learning chitectures as machine translation but face some additional problems the input is usually longer the output is short compared to the input and the compression is lossy
current abstractive summarization outputs can be repetitive absurd can misrepresent facts deal poorly with out of vocabulary words perform poorly at content selection and can be misleading in various other ways
nevertheless they often produce good outputs
many summarization tools exist for resource rich languages
cross lingual embeddings present a promising approach for low resource languages and enable the model transfer from resource rich to resource poor languages adams et al
artetxe and schwenk
typically this is done by training the model on the resource rich language using monolingual embeddings in the source guage and then applying it to the less resourced language where the input beddings in the target language are mapped to the source language embeddings
unfortunately this standard procedure does not work for cross lingual rization as the model is trained to output the sentences in grammar of the source language
blindly applying the procedure to a summarization model trained on glish would produce sentences with english grammar in the target language e

slovene
in the proposed solution we use a pretrained english summarization model proposed by chen and bansal and use english as a source language and slovene as a target language
using cross lingual embeddings we map slovene word embeddings into the english word vector space
as zero shot transfer ing is not expected to be satisfactory we further ne tune the resulting model
our cross lingual models are trained with an increasingly large portions of the able target language dataset
in the post processing stage we generated several potheses and selected the best using four evaluation metrics including base neural language model in the target language
in summary our approach is a direct cross lingual summarization model
the automatic metrics and small scale human evaluation show that it is on par with a summarization model trained from scratch on the target language
the paper is split into further ve sections
in section we present related work and in section we describe the slovene datasets used to ne tune the marization model and to build the language model
section outlines the proposed cross lingual summarization model and gives detail of the used components
we report the results in section and present conclusions and ideas for further work in section
related work most early summarization approaches use the extractive approach which is also suitable for a multi document summarization gambhir and gupta
lately deep neural networks learning sequence to sequence transformations produced state of the art abstractive summaries rush et al
nallapati et al

they encode a source document into an internal numeric representation and then decode it into an abstractive summary
these models work best for short single document summaries e

headline generation and news summarization
they use the attention mechanism which ensures that the decoder focuses on the appropriate input words bahdanau et al

additionally they frequently use the copy mechanism that copies relevant words from the input when they are not present in a dictionary see et al
and the coverage mechanism that avoids redundant contents tu et al

our cross lingual approach is based on monolingual model proposed by chen and bansal
this hybrid summarization model rst selects salient sentences and then paraphrases them
the model is comprised of two independently trained neural networks bridged by policy based reinforcement learning
as we use slovene as the target language we report the work on summarization in this language
recently zidarn built the rst abstractive summarizer for the slovene language using the architecture and deep neural networks
the best results were produced by a two layer lstm with attention mechanism copy mechanism and beam search
the author used the same dataset of approximately news and showed that it is not large enough to achieve results comparable to english
besides english there are only a few other languages with abstractive marizers
straka et al
presented sumeczech a large news summarization dataset for czech million samples
for summarization they compared vised methods such as textrank mihalcea returning a few rst sentences and supervised methods logistic regression and random forests on hand crafted features
fecht et al
used the encoder decoder architecture on german wikipedia articles samples where the summary is the rst section of the article and the subsequent text represents the document
hu et al
created a chinese summarization dataset
million samples from a chinese ging website sina weibo and used a recurrent neural network for abstractive marization
many existing approaches to a cross lingual summarization combine rization and translation steps zhu et al

two different translation schemes were proposed an early translation scheme rst translates the original document into a target language and then generates a summary the late scheme rst ates a summary and then translates it into a target language
in general machine translation systems are used to generate a cross lingual dataset
ouyang et al
constructed summarization corpora from new york times corpus haus for three low resource languages
they implemented neural network architecture proposed by see et al

zhu et al
used the corpus of sina weibo by hu et al
cnn dailymail and msmo zhu et al

their approach uses neural networks and trains both machine translation and summarization in an end to end manner
chi et al
outperformed machine translation based approaches by training a model under both monolingual and cross lingual settings
after the pre training procedure they used monolingual data to ne tune the model on downstream nlg tasks
the idea of word embeddings is to learn high dimensional vectors that ture the meaning of words
popular variants are mikolov et al
glove pennington et al
fasttext grave et al
elmo peters et al
and bert devlin et al

an important insight for our work is that relations between words in the embedded space are preserved across the languages mikolov et al

cross lingual embeddings align monolingual embeddings into a common vector space ruder et al

in the beginning these niques required parallel corpora or a bilingual dictionary that provide necessary information for mapping a word from a source to a target language
the recent approach can train cross lingual embeddings in an unsupervised manner conneau et al

a major drawback of classical word embeddings is that they can not deal with polysemy
recent contextual embeddings bert devlin et al
and elmo peters et al
learn polysemous representations of words
datasets we shortly describe creation of two datasets one for summarization task and the other for language modeling
both datasets were extracted from the gigada
corpus of written standard slovene consisting of newspapers magazines and web texts containing documents with more than
billion words
the summarization dataset contains news and their summaries where the rst paragraph of sta slovenian press agency news web texts is taken as a summary and the rest of it as the text of the news
since the gigada corpus from which we extracted sta news is sentence segmented but not paragraph segmented we designed a heuristic to extract the rst paragraph
we started with training samples but kept only texts between and characters
some texts were discarded as they contained weather reports lists of events around the world
and some of them were just too long
a total of samples remained and were split into the train test and validation set
both the test and validation set contain instances and the training set contains the remaining news
to establish a machine translation baseline we used google machine lation service
we rst translated the test set from slovene into english and then generated english summaries with a pretrained monolingual english summarizer
after that we translated the generated english summaries back into the slovene language
as our summarization model we start with the trained english model
as cross lingual word mapping alone can not produce correct texts in the target guage because the grammar of a decoder remains in a source language we train a language model for improvements of the intended cross lingual transfer
for that purpose we trained a character level slovene language model following ndings of bojanowski et al
that language models for morphologically rich guages such as slovene are improved by using character level information
as the training set we used the gigada
which is already tokenized and sentence segmented
all punctuation special characters and numbers were preserved but alphabetical characters were lower cased
a total of sentences were extracted with the average sentence length of characters
the sentences were split into the train test and validation set with ratios
architecture and implementation of cross lingual marizer in this section we rst outline our solution to the problem of cross lingual marization
after that we provide descriptions of components used cross lingual word embeddings ne tuning of pretrained english summarization model to slovene generation and evaluation of the best hypothesis with several evaluation scores cluding a slovene language model

architecture of cross lingual summarizer the scheme of the approach consists of several steps and is presented in fig

as a pretrained summarization engine step we could use several pretrained marization models but in this work we used the english summarizer of chen and bansal
to adapt it to cross lingual setting we rst replaced the english word embeddings used as its input with slovene embeddings step
to match the word semantics of the two languages we used the cross lingual procrustes alignment conneau et al
and mapped the slovene word embeddings into the english word vector space
this already allowed us to put slovene text to the input of the summarization model step and ne tune it with different amounts of slovene text as discussed in section

in step we used the trained model to generate several hypotheses and in step we evaluated the hypotheses
the uation used an independently trained slovene language model using transformer architecture and two different metrics
the best hypothesis was included in a mary
figure the outline of the proposed cross lingual summarization approach

english summarization model as our summarization model we used the pretrained summarization model posed by chen and bansal
the model uses customarily trained embeddings and thus allows a simple cross lingual mappings
the architecture of the model is relatively complex and belongs to hybrid approaches to text marization that combine abstractive and extractive elements
on a high level it consists of the extractive network that selects salient sentences the abstractive network that rewrites or paraphrases them and the reinforcement learning rl step that optimizes the model end to end
both the extractor and abstractor works are learned independently and during the rl step the model updates only the extractor weights and leaves the abstractor as it is
the model was trained on cnn dailymail dataset
it contains training pairs validation pairs and test pairs
the details are available in the original paper

cross lingual word embeddings at the input to the neural network summarization model we transform the text into numerical vectors
as we want to use the model in cross lingual setting we use the slovene input and map it into english vector space
as slovene embeddings we used the pretrained slovene fasttext embeddings grave et al
trained on a mixture of slovene wikipedia and common crawl
fasttext dings are constructed with cbow algorithm mikolov et al
extended with position weights and subword information which is especially ful for morphologically rich languages such as slovene
to transform slovene embeddings into the english vector space we used the muse library conneau et al
in supervised setting
for this transformation muse uses a train dictionary of size and test dictionary of size created internally
in addition to transformation between embeddings we also replaced the english tionary with the slovene dictionary which was built from most common words in the slovene training dataset
the role of the dictionary is to map words to their embeddings

fine tuning of the summarization model once we adapt the target language input slovene to the source language glish summarization model we can use that model for summarization in the target language zero shot transfer
however if there are target language summaries available we can improve the summarization model with additional training
to test the amount of required additional data we created several models presented in table that differ in the amount of additional data used in ne tuning
meng is the baseline zero shot transfer model which means that no target language data was used only english embeddings were swapped with the aligned slovene embeddings
the models and use or of the target language training set to ne tune the english model
we trained the extractor of each model because only the reinforcement learning optimized extractor was provided by chen and bansal
neously we updated the weights of the pretrained abstractor and in the nal step optimized the models with the rl component
mslo is not a cross lingual model and was trained on the complete target language training set from scratch
note that the training set in the target language is signicantly smaller than the training set in the source language for mslo vs for meng

cc docs crawl vectors
html slovene data size in instances details model meng mslo slovene embeddings trained extractor trained abstractor no transfer cross lingual mappings no ne tuning zero shot transfer cross lingual mappings trained extractor ne tuned abstractor cross lingual mappings trained extractor ne tuned abstractor cross lingual mappings trained extractor ne tuned abstractor cross lingual mappings trained extractor ne tuned abstractor cross lingual mappings trained extractor ne tuned abstractor table the produced models using different amounts of target language data slovene in ne tuning the original summarization model

training slovene language model the adapted and ne tuned models produce summaries in slovene but the quality is not always adequate
for that reason we generated several hypotheses cessed them and selected the best one according to different evaluation approaches described below in section

as we want to optimize uency and grammatical correctness of the output sentences one of the evaluation approaches used guage models and for that purpose we trained a character based language model in slovene
many of the current state of the art language models baevski and auli dai et al
trained on datasets similar to ours chelba et al
use ants of the transformer architecture vaswani et al

we used the transformer decoder as implemented in the library vaswani et al
adam optimizer attention heads hidden layers and position wise feed forward works with one hidden layer of size and relu activation function
these are standard hyperparameters for training on a single gpu
we increased the mum size of the input from to which is approximately the percentile of sentence character length in the training corpus
shorter sentences are padded with spaces and longer are cut off
the dictionary contains characters
the total number of learning parameters was
the language model was trained for epochs in two parts due to limited computational resources epochs using million sentences and epoch with another million sentences
the batch size was
the model was evaluated on a test set with the size of sentences
training took approximately days on nvidia titan x gb gpu

postprocessing and creation of the nal summary the summarization model is ne tuned to produce slovene summaries
less the outputs are sometimes of low quality
for example sometimes rization models produce repeating n grams which we eliminate with rule based approach
to improve the quality of the summaries we extracted a large number of hypotheses from the abstractive network we congured the size of beam search from standard to and evaluate them further
the evaluation metrics we use consist of two components that try to capture the presence of relevant contents and readability of the hypotheses
the quality of the content is assessed by two metrics the standard metric for summarization called rouge score lin and hovy and recently proposed bertscore zhang et al
based on the pretrained gual language model bert devlin et al

we calculated rouge and bertscore scores by comparing generated hypotheses from the abstractor network with the sentences extracted with the extractive network for which we assume that contain the information we want to preserve in the tive part

the readability of the generated hypotheses is assessed with two measures the internal evaluation of hypotheses with the loss function computed by the abstractive neural network and the slovene language model described above in section

the later is expressed with perplexity score computed as the average entropy per character expressed in bits
with this approach we got four different evaluations of each generated pothesis
we rst used only one evaluation metric to select the best hypothesis and observed the results
after that we considered combinations of two metrics
for example we rst used rouge scores to narrow down the selection to best potheses and then selected the best one by language model scores
we did not put the constraint that they have to belong to different categories two content based combinations or two readability based combinations were permitted
evaluation in this section we provide the analysis and results of the produced summarization models
we present automatic evaluation scores as well as human evaluation of the generated summaries
in the end we compare our results with related works

evaluation metrics rouge recall oriented understudy for gisting evaluation metrics are the most commonly used in the evaluation of automatically generated text summaries lin and hovy
it measures the quality of a summary by the number of lapping units n grams sequences of texts
between reference summaries created by humans and automatically generated summaries
the most commonly used metrics are rouge n and rouge l
the rst measures the overlapping of n grams e

for unigrams and for bigrams
rouge l measures the longest common subsequence found in both summaries

results of cross lingual ne tuning table shows results of the six models listed in table and the three baseline monolingual models
the english monolingual model meng generates more than twice as many character as the other models and on average sentences while the other models generate to
these numbers are result of learning as the corpora of english summaries contains on average more and longer summaries
the model shows that as little as of additional instances are enough to update the number of extracted sentences
average generated evaluation scores model meng mslo mt baseline pg see et al
reference slovene reference english sentences characters rouge l perplexity
table the performance of the cross lingual models meng and compared to the monolingual models mslo mt baseline pg
the last two rows represent the statistics of reference slovene and english maries
the metrics and rouge l show similar relations tween the compared models
surprisingly zero shot transfer learning model meng scores higher on rouge metrics than and
the reason for this is that it extracts more sentences generates longer summary sentences and repeats the tences
analyzing the results of meng we noticed that the model sometimes not nish a sentence properly e

it generates good content but does not stop and just continues to generate words
we speculate that the problem is in special tokens start of sentence end of sentence
that capture the grammar of source guage
these special tokens may be a hidden problem in the cross lingual model transfer
we manually inspected the returned summary to assess their readability
does not show any signicant readability improvement over meng while shows some improvement
meng often generates long sentences with dant and rare words and inserts punctuation at inappropriate places
on the trary generates too short sentences and summaries with many missing words
shows an improvement in sentence selection over and improvement in readability over both and meng
still most of the sentences are not formulated but the meaning is present in almost all of them
models and show interesting properties considering that they duce scores quite close to the models trained on much larger training sets in target language i
e
and mslo
this indicates utility of cross lingual transfer which can produce useful models with much less data
as we can observe the pg model scores are the highest but its perplexity is not on par even with
the highest rouge scores can be explained since the model is not constraint when choosing the content in the same way as are the other hybrid extractive abstractive models it is an end to end abstractive model
it still generates summaries with higher readability than mt baseline but much less than the cross lingual models trained on sufcient amount of data
cross lingual model and mslo trained from scratch are the best models for slovene summarization
these two models use the same amounts of training data
it is infeasible to conclude which model is better in terms of manually inspected readability
however consistently shows better rouge scores
is improved for for and rouge l
this suggests that our cross lingual approach works and produces better summaries

evaluation of post processing table shows additional improvements we achieve with different approaches to post processing described in section

recall that for the creation of the nal summary from the set of generated hypotheses for each extracted sentence we can use two content based metrics rouge l and bertscore and two ity metrics internal loss score and perplexity of language model
as the baseline model we take the best ne tuned model described above in section
i
e
lingual model which uses only the internal loss score to select the nal output
using the perplexity of slovene transformer based lm to rank the date hypotheses the model is improved by point on and point on rouge l
both contents based metrics bertscore and rouge l produce even larger improvements but are quite close performance wise
selection metrics baseline with nn loss transformer lm multilingual bertscore rouge l rouge l bertscore rouge l table post processing improvements in selection of the best hypothesis using with loss score as the baseline
we tested all combinations of all four tion metrics but report only the best one in the last row
initially we hypothesized that two complementary metrics are needed to select the best hypothesis one for content and one for readability
the last line of table shows that this is not the case
in the best metric pair both of them belong to the content selection group
we are aware that these results may be biased since the nal evaluation rouge metrics are content based
however manual comparison of the models with two complementary metrics and the models with only based metrics conrmed that the former produced more readable summaries than the latter but with lower content accuracy

human evaluation the automatic evaluation is limited in assessment of actual user needs and tations novikova et al

for that reason we organized a small study with human evaluation of generated summaries
for each text we used both reference in this way the evaluation and candidate but in a random order for each text
procedure is slightly modied but still enables comparison
the task of referees was to assign accuracy and readability of a summary see table for the scales
accuracy represents the amount of overlap between the given facts and summarized information and readability measures how hensive a summary is
in our study articles summaries per text a generated and reference were evaluated by referees
referees included females and males aged from to and with different degrees of education
we report averages and standard deviations in table
it is surprising that the accuracy of the reference summaries is lower than the accuracy of the generated score accuracy none little a lot of most of all readability incomprehensible poor acceptable good awless table evaluation scales for accuracy and readability of summaries
type reference generated accuracy readability table average and standard deviation of human assigned accuracy and ability of reference and generated maries
summaries
we identied several reasons that explain this result
first the erence summaries often contain true facts and information that can not be veried by the text alone
unless misleading and speculative the generated summaries should always produce veriable content
second the evaluation method does not directly measure the content quality of a summary
following the instructions ticipants may assign a high score for a summary that contains true but unimportant and irrelevant information
third our model is a hybrid model which selects and paraphrases sentences
we assume that participants can be more easily lured into thinking that there is a greater overlap of content between a text and a generated summary than it is between a text and a reference summary
finally our study is small and the standard deviation of the answers is considerable therefore there is a considerable chance or error
as we anticipated the readability score of the reference summaries is much higher than it is for the system summaries

comparison with related research we compare our best summarization model to related models for english and slovene languages
table shows the results reported by authors
in addition to standard rouge scores we also provide bertscore where possible
the only other neural summarization model for slovene was built by zidarn who used a two layer lstm neural network with attention mechanism copy mechanism and beam search
as our approach the training set used the sta news dataset extracted from gigada corpus but with different train test and idations splits
our model scored higher on difference but lower on and rouge l
bertscore of both models was tical
given the existing sources of variation different subsets of the original data different splits and problematic nature of automatic summary evaluation metrics we can conclude that both models perform similarly
in human evaluation both models produce acceptable readability scores but in terms of accuracy it seems that our model generates more accurate content
neither cross lingual nor monolingual slovene models can compare to english models in terms of performance
english models are usually trained either on the million instances of gigaword dataset appropriate for headline generation or the cnn daily mail dataset which is similar but larger than our slovene dataset
the english model used in our experiments chen and bansal achieves scores that are almost twice as high on english as ours on slovene
its results are less misleading and usually represents facts and information accurately
a lot of inspected instances show the ability of the model to omit unimportant dependent clauses
this is also present in our model although rarely
pegasus zhang et al
is currently the best abstractive summarization model
it is transformer based and presents an interesting novel insight if ing objectives resemble a downstream task a better and faster ne tuning follows
authors thus propose two pretraining objectives
one is the bert masked guage model known from devlin et al

another is the gap sentence ation that selects and masks whole sentences from documents and concatenates the gap sentences into a pseudo summary
the model is pretrained on two very large corpora
the dataset consists of texts from m web pages gb
the hugenews dataset is even larger with articles tb
the model achieved state of the art performance on summarization tasks
model zidarn slovene lstm our slovene cross lingual model english chen and bansal english zhang et al
rouge l bertscore table comparison with related research
conclusion and further work we developed a neural cross lingual model for abstractive summarization
our lution is based on the pretrained model in resource rich language english whose outputs are corrected with trained language model in target language slovene
we tested how different amounts of training data in target language used in ne tuning affect the model
in addition to automatic evaluation we used human evaluation of the summaries
additional contributions of our work are the rst slovene marization dataset consisting from news and publicly available character based transformer neural language model
our cross lingual approach generates useful summaries even in a zero shot transfer mode
by gradually increasing the amount of target language data used in ne tuning the performance gradually approaches the performance of the model trained using only the target language
with reasonably large amount of data in target language it is similar to the model directly trained on the target language
the ndings conrm that the quality and size of a dataset dene the range of neural networks performance
in our case this is most evident when considering diverse topics from the dataset
topics which are better represented in the dataset are much better summarized than less represented ones
human evaluation shows that our model generates summaries with reasonably accurate content and acceptable readability
the model can be improved in several ways
first the quality of cross lingual alignment in our case is much lower than for some other language pairs
recently introduced contextual embeddings such as bert devlin et al
or elmo peters et al
have improved almost all tasks where they were applied
ther it may be necessary to increase the vocabulary size because of the rich slovene morphology
instead of rouge reward rl step could maximize bertscore ward
readability measures can be used to assess the readability of generated summaries
we could improve the quality of the ne tuning dataset by procuring news articles with original summary text splits instead of currently used tics
additionally we could denoise the dataset by calculating bertscore scores between a reference summary and text and retain only the best matching pairs
future studies could investigate how to improve metrics for abstractive text summarization
one idea is to combine content based metrics rouge bertscore with perplexity measure to ensure both accuracy and readability in the same metric
an interesting problem for future work is how to attain greater levels of tion
in cross lingual and model transfer research the inuence of special tokens should be studied
acknowledgements the research was supported by the slovene research agency through research core funding no

the research was nancially supported by european social fund and republic of slovenia ministry of education science and sport through projects quality of slovene textbooks kauc and development of slovene in digital environment rsdo
this paper is supported by european union s zon programme project embeddia cross lingual embeddings for represented languages in european news media grant no

the results of this publication reects only the authors view and the commission is not sponsible for any use that may be made of the information it contains
references oliver adams adam makarucha graham neubig steven bird and trevor cohn
in cross lingual word embeddings for low resource language modeling
proceedings of the conference of the european chapter of the association for computational linguistics volume long papers pages
mikel artetxe and holger schwenk
massively multilingual sentence embeddings for zero shot cross lingual transfer and beyond
transactions of the ation for computational linguistics
alexei baevski and michael auli
adaptive input representations for neural guage modeling
arxiv preprint

dzmitry bahdanau kyunghyun cho and yoshua bengio
neural machine lation by jointly learning to align and translate
in international conference on learning representations
piotr bojanowski edouard grave armand joulin and tomas mikolov
enriching word vectors with subword information
transactions of the association for computational linguistics
ciprian chelba tomas mikolov mike schuster qi ge thorsten brants phillipp koehn and tony robinson
one billion word benchmark for measuring progress in statistical language modeling
in fifteenth annual conference of the tional speech communication association
yen chun chen and mohit bansal
fast abstractive summarization with selected sentence rewriting
in proceedings of the annual meeting of the association for computational linguistics volume long papers pages
zewen chi li dong furu wei wenhui wang xian ling mao and heyan huang
cross lingual natural language generation via pre training
arxiv pages arxiv
alexis conneau guillaume lample marcaurelio ranzato ludovic denoyer arxiv preprint and herve jegou
word translation without parallel data


zihang dai zhilin yang yiming yang jaime g carbonell quoc le and lan salakhutdinov
transformer xl attentive language models beyond a length context
in proceedings of the annual meeting of the association for computational linguistics pages
jacob devlin ming wei chang kenton lee and kristina toutanova
bert training of deep bidirectional transformers for language understanding
in ceedings of the conference of the north american chapter of the ation for computational linguistics human language technologies volume long and short papers pages
pascal fecht sebastian blank and hans peter zorn
sequential transfer learning in nlp for german text summarization
in proceedings of the edition of the swiss text analytics conference
mahak gambhir and vishal gupta
recent automatic text summarization niques a survey
articial intelligence review
gigada

a reference corpus of slovenian

cjvt
gigada system about
cjvt
si gigafida system about
retrieved from url
edouard grave piotr bojanowski prakhar gupta armand joulin and tomas in language resources mikolov
learning word vectors for languages
and evaluation conference
baotian hu qingcai chen and fangze zhu
lcsts a large scale chinese short text in proceedings of the conference on empirical summarization dataset
methods in natural language processing pages
chin yew lin and eduard hovy
manual and automatic evaluation of summaries
in proceedings of the workshop on automatic summarization volume pages
rada mihalcea
graph based ranking algorithms for sentence extraction applied to text summarization
in proceedings of the acl interactive poster and stration sessions pages
tomas mikolov kai chen greg corrado and jeffrey dean
efcient estimation of word representations in vector space


tomas mikolov quoc v
le and ilya sutskever
exploiting similarities among languages for machine translation


ramesh nallapati bowen zhou cicero dos santos caglar gulcehre and bing xiang
abstractive text summarization using sequence to sequence rnns and in proceedings of the signll conference on computational beyond
natural language learning pages
jekaterina novikova ondrej dusek amanda cercas curry and verena rieser
why we need new evaluation metrics for
in proceedings of the ence on empirical methods in natural language processing pages
jessica ouyang boya song and kathleen mckeown
a robust abstractive in proceedings of the conference tem for cross lingual summarization
of the north american chapter of the association for computational tics human language technologies volume long and short papers pages
jeffrey pennington richard socher and christopher manning
glove global tors for word representation
in proceedings of the conference on pirical methods in natural language processing emnlp pages
matthew e peters mark neumann mohit iyyer matt gardner christopher clark kenton lee and luke zettlemoyer
deep contextualized word representations
in proceedings of naacl hlt pages
sebastian ruder ivan vulic and anders sgaard
a survey of cross lingual word embedding models
journal of articial intelligence research
alexander m rush sumit chopra and jason weston
a neural attention model for abstractive sentence summarization
in proceedings of the conference on empirical methods in natural language processing pages
evan sandhaus
the new york times annotated corpus
linguistic data tium philadelphia
abigail see peter j liu and christopher d manning
get to the point marization with pointer generator networks
in proceedings of the annual meeting of the association for computational linguistics volume long pers pages
milan straka nikita mediankin tom kocmi zdenek vojtech hudecek and jan hajic
sumeczech large czech news based summarization dataset
in proceedings of the eleventh international conference on language resources and evaluation lrec
zhaopeng tu zhengdong lu yang liu xiaohua liu and hang li
modeling in proceedings of the annual coverage for neural machine translation
meeting of the association for computational linguistics volume long pers pages
ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan n gomez ukasz kaiser and illia polosukhin
attention is all you need
in proceedings of the international conference on neural information cessing systems pages
ashish vaswani samy bengio eugene brevdo francois chollet aidan n
gomez stephan gouws llion jones ukasz kaiser nal kalchbrenner niki parmar ryan sepassi noam shazeer and jakob uszkoreit
for neural machine translation
corr

url http
org

jingqing zhang yao zhao mohammad saleh and peter j
liu
sus pre training with extracted gap sentences for abstractive summarization


tianyi zhang varsha kishore felix wu kilian q
weinberger and yoav artzi
bertscore evaluating text generation with bert


junnan zhu haoran li tianshang liu yu zhou jiajun zhang and chengqing in zong
msmo multimodal summarization with multimodal output
ceedings of the conference on empirical methods in natural language processing pages
junnan zhu qian wang yining wang yu zhou jiajun zhang shaonan wang and chengqing zong
ncls neural cross lingual summarization
in proceedings of the conference on empirical methods in natural language processing and the international joint conference on natural language processing emnlp ijcnlp pages
rok zidarn
automatic text summarization of slovene texts using deep neural works
university of ljubljana faculty of computer and information science ljubljana
msc thesis in slovene

