faithful to the original fact aware neural abstractive summarization ziqiang furu wenjie sujian of computing the hong kong polytechnic university hong kong kong polytechnic university shenzhen research institute china research beijing china laboratory of computational linguistics peking university moe china cszqcao
polyu
edu
hk
com
edu
cn v o n r i
s c v
v i x r a abstract unlike extractive summarization abstractive summarization has to fuse different parts of the source text which inclines to create fake facts
our preliminary study reveals nearly of the outputs from a state of the art neural summarization system suffer from this problem
while previous abstractive summarization approaches usually focus on the improvement of informativeness we argue that faithfulness is also a tal prerequisite for a practical abstractive summarization tem
to avoid generating fake facts in a summary we age open information extraction and dependency parse nologies to extract actual fact descriptions from the source text
the dual attention sequence to sequence framework is then proposed to force the generation conditioned on both the source text and the extracted fact descriptions
ments on the gigaword benchmark dataset demonstrate that our model can greatly reduce fake summaries by
tably the fact descriptions also bring signicant improvement on informativeness since they often condense the meaning of the source text
introduction the exponentially growing online information has tated the development of effective automatic summarization systems
in this paper we focus on an increasingly ing task i
e
abstractive sentence summarization rush chopra and weston which generates a shorter sion of a given sentence while attempting to preserve its original meaning
this task is different from level summarization since it is hard to apply the mon extractive techniques over and yen
ing existing sentences to form the sentence summary is impossible
early studies on sentence summarization volve handcrafted rules zajic et al
syntactic tree pruning knight and marcu and statistical machine translation techniques banko mittal and witbrock
recently the application of the attentional sequence sequence framework has attracted growing attention in this area rush chopra and weston chopra et al
nallapati et al

contribution during internship at microsoft research copyright association for the advancement of articial intelligence www
aaai
org
all rights reserved
source target the repatriation of at least bosnian moslems was postponed friday after the unhcr pulled out of the rst joint scheme to return refugees to their homes in northwest bosnia
repatriation of bosnian moslems postponed bosnian moslems postponed after unhcr pulled out of bosnia table an example of fake summaries generated by the state of the art model
stands for a digit masked ing preprocessing
as we know sentence summarization inevitably needs to fuse different parts in the source sentence and is stractive
consequently the generated summaries often match with the original relations and yield fake facts
our preliminary study reveals that nearly of the outputs from a state of the art system suffer from this lem
previous researches are usually devoted to increasing summary informativeness
however one of the most sential prerequisites for a practical abstractive tion system is that the generated summaries must accord with the facts expressed in the source
we refer to this pect as summary faithfulness in this paper
a fake mary may greatly misguide the comprehension of the inal text
look at an illustrative example of the tion result using the state of the art model nallapati et al
in table
the actual subject of the verb poned is repatriation
nevertheless probably because the entity bosnian moslems is closer to postponed in the source sentence the summarization system wrongly regards bosnian moslems as the subject and counterfeits a fact bosnian moslems postponed
meanwhile the system generates another fake fact unhcr pulled out of bosnia and puts it into the summary
consequently although the informativeness
and readability of this summary are high its meaning departs far from the original
this sort of summaries is nearly useless in practice
since the fact fabrication is a serious problem itively encoding existing facts into the summarization tem should be an ideal solution to avoid fake generation
to achieve this goal the rst step is to extract the facts from the source sentence
in the relatively mature task of open information extraction openie banko et al
a fact is usually represented by a relation triple consisting of ject predicate object
for example given the source tence in table the popular openie tool angeli mar and manning generates two relation triples cluding repatriation was postponed friday and unhcr pulled out of rst joint scheme
obviously these triples can help rectify the mistakes made by the model
ever the relation triples are not always extractable e

from the imperative sentences
hence we further adopt a dency parser and supplement with the subject predicate and predicate object tuples identied from the parse tree of the sentence
this is also inspired by the work of parse tree based sentence compression e

knight and marcu
we represent a fact through merging words in a triple or tuples to form a short sentence dened as a fact description
fact descriptions actually form the skeletons of sentences
thus we incorporate them as an additional input source text in our model
our experiments reveal that the words in the extracted fact descriptions are more likely to be included in the actual summaries than the entire words in the source sentences
that is fact descriptions clearly vide the right guidance for summarization
next using both source sentence and fact descriptions as input we extend the state of the art attentional model nallapati et al
to fully leverage their information
specially we use two recurrent neural network rnn encoders to read the tence and fact descriptions in parallel
with respective tion mechanisms our model computes the sentence and fact context vectors
it then merges the two vectors according to their relative reliabilities
finally a rnn decoder makes use of the integrated context to generate the summary by word
since our summarization system encodes facts to enhance faithfulness we call it ftsum
to verify the effectiveness of ftsum we conduct sive experiments on the gigaword sentence summarization benchmark dataset rush chopra and weston
the results show that our model greatly reduces the fake maries by compared to the state of the art work
due to the compression nature of fact descriptions the use of them also brings the signicant improvement in terms of automatic informativeness evaluation
the butions of our work can be summarized as follows to the best of our knowledge we are the rst to explore the faithfulness problem of abstractive summarization
we propose a dual attention model to push the ation to follow the original facts
since the fact descriptions often condense the meaning of the source sentence they also bring the signicant benet to promote informativeness
fact description extraction based on our observation of summaries generated by state of the art models suffer from fact fabrication such as the mismatch between the predicate and its subject or object
therefore we propose to explicitly encode existing fact descriptions into the model
we leverage popular tools of open information extraction openie and dependency sentence triples i saw a cat sitting on the desk i saw cat i saw cat sitting i saw cat sitting on desk table examples of openie triples in different ities
we extract the following fact description i saw cat sitting on desk parser for this purpose
openie refers to the extraction of entity relations from the open domain text
in openie a fact is typically interpreted as a relation triple consisting of ject predicate object
we join all the items in a triple i
e
subject predicate object since it usually acts as a concise sentence
an example of the openie outputs is presented in table
as we can see openie may extract multiple triples to reect an identical fact in different granularities
in some extreme cases one relation can yield over triple variants which brings high redundancy and burdens the computation cost of the model
to balance redundancy and fact ness we remove a relation triple if all its words are covered by another one
for example only the last fact description i
e
i saw cat sitting on desk in table is reserved
when different fact descriptions are extracted at the end we use a special separator to concatenate them to accelerate the encoding process which is explained by eq
and
openie is able to give a complete description of the entity relations
however it is worth noting that the relation triples are not always extractable e

from the imperative tences
in fact about of the openie outputs are empty on our dataset
these empty instances are likely to damage the robustness of our model
as observed although the plete relation triples are not always available the subject predicate or predicate object tuples are almost present in each sentence
therefore we leverage the dependency parser to dig out the appropriate tuples to supplement the fact descriptions
a dependency parser converts a sentence into the labeled governor dependent tuples
we extract the predicate related tuples according to the labels nsubj jpass csubj csubjpass and dobj
to acquire more complete fact descriptions we also reserve the important modiers cluding the adjectival numeric nummod and noun compound compound
we then merge the tuples ing the same words and order words based on the original sentence to form the fact descriptions
take the dependency tree in fig
as an example
the output of openie is empty for this sentence
based on the dependency parser we rstly lter the following predicate related tuples prices opened opened tuesday dealers said and the modify head ples taiwan price share price lower tuesday
these tuples are then merged to form two fact descriptions taiwan share prices opened lower tuesday dealers said
in the experiments we employ the popular nlp pipeline stanford corenlp manning et al
to handle nie and dependency parse at the same time
we bine the fact descriptions derived from both parts and screen out the fact descriptions with the pattern somebody said declared announced which are usually meaningless figure a dependency tree example
the meaning of the dependency labels can be referred to de marneffe and manning
we extract the following two fact descriptions taiwan share prices opened lower tuesday dealers said and insignicant
referring to the copy ratios in table words in fact descriptions are more likely to be used in the summary than the words in the original sentence
it dicates that fact descriptions truly condense the meaning of sentences to a large extent
the above statistics also supports the practice of dependency parse based compressive marization knight and marcu
however the length sum of extracted fact descriptions is shorter than the actual summary in of the sentences and of the sentences even hold empty fact descriptions
in addition from table we can nd that on average one key source word is missing in the fact descriptions
thus without the source sentence we can not reply on fact descriptions alone to generate maries
as an example
the gru at the time step i is dened as follows hi the bigru consists of a forward gru and a backward hn gru
suppose the corresponding outputs are hn respectively
then the composite hidden and state of a word is the concatenation of the two gru sentations i
e
hi hi hi
for the relation sequence r since it contains multiple dependent fact descriptions we introduce boundary tors to separate their hidden states
specially the value of is dened as follows source avglen count
sentence
fact


table comparisons between source sentences and lations
avglen is the average number of tokens
means the proportion of source tokens can be found in the summary
fact aware neural summarization model framework as shown in figure our model consists of three ules including two encoders and a dual attention decoder equipped with a context selection gate network
the tence encoder reads the input words xn and builds its corresponding representation hx n
wise the relation encoder converts the fact descriptions r rk into hidden states hr
with the spective attention mechanisms our model computes the tence and relation context vectors cx at each ing time step t
the gate network is followed to merge the context vectors according to their relative associations with the current generation
the decoder produces summaries yl word by word conditioned on the tailored context vector which embeds the semantics of both source sentence and fact descriptions
t and encoders the input includes the source sentence and the fact scriptions r
for each sequence we employ the bidirectional gated recurrent unit bigru encoder cho et al
to construct its semantic representation
take the sentence then is used to reset the gru state in eq
i ri is otherwise i ihi in this way all the fact descriptions will start with the same zero vector
in other words they are encoded independently
finally both sentence hidden states hx i and relation den states hr i are fed to the decoder
dual attention decoder previous models have developed some task specic modications on the decoder such as to incorporate the copying mechanism gu et al
and coverage nism see liu and manning
as this paper focuses on the faithfulness problem we use the most popular coder i
e
gru with attentions bahdanau cho and gio
at each decoding time step t gru reads the vious output and context vector as inputs to pute new hidden state st st ct since we have both sentence and relation representations as input we develop two attentional layers to construct the overall context vector
for instance the context tation of the sentence at time step t is computed as luong pham and manning t i hx ex i t i j t ihx i cx t t i i t j figure model framework t and where mlp stands for multi layer perceptrons
the context vector of the relation can be computed similarly
we bine cx to build the overall context vector
we plore two alternative combination approaches
the rst one is called ftsumc which simply concatenates two context vectors ct cx t the other approach is denoted as ftsumg where we also use mlp to build a gate network and combine context tors with the weighted sum gt ct gt cx t t t gt cr t where means the element wise dot
experiments show that ftsumg signicantly outperforms ftsumc and the gate values apparently reect the relative reliability of tence and fact descriptions
finally the softmax layer is introduced to generate the next word based on previous word context vector ct and current decoder state st
ot wcct wsst t where w
stands for a weight matrix
learning the learning goal is to maximize the estimated probability of the actual summary
we adopt the common negative likelihood nll as the loss function
r r where d denotes the training dataset and stands for the model parameters
we use adam kingma and ba with mini batches as the optimization algorithm
we set the learning rate
and the mini batch size to
lar to zhou et al
we evaluate the model performance on the development set for every batches and halve the dataset count avgsourcelen avgtargetlen train dev

m



test

table data statistics for the english gigaword
sourcelen is the average input sentence length and getlen is the average headline length
learning rate if the cost increases for consecutive dations
in addition we apply gradient clipping pascanu mikolov and bengio with range during ing to enhance the stability of the model
experiments datasets we conduct experiments on the annotated english word corpus as with rush chopra and weston
this parallel corpus is produced by pairing the rst tence in the news article and its headline as the summary with heuristic rules
the training and development datasets are built through the released by rush chopra and weston
the script also performs various basic text normalization including tokenization lower casing ing all digit characters with and mask the words appearing less than times with a unk tag
it comes up with about
m sentence headline pairs as the training set and k pairs as the development set
we use the same gigaword test set as rush chopra and weston
it contains sentence headline pairs
following rush chopra and weston we remove pairs with empty titles leading to slightly different accuracy compared with rush chopra and weston
the statistics of the gigaword corpus is presented in table
evaluation metric we adopt rouge lin for automatic evaluation
rouge has been the standard evaluation metric for duc
com facebook namas encoderdual attention encoderattentiongrumlpcontext shared tasks since
it measures the quality of mary by computing overlapping lexical units between the candidate summary and actual summaries such as unigram bigram and longest common subsequence lcs
ing the common practice we report unigram bi gram and rouge l lcs in the following experiments
and mainly consider informativeness while rouge l is supposed to be linked to readability
in addition we manually inspect whether the ated summaries accord with the facts in the original tences
we mark summaries into three categories ful fake and unclear
the last one refers to the case where a generated summary is too incomplete to judge its faithfulness such as just producing a unk tag
implementation details since the dataset has already masked infrequent words with the unk tag we reserve all the rest words in the training set
as a result the sizes of source and target vocabularies are and respectively
with reference to nallapati et al
we leverage the popular framework as the starting point and set the size of word embeddings to
we initialize word embeddings with glove ton socher and manning
all the gru hidden state dimensions are xed to
we use dropout srivastava et al
with probability

with the decoder we use the beam search of size to generate the summary and strict the maximal length of a summary to words
we nd that the average system summary length from all our els about
words is very much consistent with that of the ground truth on the development set without any special tuning
baselines we compare our proposed model with the following six state of the art baselines abs rush chopra and weston used an attentive cnn encoder and nnlm decoder to summarize the tence
rush chopra and weston further tuned the abs model with additional features to balance the stractive and extractive tendency
ras elman as the extension of the abs model it used a convolutional attention based encoder and an rnn coder chopra et al

nallapati et al
used a full rnn model and added the hand crafted features such as pos tag and ner to enhance the encoder representation
luong nmt luong pham and manning applied the two layer lstms neural machine translation model with hidden units in each layer
att we implement the standard attentional with and denote this baseline as att
use the rouge evaluation option

com material model abs ras elman att ftsumc ftsumg perplexity




table final perplexity on the development set
cates the value is cited from the corresponding paper
and luong nmt do not provide this value
model abs ras elman luong nmt ftsumc ftsumg















rg l







table rouge performance
indicates statistical signicance of the corresponding model with respect to the baseline model on the condence interval in the ofcial rouge script
rg refers to rouge for short
informativeness evaluation at rst look at the nal cost values during training in ble
we can see that our model achieves the lowest ity compared against the state of the art systems
it is also noted that ftsumg largely outperforms ftsumc which veries the importance of context selection
the rouge scores are then reported in table
although the focus of our model focuses is to improve faithfulness the rouge scores it receives are also much higher than the other methods
note that and have utilized a series of hand crafted features but our model is totally data driven
even though our model surpasses by and by on
when fact descriptions are ignored our model is equivalent to the standard attentional model
therefore it is safe to conclude that fact descriptions have signicant contribute to the increase of rouge scores
one probable reason is that fact descriptions are much more formative than the original sentence as shown in table
it also largely explains why ftsumg is superior to ftsumc
ftsumc treats the source sentence and relations equally while ftsumg tells the fact descriptions are often more liable as discussed in more detail later
faithfulness evaluation next we conduct manual evaluation to inspect the ness of the generated summaries
specially we randomly select sentences from the test set
then we classify the generated summaries as faithful fake or unclear
for the sake of a complete comparison we present the sults of our system ftsumg together with the the attentional model
as shown in table about of the model att ftsumg count category faithful fake unclear faithful fake unclear table faithfulness performance on the test set
att outputs gives disinformation
this number greatly duces to by our model
nearly of summaries erated by our model is faithful which makes our model far more practical
we nd that att tends to copy the words closer to the predicate and regard them as its subject and object
however this is not always reasonable and thus it is actually counterfeiting messages
in comparison the fact scriptions indeed designate the relations between a predicate and its subject and object
as a result generation in line with the fact descriptions is usually able to keep the faithfulness
we illustrate the examples of defective outputs in ble
as shown att often attempts to fuse different parts in the source sentence to form the summary no matter whether these phrases are relevant or not
for instance treats bosnian moslems as the subject of postponed and bosnia as the object of pulled out of in example
by contract since the fact description point out the tual subject and object the output of our model is faithful
in fact it is exactly the same as the target summary
in ample neither att nor our model achieves satisfactory performance
att again mismatches the object while our model fails to produce a complete sentence
to take a closer look we nd the target summary of this sentence is what strange it merely focuses on the prepositional phrase after taking a stoke


rather than the main clause as usual
since the main clause is hard to summarize and there is no high quality fact description extracted our model fails to give a complete summary
it is also noteworthy that given multiple long fact tions the generation of our model sometimes traps into one item
for instance there are two long fact descriptions in example and our model only utilizes the rst one for eration
as a result despite the high faithfulness the mativeness is somewhat damaged
therefore it seems more reliable to introduce the coverage mechanism see liu and manning to handle the cases like this one
we leave it as our future work
gate analysis as shown in table ftsumg achieves much higher rouge scores than ftsumc
now we investigate what the gate network eq
actually learns
the changes of the gate values on the development set during training are shown in fig

at the beginning the average gate value exceeds
which means the generation is biased to the source sentence
as training proceeds the model realizes that the fact scriptions are more reliable resulting in a consecutive drop of the gate value
finally the average gate value is figure gates change during training
ally stabilized to

interestingly the ratio of sentence and relation gate values i
e



is extremely close to the ratio of copying proportions shown in table i
e




it seems that our model dicts the copy proportion and normalizes it as the gate value
then look at the standard deviation of gates
to our surprise its change is nearly anti symmetric to the mean value
the nal standard deviation reaches about of the mean gate value
thus still many sentences can dominate the ation
this strange observation urges us to carefully check the summaries with top gate values in the velopment set
we nd fact descriptions in the cases are empty and nearly contains the unk tag
our model believes these fact descriptions have not much worth to guide generation
instead there is no empty fact tions and only unk tag in the bottom cases
hence these fact descriptions are usually informative enough
in addition we nd the instances with the lowest gate values often hold the following target summary fact description pair target country share prices close open
percent fact country share prices slumped dropped rose
higher lower percent the extracted fact description itself is already a proper mary
that is why fact descriptions are particularly preferred in generation
related work abstractive sentence summarization chopra et al
aims to produce a shorter version of a given sentence while preserving its meaning
unlike document level tion it is impossible for this task to apply the common tractive techniques e

cao et al

early studies for sentence summarization included rule based methods zajic et al
syntactic tree pruning knight and marcu and statistical machine translation niques banko mittal and witbrock
recently the application of encoder decoder structures has attracted growing attention in this area
rush chopra and weston proposed the abs model which sisted of an attentive convolutional neural network cnn







source relations target att ftsum source the repatriation of at least bosnian moslems was postponed friday after the unhcr pulled out of the rst joint scheme to return refugees to their homes in northwest bosnia
unhcr pulled out of rst joint scheme repatriation was postponed friday unhcr return refugees to their homes repatriation of bosnian moslems postponed fake bosnian moslems postponed after unhcr pulled out of bosnia faithful repatriation of bosnian moslems postponed davis love said he was thinking of making the world cup of golf a full time occupation after taking a stroke lead over japan in the event with us partner fred couples here on saturday
relations making world cup full time occupation taking stroke lead target att ftsum americans lead unk by strokes fake davis love says he is thinking of the world cup unclear love in the world cup of golf example example example source relations target att ftsum the us space shuttle atlantis separated from the orbiting russian mir space station early saturday after three days of test runs for life in a future space facility nasa announced
us space shuttle atlantis separated from orbiting russian mir space station us space shuttle atlantis runs after three days of test for line in future space facility atlantis mir part ways after three day space collaboration by emmanuel unk unclear space shuttle atlantis separated after days of test runs for life faithful space shuttle atlantis separated from mir table examples of defective outputs
we use bold font to indicate the problematic parts
encoder and an neural network language model decoder
chopra et al
extended their work by replacing the coder with recurrent neural network rnn
nallapati et al
followed this line and developed a full rnn based sequence to sequence framework sutskever vinyals and le
experiments on the gigaword test set rush chopra and weston show that the above models achieve state of the art performance
in addition to the direct application of the general framework researchers attempted to import various ties of summarization
for example nallapati et al
enriched the encoder with hand crafted features such as named entities and pos tags
these features played tant roles in traditional feature based summarization tems
gu et al
found that a large proportion of words in the summary were copied from the source text
fore they proposed copynet which considered the copying mechanism during generation
later cao et al
tended this work by directly measuring the copying nism within neural attentions
meanwhile they modied the decoder to reect the rewriting behavior in summarization
recently see liu and manning used the coverage mechanism to discourage repetition
there were also studies to modify the loss function to t the evaluation metrics
for instance ayana liu and sun applied minimum risk training strategy to maximize the rouge scores of erated summaries
paulus xiong and socher used reinforcement learning algorithm to optimize a mixed jective function of likelihood and rouge scores
notably previous researches usually focused on the provement of summary informativeness
to the best of our knowledge we are the rst to explore the faithfulness lem of abstractive summarization
conclusion and future work this paper investigates the faithfulness problem in tive summarization
we employ popular openie and dency parse tools to extract fact descriptions in the source sentence
then we propose the dual attention work to force the generation conditioned on both source tence and the fact descriptions
experiments on the word benchmark demonstrate that our model greatly reduce fake summaries by
in addition since the fact tions often condense the meaning of the sentence the import of them also brings signicant improvement on ness
we believe our work can be extended in various aspects
on the one hand we plan to improve our decoder with the copying mechanism and coverage mechanism which is ther adapted to summarization
on the other hand we are interested in the automatic evaluation of summary
acknowledgments the work described in this paper was supported by research grants council of hong kong polyu tional natural science foundation of china and and the hong kong polytechnic university bcdv
nallapati r
zhou b
gulcehre c
xiang b
et al

abstractive text summarization using sequence to sequence rnns and beyond
arxiv preprint

over p
and yen j

introduction to an intrinsic evaluation of generic news text summarization tems
in proceedings of duc
pascanu r
mikolov t
and bengio y

on the culty of training recurrent neural networks
in international conference on machine learning
paulus r
xiong c
and socher r

a deep forced model for abstractive summarization
arxiv preprint

pennington j
socher r
and manning c
d

glove global vectors for word representation
in empirical ods in natural language processing emnlp
rush a
m
chopra s
and weston j

a neural tention model for abstractive sentence summarization
arxiv preprint

rush a
m
chopra s
and weston j

a neural attention model for abstractive sentence summarization
in proceedings of emnlp
see a
liu p
j
and manning c
d

get to the point summarization with pointer generator networks
arxiv preprint

srivastava n
hinton g
e
krizhevsky a
sutskever i
and salakhutdinov r

dropout a simple way to vent neural networks from overtting
journal of machine learning research
sutskever i
vinyals o
and le q
v

sequence to sequence learning with neural networks
in advances in neural information processing systems

zajic d
dorr b
j
lin j
and schwartz r
multi candidate reduction sentence compression as a tool for document summarization tasks
information processing management
zhou q
yang n
wei f
and zhou m

tive encoding for abstractive sentence summarization
arxiv preprint

references angeli g
premkumar m
j
and manning c
d

leveraging linguistic structure for open domain information in proceedings of the annual meeting of extraction
the association for computational linguistics acl
ayana s
s
liu z
and sun m

neural line generation with minimum risk training
arxiv preprint

bahdanau d
cho k
and bengio y

neural chine translation by jointly learning to align and translate
arxiv preprint

banko m
cafarella m
j
soderland s
broadhead m
and etzioni o

open information extraction from the web
in ijcai volume
banko m
mittal v
o
and witbrock m
j

line generation based on statistical translation
in ings of the annual meeting on association for putational linguistics
association for tional linguistics
cao z
wei f
dong l
li s
and zhou m

ranking with recursive neural networks and its application to multi document summarization
in proceedings of aaai
cao z
wei f
li s
li w
zhou m
and wang h

learning summary prior representation for extractive summarization
proceedings of acl short papers
cao z
chu w
li w
and li s

joint copying and restricted generation for paraphrase
in proceedings of aaai
cho k
van merrienboer b
gulcehre c
bahdanau d
bougares f
schwenk h
and bengio y

learning phrase representations using rnn encoder decoder for tical machine translation
arxiv preprint

chopra s
auli m
rush a
m
and harvard s

abstractive sentence summarization with attentive recurrent neural networks
proceedings of naacl
de marneffe m

and manning c
d

stanford typed dependencies manual
technical report technical port stanford university
gu j
lu z
li h
and li v
o

ing copying mechanism in sequence to sequence learning
arxiv preprint

kingma d
and ba j

adam a method for stochastic optimization
arxiv preprint

knight k
and marcu d

summarization beyond sentence extraction a probabilistic approach to sentence compression
articial intelligence
lin c


rouge a package for automatic evaluation of summaries
in proceedings of the acl workshop
luong m

pham h
and manning c
d

tive approaches to attention based neural machine tion
arxiv preprint

manning c
d
surdeanu m
bauer j
finkel j
bethard s
j
and mcclosky d

the stanford corenlp in proceedings of acl ural language processing toolkit
system demonstrations

