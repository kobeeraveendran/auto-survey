a noisy channel model for document compression hal daume iii and daniel marcu information sciences institute university of southern california admiralty way suite marina del rey ca hdaume
edu l u j l c
s c v
v i x r a abstract we present a document compression tem that uses a hierarchical noisy channel model of text production
our sion system rst automatically derives the syntactic structure of each sentence and the overall discourse structure of the text given as input
the system then uses a tistical hierarchical model of text tion in order to drop non important tactic and discourse constituents so as to coherent grammatical document compressions of arbitrary length
the tem outperforms both a baseline and a sentence based compression system that operates by simplifying sequentially all sentences in a text
our results support the claim that discourse knowledge plays an important role in document tion
introduction single document summarization systems proposed to date fall within one of the following three classes extractive summarizers simply select and present to the user the most important sentences in a text see mani and maybury marcu mani for comprehensive overviews of the methods and algorithms used to accomplish this
headline generators are noisy channel tic systems that are trained on large corpora of hheadline texti pairs banko et al
berger and mittal
these systems duce short sequences of words that are tive of the content of the text given as input
sentence simplication systems chandrasekar et al
mahesh carroll et al
grefenstette jing knight and marcu are capable of compressing long sentences by deleting unimportant words and phrases
extraction based summarizers often produce puts that contain non important sentence fragments
for example the hypothetical extractive summary of text which is shown in table can be pacted further by deleting the clause which is ready almost enough to win
headline based maries such as that shown in table are usually indicative of a text s content but not informative grammatical or coherent
by repeatedly applying a sentence simplication algorithm one sentence at a time one can compress a text yet the outputs erated in this way are likely to be incoherent and to contain unimportant information
when rizing text some sentences should be dropped gether
ideally we would like to build systems that have the strengths of all these three classes of approaches
the document compression entry in table shows a grammatical coherent summary of text which was generated by a hypothetical document compression system that preserves the most tant information in a text while deleting sentences phrases and words that are subsidiary to the main message of the text
obviously generating ent grammatical summaries such as that produced by the hypothetical document compression system in table is not trivial because of many conicting type of summarizer extractive summarizer headline generator sentence simplier document compressor hypothetical output output contains only important info output is coherent output is grammatical john doe has already secured the vote of most democrats in his constituency which is already almost enough to win
but without the support of the governer he is still on shaky ground
mayor vote constituency governer the mayor is now looking for re election
john doe has already secured the vote of most democrats in his constituency
he is still on shaky ground
john doe has secured the vote of most democrats
but he is still on shaky ground
table hypothetical outputs generated by various types of summarizers

the deletion of certain sentences may result in incoherence and information loss
the deletion of certain words and phrases may also lead to maticality and information loss
the mayor is now looking for re election
john doe has already secured the vote of most democrats in his constituency which is already almost enough to win
but without the support of the governer he is still on shaky grounds
in this paper we present a document compression system that uses hierarchical models of discourse and syntax in order to simultaneously manage all these conicting goals
our compression system rst automatically derives the syntactic structure of each sentence and the overall discourse structure of the text given as input
the system then uses a tistical hierarchical model of text production in der to drop non important syntactic and discourse units so as to generate coherent grammatical ument compressions of arbitrary length
the system outperforms both a baseline and a sentence based compression system that operates by simplifying quentially all sentences in a text
document compression the document compression task is conceptually simple
given a document d


wni our goal is to produce a new document d by dropping words wi from d
in order to achieve this goal we number of other systems use the outputs of tive summarizers and repair them to improve coherence duc duc
unfortunately none of these seems exible enough to produce in one shot good summaries that are taneously coherent and grammatical
extent the noisy channel model proposed by knight marcu
their system compressed tences by dropping syntactic constituents but could be applied to entire documents only on a by sentence basis
as discussed in section this is not adequate because the resulting summary may contain many compressed sentences that are vant
in order to extend knight marcu s approach beyond the sentence level we need to glue tences together in a tree structure similar to that used at the sentence level
rhetorical structure theory rst mann and thompson provides us this glue
the tree in figure depicts the rst structure of text
in rst discourse structures are binary trees whose leaves correspond to elementary discourse units edus and whose internal nodes correspond to contiguous text spans
each internal node in an rst tree is characterized by a ical relation
for example the rst sentence in text provides background information for preting the information in sentences and which are in a contrast relation see figure
each lation holds between two adjacent non overlapping text spans called nucleus and satellite
there are a few exceptions to this rule some relations such as list and contrast are multinuclear
the tinction between nuclei and satellites comes from the empirical observation that the nucleus expresses what is more essential to the writer s purpose than the satellite
our system is able to analyze both the discourse structure of a document and the syntactic structure of each of its sentences or edus
it then compresses the document by dropping either syntactic or course constituents
documents containing incoherently juxtaposed sentences
a noisy channel model for a given document d we want to nd the summary text s that maximizes p
using bayes rule we ip this so we end up maximizing p s
thus we are left with modelling two probability distributions p the probability of a document d given a summary s and p s the probability of a summary
we assume that we are given the discourse structure of each document and the syntactic structures of each of its edus
the intuitive way of thinking about this tion of bayes rule reffered to as the noisy channel model is that we start with a summary s and add noise to it yielding a longer document d
the noise added in our model consists of words phrases and discourse units
for instance given the document john doe has secured the vote of most democrats
we could add words to it namely the word already to ate john doe has already secured the vote of most democrats
we could also choose to add an tire syntactic constituent for instance a prepositional phrase to generate john doe has secured the vote of most democrats in his constituency
these are both examples of sentence expansion as used ously by knight marcu
our system however also has the ability to pand on a core message by adding discourse stituents
for instance it could decide to add another discourse constituent to the original summary john doe has secured the vote of most democrats by contrasting the information in the summary with the uncertainty regarding the support of the nor thus yielding the text john doe has secured the vote of most democrats
but without the support of the governor he is still on shaky ground
as in any noisy channel application there are three parts that we have to account for if we are to build a complete document compression system the channel model the source model and the decoder
we describe each of these below
the source model assigns to a string the ity p s the probability that the summary s is good english
the source model ideally should disfavor ungrammatical sentences and the channel model assigns to any ment summary pair a probability p
this models the extent to which d is a good expansion of s
for instance if s is the mayor is now looking for re election
is the mayor is now looking for re election
he has to secure the vote of the democrats
and is the major is now looking for re election
sharks have sharp teeth
we expect p to be higher than p because expands on s by elaboration while shifts to a different topic yielding an incoherent text
the decoder searches through all possible maries of a document d for the summary s that maximizes the posterior probability p s
each of these parts is described below

source model the job of the source model is to assign a score p s to a compression independent of the original document
that is the source model should measure how good english a summary is independent of whether it is a good compression or not
currently we use a bigram measure of quality trigram scores were also tested but failed to make a difference combined with non lexicalized context free tic probabilities and context free discourse ities giving p s pp cf pdp cf
it would be better to use a ized context free grammar but that was not possible given the decoder used

channel model the channel model is allowed to add syntactic constituents through a stochastic operation called constituent expand or discourse units through other stochastic operation called edu expand
both of these operations are performed on a bined discourse syntax tree called the ds tree
the ds tree for text is shown in figure for ence
suppose we start with the summary s the mayor is looking for re election
a sat background top s npb vp dt nn the mayor root nuc span nuc contrast nuc contrast vbz advp vpa is rb vbg pp now looking john doe has already secured the vote of most democrats in his constituency nuc span sat evaluation sat condition nuc span which is already almost enough to win
but without the support of the governer he is still on shaky ground
npb in for nn punc
reelection
figure the discourse partial tree for text
expand operation could insert a syntactic stituent such as this year anywhere in the tic tree of s
a constituent expand operation could also add single words for instance the word now could be added between is and looking yielding d the mayor is now looking for re election
the probability of inserting this word is based on the syntactic structure of the node into which it s serted
knight and marcu describe in detail a noisy channel model that explains how short tences can be expanded into longer ones by inserting and expanding syntactic constituents and words
since our constituent expand stochastic operation simply reimplements knight and marcu s model we do not focus on them here
we refer the reader to knight and marcu for the details
in addition to adding syntactic constituents our system is also able to add discourse units
consider the summary s john doe has already secured the vote of most democrats in his consituency
through a sequence of discourse expansions we can expand upon this summary to reach the original text
a plete discourse expansion process that would occur starting from this initial summary to generate the original document is shown in figure
in this gure we can follow the sequence of steps required to generate our original text ning with our summary s
first through an eration d project d for discourse we crease the depth of the tree adding an intermediate nuc span node
this projection adds a factor of p nuc span nuc nuc span to the ity of this sequence of operations as is shown under the arrow
we are now able to perform the second operation d expand with which we expand on the core sage contained in s by adding a satellite which uates the information presented in s
this expansion adds the probability of performing the expansion called the discourse expansion probabilities pde
an example discourse expansion probability ten p nuc span nuc span sat nuc span nuc span reects the probability of adding an uation satellite onto a nuclear span
the rest of figure shows some of the remaining steps to produce the original document each step beled with the appropriate probability factors
then the probability of the entire expansion is the uct of all those listed probabilities combined with the appropriate probabilities from the syntax side of things
in order to produce the nal score p for a document summary pair we multiply together each of the expansion probabilities in the path ing from s to d
for estimating the parameters for the discourse models we used an rst corpus of wall street journal articles from the penn treebank which we obtained from ldc
the documents in the corpus range in size from to words with an erage of words per document
each document is paired with a discourse structure that was root nuc span john doe has already secured the vote of most democrats in his constituency dproject nuc span span nuc span nuc span dexpand span nuc span sat evaluation nuc span nuc span root nuc span john doe has already secured the vote of most democrats in his constituency root nuc span nuc span john doe has already secured the vote of most democrats in his constituency sat evaluation which is already almost enough to win
root nuc span dproject nuc contrast span nuc contrast nuc span nuc span john doe has already secured the vote of most democrats in his constituency sat evaluation which is already almost enough to win
root sat background nuc span root nuc span nuc span sat background the mayor is now looking for reelection
root nuc span contrast nuc span nuc contrast contrast sat condiation nuc span nuc contrast nuc span dexpand dproject nuc contrast nuc contrast nuc contrast nuc contrast nuc contrast nuc span sat evaluation sat condition nuc span nuc span sat evaluation sat condition nuc span nuc span john doe has already secured the vote of most democrats in his constituency which is already almost enough to win
but without the support of the governer he is still on shaky ground
john doe has already secured the vote of most democrats in his constituency which is already almost enough to win
but without the support of the governer he is still on shaky ground
john doe has already secured the vote of most democrats in his constituency sat evaluation which is already almost enough to win
figure a sequence of discourse expansions for text with probability factors
span nuc contrast nuc contrast nuc span nuc contrast d e a n root nuc span nuc contrast he is still on shaky ground
ally built in the style of rst
see carlson et al
for details concerning the corpus and the notation process
from this corpus we were able to estimate parameters for a discourse pcfg using standard maximum likelihood methods
furthermore document from the same corpus are paired with extractive summaries on the edu level
human annotators were asked which edus were most important suppose in the example tree figure the annotators marked the second and fth edus the starred ones
these stars are propagated up so that any discourse unit that has a descendent considered important is also ered important
from these annotations we could deduce that to compress a nuc contrast that has two children nuc span and sat evaluation we can drop the evaluation satellite
similarly we can compress a nuc contrast that has two children sat condition and nuc span by dropping the rst discourse constituent
finally we can compress the root deriving into sat background nuc span by dropping the sat background constituent
we keep counts of each of these examples and once lected we normalize them to get the discourse pansion probabilities

decoder the goal of the decoder is to combine p s with p to get p
there are a vast number of potential compressions of a large ds tree but we can efciently pack them into a shared forest structure as described in detail by knight marcu
each entry in the shared forest structure has three associated probabilities one from the source syntax pcfg one from the source discourse pcfg and one from the expansion template probabilities described in section

once we have generated a forest representing all possible compressions of the original document we want to extract the best or the n best trees taking into account both the pansion probabilities of the channel model and the bigram and syntax and discourse pcfg ties of the source model
thankfully such a generic extractor has already been built langkilde
for our purposes the extractor selects the trees with the best combination of lm and expansion scores after performing an exhaustive search over all ble summaries
it returns a list of such trees one for each possible length
system the system developed works in a pipelined ion as shown in figure
the rst step along the pipeline is to generate the discourse structure
to do this we use the decision based discourse parser described by marcu
once we have the course structure we send each edu off to a discourse parser achieves an score of
for edu identication
for identifying hierarchical spans
for nuclearity identication and
for relation tagging
input document discourse parser syntax parser forest generator decoder length chooser output summary figure the pipeline of system components
tactic parser collins
the syntax trees of the edus are then merged with the discourse tree in the forest generator to create a ds tree similar to that shown in figure
from this ds tree we ate a forest that subsumes all possible compressions
this forest is then passed on to the forest ranking system which is used as decoder langkilde
the decoder gives us a list of possible compressions for each possible length
example compressions of text are shown in figure together with their respective log probabilities
in order to choose the best compression at any possible length we can not rely only on the log probabilities lest the system always choose the shortest possible compression
in order to sate for this we normalize by length
however in practice simply dividing the log probability by the length of the compression is insufcient for longer documents
experimentally we found a reasonable metric was to for a compression of length n divide each log probability by

this was the job of the length chooser from figure and enabled us to choose a single compression for each document which was used for evaluation
in figure the compression chosen by the length selector is cized and was the shortest
results for testing we began with two sets of data
the rst set is drawn from the wall street journal wsj portion of the penn treebank and consists of uments each containing between and words
the second set is drawn from a collection of tends to be the case for very short documents as the compressions never get sufciently long for the length ization to have an effect
dent compositions and consists of documents each containing between and words
we call this set the mitre corpus hirschman et al

we would liked to have run evaluations on longer ments
unfortunately the forests generated even for relatively small documents are huge
because there are an exponential number of summaries that can be generated for any given the decoder runs out of memory for longer documents therefore we lected shorter subtexts from the original documents
we used both the wsj and mitre data for uation because we wanted to see whether the formance of our system varies with text genre
the mitre data consists mostly of short sentences erage document length from mitre is sentences quite in constrast to the typically long sentences in the wall street journal articles average document length from wsj is
sentences
for purpose of comparison the mitre data was compressed using ve systems random drops random words each word has a chance of being dropped baseline
hand hand compressions done by a human
concat each sentence is compressed individually the results are concatenated together using knight marcu s system here for parison
edu the system described in this paper
sent because syntactic parsers tend not to work well parsing just clauses this system merges together leaves in the discourse tree which are in the same sentence and then proceeds as scribed in this paper
the wall street journal data was evaluated on the above ve systems as well as two additions
since the correct discourse trees were known for these data we thought it wise to test the systems using these human built discourse trees instead of the tomatically derived ones
the additionall two tems were pd edu same as edu except using the perfect discourse trees available from the rst corpus carlson et al

theory a text of n words has possible compressions
len log prob best compression
mayor is now looking which is enough

the mayor is now looking which is already almost enough to win

the mayor is now looking but without support he is still on shaky ground

mayor is now looking but without the support of governer he is still on shaky ground

the mayor is now looking for re election but without the support of the governer he is still on shaky ground
the mayor is now looking which is already almost enough to win
but without the support of the governer he is still on shaky ground

figure possible compressions for text
pd sent the same as sent except using the perfect discourse trees
six human evaluators rated the systems according to three metrics
the rst two presented together to the evaluators were grammaticality and coherence the third presented separately was summary ity
grammaticality was a judgment of how good the english of the compressions were coherence included how well the compression owed for stance anaphors lacking an antecedent would lower coherence
summary quality on the other hand was a judgment of how well the compression tained the meaning of the original document
each measure was rated on a scale from worst to best
we can draw several conclusions from the uation results shown in table along with age compression rate cmp the length of the pressed document divided by the original length
first it is clear that genre inuences the results
because the mitre data contained mostly short tences the syntax and discourse parsers made fewer errors which allowed for better compressions to be generated
for the mitre corpus compressions tained starting from discourse trees built above the sentence level were better than compressions tained starting from discourse trees built above the edu level
for the wsj corpus compression tained starting from discourse trees built above the sentence level were more grammatical but less herent than compressions obtained starting from course trees built above the edu level
choosing the manner in which the discourse and syntactic sentations of texts are mixed should be inuenced by the genre of the texts one is interested to compress
did not run the system on the mitre data with perfect discourse trees because we did not have hand built discourse trees for this corpus
wsj mitre cmp grm coh qual cmp grm coh qual















concat random







edu







sent pd edu







pd sent



hand



table evaluation results the compressions obtained starting from fectly derived discourse trees indicate that perfect discourse structures help greatly in improving ence and grammaticality of generated summaries
it was surprising to see that the summary quality was affected negatively by the use of perfect discourse structures although not statistically signicant
we believe this happened because the text fragments we summarized were extracted from longer documents
it is likely that had the discourse structures been built specically for these short text snippets they would have been different
moreover there was no nent designed to handle cohesion thus it is to be pected that many compressions would contain gling references
overall all our systems outperformed both the random baseline and the concat systems which empirically show that discourse has an important role in document summarization
we performed tests on the results and found that on the wall street journal data the differences in score between the concat and sent systems for grammaticality and coherence were statistically signicant at the level but the difference in score for summary quality was not
for the mitre data the differences in score between the concat and sent systems for cality and summary quality were statistically icant at the level but the difference in score for coherence was not
the score differences for maticality coherence and summary quality between our systems and the baselines were statistically nicant at the level
the results in table which can be also sessed by inspecting the compressions in figure show that in spite of our success we are still far away from human performance levels
an error that our system makes often is that of dropping ments that can not be dropped such as the phrase for re election which is the complement of is looking
we are currently experimenting with icalized models of syntax that would prevent our compression system from dropping required verb guments
we also consider methods for scaling up the decoder to handling documents of more realistic length
acknoledgements this work was partially supported by darpa ito grant nsf grant and a usc dean fellowship to hal daume iii
thanks to kevin knight for discussions related to the project
references michele banko vibhu mittal and michael witbrock

headline generation based on statistical lation
in proceedings of the annual meeting of the association for computational linguistics acl pages hong kong october
adam berger and vibhu mittal

query relevant summarization using faqs
in proceedings of the annual meeting of the association for tional linguistics pages hong kong october
lynn carlson daniel marcu and mary ellen okurowski

building a discourse tagged corpus in the framework of rhetorical structure theory
in ceedings of the sigdial workshop on discourse and dialogue eurospeech aalborg denmark september
john carroll guidon minnen yvonne canning siobhan devlin and john tait

practical simplication of english newspaper text to assist aphasic readers
in proceedings of the workshop on integrating articial intelligence and assistive technology
r
chandrasekar christy doran and srinivas bangalore

motivations and methods for text in proceedings of the sixteenth international tion
conference on computational linguistics coling copenhagen denmark
michael collins

three generative lexicalized models for statistical parsing
in proceedings of the annual meeting of the association for tational linguistics pages madrid spain july
proceedings of the first document understanding ference new orleans la september
proceedings of the second document understanding conference philadelphia pa july
gregory grefenstette

producing intelligent graphic text reduction to provide an audio scanning in working notes of the aaai service for the blind
spring symposium on intelligent text summarization pages stanford university ca march
l
hirschman m
light e
breck and j
burger

deep read a reading comprehension system
in ceedings of the annual meeting of the association for computational linguistics
h
jing

sentence reduction for automatic text summarization
in proceedings of the first annual meeting of the north american chapter of the ciation for computational linguistics pages seattle wa
kevin knight and daniel marcu

statistics based summarization step one sentence compression
in the national conference on articial gence pages austin tx july august
irene langkilde

forest based statistical sentence generation
in proceedings of the annual meeting of the north american chapter of the association for computational linguistics seattle washington april may
kavi mahesh

hypertext summary extraction for fast document browsing
in proceedings of the aaai spring symposium on natural language processing for the world wide web pages
inderjeet mani and mark maybury editors

vances in automatic text summarization
the mit press
inderjeet mani

automatic summarization
william c
mann and sandra a
thompson

rhetorical structure theory toward a functional ory of text organization
text
daniel marcu

the theory and practice of course parsing and summarization
the mit press cambridge massachusetts

