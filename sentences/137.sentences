r a m l c
s c v
v i x r a query and output generating words by querying distributed word representations for paraphrase generation shuming xu wei sujian wenjie xuancheng key lab of computational linguistics school of eecs peking university learning lab beijing institute of big data research peking university of computing the hong kong polytechnic university shumingma xusun lisujian
edu
cn
polyu
edu
hk abstract most recent approaches use the to sequence model for paraphrase the existing sequence to sequence tion
model tends to memorize the words and the patterns in the training dataset instead of ing the meaning of the words
therefore the generated sentences are often cally correct but semantically improper
in this work we introduce a novel model based on the encoder decoder framework called word embedding attention network wean
our proposed model generates the words by ing distributed word representations i
e
ral word embeddings hoping to capturing the meaning of the according words
following previous work we evaluate our model on two paraphrase oriented tasks namely text plication and short text abstractive rization
experimental results show that our model outperforms the sequence to sequence baseline by the bleu score of
and
on two english text simplication datasets and the score of
on a nese summarization dataset
moreover our model achieves state of the art performances on these three benchmark datasets
introduction paraphrase is a restatement of the meaning of a text using other words
many natural language generation tasks are paraphrase orientated such as text simplication and short text tion
text simplication is to make the text easier to read and understand especially for poor ers while short text summarization is to generate a brief sentence to describe the short texts e

posts on the social media
most recent approaches use sequence to sequence model for paraphrase eration prakash et al
cao et al

it code is available at
com lancopku wean compresses the source text information into dense vectors with the neural encoder and the neural decoder generates the target text using the pressed vectors
although neural network models achieve cess in paraphrase generation there are still two major problems
one of the problem is that the isting sequence to sequence model tends to orize the words and the patterns in the training dataset instead of the meaning of the words
the main reason is that the word generator i
e
the output layer of the decoder does not model the semantic information
the word generator which consists of a linear transformation and a softmax operation converts the recurrent neural network rnn output from a small dimension e

to a much larger dimension e

words in the vocabulary where each dimension sents the score of each word
the latent tion of the word generator is that each word is dependent and the score is irrelevant to each other
therefore the scores of a word and its synonyms may be of great difference which means the word generator learns the word itself rather than the lationship between words
the other problem is that the word generator has a huge number of parameters
suppose we have a sequence to sequence model with a den size of and a vocabulary size of
the word generator has up to million ters which is even larger than other parts of the encoder decoder model in total
the huge size of parameters will result in slow convergence cause there are a lot of parameters to be learned
moreover under the distributed framework the more parameters a model has the more bandwidth and memory it consumes
to tackle both of the problems we propose a novel model called word embedding attention network wean
the word generator of wean is attention based instead of the simple linear max operation
in our attention based word erator the rnn output is a query the candidate words are the values and the corresponding word in order to predict representations are the keys
the word the attention mechanism is used to lect the value matching the query most by means of querying the keys
in this way our model erates the words according to the distributed word representations i
e
neural word embeddings in a retrieval style rather than the traditional ative style
our model is able to capture the mantic meaning of a word by referring to its bedding
besides the attention mechanism has a much smaller number of parameters compared with the linear transformation directly from the rnn output space to the vocabulary space
the reduction of the parameters can increase the vergence rate and speed up the training process
moreover the word embedding is updated from three sources the input of the encoder the input of the decoder and the query of the output layer
following previous work cao et al
we evaluate our model on two paraphrase oriented tasks namely text simplication and short text abstractive summarization
experimental results show that our model outperforms the sequence sequence baseline by the bleu score of
and
on two english text simplication datasets and the score of
on a chinese marization dataset
moreover our model achieves state of the art performances on all of the mark datasets
proposed model we propose a novel model based on the decoder framework which generates the words by querying distributed word representations with the attention mechanism
in this section we rst present the overview of the model architecture
then we explain the details of the word ation especially the way to query word dings

overview word embedding attention network is based on the encoder decoder framework which consists of two components a source text encoder and a get text decoder
figure is an illustration of our model
given the source texts the encoder presses the source texts into dense representation vectors and the decoder generates the paraphrased texts
to predict a word the decoder uses the den output to query the word embeddings
the word embeddings assess all the candidate words and return the word whose embedding matches the query most
the selected word is emitted as the predicted token and its embedding is then used as the input of the lstm at the next time step
after the back propagation the word embedding is dated from three sources the input of the encoder the input of the decoder and the query of the put layer
we show the details of our wean in the following subsection

encoder and decoder the goal of the source text encoder is to vide a series of dense representation of complex source texts for the decoder
in our model the source text encoder is a long short term memory network lstm which produces the dense resentation


hn from the source text


xn the goal of the target text decoder is to generate a series of paraphrased words from the dense resentation of source texts
fisrt the lstm of the decoder compute the dense representation of erated words st
then the dense representations are fed into an attention layer bahdanau et al
to generate the context vector ct which tures context information of source texts
tion vector ct is calculated by the weighted sum of encoder hidden states ct tihi n x ti hi n hj p where hi is an attentive score between the decoder hidden state st and the encoder hidden state hi
in this way ct and st respectively represent the context information of source texts and the target texts at the tth time step

word generation by querying word embedding for the current sequence to sequence model the word generator computes the distribution of output words yt in a generative style sof st hard admission very hard key


value easy


happy


hard


competitive query admission is extremely competitive figure an overview of word embedding attention network
where w is a trainable parameter matrix k is hidden size and v is the number of words in the vocabulary
when the vocabulary is large the number of parameters will be huge
our model generates the words in a retrieval style rather than the traditional generative style by querying the word embeddings
we denote the combination of the source context vector ct and the target context vector st as the query qt in implementation we select the general attention function as the relevance score function based on the performance on the validation sets
the value pair with the highest score wt et is lected
at the test stage the decoder generates the key wt as the tth predicted word and inputs the value et to the lstm unit at the t time step
at the training stage the scores are normalized as the word probability distribution qt sof ei the candidate words wi and their corresponding embeddings ei are paired as the key value pairs wi


n where n is the number of candidate words
we give the details of how to termine the set of candidate words in section

our model uses qt to query the key value pairs wi


n by evaluating the vance between the query qt and each word tor ei with a score function ei
the query process can be regarded as the attentive selection of the word embeddings
we borrow the attention energy functions luong et al
as the vance score function ei ei t ei qt t waei vt weei dot general concat where wq and we are two trainable parameter matrices and vt is a trainable parameter vector

selection of candidate key value pairs as described in section
the model generates the words in a retrieval style which selects a word according to its embedding from a set of candidate key value pairs
we now give the details of how to obtain the set of candidate key value pairs
we extract the vocabulary from the source text in the training set and select the n most frequent words as the candidate words
we reuse the embeddings of the decoder inputs as the values of the date words which means that the decoder input and the predicted output share the same lary and word embeddings
besides we do not use any pretrained word embeddings in our model so that all of the parameters are learned from scratch

training although our generator is a retrieval style wean is as differentiable as the sequence to sequence model
the objective of training is to minimize the cross entropy between the predicted word bility distribution and the golden one hot tion l yi log x i we use adam optimization method to train the model with the default hyper parameters the learning rate
and


experiments following the previous work cao et al
we test our model on the following two paraphrase orientated tasks text simplication and short text abstractive summarization

text simplication

datasets the datasets are both from the alignments tween english wikipedia and simple glish wikipedia website
the simple english wikipedia is built for the children and adults who are learning the english language and the cles are composed with easy words and short tences
therefore simple english wikipedia is a natural public simplied text corpus
parallel wikipedia simplication corpus pwkp
pwkp zhu et al
is a widely used benchmark for evaluating text simplication systems
it consists of aligned complex text from english wikipedia as of aug
and simple text from simple wikipedia as of aug

the dataset contains tence pairs with
words on average per complex sentence and
words per following the previous simple sentence
work zhang and lapata we remove the duplicate sentence pairs and split the pus with pairs for training pairs for validation and pairs for test
english wikipedia and simple english wikipedia ew sew
ew sew is a licly available dataset provided by hwang et al

to build the corpus they rst align the complex simple sentence pairs score the semantic similarity between the complex tence and the simple sentence and classify
wikipedia
org
wikipedia
org each sentence pair as a good good partial partial or bad match
following the previous work nisioi et al
we discard the classied matches and use the good matches and partial matches with a scaled threshold greater than

the corpus contains about k good matches and k good partial matches
we use this corpus as the ing set and the dataset provided by xu et al
xu et al
as the validation set and the test set
the validation set consists of sentence pairs and the test set contains sentence pairs
besides each complex sentence is paired with reference simplied sentences provided by amazon mechanical turk workers


evaluation metrics following the previous work nisioi et al
hu et al
we evaluate our model with ferent metrics on two tasks
automatic evaluation
we use the bleu score papineni et al
as the automatic evaluation metric
bleu is a widely used metric for machine translation and text plication which measures the agreement between the model outputs and the gold erences
the references can be either single or multiple
in our experiments the ences are single on pwkp and multiple on ew sew
human evaluation
human evaluation is sential to evaluate the quality of the model outputs
following nisioi et al
and zhang et al
we ask the human raters to rate the simplied text in three dimensions fluency adequacy and simplicity
fluency assesses whether the outputs are cally right and well formed
adequacy resents the meaning preservation of the plied text
both the scores of uency and adequacy range from to is very bad and is very good
simplicity shows how simpler the model outputs are than the source text which ranges from to


settings our proposed model is based on the decoder framework
the encoder is implemented on lstm and the decoder is based on lstm with luong style attention luong et al

we pwkp pbmt wubben et al
hybrid narayan and gardent encdeca zhang and lapata dress zhang and lapata dress ls zhang and lapata our implementation wean our proposal bleu






table automatic evaluation of our model and other related systems on pwkp datasets
the results are ported on the test sets
ew sew pbmt r wubben et al
hybrid narayan and gardent sbmt sari xu et al
nts nisioi et al
nts nisioi et al
encdeca zhang and lapata dress zhang and lapata dress ls zhang and lapata our implementation wean our proposal bleu









pwkp nts dress ls wean reference fluency adequacy simplicity all















ew sew fluency adequacy simplicity all
pbmt r
sbmt sari
nts
dress ls
wean
reference

















table human evaluation of our model and other lated systems on pwkp and ew sew datasets
the results are reported on the test sets
sentence simplication models
encdeca is a model based on the implemented by decoder with attention zhang and lapata
table automatic evaluation of our model and other related systems on ew sew datasets
the results are reported on the test sets
pbmt r wubben et al
is a phrase based machine translation model which reranks the outputs
tune our hyper parameter on the development set
the model has two lstm layers
the hidden size of lstm is and the embedding size is
we use adam optimizer kingma and ba to learn the parameters and the batch size is set to be
we set the dropout rate srivastava et al
to be

all of the gradients are clipped when the norm exceeds


baselines we compare our model with several neural text simplication systems
is our implementation of the sequence to sequence model with attention mechanism which is the most popular ral model for text generation
nts and nts nisioi et al
are two sequence to sequence model with tra mechanism like prediction ranking and nts uses a pretrain
dress and dress ls zhang and lapata are two deep reinforcement learning hybrid narayan and gardent is a brid approach which combines deep tics and mono lingual machine translation
sbmt sari xu et al
is a based machine translation model which is trained on ppdb dataset ganitkevitch et al
and tuned with sari


results we compare wean with state of the art els for text simplication
table and table summarize the results of the automatic tion
on pwkp dataset we compare wean with pbmt hybrid encdeca dress and ls
wean achieves a bleu score of
performing all of the previous systems
on sew dataset we compare wean with pbmt r hybrid sbmt sari and the neural models scribed above
we do not nd any public release code of pbmt r and sbmt sari
fortunately xu et al
provides the predictions of r and sbmt sari on ew sew test set so that we can compare our model with these systems
lcsts r l


rnn et al
et al



rnn cont et al



rnn et al






et al
copynet et al






et al
rnn et al






et al






wean table rouge score on the lcsts test set
and r l denote and rouge l respectively
the models with a sufx of w in the table are word based while the rest of els are character based
it shows that the neural models have better formance in bleu and wean achieves the best bleu score with

we perform the human evaluation of wean and other related systems and the results are shown in table
dress ls is based on the forcement learning and it encourages the uency simplicity and relevance of the outputs
fore it achieves a high score in our human uation
wean gains a even better score than dress ls
besides wean generates more equate and simpler outputs than the reference on pwkp
the predictions of sbmt sari are the most adequate among the compared systems on ew sew
in general wean outperforms all of the other systems considering the balance of ency adequate and simplicity
we conduct nicance tests based on t test
the signicance tests suggest that wean has a very signicant improvement over baseline with p
over dress ls in all of the dimension on pwkp p
over dress ls in the dimension of ency p
over nts in the dimension of simplicity and p
over dress ls in the dimension of all

large scale text summarization

dataset large scale chinese social media short text summarization dataset lcsts lcsts is constructed by hu et al

the dataset sists of more than text summary pairs constructed from a famous chinese social media website called sina weibo
it is split into three parts with pairs in part i pairs in part ii and pairs in part iii
all the text summary pairs in part ii and part iii are manually annotated with relevant scores ranged from to
we only reserve pairs with scores no less than leaving pairs in part ii and pairs in part iii
following the previous work hu et al
we use part i as training set part ii as validation set and part iii as test set
is metric

evaluation metrics our rouge evaluation score lin and hovy which is lar for summarization evaluation
the metrics compare an automatically produced summary against the reference summaries by computing overlapping lexical units including unigram gram trigram and longest common subsequence lcs
following previous work rush et al
hu et al
we use igram bi gram and rouge l lcs as the evaluation metrics in the reported experimental results


settings the vocabularies are extracted from the training sets and the source contents and the summaries share the same vocabularies
we tune the parameters based on the rouge scores on the in order to alleviate the risk of validation sets
word segmentation mistakes we split the nese sentences into characters
we prune the cabulary size to which covers most of the common characters
we set the word ding size and the hidden size to the ber of lstm layers of the encoder is and the number of lstm layers of the decoder is
the batch size is and we do not use dropout srivastava et al
on this dataset
following the previous work li et al
we implement a beam search optimization and set the beam size to


baselines we compare our model with the state of the art baselines
rnn and rnn cont are two sequence sequence baseline with gru encoder and coder provided by hu et al


com param pwkp ewsew lcsts
m
m
m
m
m
m wean table the number of the parameters in the put layer
the numbers of rest parameters between and wean are the same
rnn dist chen et al
is a based neural model which the attention mechanism focuses on the different parts of the source content
copynet gu et al
incorporates a copy mechanism to allow part of the ated summary is copied from the source tent
srb ma et al
is a sequence sequence based neural model with improving the semantic relevance between the input text and the output summary
drgd li et al
is a deep recurrent generative decoder model combining the coder with a variational autoencoder
is our the sequence to sequence model with the tion mechanism
implementation of

results we report the rouge score of our model and the baseline models on the test sets
ble summarizes the comparison between our model and the baselines
our model achieves the score of

and
rouge l outperforming all of the ous models
first we compare our model with it shows that the sequence to sequence model
our model signicant outperforms the to sequence baseline with a large margin of

and
l
then we compare our model with other related models
is drgd li et al
which obtains the score of

and
rouge l
our model has a relative gain of

and
rouge l over the state of the art models
the state of the art model training curve u e l b wean epoch figure the training curve of wean and on the pwkp validation set
analysis and discussion
reducing parameters our wean reduces a large number of the eters in the output layer
to analyze the ter reduction we compare our wean model with the sequence to sequence model
table lists the number of the parameters in the output layers of two models
both pwkp and ewsewhave the vocabulary size of words and the hidden size of resulting parameters
lcsts has a vocabulary size of and the hidden size of so the has parameters in the put layers
wean only has two parameter trices and one parameter vector at most in tion without regard to the vocabulary size
it has parameters on pwkp and ewsew and parameters on lcsts
besides wean does not have any extra parameters in the other part of the model

speeding up convergence figure shows the training curve of wean and on the pwkp validation set
wean achieve near the optimal score in only epochs while takes more than epochs to achieve the optimal score
therefore wean has much faster convergence rate compared with
with the much faster training speed wean does not suffer loss in bleu and even improve the bleu score
yoghurt oryogurt isadairyproduct produced bybacterial fermentation ofmilk
yoghurt oryogurt isadairyproduct madebybacterial fermentation ofmilk

oryoghurtisadairy product producedbybacterial fermentation ofmilk
itismadebybacterial fermentation ofmilk
yoghurt oryogurt isadairyproduct producedbybacterial fermentation of
source reference nts nts pbmt r sbmt sari yogurtoryogurt isadairy product drawnupbybacterial fermentation ofmilk
wean source yoghurt oryogurt isadairyproduct madebybacterial fermentation ofmilk
depending on the context another closely related meaning of constituent is that of a citizen residing in the area governed represented or otherwise served by a politician sometimes thisisrestricted tocitizens whoelected thepolitician
thewordconstituentcanalsobeusedtorefertoacitizenwholivesintheareathat is governed represented or otherwise served by a politician sometimes the word is restricted tocitizens whoelected thepolitician
depending on the context another closely related meaning of constituent is that of a citizen living in the area governed represented or otherwise served by a politician sometimes thisisrestricted tocitizens whoelected thepolitician
thisisrestricted tocitizens whoelected thepolitician
depending on the context and meaning of closely related siemens martin is a citizen living in the area or otherwise was governed by a shurba this is restricted topeople whoelected it
reference nts nts pbmt r sbmt sari in terms of the context another closely related sense of the component is that of a citizen living in the area covered makeup or ifnot served byapolicy sometimes whoelected thepolicy
depending on the context another closely related meaning of constituent is that of a citizenwholivesintheareagoverned represented orotherwiseservedbyapolitician sometimes thewordisrestricted tocitizens whoelected thepolitician
wean table two examples of different text simplication system outputs in ew sew dataset
differences from the source texts are shown in bold

case study and very close to the original meaning
table shows two examples of different text plication system outputs on ew sew
for the rst example nts nts and pbmt r miss some essential constituents so that the sentences are incomplete and not uent
sbmt sari erates a uent sentence but the output does not preserve the original meaning
the predicted tence of wean is uent simple and the same as the reference
for the second example omits so many words that it lacks a lot of information
pbmt r generates some vant words like siemens martin and shurba which hurts the uency and adequacy of the generated sentence
sbmt sari is able to generate a uent sentence but the meaning is ferent from the source text and even more cult to understand
compared with the statistic model wean generates a more uent sentence
besides wean can capture the semantic ing of the word by querying the word embeddings so the generated sentence is semantically correct related work our work is related to the encoder decoder framework cho et al
and the attention mechanism bahdanau et al

decoder framework like sequence to sequence model has achieved success in machine lation sutskever et al
jean et al
luong et al
lin et al
text marization rush et al
chopra et al
nallapati et al
wang et al
ma and sun and other natural language processing tasks liu et al

there are many other methods to improve neural attention model jean et al
luong et al

zhu et al
constructs a wikipedia dataset and proposes a tree based simplication model
woodsend and lapata introduces a data driven model based on quasi synchronous grammar which captures structural mismatches and complex rewrite operations
wubben et al
presents a method for text simplication using phrase based machine translation with ranking the outputs
kauchak proposes a text simplication corpus and evaluates language modeling for text simplication on the proposed corpus
narayan and gardent propose a hybrid approach to sentence simplication which combines deep semantics and monolingual chine translation
hwang et al
introduces a parallel simplication corpus by evaluating the similarity between the source text and the ed text based on wordnet
glavas and stajner propose an unsupervised approach to ical simplication that makes use of word tors and require only regular corpora
xu et al
design automatic metrics for text plication
recently most works focus on the neural sequence to sequence model
nisioi et al
present a sequence to sequence model and re ranks the predictions with bleu and sari
zhang and lapata propose a deep forcement learning model to improve the ity uency and adequacy of the simplied texts
cao et al
introduce a novel sequence sequence model to join copying and restricted eration for text simplication
rush et al
rst used an attention based encoder to compress texts and a neural network language decoder to generate summaries
ing this work recurrent encoder was introduced to text summarization and gained better mance lopyrev chopra et al

wards chinese texts hu et al
built a large corpus of chinese short text summarization
to deal with unknown word problem nallapati et al
proposed a generator pointer model so that the decoder is able to generate words in source texts
gu et al
also solved this issue by incorporating copying mechanism
conclusion we propose a novel model based on the decoder framework which generates the words by querying distributed word representations
imental results show that our model outperforms the sequence to sequence baseline by the bleu score of
and
on two english text cation datasets and the score of
on a chinese summarization dataset
moreover our model achieves state of the art performances on these three benchmark datasets
acknowledgements this work was supported in part by national ral science foundation of china no
national high technology research and opment program of china program no
and the national thousand young talents program
xu sun is the sponding author of this paper
references dzmitry bahdanau kyunghyun cho and yoshua neural machine translation by corr bengio

jointly learning to align and translate


ziqiang cao chuwei luo wenjie li and sujian li

joint copying and restricted generation for paraphrase
in proceedings of the thirty first aaai conference on articial intelligence
pages
qian chen xiaodan zhu zhenhua ling si wei and hui jiang

distraction based neural networks for modeling documents
in proceedings of the international joint conference on articial gence ijcai
aaai new york ny
jianpeng cheng and mirella lapata

neural summarization by extracting sentences and words
in proceedings of the annual meeting of the sociation for computational linguistics acl august berlin germany volume long papers
kyunghyun cho bart van merrienboer c aglar gulcehre dzmitry bahdanau fethi bougares ger schwenk and yoshua bengio

learning phrase representations using rnn encoder decoder in proceedings for statistical machine translation
of the conference on empirical methods in natural language processing emnlp
pages
sumit chopra michael auli and alexander m
rush

abstractive sentence summarization with tentive recurrent neural networks
in naacl hlt the conference of the north american chapter of the association for computational guistics human language technologies
pages
juri ganitkevitch benjamin van durme and chris callison burch

the paraphrase database
in human language technologies ference of the north american chapter of the ciation of computational linguistics proceedings
pages
ppdb goran glavas and sanja stajner

simplifying lexical simplication do we need simplied pora in proceedings of the annual meeting of the association for computational linguistics acl
pages
jiatao gu zhengdong lu hang li and victor o
k
incorporating copying mechanism in li

in proceedings of sequence to sequence learning
the annual meeting of the association for putational linguistics acl
baotian hu qingcai chen and fangze zhu

sts a large scale chinese short text summarization dataset
in proceedings of the conference on empirical methods in natural language ing emnlp lisbon portugal september
pages
william hwang hannaneh hajishirzi mari ostendorf and wei wu

aligning sentences from dard wikipedia to simple wikipedia
in naacl hlt
pages
sebastien jean kyunghyun cho roland memisevic and yoshua bengio

on using very large get vocabulary for neural machine translation
in proceedings of the annual meeting of the sociation for computational linguistics acl
pages
thang luong hieu pham and christopher d
ning

effective approaches to attention based in proceedings of the neural machine translation
conference on empirical methods in natural language processing emnlp
pages
shuming ma and xu sun

a semantic vance based neural network for text summarization and text simplication
corr

shuming ma xu sun jingjing xu houfeng wang wenjie li and qi su

improving semantic relevance for sequence to sequence learning of nese social media text summarization
in ings of the annual meeting of the association for computational linguistics acl ver canada july august volume short papers
pages
ramesh nallapati bowen zhou ccero nogueira dos santos c aglar gulcehre and bing xiang

abstractive text summarization using sequence in proceedings of the sequence rnns and beyond
signll conference on computational natural language learning conll berlin germany august
pages
david kauchak

improving text simplication language modeling using unsimplied text data
in proceedings of the annual meeting of the ciation for computational linguistics acl
pages
shashi narayan and claire gardent

hybrid plication using deep semantics and machine in proceedings of the annual lation
ing of the association for computational tics acl
pages
diederik p
kingma and jimmy ba

adam corr a method for stochastic optimization


piji li wai lam lidong bing and zihao wang

deep recurrent generative decoder for stractive text summarization
in proceedings of the conference on empirical methods in natural language processing emnlp copenhagen denmark september
pages
chin yew lin and eduard h
hovy

matic evaluation of summaries using n gram occurrence statistics
in human language ogy conference of the north american chapter of the association for computational linguistics naacl
junyang lin shuming ma qi su and xu sun

decoding history based adaptive control of attention for neural machine translation
corr

tianyu liu kexiang wang lei sha baobao chang and zhifang sui

table to text tion by structure aware learning
corr

sergiu nisioi sanja stajner simone paolo ponzetto and liviu p
dinu

exploring neural text plication models
in proceedings of the nual meeting of the association for computational linguistics acl
pages
kishore papineni salim roukos todd ward and jing zhu

bleu a method for automatic uation of machine translation
in proceedings of the annual meeting of the association for tational linguistics
pages
aaditya prakash sadid a
hasan kathy lee vivek v
datla ashequl qadir joey liu and oladimeji farri

neural paraphrase generation with stacked in coling residual lstm networks
international conference on computational guistics proceedings of the conference cal papers december osaka japan
pages
alexander m
rush sumit chopra and jason weston

a neural attention model for abstractive in proceedings of the tence summarization
conference on empirical methods in natural guage processing emnlp lisbon portugal september
pages
konstantin lopyrev

generating news corr lines with recurrent neural networks


nitish srivastava geoffrey e
hinton alex krizhevsky ilya sutskever and ruslan nov

dropout a simple way to prevent neural wei xu courtney napoles ellie pavlick quanze chen and chris callison burch

optimizing statistical machine translation for text simplication
tacl
xingxing zhang and mirella lapata

tence simplication with deep reinforcement ing
in proceedings of the conference on pirical methods in natural language processing emnlp copenhagen denmark september
pages
zhemin zhu delphine bernhard and iryna gurevych

a monolingual tree based translation model for sentence simplication
in coling
pages
networks from overtting
learning research
journal of machine xu sun xuancheng ren shuming ma and houfeng wang

meprop sparsied back tion for accelerated deep learning with reduced in proceedings of the international tting
conference on machine learning icml ney nsw australia august
pages
xu sun xuancheng ren shuming ma bingzhen wei wei li and houfeng wang

training cation and model simplication for deep learning a minimal effort back propagation method
corr

xu sun bingzhen wei xuancheng ren and shuming ma

label embedding network learning bel representation for soft training of deep networks
corr

ilya sutskever oriol vinyals and quoc v
le

sequence to sequence learning with neural works
in advances in neural information ing systems annual conference on neural mation processing systems
pages
sho takase jun suzuki naoaki okazaki tsutomu rao and masaaki nagata

neural headline generation on abstract meaning representation
in proceedings of the conference on empirical methods in natural language processing emnlp austin texas usa november
pages
kexiang wang tianyu liu zhifang sui and baobao chang

afnity preserving random walk for multi document summarization
in proceedings of the conference on empirical methods in ural language processing emnlp hagen denmark september
pages
kristian woodsend and mirella lapata

ing to simplify sentences with quasi synchronous grammar and integer programming
in proceedings of the conference on empirical methods in natural language processing emnlp
pages
sander wubben antal van den bosch and emiel krahmer

sentence simplication by in the lingual machine translation
nual meeting of the association for computational linguistics proceedings of the conference
pages
jingjing xu xu sun xuancheng ren junyang lin binzhen wei and wei li

dp gan promoting generative adversarial network for corr erating informative and diversied text



