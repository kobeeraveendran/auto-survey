r a m l c
s
c
v
v
i x r a query and output generating words by querying distributed word representations for paraphrase generation shuming xu wei sujian wenjie xuancheng key lab of computational linguistics school of eecs peking university learning lab beijing institute of big data research peking university
of computing the hong kong polytechnic university shumingma xusun lisujian abstract
most recent approaches use the to sequence model for paraphrase

the existing sequence to sequence tion
model tends to memorize the words and the patterns in the training dataset instead of
ing the meaning of the words
therefore the generated sentences are often
cally correct but semantically improper
in this work we introduce a novel model based on the encoder decoder framework called word
embedding attention network wean
our proposed model generates the words by ing distributed word representations ral word embeddings hoping to capturing the meaning of the according words
following previous work we evaluate our model on two paraphrase oriented tasks namely text plication and short text abstractive rization
experimental results show that our model outperforms the sequence to sequence baseline by the bleu score of and on two english text simplication datasets and the score of on a nese summarization dataset
moreover our model achieves state of the art performances on these three benchmark introduction
paraphrase is a restatement of the meaning of a text using other words
many natural language generation tasks are paraphrase orientated such as text simplication and
short text

tion
text simplication is to make the text easier to read and understand especially for poor
ers while short text summarization is to generate a brief sentence to describe the short texts posts on the social media
most recent approaches use sequence to sequence model for paraphrase eration prakash et al cao et al
it code is available at compresses the source text information into dense vectors with the neural encoder and the neural decoder generates the target text using the pressed vectors

although neural network models achieve cess in paraphrase generation there are still two major problems
one of the problem is that the isting sequence to sequence model tends to orize the words and the patterns in the training dataset instead of the meaning of the words
the main reason is that the word generator the output layer of the decoder does not model the semantic information
the word generator which consists of a linear transformation and a softmax operation converts the recurrent neural network rnn output from a small dimension to a much larger dimension words in the vocabulary where each dimension sents the score of each word
the latent tion of the word generator is that each word is
dependent and the score is irrelevant to each other

therefore the scores of a word and its synonyms may be of great difference which means the word generator learns the word itself rather than the lationship between words
the other problem is that the word generator has a huge number of parameters
suppose we have a sequence to sequence model with a den size of and a vocabulary size of

the word generator has up to million ters which is even larger than other parts of the encoder decoder model in total
the huge size of parameters will result in slow convergence
cause there are a lot of parameters to be learned

moreover under the distributed framework the more parameters a model has the more bandwidth and memory it consumes
to tackle both of the problems we propose a novel model called word embedding attention network wean
the word generator of wean is attention based instead of the simple linear max operation
in our attention based word erator the rnn output is a query the candidate words are the values and the corresponding word
in order to predict representations are the keys
the word the attention mechanism is used to
lect the value matching the query most by means of querying the keys
in this way our model erates the words according to the distributed word representations neural word embeddings in a retrieval style rather than the traditional ative style
our model is able to capture the mantic meaning of a word by referring to its bedding
besides the attention mechanism has a much smaller number of parameters compared with the linear transformation directly from the rnn output space to the vocabulary space
the reduction of the parameters can increase the vergence rate and speed up the training process

moreover the word embedding is updated from three sources the input of the encoder the input of the decoder and the query of the output layer
following previous work cao et al we evaluate our model on two paraphrase oriented tasks namely text simplication and short text abstractive summarization
experimental results show that our model outperforms the sequence sequence baseline by the bleu score of and on two english text simplication datasets and the score of on a chinese marization dataset
moreover our model achieves state of the art performances on all of the mark datasets
proposed model
we propose a novel model based on the decoder framework which generates the words by querying distributed word representations with the attention mechanism
in this section we rst present the overview of the model architecture

then we explain the details of the word ation especially the way to query word dings
overview
word embedding attention network is based on the encoder decoder framework which consists of two components a source text encoder and a get text decoder
figure is an illustration of our model
given the source texts the encoder presses the source texts into dense representation vectors and the decoder generates the paraphrased texts
to predict a word the decoder uses the den output to query the word embeddings
the word embeddings assess all the candidate words and return the word whose embedding matches the query most
the selected word is emitted as the predicted token and its embedding is then used as the input of the lstm at the next time step
after the back propagation the word embedding is dated from three sources the input of the encoder the input of the decoder and the query of the put layer
we show the details of our wean in the following subsection
encoder and decoder the goal of the source text encoder is to vide a series of dense representation of complex source texts for the decoder

in our model the source text encoder is a long short term memory network lstm which produces the dense resentation hn from the source text xn the goal of the target text decoder is to generate a series of paraphrased words from the dense resentation of source texts
fisrt the lstm of the decoder compute the dense representation of erated words st
then the dense representations are fed into an attention layer bahdanau et al to generate the context vector ct which tures context information of source texts
tion vector ct is calculated by the weighted sum of encoder hidden states ct tihi n x ti hi
n hj
p where hi is an attentive score between the decoder hidden state st and the encoder hidden
state hi

in this way ct and st respectively represent the context information of source texts and the target texts at the tth time step
word generation by querying word embedding for the current sequence to sequence model the word generator computes the distribution of output words yt in a generative style sof st hard admission very hard key

value easy happy hard
competitive query
admission is extremely
competitive figure an overview of word embedding attention network
where w is a trainable parameter matrix k is hidden size and v is the number of words in the vocabulary
when the vocabulary is large the number of parameters will be huge
our model generates the words in a retrieval style rather than the traditional generative style by querying the word embeddings
we denote the combination of the source context vector ct and the target context vector st as the query qt in implementation we select the general attention function as the relevance score function based on the performance on the validation sets
the value pair with the highest score wt et is lected
at the test stage the decoder generates the key wt as the tth predicted word and inputs the value et to the lstm unit at the t time step

at the training stage the scores are normalized as the word probability distribution qt

sof ei

the candidate words wi and their corresponding embeddings ei are paired as the key value pairs wi

n where n is the number of candidate words
we give the details of how to termine the set of candidate words in section

our model uses qt to query the key value pairs wi by evaluating the vance between the query qt and each word tor ei with a score function ei
the query process can be regarded as the attentive selection of the word embeddings
we borrow the attention energy functions luong et al as the vance score function ei ei t ei qt t waei vt weei dot general concat where wq and we are two trainable parameter matrices and vt is a trainable parameter vector
selection of candidate key value pairs
as described in section the model generates the words in a retrieval style which selects a word according to its embedding from a set of candidate key value pairs
we now give the details of how to obtain the set of candidate key value pairs
we extract the vocabulary from the source text in the training set and select the n most frequent words as the candidate words
we reuse the embeddings of the decoder inputs as the values of the date words which means that the decoder input and the predicted output share the same
lary and word embeddings
besides we do not use any pretrained word embeddings in our model so that all of the parameters are learned from scratch
training
although our generator is a retrieval style wean is as differentiable as the sequence to sequence model
the objective of training is to minimize the cross entropy between the predicted word bility distribution and the golden one hot tion l yi log x
i
we use adam optimization method to train the model with the default hyper parameters the learning rate and
experiments
following the previous work cao et al we test our model on the following two paraphrase orientated tasks text simplication and short text abstractive summarization
text simplication datasets
the datasets are both from the alignments tween english wikipedia and simple glish wikipedia the simple english wikipedia is built for the children and adults who are learning the english language and the cles are composed with easy words and short tences
therefore simple english wikipedia is a natural public simplied text corpus
parallel wikipedia simplication corpus
pwkp
pwkp zhu et al is a widely used benchmark for evaluating text simplication systems
it consists of aligned complex text from english wikipedia as of
and simple text from simple wikipedia as of
the dataset contains tence pairs with words on average per complex sentence and words per
following the previous simple sentence
work zhang and lapata we remove the duplicate sentence pairs and split the pus with pairs for training pairs for validation and pairs for test
english wikipedia and simple english wikipedia
ew sew
ew sew is a licly available dataset provided by hwang et
al

to build the corpus they rst align the complex simple sentence pairs score the semantic similarity between the complex tence and the simple sentence and classify
each sentence pair as a good good partial partial or bad match
following the previous work nisioi et al we discard the classied matches and use the good matches and partial matches with a scaled threshold greater than
the corpus contains about k good matches and k good partial matches
we use this corpus as the ing set and the dataset provided by xu et al
xu et al as the validation set and the test set
the validation set consists of sentence pairs and the test set contains sentence pairs
besides each complex sentence is paired with reference simplied sentences provided by amazon mechanical turk workers
evaluation metrics
following the previous work nisioi et al hu et al we evaluate our model with ferent metrics on two tasks
automatic evaluation
we use the bleu score papineni et al as the automatic evaluation metric
bleu is a widely used metric for machine translation and text plication which measures the agreement between the model outputs and the gold erences
the references can be either single or multiple

in our experiments the ences are single on pwkp and multiple on ew sew
human evaluation
human evaluation is sential to evaluate the quality of the model outputs
following nisioi et al
and zhang et al
we ask the human raters to rate the simplied text in three dimensions fluency adequacy and simplicity
fluency assesses whether the outputs are
cally right and well formed
adequacy resents the meaning preservation of the plied text
both the scores of uency and adequacy range from to is very bad and is very good
simplicity shows how simpler the model outputs are than the source text which ranges from to
settings
our proposed model is based on the decoder framework
the encoder is implemented on lstm and the decoder is based on lstm with luong style attention
luong et al
we pwkp
pbmt wubben al
hybrid narayan and gardent
encdeca zhang and lapata
dress zhang and lapata
dress ls zhang and lapata
our implementation
wean our proposal bleu
table automatic evaluation of our model and other related systems on pwkp datasets
the results are ported on the test sets
ew sew pbmt r wubben al
hybrid narayan and gardent
sbmt sari xu et al
nts nisioi et al

nts nisioi et al
encdeca zhang and lapata
dress zhang and lapata
dress ls zhang and lapata
our implementation
wean our proposal bleu
pwkp nts dress ls
wean
reference fluency adequacy simplicity all ew sew fluency adequacy simplicity all
pbmt r
sbmt sari
nts dress ls wean reference table human evaluation of our model and other lated systems on pwkp and ew sew datasets
the results are reported on the test sets
sentence simplication models

encdeca is a model based on the implemented by decoder with attention
zhang and lapata
table automatic evaluation of our model and other related systems on ew sew datasets
the results are reported on the test sets
pbmt r wubben al is a phrase based machine translation model which reranks the outputs
tune our hyper parameter on the development set

the model has two lstm layers
the hidden size of lstm is and the embedding size is

we use adam optimizer kingma and ba to learn the parameters and the batch size is set to be
we set the dropout rate srivastava et al to be
all of the gradients are clipped when the norm exceeds

baselines
we compare our model with several neural text simplication systems
is our implementation of the sequence to sequence model with attention mechanism which is the most popular ral model for text generation
nts and nts nisioi et al are two sequence to sequence model with tra mechanism like prediction ranking and nts uses a pretrain
dress and dress ls zhang and lapata are two deep reinforcement learning hybrid narayan and gardent is a brid approach which combines deep tics and mono lingual machine translation
sbmt sari
xu et al is a based machine translation model which is trained on ppdb dataset ganitkevitch et al and tuned with sari
results
we compare wean with state of the art els for text simplication
table and table summarize the results of the automatic tion
on pwkp dataset we compare wean with pbmt hybrid encdeca dress and ls
wean achieves a bleu score of performing all of the previous systems
on sew dataset we compare wean with pbmt r
hybrid sbmt sari and the neural models scribed above
we do not nd any public release code of pbmt r and sbmt sari
fortunately xu al
provides the predictions of r and sbmt sari on ew sew test set so that we can compare our model with these systems
lcsts r l
rnn al
et al

rnn cont al
rnn et al

et al

copynet
al al
rnn et al

et al


wean table rouge score on the lcsts test set
and r l denote and
rouge l respectively
the models with a sufx of w in the table are word based while the rest of els are character based

it shows that the neural models have better formance in bleu and wean achieves the best bleu score with
we perform the human evaluation of wean and other related systems and the results are shown in table
dress ls is based on the forcement learning and it encourages the uency simplicity and relevance of the outputs

fore it achieves a high score in our human uation
wean gains a even better score than dress ls
besides wean generates more equate and simpler outputs than the reference on pwkp
the predictions of sbmt sari are the most adequate among the compared systems on ew sew
in general wean outperforms all of the other systems considering the balance of ency adequate and simplicity
we conduct nicance tests based on t test
the signicance tests suggest that wean has a very signicant improvement over baseline with p over dress ls in all of the dimension on pwkp p over dress ls in the dimension of ency p over nts in the dimension of simplicity and p over dress ls in
the dimension of all large scale text summarization dataset large scale chinese social media short text summarization dataset lcsts lcsts is constructed by hu et al

the dataset sists of more than text summary pairs constructed from a famous chinese social media website called sina it is split into three parts with pairs in part i pairs in part ii and pairs in part iii
all the text summary pairs in part ii and part iii are manually annotated with relevant scores ranged from to
we only reserve pairs with scores no less than leaving pairs in part ii and pairs in part iii
following the previous work hu et al we use part i as training set part ii as validation set and part iii as test set
is metric evaluation metrics
our rouge evaluation score lin and hovy which is lar for summarization evaluation
the metrics compare an automatically produced summary against the reference summaries by computing overlapping lexical units including unigram gram trigram and longest common subsequence lcs
following previous work rush al hu et al we use igram bi gram and rouge l lcs as the evaluation metrics in the reported experimental results
settings
the vocabularies are extracted from the training sets and the source contents and the summaries share the same vocabularies
we tune the parameters based on the rouge scores on the
in order to alleviate the risk of validation sets
word segmentation mistakes we split the nese sentences into characters
we prune the cabulary size to which covers most of the common characters
we set the word ding size and the hidden size to the
ber of lstm layers of the encoder is and the number of lstm layers of the decoder is
the batch size is and we do not use dropout srivastava et al on this dataset

following the previous work li et al we implement a beam search optimization and set the beam size to

baselines
we compare our model with the state of the art baselines
rnn and rnn cont are two sequence sequence baseline with gru encoder and coder provided by hu et al

param pwkp ewsew lcsts
m m m m m m wean table the number of the parameters in the put layer
the numbers of rest parameters between and wean are the same
rnn dist chen et al is a based neural model which the attention mechanism focuses on the different parts of the source content

copynet
gu et al incorporates a copy mechanism to allow part of the ated summary is copied from the source tent
srb ma al is a sequence sequence based neural model with improving the semantic relevance between the input text and the output summary
drgd
li et al is a deep recurrent generative decoder model combining the coder with a variational autoencoder
is our the sequence to sequence model with the tion mechanism
implementation of results
we report the rouge score of our model and the baseline models on the test sets
ble summarizes the comparison between our model and the baselines
our model achieves the score of and rouge l outperforming all of the ous models
first we compare our model with
it shows that the sequence to sequence model
our model signicant outperforms the to sequence baseline with a large margin of and
then we compare our model with other related models
is drgd li et al which obtains the score of and
rouge our model has a relative gain of and rouge l over
the state of the art models
the state of the art model training curve u e l b
wean
epoch figure the training curve of wean and on the pwkp validation set
analysis and discussion
reducing parameters our wean reduces a large number of the eters in the output layer
to analyze the ter reduction we compare our wean model with the sequence to sequence model
table lists the number of the parameters in the output layers of two models
both pwkp and ewsewhave the vocabulary size of words and the hidden size of resulting parameters
lcsts has a vocabulary size of and the hidden size of so the has parameters in the put layers
wean only has two parameter trices and one parameter vector at most in tion without regard to the vocabulary size

it has parameters on pwkp and ewsew and parameters on lcsts
besides wean does not have any extra parameters in the other part of the model
speeding up convergence figure shows the training curve of wean and on the pwkp validation set
wean achieve near the optimal score in only epochs while takes more than epochs to achieve the optimal score
therefore wean has much faster convergence rate compared with
with the much faster training speed wean does not suffer loss in bleu and even improve the bleu score
yoghurt oryogurt isadairyproduct produced bybacterial fermentation ofmilk

yoghurt oryogurt isadairyproduct madebybacterial fermentation ofmilk

oryoghurtisadairy product producedbybacterial fermentation ofmilk

itismadebybacterial fermentation ofmilk

yoghurt oryogurt isadairyproduct producedbybacterial fermentation of

source reference nts nts pbmt r sbmt sari yogurtoryogurt isadairy product drawnupbybacterial fermentation ofmilk

wean
source yoghurt oryogurt isadairyproduct madebybacterial fermentation ofmilk

depending on the context another closely related meaning of constituent is that of a citizen residing in the area governed represented or otherwise served by a politician sometimes thisisrestricted tocitizens whoelected thepolitician

thewordconstituentcanalsobeusedtorefertoacitizenwholivesintheareathat is governed represented or otherwise served by a politician sometimes the word is restricted tocitizens whoelected thepolitician

depending on the context another closely related meaning of constituent is that of a citizen living in the area governed represented or otherwise served by a politician sometimes thisisrestricted tocitizens whoelected thepolitician

thisisrestricted tocitizens whoelected thepolitician

depending on the context and meaning of closely related siemens martin is a citizen living in the area or otherwise was governed by a shurba this is restricted topeople whoelected it
reference nts nts pbmt r
sbmt sari
in terms of the context another closely related sense of the component is that of a citizen living in the area covered makeup or ifnot served byapolicy sometimes whoelected thepolicy

depending on the context another closely related meaning of constituent is that of a citizenwholivesintheareagoverned represented orotherwiseservedbyapolitician sometimes thewordisrestricted tocitizens whoelected thepolitician
wean
table two examples of different text simplication system outputs in ew sew dataset
differences from the source texts are shown in bold
case study and very close to the original meaning
table shows two examples of different text plication system outputs on ew sew
for the rst example nts nts and pbmt r miss some essential constituents so that the sentences are incomplete and not uent
sbmt sari erates a uent sentence but the output does not preserve the original meaning
the predicted tence of wean is uent simple and the same as the reference
for the second example omits so many words that
it lacks a lot of information
pbmt r generates some vant words like siemens martin and shurba which hurts the uency and adequacy of the generated sentence
sbmt sari is able to generate a uent sentence but the meaning is ferent from the source text and even more cult to understand
compared with the statistic model wean generates a more uent sentence

besides wean can capture the semantic ing of the word by querying the word embeddings so the generated sentence is semantically correct related work our work is related to the encoder decoder framework cho et al and the attention mechanism bahdanau et al

decoder framework like sequence to sequence model has achieved success in machine
lation sutskever et al jean et al luong et al lin et al text marization
rush al chopra et al nallapati et al wang et al ma and sun and other natural language processing tasks liu et al
there are many other methods to improve neural attention model jean et al luong et al
zhu et al
constructs a wikipedia dataset and proposes a tree based simplication model
woodsend and lapata introduces a data driven model based on quasi synchronous grammar which captures structural mismatches and complex rewrite operations
wubben et al
presents a method for text simplication using phrase based machine translation with ranking the outputs
kauchak proposes a text simplication corpus and evaluates language modeling for text simplication on the proposed corpus
narayan and gardent propose a hybrid approach to sentence simplication which combines deep semantics and monolingual chine translation
hwang et al
introduces a parallel simplication corpus by evaluating the similarity between the source text and the
ed text based on wordnet
glavas and stajner propose an unsupervised approach to ical simplication that makes use of word tors and require only regular corpora
xu et al
design automatic metrics for text plication
recently most works focus on the neural sequence to sequence model
nisioi et al

present a sequence to sequence model and re ranks the predictions with bleu and sari

zhang and lapata propose a deep forcement learning model to improve the
ity uency and adequacy of the simplied texts

cao et al
introduce a novel sequence sequence model to join copying and restricted eration for text simplication

rush al
rst used an attention based encoder to compress texts and a neural network language decoder to generate summaries

ing this work recurrent encoder was introduced to text summarization and gained better mance lopyrev
chopra et al


wards chinese texts hu et al
built a large corpus of chinese short text summarization
to deal with unknown word problem nallapati et al

proposed a generator pointer model so that the decoder is able to generate words in source texts
gu et al
also solved this issue by incorporating copying mechanism
conclusion
we propose a novel model based on the decoder framework which generates the words by querying distributed word representations
imental results show that our model outperforms the sequence to sequence baseline by the bleu score of and on two english text cation datasets and the score of on a chinese summarization dataset
moreover our model achieves state of the art performances on these three benchmark datasets
acknowledgements
this work was supported in part by national ral science foundation of china no national high technology research and opment program of china program no and the national thousand
young talents program
xu sun is the sponding author of this paper

references dzmitry bahdanau kyunghyun cho and yoshua neural machine translation by
corr bengio
jointly learning to align and translate

ziqiang cao chuwei luo wenjie li and sujian li


joint copying and restricted generation for paraphrase
in proceedings of the thirty first aaai conference on articial intelligence
pages
qian chen xiaodan zhu zhenhua ling si wei and hui jiang

distraction based neural networks for modeling documents
in proceedings of the international joint conference on articial gence ijcai
aaai new york ny
jianpeng cheng and mirella lapata

neural summarization by extracting sentences and words

in proceedings of the annual meeting of the sociation for computational linguistics acl august berlin germany volume long papers
kyunghyun cho bart van merrienboer c aglar
gulcehre dzmitry bahdanau fethi bougares ger schwenk and yoshua bengio

learning phrase representations using rnn encoder decoder
in proceedings for statistical machine translation
of the conference on empirical methods in natural language processing emnlp
pages
sumit chopra michael auli and alexander rush


abstractive sentence summarization with tentive recurrent neural networks
in naacl hlt the conference of the north american chapter of the association for computational guistics human language technologies
pages
juri ganitkevitch benjamin van durme and
chris callison burch

the paraphrase database
in human language technologies ference of the north american chapter of the ciation of computational linguistics proceedings
pages

ppdb goran glavas and sanja stajner

simplifying lexical simplication do we need simplied pora
in proceedings of the annual meeting of the association for computational linguistics acl
pages
jiatao gu zhengdong lu hang li and
victor
incorporating copying mechanism in li


in proceedings of sequence to sequence learning
the annual meeting of the association for putational linguistics acl
baotian hu qingcai chen and fangze zhu

sts
a large scale chinese short text summarization dataset
in proceedings of the conference on empirical methods in natural language ing emnlp lisbon portugal september
pages
william hwang hannaneh hajishirzi mari ostendorf and wei wu

aligning sentences from dard wikipedia to simple wikipedia
in naacl hlt
pages
sebastien jean kyunghyun cho roland memisevic and yoshua bengio
on using very large get vocabulary for neural machine translation

in proceedings of the annual meeting of the sociation for computational linguistics acl
pages
thang luong hieu pham and christopher
ning

effective approaches to attention based
in proceedings of the neural machine translation

conference on empirical methods in natural language processing emnlp
pages
shuming ma and xu sun

a semantic vance based neural network for text summarization and text simplication
corr
shuming ma xu sun jingjing xu houfeng wang wenjie li and qi su

improving semantic relevance for sequence to sequence learning of nese social media text summarization
in ings of the annual meeting of the association for computational linguistics acl ver canada july august volume short papers pages
ramesh nallapati bowen zhou ccero nogueira dos santos c aglar gulcehre and bing xiang


abstractive text summarization using sequence
in proceedings of the sequence rnns and beyond

signll conference on computational natural language learning conll berlin germany august
pages
david kauchak


improving text simplication language modeling using unsimplied text data
in proceedings of the annual meeting of the ciation for computational linguistics acl
pages
shashi narayan and claire gardent

hybrid plication using deep semantics and machine
in proceedings of the annual lation
ing of the association for computational tics acl
pages
diederik kingma and jimmy ba

adam
corr a method for stochastic optimization


piji li wai lam lidong bing and zihao wang


deep recurrent generative decoder for stractive text summarization
in proceedings of the conference on empirical methods in natural language processing emnlp copenhagen denmark september
pages

chin yew lin and eduard hovy

matic evaluation of summaries using n gram occurrence statistics
in human language ogy conference of the north american chapter of the association for computational linguistics naacl
junyang lin shuming ma qi su and xu sun


decoding history based adaptive control of attention for neural machine translation
corr
tianyu liu kexiang wang lei sha baobao chang and zhifang sui
table to text tion by structure aware learning
corr
sergiu nisioi sanja stajner simone paolo ponzetto and liviu dinu
exploring neural text plication models
in proceedings of the nual meeting of the association for computational
linguistics acl
pages

kishore papineni salim roukos todd ward and jing zhu

bleu a method for automatic uation of machine translation
in proceedings of the annual meeting of the association for tational linguistics pages
aaditya prakash sadid hasan kathy lee vivek
datla ashequl qadir joey liu and oladimeji farri


neural paraphrase generation with stacked
in coling residual lstm networks

international conference on computational guistics proceedings of the conference cal papers december osaka japan
pages
alexander rush sumit chopra and jason weston


a neural attention model for abstractive
in proceedings of the tence summarization

conference on empirical methods in natural guage processing emnlp lisbon portugal september
pages
konstantin lopyrev

generating news
corr lines with recurrent neural networks


nitish srivastava geoffrey hinton alex krizhevsky ilya sutskever and
ruslan nov
dropout a simple way to prevent neural wei xu courtney napoles ellie pavlick quanze chen and chris callison burch

optimizing statistical machine translation for text simplication
tacl

xingxing zhang and mirella lapata
tence simplication with deep reinforcement

ing
in proceedings of the conference on pirical methods in natural language processing emnlp copenhagen denmark september
pages
zhemin zhu delphine bernhard and iryna gurevych


a monolingual tree based translation model for sentence simplication
in coling
pages
networks from overtting

learning research
journal of machine

xu sun xuancheng ren shuming ma and houfeng wang

meprop sparsied back tion for accelerated deep learning with reduced
in proceedings of the international tting

conference on machine learning icml ney nsw australia august
pages
xu sun xuancheng ren shuming ma bingzhen wei wei li and houfeng wang

training

cation and model simplication for deep learning
a minimal effort back propagation method
corr
xu sun bingzhen wei xuancheng ren and shuming ma

label embedding network learning bel representation for soft training of deep networks

corr
ilya sutskever oriol vinyals and quoc le

sequence to sequence learning with neural works
in advances in neural information ing systems annual conference on neural mation processing systems
pages
sho takase jun suzuki naoaki okazaki tsutomu rao and masaaki nagata

neural headline generation on abstract meaning representation

in proceedings of the conference on empirical methods in natural language processing emnlp austin texas usa november
pages
kexiang wang tianyu liu zhifang sui and baobao chang

afnity preserving random walk for multi document summarization
in proceedings of the conference on empirical methods in
ural language processing emnlp hagen denmark september
pages
kristian woodsend and mirella lapata

ing to simplify sentences with quasi synchronous grammar and integer programming
in proceedings of the conference on empirical methods in natural language processing emnlp
pages
sander wubben antal van den bosch and emiel krahmer
sentence simplication by
in the lingual machine translation
nual meeting of the association for computational
linguistics proceedings of the conference
pages
jingjing xu xu sun xuancheng ren junyang lin binzhen wei and wei li

dp gan promoting generative adversarial network for
corr erating informative and diversied text


