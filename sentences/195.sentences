nlp driven ensemble based automatic subtitle generation and semantic video summarization technique aswin vb mohammed javed parag parihar aswanth k druval cr anpam dagar aravinda cv indian institute of information technology allahabad prayagraj uttar pradesh abstract
this paper proposes an automatic subtitle generation and semantic video summarization technique
the importance of automatic video tion is vast in the present era of big data
video summarization helps in efficient storage and also quick surfing of large collection of videos without losing the important ones
the summarization of the videos is done with the help of tles which is obtained using several text summarization algorithms
the posed technique generates the subtitle for videos with without subtitles using speech recognition and then applies nlp based text summarization algorithms on the subtitles
the performance of subtitle generation and video tion is boosted through ensemble method with two approaches such as tersection method and weight based learning method experimental results ported show the satisfactory performance of the proposed method
introduction a concept like video summarization has a huge scope in the modern era
video itory websites like google dailymotion vimeo
are gaining popularity day by day
the popularity of these websites is enormous in the present scenario
a large amount of videos are being uploaded as well as downloaded from these online video repository websites
for example the total number of people who use youtube are
hours of video are uploaded to youtube every minute almost billion videos are watched on youtube every single day
youtube gets over million visitors per
in this scenario for a concept like video summarization has a huge scope
the video summarization technique can be applied on the video thumbnail to attract more viewers
it can be developed to show only interesting and important parts of the video
it is not necessary for all the videos to come with a tle
it is very difficult to summarize the videos like security footage s as they do nt have subtitles even after applying speech recognition
this reduces the domain of video summarization
but still summarization of video using subtitles is most efficient and fastest way of doing it
if machine learning algorithms or histogram based were used to summarize videos it would have taken a long time to train them which will increase the time of development
but dealing with subtitle which is ously text is much more easy to deal with and faster which makes the video zation easier and faster
but the main problem here is that most of the videos comes without subtitles
in such cases to rectify this problem the technique of speech recognition which can be applied on the audio of the video and generate subtitles by formatting the text obtained after speech recognition
to extract different sentences from the video there is a need to detect silence in the audio as it recognizes the end of the sentence from that
once the subtitle is obtained the video can be summarized with the generated sub titles
for summarization of subtitles natural language processing nlp can be used which can be of various accuracy s
therefore overall this paper proposes nlp based subtitle generation and video marization technique
rest of the paper is organized as follows section explains the proposed model section explains experimental results and section presents the summary of the report
proposed model for summarizing a video using subtitles the proposed method uses text rization algorithms which are nlp based methods
further there is an ensemble technique using the text summarization algorithms
the text summarization algorithms were used for filtering out key contents from subtitle file
srt which will be taken as an input implement the algorithm on the input put the sentences in array and rank them according to their importance using different domains and will pick out the best sentences out of it so as to form a concise subtitle keeping in mind where those subtitles were originally placed according to subtitle i d
next is ble technique which combines the different algorithms together for a more perfect intersection of all algorithm abstract of the input
also we are training each rithm in ensemble technique to give precise models
the flowchart in fig describes the flow of video summarization using subtitles that has been implemented
the input video is fed in along with the subtitles if the subtitle is not present subtitle is ated using the subtitle generation algorithm and this subtitle along with the video is used to summarize depending upon whether single algorithm or the combined rithm is to be used and the summarized video is the output
figure
flow control of video summarization
nlp based subtitle generation since all videos may not have subtitles along with them and this method can be applied on videos which have subtitles
in case the user does not have a subtitle file then the subtitles is generated first and then process the video using methods listed below
the subtitles if not provided are generated using speech recognition api of wit
ai which is used by facebook for speech recognition
basic idea on how the subtitle is generated is that chunks of audio is extracted from the video and apply speech recognition on them
to elaborate this first the audio is extracted from the video file
then max time interval is fixed for each subtitle for the video let it be sec which is in our case
then the audio is scanned to detect silence in it if the silence occurs before seconds the audio will be cut at that point
also a threshold is defined such that above this level the part of the audio is not treated as silence and vice versa
also to be noted that second of extra silence is added at the starting and at the ending of the audio
so that words to be recognized by the speech recognizer is not missed out
each time the words are recognized in a particular chunk of the whole sentence is formatted in the form of a subtitle file such that each of the tence will be mentioned with the starting time stamp and the ending time stamp
once the subtitle is available it can be summarized to obtain the summarized video
there are text summarization algorithms which have been used to summarize a video
these are

luhn came around as an emerging year in the field of summarization when hans peter luhn suggested an for thought summarization
this was a carrying a lot of weight achievement and a big head start in this sector and followed his consider was a action in summarization area
luhn received a rule of thumb to recognize salient sentences from the text by features well known as definition and definition frequency
this rithm checks for words that have high occurrence frequency and the words are sorted based on decreasing frequency
the weight of a sentence is culated by summing weights of each relevant word and these sentences are sorted in decreasing order based on the summed weight and finally p most relevant sentences are taken from the subtitles as the output
latent semantic analysis also known as lsa is based on words and concepts
if each word represent only a single concept lsa would have been lot easier but un fortunately each word in english language represent several concepts like synonyms
because of this varying concepts within each word lsa becomes a little tricky as each word map to more than one concept
the basic concept of lsa is based on title words and index words
title words are those words which appear in all the sentences in lsa and index words are those words which appear in more that tences which has the title word
a document is considered as a bag of

lsa words
the order in which the words are arranged is not important whereas the count of a particular word plays an important role
also each word is suppose to have only one meaning
lsa is based on concepts which are represented as a pattern of words
to understand the concept behind the word clustering is used in lsa
the basic idea is to plot a xy graph cluding all the index words and title words based on their occurrences in each sentences
then all the clusters in the graph are identified
each of these clusters represent each concept and title in that sentence represent that particular concept
hence concept for each title word can be extracted
so in summarization the technique was used in such a way that the tence which is included in the most crowded cluster of the graph was taken
using this concept of title words and index words any text document can be summarized


text rank in the text rank algorithm it first takes the text which has to be split up and converted into sentences and further into vectors
a similarity trix is constructed from the vectors and a graph is created from this matrix
using this graph the sentence ranking is done
based on the ranked tences summarized text is obtained
the probability of going from tence a to sentence b is the similarity of sentences
the text units which best defines the sentence are identified
these text units are then added to the graph as vertices
the relations which connect texts are identified which are used to draw edges of the graph between vertices
a graph based ranking algorithm is used until it converges
during the ranking rithm each vertex is assigned a value this value is used for lection sions
finally the vertices are sorted on the basis of their final score value and then the sentences are sorted based on the sorted vertices in the graph


lex rank in the lex rank algorithm first all the nouns and adjectives is rated out to form a document cluster
now the idf inverse document frequency scores is found for each words in this cluster
for each word let tf be the frequency of that word in the cluster
the collection of all words which have score greater than a threshold value forms the centroid of the cluster
so importance of a sentence will be higher if it tains more number of words which are present in the centroid
so using this concept p most relevant sentences are selected
the previous works of luhn showed that the key words can be extracted out as the most frequently occurring content words except the stop words and summarize the text according containing the most key words
but since this much is not alone capable to generate proper summarized text

edmundson edmundson proposed another algorithm and added three more ods to extract out key words which are namely pragmatic words cue and heading words and structural indicators sentence tion
the results of edmundson cleared out the fact that the other three factors played a dominant role in generating the best summarized text as expected
since it uses extra words call stigma and bonus word the marization will be biased according to those files
so this method will not be used in the proposed method

video summarization from the algorithms explained in the previous section edmundson summarization will not be used because it is biased according to the bonus and stigma words given by the user
so it can not be used for a comparison
figure
output of summarization using algorithms
fig is the summarization of the movie named mrs
doubtfire contains a frame of output using algorithms lex rank luhn lsa and text rank
a video of shah rukh khan s ted talk which was of minutes was taken and summarized
the subtitle file of the input video had lines
the video was fed into the above tioned algorithms separately the graph of subtitle text with it being relevant in the summarized video for the algorithms are described in fig fig fig fig
figure
luhn algorithm
figure
lex rank algorithm figure
lsa algorithm




weight based learning algorithms figure
text rank algorithm taking intersection and combining a very basic and simple approach is to combine them directly meaning the idea is to run all the algorithms keep their outputs side by side and take a simple mathematical intersection
as it is very obvious if a tence is in all the combining algorithms then its must be of great portance hence it should be included in the output file
hence the user can choose from luhn lsa lexrank textrank which all he wants to bine multiple correct so that he can get the best results when all of them combined
also edmundson can not be concatenated in this process because it requires two extra files of bonus words and stigma words hence taking that while combining them was not ideally possible
this tersection gave very good results and implemented the ideology of ble properly
the problem with the previous method was that it gave equal powers to all the algorithms but from the above explanations it can be seen that not all algorithms behave properly so here a method was devised where each could get some weightage
the idea was simple the one that performs the best gets more
as the name suggest an initial weights were taken for all algorithm and initialized all the algorithms with the same weights wl wlsa we wlr wtr
now the check function will compare output of each of the algorithms and rank them accordingly on basis of their formance so the ones that performed may get an increment in their weights
so during future summarizations they will get their scores cording to their weights and the sentences with higher score will be part of the output file
in this way a clear view can be obtained of which rithm can give best output for the given input and the suggested algorithm can be used to obtain better results
the figure is the weight allocation for different algorithms before and after summarization of shah rukh khan s video
from the figure it can be understood that the weight of lsa was creased and that of lex was decreased
so for this video lsa performed better and lex performedthe least
so the weight of lex rank is increased by a unit and that of luhn is decreased
figure
weights before and after summarization of first video
experimental results since there is no dataset for video summarization using subtitles and the zation technique a of videos was generated which were having different time length
these videos were given to these algorithms and also the combined algorithm and the number of lines obtained in the summarized video was noted
the efficiency of each algorithm was decided based on the output of combined video which was the intersection of all these algorithms
so the efficiency can be defined for an algorithm as the ratio of the number of subtitles in the combined video to that of the output of the particular algorithm
efficiency ncombined nalgorithm
efficiency of video summarization by efficiency of an algorithm in summarization it is meant how much part of the summarized algorithm is present in the video generated by the ensemble technique
as mentioned above efficiency of each algorithm can be calculated using formula and on applying this on the intersection method following results were obtained as shown in
table
efficiency of intersection method
lex rank
lsa
luhn
text
on applying the dataset on the weighted ensemble technique with initial weights set to for all algorithms the following results were obtained as shown in table and the updated weights are shown in table
table
efficiency of weighted ensemble method
lex rank
lsa
luhn
text
table
weights ensemble method
initial finial lsa
luhn text lex rank
from the tables and it is observed that lsa performed better and lex rank formed the least
since for all videos lsa performed best and lex performed least their weights got affected whereas the weight of luhn and text rank remained changed table
there is a huge difference in the efficiency value in weight based and intersection method because in weight based ensemble technique at each iteration the weight of the better algorithm is increased and that of the worst algorithm is creased
from these two methods it is clear that lsa has a major contribution to the summarized video and lex has the least contribution

complexity time complexity
single summarization algorithm where n is the number of iterations until the summarization length is obtained and k is the number of sentences in the summarized subtitles

combined summarization algorithm where is the number of methods to be combined n is the number of tions until the summarization length is obtained k is the number of sentences in the summarized subtitles
space complexity
single summarization algorithm where r is the total number of regions in the subtitle array l is the average length of the sentences in the summarized subtitle

combined summarization algorithm where r is the total number of regions in the subtitle array l is the average length of the sentences in the summarized subtitle
conclusion large number of videos are being generated and are increasing day by day
hence video summarization technique will be very helpful
video summarization provided a faster way browsing of large video collections and more efficient content indexing and access
the use of nlp algorithms proved to be a very efficient way to form abstracts of videos
the case of no subtitles was by using subtitle generation method to convert speech to text which turned out to be of great use in normal day to day usage
many of the videos which is taken from phones
do not contain subtitles hence there is future scope to work on this problem
references
mind blowing youtube facts figures and statistics

available
com youtube
rachida hannane abdessamad elboushaki karim afdel p
naghabhushan mohammed javed an efficient method for video shot boundary detection and keyframe extraction ing sift point distribution histogram

s
m
s
k
l
g
g
j
liadh kelly johannes leveling report on summarization
n
r
pratibha devihosur automatic text summarization using natural language niques
cessing

s
liu long text summarization using neural networks and rule based approach

t
g
dietterich ensemble methods in machine learning

m
dahale text summarization for compressed inverted indexes and snippets

definition of subtitle

available
webster
com dictionary subtitle
j
constine speech recognition using wit
ai

available
facebook wit
n
nazari and m
a
mahdavi a survey on automatic text summarization journal of ai
t
k
landauer p
w
foltz and d
laham an introduction to latent semantic and data mining vol
pp

processes vol


r
mihalcea and p
tarau textrank bringing order into text

g
erkan and d
r
radev lexrank graph based lexical centrality as salience in text
artif
int
res
vol


h
p
edmundson new methods in automatic
acm vol


