l u j l c
s c v
v i x r a discovering topics in text datasets by visualizing relevant words franziska leila grgoire klaus robert and wojciech learning group technische universitt berlin berlin germany learning group fraunhofer heinrich hertz institute berlin germany of brain and cognitive engineering korea university seoul korea franziska

tu berlin
abstract
related work when dealing with large collections of uments it is imperative to quickly get an overview of the texts contents
in this per we show how this can be achieved by using a clustering algorithm to identify ics in the dataset and then selecting and visualizing relevant words which guish a group of documents from the rest of the texts to summarize the contents of the documents belonging to each topic
we demonstrate our approach by discovering trending topics in a collection of new york times article snippets
introduction large unstructured text datasets e

in the form of data dumps leaked to journalists are ing more and more frequent
to quickly get an overview of the contents of such datasets tools for exploratory analysis are essential
we propose a method for extracting from a set of texts the relevant words that distinguish these documents from others in the dataset
by using the dbscan clustering algorithm the documents in a dataset can be grouped to reveal salient topics
we can then summarize the texts belonging to each topic by visualizing the extracted relevant words in word clouds thereby enabling one to grasp the contents of the documents at a glance
by identifying relevant words in clusters of cent new york times article snippets we strate how our approach can reveal trending topics
all tools discussed in this paper as well as code to replicate the experiments are available as an open source python library

com textcatvis identifying relevant words in text documents was traditionally limited to the area of feature selection where different approaches were used to discard irrelevant features in an attempt to improve the classication performance by reducing noise as well as save computational resources
ever the primary objective here was not to identify words that best describe the documents belonging to certain clusters but to identify features that are particularly uninformative in a classication task and can be disregarded
other work was focused on selecting keywords for individual documents e

based on tf idf variants or by using ers
yet while these keywords might vide adequate summaries of single documents they do not necessarily overlap with keywords found for other documents about this topic and therefore it is difcult to aggregate them to get an overview of the contents of a group of texts
current tools available for creating word clouds as a means of ing a collection of mostly rely on term frequencies while ignoring stopwords sibly combined with part of speech tagging and named entity recognition to identify words of est
while an approach based on tf idf tures selects words occurring frequently in a group of documents these words do not reliably guish the documents from texts belonging to other clusters
in more recent work relevant features were selected using layerwise relevance tion lrp to trace a classier s decision back to the samples input features
this was fully used to understand the classication decisions made by a convolutional neural network trained on a text categorization task and to subsequently termine relevant features for individual classes by aggregating the lrp scores computed on the test samples
while in classication settings lrp works great to identify relevant words describing different classes of documents this method is not suited in our case as we are dealing with unlabeled data
methods to get a quick overview of a text dataset we want to identify and visualize the relevant words ring in the collection of texts
we dene relevant words as some characteristic features of the uments which distinguish them from other ments
as the rst step in this process the texts therefore have to be preprocessed and transformed into feature vectors section

while relevant words are supposed to occur often in the documents of interest they should also distinguish them from other documents
when analyzing a whole dataset it is therefore most revealing to look at individual clusters and obtain the relevant words for each ter i
e
nd the features that distinguish one cluster i
e
topic from another
to cluster the documents in a dataset we use the dbscan algorithm tion

the relevant words for a cluster c are identied by computing a relevancy score rc for every word ti with i


t where t is the number of unique terms in the given vocabulary and then the word clouds are created using the top ranking words
the easiest way to compute relevancy scores is to simply check for frequent features in a selection of documents
however this does not necessarily duce features that additionally occur infrequently in other clusters
therefore we instead compute a score for each word indicating in how many uments of one cluster it occurs compared to other clusters section


preprocessing feature extraction all n texts in a dataset are preprocessed by casing and removing non alphanumeric characters
then each text is transformed into a bag of words bow feature vector xk rt k


n by rst computing a normalized count the term frequency tf for each word in the text and then weighting this by the word s inverse document frequency idf to reduce the inuence of very frequent but pressive words that occur in almost all documents such as and and the
the idf of a term ti is calculated as the logarithm of the total number of documents divided by the number of documents which contain term ti i
e
idf ti log n ti
the entry corresponding to the term ti in the tf idf feature vector xk of a document k is then xki idf ti
in addition to single terms we are also ing meaningful combinations of two words i
e
grams as features
however to not inate the ture space too much since later relevancy scores have to be computed for every feature only tinctive bigrams are selected
this is achieved by computing a bigram score for every combination of two words occurring in the corpus similar as in and then selecting those with a score signicantly higher than that of random word combinations
ther details can be found in the appendix of

clustering to identify relevant words summarizing the ent topics in the dataset the texts rst have to be clustered
for this we use density based spatial clustering of applications with noise dbscan a clustering algorithm that identies clusters as areas of high density in the feature space separated by areas of low density
this algorithm was chosen as it does not assume that the clusters have a certain shape unlike e

the k means algorithm which assumes spherical clusters and it allows for noise in the dataset i
e
does not enforce that all samples belong to a certain cluster
dbscan is based on pairwise distances tween samples and rst identies core samples in areas of high density and then iteratively expands a cluster by joining them with other samples whose distance is below some user dened threshold
as the cosine similarity is a reliable measure of larity for text documents we compute the pairwise distances used in the dbscan algorithm by rst reducing the documents tf idf feature vectors to linear kernel pca components to remove noise and create more overlap between the feature tors and then compute the cosine similarity between these vectors and subtract it from to transform it into a distance measure
as clustering is an unsupervised process a value for the distance threshold has to be chosen such that the obtained clusters seem reasonable
in the experiments scribed below we found that a minimum cosine similarity of
to other samples in the cluster i
e
using a distance threshold of
leads to texts about the same topic being grouped together
we denote as yk the cluster that document k was assigned to in the clustering procedure

identifying relevant words relevant words for each cluster are identied by computing a relevancy score rc for every word ti and then selecting the highest scoring words
we compute a score for each word depending on the number documents it occurs in from one cluster compared to the documents from other clusters
we call the fraction of documents from a target cluster c that contain the word ti this word s true positive rate yk c yk
correspondingly we can compute a word s false positive rate as the mean plus the standard deviation of the tprs of this word for all other c c
the objective is to nd words that occur in many documents from the target cluster i
e
have a large but only occur in few documents of other clusteres i
e
have a low
one way to identify such words would be to compute the difference between both rates i
e
ti which is similar to traditional feature selection proaches
however while this score yields words that occur more often in the target cluster than in other clusters it does not take into account the relative differences
for example to be able to detect emerging topics in newspaper articles we are not necessarily interested in words that occur often in today s articles and infrequently in terday
instead we acknowledge that not most articles published today will be written about some new event only signicantly more articles pared to yesterday
therefore we propose instead a rate quotient which gives a score of to every word that has a tpr about three times higher than its fpr ti with
while the rate quotient extracts relevant words that would otherwise go unnoticed for a given fpr of
it assigns the same score to words with a tpr of
and a tpr of

therefore to create a proper ranking amongst all relevant words we take the mean of both scores to compute the nal score rc ti
ti ti which results in the tpr fpr relation shown in fig

figure relevancy score depending on a word s tpr and fpr for a cluster
experiments results to illustrate how the identied relevant words can help when exploring new datasets we test the ously described methods on recent article snippets from the new york times
the code to replicate the experiments is available online and includes functions to cluster documents extract relevant words and visualize them in word clouds as well as highlight relevant words in individual documents
to see if our approach can be used to discover trending topics we are using newspaper article snippets from the week of president trump s guration jan as well as three weeks prior including the last week of downloaded with the archive api from new york times
before we cluster the texts if we just ally split them into the articles published during the week of the inauguration and before the identied relevant words already reveals clear are not taking the maximum of the other clusters tprs for this word to avoid a large inuence of a cluster with maybe only a few samples
json
com textcatvis
nytimes
com
week of the inauguration were clustered using scan
when enforcing a minimum cosine larity of
to other samples of a cluster as well as at least three articles per cluster we obtain over clusters for this week as well as several ticles considered noise
while some clusters correspond to specic sections of the newspaper e

corrections to articles published the days fore others indeed refer to meaningful events that happened that week e

the nomination of betsy devos or an avalanche in italy fig

conclusion examining the relevant words that summarize ferent groups of documents in a dataset is a very helpful step in the exploratory analysis of a tion of texts
it allows to quickly grasp the contents of documents belonging to certain clusters and can help identify salient topics which is important if one is faced with a large dataset and quickly needs to nd documents of interest
we have explained how to compute a relevancy score for individual words depending on the ber of documents in the target cluster this word occurs in compared to other clusters
this method is very fast and robust with respect to small or ing numbers of samples per cluster
the usefulness of our approach was demonstrated by using the obtained word clouds to identify trending topics in recent new york times article snippets
we hope the provided code will encourage other people faced with large collections of texts to quickly dive into the analysis and to thoroughly explore new datasets
acknowledgments we would like to thank christoph hartmann for his helpful comments on an earlier version of this manuscript
franziska horn acknowledges funding from the elsa neumann scholarship from the tu berlin
references leila arras franziska horn grgoire montavon klaus robert mller and wojciech samek
plaining predictions of non linear classiers in nlp
in proceedings of the workshop on resentation learning for nlp pages
ation for computational linguistics
leila arras franziska horn grgoire tavon klaus robert mller and wojciech samek
figure relevant words in ny times article pets during the week of president trump s ration green up and three weeks prior red down
figure frequencies of selected words in ny times article snippets from different days
trends fig

obviously the main focus that week was on the inauguration itself however it ready becomes apparent that this will be followed by protest marches and also the australian open was happening at that time
when looking at the currence frequencies of different words over time fig
we can see the spike of trump at the day of his inauguration but while some stopwords occur equally frequent on all days other rather meaningless words such as tuesday have clear spikes as well on tuesdays
therefore care has to be taken when contrasting articles from different times when computing relevant words as it could easily happen that these meaningless words are picked up as well simply because e

one month contains more tuesdays than another month used for comparison
to identify trending topics the articles from the figure word clouds created from the relevant words identied for two of over clusters during the week jan and corresponding headlines
christopher d
manning prabhakar raghavan introduction to and hinrich schtze
mation retrieval
cambridge university press new york ny usa
isbn
carmel mcnaught and paul lam
using wordle as a supplementary research tool
the qualitative report
tomas mikolov ilya sutskever kai chen greg s corrado and jeff dean
distributed tions of words and phrases and their ity
in advances in neural information processing systems pages
grgoire montavon wojciech samek and robert mller
methods for interpreting and derstanding deep neural networks
arxiv preprint

bernhard schlkopf alexander smola and robert mller
nonlinear component analysis as a kernel eigenvalue problem
neural computation
yiming yang and jan o
pedersen
a comparative study on feature selection in text categorization
in proceedings of the fourteenth international ference on machine learning icml pages san francisco ca usa
morgan kaufmann publishers inc
isbn
kuo zhang hui xu jie tang and juanzi li
word extraction using support vector machine pages
springer berlin heidelberg berlin heidelberg
what is relevant in a text document an terpretable machine learning approach
arxiv preprint

sebastian bach alexander binder grgoire tavon frederick klauschen klaus robert mller and wojciech samek
on pixel wise explanations for non linear classier decisions by layer wise evance propagation
plos one
martin ester hans peter kriegel jrg sander aowei xu al
a density based algorithm for discovering clusters in large spatial databases with noise
in kdd volume pages
george forman
an extensive empirical study of feature selection metrics for text classication
the journal of machine learning research
florian heimerl steffen lohmann simon lange and thomas ertl
word cloud explorer text lytics based on word clouds
in system sciences hicss hawaii international ence on pages
ieee
franziska horn leila arras grgoire montavon klaus robert mller and wojciech samek
ploring text datasets by visualizing relevant words
arxiv preprint

anette hulth
improved automatic keyword traction given more linguistic knowledge
in ceedings of the conference on empirical methods in natural language processing pages
association for computational tics
sungjick lee and han joon kim
news keyword extraction for topic tracking
in networked puting and advanced information management

fourth international conference on volume pages
ieee
issues surrounding betsy devos the education nominee for profit law school is cut off from federal student loans betsy devos s education hearing erupts into partisan debate nominee betsy devos s knowledge of education basics is open to criticism donald trump s education nominee betsy devos avalanche death toll in italy reaches as search of hotel rubble continues avalanche in italy buries hotel leaving up to missing italy cheers as boy pulled from rubble of avalanche along with others search for survivors after avalanche in italy
