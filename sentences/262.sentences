on the impressive performance of randomly weighted encoders in summarization tasks jonathan jaehong christopher ai institute for learning algorithms polytechnique de montreal cifar ai chair
pilault jaehong

com e f l c
s c v
v i x r a abstract in this work we investigate the performance of untrained randomly initialized encoders in a general class of sequence to sequence els and compare their performance with that of fully trained encoders on the task of tive summarization
we hypothesize that dom projections of an input text have enough representational power to encode the chical structure of sentences and semantics of documents
using a trained decoder to duce abstractive text summaries we cally demonstrate that architectures with trained randomly initialized encoders perform competitively with respect to the equivalent chitectures with fully trained encoders
we further nd that the capacity of the encoder not only improves overall model tion but also closes the performance gap tween untrained randomly initialized and trained encoders
to our knowledge it is the rst time that general sequence to sequence models with attention are assessed for trained and randomly projected representations on stractive summarization
introduction recent state of the art natural language ing nlp models operate directly on raw put text thus sidestepping typical prepossessing steps in classical nlp that use hand crafted tures young et al

it is typically assumed that such engineered features are not needed since critical parts of language are modeled directly by encoded word and sentence representations in deep neural networks dnn
for instance searchers have attempted to evaluate the ability of recurrent neural networks rnn to represent cal structural or compositional semantics linzen et al
hupkes et al
lake and roni and study morphological learning in equal contribution order determined by coin ip machine translation belinkov et al
dalvi et al

various diagnostic methods have been proposed to analyze the linguistic properties that a xed length vector can hold ettinger et al
adi et al
kiela et al

nevertheless relatively little is still known about the exact properties that can be learned and encoded in sentence or document tations from training
while general linguistic structures has been shown to be important in nlp strubell and mccallum knowing whether this information comes from the architectural bias or the trained weights can be meaningful in signing better performing models
recently it was demonstrated that randomly parameterized combinations of pre trained word embeddings ten have comparable performance to fully trained sentence embeddings wieting and kiela
such experiments question the gains of trained modern sentence embeddings over random ods
by showing that random encoders perform close to state of the art sentence embeddings eting and kiela challenged the assumption that sentence embeddings are greatly improved from training an encoder
as a follow up to wieting and kiela we generalize their approaches to more complex sequence to sequence learning larly on abstractive text summarization
we tigate various aspects of random encoders using a hierarchical recurrent encoder decoder hred architecture that either has an untrained domly initialized encoders or a fully trained encoders
in this work we seek to answer three main questions how effective are untrained randomly initialized hierarchical rnns in ing document structure and semantics are untrained encoders close in performance to trained encoders on a challenging task such as long text summarization tasks how does the capacity of encoder or decoder affect the quality of ated summaries for both trained and untrained coders to answer such questions we analyse plexity and rouge scores of random hred and fully trained hreds of various hidden sizes
we go beyond the nlp classication tasks on which random embeddings were shown to be useful eting and kiela by testing its efcacy on a conditional language generation task
our main contribution is to present empirical evidence that using random projections to sent text hierarchy can achieve results on par with fully trained representations
even without ful pretrained word embeddings we show that dom hierarchical representations of an input text perform similarly to trained hierarchical tations
we also empirically demonstrate that for general models with attention the gap between random encoder and trained encoder comes smaller with increasing size of tions
we nally provide an evidence to validate that optimization and training of our networks was done properly
to the best of our knowledge it is the rst time that such analysis has been formed on a general class of with tion and for the challenging task of long text marization
related work
fixed random weights in neural networks a random neural network minsky and ridge can be dened as a neural network whose weights are initialized randomly or randomly and are not trained or optimized for a particular task
random neural networks have been studied since training and optimization cedures were often infeasible with the tional resources at the time
it was shown that for low dimensional problems feed forward neural networks ffnn with xed random weights can achieve comparable accuracy and smaller dard deviations compared to the same network trained with gradient backpropagation schmidt et al

inspired by this work extreme learning machines elm have been proposed huang g

and c


elm is a gle layer ffnn where only the output weights are learned through simple generalized inverse ations of the hidden layer output matrices
sequent theoretical studies have demonstrated that even with randomly generated hidden weights elm maintains the universal approximation bility of the equivalent fully trained ffnn huang et al

such works explored the effects of randomness in vision tasks with stationary els
in our work we explore randomness in nlp tasks with autoregressive models
similar ideas have been developed for rnns with echo state networks esn jaeger and more generally reservoir computing rc krylov and krylov
at rc s core the dynamics of an input sequence are modeled by a large reservoir with random untrained weights whose state is mapped to an output space by a trainable readout layer
esn leverage the vian architectural bias of rnns and are thus able to encode input history using recurrent random projections
esn has comparable generalization to elm and is generally known to be more robust for non linear time series prediction problems li in such research randomness was et al

used in autoregressive models but not within the context of encoder decoder architectures in nlp

random encoders in deep architectures fixed random weights have been studied to encode various types of input data
in computer vision it was shown that random kernels in convolutional neural networks cnn perform reasonably well on object recognition tasks jarrett et al

other works highlighted the importance of setting random weights in cnn and found that mance of a network could be explained mostly by the choice of a cnn architecture instead of its timized weights saxe et al

similarly random encoder architectures in ural language processing nlp were deeply vestigated in wieting and kiela in the context of sentence embeddings
in their ments pre trained word embeddings were passed through different randomly weighted and trained encoders a bag of random embedding jections a random long short term memory work hochreiter and schmidhuber lstm and an echo state network esn
the dom sentence representation was passed through a learnable decoder to solve senteval conneau and kiela downstream tasks sentiment analysis question type product reviews jectivity opinion polarity paraphrasing ment and semantic relatedness
interestingly the performance of random sentence figure the architecture of a random encoder summarization model
the weight parameters of sentence and document encoder lstms are randomly initialized and xed
other parameters for word embeddings word and sentence level attention and decoder lstms are learned during training
the blue parts of the architecture are encoder recurrent neural networks whose weights have been randomly initialized and that are not trained
the orange parts are decoder lstms whose weights are trained
c is the encoder context vector y is the ground truth target summary token and y is the predicted token
tions was close to other modern sentence dings such as infersent conneau et al
and skipthought kiros et al

the authors gued that the effectiveness of current sentence bedding methods seem to benet largely from the representational power of pre trained word beddings
while random encoding of the put text was deployed to solve nlp classication tasks random encoding for conditional text ation has still not been studied
approach we hypothesize that the markovian representation of word history and position within the text as provided by randomly parameterized encoders is rich enough to achieve comparable results to fully trained networks even for difcult nlp tasks
in this work we compare hierarchical recurrent encoder decoder hred models with randomly xed encoder with ones fully trained in a normal end to end manner on abstractive summarization tasks
in particular we examine the performances of random encoder models with varying coder and decoder capacity
to isolate root causes of performance gaps between trained coders and random untrained encoders we also provide an analysis of gradient ows and relative weight changes during training

model our hierarchical recurrent encoder decoder model is similar to that of nallapati et al

the model consists of two encoders sentence encoder and document encoder
the sentence encoder is a recurrent neural network which encodes a quence of input words in a sentence into a xed size sentence representation
specically we take the last hidden state of the recurrent neural work as the sentence encoding



xn where xi are embeddings of i th input token and n is the length of the corresponding sentence
the sentence encoder is shared for all sentences in the input document
the sequence of sentence codings are then passed to the document encoder which is another recurrent neural network



where denotes the encoding of j sentence and m is the number of sentences in the input ument
the decoder is another recurrent neural network that generates a target summary token by token
model encoder abstractive model nallapati et al
attn k vocab attn k vocab pointer generator network see et al
hred attn pointer ours hred attn pointer ours hred attn pointer ours hred attn pointer ours trained hierarchical gru







trained lstm trained lstm trained lstm trained h lstm random h lstm random lstm esn identity h lstm rouge







l







table results of the cnn daily mail test dataset
rouge scores have a condence interval of at most
as reported by the ofcial rouge script
to capture relevant context from the source ument the decoder leverages the same cal attention mechanism used in nallapati et al

concretely the decoder computes level attention weight by using the sentence coder states of input tokens
the decoder also tains sentence level attention weight using ument encoder hidden states
the nal attention weight integrates and sentence level tention to capture salient part of the input in both word and sentence levels
k where and l denote the word level attention weight on the and l th tokens of the input ument respectively m is the sentence level tion weight for the m th sentence of the input ument returns the index of the sentence at the l th word position and nd is the total number of tokens in the input document
a pointer generator architecture enables our coder to copy words directly from the source ument
the use of pointer generator allows the model to effectively deal with out of vocabulary tokens
additionally decoder coverage is used to prevent the summarization model from generating the same phrase multiple times
the detailed scription of pointer generator and decoder age can be found in cohan et al

experiment and analysis in our experiments we aim to demonstrate that random encoding can reach similar performances of trained encoding in a conditional natural guage generation task
to appreciate this bution we will rst describe the experimental and architectural setup before deep diving into the sults
the cnn daily mail hermann et al
nallapati et al
is used for the summarization task
for fully trainable chical encoder models we use two bi directional lstms for the sentence and the document encoder trained h lstm
there are different types of untrained random hierarchical encoders that we investigate random h lstm random bi directional lstms rand lstm for the sentence and encoder with weight document ces and biases initialized uniformly from u where is the hidden size of d lstms
identity h lstm similar to a but with lstm hidden weights and biases matrices set to the identity i
random lstm esn a random directional lstm sentence encoder ized in the same way as a and an echo state network esn for the document encoder with weights sampled i
i

randomly from the normal distribution n
the architecture of a random encoder rization model is depicted in figure
all current networks including the echo state network have a single layer
note that by using tied beddings press and wolf source word beddings are learned in both random and trainable encoders
this is an important setting in our periments as we aim to isolate the effect of trained used the data and preprocessing code provided in
com abisee cnn dailymail enc dec trained random random lstm h lstm h lstm esn

















table test perplexity of trained and random chical encoder models on the cnn daily mail dataset lower is better
note that enc encoder hidden size and dec decoder hidden size
percentages in these s are the relative perplexity degradation in random encoder models with respect to the associated trained h lstm encoder model
encoders from that of trained word embeddings
in all experiments a single directional lstm is used for the decoder
we generally follow the standard hyperparameters suggested in see et al
and cohan et al

we guide the readers to the appendix for more details on training and evaluation steps

performance of random encoder models table shows rouge scores lin a monly used performance metric in summarization for trained and untrained random encoder els
of note our hierarchical random encoder models random h lstm and random lstm esn obtain rouge scores close to other trained models
the gap between our trained and dom hierarchical encoders is about
point for all rouge scores
hierarchical random encoders even outperform a competitive baseline pati et al
in terms of even though the cited model uses pre trained dings
with respect to the trained h lstm the random h lstm achieves rouge scores that the gap is
in are very similar
in and
in rouge l
we also tested the identity h lstm to get an idea of the role that trained word embeddings play in the performance
the identity h lstm creates sentences representations by accumulating trained word embeddings in equation
to measure the representational power of random projection in the encoder we compare the rouge scores of random hierarchical encoder models with those of the identity h lstm model
we notice that the random encoders greatly the identity h lstm encoder
this has brought us closer to gauging the effectiveness of randomly projected recurrent encoder hidden states over an lation of word embeddings

impact of increasing capacity table shows the test perplexity of random archical encoder models with different encoder or decoder hidden sizes
we chose to base our ysis on perplexity instead of rouge to isolate model performance from the effect of word pling methods such as beam search and to show the quality of overall predicted word ties
it is shown that increased model capacity leads to lower test perplexity which implies ter target word prediction
the improvement in performance however is not equal across models
we notice that random encoders close the mance gap with the fully trained counterpart as the encoder hidden size increases
for instance as we vary the encoder hidden size of the random lstm from to the relative perplexity gap with the trained h lstm diminishes from to
this pattern aligns with the previous work from wieting and kiela where authors covered that the performance of random sentence encoder converged to that of trained one as the mensionality of sentence embeddings increased
we perform similar experiments with the coder hidden size which varies from to while xing the encoder hidden size to
we rst expected that the hidden size of a trained coder would play a larger role in enhancing the model performance than that of a random encoder
as shown in table however the perplexity of random encoder models with the largest encoder hidden size
and
is close to that of random encoder models with the largest decoder hidden size
and

moreover the formance gaps of the previously mentioned guration with respect to its fully trained part are just as small vs and vs
there are two conclusions that we can draw from this result
first increasing the capacity of random encoder closes the performance gap tween fully trained and random encoder models
second increasing the number of parameters in a random encoder yields similar improvements to performance gap between random and identity tialization could be more pronounced if the input word beddings had not been trained
document encoder sentence encoder figure relative weight change w w legend indicates different combinations of encoder decoder hidden sizes
at every update for the encoders of the trained h lstm
the gure increasing the number of parameters of a trainable decoder
this illustrates important advantages of using random encoder in terms of the number of parameters to train

gradient ow of trainable encoders one may suspect that the smaller performance gap in bigger encoder or decoder hidden size might arise from optimization issues in rnns bengio et al
hochreiter et al
such as the vanishing gradient problem
to verify that the rameters of large trained models are learned erly we analyze the distribution of weight eters and gradients of each model
from our results we rst notice that networks with different capacity have different scale of rameter and gradient values
this makes it sible to directly compare the gradient distributions of models with different capacity
we thus amine the relative amount of weight changes in trained encoder lstms as follows w w i n n wj wj i wj i where w is a weight parameter of the encoder lstm n is the number of parameters in the coder lstm j is the weight index and i is the number of training updates
the relative encoder s weight change is depicted in figure over iterations
we observe that there is no ca nt difference in the relative weight changes tween small and large encoder models
sentence and document encoder weights with and hidden sizes show similar patterns over training iterations
for more details on the distributions of weight parameters and gradients we refer the reader to the appendix
we have also added ing curves in the appendix to show that our trained models indeed converged
given that the eters of the trained h lstm were properly timized we can thus conclude that the trained weights do not contribute signicantly to model performance on a conditional natural language generation task such as summarization
conclusion and future work in this comparative study we analyzed the mance of untrained randomly initialized encoders on a more complex task than classication ing and kiela
concretely the performance of random hierarchical encoder models was uated on the challenging task of abstractive marization
we have shown that untrained dom encoders are able to capture the hierarchical structure of documents and that their tion qualiy is comparable to that of fully trained models
we further provided empirical evidence that increasing the model capacity not only hances the performance of the model but closes the gap between random and fully trained chical encoders
for future works we will further investigate the effectiveness of random encoders in various nlp tasks such as machine translation and question answering
acknowledgements we would like to thank devon hjelm ioannis mitliagkas catherine lefebvre for their useful comments and minh dao for helping with the ures
this work was partially supported by the ivado excellence scholarship and the canada first research excellence fund
references yossi adi einat kermany yonatan belinkov ofer lavi and yoav goldberg

fine grained ysis of sentence embeddings using auxiliary tion tasks
corr

yonatan belinkov nadir durrani fahim dalvi san sajjad and james r
glass

what do ral machine translation models learn about ogy corr

y
bengio p
simard and p
frasconi

ing long term dependencies with gradient descent is difcult
trans
neur
netw

arman cohan franck dernoncourt doo soon kim trung bui seokhwan kim walter chang and nazli goharian

a discourse aware attention model for abstractive summarization of long documents
corr

alexis conneau and douwe kiela

senteval an evaluation toolkit for universal sentence tions
arxiv preprint

alexis conneau douwe kiela holger schwenk loic barrault and antoine bordes

supervised learning of universal sentence representations from arxiv preprint natural language inference data


fahim dalvi nadir durrani hassan sajjad yonatan belinkov and stephan vogel

understanding and improving morphological learning in the neural machine translation decoder
in ijcnlp
john duchi elad hazan and yoram singer

adaptive subgradient methods for online learning journal of machine and stochastic optimization
learning research
allyson ettinger ahmed elgohary and philip resnik

probing for semantic evidence of composition by means of simple classication tasks
in ings of the workshop on evaluating vector space representations for nlp pages berlin germany
association for computational tics
karl moritz hermann tomas kocisky edward grefenstette lasse espeholt will kay mustafa leyman and phil blunsom

teaching chines to read and comprehend
in proceedings of the international conference on neural mation processing systems volume pages cambridge ma usa
mit press
sepp hochreiter yoshua bengio and paolo frasconi

gradient ow in recurrent nets the difculty of learning long term dependencies
in j
kolen and s
kremer editors field guide to dynamical current networks
ieee press
sepp hochreiter and jurgen schmidhuber

long short term memory
neural comput

guang bin huang lei chen and chee kheong siew

universal approximation using incremental constructive feedforward networks with random den nodes
trans
neur
netw

zhu q

huang g

and siew c


extreme learning machine a new learning scheme of ward neural networks
in ieee international joint conference on neural networks ieee cat
no
volume pages vol

dieuwke hupkes sara veldhoen and willem h
zuidema

visualisation and diagnostic siers reveal how recurrent and recursive neural corr networks process hierarchical structure


herbert jaeger

the echo state approach to analysing and training recurrent neural with an erratum note
bonn germany german national research center for information ogy gmd technical report
kevin jarrett koray kavukcuoglu marcaurelio zato and yann lecun

what is the best in stage architecture for object recognition ieee international conference on computer vision iccv pages
douwe kiela alexis conneau allan jabri and imilian nickel

learning visually grounded sentence representations
corr

ryan kiros yukun zhu ruslan r salakhutdinov richard zemel raquel urtasun antonio torralba and sanja fidler

skip thought vectors
in advances in neural information processing systems pages
v krylov and s krylov

reservoir computing journal of echo state network classier training
physics conference series
brenden m
lake and marco baroni

still not systematic after all these years on the tional skills of sequence to sequence recurrent works
corr

bin li yibin li and xuewen rong

son of echo state network and extreme learning chine on nonlinear prediction
journal of tional information systems
chin yew lin

looking for a few good metrics automatic summarization evaluation how many samples are enough in ntcir
tal linzen emmanuel dupoux and yoav goldberg

assessing the ability of lstms to learn sensitive dependencies
corr

marvin minsky and oliver g
selfridge

ing in random nets
ieee transactions on tion theory
ramesh nallapati bowen zhou caglar gulcehre bing xiang al

abstractive text rization using sequence to sequence rnns and yond
arxiv preprint

or press and lior wolf

using the output arxiv embedding to improve language models
preprint

andrew m
saxe pang wei koh zhenghao chen neesh bhand bipin suresh and andrew y
ng

on random weights and unsupervised in proceedings of the ture learning
national conference on international conference on machine learning pages usa
omnipress
w
f
schmidt m
a
kraaijveld and r
p
w
duin

feedforward neural networks with random weights
in proceedings
iapr international conference on pattern recognition
vol
ii
ence b pattern recognition methodology and tems pages
abigail see peter j
liu and christopher d
manning

get to the point summarization with generator networks
corr

emma strubell and andrew mccallum

tax helps elmo understand semantics is syntax still relevant in a deep neural architecture for srl in proceedings of the workshop on the relevance of linguistic structure in neural architectures for nlp pages melbourne australia
tion for computational linguistics
john wieting and douwe kiela

no training required exploring random encoders for sentence classication
corr

tom young devamanyu hazarika soujanya poria and erik cambria

recent trends in deep ieee learning based natural language processing
computational intelligence magazine
a appendix a
training and evaluation the dimensionality of embeddings is and beddings are trained from scratch
the vocabulary size is limited to
during training we strain the document length to tokens and the summary length to tokens
batch size is and learning rate is

adagrad duchi et al
with an initial accumulator value of
is used for optimization
maximum gradient norm is set to
training is performed for epochs
at test time we set the maximum number of generated tokens to
beam search with beam size is used for decoding
to evaluate the qualities of ated summaries we use the standard rouge ric lin and report standard rouge scores
a
learning curves figure and show the learning curves of able and random encoder summarization models with different encoder and decoder hidden sizes
note that the gap in training and validation plexity between trained and random encoder els get smaller as the encoder or decoder hidden size increases
a
weight and gradient distribution figure and present the distributions of model parameters and gradients of fully trainable models with different capacities
note that models with different capacities have different scale of bution
models with smaller encoder hidden size tend to have larger scale of parameter and gradient values
c e figure training perplexity of trained and random encoder summarization models with different encoder and decoder hidden sizes
enc denotes encoder and dec denotes decoder
numbers in parentheses are corresponding hidden sizes
c e figure validation perplexity of trained and random encoder summarization models with different encoder and decoder hidden sizes
enc denotes encoder and dec denotes decoder
numbers in parentheses are corresponding hidden sizes
weight weight c weight gradient e gradient gradient figure distribution of weight parameters and gradients of the document encoder
a weight weight c weight gradient e gradient gradient figure distribution of weight parameters and gradients of the sentence encoder

