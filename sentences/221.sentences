neural text summarization a critical evaluation wojciech krysci nski nitish shirish keskar bryan mccann caiming xiong richard socher salesforce research kryscinski nkeskar bmccann cxiong
com g u a l c
s c v
v i x r a abstract text summarization aims at compressing long documents into a shorter form that conveys the most important parts of the original ment
despite increased interest in the munity and notable research effort progress on benchmark datasets has stagnated
we ically evaluate key ingredients of the current research setup datasets evaluation metrics and models and highlight three primary comings automatically collected datasets leave the task underconstrained and may tain noise detrimental to training and tion current evaluation protocol is weakly correlated with human judgment and does not account for important characteristics such as factual correctness models overt to layout biases of current datasets and offer limited versity in their outputs
introduction text summarization aims at compressing long tual documents into a short human readable form that contains the most important information from the source
two strategies of generating maries are extractive dorr et al
nallapati et al
where salient fragments of the source document are identied and directly copied into the summary and abstractive rush et al
see et al
where the salient parts are tected and paraphrased to form the nal output
in network the number of summarization models duced every year has been increasing rapidly
advancements neural tures sutskever et al
bahdanau et al
vinyals et al
vaswani et al
and the availability of large scale data sandhaus nallapati et al
grusky et al
enabled the transition from systems based on expert knowledge and heuristics to data driven approaches powered by end to end deep neural current approaches to text models
rization utilize advanced attention and copying mechanisms see et al
tan et al
cohan et al
multi task and multi reward training techniques guo et al
pasunuru and bansal kryscinski et al
forcement learning strategies paulus et al
narayan et al
dong et al
wu and hu and hybrid extractive abstractive models liu et al
hsu et al
gehrmann et al
chen and bansal
many of the introduced models are trained on the cnn dailymail nallapati et al
news corpus a popular benchmark for the eld and are evaluated based on n gram overlap between the generated and target summaries with the rouge package lin
despite substantial research effort the progress on these benchmarks has stagnated
state of art models only slightly outperform the baseline which generates summaries by ing the rst three sentences of the source ument
we argue that this stagnation can be partially attributed to the current research setup which involves uncurated automatically collected datasets and non informative evaluations cols
we critically evaluate our hypothesis and support our claims by analyzing three key nents of the experimental setting datasets tion metrics and model outputs
our motivation is to shift the focus of the research community into developing a more robust research setup for text summarization
related work
datasets to accommodate the requirements of ern data driven approaches several large scale datasets have been proposed
the majority of available corpora come from the news domain
gaword graff and cieri is a set of cles and corresponding titles that was originally used for headline generation takase et al
but it has also been adapted to single sentence summarization rush et al
chopra et al

nyt sandhaus is a collection of ticles from the new york times magazine with stracts written by library scientists
it has been marily used for extractive summarization hong and nenkova li et al
and importance prediction yang and nenkova nye and nenkova
the cnn dailymail nallapati et al
dataset consists of ticles with summaries composed of highlights from the article written by the authors themselves
it is commonly used for both abstractive see et al
paulus et al
kryscinski et al
and extractive dong et al
wu and hu zhou et al
neural tion
the collection was originally introduced as a cloze style qa dataset by hermann et al

xsum narayan et al
is a lection of articles associated with one sentence summary targeted at abstractive models
newsroom grusky et al
is a diverse lection of articles sourced from major online news outlets
this dataset was released together with a leaderboard and held out testing split
outside of the news domain several datasets were collected from open discussion boards and other portals offering structure information
dit tifu kim et al
is a collection of posts scraped from reddit where users post their daily stories and each post is required to contain a too long did nt read summary
how koupaee and wang is a collection of articles from the wikihow knowledge base where each article contains instructions for performing procedural multi step tasks covering various eas including arts nance travel and health

evaluation metrics manual and semi automatic nenkova and sonneau passonneau et al
evaluation of large scale summarization models is costly and cumbersome
much effort has been made to velop automatic metrics that would allow for fast and cheap evaluation of models
the rouge package lin offers a set of automatic metrics based on the lexical lap between candidate and reference summaries
overlap can be computed between consecutive n grams and non consecutive skip grams sequences of tokens
rouge scores are based on exact token matches meaning that computing overlap between synonymous phrases is not ported
many approaches have extended rouge with support for synonyms and paraphrasing
val zhou et al
uses a three step son strategy where the rst two steps perform timal and greedy paraphrase matching based on paraphrase tables before reverting to exact token overlap
rouge we ng and abrecht places exact lexical matches with a soft tic similarity measure approximated with the sine distances between distributed representations of tokens
rouge
ganesan leverages synonym dictionaries such as wordnet and siders all synonyms of matched words when puting token overlap
rouge g shaeibavani et al
combines lexical and semantic ing by applying graph analysis algorithms to the wordnet semantic network
despite being a step in the direction of a more comprehensive ation protocol none of these metrics gained cient traction in the research community ing rouge as the default automatic evaluation toolkit for text summarization

models existing summarization models fall into three egories abstractive extractive and hybrid
extractive models select spans of text from the input and copy them directly into the mary
non neural approaches neto et al
dorr et al
filippova and altun menares et al
utilized domain expertise to develop heuristics for summary content tion whereas more recent neural techniques low for end to end training
in the most common case models are trained as or level classiers that predict whether a fragment should be included in the summary nallapati et al
narayan et al
liu et al
xu and durrett
other approaches apply reinforcement learning training strategies to directly optimize the model on task specic differentiable reward functions narayan et al
dong et al
wu and hu
abstractive models paraphrase the source uments and create summaries with novel phrases not present in the source document
a mon approach in abstractive summarization is to use attention and copying mechanisms see et al
tan et al
cohan et al

other approaches include using multi task and reward training paulus et al
jiang and bansal guo et al
pasunuru and bansal kryscinski et al
and pervised training strategies chu and liu schumann
hybrid models hsu et al
liu et al
gehrmann et al
chen and bansal include both extractive and abstractive modules and allow to separate the summarization process into two phases content selection and paraphrasing
for the sake of brevity we do not describe tails of different models we refer interested ers to the original papers

analysis and critique most summarization research revolves around new architectures and training strategies that prove the state of the art on benchmark problems
however it is also important to analyze and tion the current methods and research settings
zhang et al
conducted a quantitative study of the level of abstraction in abstractive summarization models and showed that level copy only extractive models achieve parable results to fully abstractive models in the measured dimension
kedzie et al
offered a thorough analysis of how neural models perform content selection across different data domains and exposed data biases that dominate the ing signal in the news domain and architectural limitations of current approaches in learning bust sentence level representations
liu and liu examine the correlation between rouge scores and human judgments when evaluating meeting summarization data and show that the correlation strength is low but can be improved by leveraging unique meeting characteristics such as available speaker information
owczarzak et al
inspect how inconsistencies in man annotator judgments affect the ranking of summaries and correlations with automatic uation metrics
the results showed that level rankings considering all summaries were stable despite inconsistencies in judgments ever summary level rankings and automatic ric correlations benet from improving annotator consistency
graham compare the tness of the bleu metric papineni et al
and a number of different rouge variants for ing summarization outputs
the study reveals perior variants of rouge that are different from the commonly used recommendations and shows that the bleu metric achieves strong correlations with human assessments of generated summaries
schulman et al
study the problems related to using rouge as an evaluation metric with respect to nding optimal solutions and provide proof of np hardness of global optimization with respect to rouge
similar lines of research where the authors put under scrutiny existing methodologies datasets or models were conducted by callison burch et al
tan et al
post in chine translation gkatzia and mahamood reiter and belz reiter in ral language generation lee et al
chen et al
kaushik and lipton in ing comprehension gururangan et al
liak et al
glockner et al
in ral language inference goyal et al
in sual question answering and xian et al
in zero shot image classication
comments on the general state of scholarship in the eld of chine learning were presented by sculley et al
lipton and steinhardt and ences therein
datasets
underconstrained task the task of summarization is to compress long documents by identifying and extracting the most important information from the source documents
however assessing the importance of information is a difcult task in itself that highly depends on the expectations and prior knowledge of the target reader
we show that the current setting in which els are simply given a document with one ated reference summary and no additional mation leaves the task of summarization constrained and thus too ambiguous to be solved by end to end models
to quantify this effect we conducted a human study which measured the agreement between ferent annotators in selecting important sentences article the glowing blue letters that once lit the bronx from above yankee stadium failed to nd a buyer at an auction at sotheby s on wednesday
while the letters were expected to bring in anywhere from to the only person who raised a paddle for was a sotheby s employee trying to jump start the bidding
the current owner of the signage is yankee hall of famer reggie jackson who purchased the feet tall letters for an undisclosed amount after the stadium saw its nal game in
no love letters that hung over yankee stadium were estimated to bring in anywhere from to but received no bids at a sotheby s auction wednesday
the year old yankee said he wanted a new generation to own and enjoy this icon of the yankees and of new york city
the letters had beamed from atop yankee stadium near grand concourse in the bronx since the year before jackson joined the team



summary questions when was the auction at sotheby who is the owner of the signage when had the letters been installed on the stadium glowing letters that had been hanging above the yankee dium from to were placed for auction at sotheby s on wednesday but were not sold the current owner of the sign is reggie jackson a yankee hall of famer
constrained summary a unconstrained summary a constrained summary b unconstrained summary b an auction for the lights from yankee stadium failed to duce any bids on wednesday at sotheby s
the lights rently owned by former yankees player reggie jackson lit the stadium from until
there was not a single buyer at the auction at sotheby s on wednesday for the glowing blue letters that once lit the bronx s yankee stadium
not a single non employee raised their paddle to bid
jackson the owner of the letters was prised by the lack of results
the venue is also auctioning off other items like mets memorabilia
the once iconic and attractive pack of letters that was placed at the yankee stadium in and later removed in was unexpectedly not favorably considered at the sotheby s auction when the year old owner of the letters attempted to transfer its ownership to a member the younger populace
thus when the minimum estimate of was not met a further attempt was made by a former player of the yankees to personally visit the new owner as an table example summaries collected from human annotators in the constrained left and unconstrained right task
in the unconstrained setting annotators were given a news article and asked to write a summary covering the parts they considered most important
in the constrained setting annotators were given a news article with three associated questions and asked to write a summary that contained the answers to the given questions
from a fragment of text
we asked workers to write summaries of news articles and highlight tences from the source documents that they based their summaries on
the experiment was ducted in two settings unconstrained where the annotators were instructed to summarize the tent that they considered most important and strained where annotators were instructed to write summaries that would contain answers to three questions associated with each article
this is ilar to the construction of the tac ion summarization task
the questions ated with each article where collected from human workers through a separate assignment
ments were conducted on randomly sampled articles further details of the human study can be found in appendix a

table shows the average number of sentences per article that annotators agreed were important

nist
summarization op
summ

guidelines
html the rows show how the average changes with the human vote threshold needed to reach consensus about the importance of any sentence
for ple if we require that three or more human votes are necessary to consider a sentence important notators agreed on average on the importance of
and
sentences per article in the constrained and constrained settings respectively
the average length in sentences of sampled cles was
with a standard deviation of

the study demonstrates the difculty and ity of content selection in text summarization
we also conducted a qualitative study of maries written by annotators
examples ing summaries written in the constrained and constrained setting are shown in table
we ticed that in both cases the annotators correctly identied the main topic and important fragments of the source article
however constrained maries were more succinct and targeted out sacricing the natural ow of sentences
human vote threshold sent
per article considered important unconstrained constrained









table average number of sentences per article which annotators agreed were important
the human vote threshold investigates how the average agreement changes with the threshold of human votes required to consider any sentence important
rows and correspond to the set intersection and union of selected sentences accordingly
constrained writers tended to write more verbose summaries that did not add information
the study also highlights the abstractive nature of human written summaries in that similar content can be described in unique ways

layout bias in news data figure the distribution of important sentences over the length of the article according to human annotators blue and its cumulative distribution red
news articles adhere to a writing structure known in journalism as the inverted mid purdueowl
in this form initial paragraphs contain the most newsworthy tion which is followed by details and background information
to quantify how strongly articles in the cnn dm corpus follow this pattern we conducted a human study that measured the importance of different sections of the article
annotators read news articles and selected sentences they found most important
experiments were conducted on randomly sampled articles further details of the human study are described in appendix a

figure presents how annotator selections were distributed over the length of the article
the distribution is skewed towards the rst quarter of the length of articles
the cumulative plot shows that nearly of the important information was present in the rst third of the article and mately and of selections pointing to the second and last third respectively
it has become standard practice to exploit such biases during training to increase performance of models see et al
paulus et al
kryscinski et al
gehrmann et al
jiang and bansal pasunuru and bansal but the importance of these heuristics has been accepted without being quantied
these same heuristics would not apply to books or legal documents which lack the inverted pyramid out so common in the news domain so it is tant that these heuristics be part of ablation ies rather than accepted as default pre processing step

noise in scraped datasets given the data requirements of deep neural works and the vast amounts of diverse resources available online automatically scraping web tent is a convenient way of collecting data for new corpora
however adapting scraped content to the needs of end to end models is problematic
given that manual inspection of data is infeasible and man annotators are expensive data curation is ally limited to removing any markup structure and applying simple heuristics to discard obviously awed examples
this in turn makes the quality of the datasets heavily dependent on how well the scraped content adheres to the assumptions made by the authors about its underlying structure
this issue suggests that available tion datasets would be lled with noisy examples
manual inspection of the data particularly the erence summaries revealed easily detectable sistent patterns of awed examples many such amples can be isolated using simple regular pressions and heuristics which allows mation of how widespread these aws are in the dataset
we investigated this issue in two large marization corpora scraped from the internet cnn dm links to other articles michael carrick has helped manchester united win their last six games
carrick should be selected alongside gary cahill for england
carrick has been overlooked too many times by his country
read carrick and man united team mates enjoy second christmas party
newsroom links to news sources the latest breaking news get washington dc virginia maryland and national featuring national news
get security read news headlines from the nation and from the washington post
visit www
washingtonpost
com nation today
science and courts
table examples of noisy reference summaries found in the cnn dm and newsroom datasets
article quick thinking brady olson a teacher at north thurston high took down a gunman on monday
a washington high school teacher is being hailed a hero for tackling a old student to the ground after he opened re on monday morning


summary factually incorrect brady olson a washington high school teacher at north thurston high opened re on monday morning
no one was injured after the boy shot twice toward the ceiling in the school commons before classes began at north thurston high school in lacey


table example of a factually incorrect summary generated by an abstractive model
top ground truth article
bottom summary generated by model
cnn dm nallapati et al
and the room grusky et al

the problem of noisy data affects

and
of the ing validation and test split of the cnn dm dataset and

and
of the spective splits of the newsroom dataset
ples of noisy summaries are shown in table
flawed examples contained links to other cles and news sources placeholder texts unparsed html code and non informative passages in the reference summaries
evaluation metrics
weak correlation with human judgment the effectiveness of rouge was previously uated lin graham through cal correlations with human judgment on the duc datasets over and yen
ever their setting was substantially different from the current environment in which summarization models are developed and evaluated
to investigate the robustness of rouge in the setting in which it is currently used we evaluate how its scores correlate with the judgment of an average english speaker using examples from the cnn dm dataset
following the human ation protocol from gehrmann et al
we asked annotators to rate summaries across four mensions relevance selection of important tent from the source consistency factual ment between the summary and the source ency quality of individual sentences and ence collective quality of all sentences
each summary was rated by distinct judges with the nal score obtained by averaging the individual scores
experiments were conducted on domly sampled articles with the outputs of summarization systems provided by the original authors
correlations were computed between all pairs of rouge scores for all tems
additional summaries were collected from annotators to inspect the effect of using multiple ground truth labels on the correlation with matic metrics
further details of the human study can be found in appendix a

results are shown in table
the left section of the table presents pearson s correlation cients and the right section presents kendall rank correlation coefcients
in terms of pearsons s efcients the study showed minimal correlation with any of the annotated dimensions for both stractive and extractive models together and for abstractive models individually
weak tion was discovered for extractive models ily with the uency and coherence dimensions
we hypothesized that the noise contained in the ne grained scores generated by both human notators and rouge might have affected the relation scores
we evaluated the relation on a higher level of granularity by means of correlation between rankings of models that were obtained from the ne grained scores
the study showed weak correlation with all measured dimensions when evaluated for both abstractive and extractive models together and for abstractive models vidually
moderate correlation was found for tractive models across all dimensions
a ing result was that correlations grew weaker with the increase of ground truth references
our results align with the observations from liu and liu who also evaluated rouge side of its original setting
the study highlights the limited utility in measuring progress of the eld reference r l pearson correlation references r l references r l reference r l kendall rank correlation references r l references r l relevance consistency fluency coherence relevance consistency fluency coherence relevance consistency fluency coherence



























































all models























abstractive models extractive models



































































































































table correlations between human annotators and rouge scores along different dimensions and multiple reference set sizes
left pearson s correlation coefcients
right kendall s rank correlation coefcients
solely by means of rouge scores

insufcient evaluation protocol the goal of text summarization is to cally generate succinct uent relevant and tually consistent summaries
the current tion protocol depends primarily on the exact cal overlap between reference and candidate maries measured by rouge
in certain cases rouge scores are complemented with human studies where annotators rate the relevance and uency of generated summaries
neither of the methods explicitly examines the factual tency of summaries leaving this important sion unchecked
to evaluate the factual consistency of existing models we manually inspected randomly pled articles with summaries coming from domly chosen abstractive models
we focused exclusively on factual incorrectness and ignored any other issues such as low uency
out of article summary pairs that were reviewed ally we found that contained tency issues
table shows examples of covered inconsistencies
some of the discovered inconsistencies despite being factually incorrect could be rationalized by humans
however in many cases the errors were substantial and could have severe repercussions if presented as is to get readers
models
layout bias in news data we revisit the problem of layout bias in news data from the perspective of models
kedzie et al
showed that in the case of news articles the layout bias dominates the learning signal for neural models
in this section we approximate the degree with which generated summaries rely on the leading sentences of news articles
we computed rouge scores for collected models in two settings rst using the cnn dm reference summaries as the ground truth and ond where the leading three sentences of the source article were used as the ground truth i
e
the baseline
we present the results in ble
for all examined models we noticed a tial increase of overlap across all rouge variants
results suggest that performance of current els is strongly affected by the layout bias of news corpora
is a strong baseline that exploits the described layout bias
however there is still a large gap between its performance and an upper bound for extractive models extractive oracle

diversity of model outputs models analyzed in this paper are considerably different from each other in terms of architectures training strategies and underlying approaches
we inspected how the diversity in approaches translates into the diversity of model outputs
we computed and scores between pairs of model outputs to compare them by means of token and phrase overlap
results are visualized in figure where the values above and below the diagonal are and scores accordingly and model names follow the der from table
extractive oracle grusky et al
baseline





target reference r l reference











abstractive models



















extractive models





















r l













































































model hsu et al
model gehrmann et al
model jiang and bansal model chen and bansal model see et al
model kryscinski et al
model li et al
model pasunuru and bansal model zhang et al
model guo et al
model dong et al
model wu and hu model zhou et al
table rouge scores computed for different models on the test set of the cnn dm dataset
left scores computed with the original reference summaries
right scores computed with used as the reference
we notice that the scores vary siderably less than scores
this gests that the models share a large part of the cabulary on the token level but differ on how they organize the tokens into longer phrases
comparing results with the n gram overlap tween models and reference summaries table shows a substantially higher overlap between any model pair than between the models and reference summaries
this might imply that the training data contains easy to pick up patterns that all models overt to or that the information in the training signal is too weak to connect the content of the source articles with the reference summaries
conclusions this critique has highlighted the weak points of the current research setup in text summarization
we showed that text summarization datasets quire additional constraints to have well formed summaries current state of the art methods learn to rely too heavily on layout bias associated with the particular domain of the text being rized and the current evaluation protocol reects human judgments only weakly while also failing to evaluate critical features e

factual ness of text summarization
we hope that this critique provides the rization community with practical insights for ture research directions that include the tion of datasets models less t to a particular figure pairwise similarities between model outputs computed using rouge
above diagonal unigram overlap
below diagonal gram overlap
model order follows table
main bias and evaluation that goes beyond current metrics to capture the most important features of summarization
acknowledgements we thank all the authors listed in table for ing their model outputs and thus contributing to this work
we also thank shaq rayhan joty for reviewing this manuscript and providing valuable feedback
references dzmitry bahdanau kyunghyun cho and yoshua gio

neural machine translation by jointly learning to align and translate
in iclr
chris callison burch cameron s
fordyce philipp koehn christof monz and josh schroeder

evaluation of machine translation
in pages
association for putational linguistics
chris callison burch miles osborne and philipp koehn

re evaluation the role of bleu in chine translation research
in eacl
the tion for computer linguistics
danqi chen jason bolton and christopher d
ning

the cnn daily mail reading comprehension task
in acl
the association for computer linguistics
a thorough examination of yen chun chen and mohit bansal

fast tive summarization with reinforce selected sentence rewriting
in acl pages
association for computational linguistics
sumit chopra michael auli and alexander m
rush

abstractive sentence summarization with in naacl hlt tentive recurrent neural networks
the conference of the north american chapter of the association for computational guistics human language technologies san diego california usa june
eric chu and peter j
liu

unsupervised neural multi document abstractive summarization
corr

arman cohan franck dernoncourt doo soon kim trung bui seokhwan kim walter chang and nazli goharian

a discourse aware attention model for abstractive summarization of long documents
in proceedings of the conference of the north american chapter of the association for tional linguistics human language technologies naacl hlt new orleans louisiana usa june volume short papers
carlos a colmenares marina litvak amin mantrach and fabrizio silvestri

heads headline eration as sequence prediction using an abstract feature rich space
in hlt naacl pages
yue dong yikang shen eric crawford herke van hoof and jackie chi kit cheung

banditsum extractive summarization as a contextual bandit
in proceedings of the conference on empirical methods in natural language processing brussels belgium october november
bonnie dorr david zajic and richard schwartz

hedge trimmer a parse and trim approach to line generation
in hlt naacl
katja filippova and yasemin altun

ing the lack of parallel data in sentence compression
in proceedings of emnlp pages
seer
kavita ganesan

rouge
updated and improved measures for evaluation of summarization tasks
corr

sebastian gehrmann yuntian deng and alexander m
rush

bottom up abstractive summarization
in emnlp pages
association for putational linguistics
dimitra gkatzia and saad mahamood

a in shot of nlg evaluation practices
enlg proceedings of the european workshop on natural language generation september university of brighton brighton uk pages
max glockner vered shwartz and yoav goldberg

breaking nli systems with sentences that quire simple lexical inferences
in acl pages
association for computational tics
yash goyal tejas khot douglas summers stay dhruv batra and devi parikh

making the v in vqa matter elevating the role of image derstanding in visual question answering
in cvpr pages
ieee computer society
david graff and c cieri

english gigaword guistic data consortium
yvette graham

re evaluating automatic marization with bleu and shades of rouge
in emnlp pages
the association for computational linguistics
max grusky mor naaman and yoav artzi

newsroom a dataset of
million summaries with diverse extractive strategies
in proceedings of the conference of the north american chapter of the association for computational linguistics human language technologies naacl hlt new orleans louisiana usa june ume long papers
han guo ramakanth pasunuru and mohit bansal

soft layer specic multi task summarization in with entailment and question generation
ceedings of the annual meeting of the tion for computational linguistics acl bourne australia july volume long papers
suchin gururangan swabha swayamdipta omer levy roy schwartz samuel r
bowman and noah a
smith

annotation artifacts in in naacl hlt ural language inference data
pages
association for computational guistics
karl moritz hermann tomas kocisky edward grefenstette lasse espeholt will kay mustafa leyman and phil blunsom

teaching chines to read and comprehend
in nips
kai hong and ani nenkova

improving the estimation of word importance for news in proceedings of the document summarization
conference of the european chapter of the association for computational linguistics eacl april gothenburg sweden
wan ting hsu chieh kai lin ming ying lee kerui min jing tang and min sun

a unied model for extractive and abstractive summarization using inconsistency loss
in proceedings of the annual meeting of the association for tional linguistics acl melbourne australia july volume long papers
yichen jiang and mohit bansal

closed book training to improve summarization encoder memory
in emnlp pages
association for putational linguistics
divyansh kaushik and zachary c
lipton

how much reading does reading comprehension require a critical investigation of popular benchmarks
in emnlp pages
association for tational linguistics
chris kedzie kathleen r
mckeown and hal daume iii

content selection in deep learning models in emnlp pages
of summarization
association for computational linguistics
byeongchang kim hyunwoo kim and gunhee kim

abstractive summarization of reddit posts with multi level memory networks
corr

mahnaz koupaee and william yang wang

ihow a large scale text summarization dataset
corr

wojciech kryscinski romain paulus caiming xiong and richard socher

improving abstraction in text summarization
in emnlp pages
association for computational linguistics
moontae lee xiaodong he wen tau yih jianfeng gao li deng and paul smolensky

ing in vector space an exploratory study of tion answering
in iclr
junyi jessy li kapil thadani and amanda stent

the role of discourse units in near extractive in proceedings of the sigdial summarization
conference the annual meeting of the special interest group on discourse and dialogue september los angeles ca usa
wei li xinyan xiao yajuan lyu and yuanzhuo improving neural abstractive wang

ment summarization with structural regularization
in emnlp pages
association for putational linguistics
chin yew lin

rouge a package for automatic evaluation of summaries
in proc
acl workshop on text summarization branches out page
zachary c
lipton and jacob steinhardt

bling trends in machine learning scholarship
acm queue
feifan liu and yang liu

exploring correlation between rouge and human evaluation on meeting summaries
ieee trans
audio speech language processing
jingyun liu jackie chi kit cheung and annie louis

what comes next extractive corr marization by next sentence prediction


peter j
liu mohammad saleh etienne pot ben goodrich ryan sepassi lukasz kaiser and noam shazeer

generating wikipedia by ing long sequences
in international conference on learning representations iclr ver bc canada april may ence track proceedings
ramesh nallapati feifei zhai and bowen zhou

summarunner a recurrent neural network based quence model for extractive summarization of ments
in aaai
ramesh nallapati bowen zhou c aglar gulcehre bing xiang al

abstractive text marization using sequence to sequence rnns and yond
proceedings of signll conference on putational natural language learning
ramesh nallapati bowen zhou and mingbo ma

classify or select neural architectures for extractive document summarization
corr

shashi narayan shay b
cohen and mirella lapata

do nt give me the details just the summary topic aware convolutional neural networks for in proceedings of the treme summarization
conference on empirical methods in natural guage processing brussels belgium
shashi narayan shay b
cohen and mirella lapata

ranking sentences for extractive in rization with reinforcement learning
hlt pages
association for tional linguistics
shashi narayan nikos papasarantopoulos mirella pata and shay b
cohen

neural tive summarization with side information
corr

ani nenkova and rebecca j
passonneau

ating content selection in summarization the mid method
in human language technology ference of the north american chapter of the ciation for computational linguistics hlt naacl boston massachusetts usa may
joel larocca neto alex a freitas and celso aa kaestner

automatic text summarization ing a machine learning approach
in brazilian posium on articial intelligence pages
springer
jun ping ng and viktoria abrecht

better marization evaluation with word embeddings for rouge
corr

benjamin nye and ani nenkova

identication and characterization of newsworthy verbs in world news
in naacl hlt the conference of the north american chapter of the association for computational linguistics human language nologies denver colorado usa may june
paul over and james yen

an introduction to intrinsic evaluation of generic news text summarization systems
paul over and james yen

an introduction to intrinsic evaluation of generic news text summarization systems
paul over and james yen

an introduction to intrinsic evaluation of generic news text summarization systems
karolina owczarzak peter a
rankel hoa trang dang and john m
conroy

assessing the fect of inconsistent assessors on summarization uation
in acl pages
the association for computer linguistics
kishore papineni salim roukos todd ward and jing zhu

bleu a method for automatic uation of machine translation
in acl pages
acl
rebecca j
passonneau emily chen weiwei guo and dolores perin

automated pyramid scoring of in acl summaries using distributional semantics
pages
the association for computer linguistics
ramakanth pasunuru and mohit bansal

reward reinforced summarization with saliency and entailment
corr

romain paulus caiming xiong and richard socher

a deep reinforced model for abstractive marization
in iclr
adam poliak jason naradowsky aparajita haldar rachel rudinger and benjamin van durme

hypothesis only baselines in natural language ence
in hlt pages
sociation for computational linguistics
matt post

a call for clarity in reporting bleu in wmt pages
association for scores
computational linguistics
purdueowl

journalism and journalistic ing the inverted pyramid structure
accessed
ehud reiter

a structured review of the validity of bleu
computational linguistics
ehud reiter and anja belz

an investigation into the validity of some metrics for automatically ating natural language generation systems
tational linguistics
alexander m rush sumit chopra and jason weston

a neural attention model for abstractive tence summarization
proceedings of emnlp
evan sandhaus

the new york times annotated corpus
linguistic data consortium philadelphia
john schulman nicolas heess theophane weber and pieter abbeel

gradient estimation using stochastic computation graphs
in nips
raphael schumann

unsupervised tive sentence summarization using length controlled variational autoencoder
corr

d
sculley jasper snoek alexander b
wiltschko and ali rahimi

winner s curse on pace progress and empirical rigor
in iclr workshop
openreview
net
abigail see peter j
liu and christopher d
manning

get to the point summarization with generator networks
in acl
elaheh shaeibavani mohammad ebrahimi mond k
wong and fang chen

a theoretic summary evaluation for rouge
in emnlp pages
association for computational guistics
ilya sutskever oriol vinyals and quoc v
le

sequence to sequence learning with neural works
in nips
sho takase jun suzuki naoaki okazaki tsutomu rao and masaaki nagata

neural headline generation on abstract meaning representation
in proceedings of the conference on empirical methods in natural language processing emnlp austin texas usa november
jiwei tan xiaojun wan and jianguo xiao

abstractive document summarization with a in proceedings based attentional neural model
of the annual meeting of the association for computational linguistics acl vancouver canada july august volume long papers
liling tan jon dehdari and josef van genabith

an awkward disparity between bleu ribes scores and human judgements in machine in wat pages
workshop on asian tion
translation
ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan n
gomez lukasz kaiser and illia polosukhin

attention is all you need
in advances in neural information cessing systems annual conference on neural information processing systems ber long beach ca usa pages
oriol vinyals meire fortunato and navdeep jaitly

pointer networks
in nips
yuxiang wu and baotian hu

learning to extract coherent summary via deep reinforcement in proceedings of the thirty second aaai ing
conference on articial intelligence the innovative applications of articial ligence and the aaai symposium on educational advances in articial intelligence new orleans louisiana usa february
yongqin xian bernt schiele and zeynep akata

zero shot learning the good the bad and the ugly
in cvpr pages
ieee computer ety
jiacheng xu and greg durrett

neural tive text summarization with syntactic compression
corr

yinfei yang and ani nenkova

detecting information dense texts in multiple news domains
in proceedings of the twenty eighth aaai ence on articial intelligence july quebec city quebec canada
fangfang zhang jin ge yao and rui yan

on the abstractiveness of neural document in emnlp pages
association for tion
computational linguistics
liang zhou chin yew lin dragos stefan munteanu and eduard h
hovy

paraeval using phrases to evaluate summaries automatically
in man language technology conference of the north american chapter of the association of tational linguistics proceedings june new york new york usa
qingyu zhou nan yang furu wei shaohan huang ming zhou and tiejun zhao

neural ment summarization by jointly learning to score and select sentences
in proceedings of the annual meeting of the association for computational guistics acl melbourne australia july volume long papers
a human study details reference summary
a
layout bias in news data human annotators were asked to read news ticles and highlight the sentences that contained the most important information
the study was conducted on randomly sampled articles with each article annotated by unique annotators
human studies were conducted through the zon mechanical turk platform
prices of tasks were carefully calculated to ensure that workers would have an average compensation of per hour
in all studies examples were sampled from the test split of the cnn dm dataset that contains a total of examples
as with any human study there is a trade off between the number of examples annotated the breadth of the experiments and the quality of notations
studies conducted for this paper were calibrated to primarily assure high quality of sults and the breadth of experiments
a
underconstrained task human annotators were asked to write summaries of news articles and highlight fragments of the source documents that they found useful for ing their summary
the study was conducted on randomly sampled articles with each article annotated by unique annotators
the same guration and articles were used in both the strained and unconstrained setting
questions for the constrained setting were ten by human annotators in a separate assignment and curated before being used for to collect maries
a
rouge weak correlation with human judgment this study evaluated the quality of summaries erated by different neural models tive and extractive
a list of evaluated models is available in table
the study was conducted on randomly pled articles with each article annotated by unique annotators
given the large number of evaluated models the experiment was split into groups
two groups contained models one group contained models
to prevent from lecting biased data models were assigned to periment groups on a per example basis thus domizing the context in which each model was evaluated
to establish a common reference point between groups the reference summaries from the dataset were added to the pool of annotated els however annotators were not informed which of this fact
the order in which summaries were displayed in the annotation interface was ized with the rst position always reserved for the
