cx a queryable extractive summarizer and semantic search engine allen roush university of oregon usa abstract competitive debate s increasingly technical nature has left competitors looking for tools to accelerate evidence production
we find that the unique type of extractive summarization performed by competitive debaters summarization with a bias towards a particular target meaning can be performed using the latest innovations in unsupervised pre trained text vectorization models
we introduce a queryable word level extractive summarizer and evidence creation framework which allows for summarization of rapid biasable arbitrarily sized texts
usage of the embedding framework flair means that as the underlying models improve will also improve
we observe that also functions as a semantic search engine and has application as a supplement to traditional find in programs and webpages
is currently used by competitive debaters and is made available to the public at
com hellis otherpeople
functionality introduction is the summarization extractive task of automatically producing a summary of a text by deleting uninformative tokens
this is in contrast to abstractive summarization which allows for deletion and replacement or insertion of tokens
one way of characterizing their difference is by using the highlighter vs pen analogy
extractive summarization is the process of highlighting a document to only include the most important or figure an example of a document used in the competitive debate community
the first three bolded and highlighted sentences are an abstract of the document
an extract of the document is made by highlighting all important text after the author and date
is an salient parts of the document
abstractive summarization is when one writes a completely new abstract based on the document
this abstract may include tokens which are not found in the document
extractive summarization system
prior work in extractive summarization systems includes many recent advances made in nlp jadhav and rajan zhao al

unfortunately these systems suffer from some key limitations
most of them do not utilize the latest in pre trained word and character embeddings
due to a focus on maximizing grammatical correctness they only populate summaries with candidate text at the sentence level
they also have a focus on creating faithful summaries e

they usually select sentences with the goal of taking the most similar sentences to the document as a whole
we find that documents can sometimes be summarized to say whatever a reader wants them to say
confidential review copy
do not distribute
is inspired by how s algorithm competitive debaters abstractively summarize documents which presents their accounting of what the evidence says and how it supports their argument
after presenting this summarization they recite an extractive summarization of that evidence
imitates this process
related work is not summarization queryable a new development
yulianti al
describe a system that concatenates tweets about a document alongside the document before summarizing
users found the summaries of this system preferable to those of unbiased summaries
azar et al
describes a query based summarizer using ensemble noisy auto encoders to select sentences
they test this system on emails using email subject lines as the query
lierde and chow describes a hypergraph based system from summarization based on a query which tries to maximize coverage of the query by selecting the most semantically meaningful sentences via graph traversal
chaudhari and mattukoyya describes a tf idf based document summarization model that biases towards specific words by incorporating a secondary polarity measuring model which selects sentences based on their sentiment
notably they anticipate the possibility of applications in businesses using biased summaries to create positive sounding testimonials from product buyers
they also anticipate the possibility of users content in a graceful manner
barve and desai describes three query based extractive models for summarizing sanskrit documents
sanskrit was chosen due to all of the known sanskrit texts in existence being digitized
most contemporary extractive summarizers focus on sentence level document summarization
nallapati et al
narayan et al
shi et al

even current word embedding based techniques for extractive summarization only work on the sentence level liu
yet word level extractive summarization systems do exist and are sometimes called sentence compression systems filippova al
klerke al

to our knowledge none of these systems are queryable nor do they utilize pre trained word embeddings at any point in their training process
our algorithm is similar to the existing text rank algorithm mihalcea and tarau
textrank can find keywords or to select sentences to form a summary
blends a sliding word window approach to computing word importances with the ranking mechanism described by mihalcea and tarau to generate its summaries
finally we observe parallels between our work and semantic search engines
a chrome extension called fuzbal ilchenk utilizes and glove embeddings to perform semantic searches of webpages
performs a similar process on a document using a customizable set of embeddings
fuzbal utilizes small pre trained models in the interests of not impacting page render speed
backend architecture underlying architecture are unsupervised pre trained text vectorization models
text vectorization is the process of converting text into an n dimensional vector
the vector properties of magnitude and direction allow for comparison between words using cosine distance
some text vectorization models are trained to predict the next word given previous context or are designed to predict the context words given a candidate word mikolov al

more recent methods utilize newer neural network architectures like the transformer and introduce and bidirectionality devlin et al
for improved contextual disambiguation
like masking techniques work in the field of text embeddings is rapidly progressing
for this reason is powered by the text embedding framework flair akbik and blythe
flair is chosen because it has a simple interface that allows a user to combine word or document embeddings together arbitrarily
the authors of flair make it a point to quickly incorporate new text embedding models meaning that s summarization capabilities will improve as the state of the art models which power it improve
many techniques summarization utilize supervised techniques but we consciously avoided them as we desired the stronger generalization performance available from unsupervised models
since unsupervised models are usually trained on massive corpuses like wikipedia or common crawl penninglon et al
they do not overfit as much to any particular topic or domain
furthermore they offer the user the ability to tune the embedding models with a domain specific confidential review copy
do not distribute
figure comparison between different lengths of word window
top is with length of middle with length of and the bottom with length of
embeddings used fasttext trained on wikipedia data with the top underlined and the top highlighted bias query economic decline causes unending war figure scaling word windows from the beginning and end of a document with a word window size of
the window for the first word we is the first words of the summary
the window for the second word stand adds the next word to the word window appends additional words to the list as it slides the window through the text eventually creating word bidirectional word windows through the middle of the text and reducing back to for the final word true unlabeled corpus rather than requiring labeled text data
algorithm overview
word windows a between similarity can generate a user specified length summary from a document by computing the cosine vectorized representation of a user inputted query and each the words corresponding word window document
this produces a scaler for each word which corresponds to the similarity of this word and context to the query
figure displays the difference in summarization as the word window is lengthened
we observe that as the word window increases the summaries tend to include longer runs of words roughly proportional to the size of the word window
in compared to sentence level summarization which assigns scalers to sentences rather than words this technique trades off a significant amount of grammatical correctness
for the purposes of the competitive debate community such tradeoffs are worth the benefits in scalable summarization
furthermore it improves the capacity for a biased summary to piece together a confidential review copy
do not distribute
figure unbiased summary top vs biased summary bottom hyperparamaters chosen fasttext trained on wikipedia data with the top underlined and the top highlighted bias query the moderns seek enjoyment of life rather than staying alive meaning that was not intended by the original documents authors
extremely useful tool for guiding the algorithm s summary creation
the scaling word window as described in figure was chosen for its perceived natural correspondence to how a human may read a text as well as its usage in the pretraining of the models that uses

bias query when executing it first asks a user to input a bias query
this can be a single word for semantic search a sentence or a tagline as debaters call it or even a whole different document
users who wish to generate an unbiased summary can enter and will set the query to be the document to be summarized
each word is vectorized and the bias vector is computed by mean pooling each word vector in the query
the differences figure highlights in summarization between unbiased and biased summaries
in theory a summary can be generated that says what a user wants it to say but in practice the nature of word embeddings makes this challenging
because word embeddings predict context the sentences i love ice cream and i hate ice cream will rank as being extremely similar to each other and a query looking for the ills of ice cream may accidentally end up including all of the information about how good ice cream is
still we observe the queryability feature to be an
summarization during the main execution loop the user enters the documents that they wish to summarize
takes input in through the system s standard input stream and a user indicates that they are done entering their document by pressing d
once the user has entered their document works as follows for each word in the source document the vectorized representation of the word window is computed
then computes the cosine similarity between the query vector and each word window vector
prompts the user to specify the percentage of the document they want underlined and highlighted ahead of time
the words with the highest similarity are selected to be included in the summary
also allows for a user to underline or re highlight a document if they want a different sized summary or a summary with different settings
that
pooling inherits all hyperparamaters available in flair s documentpoolembeddings class
any set of word vectors available through flair as well as confidential review copy
do not distribute
custom user trained models can be selected to power
for instance a user could leverage fine tuned glove fasttext and bert models to summarize a document
flair concatenates the embeddings together seamlessly
computing the vector representation of a word string is usually done by averaging each word or character vector though flair makes it possible to take the maximum or minimum of each vector
we utilize average word pooling by default
as was written originally for the competitive debate community it asks the user if more summaries are to be produced
if the user indicates that they are finished it produces a word document in a similar format as the evidence shown in figure
it also outputs this summary using standard output and sty for dynamic highlighting in a terminal
use cases this section describes current domains where is successfully used and proposes further use cases and extensions

competitive speech and debate the original idea and raison detre for this tool was to assist in the creation of evidence for users who compete in american style cross examination debate
this style of debate is characterized by its length extreme technical style its heavy reliance topic
many on evidence and competitors painstakingly spend hundreds of hours researching and preparing evidence in the format shown in figure
s name pays homage to this community
its annual this into when tool was introduced the competitive debate community it caused mostly positive reactions
many were impressed with the customizability and speed of summarization but some believe that the automation of evidence production risks incentivizing competitors to not properly read research that they might include as a core part of their debate case
the ideal use case for this tool is to quickly summarize documents which were found immediately prior to an important speech
as there is extremely limited preparation time any competitor who utilizes can have a significant advantage over their competition

com feluxe sty
semantic search some users of noted the similarity that this model has to a semantic powered find or search tool
typing a query like policies on a politician s web page may easily guide a user to their list of policies even if the webpage chooses different but semantically similar words to describe their policy page
fuzbal proved that there is a significant group of users who would find such a feature useful if it were built into a web browser
one especially interesting use case of is in information retrieval
a user mentioned that they were trying to use in tandem with embeddings trained on pubmed abstracts to process medical documents about cancer and search for experimental or novel treatments for their family
we strongly support these kind of efforts and hope that tools like can be useful to those who need to quickly parse large semantic of amounts understanding information with
future work a have sentence requested while many in the debate community are satisfied with grammatically incorrect summaries several users level summarization mode
we plan to add this feature in a future update
furthermore there are exciting possibilities related to trying to incorporate further context into the ranking mechanism such as by concatenating sparse and wide embedding models like tf idf alongside current deep embedding models or by fine tuning text vector models on debate corpuses
conclusion we presented a queryable word level extractive summarization system designed for a specific domain
this tool leverages state of art pre trained language models to generate its summaries
we explain the underlying architecture behind muse about potential use cases and serve as a call to action for more work in the field of semantic search
our system is utilized extensively within the competitive speech and debate community and is made available to the public on github
confidential review copy
do not distribute
references summarization
retrieved from
org
akbik alanand blythe d
and v
r

contextual string embeddings for sequence labeling
proceedings of the international conference on computational linguistics
retrieved from
org anthology mihalcea r
tarau p

textrank bringing order into texts
proceedings of the conference on empirical methods in natural language processing


azar m
y
sirts k
moll d

mikolov t
chen k
corrado g
dean j

distributed representations of words and phrasesand their compositionality
advances in neural information processing systems


jmlr



nallapati r
zhai f
zhou b

summarunner a recurrent neural network based sequence model for extractive summarization of documents
proceedings of the thirty first aaai conference on artificial intelligence
retrieved from
org
narayan s
cohen s
b
lapata m

ranking sentences for extractive summarization with reinforcement learning
retrieved from
org
penninglon j
socher r
manning c
d
m

glove global vectors for word representation
proceedings of the conference on empirical methods in natural language processing emnlp


nq iv

c shi j
liang c
hou l
li j
liu z
zhang h

deepchannel salience estimation by contrastive learning for extractive document summarization
retrieved from
org
yulianti e
huspi s
sanderson m

tweet biased summarization
journal of the association for information science and technology


asi
zhao y
luo z
aizawa a

a language model based evaluator for sentence compression
proceedings of the annual meeting of the association for computational linguistics volume short papers
retrieved from
org anthology based single document summarization using an ensemble noisy
proceedings of australasian language technology association workshop
barve s desai s s
r

query based extractive text summarization for sanskrit
advances in intelligent systems and computing
retrieved from
springer
com
chaudhari m
mattukoyya a
n

tone biased mmr text summarization
retrieved from
org
devlin j
chang m

lee k
toutanova k

bert pre training of deep bidirectional transformers for language understanding
mlm
retrieved from
org
filippova k
alfonseca e
colmenares c
a
kaiser l
vinyals o

sentence compression by deletion with lstms
proceedings of the conference on empirical methods in natural language processing


ilchenk a

fuzbal gives ctrl f like find results
retrieved from
com ijkilchenko fuzbal jadhav a
rajan v

extractive summarization with swap net sentences and words from alternating pointer networks
association for computational linguistics


bjr
klerke s
goldberg y
sgaard a

improving sentence compression by learning to predict gaze
proceedings of naacl hlt
retrieved from
org
lierde h
van chow t
w
s

oriented text summarization based on hypergraph transversals hadrien

liu y

fine tune bert for extractive
