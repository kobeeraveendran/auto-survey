low resource neural headline generation ottokar tilk and tanel alumae department of software science school of information technologies tallinn university of technology estonia ottokar

ee tanel

ee l u j l c
s c v
v i x r a abstract recent neural headline generation models have shown great results but are generally trained on very large datasets
we focus our efforts on improving headline quality on smaller datasets by the means of training
we propose new methods that enable pre training all the parameters of the model and utilize all available text sulting in improvements by up to
relative in perplexity and
points in rouge
introduction neural headline generation nhg is the process of automatically generating a headline based on the text of the document using articial neural works
headline generation is a subtask of text marization
while a summary may cover tiple documents generally uses similar style to the summarized document and consists of tiple sentences headline in contrast covers a gle document is often written in a different style headlinese mardh and is much shorter frequently limited to a single sentence
due to shortness and specic style ing the the document into a headline often quires the ability to paraphrase which makes this task a good t for abstractive summarization proaches where neural networks based attentive encoder decoder bahdanau et al
type of models have recently shown impressive results e

rush et al
nallapati et al

while state of the art results have been obtained by training nhg models on large datasets like gaword access to such resources is often not sible especially when it comes to low resource in this work we focus on languages
ing performance on smaller datasets with different pre training methods
one of the reasons to expect pre training to be an effective way to improve performance on small datasets is that nhg models are erally trained to generate headlines based on the documents just a few rst sentences of rush et al
shen et al
chopra et al
nallapati et al

this leaves the rest of the text unutilized which can be alleviated by pre training subsets of the model on full ments
additionally the decoder component of nhg models can be regarded as a language model lm whose predictions are biased by the external information from the encoder
as a lm it sees only headlines during training which is a small fraction of text compared to the documents
plementing the training data of the decoder with documents via pre training might enable it to learn more about words and language structure
although some of the previous work has used pre training before nallapati et al
alimoff it is not fully explored how much pre training helps and what is the optimal way to do it
another problem is that in previous work only a subset of parameters usually just dings is pre trained leaving the rest of the eters randomly initialized
the main contributions of this paper are lm pre training for fully initializing the encoder and decoder sections
and
combining lm pre training with distant supervision mintz et al
pre training using ltered sentences of the documents as noisy targets i
e
predicting one sentence given the rest to maximally utilize the entire available dataset and pre train all the paramters of the nhg model section
and analysis of the effect of pre training different ponents of the nhg model section

yt encoder attention decoder enc
emb
init
dec
emb



xn


figure a high level description of the nhg model
the model predicts the next headline word yt given the words in the document


xn and already generated headline words



method the model that we use follows the architecture scribed by bahdanau et al

although inally created for neural machine translation this architecture has been successfully used for nhg e

by shen et al
nallapati et al
and in a simplied form by chopra et al

the nhg model consists of a bidirectional schuster and paliwal encoder with gated recurrent units gru cho et al
a rectional gru decoder and an attention nism and a decoder initialization layer that connect the encoder and decoder bahdanau et al

during headline generation the encoder reads and encodes the words of the document
ized by the encoder the decoder then starts ating the headline one word at a time attending to relevant parts in the document using the attention mechanism figure
during training the eters are optimized to maximize the probabilities of reference headlines
while generally at the start of training the parameters of all the components are randomly initialized or only pre trained dings with dashed outline in figure are used nallapati et al
paulus et al
gulcehre et al
we propose pre training methods for more extensive initialization

encoder pre training when training a nhg model most approaches generally use a limited number of rst sentences or tokens of the document
for example rush et al
shen et al
chopra et al
use only the rst sentence of the document and nallapati et al
use up to rst sentences
while efcient training is faster and takes less memory as the input sequences are shorter and effective the most informative content tends to be at the beginning of the document nallapati et al
this leaves the rest of the sentences in the document unused
better understanding of words and their context can be learned if all sentences are used especially on small training sets
to utilize the entire training set we pre train the encoder on all the sentences of the training set uments
since the encoder consists of two rent components a forward and backward gru we pre train them separately
first we add a max output layer to the forward gru and train it on the sentences to predict the next word given the previous ones i
e
we train it as a lm
after convergence on the validation set sentences we take the embedding weights of the forward gru and use them as xed parameters for the backward gru
then we train the backwards gru ing the same procedure as with the forward gru with the exception of processing the sentences in a reverse order
when both models are fully trained we remove the softmax output layers and ize the encoder of the nhg model with the beddings and gru parameters of the trained lms highlighted with gray background in figure

decoder pre training pre training the decoder as a lm seems natural since it is essentially a conditional lm
during nhg model training the decoder is fed only line words which is relatively little data compared to the document contents
to improve the quality of the headlines it is essential to have high ity embeddings that are a good semantic sentation of the input words and to have a well trained recurrent and output layer to predict ble words that make up coherent sentences
when it comes to statistical models the simplest way to improve the quality of the parameters is to train the model on more data but it also has to be the right kind of data moore and lewis
to increase the amount of suitable training data for the decoder we use lm pre training on tered sentences of the training set documents
for ltering we use the xenc tool by rousseau with the cross entropy difference ltering moore and lewis
in our case the domain data is training set headlines out domain data is the sentences from training set documents and the best cut off point is evaluated on validation set headlines
the careful selection of sentences is mostly motivated by preventing the pre trained coder from deviating too much from headlinese but it also reduces training time
before pre training we initialize the input and output embeddings of the lm for words that are common in both encoder and decoder vocabulary with the corresponding pre trained encoder beddings
we train the lm on the selected tences until perplexity on the validation set lines stops improving and then use it to initialize the decoder parameters of the nhg model lighted with dotted background in figure
a similar approach without data selection and embedding initialization has also been used by alimoff

distant supervision pre training approaches described in sections
and
able full pre training of the encoder and decoder but leaves the connecting parameters with white background in figure untrained
this still as results in language modelling suggest surrounding sentences contain useful tion to predict words in the current sentence wang and cho
this implies that other sentences contain informative sections that the tention mechanism can learn to attend to and eral context that the initialization component can learn to extract
to utilize this phenomenon we propose using carefully picked sentences from the documents as pseudo headlines and pre train the nhg model to generate these given the rest of sentences in the document
our pseudo headline picking strategy consists of choosing sentences that occur within rst tokens of the document and were retained during cross entropy ltering in section

ing sentences from the beginning of the document should give us the most informative sentences and cross entropy ltering keeps sentences that most closely resemble headlines
the pre training procedure starts with ing the encoder and decoder with lm pre trained parameters sections
and

after that we continue training the attention and initialization parameters until perplexity on validation set lines converges
we then use the trained ters to initialize all parameters of the nhg model
has been also used by supervision multi document summarization distant for no pre training embeddings encoder decoder enc
dec
distant all enc
dec
dist
y t i e l p r e p epoch figure validation set en perplexities of the nhg model with different pre training methods
model no pre training embeddings encoder
decoder
enc
dec
distant all enc
dec
dist

ppl en













ppl et













table perplexities on the test set with a klakow and peters
condence interval all pre trained models are signicantly better than the no pre training baseline
bravo marquez and manriquez
experiments we evaluate the proposed pre training methods in terms of rouge and perplexity on two relatively small datasets english and estonian

training details all our models use hidden layer sizes of and the weights are initialized according to glorot and bengio
the vocabularies sist of up to most frequent training set words that occur at least times
the model is implemented in theano bergstra et al
bastien et al
and trained on gpus using mini batches of size
during training the weights are updated with adam kingma and ba parameters


and and norm of is kept within a threshold of
the gradient en et model no pre training embeddings encoder
decoder
enc
dec
distant all enc
dec
dist















rlr rlp



























rlr rlp













table recall and precision of and rouge l on the test sets
best scores in bold
results with statistically signicant differences condence compared to no pre training underlined
pascanu et al

during headline generation we use beam search with beam size

datasets the use cnn daily mail for dataset we hermann et al
experiments on english en
the number of headline document pairs is and in training validation and test set correspondingly
the processing consists of tokenization lowercasing replacing numeric characters with and ing irrelevant parts editor notes timestamps
from the beginning of the document with heuristic rules
for estonian et experiments we use a ilarly sized and training validation and test split dataset that also consist of news from two sources
during ing compound words are split words are cased and numbers are written out as words
we used estnltk orasmaa et al
stemmer for rouge evaluations

results and analysis models are evaluated in terms of perplexity ppl and full length rouge lin
in tion to pre training methods described in sections

we also test initializing only the dings using parameters from the lm pre trained encoder and decoder embeddings initializing the encoder and decoder but leaving connecting parameters randomized enc
dec
pre training the whole model from random initialization with distant supervision only distant all and a line that is not pre trained at all no pre training
all pre training methods gave signicant provements in ppl table
the best method
nyu
enc
dec
dist
improved the test set ppl by

relative
pre trained nhg models also converged faster during training figure and most of them beat the nal ppl of the baseline already after the rst epoch
general trend is that pre training a larger amount of parameters and the parameters closer to the outputs of the nhg model improves the ppl more
distant all is an tion to that observation as it used much less ing data same as baseline than other methods
for rouge evaluations we report and rouge l table
in contrast with ppl evaluations some pre training methods ther do nt improve signicantly or even worsen rouge measures
another difference pared to ppl evaluations is that for rouge training parameters that reside further from puts embeddings and encoder seems more ecial
this might imply that a better ment representation is more important to stay on topic during beam search while it is less tant during ppl evaluation where predicting next target headline word with high condence is warded and the process is aided by previous get headline words that are fed to the decoder as inputs
it is also possible that a well trained decoder becomes too reliant on expecting rect words as inputs making it sensitive to errors during generation which would somewhat explain why enc
dec
performs worse than encoder alone
this hypothesis can be checked in further work by experimenting with methods like uled sampling bengio et al
that should crease the robustness to mistakes during tion
pre training all parameters on all available text enc
dec
dist
still gives the best result on english and quite decent results on estonian
best models improve rouge by

points
document reference headline no pre training embeddings encoder
decoder
enc
dec
distant all enc
dec
dist

document reference headline no pre training embeddings encoder
decoder
enc
dec
distant all enc
dec
dist

a democratic congressman is at the head of a group of representatives trying to help undocumented immigrants avoid deportations with what they have called the family defender toolkit
the informational pamphlet includes a bilingual card that some are calling a get out of deportation free card that lists reasons a person should not be deported under expanded
congressman is developing a get out of deportation toolkit to help mented immigrants if they are detained congressman calls for undocumented immigrants congressman calls for help from immigrants trying to help immigrants ing deportation republican congressman calls for immigrants trying to avoid deportation congressman who tried to stop deportations of immigrants immigration congressman at the head of the head of the group who tries to avoid deportation congressman calls for deportation to immigrants who stay in the country congressman tries to help undocumented immigrants avoid deportation a chihuahua and a bearded dragon showed off their interspecies friendship when they embarked upon a game of tag together
videoed in their front room the dog named foxxy cleopatra and the reptile called ryuu can be seen chasing after one another around a coffee table
standing perfectly still while looking in the other direction the bearded dragon initially appears ested as the chihuahua jumps around excitedly
you re it is this the creepiest crawly meet the poodle it s a knockout the bearded dragon lizard the bearded dragon lizard spotted in the middle of the street oh this is a lion meet the dragon dragon meet the dragon dragon is this the world s youngest lion table examples of generated headlines on cnn daily mail dataset
some examples of the generated headlines on the cnn daily mail dataset are shown in table
conclusions we proposed three new nhg model pre training methods that in combination enable utilizing the entire dataset and initializing all parameters of the nhg model
we also evaluated and analyzed training methods and their combinations in terms of perplexity ppl and rouge
the results vealed that better ppl does nt necessarily late to better rouge ppl tends to benet from pre training parameters that are closer to outputs but for rouge it is generally the opposite
also ppl beneted from pre training more parameters while for rouge it was not always the case
training in general proved to be useful our best results improved ppl by

relative and rouge measures by

points compared to a nhg model without pre training
current work focused on maximally ing available headlined corpora
one ing future direction would be to additionally lize potentially much more abundant corpora of documents without headlines also proposed by shen et al
for pre training
another open question is the relationship between the dataset size and the effect of pre training
acknowledgments we would like to thank nvidia for the donated gpu the anonymous reviewers for their valuable comments and kyunghyun cho for the help with the cnn daily mail dataset
references alex alimoff

abstractive sentence rization with attentive deep recurrent neural works
dzmitry bahdanau kyunghyun cho and yoshua neural machine translation by bengio

jointly learning to align and translate


bastien pascal lamblin razvan pascanu james bergstra ian j
goodfellow arnaud eron nicolas bouchard and yoshua bengio

theano new features and speed improvements
in deep learning and unsupervised feature learning nips workshop
samy bengio oriol vinyals navdeep jaitly and noam shazeer

scheduled sampling for quence prediction with recurrent neural networks
in advances in neural information processing tems pages
james bergstra olivier breuleux bastien pascal lamblin razvan pascanu guillaume jardins joseph turian david warde farley and a cpu and yoshua bengio

in proceedings gpu math expression compiler
of the python for scientic computing conference scipy
oral presentation
theano felipe bravo marquez and manuel manriquez

a zipf like distant supervision approach for document summarization using wikinews articles
in international symposium on string processing and information retrieval pages
springer

association for computational tics
karl moritz hermann tomas kocisky ward grefenstette lasse espeholt will kay mustafa suleyman and phil blunsom

teaching machines to read and comprehend
in vances in neural information processing systems nips
diederik kingma and jimmy ba

adam a method for stochastic optimization
arxiv preprint

dietrich klakow and jochen peters

testing the correlation of word error rate and perplexity
speech communication
chin yew
text summarization branches out chapter rouge a package for automatic evaluation of summaries
lin
ingrid mardh

headlinese on the mar of english front page headlines volume
liberlaromedel gleerup
mike mintz and steven bills jurafsky
rion
daniel snow distant supervision for relation extraction without labeled data
in proceedings of the joint conference of the annual meeting of the acl and the national joint conference on natural language processing of the afnlp pages
association for computational linguistics
kyunghyun cho bart van merrienboer caglar gulcehre dzmitry bahdanau fethi bougares holger schwenk and yoshua bengio

learning phrase representations using rnn encoder decoder for statistical machine translation
in proceedings of the conference on ical methods in natural language processing emnlp pages
association for computational linguistics
c
robert moore and william lewis

intelligent selection of language model training data
in proceedings of the acl conference short papers pages
association for tional linguistics
ramesh nallapati bowen zhou cicero dos tos caglar gulcehre and bing xiang

abstractive text summarization using sequence to sequence rnns and beyond
in proceedings of the signll conference on computational natural language learning pages
association for computational linguistics
m
chopra michael alexander auli
sumit rush
and abstractive sentence summarization with attentive recurrent neural networks
in proceedings of the conference of the north american chapter of the association for tional linguistics human language technologies pages
association for computational linguistics
siim orasmaa xavier glorot and yoshua bengio

ing the difculty of training deep feedforward neural networks
in international conference on articial intelligence and statistics pages
caglar gulcehre sungjin ahn ramesh ati bowen zhou and yoshua bengio

pointing the unknown words
in proceedings of the annual meeting of the association for tational linguistics volume long papers pages timo petmanson alexander tkachenko sven laur and heiki jaan kaalep
in
estnltk nlp toolkit for estonian
the tenth international conference ceedings of on language resources and evaluation lrec paris france
european language resources association elra
razvan pascanu tomas mikolov and yoshua bengio

on the difculty of training recurrent ral networks
proceedings of the international conference on machine learning icml
romain paulus caiming xiong and richard socher

a deep reinforced model for abstractive marization
arxiv preprint

anthony rousseau

xenc an open source tool for data selection in natural language processing
the prague bulletin of mathematical linguistics
m
alexander rush sumit jason weston
and a neural attention model for abstractive sentence summarization
in proceedings of the conference on ical methods in natural language processing pages
association for computational linguistics
chopra
mike schuster and kuldip k paliwal

tional recurrent neural networks
ieee transactions on signal processing
shiqi shen yu zhao zhiyuan liu maosong neural headline generation arxiv preprint sun et al

with sentence wise optimization


tian wang and kyunghyun cho

larger context language modelling with recurrent neural network
in proceedings of the annual meeting of the association for computational linguistics volume long papers pages
association for computational linguistics

