privacy preserving multi document summarization lus jos wang david martins de joo p
anatole jaime isabel bhiksha language technologies institute carnegie mellon university pittsburgh pa usa instituto superior tcnico universidade de lisboa lisbon portugal inesc id lisbon portugal luis
marujo jose
portelo wlin david
matos joao
neto isabel
id
anatoleg jgc
cmu
edu g u a r i
s c v
v i x r a abstract state of the art extractive multi document summarization systems are usually designed without any concern about vacy issues meaning that all documents are open to third parties
in this paper we propose a privacy preserving proach to multi document summarization
our approach enables other parties to obtain summaries without learning anything else about the original documents content
we use a hashing scheme known as secure binary dings to convert documents representation containing key phrases and bag of words into bit strings allowing the putation of approximate distances instead of exact ones
our experiments indicate that our system yields similar sults to its non private counterpart on standard multi ment evaluation datasets
categories and subject descriptors h
information storage and retrieval i

natural language processing text analysis k

computers and society public policy issues privacy general terms algorithms experimentation keywords secure summarization multi document summarization terfall kp centrality secure binary embeddings data vacy
introduction extractive multi document summarization ems is the problem of extracting the most important sentences in a set of documents
state of the art solutions for ems based on waterfall kp centrality achieve excellent results
a limitation to the usage of such methods is their tion that the input texts are of public domain
however copyright is held by the author
preserving ir when information retrieval meets privacy and security sigir workshop august santiago chile problems arise when these documents can not be made lic
consider the scenario where a company has millions of classied documents organized into several topics
the pany may need to obtain a summary from each topic but it lacks the computational power or know how to do so
at the same time they can not share those documents with a third party with such capabilities as they may contain sensitive information
as a result the company must obfuscate their own data before sending it to the third party a requirement that is seemingly at odds with the objective of extracting summaries from it
in this paper we propose a new privacy preserving nique for ems based on secure binary embeddings sbe that enables exactly this it provides a mechanism for obfuscating not only named entities but the complete data while still achieving near state of art performance in ems
sbe is a kind of locality sensitive hashing algorithm which converts data arrays such as bag of words vectors to obfuscated bit strings through a combination of random jections followed by banded quantization
the method has information theoretic guarantees of security ensuring that the original data can not be recovered from the bit strings
they also provide a mechanism for computing distances between vectors that are close to one another without ing the global geometry of the data such as the number of features consequently enabling tasks such as ems
this is achievable because unlike other hashing methods which quire exact matches for performing retrieval or classication tasks sbe allows for a near exact matching the hashes can be used to estimate the distances between vectors that are very close but provably provide no information whatsoever about the distance between vectors that are farther apart
the usefulness of sbe has already been shown in preserving important passage retrieval and speaker ication systems yielding promising results

related work
multi document summarization most of the current work in automatic summarization cuses on extractive summarization
popular baselines for multi document summarization fall into one of the ing general models centrality based maximal marginal relevance mmr and coverage based methods
additionally methods such as kp centrality which is centrality and coverage based follow more than one paradigm
in general centrality based models are used to produce generic summaries while the mmr ily generates query oriented ones
coverage based models produce summaries driven by words topics or events
we use the waterfall kp centrality method because it is a state of the art ems method but the ideas in this work could be applied to any other ems methods

privacy preserving methods in this work we focus on creating a method for ing ems while keeping the original documents private
to the best of our knowledge the combination of research lines has only been explored for the single document tion case
however there are some additional recent works combining information retrieval and privacy
most of these works use data encryption to transfer the data in a secure way
the problem with these methods is that the entity responsible for producing the summaries will have cess to the documents content while our method guarantees that no party aside from the owner of the documents will have access to their content
another secure information retrieval methodology is to obfuscate queries which hides user topical intention but does not secure the content of the documents
in many areas the interest in privacy preserving ods where two or more parties are involved and they wish to jointly perform a given operation without disclosing their private information is not new and several techniques such as garbled circuits gc homomorphic encryption he and locality sensitive hashing lsh have been introduced
however they all have limitations regarding the ems task we wish to address
until recently gc methods were tremely inecient and dicult to adapt specially when the computation of non linear operations such as the cosine tance is required
systems based on he techniques usually require extremely long amounts of time to evaluate any tion of interest
the lsh technique allows for near exact match detection between data points but does not provide any actual notion of distance leading to degradation of formance in some applications
as a result we decided to consider sbe as the data privacy for our approach as it does not show any of the disadvantages mentioned above for the task at hand

multi document summarization to determine the most representative sentences of a set of documents we used a multi document approach based on kp centrality
this method is adaptable and robust in the presence of noisy input
this is an important aspect since using several documents as input frequently increases the amount of unimportant content
waterfall kp centrality iteratively combines the ries of each document that was generated using kp ty following a cascade process it starts by merging the mediate summaries of the rst two documents according to their chronological order
this merged intermediate mary is then summarized and merged with the summary of following document
we iterate this process through all documents until the most recent one
the summarization method uses as input a set of key phrases that we extract from each input document joins the extracted sets and ranks the key phrases using their frequency
to generate each intermediate summary we use the top key phrases cluding the ones that do not occur in the input document
kp centrality extracts a set of key phrases using a


pervised approach and combines them with a bag of words model in a compact matrix representation given by


pn


km





pn


km where w is a function of the number of occurrences of each term t in every passage p or key phrase k t is the number of terms n is the number of sentences and m is the number of key phrases
then using i k


pn


km


qn m a support set si is computed for each passage pi using si s i k qi i s for i


n m
passages are ranked excluding the set of key phrases articial passages according to arg max
a support set is a group of the most semantically related passages
these semantic passages are selected using tics based on the passage order method
the metric that is normally used is the cosine distance

secure binary embeddings an sbe is a scheme for converting vectors to bit sequences using quantized random projections
it produces a lsh method with an interesting property if the euclidean tance between two vectors is lower than a certain threshold then the hamming distance between their hashes is tional to the euclidean distance otherwise no information can be infered
this scheme is based on the concept of versal quantization which redenes scalar tization by forcing the quantization function to have contiguous quantization regions
that is the quantization process converts an l dimensional vector rl into an m binary sequence where the m th bit is dened by q wm m here represents a dot product
am rl is a surement vector comprising l i
i

samples drawn from n m is a precision parameter and wm is dom number drawn from a uniform distribution over m
q is a quantization function given by
we can represent the complete quantization into m bits pactly in vector form q here is an m binary vector which we will refer to as the hash of x a rm l is a matrix of random elements drawn from n is a diagonal matrix with tries m and w rm is a vector of random elements drawn from a uniform distribution over m
the universal bit quantizer of equation maps the real line onto in a banded manner where each band is m wide
figure pares conventional scalar bit quantization left panel with the equivalent universal bit quantization right panel
the binary hash vector generated by the universal tizer of equation has an interesting property the ming distance between the hashes of figure bit quantization functions
two vectors and y is correlated to the euclidean distance between the two vectors if the euclidean distance between the two vectors is less than a threshold which pends on m
however if the distance between and y is greater than this threshold yields no information about the true distance between the vectors
in order to illustrate how this scheme works we randomly generated samples in a high dimensional space l and plotted the normalized hamming distance between their hashes against the euclidean distance between the tive samples
this is presented in figure
the number of bits in the hash is also shown in the gures
figure embedding behaviour for dierent values of and dierent amounts of measurements m
we note that in all cases once the normalized distance exceeds the hamming distance between the hashes of two vectors ceases to provide any information about the true distance between the vectors
we will nd this properly useful in developing our privacy preserving mds system
we also see that changing the value of the precision rameter allows us to adjust the distance threshold until which the hamming distance is informative
also increasing the number of bits m leads to a reduction of the variance of the hamming distance
yet another interesting property conjectured for the sbe is that recovering from is n p even given a

secure multi document summarization our methodology consists in iteratively running the secure single document summarization method which prises four stages
in the rst stage we obtain a tation of each document which is the rst step of the centrality method
in the second stage we compute sbe hashes using the document representation
the third stage ranks the passages which corresponds to the second step of the kp centrality method
because we are now ing with sbe hashes instead of the original document resentation this is performed using the hamming distance instead of the cosine distance
finally the last stage is to use the ranks of sentences to obtain the summary
our approach for a privacy preserving multi document summarization system closely follows the formulation sented in section
however there is a very important dierence in terms of who performs each of the steps of the single document summarization method
typically the only party involved alice who owns the original documents forms key phrase extraction combines them with the bag words model in a compact matrix representation computes the support sets for each document and nally uses them to retrieve the summaries
in our scenario alice does not know how to extract the important passages from the document collection does not possess the computational power to do so
therefore she must outsource the summarization process to a another entity bob who has these ties
however alice must rst obfuscate the information contained in the compact matrix representation
if bob ceives this information as is he could use the term cies to infer on the contents of the original documents and gain access to private or classied information alice does not wish to disclose
alice computes binary hashes of her pact matrix representation using the method described in section keeping the randomization parameters a and w to herself
she sends these hashes to bob who computes the support sets and extracts the important passages
because bob receives binary hashes instead of the original matrix representation he must use the normalized hamming tance instead of the cosine distance in this step since it is the metric the sbe hashes best relate to
finally he returns the hashes corresponding to the important passages to ice who then uses them to get the information she desires
these steps are repeated as many times as needed until the multi document summarization process is complete

experiments and results in this section we illustrate the performance of our preserving approach to ems and how it compares to its private counterpart
we start by presenting the datasets we used in our experiments then we describe the experimental setup and nally we present some results
to assess the quality of the summaries generated by our methods we used on duc and tac datasets
duc dataset includes clusters of newswire documents and human created word erence summaries
tac has topic clusters
each topic has sets of news documents
there are created word reference summaries for each set
the erence summaries for the rst set are query oriented and for the second set are update summaries
in this work we used the rst set of reference summaries
we evaluated the dierent models by generating summaries with words
we present some baseline experiments in order to obtain reference values for our approach
we generated words summaries for both tac and duc datasets
for both experiments we used the cosine and the euclidean tance as evaluation metrics since the rst is the usual metric for computing textual similarity but the second is the one that relates to the secured binary embeddings technique
all results are presented in terms of rouge in ular which is the most widely used evaluation measure for this scenario
the results we obtained for the tac and the duc are presented in table
we considered key phrases in our experiments since it nlpir
nist
gov projects duc tasks
html
nist
gov metric cosine distance euclidean distance tac duc



table reference waterfall kp centrality results with key phrases in terms of
leakage














table waterfall kp centrality using sbe and the duc corpus in terms of
leakage














table waterfall kp centrality using sbe and the tac corpus in terms of
is the usual choice when news articles are considered
as expected we notice some slight degradation when the euclidean distance is considered but we still achieve better results than other state of the art methods such as mead mmr expect n and lexrank
reported results in the literature include
and
using mead
and
using mmr
and
using expect for the duc and tac datasets tively
this means that the forced change of metric due to the intrinsic properties of sbe and the multiple tion of sbe does not aect the validity of our approach in any way
for our privacy preserving approach we performed iments using dierent values for the sbe parameters
the results we obtained in terms of rouge for the duc and the tac datasets are presented in tables and leakage denotes the percentage of sbe respectively
hashes that the normalized hamming distance dh is portional to the euclidean distance de between the original data vectors
the amount of leakage is controlled by
bits per coecient bpc is the ratio between the number of measurements m and the dimensionality of the original data vectors l i
e
bpc m l
unsurprisingly increasing the amount of leakage i
e
increasing leads to improving the summarization results
however changing bpc does not lead to improved performance
the reason for this might be due to the waterfall kp centrality method using port sets that consider multiple partial representations of all documents
even so the most signicant results is that for leakage there is an almost negligible loss of mance
this scenario however does not violate our privacy requisites in any way since although most of the distances between hashes are known it is not possible to use this mation to obtain anything about the original information

conclusions and future work in this work we introduced a privacy preserving technique for performing extractive multi document summarization that has similar performance to their non private part
our secure binary embeddings based approach vides secure multiple documents representations that allows for sensitive information to be processed by third parties without any risk of sensitive information disclosure
we also found it rather interesting to observe such a small tion on the results given that we needed to compute sbe hashes on each iteration of our algorithm
future work will explore the possibility of having multiple rather than a single entity supplying all the documents

acknowledgments we would like to thank fct for supporting this research through grants uid ptdc eia cmup epb and cmu portugal

references p
boufounos
universal rate ecient scalar quantization
ieee toit
p
boufounos and s
rane
secure binary embeddings for privacy preserving nearest neighb
in wifs
j
carbonell and j
goldstein
the use of mmr diversity based reranking for reordering documents and producing summaries
in sigir
g
erkan and d
r
radev
lexrank graph based centrality as salience in text summariz
jair
s
guo and s
sanner
probabilistic latent maximal marginal relevance
in acm sigir
w
jiang l
si and j
li
protecting source privacy in federated search
in acm sigir
k
w
lim s
sanner and s
guo
on the math
relationship between expected n and the relevance vs
diversity trade o
in sigir
c

lin
rouge a package for automatic evaluation of summaries
in acl workshop
c

lin and e
hovy
the automated acquisition of topic signatures for text summ
in coling
l
marujo j
portelo d
m
matos j
p
neto a
gershman j
carbonell i
trancoso and b
raj
privacy preserving important passage retrieval
in acm sigir pir workshop
l
marujo r
ribeiro d
m
matos j
p
neto a
gershman and j
carbonell
extending a single document summarizer to multi document a hierarchical approach
in proc
of sem
m
murugesan w
jiang c
clifton l
si and j
vaidya
ecient privacy preserving similar document detection
vldb journal
h
pang x
xiao and j
shen
obfuscating the topical intention in enterp
text search
in icde
j
portelo b
raj p
boufounos i
trancoso and a
alberto
speaker verication using secure binary embeddings
in eusipo
d
r
radev h
jing m
stys and d
tam
centroid based summarization of multiple documents
inform
processing and management
r
ribeiro l
marujo d
m
matos j
p
neto a
gershman and j
carbonell
self reinforcement for important passage retrieval
in sigir
s
zhang h
yang and l
singh
increased information leakage from text
in sigir pir

