abstractive summarization using attentive neural techniques jacob krantz gonzaga university spokane wa
gonzaga
edu jugal kalita university of colorado colorado springs colorado springs co
edu t c o l c
s c v
v i x r a abstract in a world of proliferating data the ity to rapidly summarize text is ing in importance
automatic tion of text can be thought of as a quence to sequence problem
another area of natural language processing that solves a sequence to sequence problem is chine translation which is rapidly ing due to the development of based encoder decoder networks
this work applies these modern techniques to abstractive summarization
we perform analysis on various attention mechanisms for summarization with the goal of oping an approach and architecture aimed at improving the state of the art
in ticular we modify and optimize a lation model with self attention for ating abstractive sentence summaries
the effectiveness of this base model along with attention variants is compared and lyzed in the context of standardized uation sets and test metrics
however we show that these metrics are limited in their ability to effectively score abstractive summaries and propose a new approach based on the intuition that an abstractive model requires an abstractive evaluation
introduction the goal of summarization is to take a textual document and distill it into a more concise form while preserving the most important information and meaning
to this end two approaches have historically been taken extractive and abstractive
extractive summarization selects the most tant words of a given document and combines and rearranges them to form a nal summarization nallapati et al

this approach is restricted to using words directly from the source document and so is unable to paraphrase
abstractive rithms generate a summary from an attempt to derstand a document s meaning allowing for phrasing much like a human may do
abstractive approaches are more difcult to develop than tractive ones because an intermediate tion of knowledge is required
as such dominant techniques of summarization have been extractive in nature with wide ranging solutions utilizing statistical topic based graph based and machine learning approaches gambhir and gupta
with the potential for generating more coherent and insightful summaries abstractive approaches are gaining in popularity fueled by novel deep learning techniques see et al

the stractive summarization process includes ing words to their respective embeddings puting a document representation and ing output words
neural networks have recently been shown to perform well for every step dong
in deep learning models attention allows a coder to focus on different segments of an input while stepping through output regions
in the lated sequence to sequence task of machine lation attention was introduced to the existing encoder decoder model bahdanau et al

this resulted in large improvements over past tems due to the ability to consider a larger window of context during the output generation
ing this further vaswani et al
showed that multi headed self attention can replace recurrence and convolutions entirely
as the areas of machine translation and abstractive summarization are lated both structurally and semantically the opments in machine translation may inform the rection of research in abstractive summarization
in this paper we apply these advancements and develop them further in pursuit of sentence marization
in any attempt at summarization the resulting text must be much more condensed than the original
in this task all generated summaries are constrained to a xed maximum length so that tested models must learn how to decide what formation should be reproduced
related work successful sentence summarization approaches have classically used statistical methods
iary zajic et al
detected salient ics that guided sentence compression while ing linguistic transformations
moses a tical machine translation system also performed well when directly used for summarization koehn et al

attention mechanisms have been shown to improve the results of abstractive marization
rush et al
improved over sic statistical results by using a neural language model with a minimal contextual attention coder
after the primary model training an tractive tuning step was performed on an cent dataset
a related extension of this used a convolutional attentive encoder and experimented with replacing the decoder language model with rnn variants
lstm cells and rnn elman both showed improved rouge scores chopra et al

an attentive encoder decoder was also ployed by zeng et al
with one rnn chitecture to re weight another to improve context across the input sequence
their decoder used tention with a copy mechanism that differentiated between out of vocabulary words based on their usage in the input
nallapati et al
tinued progress on encoder decoder architectures by employing a bidirectional gru rnn encoder with a unidirectional gru rnn decoder
posing dynamic vocabulary restrictions also proved results while reducing the dimensionality of the softmax output layer
pointer generator works encode with a bidirectional lstm and code with attention restriction
a coverage tor that limits the attention of words previously tended over is maintained see et al

recently summarization has made progress at the paragraph level due to reinforcement learning
a recurrent abstractive summarization model used teacher forcing and a similarity metric that pared the generated summary with the target mary paulus et al

the architecture figure transformer based network ture
the multi headed attention mechanisms tain various recall options similar to and that pand upon vaswani et al

tained a bi directional lstm with intra attention
actor critic reinforcement learning was used by li et al
to produce the highest scores for sentence summarization
one important eration when optimizing purely on the test ric is that while overall recall is improved higher rouge scores do not necessarily correlate with the readability of summaries
models encoder decoder architectures provide an able structure for the development of systems that solve sequence to sequence problems
the coder maps the input sequence to a latent vector representation
the decoder takes this tion called the context vector and generates the output sequence
the models and their variants that follow are structured as such
we select a base architecture that provides a strong foundation on which to analyze the effect of self attention ants

the transformer the transformer architecture as proposed by vaswani et al
is notable for performing state of the art machine translation and is more efcient to train than past systems by orders of magnitude
this is made possible by replacing sequence aligned recurrence with parallel attention
the sequence order is preserved in the self attention modules by including positional beddings
instead of incremental values the tional embeddings are determined by position on a sinusoidal time series curve
further masking of the decoder self attention is performed ing the output of the next token dependent on that which has already been generated
multi headed self attention is used in both the encoder and coder
these mechanisms map a query vector to a key value vector pair which results in an output vector
tying together the encoder and decoder is a third multi headed attention mechanism
the query comes from the self attentive output of the decoder and the keys and values from the attentive output of the encoder
in the work done by vaswani et al
all attention heads used scaled dot product attention which is tionally efcient as multiple query key and value vectors can be implemented as a combined matrix
scaled dot product attention also denes the ture for the self attention mechanisms we present below
attention sof tmax qkt v many other attention mechanisms exist beyond the base dot product attention
we analyze the performance of these mechanisms in the context of abstractive summarization
changing the way the query key and value vectors interact allows an attention mechanism to learn different ships between sequence elements
relative dot product attention uses scaled product attention but instead of using absolute sitional encodings uses a relative positional coding
these relative encodings learn to relate the elements of the query to both the elements of the keys and values gehring et al

the codings can be distance limited to a context dow in the vector sequences
local attention divides the key value vectors into localized blocks liu et al

each query is strided over a corresponding block with a given lter size
blocks can contain positions both prior to and following a given position thereby not masking any element based on absolute position
self attention is performed over each block in lation
local masked attention adds a mask to the blocks of local attention
blocks in a future quential position are masked from the query but all elements within a block remain visible to a given intuitively masking future query position
tions forces a mechanism to attend to current and past positions which may be an important tion of the attention distribution
local block masked attention masks both ous blocks and future blocks for a query position
further future positions within individual blocks are masked
dilated attention also divides the key value tors into blocks but introduces a gap in between each block
each query position is limited to a context window of a specied number of blocks both preceding and following the memory tion
dilated masked attention performs the same operations as dilated attention and masks future memory positions within each block
evaluation the standard test metric for automatic summary generation is rouge or recall oriented study for gisting evaluation lin
fore the rouge metrics were introduced man judges were used for summary evaluation
human judges provide an ideal evaluation but are impractical for regular use
rouge allows for automatic comparison of generated summaries to target summaries where target summaries are human generated
limited length recall is monly reported using and rouge l
and compare unigram and bigram overlap respectively
this generalizes to rouge n for n gram overlap
rouge l determines the longest common quence lcs
evaluation quality of tion models can be directly compared to ous work because the same metrics were reported for past models by rush et al
zeng et al
nallapati et al
li et al
and others
these metrics allow for reasonably rate comparison of summary generation models but inherent problems exist
one critical limitation is that rouge does not consider equivalent phrasing or synonymous concepts
since rouge works at the word level meaning can only be tured and compared in a binary manner either a word appears in the generated summary or it does not
rouge
was proposed to alleviate this lem as well as remove the expectation that erated summaries need to be identical to the get summary ganesan
as pointed out by rush et al
even the best human tor scored just
on the target endeavour astronauts join two segments of international space station
endeavour astronauts join two sections of international space station
endeavour astronauts remove two segments of international space station
endeavour astronauts join two segments of international space station
sentence rouge l cos sim wmd vert

















table highlighted differences between rouge and vert scoring
notice that an incorrect word placement scores the same as a reasonable word replacement in rouge
vert discounts the score of accordingly
is included to show the perfect scores for an identical summary
dataset
this illustrates the idea that two maries do not need to be the same in order for both to be of high quality
thus a more ate approach to summary comparison may be to evaluate the semantic similarity between the erated and target summaries instead of using lated word counts
rouge
captures tic similarity using a synonym dictionary while still evaluating n grams and lcs
while this dresses the word level shortcoming of the nal rouge metrics similarity is still xed to a discrete list of acceptable alternatives which does not fully capture phrase substitution
a further provement could be to evaluate the semantic larity between two entities on a continuous scale

vert metric to improve the quality of summary evaluation we introduce the vert an evaluation tool that scores the quality of a generated hypothesis summary as compared to a reference target mary
vert stands for versatile evaluation of duced texts
vert compares summaries on their underlying semantics rather than word count tios
to calculate a vert score for a summary pair a similarity sub score and dissimilarity score are calculated and functionally combined
naturally a higher similarity score and a lower dissimilarity score leads to a higher better vert score
the similarity sub score considers the mantics of each summary taken at the document level
a sentence embedding vector is sized for both generated and target summaries and the cosine similarity between these two vectors vert implementation is made publicly
com able at vertmetric provides the similarity score
the sentence beddings are generated using infersent an source neural encoder trained on natural language inference tasks conneau et al

infersent was chosen because it has been shown to ize well for use in various problems requiring tence representations
the dissimilarity sub score operates at the individual word level rather than at the sentence level
an aggregate euclidean tance is calculated between the words of the erated summary and the words of the target mary
this is done using the word mover s tance wmd algorithm a measure of how far document a must travel to match document b within a word vector space kusner et al

stop words are discarded prior to the distance culation as their effect on the distance between documents is negligible

sub score motivations a consideration would be to use just one of the two sub scores as they are independent tions
however both the infersent cosine ity and wmd are made more robust by the ence of the other score
wmd is unaffected by word ordering whereas the encoder of infersent maintains sequential input
to illustrate suppose the target sentence is go right and then left and the generated sentence switches the order ing go left and then right
wmd gives this a perfect distance of
but the infersent ity more accurately discounts the score by

on the other hand when longer summaries are compared infersent embeddings begin to lose the effect of individual words because the word beddings are replaced with a singular embedding
this is less of a problem for wmd
finally the similarity sub score uses glove trained on common crawl while the dissimilarity sub score uses trained on the google news dataset
using different word embeddings provides resistance to potential learned tation biases

formula specication is dened similarity as sub score the and the dissimilarity sub score is dened as
the maximum dissimilarity value is the default distance when all of the generated words are out of vocabulary
without this default maries with no words to compare would have an innite distance and too strongly inuence vert score averages
resulting sub score values range as such
r
and
r
we seek to combine these scores such that the nal vert score can be treated as a percentage
v ert r

further and should be given equal weight in the nal vert score
to satisfy both criteria we present the vert equation v ert where

the dissimilarity is normalized by and the outer linearity as multiplied by shifts the range from

to


for the choice of we observe an empirical distance ceiling of
in table
incorporating this ing gives both sub scores equal precedence while removing the necessity of a nonlinearity such as normalization by the hyperbolic tangent

hyperparameters and baseline the similarity sub score uses a pre trained fersent encoder for reproducibility and thus needs no hyperparameter adjustments
the dissimilarity requires just the hyperparameter to specify the maximum threshold of wmd and can stay at the default value of

with the same value used to normalize the dissimilarity vert is ward to use with just this single hyperparameter

stanford
edu wmd summary count table wmd among human summaries on
for each article every human mary was held out as the target to compare the other human summaries to resulting in parisons
metric rouge l vert pearson p value







table pearson correlation coefcient between automatic metrics and human evaluation of sponsiveness
to provide a scoring reference we test each man summary of on vert using the same holdout process as done in table
the erage similarity sub score is
the average dissimilarity sub score is
and combined the average vert score is


comparison to human evaluation to evaluate the effectiveness of vert we culate the correlation between vert scores and scores given by human judges for generated tence summaries
using the relative dot product attention model summaries are generated on the dataset and evaluated with the vert metric by averaging the vert scores tween the four target summaries
we then duct an experiment in which two human uators score the generated summaries based on the duc responsiveness
the primary consideration of responsiveness is the amount of information in the summary that relates to the original sentence
the evaluators score the level of responsiveness on a point likert scale with being the best possible
table shows that vert correlates with human judgment of
google
com archive
nist
gov responsiveness
assessment
instructions siveness stronger than all three standard rouge metrics
experiments
experiment setup the environment and evaluation of all models strictly follows the precedent set by rush et al

for both training and testing we extract sentence summary pairs from news articles
the rst sentence of each article is treated as the tence to be summarized while the headline of the article acts as the target summary

datasets the training data comes from the gigaword dataset which is a collection of about million news articles graff et al

it is necessary to discard certain article headline pairs as some news articles open with a sentence that poorly relates to the headline such as a question
preprocessing tasks includes ltering ptb tokenization casing replacing digit characters with and placing low frequency words with unk
uation for hyperparameter tuning is performed on the
testing is done on the where the summaries are capped at a length of bytes
for both and each article has four target maries to be compared against
for processing gaword we used the same data provided by rush et al
but both duc datasets had to be processed according to the tasks specied
certain sentence summary pairs within duc poorly relate to each other due to the fact that the generated summaries used the context of the entire duc article to decide on an adequate summary
since this shortcoming is present across all els attempting sentence summarization on duc we made no effort to remove these difcult ings from the test set

base implementation for the hyperparemeter specication models used attention heads and a dimension of for the dense feed forward layers
cross entropy was used for the loss function and optimization was performed with the adam optimizer using a able learning rate to encourage nal convergence

nist
gov tasks
html dataset gigaword articles sent len sum len





table comparison of general dataset details
sentence and summary lengths are reported as the average word count
gigaword has noticeably shorter target summaries than either duc dataset
to counteract the models generating too short of summaries we augment the beam search ing probabilities to encourage longer summaries
training required approximately epochs
a promising feature of using an attention based chitecture is that the models used here are ble of being trained in approximately hours on a single gpu whereas recent state of the art rent summarization models have been mentioned to take days rush et al

we mented these models using the brary backed by tensorflow
a strong local imum exists when training which closely relates to extracting the rst n words of the input text up to bytes
such a trivial approach produces atively high rouge scores simply due to the ural similarity between target summaries and put sentences
diversity of attention can be couraged by varying the learning rate and ing the attention mechanism itself
for the ing step beam search is used with a beam size of
this results in rouge scores that are higher than a more simple greedy inference
decoding to a xed length of bytes does not align easily with word level decoding so for the tation we approximate the cutoff by limiting the summary sequence to words
results
attention comparisons for each of the attention mechanisms described above we performed a full scale analysis of their performance by training each model on the gigaword dataset and evaluating on
for each experiment the foundational ture was held constant
we modied both the coder self attention and decoder self attention to perform as specied by the given attention in table the model that used scaled anism

com
nist
gov table comparison of attention mechanisms using
rg represents rouge recall vert s is the infersent cosine similarity sub score and vert d is the average wmd sub score
mechanism s dot prod rel s dot prod local local mask local blk mask dilated dilated mask rg l vert s vert d vert









































model topiary zajic et al
abs rush et al
ras lstm chopra et al
koehn et al
ras elman chopra et al
rush et al
ra c lstm zeng et al
words nallapati et al
s att rel ours ac abs li et al
rg l vert

































table rouge recall scores of compared models on
sorted by score
abs and ac abs vert scores were calculated using summaries provided by their respective authors
dot product attention acted as the baseline s prod
the highest performing mechanism was relative scaled dot product attention showing that relative positional encodings can be more ful than absolute encodings
this demonstrates that token generation may rely more heavily on the relationships between surrounding words than relationships at a global sequential level
cal masked attention attained marginally higher and rouge l scores than scaled product attention
however scaled dot product tention scored noticeably higher with vert marily due to the similarity sub score
this gests the scaled dot product model is better than the local mask model when considering the mary semantics across an entire sequence
both local and dilated attention mechanisms repeated the same words regardless of input sentence both masked counterparts did not have this problem
we found a high dependence on batch size ing the training process
models would not verge when batch sizes were at or below kens per batch
the batch size used to train the above models was tokens
dilated attention and dilated mask attention models were trained at lower batch sizes due to higher memory ments
this may have negatively effected results

model comparisons we compare our best model with past work by comparing published rouge scores
slight ances may be present in the reported metrics due to potential differences in data preprocessing tines
in table we compare our best model with that of published results
the relative product self attention model s att rel beats all rouge scores of abs but has a lower when abs is tuned with an tive routine on
s att rel is comparable to but lower than certain els when it comes to scores
ever over the longer subsequence comparisons of and rouge l s att rel performs very well
this can be attributed to the ability of self attention mechanisms to retain a strong ory over past elements of the input and decoded sequences
only the actor critic method abs beats s att rel in all tested categories

qualitative discussion the summaries generated by our best model are strongly abstractive illustrated by example in figure
example showcases the ity to utilize long range recall
from the tive phrase the model determined that hariri was the prime minister of lebanon and adjusted the morphology of the country for succinctness
the model also determined hariri was resigning based on the words bowing out
occasionally tion heads are misdirected and attend to words or phrases that do not contain the primary ing
this occurred in example with was correctly modied by the inclusion of not
the generated summaries exhibit information beyond what was directly in the input sentence example correctly identies premier romano as italian which greatly improves the informedness of the summary
a primary strength of the self attentive model is incorporating abstract information from all segments of the input sentence
this is gested in the long subsequence rouge scores above and seen clearly in qualitative analysis
an assessment of linguistic was formed alongside the duc responsiveness sessment
this followed the same procedure tailed in section

questions pertained to maticality non redundancy referential clarity and structure and coherence
grammaticality scored
non redundancy scored
referential ity scored
and structure and coherence scored

all scores averaged between good and very good
non redundancy is nearly perfect likely because the summaries are too short for redundancy to be of issue
the referential ity scored high as well which can be associated with the performance of the self attention over the words already decoded
conclusion the effect of modern attention mechanisms as plied to sentence summarization has been tested and analyzed
we have shown that a self attentive encoder decoder can perform the sentence marization task without the use of recurrence or convolutions which are the primary mechanisms in state of the art summarization approaches day
an inherent limitation of these existing tems is the computational cost of training
nist
gov quality questions
txt figure examples of generated summaries by the relative dot product self attention model
ated with recurrence
the models presented can be trained on the full gigaword dataset in just hours on a single gpu
our relative dot product self attention model generated the highest ity summaries among our tested models and played the ability of abstracting and reducing plex dependencies
we also have shown that gram evaluation using rouge metrics falls short in judging the quality of abstractive summaries
the vert metric has been proposed as an native to evaluate future automatic summarization based on the premise that an abstractive summary should be judged in an abstractive manner
acknowledgments this material is based upon work supported by the national science foundation under grant no
at the university of colorado colorado springs reu site
peter j
liu mohammad saleh etienne pot ben goodrich ryan sepassi lukasz kaiser and noam shazeer

generating wikipedia by ing long sequences
in international conference on learning representations
ramesh nallapati feifei zhai and bowen zhou

summarunner a recurrent neural network based quence model for extractive summarization of ments
in aaai pages
ramesh nallapati bowen zhou ccero nogueira dos santos c aglar gulcehre and bing xiang

abstractive text summarization using sequence in proceedings of the sequence rnns and beyond
signll conference on computational natural language learning conll berlin germany august pages
romain paulus caiming xiong and richard socher

a deep reinforced model for abstractive marization
arxiv preprint

alexander m rush sumit chopra and jason weston

a neural attention model for abstractive in proceedings of the tence summarization
conference on empirical methods in natural guage processing pages
abigail see peter j
liu and christopher d
manning

get to the point summarization with generator networks
in proceedings of the nual meeting of the association for computational linguistics volume long papers pages
association for computational linguistics
ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan n gomez ukasz kaiser and illia polosukhin

attention is all you need
in advances in neural information cessing systems pages
david zajic bonnie dorr and richard schwartz

in proceedings bbn umd at topiary
of the hlt naacl document understanding workshop boston pages
wenyuan zeng wenjie luo sanja fidler and raquel efcient summarization with arxiv preprint urtasun

read again and copy mechanism


references dzmitry bahdanau kyunghyun cho and yoshua gio

neural machine translation by jointly arxiv preprint learning to align and translate


sumit chopra michael auli and alexander m rush

abstractive sentence summarization with tentive recurrent neural networks
in proceedings of the conference of the north american ter of the association for computational linguistics human language technologies pages
alexis conneau douwe kiela holger schwenk loic barrault and antoine bordes

supervised learning of universal sentence representations from arxiv preprint natural language inference data


yue dong

based summarization methods


a survey on neural arxiv preprint mahak gambhir and vishal gupta

recent matic text summarization techniques a survey
ticial intelligence review
kavita ganesan

rouge
updated and proved measures for evaluation of summarization tasks
arxiv preprint

jonas gehring michael auli david grangier nis yarats and yann n dauphin

in lutional sequence to sequence learning
national conference on machine learning pages
david graff junbo kong ke chen and kazuaki maeda

english gigaword
linguistic data consortium philadelphia
philipp koehn hieu hoang alexandra birch chris callison burch marcello federico nicola bertoldi brooke cowan wade shen christine moran richard zens al

moses open source in toolkit for statistical machine translation
ceedings of the annual meeting of the acl on interactive poster and demonstration sessions pages
association for computational tics
matt kusner yu sun nicholas kolkin and kilian weinberger

from word embeddings to ment distances
in international conference on chine learning pages
piji li lidong bing and wai lam

critic based training framework for abstractive marization
arxiv preprint

chin yew lin

rouge a package for automatic evaluation of summaries
proceedings of the acl workshop text summarization branches out

