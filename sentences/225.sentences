earlier is nt always better sub aspect analysis on corpus and system biases in summarization taehee jung dongyeop kang lucas mentch eduard hovy department of statistics university of pittsburgh pittsburgh pa usa school of computer science carnegie mellon university pittsburgh pa usa
edu dongyeok
cmu
edu g u a l c
s c v
v i x r a abstract despite the recent developments on neural summarization systems the underlying logic behind the improvements from the systems and its corpus dependency remains largely explored
position of sentences in the nal text for example is a well known bias for news summarization
following in the spirit of the claim that summarization is a combination of sub functions we dene three sub aspects of summarization position importance and diversity and conduct an extensive analysis of the biases of each sub aspect with respect to the domain of nine dierent marization corpora e

news academic pers meeting minutes movie script books posts
we nd that while position exhibits substantial bias in news articles this is not the case for example with academic papers and meeting minutes
furthermore our empirical study shows that dierent types of tion systems e

neural based are composed of dierent degrees of the sub aspects
our study provides useful lessons regarding sideration of underlying sub aspects when lecting a new summarization dataset or oping a new system
introduction despite numerous recent developments in neural summarization systems narayan et al
nallapati et al
see et al
kedzie et al
gehrmann et al
paulus et al
the underlying rationales behind the provements and their dependence on the training remain largely unexplored
edmundson put forth the position hypothesis tant sentences appear in preferred positions in the document
lin and hovy provide a method to empirically identify such positions
later hong and nenkova showed an intentional lead equal contribution name order decided by coin ip
bias in news writing suggesting that sentences appearing early in news articles are more tant for summarization tasks
more generally it is well known that recent state of the art models nallapati et al
see et al
are ten marginally better than the rst k baseline on single document news summarization
in order to address the position bias of news articles narayan et al
collected a new dataset called xsum to create single sentence summaries that include material from multiple sitions in the source document
kedzie et al
showed that the position bias in news ticles is not the same across other domains such as meeting minutes carletta et al

in addition to position lin and bilmes dened other sub aspect functions of marization including coverage diversity and information
lin and bilmes claim that many existing summarization systems are instances of mixtures of such sub aspect tions for example maximum marginal relevance mmr carbonell and goldstein can be seen as an combination of diversity and tance functions
following the sub aspect theory we explore three important aspects of summarization sition for choosing sentences by their position portance for choosing relevant contents and versity for ensuring minimal redundancy between summary sentences
we then conduct an in depth analysis of these aspects over nine dierent domains of tion corpora including news articles meeting minutes books movie scripts academic papers and personal posts
for each corpus we tigate which aspects are most important and velop a notion of bias
we provide an empirical result showing how current tion systems are compounded of which sub aspect formance with simple single aspect systems
reference summaries in current corpora include less than of new words that do not appear in the source document except for abstract text of academic papers
semantic volume yogatama et al
lap between the reference and model summaries is not correlated with the hard evaluation rics such as rouge lin
related work we provide here a brief review of prior work on summarization biases
lin and hovy ied the position hypothesis especially in the news article writing hong and nenkova narayan et al
but not in other domains such as versations kedzie et al

narayan et al
collected a new corpus to address the bias by compressing multiple contents of source ument in the single target summary
in the bias analysis of systems lin and bilmes studied the sub aspect hypothesis of tion systems
our study extends the hypothesis to various corpora as well as systems
with a specic focus on importance aspect a recent work peyrard divided it into three categories redundancy relevance and tiveness and provided quantities of each to sure
compared to this ours provide broader scale of sub aspect analysis across various corpora and systems
we analyze the sub aspects on dierent mains of summarization corpora news articles nallapati et al
grusky et al
narayan et al
academic papers or nals kang et al
kedzie et al
movie scripts gorinski and lapata books halcea and ceylan personal posts ouyang et al
and meeting minutes carletta et al
as described further in
beyond the corpora themselves a variety of summarization systems have been developed halcea and tarau erkan and radev used graph based keyword ranking algorithms
lin and bilmes carbonell and goldstein found summary sentences which are highly relevant but less redundant
yogatama et al
used semantic volumes of bigram features for tractive summarization
internal structures of uments have been used in summarization tic parse trees woodsend and lapata cohn corpus bias system bias figure corpus and system biases with the three aspects showing what portion of aspect is used for each corpus and each system
the portion is measured by calculating rouge score between summaries tained from each aspect and target summaries or summaries obtained from each aspect and each system
factors called system bias
at last we marize our actionable messages for future rization researches
we summarize some table ndings as follows summarization of personal post and news cles except for xsum narayan et al
are biased to the position aspect while academic papers are well balanced among the three pects see figure
summarizing long uments e

books and movie scripts and versations e

meeting minutes are extremely dicult tasks that require multiples aspects gether
biases do exist in current summarization tems figure b
simple ensembling of tiple aspects of systems show comparable and lapata topics zajic et al
lin and hovy semantic word graphs mehdad et al
gerani et al
ganesan et al
filippova boudin and morin and abstract meaning representation liu et al

concept based integer linear ming ilp solver mcdonald is used for optimizing the summarization problem gillick and favre banerjee et al
boudin et al
berg kirkpatrick et al

durrett et al
optimized the problem with ical and anarphorcity constraints
with a large scale of corpora for training ral network based systems have recently been developed
in abstractive systems rush et al
proposed a local attention based to sequence model
on top of the work many other variants have been studied ing convolutional networks cheng and lapata allamanis et al
pointer networks see et al
scheduled sampling bengio et al
and reinforcement learning paulus et al

in extractive systems dierent types of encoders cheng and lapata nallapati et al
kedzie et al
and optimization techniques narayan et al
have been veloped
our goal is to explore which types of tems learns which sub aspect of summarization
sub aspects of summarization we focus on three crucial aspects position versity and importance
for each aspect we use dierent extractive algorithms to capture how much of the aspect is used in the oracle tive
for each algorithm the goal is to select k extractive summary sentences equal to the number of sentences in the target summaries for each sample out of n sentences appearing in the original source
the chosen sentences or their indices will be used to calculate the various ation metrics described in for some algorithms below we use vector resentation of sentences
we parse a document into a sequence of sentences

xn where each sentence consists of a sequence of words xi


each sentence is then encoded

wi s where bert devlin et al
is a pre trained bidirectional encoder from transformers vaswani for our oracle set construction
et al

we use the last layer from bert as a representation of each token and then average them to get nal representation of a sentence
all tokens are lower cased

position position of sentences in the source has been gested as a good indicator for choosing summary sentences especially in news articles lin and hovy hong and nenkova see et al

we compare three position based rithms first last and middle by simply ing k number of sentences in the source document from these positions

diversity yogatama et al
assume that extractive summary sentences which maximize the tic volume in a distributed semantic space are the most diverse but least redundant sentences
tivated by this notion our goal is to nd a set of k sentences that maximizes the volume size of them in a continuous embedding space like the bert representations in eq
our objective is to nd the optimal search function s that mizes the volume size v of searched sentences arg

k v





default heuristic convexfall figure volume maximization functions
black dots are sentences in source document and red dots are sen summary sentences
the red shaded polygons are volume space of the summary sentences
if k n we use every sentence from the source document
figure
however its volume space does not guarantee to maximize the volume size because of the non convex polygonality
in order to nd a convex maximum volume we sider two dierent algorithms described below
heuristic
yogatama et al
heuristically choose a set of summary sentences using a greedy algorithm it rst chooses a sentence which has the farthest vector representation from the centroid of whole source sentences and then repeatedly nds sentences whose representation is farthest from other encoders such as averaging word embeddings pennington et al
show comparable performance
the centroid of vector representations of the sen sentences
unlike the original algorithm in yogatama et al
restricting the number of words we constrain the total number of selected sentences to
this heuristic algorithm can fail to nd the maximum volume depending on its ing point the farther distance between two points detected figure
convexfall
here we rst nd the using quickhull barber et al
mented by qhull
it guarantees the imum volume size of selected points with mum number of points figure c
however it does not reduce a redundancy between the points over the convex hull and usually choose larger number of sentences than k
marcu shows an interesting study regarding an importance of sentences given a document if one deletes the least central sentence from the source text then at some point the similarity with the reference text rapidly drops at sudden called the waterfall nomena
motivated by his study we similarly prune redundant sentences from the set chosen by convex hull search
for each turn the sentence with the lowest volume reduction ratio is pruned until the number of remaining sentences is alent to

importance we assume that contents that repeatedly occur in one document contain important information
we nd sentences that are nearest to the bour sentences using two distance measures nearest calculates an averaged pearson tion between one and the rest for all source tence vector representations
k sentences having the highest averaged correlation are selected as nal extractive summaries
on the other hand k nearest chooses the k nearest sentences per each sentence and then averages distances tween each nearest sentence and the selected one
the one has the lowest averaged distance is sen
this calculation is repeated k times and the selected sentences are removed from the ing pool
metrics metrics rouge is recall oriented understudy for ing evaluation lin and hovy for ating summarization systems
we use and rouge l rl measure scores which corresponds to uni gram bigrams and longest common subsequences spectively and their averaged score r
volume overlap vo ratio
hard metrics like rouge often ignore semantic similarities tween sentences
based on the volume assumption in yogatama et al
we measure overlap ratio of two semantic volumes calculated by the model and target summaries
we obtain a set of vector representations of the reference summary sentences y and the model summary sentences y predicted by any algorithm algo in for the i th document yi

yi k y algo i yalgo

yalgo i each volume v is then calculated using the hull algorithm and their overlap is calculated using a shapely
the nal vo is then voalgo v algo v e yi i v e yi i where n is the total number of input documents e is the bert sentence encoder in eq and e yi and algo are a set of vector representations of the reference and model summary sentences spectively
the volume overlap indicates how two summaries are semantically overlapped in a tinuous embedding space
sentence overlap so ratio
even though rouge provides a recall oriented lexical overlap we do nt know the upper bound on performance called oracle of the extractive summarization
we extract the oracle extractive sentences i
e
a set of input sentences which maximizes l f measure score with the reference summary
we then measure sentence overlap so which termines how many extractive sentences from our algorithms are in the oracle summary
the so is soalgo yi algo i c yi in order to determine the aspects most crucial to the summarization task we use three evaluation where c is a function for counting the number of elements in a set
the sentence overlap indicates a set of points is dened as the smallest vex set that includes the points

qhull

org project to the lack of overlap calculation between two gons of high dimensions we reduce it to pca space
how well the algorithm nds the oracle summaries for extractive summarization
summarization corpora summarization we use various domains of datasets to conduct the bias analysis across pora and systems
each dataset has source uments and corresponding abstractive target maries
we provide a list of datasets used along with a brief description and our pre processing scheme cnndm nallapati et al
contains k number of online news articles
it has multiple sentences
on average as a summary
newsroom grusky et al
contains
m news articles and written summaries by authors and editors from to
it has both tractive and abstractive summaries
xsum narayan et al
has news articles and their single but abstractive sentence maries mostly written by the original author
peerread kang et al
consists of entic paper drafts in top tier computer science venues as well as arxiv
org
we use full text of introduction section as source document and of abstract section as target summaries
pubmed kedzie et al
is medical journal papers from the pubmed open access subset
unlike peerread full paper except for abstract is used as source documents
mscript gorinski and lapata is a lection of movie scripts from scriptbase and their corresponding user summaries of the movies
booksum mihalcea and ceylan is a dataset of classic books paired to summaries from grade and clis
due to a large number of sentences we only choose the rst k sentences for source document and the rst sentences for target summaries
reddit ouyang et al
is a collection of personal posts from reddit
com
we use a single abstractive summary per post
the same data split from kedzie et al
is used
ami carletta et al
is documented ing minutes from a hundred hours of recordings and their abstractive summaries

ncbi
nlm
nih
gov pmc
gradesaver
com
cliffsnotes
table summarizes the characteristics of each dataset
we note that the gigaword gra et al
new york and document derstanding conference are also popular datasets commonly used in summarization ses though here we exclude them as they represent only additional collections of news articles ing similar tendencies to the other news datasets such as cnndm
analysis on corpus bias we conduct dierent analyses of how each corpus is biased with respect to the sub aspects
we light some key ndings for each sub section

multi aspect analysis table shows a comparison of the three aspects for each corpus where we include random tion and the oracle set
for each dataset metrics are calculated on a test set except for booksum and ami where we use due to the smaller sample size
earlier is nt always better
sentences lected early in the source show high rouge and so on cnndm newsroom reddit and booksum but not in other domains such as medial journals and meeting minutes and the condensed news summaries xsum
for summarization of movie scripts in particular the last sentences seem to vide more important summaries
xsum requires much importance than other corpora
interestingly the most powerful rithm for xsum is n nearest
this shows that summaries in xsum are indeed collected by stracting multiple important contents into single sentence avoiding the position bias
first convexfall and n nearest tend to work better than the other algorithms for each aspect
first is better than last or middle in new articles except for xsum and personal posts while not in academic papers i
e
peerread pubmed and meeting minutes
convexfall nds the set of sentences that maximize the semantic volume overlap with the target sentences better than the heuristic one
rouge and so show similar behavior while vo does not
in most evaluations rouge scores are linear to so ratios as expected
however vo has high variance across algorithms and aspects

ldc
upenn
edu
nist
gov cnndm newsroom xsum peerread pubmed reddit ami booksum mscript source multi sents
news x news news papers papers minutes books script post x avg src sents
avg tgt sents
data size k k k

k avg src tokens avg tgt tokens
k
k

k

k k
k k k
k table data statistics on summarization corpora
source is the domain of dataset
multi sents
is whether the summaries are multiple sentences or not
all statistics are divided by train test except for booksum and mscript
cnndm newsroom xsum peerread pubmed reddit ami booksum mscript r vo so r vo so r vo so r vo so r vo so r vo so r vo so r vo so r vo so random





oracle



























first





































last





















middle















convfall





heuris













































nnear




























knear






















n o i t i s o p s r e v d i r o m i table comparison of dierent corpora w

t the three sub aspects position diversity and importance
we averaged and rl as r see appendix for full scores
note that volume overlap vo does nt exist when target summary has a single sentence
i
e
xsum reddit this is mainly because the semantic volume sumption maximizes the semantic diversity but sacrices other aspects like importance by ing the outlier sentences over the convex hull
social posts and news articles are biased to the position aspect while the other two aspects appear less relevant
figure however xsum requires all aspects equally but with tively less relevant to any of aspects than the other news corpora
paper summarization is a well balanced task
the variance of so across the three aspects in peerread and pubmed is relatively smaller than other corpora
this indicates that abstract mary of the input paper requires the three aspects at the same time
peerread has relatively higher so then pubmed because it only summarize text in introduction section while pubmed summarize whole paper text which is much dicult almost random performance
conversation movie script and book marization are very challenging
conversation of spoken meeting minutes includes a lot of witty replies repeatedly e

okay
mm
yeah
causing importance and diversity sures to suer
mscript and booksum which clude very long input document seem to be tremely dicult task showing almost random formance

intersection between the sub aspects averaged ratios across the sub aspects do not ture how the actual summaries overlap with each other
figure shows venn diagrams of how sets of summary sentences chosen by dierent aspects are overlapped each other on average
xsum booksum and ami have high oracle recall
if we develop a mixture model of the three aspects the oracle recall means its upper bound meaning that another sub aspect should be ered regardless of the mixture model
this cates that existing procedures are not enough to cover the oracle sentences
for example ami and booksum have a lot of repeated noisy sentences some of which could likely be removed without a signicant loss of pertinent information
importance and diversity are less overlapped with each other
this means that important tences are not always diverse sentences indicating that they should be considered together
cnndm
xsum
peerread
reddit
e ami
booksum
figure intersection of averaged summary sentence overlaps across the sub aspects
we use first for sition convexfall for diversity and n nearest for importance
the number in the parenthesis called cle recall is the averaged ratio of how many the oracle sentences are not chosen by union set of the three sub aspect algorithms
other corpora are in appendix with their oracle recalls
pubmed
and mscript


summaries in a embedding space figure shows two dimensional pca projections of a document in cnndm on the embedding space
source sentences are clustered on the vexhull border not in the middle
we ture that sentences are not uniformly distributed in the embedding space but their positions gradually move over the convexhull
target summaries ect dierent sub aspects according to the sample and corpora
for example many target sentences in cnndm are near by first k sentences
figure pca projection of extractive summaries sen by multiple aspects of algorithms cnndm
source and target sentences are black circles and cyan angles respectively
the blue green red circles are summary sentences chosen by first convexfall nn respectively
the yellow triangles are the oracle sentences
shaded polygon represents a convexhull volume of sample source document
best viewed in color
please nd more examples in appendix
source sentences are ranked dierently according to the algorithm of each aspect see figure
heavily skewed histograms indicate that oracle sentences are positively right skewed or tively left skewed related to the sub aspect
in most cases some oracle sentences are lapped to the rst part of the source sentences
even though their degrees are dierent cle summaries from many corpora i
cnndm newsroom peerread booksum mscript are highly related to the position
compared to the other corpora pubmed and ami contain more ranked important sentences in their oracle maries
news articles and papers tend to nd oracle sentences without diversity i
e
skewed meaning that non diverse sentences are frequently selected as part of the oracle
we also measure how many new words occur in abstractive target summaries by comparing lap between oracle summaries and document tences table
one thing to note is that xsum and ami have less new words in their target maries
on the other hand paper datasets i
e
peerread and pubmed include a lot indicating that abstract text in academic paper is indeed stract
analysis on system bias
single aspect analysis we calculate the frequency of source sentences overlapped with the oracle summary where the we study how current summarization systems are biased with respect to three sub aspects
in dition we show that a simple ensemble of cnndm newsroom xsum peerread pubmed reddit ami booksum mscript position diversity importance figure sentence overlap proportion of each sub aspect row with the oracle summary across corpora column
y axis is the frequency of overlapped sentences with the oracle summary
x axis is the normalized rank of individual sentences in the input document where size of bin is

e

the rst the most diverse the most important sentence is in the rst bin
if earlier bars are frequent the aspect is positively relevant to the corpus
t ot unigram bigram unigram bigram cnndm newsroom xsum peerread pubmed reddit ami booksum mscript












































table rouge of oracle summaries and averaged n gram overlap ratios
o t and s are a set of n grams from oracle target and source document tively
t is the averaged rouge between oracle and target summaries showing how similar they are
ot shows n gram overlap between oracle and target summaries
the higher the more overlapped words in between
is a proportion of n grams in target maries not occurred in source document
the lower the more abstractive i
e
new words target summaries
tems shows comparable performance to the aspect systems
existing systems
we compare various tive and abstractive systems for extractive tems we use k means lin and bilmes maximal marginal relevance mmr carbonell and goldstein cilp gillick and favre boudin et al
texrank mihalcea and tarau lexrank erkan and radev and three recent neural systems cl cheng and lapata sumrun nallapati et al
and kedzie et al

for abstractive systems we use wordilp banerjee et al
and four neural systems rush et al
pointer see et al
teacher bengio et al
and rl paulus et al

the tailed description and experimental setup for each algorithm are in appendix
proposed ensemble systems
motivated by the sub aspect theory lin and bilmes we combine dierent types of systems together from two dierent pools of extractive systems asp from the three best algorithm from each pect and ext from all extractive systems
for each combination we choose the sumary sentences domly among the union set of the predicted tences rand or the most frequent unique tences topk
results
table shows a comparison of ing and proposed summarization systems on the set of corpora in except for
ral extractive systems such as cl sumrun and outperform the others in general
lexrank is highly biased toward the position aspect
on the other hand mmr is extremely biased to the tance aspect on xsum and reddit
interestingly neural extractive systems are somewhat balanced compared to the others
ensemble systems seem to have the three sub aspects in balance compared to the neural extractive systems
they also form the others either rouge or so on ve out of eight datasets
exclude it because of its similar behavior as cnndm
cnndm xsum peerread pubmed reddit ami booksum mscript r so d i r so d i r so d i r so d i r so d i r so d i r so d i r so d i e v i t c a r t e kmeans











mmr















texrank













lexrank















wilp













cl











sumrun





































v i t c a r t s b a cilp













pointer








teacher



rl








e















l















b m















e s















n e table comparison of dierent systems using the averaged rouge scores l with target summaries r and averaged oracle overlap ratios so only for extractive systems
we calculate r between systems and selected summary sentences from each sub aspect d i where each aspect uses the best algorithm first convexfall and nnearest
d i is rounded by the decimal point
indicates the system has too few samples to train the neural systems
indicates so is not applicable because abstractive systems have no sentence indices
the best score for each corpora is shown in bold with dierent colors
conclusion and future directions very important for future directions
we dene three sub aspects of text tion position diversity and importance
we analyze how dierent domains of summarization dataset are biased to these aspects
we observe that news articles strongly reect the position pect while the others do not
in addition we vestigate how current summarization systems ect these three sub aspects in balance
each type of approach has its own bias while neural tems rarely do
simple ensembling of the tems shows more balanced and comparable formance than single ones
we summarize actionable messages for future summarization research dierent domains of datasets except for news articles pose new challenges to the appropriate design of summarization systems
for ple summarization of conversations e

ami or dialogues mscript need to lter out peated rhetorical utterances
book tion e

booksum is very challenging due to its extremely large document size
here current neural encoders suer from computation limits
summarization systems to be developed should clearly state their computational limits as well as eectiveness in each aspect and in each pus domain
a good summarization system should reect dierent kinds of the sub aspects harmoniously regardless of bias
veloping such bias free or robust models can be nobody has clearly dened the deeper nature of meaning abstraction yet
a more cal study of summarization and the various pects is required
a recent notable example is peyrard attempt to theoretically ne dierent quantities of importance aspect and demonstrate the potential of the framework on an existing summarization system
similar studies can be applied to other aspects and their combinations in various systems and dierent domains of corpora
one can repeat our bias study on evaluation metrics
peyrard showed that widely used evaluation metrics e

rouge shannon divergence are strongly mismatched in scoring summary results
one can compare dierent measures e

n gram recall sentence overlaps embedding similarities word edness centrality importance reected by course structures and study bias of each with respect to systems and corpora
acknowledgements this work would not have been possible out the eorts of the authors who kindly share the summarization datasets publicly
we thank rada mihalcea for sharing the book tion dataset
we also thank diane j
litman lor berg kirkpatrick hiroaki hayashi and mous reviewers for their helpful comments
references miltiadis allamanis hao peng and charles sutton

a convolutional attention network for treme summarization of source code
arxiv preprint

siddhartha banerjee prasenjit mitra and kazunari sugiyama

multi document abstractive marization using ilp based multi sentence sion
in proceedings of the twenty fourth tional joint conference on articial intelligence cai
c bradford barber david p dobkin david p dobkin and hannu huhdanpaa

the quickhull rithm for convex hulls
acm transactions on ematical software toms
samy bengio oriol vinyals navdeep jaitly and noam shazeer

scheduled sampling for quence prediction with recurrent neural networks
in advances in neural information processing tems pages
taylor berg kirkpatrick dan gillick and dan klein

jointly learning to extract and compress
in proceedings of the annual meeting of the ciation for computational linguistics human guage technologies volume pages
sociation for computational linguistics
florian boudin and emmanuel morin

reranking in in north american the association for computational keyphrase extraction for n best multi sentence compression
chapter of linguistics naacl
florian boudin hugo mougard and benoit favre

concept based summarization using integer linear programming from concept pruning to tiple optimal solutions
in conference on empirical methods in natural language processing emnlp
jaime carbonell and jade goldstein

the use of mmr diversity based reranking for reordering in proceedings uments and producing summaries
of the annual international acm sigir ence on research and development in information retrieval pages
acm
jean carletta simone ashby sebastien bourban mike flynn mael guillemot thomas hain jaroslav kadlec vasilis karaiskos wessel kraaij melissa kronenthal al

the ami meeting corpus in international workshop a pre announcement
on machine learning for multimodal interaction pages
springer
jianpeng cheng and mirella lapata

neural summarization by extracting sentences and words
arxiv preprint

trevor cohn and mirella lapata

sentence in proceedings pression beyond word deletion
of the international conference on tional linguistics volume pages
ciation for computational linguistics
jacob devlin ming wei chang kenton lee and kristina toutanova

bert pre training of deep bidirectional transformers for language ing
arxiv preprint

greg durrett taylor berg kirkpatrick and dan klein

learning based single document tion with compression and anaphoricity constraints
arxiv preprint

harold p edmundson

new methods in journal of the acm jacm tomatic extracting

gunes erkan and dragomir r radev

lexrank graph based lexical centrality as salience in text journal of articial intelligence summarization
research pages
katja filippova

multi sentence compression nding shortest paths in word graphs
in ings of the international conference on putational linguistics pages
association for computational linguistics
kavita ganesan chengxiang zhai and jiawei han

opinosis a graph based approach to tive summarization of highly redundant opinions
in proceedings of the international conference on computational linguistics pages
tion for computational linguistics
sebastian gehrmann yuntian deng and alexander m rush

bottom up abstractive summarization
arxiv preprint

shima gerani yashar mehdad giuseppe carenini raymond t ng and bita nejat

abstractive summarization of product reviews using discourse structure
in proceedings of emnlp
dan gillick and benoit favre

a scalable global in proceedings of the model for summarization
workshop on integer linear programming for ral langauge processing pages
association for computational linguistics
philip john gorinski and mirella lapata

movie script summarization as graph based scene in proceedings of the conference of tion
the north american chapter of the association for computational linguistics human language nologies pages
david gra junbo kong ke chen and kazuaki maeda

english gigaword
linguistic data consortium philadelphia
max grusky mor naaman and yoav artzi

newsroom a dataset of
million summaries with diverse extractive strategies
in proceedings of the conference of the north american chapter of the association for computational linguistics man language technologies pages new orleans louisiana
association for computational linguistics
kai hong and ani nenkova

improving the estimation of word importance for news in proceedings of the document summarization
conference of the european chapter of the sociation for computational linguistics pages
dongyeop kang waleed ammar bhavana dalvi madeleine van zuylen sebastian kohlmeier uard hovy and roy schwartz

a dataset of peer reviews peerread collection insights and nlp applications
in meeting of the north american chapter of the association for computational guistics naacl new orleans usa
chris kedzie kathleen mckeown and hal daume iii

content selection in deep learning models of summarization
arxiv preprint

chin yew lin

rouge a package for matic evaluation of summaries
in text tion branches out proceedings of the shop volume
chin yew lin and eduard hovy

identifying topics by position
in fifth conference on applied natural language processing
chin yew lin and eduard hovy

the automated acquisition of topic signatures for text tion
in proceedings of the conference on putational linguistics volume pages
sociation for computational linguistics
hui lin and je bilmes

multi document marization via budgeted maximization of ular functions
in human language technologies the annual conference of the north american chapter of the association for computational guistics pages
association for tional linguistics
hui lin and je bilmes

a class of lar functions for document summarization
in ceedings of the annual meeting of the ation for computational linguistics human guage technologies volume pages
sociation for computational linguistics
hui lin and je a bilmes

learning mixtures of submodular shells with application to document summarization
arxiv preprint

fei liu jerey flanigan sam thomson norman sadeh and noah a smith

toward tive summarization using semantic representations
in proceedings of the conference of the north american chapter of the association for tional linguistics human language technologies pages
daniel marcu

discourse trees are good tors of importance in text
advances in automatic text summarization
ryan mcdonald

ence algorithms in multi document summarization
springer
a study of global yashar mehdad giuseppe carenini and raymond ng

abstractive summarization of spoken and in written conversations based on phrasal queries
proc
of acl pages
rada mihalcea and hakan ceylan

explorations in proceedings in automatic book summarization
of the joint conference on empirical methods in natural language processing and computational natural language learning emnlp conll
rada mihalcea and paul tarau

textrank ing order into text
in proceedings of the ference on empirical methods in natural language processing pages
ramesh nallapati feifei zhai and bowen zhou

summarunner a recurrent neural network based quence model for extractive summarization of ments
in thirty first aaai conference on articial intelligence
ramesh nallapati bowen zhou cicero dos santos caglar gulcehre and bing xiang

stractive text summarization using sequence sequence rnns and beyond
in proceedings of the signll conference on computational ral language learning pages
shashi narayan shay b cohen and mirella just the ata

do nt give me the details summary topic aware convolutional neural works for extreme summarization
arxiv preprint

shashi narayan shay b cohen and mirella lapata

ranking sentences for extractive rization with reinforcement learning
arxiv preprint

jessica ouyang serina chang and kathy mckeown

crowd sourced iterative annotation for tive summarization corpora
in proceedings of the conference of the european chapter of the sociation for computational linguistics volume short papers pages
romain paulus caiming xiong and richard socher

a deep reinforced model for abstractive marization
arxiv preprint

jerey pennington richard socher and christopher manning

glove global vectors for word representation
in proceedings of the ence on empirical methods in natural language cessing emnlp pages
maxime peyrard

a simple theoretical model of importance for summarization
in proceedings of the annual meeting of the association for putational linguistics pages florence italy
association for computational linguistics
maxime peyrard

studying summarization evaluation metrics in the appropriate scoring range
in proceedings of the annual meeting of the association for computational linguistics pages florence italy
association for tational linguistics
alexander m rush sumit chopra and jason a neural attention model for arxiv preprint ston

stractive sentence summarization


abigail see peter j liu and christopher d to the point summarization arxiv preprint ning

get with pointer generator networks


ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan n gomez ukasz kaiser and illia polosukhin

attention is all you need
in advances in neural information cessing systems pages
kristian woodsend and mirella lapata

ing to simplify sentences with quasi synchronous grammar and integer programming
in proceedings of the conference on empirical methods in natural language processing pages
association for computational linguistics
dani yogatama fei liu and noah a smith

tractive summarization by maximizing semantic ume
in emnlp pages
david zajic bonnie dorr and richard schwartz

bbn umd at topiary
in proceedings of the hlt naacl document understanding workshop boston pages
a systems and setup details for extractive systems k means rank sentences clusters by descending order of cluster sizes and then using a greedy algorithm lin and bilmes to select the nearest sentences to the troid
maximal marginal relevance mmr nds sentences which are highly relevant to the ument but less redundant with sentences already cilp gillick and selected for a summary
favre boudin et al
weights sentences and maximizes their coverage by imizing redundancy globally using integer ear program ilp
texrank mihalcea and rau automatically extracts keywords using levenshtein distance between the text keywords
lexrank erkan and radev uses module centrality for ranking the keywords
in addition we also use the recent three neural extractive tems cl cheng and lapata sumrun nallapati et al
and kedzie et al
where each has a little variation in their traction
in training cl sumrun and we use weight positive labels to make them proportional to the negative labels
we use embedding size of glove pennington et al
pre trained beddings with
dropout on embeddings xing it not to be trained during training
we use cnn encoder with window size as feature maps
we use layer of sequence sequence model with size of lstm and size of mlp with
dropout
sumrun uses size of segment and size of position dings
for abstractive systems we use wordilp banerjee et al
that produces a word graph of important sentences and then choose sentences from the word graph employing a ilp solver
we also use incremental sequence to sequence a basic rush et al
with els pointer network see et al
with teacher forcing teacher bengio et al
and with reinforcement learning on the evaluation metrics and rl paulus et al

in training pointer pointer and rl we use hidden size of gru with size of glove embeddings
pointer uses maximum erage function using nll loss
teacher uses
ratio of teach forcing with exponential decaying function
and rl uses
ratio of rl kedzie et al
for a detailed comparison
tion after the rst epoch of training
we use size of beam searching at decoding
we use batch size with adam optimizer of
ing rate
for mscript the original dataset has no data split so we randomly split it by


for train valid test set respectively
b venn diagram for all datasets sentence venn diagrams among three aspects and oracle for all datasets are shown in figure
newsroom has an analogous pattern to xsum
compared to peerread pubmed has relatively less sentence overlap between first k and the other two aspects
mscript has extremely small oracle sentence overlaps to all three aspects
ever it is mainly because of the characteristics of the dataset it has long source documents tences on average with short sentences on erage summary
c full rouge f scores for corpus bias analysis in table we provide a full list of rouge f scores for all datasets w

t three sub aspects
we nd that in mscript the best algorithms for each of l are dierent
d documents in an embedding space for all datasets in figure we have more two dimensional pca projection examples for source documents from all datasets
we nd a weak pattern about where target sentences lie on according to the number of them
for example from xsum and reddit which have a single target sentence we investigate that some target sentences are located in the middle of convexhull which are far from any source sentences
e system biases per each corpus with the three sub aspects in figure we have more diagrams showing tem biases toward each of three sub aspects
we nd that there exists a bias according to the corpus for example in reddit many systems have a portance bias in common
on the other hand tems are biased toward a diversity aspect in ami
also some systems tend to be biased in certain aspect across the dierent corpus systems such as cnndm l xsum l newsroom l random








oracle








first k











last k














middle k









convexfall








heuristic









n nearest k nearest







n o i t i s o p s r e v d i r o m i peerread l pubmed l reddit l n o i t i s o p random








oracle








first k








last k








middle k

















heuristic









n nearest








k nearest









convexfall s r e v d i r o m i ami l mscript l booksum l random








oracle

















first k








last k








middle k









convexfall





heuristic












n nearest








k nearest n o i t i s o p s r e v d i r o m i table full l f scores for dierent pora w

t three sub aspects algorithms
kmeans and mmr many corpora are biased ward a importance aspect

newsr



e

g
h
mscript
figure venndiagram of averaged summary sentence overlaps across the the sub aspects for all datasets
we use first k for position p convexfall for diversity d and n nearest for importance i
the number called oracle recall in the parenthesis is the averaged ratio of how many the oracle sentences are not chosen by union set of the three sub aspect algorithms
cnndm newsroom xsum peerread figure pca projection of extractive summaries chosen by multiple aspects of algorithms cnndm newsroom xsum peerread and pubmed
source and target sentences are black circles and purple stars respectively
the blue green red circles are summary sentences chosen by first convexfall kn respectively
the yellow stars are the oracle sentences
best viewed in color
pubmed reddit ami booksum figure pca projection of extractive summaries chosen by multiple aspects of algorithms reddit ami booksum and mscript
source and target sentences are black circles and purple stars respectively
the blue green red circles are summary sentences chosen by first convexfall kn respectively
the yellow stars are the oracle sentences
best viewed in color
cnndm xsum peerread pubmed e reddit ami g booksum mscript figure system biases with the three sub aspects per each corpus showing what portion of aspect is used for each system

