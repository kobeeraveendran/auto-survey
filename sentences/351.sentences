t c o l c
s c v
v i x r a learning to summarize long texts with ory compression and transfer jaehong park jonathan and christopher ai montreal of montreal cifar ai chair firstname

com abstract we introduce a memory to memory mechanism for hierarchical current neural network based encoder decoder architectures and we explore its use for abstractive document summarization
transfers memories via readable writable external memory modules that augment both the encoder and decoder
our memory regularization compresses an encoded input article into a more compact set of sentence representations
most importantly the ory compression step performs implicit extraction without labels sidestepping issues with suboptimal ground truth data and exposure bias of hybrid abstractive summarization techniques
by allowing the decoder to read write over the encoded input memory the model learns to read salient information about the input article while keeping track of what has been generated
our approach yields results that are competitive with state of the art transformer based summarization methods but with times fewer parameters
introduction automatic summarization is the automated process of reducing the size of an input text while serving its most relevant information content and its core semantics
techniques for summarization are often characterized as being either extractive or abstractive
extractive methods construct summaries by combining the most salient passages usually whole sentences of a source text a process similar to human way of identifying the right information
one way to achieve tive summarization is to dene the problem as a sentence classication task using some form of representation of the sentences in a document nallapati et al
cheng lapata
to avoid content overlap issues previous work has used sentence reranking narayan et al
or sentence ordering by extracting sentences recurrently chen bansal
abstractive methods generate summaries by generating new sentence constructs from scratch or from representation of document content a process that is conceptually more similar to the notion of paraphrasing
stractive text summarization has attracted interest since it is capable of generating novel formulations of summaries using language generation models conditioned on the source text
several based recurrent neural network rnn encoder decoders have been introduced to tackle varying text generation issues of standalone abstractive sequence to sequence models
copy and pointer mechanisms gu et al
tu et al
vinyals et al
see et al
for ample have enabled decoders to better generate unseen words out of vocabulary words and named entities
most recently hybrid extractive and abstractive architectures have been proposed and have shown promising results in both quantitative performance measures and human evaluations
in such ups the extractive model rst selects salient sentences from a source article and the abstractive model paraphrases the extracted sentences into a nal summary
the majority of current state the art abstractive summarization are based on the hybrid approach chen bansal gehrmann et al
hsu et al
bae et al
subramanian et al

nonetheless authors contributed equally to this work summarization models using large scale pre trained language models such as bert devlin et al
hybrid models can be limited by three disadvantages
first since ground truth labels for extractive summarization are usually not provided extractive labels must be generated by a potentially mal algorithm hsu et al
subramanian et al

the performance of models trained with such labels is therefore bounded by the quality of the performance of the extractive heuristics
ond since ground truth binary labels for recurrently extracted sentences are typically teacher forced as in chen bansal exposure bias ranzato et al
may negatively affect content selection performance at inference
finally given that the hard extraction step is not differentiable existing hybrid models typically require multi step training not end to end gehrmann et al
subramanian et al
or reinforcement learning chen bansal bae et al
to train the whole model
in this paper we introduce a novel abstractive summarization model that incorporates an ate extractive step but does not require labels for this type of extractive content selection and it is fully end to end trainable
to achieve this we propose a new memory augmented encoder decoder maed architecture wang et al
yogatama et al
ma principe le et al
called
has memorization modes absorb key information of the encoded source sequence via a compression mechanism and sequentially update the ternal memory during target summary generation
without using extractive ground truth labels we nd in our analysis that s compression mechanism behaves as an implicit sentence extractor that stores sentence representations of the salient content
the choice of sentence tations is only guided by the memory regularization and conditional language modeling loss of the decoder thus avoiding exposure bias from maximizing the likelihood of sequential binary tion labels
finally the encoded memory is transferred to the decoder memory which is iteratively rened during the decoding process
to our knowledge is the rst abstractive rization model that uses memory compression for sentence extraction and that directly employs the memorized representations during summary generation
we empirically demonstrate the merits of this approach by setting a new state of the art on long text abstractive summarization tasks on the pubmed arxiv and newsroom datasets cohan et al
grusky et al

our contributions are three fold
we introduce the approach that i stores salient sentence level representations via memory compression ii transfers memory from encoder to decoder and iii updates the memory as the summary generation proceeds

our method yields results that are competitive with state of the art
unlike previous works our model combines the best of extractive and abstractive rization in a fully end to end trainable manner without supervision on the extraction step
transformer vaswani et al
language model based techniques but with substantially fewer rameters only of the parameters used by a recent state of the art transformer based method
background for our baseline we use a hierarchical recurrent encoder decoder structure hred based model from nallapati et al
and cohan et al

the hred has two encoder grus a sentence encoder and a document encoder
given an input sentence of length n the sentence encoder takes a sequence of token embeddings and transforms it into a sequence of hidden states



n


xn the last hidden state of the sentence encoder is used as a corresponding sentence embedding s
the sequence of sentence embeddings s are then processed by the document encoder



l


sl where sj is the j th sentence embedding j l is the total number of sentences in the document
is the associated document encoder hidden state and the decoder gru generates a target summary one token at a time
at each decoding step t the decoder creates a decoder hidden state
the decoder then obtains a context vector ct via ti t figure architecture sentence level representations from the document encoder are reduced to a sized encoder memory me
the encoder memory is transferred to the decoder memory md and is read using a ram like mechanism
the resulting memory readout vector is used to condition sentence and word level attention
the decoder hidden states and the memory readout vector are then used to update the memory state md via a gated write operation zi
and an alignment between t sentence level attention such that i
ti is computed by combining token level attention and ti t i tj t j ti nd p where denotes the index of the sentence corresponding to the l th word and nd is the total number of tokens in the input document
attn in equation is dened as in luong et al

the probability distribution of the next target word yt is estimated using the decoder hidden state and the context vector
the training objective l is the average negative log likelihood of the t target word yt over the whole ground truth summary of length t
finally the pointer generator and decoder coverage method in see et al
are used for the baseline hred
more details of the baseline architecture can be found in appendix
memory to memory mechanism has three main features
an encoder memory bank that compresses a large set of encoder hidden representations into a smaller subset of salient representations
read write operations that allow the decoder to read and update the encoder memory bank with new information during summary generation
token generation that is conditioned on the extracted sentence representations accessed from the dynamic memory
the components can be seamlessly integrated with existing hred architectures
in essence the whole process can be seen as extraction followed by generation
the architecture is depicted in figure

memory compression on encoder the aim of having an external memory on the encoder is to create a xed size representation that reduces the set of l hidden representations from grudoc to a subset of r representations
from a


quence of sentence level document encoder hidden representations l we construct an intermediate d matrix h rld where is the document encoder hidden size h t


l i
a smaller sized memory bank is generated by taking a linear combination of each row in h
the weight vector for the linear combination a is computed with self attention mechanism
a where rda rdad and da is the size of the hidden layer which is a hyperparameter
to capture various aspects of the input document a is extended to a multi head memory write attention matrix a with r heads a where rrda and r is a hyperparameter
this results in r different convex combinations of h which gives us the nal multi head encoder memory matrix me rrd me ah
to ensure the attention weights in various heads focus on a diverse set of salient input sentences we propose a novel regularization loss for memory compression
the following regularization term encourages the diversity of compressed encoded states
where is the frobenius norm
the regularization loss achieves two goals neously it promotes diversity over the r sentence representations stored in the memory thereby reducing the risk of redundant information
it hardens the attention probabilities of each head assuring that each memory slot is approximately associated to a single sentence representation
as a result the encoder memory me essentially performs implicit extraction over the encoder hidden states in a fully differentiable manner
figure in appendix shows the effect of regularization on the encoder memory compression
note that no supervision exists on this extractive step and the ory compression is only guided by the memory regularization and back propagated error signals from the target summary generation

read write operations on decoder once the encoder memory is constructed the context read from the memory is used to augment both attention mechanism and the target token generation
as a rst step the encoder memory me is transferred to the decoder and used as an initial state of the decoder memory md
at every time step t the decoder reads from the memory and generates a memory read vector mt
specically the decoder takes the weighted sum over r memory slots via a ram like attention mechanism tk t mt r x where is the vector representation of the head or slot of md
is then combined with mt and generate a memory augmented decoder hidden state and the t next token estimation of the baseline system is replaced with
as a consequence the attention over the source text and the prediction of the target token are conditioned on the memory read mt
thus there is a direct link between the contents of the memory and the text generation
t t during the summary generation process the semantics of the source sequence that is kept in the decoder memory needs to be modied
the memory write operation of enables the memory to log the history of what has been attended and generated
the decoder memory write operation outlined below removes and adds information using a gated mechanism to forget used memories and update each memory slot
the gating mechanism is conditioned on the the memory content md the memory read vector mt and the decoder hidden state t zk t mt zk t zk t t although text generation is directly conditioned on the memory context mt the benet of ing memory was limited in our preliminary experiments
we observed only a few memory slots table results on the pubmed arxiv and newsroom dataset
each type corresponds to purely abstractive a extractive e or extractive abstractive hybrid h approaches
tlm uses the model radford et al
that has times larger parameters than
the highest rouge scores for abstractive methods are boldfaced
all rouge scores have a condence interval of at most
as reported by the ofcial rouge
results taken from et al
and et al

newsroom abstractive summarization test set results
model type sent sent attn ptr gen discourse ours e e e a a a a h a of params
pubmed















m


m


m



m


l arxiv


























newsroom























were repetitively attended during decoding
to ensure that the decoder fully utilizes its memory to improve generation we propose another regularization term
t t j t t ls x t t x r x where is the vector representation of the kth memory head of me and is a sentence level context vector
the regularization assumes that if the memory context mt which is a level representation is combined properly within the decoding the sentence level context vector t would correlate with it
the initial state of the decoder memory me is used to compute since t the representation of md deviates from the original representation space of due to the write j operation
the nal training objective of becomes as follows
t l where the weights for regularization and are hyperparameters
related work recent works in abstractive summarization have leveraged intermediate content selection
in these approaches writing a summary is factorized into two steps extraction and generation
an extractor is used to prioritize and select the most important part of the input text
the extractor is normally trained on a sequence of binary labels where each label indicates whether the corresponding text unit should be selected or not
the level of extraction can be word level gehrmann et al
cho et al
or sentence level chen bansal hsu et al
bae et al
subramanian et al

as the ground truth for extraction is typically missing heuristics that measure n gram overlap with the target summary are used to build extractive oracles
similar to other approaches performs sentence level extraction to deal with long source
determines the alignment between source and target sentences in a latent space without relying on possibly suboptimal extractive heuristics
in addition sentence extraction is not sequentially done in which addresses the exposure bias issue ranzato et al

memory augmented encoder decoder maed architectures wang et al
yogatama et al
ma principe le et al
have been proposed for conditional natural language preliminary experiments we applied word level selection on the pubmed and arxiv datasets which led to poor results table model ablation study on the pubmed dataset
model baseline hred
encoder mem
decoder mem

mem transfer reg
reg
rouge





l





generation tasks such as machine translation kaiser et al
and image captioning park et al

using differentiable read and write operations to an external module maed can represent non local context of rnns with enhanced memory capacity
such models are able to store rally distant information of large input sequences a feature that is particularly useful for long text summarization
in the context of short text abstractive summarization kim et al
proposed a memory architecture named multi level memory networks mmn
mmn can exibly reduce sentations at different levels of document hierarchy into a xed size external memory
authors used multi layer dilated convolutional neural networks cnn yu koltun van den oord et al
to build a hierarchical representation of the document
also constructs memory from the hierarchical representation of the document but by compressing it into a sparse set of sentence representations
further mmn s memory representations remain static throughout the decoding process while dynamically updates its memory which is more effective in learning long term dependency
lastly but not least our work proposes novel regularization for memory read and compression
results and discussion
experiment setup we evaluate on the pubmed arxiv cohan et al
and newsroom abstractive datasets grusky et al
which are large scale summarization datasets
the average lengths of source articles and target summaries are pubmed arxiv and room respectively
they are up to times longer than the widely used cnn dailymail dataset hermann et al
see et al

our pre processing and training setups are tical to cohan et al
and subramanian et al

more details on training and evaluation can be found in appendix
for quantitative evaluation we use the rouge metric lin and report rouge scores

results table shows the rouge scores on three summarization datasets
the hybrid type h refers to models that use two step extractive abstractive summarization
on the pubmed dataset the tlm model shows the highest scores in and r l
is close to those scores and shows higher
scores with times less parameters
m vs
m
it achieves such performance over the tlm model without ground truth labels for sentence extraction
we also reiterate that is trained completely end to end whereas the hybrid tlm requires separate training for the extractor and the conditional transformer language model
we nd similar results on the arxiv and newsroom datasets
on the arxiv dataset even surpasses the transformer based tlm model in
and
scores
also shows competitive results on the newsroom abstractive dataset

ablation study to assess the importance of components we conduct an ablation study on the pubmed dataset
table shows the effects of adding different add ons on rouge scores
mem is the baseline hred augmented with the encoder memory described in section

table rouge scores of unsupervised extractive methods on the pubmed and the arxiv dataset
the result of baseline extractive methods is from subramanian et al

data pubmed arxiv model lexrank







gold ext lexrank gold ext rouge







l







s s s s s s s s s s s s s s s s t t t t t t t t t t t t t t t t t t t t









decoding timstep a multi head memory write attention matrix a memory read attention matrix figure multi head memory write attention matrix a and memory read attention matrix
in a rows denote memory heads or slots and columns indicate input sentence indices
in b columns indicate decoding time steps
the result demonstrates that memory context indeed enhances the performance measures of the generated summaries
decoder mem adds the write operation to the memory but without memory transfer
in this case the decoder memory md is initialized with zeros not with the encoder memory me
compared to the baseline hred the result shows that the write mechanism on the decoder memory helps the generation even without the memory transfer
this indicates that the summary writing process largely benets from accessing long term contextual information of the output text
transferring memory mem transfer brings substantial improvements in rouge scores
it seems to be crucial to initiate the summary generation from the selected memory representations showing the importance of the memory compression and transfer steps
furthermore it can be observed that discouraging redundancies over the encoder memory head via leads to additional ments on all rouge scores
finally adding another regularization on the decoder memory read operation completes the architecture and achieves the best rouge scores

implicit extraction via memory compression our initial hypothesis was that the encoder memory me would pick a set of the most salient input sentences for summarization
to conrm we analyze the quality of the extractive summarization by memory compression
concretely we concatenate the sentences with the highest attention weight of each memory head to generate a summary
table shows the rouge scores of different pervised extractive summarization methods on the pubmed and arxiv datasets
the extractive marization performed by the s memory compression outperforms existing unsupervised extractive summarization baselines
the result indicates that s memory compression is able to prioritize amongst a large set of input sentences without ground truth sentence labels
table percentage ratios of output summary n grams found in the pubmed original input article
table results of human evaluation
maximum score for each criterion is
models reference baseline hred tlm hybrid ours



n grams











models baseline hred tlm hybrid ours human evaluation scores coh inf





rel


flu



dynamic memory read by decoder the benet of implicit extraction is maximized when the extracted representations are properly sumed in the text generation
to understand the link between the representations stored in the ory and the summary generation we analyze the memory read attention weights tk in equation throughout the decoding
figure shows that the decoder fully utilizes all memory representations
we also nd a pattern that memory read attention weights are mostly concentrated on the front part of the source text in the beginning of the decoding mem slots and gradually moves to the latter part mem slots
this demonstrates s ability to update the read operation to dynamically capture relevant input contexts during the summary generation

abstractiveness of the summary to analyze the abstractiveness of generated summaries we present the ratio of output summary grams present in the original input article
table shows that copies less n grams than the baseline hred
compared to the baseline hred generates approximately more novel grams and more grams respectively
the result along with higher rouge scores highlights the s ability to generate novel words for abstractive summarization while staying focused on important parts of the article
although tlm subramanian et al
shows the highest abstractiveness achieves its result with a signicanly smaller model

human evaluation we also perform a human evaluation to assess the quality of generated summaries
for the human evaluation random arxiv testset article summary pairs are presented to three amazon cal turk workers
workers judge generated summaries on four different aspects coherence coh does the summary make sense informativeness inf are the most important points of the article captured redundancy red does the summary repeat itself and fluency flu how uent is the summary
table shows that obtains the highest scores in informativeness and dundancy
its coherence score is also close to the best score by tlm
fluency is an advantage for the transformer based tlm as expected but is close and still greatly superior to vanilla hierarchical encoder decoders
while the rouge difference between the baseline and are substantial in table the results of the human evaluation show more pronounced differences in the quality of generated text
for output summary examples please refer to table in appendix
authors provided example summaries from their model
conclusion this work proposes a novel maed based mechanism for very long text abstractive summarization
involves two memory types a static encoder memory for compressing input texts and a dynamic decoder memory which renes the generation process
memory fer between them links two memories and maximizes the benet of content extraction aimed for summarization
different from existing hybrid extractive and abstractive approaches incorporates an extraction step without ground truth sentence labels and multi step training
we demonstrate the effectiveness of by showing promising results on the pubmed arxiv and newsroom summarization datasets with an order of magnitude less parameters than competing transformer based models
the s memory compression can be generalized to other mains that require text generation guided by content selection
in future work we will extend and validate the strength of our approach on a variety of language learning tasks
references sanghwan bae taeuk kim jihoon kim and sang goo lee
training the of sentence rewriting for abstractive summarization
shop on new frontiers in summarization pp
hong kong china november
association for computational linguistics
url doi
aclweb
org anthology


in proceedings of summary level yen chun chen and mohit bansal
fast abstractive summarization with reinforce selected the association for sentence rewriting
computational linguistics volume long papers pp
melbourne australia july
association for computational linguistics


url
aclweb
org anthology
the annual meeting of in proceedings of in proceedings of jianpeng cheng and mirella lapata
neural summarization by extracting sentences and the association for the annual meeting of words
tational linguistics volume long papers pp
berlin germany august
association for computational linguistics
url doi
aclweb
org anthology


jaemin cho minjoon seo and hannaneh hajishirzi
mixture content selection for in proceedings of the conference on empirical verse sequence generation
ods in natural language processing and the international joint conference on ral language processing emnlp ijcnlp pp
hong kong china ber
association for computational linguistics


url
aclweb
org anthology
kyunghyun cho bart van merrienboer caglar gulcehre dzmitry bahdanau fethi bougares holger schwenk and yoshua bengio
learning phrase representations using rnn encoder in proceedings of the conference on decoder for statistical machine translation
pirical methods in natural language processing emnlp pp
doha qatar tober
association for computational linguistics


url
aclweb
org anthology
arman cohan franck dernoncourt doo soon kim trung bui seokhwan kim walter chang and nazli goharian
a discourse aware attention model for abstractive summarization of long ments
in proceedings of the conference of the north american chapter of the association for computational linguistics human language technologies volume short papers pp
new orleans louisiana june
association for computational linguistics
doi

url
aclweb
org anthology
jacob devlin ming wei chang kenton lee and kristina toutanova
bert pre training of deep bidirectional transformers for language understanding
in proceedings of the ference of the north american chapter of the association for computational linguistics human language technologies volume long and short papers pp
minneapolis nesota june
association for computational linguistics


url
aclweb
org anthology
sebastian gehrmann yuntian deng and alexander rush
bottom up abstractive the conference on empirical methods in marization
ral language processing pp
brussels belgium october november
association for computational linguistics
url
aclweb
org anthology


in proceedings of doi max grusky mor naaman and yoav artzi
newsroom a dataset of
million summaries with diverse extractive strategies
in proceedings of the conference of the north american ter of the association for computational linguistics human language technologies volume long papers pp

jiatao gu zhengdong lu hang li and victor o
k
li
incorporating copying mechanism in in proceedings of the annual meeting of the sequence to sequence learning
tion for computational linguistics volume long papers pp
berlin germany august
association for computational linguistics


url
aclweb
org anthology
karl moritz hermann tomas kocisky edward grefenstette lasse espeholt will kay teaching machines to read and comprehend
mustafa suleyman and phil blunsom
in advances in neural information processing systems pp


url
nips
cc teaching machines to read and comprehend
pdf
wan ting hsu chieh kai lin ming ying lee kerui min jing tang and min sun
a unied model for extractive and abstractive summarization using inconsistency loss
in proceedings of the annual meeting of the association for computational linguistics volume long papers pp
melbourne australia july
association for computational linguistics
doi

url
aclweb
org anthology
lukasz kaiser or nachum aurko roy and samy bengio
learning to remember rare in international conference on learning representations iclr toulon events
france april conference track proceedings
openreview
net
url
net sjtqldqlg
byeongchang kim hyunwoo kim and gunhee kim
abstractive summarization of reddit in proceedings of the conference of the posts with multi level memory networks
north american chapter of the association for computational linguistics human language technologies volume long and short papers pp
minneapolis minnesota june
association for computational linguistics


url
aclweb
org anthology
diederik p
kingma and jimmy ba
adam a method for stochastic optimization
in yoshua bengio and yann lecun eds
international conference on learning representations iclr san diego ca usa may conference track proceedings
url
org

hung le truyen tran thin nguyen and svetha venkatesh
variational memory decoder
in advances in neural information processing systems pp


url
nips
cc variational memory encoder decoder
pdf
c
y
lin
looking for a few good metrics automatic summarization evaluation how many samples are enough in proceedings of the ntcir workshop
thang luong hieu pham and christopher d
manning
effective approaches to the conference on based neural machine translation
cal methods in natural language processing pp
lisbon portugal ber
association for computational linguistics


url
aclweb
org anthology
in proceedings of y
ma and j
c
principe
a taxonomy for neural memory networks
ieee transactions on neural networks and learning systems pp

ramesh nallapati bowen zhou cicero dos santos c aglar and bing xiang
abstractive text summarization using sequence to sequence rnns and beyond
in proceedings of the signll conference on computational natural language learning pp
berlin many august
association for computational linguistics


url
aclweb
org anthology
ramesh nallapati bowen zhou and mingbo ma
classify or select neural architectures for tractive document summarization
arxiv preprint

shashi narayan shay b
cohen and mirella lapata
ranking sentences for extractive the marization with reinforcement learning
north american chapter of the association for computational linguistics human guage technologies volume long papers pp
new orleans louisiana june
association for computational linguistics


url
aclweb
org anthology
the conference of in proceedings of cesc chunseong park byeongchang kim and gunhee kim
attend to you personalized image captioning with context sequence memory networks
ieee conference on computer vision and pattern recognition cvpr pp

alec radford jeffrey wu rewon child david luan dario amodei and ilya sutskever
language models are unsupervised multitask learners
openai blog
marcaurelio ranzato sumit chopra michael auli and wojciech zaremba
sequence level ing with recurrent neural networks
in yoshua bengio and yann lecun eds
international conference on learning representations iclr san juan puerto rico may conference track proceedings
url
org

abigail see peter j
liu and christopher d
manning
get to the point summarization with in proceedings of the annual meeting of the association pointer generator networks
for computational linguistics volume long papers pp
vancouver canada july
association for computational linguistics


url
aclweb
org anthology
sandeep subramanian raymond li jonathan pilault and christopher pal
on extractive and abstractive neural document summarization with transformer language models
arxiv preprint

zhaopeng tu zhengdong lu yang liu xiaohua liu and hang li
coverage based neural machine translation
corr

url
org

aaron van den oord sander dieleman heiga zen karen simonyan oriol vinyals alexander graves nal kalchbrenner andrew senior and koray kavukcuoglu
wavenet a generative model for raw audio
in arxiv
url
org

ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan n gomez ukasz kaiser and illia polosukhin
attention is all you need
in i
guyon u
v
luxburg s
bengio h
wallach r
fergus s
vishwanathan and r
garnett eds
advances in ral information processing systems pp

curran associates inc

url
nips
cc attention is all you need
pdf
oriol vinyals meire fortunato and navdeep jaitly
pointer networks
vances in neural
nips
cc pointer networks
pdf
information processing systems pp


in url mingxuan wang zhengdong lu hang li and qun liu
memory enhanced decoder the conference on in proceedings of for neural machine translation
cal methods in natural language processing pp
austin texas november
association for computational linguistics
url doi
aclweb
org anthology


dani yogatama yishu miao gabor melis wang ling adhiguna kuncoro chris dyer and in phil blunsom
memory architectures in recurrent neural network language models
international conference on learning representations iclr vancouver bc canada april may conference track proceedings
openreview
net
url
net
fisher yu and vladlen koltun
multi scale context aggregation by dilated convolutions
in yoshua bengio and yann lecun eds
international conference on learning representations iclr san juan puerto rico may conference track proceedings
url
org

a appendix a
baseline architecture in addition to the hierarchical recurrent encoder decoder hred architecture described in section the following features are used for both baseline and architectures
a

pointer generator network in order to handle out of vocabulary oov token predictions pointer generator in see et al
is used to copy words directly from the input document
at each step t the decoder decides whether to predict the next target word from the input text or the generation mechanism
the pointer ator computes zt which denotes the probability of choosing pg for sampling the next target token
zt c ct t wt x t where x t is the embedding of the previous target token
the probability zt is used as a variable for soft switch between generating a word from the vocabulary pg or directly copying from the source document pc
the probability of copying a word w from the source text is calculated based on the attention weights
ti x i xi w note that if w does not exist in the source document
likewise if w is an out of vocabulary word
combining two probability distributions the nal probability of the next word yt being w is as follows
p yt a

decoder coverage it is well known that rnn sequence to sequence models tend to suffer from repeated phrases when generating long target sequences
see et al
tackled this issue by keeping track of the attention coverage
more concretely the coverage vector covt at the decoding step t is computed by taking the summation of the token level attention weights until the last step t
covt t x to inform the decoder of the history of attention weights the coverage vector is fed into the level attention mechanism which modies the equation to the following equation
ti i t wc covt t a
training details we generally follow the pre processing steps in cohan et al
for the pubmed and arxiv datasets
the maximum number of sections is set to and the maximum number of tokens for each section is
the length of the target summary is limited to tokens
single layer bidirectional grus cho et al
are used for the sentence and the document coders
the decoder is also a single layer gru
all grus have the hidden size of
the sionality of token embeddings is and embeddings are trained from scratch
the vocabulary size is limited to
batch size is and adam kingma ba with learning rate is used for training
maximum gradient norm is set to
we train all models for epochs
at the test time beam search with the beam size is used for decoding
for hyperparameters the number of heads for the memory compression is and the self attention hidden size is
the weights and for the regularization and are
and
respectively
a
the effect of regularization the following gures show the effect of regularization in
s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s











with regularization without regularization figure the effect of regularization on memory compression
examples of the multi head encoder memory write attention matrix a are illustrated
rows denote memory heads or slots and columns indicate input sentence indices
note that the regularization loss removes the redundancy over different memory heads and guides each slot to focus on a single sentence
t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t decoding timstep with regularization decoding timestep without regularization figure the effect of regularization on memory read
examples of the decoder memory read attention matrix are illustrated
rows denote memory heads or slots and columns indicate ing time steps
note that the regularization loss encourages the model to fully utilize the compressed memory representations
a
qualitative results reference and system generated summaries from the test set of the arxiv dataset are shown in the following table
compared to the baseline shows much less repetition and produces more concise summaries
even the tlm model repeats several phrases or sentences










table reference and system generated summaries from the test set of the arxiv dataset
in the number of variables and constraints
reference we study the behavior of simple principal pivoting methods for the p matrix linear mentarity problem p lcp
we solve an open problem of morris by showing that murty s least index pivot rule under any xed index order leads to a quadratic number of iterations on morris s highly cyclic p lcp examples
we then show that on k matrix lcp instances all pivot rules require only a linear number of iterations
as the main tool we employ unique sink orientations of cubes a useful combinatorial abstraction of the lcp
baseline hred the third author of this paper still vividly recollects his visit to victor klee at the versity of washington seattle in august
the third author of this paper still vividly recollects his visit to victor klee at the university of washington seattle in august
the third author of this paper still vividly recollects his visit to victor klee at the university of washington seattle in august
in this paper
we introduce the digraph model behind p lcps and show that the simplex method is polynomial time for which the number of iterations is exponential tlm hybrid we study linear complementarity problems with sufcient matrices and the criss cross linear complementarity to derive linear complementarity problems method
we use a novel notion of with sufcient matrices and the linear version of the quadratic version of the linear version of the linear version
this yields a new family of np complete problems for which both linear complementarity and the quadratic version of the linear version of the quadratic version are possible
in this paper we introduce a digraph model behind p lcps and show that the simplex exponential method can be used to solve a linear program lp for which the number of iterations is in the number of variables and constraints on the number of iterations
we show that the number of arithmetic operations does not depend on the bit lengths of the input numbers
we also show that the number of iterations required to solve the linear complementarity problem can be used to determine the expected performance of the lp
reference we have developed a procedure for the classication of eclipsing binaries from their light the procedure was tested on more than systems with known curve parameters and spectral type
classication and its efciency was estimated for every evolutionary status we use
the procedure was applied to about binaries with no classication and the vast majority of them was classied fully
systems of relatively rare evolutionary classes were detected in that process as well as systems with unusual contradictory parameters
also for previously unclassied cluster binaries evolutionary classes were identied
these stars can serve as tracers for age and distance estimation of their parent stellar systems
the procedure proved itself as fast exible and effective enough to be applied to large ground based and space born surveys containing tens of thousands of eclipsing binaries
baseline hred in this paper we present a novel procedure for determination of the evolutionary status of eclipsing binaries
the procedure is based on the most comprehensive set of rules for the classication of eclipsing binaries which is the world s principal database of eclipsing binary systems with available classication
the main goal of this paper is to develop a fast and effective procedure for determination of the binary s spectral type or color index
the procedure is based on the most comprehensive set of rules for the classication of eclipsing binaries which is the world s principal database of eclipsing binary systems with available classication
the main goal of this paper is to develop a fast and effective procedure for determination of the evolutionary status of eclipsing binaries
the method is based on a catalogue which includes a number and the eclipsing variables and a classication procedure
we nd a good t of all known parameters which can not have any value in a large range
we nd a signicant fraction mp depth for each of these systems and that a given number is mag tlm hybrid a fast and effective procedure for determination of the evolutionary status of eclipsing binaries is presented
a complete set of rules is developed that utilizes all available data for a specic class of eclipsing binaries while requiring only light curve parameters and an estimate of the binary s spectral type or color index
the procedure is tested with the catalogue of eclipsing binaries cev which is the world s principal database of eclipsing binaries with available classication
the main goal of our work is to develop a fast and effective procedure for determination of the evolutionary status of eclipsing binaries
we apply it to cev binaries with already available classication
the main objective of this work is to develop a fast and effective procedure for determination of the evolutionary status of eclipsing binaries
the main objective of our work is to develop a fast and effective procedure for determination of the evolutionary status of eclipsing binaries
we apply it to cev binaries with already available classication
the main objective of this work is to demonstrate the usefulness of our method and to propose a possible application of it to cev binaries with already known classication
in this paper we present a novel procedure for determination of the evolutionary status of eclipsing binaries while requiring only light curve parameters and an estimate of the binary s spectral type or color index
the procedure is based on the most comprehensive set of rules for the classication of ing binaries which can be used to quickly characterize large numbers of eclipsing binaries
the procedure is tested with the catalogue of eclipsing variables dr sh which is the world s principal database of ing binary systems with available classication
we nd that the number of fully characterized eclipsing binaries can be used to quickly characterize large numbers of eclipsing binaries

