hierarchical transformers for multi document summarization yang liu and mirella lapata institute for language cognition and computation school of informatics university of edinburgh yang

ac
uk
ed
ac
uk a m l c
s c v
v i x r a abstract in this paper we develop a neural rization model which can effectively process multiple input documents and distill tive summaries
our model augments a ously proposed transformer architecture liu et al
with the ability to encode ments in a hierarchical manner
we represent cross document relationships via an attention mechanism which allows to share information as opposed to simply concatenating text spans and processing them as a at sequence
our model learns latent dependencies among tual units but can also take advantage of plicit graph representations focusing on larity or discourse relations
empirical results on the wikisum dataset demonstrate that the proposed architecture brings substantial provements over several strong baselines
introduction automatic summarization has enjoyed renewed interest in recent years thanks to the ity of neural network models and their ability to learn continuous representations without recourse to preprocessing tools or linguistic annotations
the availability of large scale datasets sandhaus hermann et al
grusky et al
containing hundreds of thousands of summary pairs has driven the development of neural architectures for summarizing single uments
several approaches have shown ing results with sequence to sequence models that encode a source document and then decode it into an abstractive summary see et al
ilmaz et al
paulus et al
gehrmann et al

multi document summarization the task of producing summaries from clusters of code and data is available at
com nlpyang hiersumm
cally related documents has received icantly less attention partly due to the paucity of suitable data for the application of learning methods
high quality multi document rization datasets i
e
document clusters paired with multiple reference summaries written by mans have been produced for the document derstanding and text analysis conferences duc and tac but are relatively small in the range of a few hundred examples for training ral models
in an attempt to drive research ther liu et al
tap into the potential of wikipedia and propose a methodology for ating a large scale dataset wikisum for document summarization with hundreds of sands of instances
wikipedia articles specically lead sections are viewed as summaries of various topics indicated by their title e

florence or natural language processing
documents cited in the wikipedia articles or web pages returned by google using the section titles as queries are seen as the source cluster which the lead section purports to summarize
aside from the difculties in obtaining ing data a major obstacle to the application of end to end models to multi document tion is the sheer size and number of source uments which can be very large
as a result it is practically infeasible given memory limitations of current hardware to train a model which codes all of them into vectors and subsequently generates a summary from them
liu et al
propose a two stage architecture where an tive model rst selects a subset of salient passages and subsequently an abstractive model generates the summary while conditioning on the extracted subset
the selected passages are concatenated into a at sequence and the transformer vaswani et al
an architecture well suited to guage modeling over long sequences is used to decode the summary
related work although the model of liu et al
takes an important rst step towards abstractive document summarization it still considers the multiple input documents as a concatenated at sequence being agnostic of the hierarchical tures and the relations that might exist among uments
for example different web pages might repeat the same content include additional tent present contradictory information or discuss the same fact in a different light radev
the realization that cross document links are portant information nating redundancy and creating overall coherent summaries has led to the widespread adoption of graph based models for multi document marization erkan and radev christensen et al
wan parveen and strube
graphs conveniently capture the ships between textual units within a document lection and can be easily constructed under the sumption that text spans represent graph nodes and edges are semantic links between them
in isolating salient in this paper we develop a neural tion model which can effectively process ple input documents and distill abstractive maries
our model augments the previously posed transformer architecture with the ability to encode multiple documents in a hierarchical ner
we represent cross document relationships via an attention mechanism which allows to share information across multiple documents as opposed to simply concatenating text spans and feeding in this them as a at sequence to the model
way the model automatically learns richer tural dependencies among textual units thus corporating well established insights from earlier work
advantageously the proposed architecture can easily benet from information external to the model i
e
by replacing inter document attention with a graph matrix computed based on the basis of lexical similarity erkan and radev or discourse relations christensen et al

we evaluate our model on the wikisum dataset and show experimentally that the proposed tecture brings substantial improvements over eral strong baselines
we also nd that the dition of a simple ranking module which scores documents based on their usefulness for the target summary can greatly boost the performance of a multi document summarization system
most previous multi document summarization methods are extractive operating over graph based representations of sentences or passages
proaches vary depending on how edge weights are computed e

based on cosine similarity with tf idf weights for words erkan and radev or on discourse relations christensen et al
and the specic algorithm adopted for ranking text units for inclusion in the nal summary
eral variants of the pagerank algorithm have been adopted in the literature erkan and radev in order to compute the importance or salience of a passage recursively based on the entire graph
more recently yasunaga et al
propose a neural version of this framework where salience is estimated using features extracted from tence embeddings and graph convolutional works kipf and welling applied over the relation graph representing cross document links
abstractive approaches have met with limited success
a few systems generate summaries based on sentence fusion a technique which ties fragments conveying common information across documents and combines these into tences barzilay and mckeown filippova and strube bing et al

although neural abstractive models have achieved ing results on single document summarization see et al
paulus et al
gehrmann et al
celikyilmaz et al
the tension of sequence to sequence architectures to multi document summarization is less ward
apart from the lack of sufcient training data neural models also face the computational challenge of processing multiple source ments
previous solutions include model fer zhang et al
lebanoff and liu where a sequence to sequence model is pretrained on single document summarization data and tuned on duc multi document benchmarks or unsupervised models relying on reconstruction jectives ma et al
chu and liu
liu et al
propose a methodology for constructing large scale summarization datasets and a two stage model which rst extracts salient information from source documents and then uses a decoder only architecture that can attend to very long sequences to generate the summary
we low their setup in viewing multi document marization as a supervised machine learning troduced in vaswani et al
it generates a summary token by token while attending to the source input
we also use beam search and a length penalty wu et al
in the decoding process to generate more uent and longer maries

paragraph ranking unlike liu et al
who rank paragraphs based on their similarity with the title using tf based cosine similarity we adopt a based approach
a logistic regression model is applied to each paragraph to calculate a score dicating whether it should be selected for rization
we use two recurrent neural networks with long short term memory units lstm hochreiter and schmidhuber to represent tle t and source paragraph p utm wtm upn wpn where wti wpj are word embeddings for tokens in t and p and uti upj are the updated vectors for each token after applying the lstms
a max pooling operation is then used over title vectors to obtain a xed length representation ut ut utm we concatenate ut with the vector upi of each ken in the paragraph and apply a non linear formation to extract features for matching the title and the paragraph
a second max pooling tion yields the nal paragraph vector p pi ut pn finally to estimate whether a paragraph should be selected we use a linear transformation and a moid function s where s is the score indicating whether graph p should be used for summarization
all input paragraphs pl receive scores sl
the model is trained by minimizing the cross entropy loss between si and ground truth scores yi denoting the relatedness of a paragraph to the gold standard summary
we adopt recall of paragraph pi against figure pipeline of our multi document tion system
l source paragraphs are rst ranked and the ones serve as input to an encoder decoder model which generates the target summary
lem and for this purpose assume access to large labeled datasets i
e
source documents summary in contrast to their approach we use a pairs
learning based ranker and our abstractive model can hierarchically encode the input documents with the ability to learn latent relations across uments and additionally incorporate information encoded in well known graph representations
model description we follow liu et al
in treating the eration of lead wikipedia sections as a document summarization task
the input to a pothetical system is the title of a wikipedia cle and a collection of source documents while the output is the wikipedia article s rst section
source documents are webpages cited in the erences section of the wikipedia article and the top search results returned by google with the title of the article as the query
since source documents could be relatively long they are split into multiple paragraphs by line breaks
more formally given title t and l input paragraphs pl retrieved from wikipedia citations and a search engine the task is to generate the lead section d of the wikipedia article
our summarization system is illustrated in ure
since the input paragraphs are numerous and possibly lengthy instead of directly applying an abstractive system we rst rank them and marize the ones
our summarizer follows the very successful encoder decoder architecture bahdanau et al
where the encoder codes the input text into hidden representations and the decoder generates target summaries based on these representations
in this paper we focus exclusively on the encoder part of the model our decoder follows the transformer architecture











ranked paragraphssource paragraphsparagraph rankerencoderpara l para ldecoderabstractive summarizertarget summary gold target text d as yi
in testing input graphs are ranked based on the model predicted scores and an ordering rl is ated
the rst paragraphs are selected as input to the second abstractive stage

paragraph encoding instead of treating the selected paragraphs as a very long sequence we develop a cal model based on the transformer architecture vaswani et al
to capture inter paragraph relations
the model is composed of several cal and global transformer layers which can be stacked freely
let tij denote the j token in the i ranked paragraph ri the model takes vectors ij for all tokens as input
for the l former layer the input will be and the output is written as xl ij ij


embeddings input tokens are rst represented by word dings
let wij rd denote the embedding signed to tij
since the transformer is a recurrent model we also assign a special tional embedding peij to tij to indicate the sition of the token within the input
to calculate positional embeddings we follow vaswani et al
and use sine and cosine tions of different frequencies
the embedding ep for the th element in a sequence is d d where indicates the i th dimension of the bedding vector
because each dimension of the positional encoding corresponds to a sinusoid for any xed offset o can be represented as a linear function of ep which enables the model to distinguish relative positions of input elements
in multi document summarization token tij has two positions that need to be considered namely i the rank of the paragraph and j the position of the token within the paragraph
positional embedding peij rd represents both positions via concatenation and is added to word ding wij to obtain the nal input vector ij peij ei ij wij peij

local transformer layer a local transformer layer is used to encode textual information for tokens within each graph
the local transformer layer is the same as the vanilla transformer layer vaswani et al
and composed of two sub layers h xl where layernorm is layer normalization posed in ba et al
mhatt is the head attention mechanism introduced in vaswani et al
which allows each token to attend to other tokens with different attention tions and ffn is a two layer feed forward work with relu as hidden activation function


global transformer layer a global transformer layer is used to exchange formation across multiple paragraphs
as shown in figure we rst apply a multi head pooling eration to each paragraph
different heads will code paragraphs with different attention weights
then for each head an inter paragraph attention mechanism is applied where each paragraph can collect information from other paragraphs by attention generating a context vector to capture contextual information from the whole input
nally context vectors are concatenated linearly transformed added to the vector of each token and fed to a feed forward layer updating the resentation of each token with global information
multi head pooling to obtain xed length paragraph representations we apply a pooling operation instead of using only one resentation for each paragraph we introduce a multi head pooling mechanism where for each paragraph weight distributions over tokens are calculated allowing the model to exibly encode paragraphs in different representation subspaces by attending to different words
let ij rd denote the output vector of the last transformer layer for token tij which is used as input for the current layer
for each paragraph ri for head z nhead we rst form the input vectors into attention scores az ij and value vectors bz ij
then for each head we calculate a probability distribution az ij over tokens within the paragraph based on attention scores az ij w z ij w bz a ij ij n ij az ij a and w z rdheadd are where w z weights
dhead d nhead is the dimension of each head
n is the number of tokens in ri
we next apply a weighted summation with other linear transformation and layer tion to obtain vector headz i for the paragraph headz i z c ijbz az ij n where w z rdheaddhead is the weight
the model can exibly incorporate multiple heads with each paragraph having multiple tention distributions thereby focusing on different views of the input
inter paragraph attention we model the pendencies across multiple paragraphs with an inter paragraph attention mechanism
similar to self attention inter paragraph attention allows for each paragraph to attend to other paragraphs by calculating an attention distribution w z qz kz i w z i w z vz m q headz i headz i v headz i t kz i t kz i vz contextz i vz i i kz rdheaddhead are query where key and value vectors that are linearly formed from headz as in vaswani et al
i rdhead represents the context contextz tor generated by a self attention operation over all paragraphs
m is the number of input graphs
figure provides a schematic view of inter paragraph attention
feed forward networks we next update token representations with contextual information
we rst fuse information from all heads by nating all context vectors and applying a linear transformation with weight wc rdd i contextnhead ci i figure a global transformer layer
different ors indicate different heads in multi head pooling and inter paragraph attention
we then add ci to each input token vector and feed it to a two layer feed forward network with relu as the activation function and a way layer normalization on top ij gij ij ci ij xl ij where rdf d and rddf are the weights df is the hidden size of the feed forward later
this way each token within paragraph ri can collect information from other paragraphs in a hierarchical and efcient manner


graph informed attention the inter paragraph attention mechanism can be viewed as learning a latent graph representation self attention weights of the input paragraphs
although previous work has shown that lar latent representations are benecial for stream nlp tasks liu and lapata kim et al
williams et al
niculae et al
fernandes et al
much work in multi document summarization has taken tage of explicit graph representations each ing on different facets of the summarization task multi head poolingmulti head poolinghead paragraph attentioninter paragraph attentioninter paragraph attentioncontext thisisparaonefeed forwardfeed forwardfeed forwardfeed forwardcontext thisisparatwofeed forwardfeed forwardfeed forwardfeed forwardthisisparaonethisisparatwo e

capturing redundant information or senting passages referring to the same event or entity
one advantage of the hierarchical former is that we can easily incorporate graphs ternal to the model to generate better summaries
we experimented with two well established graph representations which we discuss briey low
however there is nothing inherent in our model that restricts us to these any graph eling relationships across paragraphs could have been used instead
our rst graph aims to capture lexical relations graph nodes correspond to graphs and edge weights are cosine similarities based on tf idf representations of the paragraphs
our second graph aims to capture discourse it builds an lations christensen et al
approximate discourse graph adg yasunaga et al
over paragraphs edges between graphs are drawn by counting a co occurring tities and discourse markers e

however nevertheless connecting two adjacent paragraphs see the appendix for details on how adgs are constructed
we represent such graphs with a matrix g where is the weight of the edge connecting paragraphs i and
we can then inject this graph into our hierarchical transformer by simply tuting one of its learned heads with g
tion for calculating the context vector for this head is modied as m gio experimental setup wikisum dataset we used the scripts and urls provided in liu et al
to crawl wikipedia articles and source reference documents
we cessfully crawled
of the original documents some urls have become invalid and ing documents could not be retrieved
we ther removed clone paragraphs which are exact copies of some parts of the wikipedia articles these were paragraphs in the source documents whose bigram recall against the target summary was higher than

on average each input has paragraphs and each paragraph has
tokens
the average length of the target mary is
tokens
we split the dataset with instances for training for dation and for test
methods rouge l recall similarity

ranking





table rouge l recall against target summary for paragraphs obtained with tf idf cosine ity and our ranking model
for both ranking and summarization stages we encode source paragraphs and target maries using subword tokenization with piece kudo and richardson
our lary consists of subwords and is shared for both source and target
paragraph ranking to train the regression model we calculated the recall lin of each paragraph against the target mary and used this as the ground truth score
the hidden size of the two lstms was set to and dropout with dropout probability of
was used before all linear layers
adagrad duchi et al
with learning rate
is used for optimization
we compare our ranking model against the method proposed in liu et al
who use the tf idf cosine similarity between each paragraph and the article title to rank the input paragraphs
we take the rst paragraphs from the ordered paragraph set produced by our ranker and the similarity based method respectively
we concatenate these paragraphs and calculate their rouge l recall against the gold target text
the results are shown in table
we can see that our ranker effectively extracts related paragraphs and produces more informative input for the stream summarization task
training conguration in all abstractive els we apply dropout with probability of
fore all linear layers label smoothing szegedy et al
with smoothing factor
is also used
training is in traditional sequence to sequence manner with maximum likelihood estimation
the optimizer was adam kingma and ba with learning rate of
and
we also applied learning rate warmup over the rst steps and decay as in vaswani et al

all transformer based models had den units the feed forward hidden size was for all layers
all models were trained on gpus nvidia titan xp for steps
we used model lead lexrank ft tokens no ranking ft tokens ft tokens ft tokens t dmca tokens ht tokens ht tokens similarity graph ht tokens discourse graph ht train on tokens test on tokens rouge l
































table test set results on the wikisum dataset using rouge
gradient accumulation to keep training time for all models approximately consistent
we selected the best checkpoints based on performance on the validation set and report averaged results on the test set
during decoding we use beam search with beam size and length penalty with
wu et al
we decode until an end of sequence token is reached
comparison systems we compared the transformer against several posed hierarchical strong baselines lead is a simple baseline that concatenates the tle and ranked paragraphs and extracts the rst k tokens we set k to the length of the ground truth target
lexrank erkan and radev is a used graph based extractive summarizer we build a graph with paragraphs as nodes and edges weighted by tf idf cosine similarity we run a pagerank like algorithm on this graph to rank and select paragraphs until the length of the ground truth summary is reached
flat transformer ft is a baseline that applies a transformer based encoder decoder model to a at token sequence
we used a layer transformer
the title and ranked paragraphs were concatenated and truncated to and tokens
t dmca is the best performing model of liu et al
and a shorthand for transformer decoder with memory compressed tion they only used a transformer decoder and compressed the key and value in attention with a convolutional layer
the model has layers as in liu et al

its hidden size is and its feed forward hidden size is
the title and ranked paragraphs were concatenated and truncated to tokens
hierarchical transformer ht is the model proposed in this paper
the model tecture is a layer network with attention layers at the bottom and global tention layers at the top
the model takes the title and paragraphs as input to produce a target summary which leads to proximately input tokens per instance
results automatic evaluation we evaluated rization quality using rouge lin
we report unigram and bigram overlap and as a means of assessing mativeness and the longest common subsequence rouge l as a means of assessing uency
table summarizes our results
the rst block in the table includes extractive systems lead lexrank the second block includes eral variants of flat transformer based models ft t dmca while the rest of the table presents the results of our hierarchical transformer ht
as can be seen abstractive models generally perform extractive ones
the flat transformer achieves best results when the input length is set to tokens while longer input i
e
kens actually hurts performance
the transformer with input tokens model
ht ht pp
ht mp
ht gt




rl



qa model
lead ft
t dmca

ht rating



table hierarchical transformer and versions thereof without paragraph position pp multi head pooling mp and global transformer layer gt
table system scores based on questions answered by amt participants and summary quality rating
forms ft and even t dmca when the latter is presented with tokens
adding an external graph also seems to help the summarization cess
the similarity graph does not have an vious inuence on the results while the discourse graph boosts rouge l by

we also found that the performance of the erarchical transformer further improves when the model is presented with longer input at test time
as shown in the last row of table when ing on input tokens summarization quality improves across the board
this suggests that the model can potentially generate better summaries without increasing training time
table summarizes ablation studies aiming to assess the contribution of individual components
our experiments conrmed that encoding graph position in addition to token position within each paragraph is benecial see row pp as well as multi head pooling mp is a model where the number of heads is set to and the global transformer layer gt is a model with only local transformer layers in the encoder
human evaluation in addition to automatic evaluation we also assessed system performance by eliciting human judgments on randomly lected test instances
our rst evaluation study quantied the degree to which summarization models retain key information from the documents following a question answering qa paradigm clarke and lapata narayan et al

we created a set of questions based on the gold summary under the assumption that it contains the most important information from the input graphs
we then examined whether participants were able to answer these questions by reading system summaries alone without access to the gold summary
the more questions a system can swer the better it is at summarization
we ated questions in total varying from two to was not the case with the other transformer models
four questions per gold summary
examples of questions and their answers are given in table
we adopted the same scoring mechanism used in clarke and lapata i
e
correct answers are marked with partially correct ones with
and otherwise
a system s score is the average of all question scores
our second evaluation study assessed the all quality of the summaries by asking pants to rank them taking into account the lowing criteria informativeness does the mary convey important facts about the topic in question fluency is the summary uent and grammatical and succinctness does the mary avoid repetition
we used best worst ing louviere et al
a less labor intensive alternative to paired comparisons that has been shown to produce more reliable results than rating scales kiritchenko and mohammad
ticipants were presented with the gold summary and summaries generated from out of systems and were asked to decide which summary was the best and which one was the worst in relation to the gold standard taking into account the criteria mentioned above
the rating of each system was computed as the percentage of times it was chosen as best minus the times it was selected as worst
ratings range from worst to best
both evaluations were conducted on the zon mechanical turk platform with responses per hit
participants evaluated summaries duced by the lead baseline the flat transformer t dmca and our hierarchical transformer
all evaluated systems were variants that achieved the best performance in automatic evaluations
as shown in table on both evaluations participants overwhelmingly prefer our model ht
all wise comparisons among systems are statistically signicant using a one way anova with tukey hsd tests p

examples of system output are provided in table
pentagoet archeological district the pentagoet archeological district is a national historic landmark district located at the southern edge of the bagaduce peninsula in castine maine
it is the site of fort pentagoet a century fortied trading post established by fur traders of french acadia
from to this site was a center of trade with the local abenaki and marked the effective western border of acadia with new england
from to the site was under english control after which it was returned to france by the treaty of breda
the fort was destroyed in by dutch raiders
the site was designated a national historic landmark in
it is now a public park
a national historic landmark district what is the pentagoet archeological district castine maine where is it located what did the abenaki indians use the site for trading center the pentagoet archeological district is a national historic landmark district located in castine maine
this district forms part of the traditional homeland of the abenaki indians in particular the penobscot tribe
in the colonial period abenakis frequented the fortied trading post at this site bartering moosehides sealskins beaver and other furs in exchange for european commodities
pentagoet archeological district is a national historic landmark district located at the southern edge of the bagaduce peninsula in treaty of breda
the pentagoet archeological district is a national historic landmark district located at the southern edge of the bagaduce peninsula in treaty of breda
it was listed on the national register of historic places in
the pentagoet archeological district is a national historic landmark district located in castine maine
this district forms part of the traditional homeland of the abenaki indians in particular the penobscot tribe
the district was listed on the national register of historic places in
the pentagoet archeological district is a national historic landmark district located in castine maine
this district forms part of the traditional homeland of the abenaki indians in particular the penobscot tribe
in the colonial period abenaki frequented the fortied trading post at this site bartering moosehides sealskins beaver and other furs in exchange for european commodities
d l o g a q d a e l t f a c m d t t h melanesian whistler d the melanesian whistler or vanuatu whistler pachycephala chlorura is a species of passerine bird in the l whistler family pachycephalidae
it is found on the loyalty islands vanuatu and vanikoro in the far o g eastern solomons
where is it found a what is the melanesian whistler a species of passerine bird in the whistler family pachycephalidae q loyalty islands vanuatu and vanikoro in the far south eastern solomons d the australian golden whistler pachycephala pectoralis is a species of bird found in forest woodland mallee a mangrove and scrub in australia except the interior and most of the north most populations are resident but e l some in south eastern australia migrate north during the winter
t the melanesian whistler p
caledonica is a species of bird in the family muscicapidae
it is endemic to f melanesia
the australian golden whistler pachycephala chlorura is a species of bird in the family pachycephalidae which is endemic to fiji
a c m d t t the melanesian whistler pachycephala chlorura is a species of bird in the family pachycephalidae which is h endemic to fiji
table gold human authored summaries questions based on them answers shown in square brackets and automatic summaries produced by the baseline the flat transformer ft t dmca liu et al
and our hierachical transformer ht
conclusions in this paper we conceptualized abstractive document summarization as a machine learning problem
we proposed a new model which is able to encode multiple input documents chically learn latent relations across them and ditionally incorporate structural information from well known graph representations
we have also demonstrated the importance of a learning based approach for selecting which documents to marize
experimental results show that our model produces summaries which are both uent and formative outperforming competitive systems by a wide margin
in the future we would like to apply our hierarchical transformer to question answering and related textual inference tasks
acknowledgments we would like to thank laura perez beltrachini for her help with preprocessing the dataset
this research is supported by a google phd ship to the rst author
the authors gratefully knowledge the nancial support of the european research council award number
references jimmy lei ba jamie ryan kiros and geoffrey e arxiv preprint ton

layer normalization


dzmitry bahdanau kyunghyun cho and yoshua gio

neural machine translation by jointly learning to align and translate
in in proceedings of the international conference on learning resentations san diego california
regina barzilay and kathleen r
mckeown

sentence fusion for multidocument news rization
computational linguistics
lidong bing piji li yi liao wai lam weiwei guo and rebecca passonneau

abstractive document summarization via phrase selection and merging
in proceedings of the annual ing of the association for computational linguistics and the international joint conference on ral language processing volume long papers pages beijing china
asli celikyilmaz antoine bosselut xiaodong he and yejin choi

deep communicating agents for in proceedings of the abstractive summarization
conference of the north american chapter of the association for computational linguistics man language technologies volume long pers pages new orleans louisiana
janara christensen mausam stephen soderland and towards coherent oren etzioni

in proceedings of the document summarization
conference of the north american chapter of the association for computational linguistics man language technologies pages lanta georgia
association for computational guistics
eric chu and peter j liu

unsupervised neural multi document abstractive summarization
arxiv preprint

james clarke and mirella lapata

discourse constraints for document compression
tional linguistics
john duchi elad hazan and yoram singer

adaptive subgradient methods for online learning journal of machine and stochastic optimization
learning research
katja filippova and michael strube

sentence fusion via dependency graph compression
in ceedings of the conference on empirical ods in natural language processing pages honolulu hawaii
sebastian gehrmann yuntian deng and alexander rush

bottom up abstractive summarization
in proceedings of the conference on cal methods in natural language processing pages brussels belgium
max grusky mor naaman and yoav artzi

newsroom a dataset of
million summaries with diverse extractive strategies
in proceedings of the conference of the north american chapter of the association for computational linguistics man language technologies volume long pers pages new orleans louisiana
karl moritz hermann tomas kocisky edward grefenstette lasse espeholt will kay mustafa leyman and phil blunsom

teaching chines to read and comprehend
in advances in ral information processing systems pages
curran associates inc
sepp hochreiter and jurgen schmidhuber

neural computation long short term memory

yoon kim carl denton luong hoang and der m rush

structured attention networks
in proceedings of the international conference on learning representations toulon france
diederik p kingma and jimmy ba

adam a method for stochastic optimization
arxiv preprint

thomas n kipf and max welling

supervised classication with graph convolutional in proceedings of the international networks
conference on learning representations san juan puerto rico
svetlana kiritchenko and saif mohammad

best worst scaling more reliable than rating scales a case study on sentiment intensity annotation
in proceedings of the annual meeting of the sociation for computational linguistics pages vancouver canada
gunes erkan and dragomir r radev

lexrank graph based lexical centrality as salience in text summarization
journal of articial intelligence search
taku kudo and john richardson

sentencepiece a simple and language independent subword enizer and detokenizer for neural text processing
arxiv preprint

patrick fernandes miltiadis allamanis and marc brockschmidt

structured neural tion
in proceedings of the international ference on learning representations new orleans louisiana
logan lebanoff and fei liu

automatic tion of vague words and sentences in privacy cies
in proceedings of the conference on pirical methods in natural language processing pages brussels belgium
chin yew lin

rouge a package for automatic in text summarization evaluation of summaries
branches out proceedings of the shop pages barcelona spain
association for computational linguistics
peter j liu mohammad saleh etienne pot ben goodrich ryan sepassi lukasz kaiser and noam shazeer

generating wikipedia by ing long sequences
in proceedings of the national conference on learning representations vancouver canada
yang liu and mirella lapata

learning tured text representations
transactions of the ciation for computational linguistics
jordan j louviere terry n flynn and anthony fred john marley

best worst scaling ory methods and applications
cambridge sity press
shulei ma zhi hong deng and yunlun yang

an unsupervised multi document summarization in framework based on neural document model
proceedings of coling the tional conference on computational linguistics technical papers pages osaka japan
shashi narayan shay b
cohen and mirella lapata

ranking sentences for extractive tion with reinforcement learning
in proceedings of the conference of the north american ter of the association for computational linguistics human language technologies volume long pers pages new orleans louisiana
vlad niculae andre f
t
martins and claire cardie

towards dynamic computation graphs via sparse latent structure
in proceedings of the conference on empirical methods in natural guage processing pages brussels gium
daraksha parveen and michael strube

document summarization using bipartite graphs
in proceedings of the workshop on graph based methods for natural language cessing pages doha qatar
abigail see peter j
liu and christopher d
manning

get to the point summarization with generator networks
in proceedings of the nual meeting of the association for computational linguistics volume long papers pages vancouver canada
christian szegedy vincent vanhoucke sergey ioffe jon shlens and zbigniew wojna

rethinking the inception architecture for computer vision
in the ieee conference on computer vision and tern recognition cvpr
ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan n gomez ukasz kaiser and illia polosukhin

attention is all you need
in advances in neural information cessing systems pages
curran ciates inc
xiaojun wan

an exploration of document impact on graph based multi document tion
in proceedings of the conference on pirical methods in natural language processing pages honolulu hawaii
adina williams andrew drozdov and samuel r
bowman

do latent tree learning models tify meaningful structure in sentences tions of the association for computational tics
yonghui wu mike schuster zhifeng chen quoc v le mohammad norouzi wolfgang macherey maxim krikun yuan cao qin gao klaus macherey et al

google s neural machine translation system bridging the gap between in arxiv preprint man and machine translation


michihiro yasunaga rui zhang kshitijh meelu ayush pareek krishnan srinivasan and dragomir radev

graph based neural multi document summarization
in proceedings of the ence on computational natural language learning conll pages vancouver canada
romain paulus caiming xiong and richard socher

a deep reinforced model for abstractive marization
in proceedings of the international conference on learning representations ver canada
jianmin zhang jiwei tan and xiaojun wan

adapting neural single document summarization model for abstractive multi document tion a pilot study
in proceedings of the tional conference on natural language generation
dragomir radev

a common theory of mation fusion from multiple text sources step one cross document structure
in sigdial workshop on discourse and dialogue pages hong kong china
evan sandhaus

the new york times annotated corpus
linguistic data consortium philadelphia
a appendix we describe here how the similarity and discourse graphs discussed in section

were created
these graphs were added to the hierarchical former model as a means to enhance summary quality see section for details
a
similarity graph the similarity graph s is based on tf idf cosine similarity
the nodes of the graph are paragraphs
we rst represent each paragraph pi as a bag of words
then we calculate the tf idf value vik for each token tik in a paragraph if two paragraphs pi are adjacent in one source webpage and they are connected with one of the above discourse markers will be otherwise it will be
the nal edge weight is the weighted sum of and
vik nd where n is the count of word t in the graph nd is the total number of paragraphs and is the total number of paragraphs taining the word
we thus obtain a tf idf vector for each paragraph
then for all paragraph pairs pi we calculate the cosine similarity of their tf idf vectors and use this as the weight for the edge connecting the pair in the graph
we remove edges with weights lower than

a
discourse graphs to build the approximate discourse graph adg d we follow christensen et al
and yasunaga et al

the original adg makes use of several complex features
here we create a simplied version with only two features nodes in this graph are again paragraphs
co occurring entities for each paragraph pi we extract a set of entities ei in the paragraph using the ner recognizer
we only use entities with type person norp fac org gpe loc event work of art law
for each paragraph pair pi pj we count eij the number of entities with exact match
discourse markers we use the following plicit discourse markers to identify edges between two adjacent paragraphs in a source webpage again also another comparatively thermore at the same time however mediately indeed instead to be sure likewise meanwhile moreover theless nonetheless notably otherwise regardless similarly unlike in addition even in turn in exchange in this case in any event nally later as well cially as a result example in fact then the day before
io api entityrecognizer
