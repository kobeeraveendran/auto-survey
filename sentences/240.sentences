on semi supervised multiple representation behavior learning ruqian lua shengluan houa c ainstitute of computing technology key lab of iip chinese academy of sciences beijing china bacademy of mathematics and systems sciences key lab of madis chinese academy of sciences beijing china cuniversity of chinese academy of sciences beijing china t c o l c
s c v
v i x r a abstract we propose a novel paradigm of semi supervised learning ssl the semi supervised multiple representation havior learning ssmrbl
ssmrbl aims to tackle the diculty of learning a grammar for natural language parsing where the data are natural language texts and the labels for marking data are parsing trees grammar rule pieces
we call such labels as compound structured labels which require a hard work for training
ssmrbl is an incremental learning process that can learn more than one representation which is an appropriate solution for dealing with the scarce of labeled training data in the age of big data and with the heavy workload of learning compound structured labels
we also present a typical example of ssmrbl regarding behavior learning in form of a ical approach towards domain based multiple text summarization dbmts
dbmts works under the framework of rhetorical structure theory rst
ssmrbl includes two representations text embedding for representing tion contained in the texts and grammar model for representing parsing as a behavior
the rst representation was learned as embedded digital vectors called impacts in a low dimensional space
the grammar model was learned in an iterative way
then an automatic domain oriented multi text summarization approach was proposed based on the two representations discussed above
experimental results on large scale chinese dataset sogouca indicate that the proposed method brings a good performance even if only few labeled texts are used for training with respect to our dened automated metrics
keywords semi supervised representation learning semi supervised multiple representation behavior learning incremental learning modular learning rhetorical structure theory multi text summarization lexical core
introduction usually machine learning programmers are embarrassed by the scarce of labeled training data
experts then said do nt worry
there are lots of source data which are unlabeled but useful
with this idea people have invented semi supervised learning ssl which detects some common features of labeled and unlabeled examples to help determine the model characteristics
there are many dierent types of ssl
to name a few we have seen supervised classication learning sscl semi supervised representation learning ssrl semi supervised reinforcement learning ssrinfl semi supervised behavior learning ssbl
there is another situation which leads to the invention and development of ssl
this is not because of the lack of a massive set of training data but because of the diculty or complexity of training a model or more precisely because of the diculty or complexity of training each sample of a model
examples include learning how to play go by analyzing a set of go playing records learning how to prove a mathematical theorem by reading through a set of mathematical theorems proves learning how to command a campaign by reading a few historical records of military campaigns and nally as we will introduce in a later section of this paper how to learn a grammar of natural languages in a certain domain by manually parsing a limited set of natural language texts
the common property of these examples is the enormous cost of training a single example such that usually one can not tolerate this high cost
corresponding author email addresses
ac
cn ruqian lu
com shengluan hou preprint submitted to elsevier october fig

the relations between dierent ssl schemas generally speaking an ssl algorithm of the above type usually presents a multi lateral style
take the military campaign as an example
it tells ocers how to conduct a campaign
this concerns human behavior so it is ssbl
ocers always learn from their war practices
they get positive or negative lessons from practice
their experiences should be fostered through many practical combats so it is ssrinfl
ocers have to write strategic and tactic plans for each campaign before it and summaries after it
the general lessons acquired from these materials will be written into textbooks for training latter ocers so it is ssrl
technically an ssl algorithm of the above type is often more dicult and complicated to design and to deal with
firstly it concerns complicated problems which generally do nt have xed and unique solutions
on the contrary the solutions are usually heuristic and probabilistic even if a massive data set has been used for training
secondly the massive data set to be trained is usually growing steadily a big data stream
so an incremental approach is needed
thirdly it usually involves multi factors multi views such that a single criterion is not enough for deciding the resulting model
fourthly given that the problem is of ssrl type the algorithm is usually not limited to nd a single representation
in that case although a rst representation is learned the result is not yet the wanted nal result
often is a second or even more representations are needed which play dierent roles in the learning process
we call this paradigm as semi supervised multiple representation behavior learning ssmrbl
in this paper we will present an example of ssmrbl
it is a project about domain based multiple text tion dbmts which we have been working on since last years
given a set ds of natural language texts from some application domain d
a limited subset t is selected from ds as training data
each text of t will be parsed manually to get a parsing tree based on domain knowledge
during this process the programmer s behavior will be collected by computer to form a grammar where the tree structures form a basis for reduction rules and the contexts of parsing will be recorded for building preference rules for resolving shift reduce and reduce reduce conicts
the parsing tree of each text together with the set of reduction and preference rules contributed by it are considered as label of this text
the whole set of these label s forms a grammar considered as representation of the training set
thus the learning is semi supervised
to cope with unlabeled data we introduce another representation a digital vector space for text embedding
it makes use of closeness concept of the digital space to select the most representative texts for initial labeling to transfer labels to unlabeled data and to select contents for automatic summarization
in order to put forward the special characteristics of our approach we also call it as semi supervised behavioral representation learning because learning a grammar is learning the way as how to analyze the texts of a particular domain
the relations between dierent ssl schemas are illustrated in fig

the contributions of this work are summarized as follows we propose ssmrbl a novel paradigm about incremental multiple representation learning that can learn more than one representation according to specic tasks
ssmrbl is an appropriate solution for dealing with the scarce of labeled data
under the framework of ssmrbl we give an instance a project about dbmts
dbmts here is a oriented multi text summarization architecture which rst parse texts into discourse trees and then extract summary units using a representative selection strategy
we also incorporate rhetorical structure theory rst into dbmts such that the learned grammar model is tributed rhetorical structure grammar arsg and the basic processing unit is elementary discourse unit edu which is a key concept in rst that often corresponds to a clause or a simple sentence
to investigate the factors involved in representation learning and summary generation we perform extensive experiments propose novel automated metrics and report ndings on them
the remainder of this paper is organized as follows
section is related works
in section learning of text embedding i
e
the rst representation is detailed
section and section are about attributed rhetorical structure grammar and how to obtain the initial grammar model
in section we learn the grammar model in an iterative way
domain oriented multi text summarization is introduced in section
experimental results and analysis are in section
we conclude this paper in section

related works our research builds on previous works in the eld of semi supervised learning representation learning rhetorical structure theory and multi text summarization
semi supervised learning aims to improve generalization on supervised tasks using unlabeled data
ssl usually combine unlabeled data with very smaller set of labeled data to gain better data representation or classication accuracy
there are wide range of applications such as text classication discourse relation classication and object detection
based on both labeled and unlabeled instances the problem of ssl is dened as learning a function from labeled data to unlabeled data
ssl contains two learning paradigms transductive learning and inductive learning
transductive learning directly apply the function on the unlabeled but observed instances at training time
on the other hand the aim of inductive learning is to learn a parameterized function that is generalizable to unobserved instances
the basic assumption of ssl is that nearby nodes tend to have the same labels
representation learning is the process of learning representations of the data that make it easier to extract useful information when building classiers or other predictors
a word embedding which was rst introduced by bengio et al
is a continuous vector representation that captures semantic and syntactic information about a word
word embeddings have proven useful in many nlp tasks
embeddings for longer texts phrase sentence and even texts are also necessary in many circumstances
for deriving them based on word embedding there are many methods
for instance vector addition a simple way that consider it as the sum of all words without regarding word orders recurrent neural network a text embedding is the concatenation of the output states of a rnn or bidirectional rnn over its word embeddings in which lstm or gru is commonly used convolutional neural network a series of convolutional feature maps are used over word embedding matrix the text representation is the concatenation of all the convolutional lter outputs after max pooling over time
note that rhetorical structure theory rst is a theory proposed by mann and thompson in last century for text structuring
rst aims to investigate how clauses sentences and even text spans connect together into a whole in a logical and topological way
rst explains text coherence by postulating a hierarchical connected tree structure denote as rs tree for a given text in which every part has a role a function to play with respect to other parts in the text
each leaf node in an rs tree is called an elementary discourse unit edu
rst assumes that each edu or text span of a natural language sentence or text may either be a nucleus or a satellite
an rs tree is a binary tree
its two branches are either a pair nucleus satellite or two nuclei
rhetorically the nucleus is always in the center and considered as more signicant while the satellite is used to modify the nucleus and as such is less signicant
rst has been widely used in natural language analysis understanding and also generation that need to combine meanings of larger text units
according to the number of input texts text summarization can be broadly categorized into single text tion sds and multi text summarization mds
mds is more complicated since there are more problems should be tackled such as contradiction redundancy and complementarity when generating summarization from multiple texts
for decades there are various kinds of approaches have been proposed including graph based methods lexical chain based methods constraint optimization based methods traditional machine learning based methods and deep learning based methods
deep learning based approaches are able to age large scale training data and have achieved competitive or better performance than traditional methods with the availability of large scale corpora
the above methods on the other hand can also be grouped into abstractive ones and extractive counterparts
in contrast to extractive summarization where a summary is composed of a subset of tences or words lifted from the input texts abstractive summarization concerns the generation of new sentences new phrases while retaining the same meaning as the same source have which are more complex than extractive ones
extractive approaches are now the mainstream ones
this work is a novel framework that mainly based on the above related works which attempts to teach the system to parse texts into discourse structure trees based on small set of human annotated texts and then to determine whether an edu should be selected to produce the nal summary of multiple texts
in this way the summary generated by our method is the extractive one

text embedding learn the first representation the rst representation of domain texts is given as a vector set in a target digital space
each text of the corpus is mapped to a vector in this space
two texts are similar if their mappings in this space are close neighbors
usually a domain related text introduces a review on the current state of the related domain
the major content of such texts is manifested by comments and reviews consisting of a set of domain concepts and expressions which also form the basis of grammar reduction in our arsg
in this paper a lexical core is dened as a quadruple of domain concepts for representing domain news or reviews
its semantics can be summarized as when who where what how which are the well known ve ws factors for news reports
following is an example
assume we have the following text news january today imf noticed a downward risk of global economy
the growth has surpassed its peak value
imf has reduced growth anticipation of developed new markets and developing economies resp
a uno s report pointed out that usa s economy growth rate will be going down to
in and
in while eu s growth rate will continue to be and china s rate from
will go down to

this text contains the following lexical cores imf notice global economy downward risk imf anticipation growth of economies reduced usa economy growth rate going down eu economy growth rate continue china economy growth rate go down after careful analysis on many articles about news reports we found that most of the articles have time tion
time information often present in the beginning of these articles most of which have only one time information and a small number of them have one more time changes
these time changes imply the object changes of an event in most cases
table shows the excerpts of news articles
on the other hand time information is also the important component about the domain news or reviews with respect to its semantics
it s useful and necessary to import time information into lexical cores
based on the above observations we empirically take time information as a component of the lexical core
the procedure of embedding is as follows the computer scans each text to extract all its lexical cores
we use q t a o s to denote any lexical core where t a o s means time agent object and state change respectively
since our attention is focused on domain oriented texts the design of lexical cores has in particular taken those terms in consideration which occur most frequently in texts of type domain news
for example we will put more weight to the ve ws
among the four components of a lexical core t a o s corresponds to when who or where what and how resp
each of these components is represented as a phrase such as price of rice
an unied function g maps each lexical core to a vector in a w dimensional space r with phrase embedding technique the requirement that a o s where is a vector in r while are its four components each of which is a w dimensional vector where w is the dimension size of phrase embedding
given that each lexical core is represented by a vector in space r and each text usually provides several lexical cores the embedding of the whole text is represented by the addition called impact of all vectors representing its lexical cores
although the lexical core and impact vectors are in the same space r we consider them as being in two separate spaces for easing the discussion
we call the space for lexical cores impacts as core space impact space resp
chinese text











cpi




table excerpts of news reports english translation on september chen xiongxiong deputy minister of miit was invited to be the guest of the zhongpu forum and gave a report entitled accelerating the digital tion of manufacturing industry
chen shaoxiong pointed out that since the founding of china our country s manufacturing industry has maintained sustained and rapid development and made outstanding contributions to china s economic construction
especially since the tional congress of the communist party of china the turing industry has achieved historic breakthroughs and achieved its comprehensive strength has been historic achievements
steadily improved its innovation capability has been signicantly enhanced its industrial structure has been accelerated its opment environment has been continuously optimized and its openness has been greatly improved
solid foundation has been laid for achieving the two hundred years goal






the latest monitoring data of the tianjin investigation team of national statistical bureau shows that the cpi of the city in gust increased by
year on year
among them food prices rose by
non food prices rose by
consumer prices rose by
and service prices rose by

from january to august the city s average cpi increased by
over the same period of the previous year
in august the city s cpi rose by

among them food prices rose by
non food prices rose by
consumer prices rose by
and service prices rose by

fig
is a simplied illustration of core space where we assume w and omit for the moment the rst component t of a core
note that if we only consider impacts of the cores then the space is an impact space
for the four components of the same lexical core the equation is fullled
thus the loss function for training lexical core embedding is t a o a o s w a o s t o s t a t a o l t a o t a o where correspond to corrupted t a o s respectively
fig
is an illustration of text embedding
each text is represented by an impact vector in the vector space whose coordinates are computed by the trained lexical cores according to algorithm
in this way the embedding for each domain text can be derived which is the basis of the following procedures
algorithm embed texts in a vector space input a set d of natural language texts from an application domain
extract lexical cores in form of t a o s from edus of each text embed each lexical core to a vector simply called core in a w dimensional space r by embedding the nents t a o s as the w coordinates of the lexical core where w is the dimension of phrase embedding for each text calculate the impact of all its core vectors
this impact is considered as the embedded mapping of the text
fig

the w dimensional space for cores and impacts as vectors
attributed rhetorical structure grammar a behavioral representation in a previous paper we have introduced an approach of attribute grammar based text summarization
the main idea is as follows
assume a large set of texts is given relating to an application domain d
build a domain knowledge base dkb of domain concepts and domain relations
consider them as terminal and non terminal symbols of a grammar
further use rhetorical relations as glue to build production rules from these terminal and non terminal grammar symbols text by text
then introduce dierent kinds of attributes including the rhetorical relations to make the grammar an attributed one
in this way we get an attributed rhetorical structure grammar
we start from the denition of a simple version without probability of this attribute grammar
denition
a simple attributed rhetorical structure grammar sarsg is a six tuple rs dre kcp rre at where rs is the start symbol dre the set of domain relations kcp the set of domain concepts rre the set of rhetorical relations at the synthesized attributes where each attribute is an arithmetical or logical function with grammar symbols as arguments
is the set of production rules attached with attribute equations rs dre the non terminals rre kcp the terminals
a production rule of has the following form ae where ae is the attribute equations denoting how the values of d s attributes are calculated from those of a and b attributes
d is the left side parent symbol a and b the right side child symbols
x y is n s or s n or n n where n means nucleus and s means satellite
a b d are domain relations l the reason a logic formula of conrming this rule as a legal production rule reducing the string ab to d under the current parsing context
impacts of lexical cores of domain texts impacts distributed in cubes fig

text embedding the rst representation the attribute equation ae is a function calculating attribute values of the production s left symbol d with attribute values of same production s right side as arguments
denition
an attributed rhetorical structure grammar arsg is a seven tuple rs dre kcp rre at where rs dre kcp rre and at are the same as in sarsg
is the set of probabilistic production rules attached with attributes and reasons see denition and below the set of shift reduce precedence rules
a production rule of has the following form ae n that means it is a production rule of sarsg followed by a positive integer n for calculating the probability of reducing ab to d dynamically in case reduction is required
more precisely if there are k productions ae ni i then the probability that the j th production will be chosen is equal to is a set of precedence tuples where each tuple is in the form n j ni
a b c sl fa b c ps or a b c rl fa b c pr where abc is a string of three neighboring grammar symbols during parsing
sl fa b c short for shift logic formula is a reason for shifting the parser over c while rl fa b c short for reduce logic formula is a reason for reducing a b to some dre
the truth values of both sl fa b c and rl fa b c depend on the attribute values of a b and c
ps and pr are probabilities with ps pr
thus the six tuples are used for resolving shift reduce conicts

training a sample set to get the initial grammar model the basic idea is as follows we rst decide a target application domain d
with the aid of some domain oriented dictionaries and thesaurus we build a knowledge base kb of d including domain concepts and domain relations where the domain concepts are abstract or concrete entities while domain relations are states or state changes of domain concepts
for example if the domain is world economy and trade wet then its domain concepts may be market price stock country bank
while ination balance improved expended
are its domain relations
given a set ds of texts in d we want to learn a grammar g see section from ds based on kb such that other texts of d may be parsed by g to produce a parsing tree for each text
all domain concepts relations are considered as terminal non terminal symbols of g which is context sensitive and designed as an attributed grammar where the rhetorical relations are the most important attributes
all grammar rules are binary with a rhetorical relation for parent node and attributes nucleus satellite or nucleus nucleus for the two child nodes
accordingly the parsing result is a binary tree parsing tree called an attributed rhetorical structure tree
given a subset ds of ds for training the process of parsing texts in ds is done by a cooperation of programmer and computer
this process consists of two sub processes
in the rst sub process the programmer mimics a machine compiler by scanning the text from left to right
he she decides a shift or a reduce whatever the right action is due which transforms the text as a sentential step by step bottom up
the parsing tree is then constructed level by level
at the same time the computer observes collects and records all information during every action done by the human programmer
in particular if the computer detects that the human shows dierent behaviors within the same parsing context it will produce a separate grammar rule for each behavior and record the number of its occurrences as a part of the rule
finally in the second sub process for each text of the training set there will be generated a parsing tree and a set of grammar rules
the computer synthesizes all these rule sets to form the wanted grammar
there may possibly be conicting rules which make the grammar a probabilistic one
the number of instances of each rule represents its weight
these numbers will be used during application of the grammar to calculate the probability for each rule to apply
more precisely in the rst sub process each time when the programmer makes a decision of shift or reduce the computer records this decision and the context of the sentential where the decision is made
if the decision is a reduce the machine produces a rough reduction rule as shown in a b where l f is the recorded context of the sentential during reduction
we call it the reason of reduction
at this time the human parser may or may not assign additional information to to enrich it into the form of
note that if this additional information is missing the default meaning can be represented as the following straight forward care care where strait forward means d accepts all attribute values of a and b do nt care means the rhetorical relations of a and b may take any valid values
note that no matter the decision is shift or reduce a rough preference rule is always produced by the computer
it has the following form a b c sl fa b c or a b c rl fa b c where sl fa b fa b c denotes the context when shift from a b to a b c reduction of a and b in front of c is made
the blank spaces in parentheses are saved for possible probability values
in the second sub process all instances of the same rule in form of or are collected to calculate the weight
regarding production rules in form of if the parent d of a and b is unique then rule gets the form ae it means the string ab always reduces to d if the decision is reduction
otherwise if there are several parents dk for reducing ab then we divide all such rules in k groups such that all rules in the i th group i share the same parent symbol di
assume the number of rule instances in the i th group is ni then we obtain k probabilistic rules this means if a reduction of ab is to be made during parsing the possibility of reducing ab to d j is equal to therefore it needs runtime calculation to decide which rule to take
n j ni
aek fk nk in the same way instances of can be divided in two groups a b c sl fa b c ns or a b c rl fa b c nr it means at the context between ab and c the computer has noted ns times of shift and nr times of reduce done by the human parser
this makes the generated grammar g a probabilistic one
when the computer parser of g meets the same context in any future application it may perform a shift reduce with probability ns
the decision will be made during compile time
nr
iterative grammar model development learn the second representation in section we have shown how to embed all texts of d in a w dimensional space where each lexical core is mapped to a vector called core and each text is mapped to an impact which is the sum of all its core vectors
in this section we will show how to learn the target representation an attributed rhetorical structure grammar
this will be illustrated in algorithm
algorithm learn an attributed rhetorical structure grammar input a set ds of natural language texts from an application domain d
perform algorithm to embed all texts of ds and their lexical cores to two sets of vectors impacts and cores in a w dimensional space r perform algorithm to select a most representative subset s of s where s is the set of all impacts for training and learn an initial model a starting attributed rhetorical structure grammar perform algorithm recursively to attach labels to s s s and to learn the target model an attributed rhetorical structure grammar for the whole set s thus also for whole ds
according to dierent strategies of representative selection algorithm contains two alternative versions rithm and algorithm
besides algorithm for learning an initial model with a xed threshold q of representative selection we provide also its alternative version algorithm which selects representatives based on number of population in each cube
this tactic is more suitable for the case where the size of cubes is rather big and the impacts within most cubes are rather far from each other
in algorithm the steps are the same as algorithm except its step which is replaced by the new step
algorithm learn an initial model using a control number as threshold input a set s of impacts distributed in the w dimensional space r
determine a w dimensional cube c with smallest edge lengths and such that c is just enough large to contain the whole set s determine four positive integers and to divide each cube in smaller cubes such that the lengths of small cube edges are and resp
count the numbers of impacts in each small cube calculate the divergence of each impact imp where the sum covers euclidian distances of all pairs a b of cores composing imp with a b
is the coordinates of the cube containing x determine a positive integer q such that from each cube randomly q impacts are selected or p impacts if it contains only p q impacts where the probability of selecting an impact is its divergence divided by the sum of divergences of all impacts in the same cube
remove them from the cubes let s be the set of all these other impacts ds be those domain texts embedded to s
ds is now the most representative set of texts and s the most representative impacts of s use ds as training set for performing procedure in to build an initial model a starting attributed rhetorical structure grammar
algorithm learn an initial model using a percentage value as threshold p q determine a positive integer q such that from each cube randomly impacts are selected where p is the total number of impacts in this cube where the probability of selecting an impact is its divergence divided by the sum of divergences of all impacts in the same cube
remove them from the cubes note that there are yet other strategies beyond algorithm and algorithm for selecting the most tative set of impacts for the initial model
it depends on the concrete situation and programmer s decision
the basic idea of this step is to cluster all unlabeled unparsed texts to the labeled ones according to their mutual distances
the closeness of two texts is measured according to the dierence of their impacts in r
in some areas of machine learning research it is not always necessary or possible to train the whole immense data relevant to the current topic
we assume that there is a limit controlling the learning algorithm
this limit may be a concrete number or a percentage value of how much data at most should be trained from the whole data corpus
this is important because as we said above training a label in our case may be quite expensive
we will discuss practical cases below in section
in particular only the training of those impacts mentioned in step of algorithm step in algorithm and step in algorithm should be counted for this threshold because they need manual processing of human
algorithm incrementally learn and improve the model input two constants and k
dene distance between any impact set m and impact e as in f for all m let s be the set of impacts selected in algorithm or algorithm s be the set of remaining impacts while the training limit is not yet reached do let s s e s if s then goto step call algorithm s to build an enhanced model let s e s s s s s s s let k if then select s s such that e s s s call algorithm s to build an increased model s s s s s s
end if end while the spatial division of impacts during algorithm s running is illustrated in fig

in algorithm we denote that text which has produced impact e as
algorithm enhance the model input s
for each e s do for each grammar rule r of do number of instances r end for end for the functions of these algorithms can be illustrated in fig

fig
is a more illustrative form of fig
where gr means a set of grammar rules means the number of instances of each rule in set will be increased by
notice please the dierence between model enhancement first round next round fig

learn the grammar iteratively the space division algorithm increase the model input s
perform the actions described in section with ds s to get a parsing tree and the corresponding set of grammar rules
and model increase in fig

in model enhancement the numbers of dierent grammar rule types are unchanged
only the numbers of rule instances are subject to change increase
however in case of model increase there will be possibly new rule types generated as we can see in fig

note that the concept of impacts is dierent from the concept centroid used in other literature such as radev et al
where centroid is dened as a group of words that statistically represent a cluster of texts while impacts in this paper are dened as vector sums of cores in the embedding space

multiple domain texts summarization from rhetorical trees in this section we show how the two representations discussed above work together to help automatic domain texts summarization
first we mention an algorithm in a previous paper of us for making summary of a single text where we established a grammar model for representing a text with its parsing tree which is an attributed rhetorical structure tree
algorithm summarize a single domain text a sketch input a rhetorical structure tree and the number n of requested summary length
traverse the tree according to the following principles it starts from the root of the tree it is top down
once a node is traversed the next one to be traversed is its child node whenever such node exists it is nucleus rst
when the traversal is going down the nucleus child will be selected rst it keeps going down until reaching a leaf which is the next signicant node which will be outputted it is balanced
whenever reaching a leaf it restarts traversal from the highest brother node of its ancestor
it stops when the requested number of edus have been outputted
algorithm performs a search on the rhetorical tree to obtain a priority sequence of edus leaves of the tree
the number beside each leaf shows its priority alphabetically in summary sequence selection
see fig

in the above sections we have described a novel technique of how to learn a natural language grammar in a supervised way for multiple text summarization
in the following we will show that even after the grammar is learned fig

semi supervised behavioral representation learning an overview fig

learn the grammar iteratively the second representation compare it with fig
there may be also tricks of applying the grammar during real summarization scripts
first we acknowledge that when the quantity of texts to be summarized is immense or when the scopes of the texts are very distributed we may want to grasp and summarize only those texts which provide the mainstream information in the given domain
this leads to the strategy of given up the isolated pieces of information and concentrating on the main stream information contained in a subset of given texts to be summarized
for this purpose we again make use of the embedding technique to select the most representative texts as shown by the following algorithm
it is worth noting that dierent from the usual approaches of summarizing all texts of a given text set our approach evaluates the signicance of each given text and summarizes only those texts above some signicance degree
by signicance of a text we mean its representativeness
this strategy makes the approach more ecient in particular in case where the amount of texts to be summarized is immense
the number n in algorithm is the number of edus rather than the number of words which is dierent from traditional text summarization approaches since our method is based on lexical cores and rst techniques
note that it is possible to process the usual problems which may appear in the summary such as information redundancy inconsistency based on our technique
but since the technique of multiple text summarization does not stay in the focus of our attention in this paper we will consider them in later publications
fig

balanced search on the rhetorical tree algorithm summarize multiple domain texts input the arsg g learned in algorithm a set t of texts to be summarized from the same domain d and a requested length n edus of extractive content summary of t
call algorithm to embed all lexical cores of t to the core space space of embedded lexical cores and impact space space of impacts calculated from the cores call algorithm with the modication that in each cube q always equals to and each selected impact is attached with the number of impacts in that cube as weight of it to determine a most representative ordered sequence res of impacts from the impact space where the order is determined by the weights of impacts remove all impacts not belonging to res from the impact space use g to parse the set t es of all texts which were embedded to res to get a priority sequence s t r of attributed rhetorical trees where the priority of is determined by the weight of impact e in step
call algorithm to generate a priority sequence s of edus for each text ti of t with the help of the rhetorical trees t r use a priority function xi y j to calculate the synthesized priority of j th edu d j of i th text ti in t es where xi is the s t r priority and y j the s priority
the summarization edus will be selected according to and in the order of the values of priority function

experiments evaluation and results it is very expensive to create a large parallel summarization corpus and the most common case is that we have many texts to summarize but have few or no instances of summaries
to side step this problem in this paper we pose ssmrbl especially regarding behavior learning in the form of a grammatical approach towards domain based multiple text summarization
we apply our method to publically available chinese dataset
sogouca is a large scale chinese dataset that crawled and provided by sogou labs from dozens of chinese news websites ing news reports and reviews
each text in sogouca contains elds of url docno contenttitle and content
leveraging url information we can categorize texts into their corresponding domain
in this work we choose the domain d as finance
the texts of finance domain were selected as our perimental dataset in this work
texts in this domain are almost news reports and comments
we did preprocessing including delete empty or very short lines ignore extreme long lines
unlike english to manipulate text at the word level word segmentation is needed for chinese text processing
we used hanlp for chinese word mentation part of speech pos tagging and named entity recognition ner which is a chinese natural language processing tool
after preprocessing the statistics of texts are listed in table
we rst extract lexical cores for each text in an automatic way
named entity recognition techniques and regular expressions were used as tool to detect time
the other three components i
e
agent object and state change were
sogou
com labs resource ca
php table the statistics of texts of finance domain in sogouca the number of texts the number of ponents a agent the number of ponents object the number of nents s state change average number of edus per text
recognized by the pos tag and keyword extraction algorithms
then they were used to scan each text and segment it into an edu sequence
the average number of edus after segmentation are also shown in table
in table the number of components of each type denotes the number of unique components that appear in sogouca texts
the total number of unique recognized times in all texts is which is larger than other three components
these four components were employed to train the embedding that embed all lexical cores to vector space
we split the dataset into training validation test sets according to the proportion of resulting in the number of texts in each sets are and respectively
we trained our model on sets of instances created from the training set tuned hyper parameters using instances from the validation set and tested our model on test set
we then learn the rst representation text embedding according to algorithm
our model was trained with adam optimizer with the initial learning rate as

in the margin based objective function of the margin was set to
and the dissimilarity measure d was set to the norm
the dimension of time agent object and state change embeddings were all set to w
thus the core space is a dimensional space
finally the impact of each text was calculated as the sum of all its lexical core embeddings which is considered as the embedded mapping of the text
after the above procedure each lexical core was embedded into the core space and each text was represented by the impact in the impact space
for multi text summarization what is missing is to determine topic similar clusters each of which will contribute to the multiple text source for generating a summary
intuitively two texts are similar if their corresponding impacts are near to each other in the impact space with respect to euclidian distance
we have validated this using a keyword based method we randomly sampled a bigger cube that contains impacts of dierent texts we applied rake rapid automatic keyword extraction an unsupervised and domain independent method for extracting keywords from individual texts to extract keywords from each text the statistical results show that the closer two impacts are the more common keywords their corresponding texts share
the average text similarity recall of any two closer impacts achieved

we have quantitatively proved our assumption
in the following summarization step based on the impacts we will classify texts into small clusters each of which has approximate texts
according to algorithm we next learn the target representation i
e
an arsg
we have assumed in section that there is a threshold controlling how much data should be trained from the whole dataset in the learning procedure which may be a concrete number algorithm or a percentage value algorithm
to validate this assumption we tested dierent combination of parameters
we set q among
when doing picking near training far iteration according to algorithm the quantitative results the number of new labeled texts are shown in table
note that picking near k denotes the number of new labeled texts in k th iteration the same as training far k
to show our experiment more intuitive we list the results that k among
from this table we can see that more labeled texts can be derived when using more labeled training data to build the initial grammar model
moreover in the training far steps k can still obtain considerable number of labeled texts when both using the concrete number or the percentage value as the threshold
in the following experiments we will set k
the next step is to apply algorithm for multi text summarization
when measuring the quality of generated summary the commonly used evaluation metric for text summarization is rouge
rouge evaluates n gram co occurrences between summary pairs
it works by comparing an automatically produced summary against a set of reference summaries
since our method is semi supervised we do not only assume there is only a small number of rst labeled texts but assume generating summary from multiple texts about the same topic without any parallel texts summary pair
we dened proxy metrics to evaluate our generated summaries without example summaries
to build a quantization standard we consider the following automatic statistics
sentiment polarity accuracy
a summary should reect and be consistent with the overall sentiment of the original texts
to address the problem that resources for chinese sentiment analysis are limited we translated table quantitative results when doing picking near training far iteration concrete threshold algorithm q k q k percentage threshold algorithm q k q k parameters initial picking near training far picking near training far picking near training far the number of all labeled texts the scale of mate grammars the title of each text into english by open source machine translation services and then identied the sentiment polarity of the english version by directly leveraging english oriented algorithm whose result was taken as the sentiment of chinese text
we then trained a cnn based sentiment analyzer that given a text predicts the sentiment polarity among
for each summary we compute the accuracy of whether the sentiment analyzer s predicted polarity is equal to the average polarity of all the texts in the cluster

novelty of summary edus
the similarities between each two of the summary edus should be lower in case of redundancy
we compute the redundancy by averaging the scores of and l between each two of the edus and then average these averaged scores
the novelty of summary edus is computed as novelty avg a bedu s
the appearance of named entities
the summary should contain more named entities nes that the texts in the cluster have
we obtain this score by computing the number of unique nes appeared in the original texts divided by the number of unique nes appeared in the summary

word overlap score
word overlap score can be used to measure how much the generated summary lates the original texts
we compute this score by using the scores of between the summary and each text in the cluster and then average these scores
formally the score of a generated summary of multiple texts is computed as s core s n e w where s n e w represents sentiment polarity accuracy novelty of summary edus the appearance of named entities and word overlap score respectively are weights which were empirically set to thus the range of score is
since the initial labeled texts are obtained manually which is complex and time consuming
we also ducted experiments on dierent number of labeled texts for initial grammar learning and on dierent total number of training texts
we set the size of among and set the number of training texts among all
for each combination of parameters we applied algorithm to learn an arsg and applied algorithm to the clustered texts
we set the parameter n in algorithm
table shows the scores of generated summaries which reect the eect of initial labeled texts on the performance of summary
when using training texts and labeled texts our model can obtain relatively good performance since there is little promotion when adding more training texts or labeled texts and maybe even misleading when using more training texts
table the eect of initial labeled texts on the performance of summary n all















table experimental results of our model comparing with baselines model n lead textrank dbmts



n



moreover to further assess the quality of summaries we also conducted experiments on our test set using lines
since our method generates summary without any parallel texts summary pair we select representative supervised multi text summarization approaches as the baselines including the strong baseline lead graph based method textrank and deep learning based method

lead takes the rst edus one by one in the text of the cluster until length limit where texts are assumed to be ordered according to their weight which has been assigned in algorithm

textrank builds a graph and adds each sentence as vertices the overlap of two sentences is treated as the relation that connects sentences
then graph based ranking algorithm is applied until convergence
sentences are sorted based on their nal score and a greedy algorithm is employed to impose diversity penalty on each sentence and select summary sentences

applies cnn to project sentences to continuous vector space and uses textrank for sentence ranking
cnnlm extracts the sentence representation by cnn and combines that sentence tion with the representations of context words to predict the next word
cnnlm was trained in an unsupervised scheme which resembles the cbow scheme in
after nishing training a sentence adjacent graph based on cosine similarity was built for textrank
we denote our method by dbmts with training texts and labeled texts the automated score for our model and the baselines are shown in table
the dbmts model outperforms all the baselines in our proposed evaluation metric
it also obtains a slightly promotion when n comparing with whereas the performance is better when n
from the tables we observe that our proposed method brings a good performance when small labeled texts are used with respect to our dened automated metrics

concluding remarks we note some features of our approach firstly our approach is cross representational
it is implemented in parallel both on natural language level and on embedded digital level
secondly since behaviorally training a natural language grammar is always a hard work we only train a part of the text corpus to the amount of necessity
thirdly the training set is not arbitrary taken but optimally selected according to its representation capability which helps optimize the algorithm
fourthly this approach is incremental such that new data texts can be added at any time to generalize and improve the grammar
fifthly the approach is also modular since the grammar is implemented in a group of modules rule pieces which can be reorganized to meet dierent needs and situations
sixthly this approach is exible since not only our arsg but any other grammar can be used
seventh this approach is even universal since its idea can be applied to any other semi supervised learning where the training and transition of data labels is a hard work
this work was supported by the national key research and development program of china under grant and the national natural science foundation of china no
and
acknowledgments references references
pp


ahmed m
s
khan l

sisc a text classication approach using semi supervised subspace clustering in ieee international conference on data mining workshops ieee
pp

banijamali e
ghodsi a

semi supervised representation learning based on probabilistic labeling
arxiv preprint
barzilay r
elhadad m

using lexical chains for text summarization in acl workshop on intelligent scalable text summarization bengio y
ducharme r
vincent p
jauvin c

a neural probabilistic language model
journal of machine learning research berg kirkpatrick t
gillick d
klein d

jointly learning to extract and compress in proceedings of the annual meeting of the association for computational linguistics human language technologies volume association for computational linguistics
pp

canhasi e

graph based models for multi document summarization
ph
d
thesis
doktora tezi ljubljana universitesi slovenya
chapelle o
scholkopf b
zien a

semi supervised learning chapelle o
al
eds
reviews
ieee transactions on chen y
wang x
guan y

automatic text summarization based on lexical chains in international conference on natural neural networks
computation springer
pp

cheng j
lapata m

neural summarization by extracting sentences and words in proceedings of the annual meeting of the association for computational linguistics volume long papers pp

cheng y
xu w
he z
he w
wu h
sun m
liu y

semi supervised learning for neural machine translation in proceedings of the annual meeting of the association for computational linguistics volume long papers pp

dai a
m
le q
v

semi supervised sequence learning in advances in neural information processing systems pp

fattah m
a

a hybrid machine learning model for multi document summarization
applied intelligence
finn c
yu t
fu j
abbeel p
levine s

generalizing skills with semi supervised reinforcement learning
arxiv preprint

gambhir m
gupta v

recent automatic text summarization techniques a survey
articial intelligence review
gillick d
favre b

a scalable global model for summarization in proceedings of the workshop on integer linear programming for natural langauge processing pp

he h

hanlp han language processing
url
com hankcs hanlp
hernault h
bollegala d
ishizuka m

a semi supervised approach to improve classication of infrequent discourse relations using feature vector extension in proceedings of the conference on empirical methods in natural language processing pp

hou s
huang y
fei c
zhang s
lu r

holographic lexical chain and its application in chinese text summarization in asia pacic web apweb and web age information management waim joint conference on web and big data springer
pp

kedzie c
mckeown k
daume iii h

content selection in deep learning models of summarization in proceedings of the conference on empirical methods in natural language processing pp

kim y

convolutional neural networks for sentence classication in proceedings of the conference on empirical methods in natural language processing emnlp pp

kingma d
p
ba j

adam a method for stochastic optimization
arxiv preprint

kipf t
n
welling m

semi supervised classication with graph convolutional networks
arxiv preprint

lin c
y

rouge a package for automatic evaluation of summaries
text summarization branches out
lu r
hou s
wang c
huang y
fei c
zhang s

attributed rhetorical structure grammar for domain text summarization
arxiv mann w
c
thompson s
a

rhetorical structure theory toward a functional theory of text organization
text interdisciplinary mcdonald r

a study of global inference algorithms in multi document summarization in european conference on information preprint

journal for the study of discourse
retrieval springer
pp

mihalcea r
tarau p

textrank bringing order into text in proceedings of the conference on empirical methods in natural mikolov t
chen k
corrado g
dean j

ecient estimation of word representations in vector space
arxiv preprint peyrard m

a simple theoretical model of importance for summarization in proceedings of the conference of the association for computational linguistics pp

radev d
r
jing h
stys m
tam d

centroid based summarization of multiple documents
information processing management rose s
engel d
cramer n
cowley w

automatic keyword extraction from individual documents
text mining applications and language processing



theory
rosenberg c
hebert m
schneiderman h

semi supervised self training of object detection models
wacv motion
rush a
m
chopra s
weston j

a neural attention model for abstractive sentence summarization in proceedings of the conference on empirical methods in natural language processing pp

taboada m
c
mann w

rhetorical structure theory looking back and moving ahead
discourse studies
weston j
ratle f
mobahi h
collobert r

deep learning via semi supervised embedding in neural networks tricks of the trade
springer pp

yang z
cohen w
w
salakhutdinov r

revisiting semi supervised learning with graph embeddings in proceedings of the international conference on international conference on machine learning volume jmlr
org
pp

yin w
pei y

optimizing sentence modeling and selection for document summarization in twenty fourth international joint conference on articial intelligence

