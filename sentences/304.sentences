a m l c
s c v
v i x r a deep learning models for automatic summarization the next big thing in nlp pirmin lemberger

com onepoint sablons paris groupeonepoint
com may abstract text summarization is an nlp task which aims to convert a textual document into a shorter one while keeping as much meaning as possible
this pedagogical article reviews a number of recent deep learning tectures that have helped to advance research in this eld
we will discuss in particular applications of pointer networks hierarchical transformers and reinforcement learning
we assume basic knowledge of chitecture and transformer networks within nlp
perhaps the most helpful nlp task of all for over a quarter of century we have been able to search the web by querying a search engine using a couple of relevant keywords
without such a tool the internet would be nothing but useless garbage dump of data
in google s pagerank algorithm redened what we can expect as far as relevance of search results is concerned
more recently some semantic processing has been added to the wizardry that helps the engine to interpret a query that was expressed in plain language
in a not too distant future we may perhaps pinpoint documents by engaging a short kind of conversation with a search engine just as we would with a bookseller
there is an important dierence though between a bookseller and a search engine
if you are hesitant about which book you should read you could try to ask the bookseller to summarize it for you in few of sentences
this kind summarization task has long seemed totally out of reach within the classic rule based nlp approaches and neither was it not considered realistic in foreseeable future
but slowly things are now changing with recent progress in deep learning models for nlp
for the moment just imagine you had a drop down list next to the input eld of your favorite search engine that would allow you to set the length of an automatic summary for a given document
say a sentence a sentences or a one page summary
would that be helpful actually it is quite possible that it could quickly prove so useful that it could become ubiquitous
besides improving document search it could also help in a multitude of other tasks
for instance it could help scientists keep up with a dizzying ow of publications in elds like medicine or ai
more prosaically it could help producing short product descriptions for online stores with logues too large to be handled manually
many more examples of applications of automatic summarization are described for instance here
for larger documents with several hundreds of pages like novels such generic summarization tools still belong to the realm of science ction
however thanks to the ever surprising exibility of deep learning models the wait may not be that long for tools that could summarize or two page documents in a few sentences at least within specic areas of knowledge
the aim of this article is to describe recent data sets and deep learning architectures that have brought us a little closer to the goal
a dicult task the summarizing task is dicult for a number of reasons some of which are common to other nlp tasks like translation for instance for a given document there is no summary which is objectively the best
as a general rule many of them that would be judged equally good by a human
it is hard to dene precisely what a good summary is and what score we should use for its evaluation
good training data has long been scarce and expensive to collect
human evaluation of a summary is subjective and involves judgments like style coherence completeness and readability
unfortunately no score is currently known which is both easy to compute and faithful to human judgment
the rouge score is the best we have but it has obvious shortcomings as we shall see
rouge simply counts the number of words or grams that are common to the summary produced by a machine and a reference summary written by a human
more precisely it reports a combination of the corresponding recall and precision recall overlapping n grams words in the reference summary precision overlapping n grams words in the machine summary
the combination reported in rouge n is their geometric mean known as the score
although the rouge score does not faithfully reect a human judgment it has the advantage of computational simplicity and it takes into count some of the exibility associated with the multiple summaries that could result by rearranging words within a valid summary
there are two types of summarization systems extractive summarization systems select a number of segments from the source document to make up a summary
the advantage of this proach is that the resulting summary is guaranteed to be grammatically correct
in general extractive systems achiever high rouge scores and are more reliable than the option we discuss next
abstractive summarization systems on the other hand generate their own words and sentences to reformulate the meaning of the source as a human writer would do
they can be viewed as compression systems that attempt to preserve meaning
this latter kind of systems is obviously more dicult to develop because it involves the ability to paraphrase formation and to include external knowledge
we will describe instances of both kinds below
more and better data until recently the main data set used for training summarization models was the cnn daily mail data set which contains examples of news ticle paired with their multiline summary
a detailed examination however has revealed various limitations in this data set that could bias the evaluation of the ability of a system to perform text summarization
it turned out for instance that useful information is spread unevenly across the source namely mostly at the beginning of the documents
moreover many summaries contain large fragments of the source
this is certainly not the best way for teaching a system how to produce good abstractive summaries
but things have changed recently
the bigpatent dataset for instance contains
millions patent documents together with their summaries that leviate most of the above shortcomings
a novel approach to produce ever growing data sets for training rization models uses video transcripts of talks given at international scientic conferences
the basic assumption here is that these transcripts make a good starting point for producing high quality summaries of scientic papers
the transcript itself is not directly the summary of a paper
rather the authors of the talksumm method propose to create a summary by retrieving a sequence of relevant sentences from the paper presented in the talk
a sentence is deemed relevant depending on how many words the speaker uses to describe it in her talk assuming that she has a given sentence of the paper in mind at any given point in time
clever architectures improved cost functions in this section we describe neural network models that have be developed cently for the summarization task
the aim here is not completeness of course but merely to illustrate the diversity of ideas which have been proposed to tackle this fundamental nlp problem
the basic neural network architectures that make it possible to learn this kind of task are the architectures the lstm recurrent neural networks rnn the bert and the transformer models as well as the tention mechanism
figure basic encoder decoder architecture with attention
the xi are the input token embeddings the coeci at i are the attention weights at step t the hi are the context vectors ht is the sentence embedding at step t obtained by weighting the context vectors with the attention weights are the decoder states i are the embeddings of the generated token at inference time or ground truth tokens at training time when using teacher forcing
at last pt vocab is the probability distribution at time t over a xed vocabulary
for the readers unfamiliar with any of these topics we recommend the above links which will provide excellent introductions to each of them
figure sents the architecture which converts a sequence of tokens into another sequence with a possibly dierent length
it denes the vectors we will refer to when talking about
figure sketches a transformer network with the self attention cies between embeddings an hidden vectors
roughly speaking a transformer converts a sequence of token embeddings xi into another sequence of context aware embeddings hi
the input vectors also typically include positional figure bert as the encoder part of the transformer architecture
the core idea behind the transformer is a smart implementation of attention mechanism that allows computations to be parallelized eciently on gpu something that was not possible with classical rnn
each input vector xj is a sum of a token embedding and a position embedding
the outputs hi are context aware token embeddings
information
this is needed in contrast to rnn networks because of the mutation symmetry of inputs in a transformer

summarizing without stuttering the rst architecture we present addresses the abstractive summarization task
early attempts to apply vanilla architectures to the summarization revealed a number of issues with this straightforward approach factual details in the source document like dates locations or phone numbers were often reproduced incorrectly in the summary
a nite vocabulary prevents some words like proper names from being taken into accounts
unnecessary repetitions of source fragments often occur in other words the model tends to stutter
figure shows examples of these unwanted behaviors
the authors in pose two improvements over the vanilla with attention mechanism to mitigate these shortcomings
first to overcome the nite vocabulary limitation they allow the network to copy a word directly from the source and use it in the summary when needed
the precise mechanism to do this is called a pointer network
remember that figure the last section pointer gen coverage contains the output of the system proposed in
the fragments used in the summary are shown in blue factual errors in red and unnecessary repetitions in green
in a vanilla network the decoder computes a probability distribution pt at each time step t over the words w in a xed nite vocabulary
as usual pt vocab is computed with a softmax layer that takes the attention context vector ht and the decoder state st as inputs
in a pointer network an additional copy probability pcopy is computed which represents the probability that a word should be copied from the source rather than generated by the decoder
the probability pcopy is computed using a sigmoid layer having ht st and xt vectors as inputs see gure
which word should actually be copied is mined by the attention weights at i that the decoder puts at time to each word wi in the source
putting it all together the full probability for the model to produce the word w is thus given by the following mixture pcopy pt pcopy at i
i wi w second to avoid repeating the same segments the authors dene a coverage vector ct at each time step t which estimates the amount of attention that each word wi in the source has received from the decoder until time t ct i as i
this coverage vector is then used in two dierent places within the network
first it is used to inform the attention mechanism in charge of computing the attention weights at i in addition to the usual dependence on the encoder context vector hi for the word wi and the decoder state st
the decoder is thus aware of the words it has already been paying attention to
second it is used to correct the loss function
remember that at time step t the weight at i is the attention put on word wi while ct i is the attention this same word has received in the past
if the word wi receives more attention at time t than it has already received in the past that is if at i then the cost function should penalize large values of ct i and also the other way around
to penalize attention to repeated words one denes an additional term in the loss function at time step t as a sum over input tokens i ct this is then added with an additional hyperparameter to the usual negative log likelihood lt t of the target word w ml log t in the train set lt coverage min i ct i
i lt lt ml lt coverage
results with and without these additional tricks are shown in gure

documents as sequences of contextualized sentences our next example illustrates recent ideas that dened a new sota for the extractive summary task
it builds directly upon a key idea that lead to the bert model in namely that of transfer learning based on a clever training task for a transformer encoder
let s go into a little more detail and summarize the hibert architecture for document summarization
the basic observation is that extractive classication can be cast as a tence tagging problem simply train a model to identify which sentence in a document should be kept to make up summary for this purpose the ert architecture uses two nested encoder transformers as illustrated in gure
the rst transformer encoder at the bottom is a classic sentence encoder that make up the kth that transforms the sequence of words wk sentence of the document to be summarized into a sentence embedding hk
this vector is conventionally identied as the context vector above the end of tence token



wk wk the second transformer encoder which sits on the top is a document coder that transforms the sequence of sentence embeddings


hd into a sequence of document aware sentence embeddings


dd
these embeddings are in turn converted into a sequence of probabilities


pd figure the hibert architecture involves a hierarchy of two transformer encoders used to classify each sentence within a document as being part of the summary or not
where pj is the probability that the jth sentence should be part of the summary
training such a complex hierarchical network from scratch is impractical because it would require an unrealistic amount of document summary pairs
as is well known the best strategy to train such a complex network with a limited amount of data is to use transfer learning
for this purpose the hibert architecture is rst pretrained on an auxiliary task which consists in predicting sentences that are randomly masked of them within in a large corpus of documents turing was an english mathematician
he was highly inuential
turing is widely considered to be the father of articial intelligence
mask turing was an english mathematician

turing is widely considered to be the father of articial intelligence
figure shows the architecture used for this masked sentence prediction task
it adds a transformer decoder on top of the hibert architecture in order to convert the document aware sentence embedding dk into the sequence of the kth sentence which was masked
to generate of words wk


the word at step i the decoder uses both its context vector hi and the document aware sentence embedding dk from the document encoder
figure the architecture used for the masked sentence prediction task
a tence transformer decoder is added on top of the hibert architecture to recover the words of a masked sentence using the information encapsulated in its document aware embedding dk
trained this way the network gathers a large amount of semantic knowledge without requiring any expansive labeling procedure
in a second stage aging what it learned during the pretraining task the network is ne tuned on the actual target task namely summarization as a sentence binary tagging task as in gure describes
this masked sentence prediction task is obviously reminiscent on a sentence level of the masked language model mlm used for pretraining the original bert model
remember that the mlm task consisted in recovering randomly masked words within sentences

reinforcement learning comes to the rescue as we explained earlier one central issue with the summarization task is the lack of a unique best summary
the rouge score takes this into account up to some level because it ignores the order of the words or grams within the generated summary
therefore the cost function we would actually like to minimize should be something like this rouge score or at least the nal loss function should include such a term
this is the strategy that was followed in the last work we present here which again concerns abstractive tion
the problem with a score like rouge is that for any sequence of words


wj generated by the decoder it is constant with respect to the eters of the network thus making backpropagation impossible
the situation is not hopeless though because the expectation of the rouge score for sentences


wj sampled from the joint probability distribution


wj ned by the generator is indeed a dierentiable function of those parameters the way to go is clear then
just minimize the loss dened by that expectation


wj p rouge


wj actually we can view the generator of a model as a reinforcement learning rl agent whose action at time step t is to generates a word wt depending on an inner state st which encapsulates the history from previous actions
from here on we just need to open a book on rl to learn how to minimize
a basic result in rl known as the policy gradient theorem states that the gradient of lrl


wj p rouge


wj log p


wj where log p


wj log p


j and the last index j is that of the token
the reinforce algorithm approximates the above expectation with a single sample


wj from the distribution


wj computed by the generator lrl


log


j in practice scores like rouge can have a large variance which hinders vergence of the gradient descent
fortunately we can enhance the speed of convergence by comparing


wj to a baseline b which is dependent of


wj
this does not change the gradient of lrl as can readily be veried but it can considerably reduce the variance and thus dramatically improve convergence lrl


wj log


j the main point thus is to nd an appropriate baseline
the idea in the work we are discussing is to take the baseline equal to the rouge score of the sequence of words


wj the generator actually generates at inference time
remember that this is the sequence of words that successively maximize the conditional probabilities as computed by the softmax of the decoder at each step t wt arg max



wt this choice for the baseline b is called self critical sequence training scst
altogether the reinforcement loss term thus reads lrl rouge


wj


wj log



where wt is sampled successively from


for t


wt successively maximizes


for t


j
this loss term as we can see prompts p to generate word sequences


wj whose rouge score is larger than that of the sequence


wj that was currently generated by the decoder
there are two benets for including such a scst reinforcement learning term lrl in the loss function
the rst which motivated the construction lrl of in the rst place is that it makes it possible to use a non dierentiable score like rouge within a stochastic gradient descent training procedure
the second benet is that it also cures the so called exposure bias
exposure bias results from the classic teacher forcing procedure that is typically used to train a model
this procedure trains the decoder rnn using the ground truth words w j from the train set while at inference time the decoder must of course use its own generated tokens


wj which could therefore result in an accumulation of errors
the scst choice for the baseline b amounts to train the decoder using the distribution it will actually see at inference time



w the nal loss function used is a weighted sum of the reinforcement ing loss lrl and a standard maximum likelihood objective lml
the former takes into account the non uniqueness of summaries at least up to some point but by itself it is certainly not an incentive for the model to produce readable messages
the latter on the other hand favors readable sentences as it is basically denes a language model
in order to avoid repetitions the authors also use an enhanced attention mechanism that involves a pointer network similar to the one we described in the rst example
what s next the three models we described in the previous section all use deep learning and therefore implement a purely statistical approach to the summarization task
recent research also tries to nd better loss functions
researchers at recital for instance explore the interesting idea that a good summary should answer questions as well as the original text allows
on the whole these models work indeed surprisingly well for short documents
but can we sonably expect to build a system that could summarize a pages novel in a page using techniques that only rely on crunching huge amounts of textual data this is far from obvious
abstract summarization should in principle be able to leverage real world knowledge to make sense of a document or a book to be summarized
it is unlikely though that language models alone even when initialized with clever pretraining tasks can ever capture such common sense which is more likely collected by sensory experience
one short term ity for building useful summarization tools could be to narrow their scope down to specic areas of expertise where knowledge basis or ontologies are already available
a more radical step towards building system with better real world understanding could arise from multimodal learners designed to aggregate audio video and text modalities from movies from instance
promising results have already been obtained along this path
acknowledgments i would like here to thank thomas scialom researcher at recital who kindly share his knowledge with me by pointing my attention to his summarizing summarization page
this helped me kick start my exploration of deep learning summarization models
references bigpatent a large scale dataset for abstractive and coherent marization eva sharma chen li lu wang

talksumm a dataset and scalable annotation method for tic paper summarization based on conference talks guy lev michal shmueli scheuer jonathan herzig achiya jerbi david konopnicki

get to the point summarization with pointer generator networks abigail see peter j
liu christopher d
manning

document level pre training of hierarchical bidirectional transformers for document summarization xingxing zhang furu wei ming zhou

a deep reinforced model for abstractive summarization romain paulus caiming xiong richard socher

rouge a package for automatic evaluation of summaries chin yew lin
self critical sequence training for image captioning steven j
nie etienne marcheret youssef mroueh jarret ross vaibhava goel

with attention and beam search guillaume genthial blog
understanding lstm networks christopher olah colah s blog
the illustrated bert elmo and co
jay alammar blog
the illustrated transformer jay alammar blog
neural machine translation by jointly learning to align and translate dzmitry bahdanau kyunghyun cho yoshua bengio

reinforcement learning an introduction richard s
sutton and andrew g
barto mit press cambridge ma
answers unite unsupervised metrics for reinforced summarization els thomas scialom sylvain lamprier benjamin piwowarski jacopo iano

multimodal abstractive summarization for videos shruti palaskar jindrich libovicky spandana gella florian metze

summarizing summarization on github thomas scialom recital

