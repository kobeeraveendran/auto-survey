windowing models for abstractive summarization of long texts leon sch florian nico and goran data and web science group university of mannheim inovex gmbh leon

com
uni mannheim
r a l c
s c v
v i x r a abstract neural summarization models suffer from the xed size input limitation if text length passes the model s maximal number of put tokens some document content bly summary relevant gets truncated dently summarizing windows of maximal put size disallows for information ow tween windows and leads to incoherent maries
we propose windowing models for neural abstractive summarization of ily long texts
we extend the sequence sequence model augmented with pointer erator network by allowing the encoder to slide over different windows of the input ment and sharing the decoder and retaining its state across different input windows
we explore two windowing variants static dowing precomputes the number of tokens the decoder should generate from each window in based on training statistics namic windowing the decoder learns to emit a token that signals encoder s shift to the next put window
empirical results render our els effective in their intended use case marizing long texts with relevant content not bound to the very document beginning
background and motivation while extractive summarization selects and copies the most relevant source phrases and sentences to the summary abstractive summarization as aims to capture the source meaning and generate summaries not necessarily containing portions of the source texts nenkova and mckeown holding promise of producing summaries more like human created ones
state of the art ral as models nallapati et al
see et al
paulus et al
tan et al
makino et al
you et al
extend a standard sequence to sequence architecture ing either recurrent rnn bahdanau et al
or transformer based vaswani et al
coder and decoder components
see et al
extend the standard model with a generator network pg net providing the model with extractive capabilities i
e
allowing it to choose between generating a token and copying source text tokens
tan et al
propose a archical model that introduces an additional based attention mechanism which serves to model interactions between encoded sentence tations
paulus et al
incorporate a reward expectation based on reinforcement learning into a mixed training objective to steer the model towards predicting globally meaningful sequences
with respect long document summarization likyilmaz et al
distribute the encoding task to multiple collaborating encoder agents whereas cohan et al
propose a hierarchical encoder that captures the document s discourse structure and an attentive discourse aware decoder that erates the summary
the latter requires a predened discourse structure and is designed for specic texts e

scientic publications
despite multiple encoders operating on different document segments these models still limit the maximal ument length at inference
in this work we address a prominent limitation of neural as models they can not summarize texts longer than the maximal input length tx set during model training
at inference documents longer than tx tokens are truncated which renders the potentially summary relevant truncated content inaccessible to the model
we propose novel as models based on windowing of source text we sequentially shift encoder s attention over ent windows of source text
the decoder is shared across windows thereby preserving semantic formation from a previous window when decoding the next
we investigate two windowing strategies static windowing model swm precomputes state across all input windows
sharing a coder across input windows allows the ow of semantic information between adjacent windows and holds promise of retaining summary ence
at each decoding step t we attend over the window representations using the decoder s hidden state st as the attention query and obtain the conditioned window encoding ct for the coding step t ct t jhj with attention jtw weight t computed as the softmax normalized value of the dot product t hj between the encoded hj and the decoder s state st
decoder puts the embedding via feed forward projection of the concatenation of the attended input sentation ct and its own hidden state st wl st bl with wl rd bl rd as parameters
the output probability distribution pv over training vocabulary v is then simply computed by applying the softmax function on the vector of dot product values computed between lt and each of the pretrained word embeddings
we augment the base model with the pointer generator network pg net as in see et al
allowing the decoder to choose in each step between generating a token from the training vocabulary and copying a token from the source document
generation probability is based on the context vector ct decoder s hidden state st and decoder s input xt pgen ct s st xt bptr with wc rd wx rdemb bptr r as rameters
the output probability for a word from the extended vocabulary v union of v and source text words interpolates between generation and copying distributions p v pgen pv t j j this species the pg net augmented as model that operates on a window tw tokens
we next need to specify when to transition from one window of source text to another

static windowing model the static windowing model precomputes the number of tokens the decoder needs to generate for each input window
let


wn be the equally sized source windows determined with tw and ss
we use the following function to termine the importance weight for each window figure high level illustration of the windowing model for long document summarization
based on the training corpus statistics the ber of tokens the decoder is to generate from each source window for the dynamic windowing model dwm we rst heuristically based on mantic similarity between source text and summary sentences inject special window shift tokens into the training reference summaries and then let the decoder learn to emit window shift tokens during generation
signaling the window shift by erating a special token conceptually allows the dwm model to summarize arbitrarily long texts during inference
evaluation on the wikihow pus koupaee and wang of long texts with more even distribution of summary relevant tent renders our windowing models effective
windowing as models h j figure contains the high level depiction of the windowing as model
we start from the based model with recurrent components bahdanau et al
which maps the put sequence


xtx into an output sequence


yty
a bidirectional lstm bi lstm encoder produces contextualized representations h for each input token
decoder s state is initialized with the concatenation of the h
end states of encoder s lstms we apply an attention mechanism similar to luong et al

however instead of learning a local attention span around each source text position which would limit the model to a xed size input during training we attend over a window of tw tokens and sequentially slide the window over the long text
this way the decoder learns to model transitions between content windows allowing to summarize arbitrarily long documents at inference
window size tw and a stride step ss divide the source text tx tokens into overlapping dows
we use the same decoder retaining its h tx also experimented with transformer vaswani et al
encoder decoder but obtained weaker performance
pad the last if shorter than tw tokens
with k and d as rameters dening the shape of the summary bution over windows
the unnormalized weights are converted into probabilities using the softmax function
we next compute the expected summary length for a given document based on the document length and training corpus statistics
let d be the set of documents and s the set of their respective reference summaries in the training corpus
we compute the expected summary length for a new document d as where is the length that covers of training documents i
e
of d are at most and is the length that covers of reference summaries from s
the number of tokens the decoder is to generate for a window wi is now simply a product of and the normalized weight

dynamic windowing model swm still relies on the document and summary lengths of the training corpus and the number of summary tokens decoded for a window does not it s content
dynamic windowing model dwm aims to be more exible by allowing the decoder to dynamically signal via a special token the ration of the current window and shift to the next
because the decoder needs to learn to emit this window shift token and we still want an end to end trainable as model we need to how inject window shift tokens into reference summaries of the training corpus
we achieve this heuristically by computing semantic similarity scores between source text sentences and reference summary sentences
for simplicity we obtain the sentence embedding as a sum of its respective word embeddings and compute the cosine similarity tween sentence embeddings
for every reference summary sentence we tify the most similar source document sentence and determine its respective window
this way example with
and k
the early dows will receive larger weights than the later windows
acknowledge that this is a rudimentary method for computing semantic similarity between sentences
we intend to experiment with more advanced sentence embedding els and more accurate sentence similarity measures kusner et al
conneau et al
devlin et al
niak et al
inter alia in subsequent work
on tw and ss a sentence be in more than we map each reference summary sentence to one source window
the order of windows assigned to summary sentences is however not necessarily sequential e

for some reference summary with ve sentences
since our model lows only sequential window shifts we rst make the window order sequential by replacing breaking windows with accumulated maximums e

becomes
we then inject window shift tokens between summary sentences with different assigned source windows e

for the window assignment we inject between the rst and second mary sentence and between the third and fourth sentence
during inference the input window is shifted whenever the decoder outputs the token
evaluation data
we evaluate our windowing models on two benchmark datasets cnn dailymail news corpus created by nallapati et al
from the question answering dataset of hermann et al
and wikihow corpus koupaee and wang
news place the most relevant formation at the beginning the so called lead body principle the standard models that truncate long documents are thus likely to perform well in the cnn dailymail evaluation
the wikihow dataset does not have such a construction bias summary relevant information is more evenly tributed across the texts
experimental setup
we use the negative log likelihood objective and optimize the models by maximizing the rouge l performance on opment sets
we use a batch level beam search decoder with beam size b
unlike standard beam search b does not decrease when the end summary token eos is predicted
longer yet incomplete partial hypotheses can thus take over completed beams whenever they prevail in terms of length normalized log probability
we set the hidden state sizes for both encoder s lstms and decoder s lstm to
we employ the adam timizer kingma and ba

and
for word representations we use pretrained dim
fasttext embeddings most frequent one window
in such cases we map the sentence to the last containing window

com model stan swm dwm







r l



table results on the cnn dailymail test set maries of ty tokens stan trained with size input of tx tokens swm d
k
dwm trained on tx tokens with windows of tw tokens stride ss
model stan dwm stan swm dwm tx tw ss











r l





table results on the wikihow dataset ty d for swm
baselines
we compare different variants of swm and dwm against the standard pg net model stan with the xed size input see et al
as well as against the commonly employed baseline which simply copies the rst three document sentences to the summary
results and discussion
table contains the sults on the cnn dailymail dataset
ingly the simple baseline outperforms stan and both our static and dynamic windowing models
this is because in cnn dailymail ments almost all of the summary relevant content is found at the very beginning of the document
the ability to process all windows does not benet to swm and dwm in this setting as there is virtually no summary relevant content in later windows
in table we display the results on the how dataset which is bound to be more ate for the windowing models because of the more even distribution of the summary relevant content across the source documents
on the wikihow dataset the windowing models swm and dwm generally have an edge over the standard pg net model stan when the xed size input for stan matches the windows size of the dowing models
for a larger input size tx stan performs comparably to dwm with the same window size tw
notably the dwm has the advantage of being able to process longer overall put
lowering tx for stan to and comparing it against swm dwm with windows of the same figure summary for the wikipedia page lionel messi
tokens produced by dwm trained on cnn dailymail tx
tokens
colors spond to different source text windows over which the decoder attended during generation
size tw we see that the windowing models clearly prevail
this renders our windowing models as a more approriate solution for summarization of documents for which the following two properties hold the document length massively surpasses the maximal number of tokens we can feed to the xed input size model and summary relevant information is present all across the document and not just at its beginning
while swm seems to outperform dwm in tice swm can not really summarize arbitrarily long texts at inference
despite transitioning across content windows swm adapts to the summary lengths seen in the training corpus and generates the eos token too early during inference on the long texts
in contrast by learning to emit window transitions the dynamic windowing model can truly generate summaries for arbitrarily long texts at inference time regardless of the observed lengths of training document and their respective reference summaries
figure depicts the summary of a very long document
tokens produced by a dws model trained on an order of magnitude shorter documents tx
tokens
conclusion neural summarization models x the length of the source texts in training e

based on the age source document length in the training set forcing documents longer than this threshold to be truncated at inference
in this work we proposed windowing summarization models which allow to process arbitrarily long documents at inference taking into account full source text
our models are effective in summarizing long texts with evenly distributed summary relevant content
under length constraint for neural text in proceedings of the annual meeting tion
of the association for computational linguistics pages
ramesh nallapati bing xiang and bowen zhou

sequence to sequence rnns for text summarization
in proceedings of iclr workshop track
ani nenkova and kathleen r
mckeown

tomatic summarization
foundations and trends in information retrieval
romain paulus caiming xiong and richard socher

a deep reinforced model for abstractive marization
in proceedings of iclr
abigail see peter j liu and christopher d manning

get to the point summarization with generator networks
in proceedings of the nual meeting of the association for computational linguistics volume long papers pages
jiwei tan xiaojun wan and jianguo xiao

abstractive document summarization with a in proceedings based attentional neural model
of the annual meeting of the association for computational linguistics volume long papers pages
association for computational linguistics
ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan n gomez lukasz kaiser and illia polosukhin

attention is all you need
in proceedings of neurips
yongjian you weijia jia tianyi liu and wenmian yang

improving abstractive document in marization with salient information modeling
proceedings of the annual meeting of the ciation for computational linguistics pages
vitalii zhelezniak aleksandar savkov april shen francesco moramarco jack flann and nils y merla

do nt settle for average go for the max fuzzy sets and max pooled word vectors
in proceedings of iclr
references dzmitry bahdanau kyunghyun cho and yoshua gio

neural machine translation by jointly in proceedings of learning to align and translate
iclr
asli celikyilmaz antoine bosselut xiaodong he and yejin choi

deep communicating agents for in proceedings of the abstractive summarization
conference of the north american chapter of the association for computational linguistics man language technologies volume long pers pages
arman cohan franck dernoncourt doo soon kim trung bui seokhwan kim walter chang and nazli goharian

a discourse aware attention model for abstractive summarization of long documents
in proceedings of the conference of the north american chapter of the association for tional linguistics human language technologies volume short papers pages
alexis conneau douwe kiela holger schwenk loc barrault and antoine bordes

supervised learning of universal sentence representations from natural language inference data
in proceedings of the conference on empirical methods in ral language processing pages
jacob devlin ming wei chang kenton lee and kristina toutanova

bert pre training of deep bidirectional transformers for language standing
in proceedings of the conference of the north american chapter of the association for computational linguistics human language nologies volume long and short papers pages
karl moritz hermann tomas kocisky edward stette lasse espeholt will kay mustafa suleyman and phil blunsom

teaching machines to read and comprehend
in advances in neural information processing systems pages
diederik p
kingma and jimmy ba

adam a method for stochastic optimization
in iclr
mahnaz koupaee and william yang wang

ihow a large scale text summarization dataset
corr

matt kusner yu sun nicholas kolkin and kilian weinberger

from word embeddings to ument distances
in international conference on chine learning pages
minh thang luong hieu pham and christopher d
manning

effective approaches to based neural machine translation
in proceedings of emnlp pages
takuya makino tomoya iwakura hiroya takamura and manabu okumura

global optimization
