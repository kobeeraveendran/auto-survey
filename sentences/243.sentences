discourse aware neural extractive text summarization jiacheng zhe yu jingjing university of texas at austin dynamics ai research
utexas
edu zhe
gan yu
cheng
com r a l c
s c v
v i x r a abstract recently bert has been adopted for ument encoding in state of the art text marization models
however sentence based extractive models often result in redundant or uninformative phrases in the extracted summaries
also long range dependencies throughout a document are not well tured by bert which is pre trained on tence pairs instead of documents
to address these issues we present a discourse aware neural summarization model
discobert extracts sub sentential discourse units instead of sentences as candidates for extractive selection on a ner granularity
to capture the long range dependencies among discourse units structural discourse graphs are constructed based on rst trees and ence mentions encoded with graph tional networks
experiments show that the proposed model outperforms state of the art methods by a signicant margin on popular summarization benchmarks compared to other bert base models
introduction neural networks have achieved great success in the task of text summarization nenkova et al
yao et al

there are two main lines of research abstractive and extractive
while the abstractive paradigm rush et al
see et al
celikyilmaz et al
sharma et al
focuses on generating a summary word by word after encoding the full document the extractive approach cheng and lapata zhou et al
narayan et al
directly selects tences from the document to assemble into a mary
the abstractive approach is more exible most of this work was done when the rst author was an intern at microsoft
illustration and datasets are available at
com jiacheng xu discobert
illustration of discobert for text figure marization
sentence based bert model baseline selects whole sentences and
the proposed discourse aware model discobert selects edus
the right side of the gure illustrates the two discourse graphs we use i graph with the mentions of pulitzer prizes highlighted as examples and rst graph induced by rst discourse trees
and generally produces less redundant summaries while the extractive approach enjoys better ity and efciency cao et al

recently some hybrid methods have been posed to take advantage of both by designing a two stage pipeline to rst select and then rewrite or compress candidate sentences chen and bansal gehrmann et al
zhang et al
xu and durrett
compression or rewriting aims to discard uninformative phrases in the lected sentences
however most of these hybrid systems suffer from the inevitable disconnection between the two stages in the pipeline
meanwhile modeling long range context for document summarization remains a challenge xu
it is one of the most prestigious bestowed upon journalists and people in the arts

and today the pulitzer prize for journalism went to the post and courier newspaper of charleston south which has a tiny staff of just and a daily circulation of

winner this iconic photo by new york times photographer daniel berehulak was part of a winning and shows james dorbor suspected of being infected with being carried by medical staff to an ebola treatment center in monrovia liberia

the pulitzer awarded annually by columbia recognize extraordinary work in u
s
journalism literature drama and other categories

other winners of the coveted award included the st
louis post dispatch
coref graphrst graphdocument of pulitzer
winner this iconic photo by new york times photographer daniel berehulak was part of a winning and shows james dorbor suspected of being infected with being carried by medical staff to an ebola treatment center in monrovia liberia

it is one of the most prestigious bestowed upon journalists and people in the arts

and today the pulitzer prize for journalism went to the post and courier newspaper of charleston south which has a tiny staff of just and a daily circulation of
selection et al

pre trained language models vlin et al
are designed mostly for sentences or a short paragraph thus poor at capturing range dependencies throughout a document
pirical observations liu and lapata show that adding standard encoders such as lstm or transformer vaswani et al
on top of bert to model inter sentential relations does not bring in much performance gain
in this paper we present discobert a discourse aware neural extractive summarization model built upon bert
to perform compression with extraction simultaneously and reduce dancy across sentences we take elementary course unit edu a sub sentence phrase unit inating from rst mann and thompson carlson et al
as the minimal selection unit instead of sentence for extractive summarization
figure shows an example of discourse tion with sentences broken down into edus tated with brackets
by operating on the discourse unit level our model can discard redundant details in sub sentences therefore retaining additional pacity to include more concepts or events leading to more concise and informative summaries
furthermore we netune the representations of discourse units with the injection of prior edge to leverage intra sentence discourse relations
more specically two discourse oriented graphs are proposed rst graph gr and coreference graph gc
over these discourse graphs graph convolutional network gcn kipf and welling is imposed to capture long range interactions among edus
rst graph is constructed from rst parse trees over edus of the document
on the other hand coreference graph connects entities and their coreference clusters mentions across the document
the path of coreference navigates the model from the core event to other occurrences of that event and in parallel explores its interactions with other concepts or events
the main contribution is threefold we pose a discourse aware extractive summarization model discobert which operates on a sentential discourse unit level to generate cise and informative summary with low we propose to structurally model dancy
adopt rst as the discourse framework due to the availability of existing tools the nature of the rst tree ture for compression and the observations from louis et al

other alternatives includes graph bank wolf and gibson and pdtb miltsakaki et al

figure example of discourse segmentation and rst tree conversion
the original sentence is segmented into edus in box a and then parsed into an rst discourse tree in box
the converted based rst discourse tree is shown in box c
cleus nodes including and and satellite nodes including and are denoted by solid lines and dashed lines respectively
relations are in italic
the edu is the head of the whole tree span while the edu is the head of the span
inter sentential context with two types of discourse graph
iii discobert achieves new state of the art on two popular newswire text summarization datasets outperforming other bert base models
discourse graph construction in this section we rst introduce the cal structure theory rst mann and son a linguistic theory for discourse ysis and then explain how we construct discourse graphs used in discobert
two types of course graph are considered rst graph and erence graph
all edges are initialized as nected and connections are later added for a subset of nodes based on rst discourse parse tree or coreference mentions

discourse analysis discourse analysis focuses on inter sentential tions in a document or conversation
in the rst framework the discourse structure of text can be represented in a tree format
the whole document can be segmented into contiguous adjacent and non overlapping text spans called elementary course units edus
each edu is tagged as either nucleus or satellite which characterizes its arity or saliency
nucleus nodes are generally more central and satellite nodes are more peripheral and less important in terms of content and grammatical reliance
there are dependencies among edus that represent their rhetorical relations
in this work we treat edu as the minimal unit for content selection in text summarization
rst discourse tree converted rst discourse winner this iconic photo by new york times photographer daniel berehulak was part of a winning series and shows james dorbor suspected of being infected with ebola being carried by medical staff to an ebola treatment center in monrovia liberia
conversion sec
ure shows an example of discourse segmentation and the parse tree of a sentence
among these edus rhetorical relations represent the functions of different discourse units
as observed in louis et al
the rst tree structure already serves as a strong indicator for content selection
on the other hand the agreement between rhetorical tions tends to be lower and more ambiguous
thus we do not encode rhetorical relations explicitly in our model
in content selection for text summarization we expect the model to select the most concise and pivotal concept in the document with low dancy
however in traditional extractive rization methods the model is required to select a whole sentence even though some parts of the sentence are not necessary
our proposed approach can select one or several ne grained edus to der the generated summaries less redundant
this serves as the foundation of our discobert model

rst graph when selecting sentences as candidates for tive summarization we assume each sentence is grammatically self contained
but for edus some restrictions need to be considered to ensure maticality
for example figure illustrates an rst discourse parse tree of a sentence where this iconic


series is a grammatical sentence but and shows


is not
we need to stand the dependencies between edus to ensure the grammaticality of the selected combinations
the detail of the derivation of the dependencies could be found in sec

the construction of the rst graph aims to vide not only local paragraph level but also range document level connections among edus
we use the converted dependency version of the tree to build the rst graph gr by initializing an empty graph and treating every discourse dency from the i th edu to the j th edu as a directed edge i
e


coreference graph text summarization especially news tion usually suffers from the well known position bias issue kedzie et al
where most of the key information is described at the very beginning example in figure details such as the name of the suspected child in the exact location of the photo in and who was carrying the child in are unlikely to be reected in the nal summary
algorithm construction of the coreference graph gc
require coreference clusters c cn mentions for each cluster ci eim
initialize the graph gc without any edge gc
for i to n do collect the location of all occurences eim to l lm
for j to m k to m do gc end for end for return constructed graph gc
of the document
however there is still a decent amount of information spread in the middle or at the end of the document which is often ignored by summarization models
we observe that around of oracle sentences appear after the rst sentences in the cnndm dataset
besides in long news articles there are often multiple core acters and events throughout the whole document
however existing neural models are poor at ing such long range context especially when there are multiple ambiguous coreferences to resolve
to encourage and guide the model to capture the long range context in the document we pose a coreference graph built upon discourse units
algorithm describes how to construct the coreference graph
we rst use stanford corenlp manning et al
to detect all the coreference clusters in an article
for each coreference cluster all the discourse units containing the mention of the same cluster will be connected
this process is iterated over all the coreference mention clusters to create the nal coreference graph
figure provides an example where pulitzer prizes is an important entity and has occurred multiple times in multiple discourse units
the constructed coreference graph is shown on the right side of the
when graph gc is constructed edges among and are all connected due to the mentions of pulitzer prizes
discobert model
overview figure provides an overview of the proposed model consisting of a document encoder and a graph encoder
for the document encoder a trained bert model is rst used to encode the intentionally ignore other entities and mentions in this example for simplicity
figure left model architecture of discobert
the stacked discourse graph encoders contain k stacked dge blocks
right the architecture of each discourse graph encoder dge block
whole document on the token level
then a attentive span extractor is used to obtain the edu representations from the corresponding text spans
the graph encoder takes the output of the ment encoder as input and updates the edu resentations with graph convolutional network based on the constructed discourse graphs which are then used to predict the oracle labels
assume that document d is segmented into n edus in total i
e
d dn where denotes the i th edu
following liu and lapata we formulate extractive summarization as a sequential labeling task where each edu di is scored by neural networks and decisions are made based on the scores of all edus
the oracle labels are a sequence of binary labels where stands for being selected and for not
we denote the labels as y y y n
during training we aim to predict the sequence of labels y given the document d
during inference we need to further consider discourse dependency to ensure the coherence and grammaticality of the output summary
y
document encoder bert is a pre trained deep bidirectional former encoder vaswani et al
devlin et al

following liu and lapata we code the whole document with bert and netune the bert model for summarization
bert is originally trained to encode a single sentence or sentence pair
however a news article typically contains more than words hence we need to make some adaptation to apply bert for document encoding
specically we insert and tokens at the beginning and the end of each sentence respectively
in order to encode long documents such as news articles we also tend the maximum sequence length that bert can take from to in all our experiments
the input document after tokenization is denoted as d dn and di where is the number of bpe tokens in the i th edu
if is the rst edu in a sentence there is also a token prepended to di if dj is the last edu in a sentence there is a token appended to see figure
the schema of insertion of and is an approach used in liu and lapata
for simplicity these two tokens are not shown in the equations
bert model is then used to encode the document hb hb where hb whole document in the same length as the input
is the bert output of the hb after the bert encoder the representation of the token can be used as sentence tation
however this approach does not work in our setting since we need to extract the tation for edus instead
therefore we adopt a also tried inserting and at the beginning and the end of every edu and treating the corresponding representation as the representation for each edu but the performance drops drastically
it is one of the most prestigious honors cls sep bestowed upon the arts
and today the pulitzer prize for south carolina which has a tiny
cls discourse graph convolutional network self attentive span extractor spanext proposed in lee et al
to learn edu representation
for the i th edu with words with the output we from the bert encoder hb obtain edu representation as follows hb hb ij ij aij hs aij hb ij where ij is the score of the j th word in the edu aij is the normalized attention of the j th word w

t
all the words in the span
hs is a weighted sum i of the bert output hidden states
throughout the paper all the w matrices and b vectors are eters to learn
we abstract the above self attentive span extractor as hs
i hb after the span extraction step the whole ment is represented as a sequence of edu n rdhn which hs sentations hs hs will be sent to the graph encoder

graph encoder given the constructed graph g v e nodes v correspond to the edus in a document and edges e correspond to either rst discourse relations or coreference mentions
we then use graph volutional network to update the representations of all the edus to capture long range cies missed by bert for better summarization
to modularize architecture design we present a single discourse graph encoder dge layer
multiple dge layers are stacked in our experiments
assume that the input for the k th dge layer is denoted as


n rdhn and the corresponding output is denoted as


rdhn
the k th dge layer is designed as follows n i i relu i j i i jni i i i where ln represents layer normalization ni denotes the neighorhood of the i th edu node
is the output of the i th edu in the k th i dge layer and hs which is the output from the document encoder
after k layers of dataset document sum
e in graph sent
edu tok
tok
gr cnndm nyt gc table statistics of the datasets
the rst block shows the average number of sentences edus and tokens in the documents
the second block shows the average number of tokens in the reference summaries
the third block shows the average number of edges in the constructed rst graphs gr and coreference graphs gc respectively
graph propagation we obtain hg rdhn which is the nal representation of all the edus after the stacked dge layers
for different graphs the parameter of dges are not shared
if we use both graphs their output are concatenated hg r
c hg
training inference during training hg is used for predicting the acle labels
specically yi i where represents the logistic function and yi is the prediction probability ranging from to
the training loss of the model is binary cross entropy loss given the predictions and oracles l i y yi
for discobert without graphs the output from document encoder hs is used for prediction stead
the creation of oracle is operated on edu level
we greedily pick up edus with their sary dependencies until drops
during inference given an input document ter obtaining the prediction probabilities of all the edus i
e
y yn we sort y in scending order and select edus accordingly
note that the dependencies between edus are also forced in prediction to ensure grammacality of erated summaries
experiments in this section we present experimental results on two popular news summarization datasets
we compare our proposed model with state of the art baselines and conduct detailed analysis to validate the effectiveness of discobert

datasets we evaluate the models on two datasets new york times nyt sandhaus cnn and mail cnndm hermann et al

we use the script from see et al
to extract summaries from raw data and stanford corenlp for sentence boundary detection tokenization and parsing ning et al

due to the limitation of bert we only encode up to bert bpes
table provides statistics of the datasets
the edges in gc are undirected while those in gr are directional
for cnndm there are and samples for training validation and test respectively
we use the un anonymized version as in previous summarization work
nyt is licensed by
following previous work zhang et al
xu and durrett we use and samples for training validation and test respectively

state of the art baselines we compare our model with the following state the art neural text summarization models
extractive models banditsum treats tive summarization as a contextual bandit lem trained with policy gradient methods dong et al

neusum is an extractive model with architecture where the attention nism scores the document and emits the index as the selection zhou et al

compressive models jecs is a neural compression based summarization model using blstm as the encoder xu and durrett
the rst stage is selecting sentences and the ond stage is sentence compression by pruning stituency parsing tree
bert based models bert based models have achieved signicant improvement on cnndm and nyt when compared with lstm counterparts
bertsum is the rst bert based extractive marization model liu and lapata
our baseline model bert is the re implementation of bertsum
pnbert proposed a bert based model with various training strategies including ment learning and pointer networks zhong et al

hibert is a hierarchical bert based model for document encoding which is further pretrained with unlabeled data zhang et al


implementation details we use allennlp gardner et al
as the code framework
the implementation of graph model oracle sentence oracle discourse r l











neusum zhou et al



banditsum dong et al



jecs xu and durrett


pnbert zhong et al
pnbert w
rl





bert zhang et al



hiberts hibert


s hibert


m bertsum liu and lapata





base raffel et al
bert discobert discobert w
gc discobert w
gr discobert w
gr gc














table results on the test set of the cnndm dataset
and are reported
models with the asterisk symbol used extra data for pre training
and are shorthands for unigram and bigram lap r l is the longest common subsequence
encoding is based on dgl wang et al

periments are conducted on a single nvidia card and the mini batch size is set to due to gpu memory capacity
the length of each document is truncated to bpes
we use the pre trained bert base uncased model and ne tune it for all periments
we train all our models for up to steps
rouge lin is used as the evaluation metrics and is used as the validation criteria
the realization of discourse units and structure is a critical part of edu pre processing which quires two steps discourse segmentation and rst parsing
in the segmentation phase we use a neural discourse segmenter based on the bilstm crf framework wang et al

the segmenter achieved
score on the rst dt test set in which the human performance is

in the ing phase we use a shift reduce discourse parser to extract relations and identify neuclrity ji and eisenstein
the dependencies among edus are crucial to the grammaticality of selected edus
here are the two steps to learn the derivation of cies head inheritance and tree conversion
head inheritance denes the head node for each valid non terminal tree node
for each leaf node the
ldc
upenn
neuraleduseg
com pku
com jiyfeng dplp head is itself
we determine the head of non terminal nodes based on their nuclearity
for example in figure the heads of text spans and need to be grounded to a gle edu
we propose a simple yet effective schema to convert rst discourse tree to a based discourse tree
we always consider the dependency restriction such as the reliance of lite on nucleus when we create oracle during processing and when the model makes the tion
for the example in figure if the model selects being carried


liberia
as a date span we will enforce the model to select and shows


and this


series as well
the number of chosen edus depends on the average length of the reference summaries dencies across edus as mentioned above and the length of the existing content
the optimal average number of edus selected is tuned on the ment set

experimental results results on cnndm table shows results on cnndm
the rst section includes baseline sentence based oracle and discourse based oracle
the second section lists the performance of line models including non bert based and based variants
the performance of our proposed model is listed in the third section
bert is our implementation of sentence based bert model
discobert is our discourse based bert model without discourse graph encoder
discobert w
gc and discobert w
gr are the based bert model with coreference graph and rst graph respectively
discobert w
gr gc is the fusion model encoding both graphs
the proposed discobert beats the based counterpart and all the competitor els
with the help of discourse graph coder the graph based discobert beats the of the art bert model by a signicant margin


on on
ablation study with individual graphs shows that the rst graph is slightly more helpful than the coreference both children are then the head of the current node inherits the head of the left child
otherwise when one child is n and the other is s the head of the current node inherits the head of the n child
one child node is n and the other is s the head of the s node depends on the head of the n node
if both children are n and the right child does not contain a subject in the discourse the head of the right n node depends on the head of the left n node
model oracle sentence oracle discourse r l








jecs xu and durrett

bert zhang et al

hiberts
hibertm hibert
s hibert
m











bert discobert discobert w
gc discobert w
gr discobert w
gr gc














table results on the test set of the nyt dataset
models with the asterisk symbol used extra data for pre training
graph while the combination of both achieves ter performance overall
results on nyt results are summarized in ble
the proposed model surpasses previous state of the art bert based model by a signicant s and hibert margin
hibert m used extra data for pre training the model
we notice that in the nyt dataset most of the improvement comes from the use of edus as minimal selection units
cobert provides


gain on over the bert baseline
however the use of course graphs does not help much in this case

grammaticality due to segmentation and partial selection of tence the output of our model might not be as grammatical as the original sentence
we ally examined and automatically evaluated model output and observed that overall the generated summaries are still grammatical given the rst dependency tree constraining the rhetorical tions among edus
a set of simple yet effective post processing rules helps to complete the edus in some cases
automatic grammar checking we followed xu and durrett to perform automatic mar checking using grammarly
table shows the grammar checking results where the average number of errors in every characters on ndm and nyt datasets is reported
we compare discobert with sentence based bert model
all shows the summation of the number of rors in all categories
as shown in the table the source m cr pv pt cnndm nyt sent disco sent disco all















o



table number of errors per characters based on automatic grammaticality checking with marly on cnndm and nyt
lower values are ter
detailed error categories including correctness cr passive voice pv misuse punctuation pt in compound complex sentences and others o are listed from left to right
model sent disco ref all coherence grammaticality

















table human evaluation results
we ask turkers to grade the overall preference coherence and icality from to
mean values along with standard deviations are reported
clare hines who lives in brisbane was diagnosed with a brain tumour after suffering epileptic seizures
after a number of tests doctors discovered she had a benign tumour that had wrapped itself around her acoustic facial and balance nerve and told her she had have it surgically removed or she risked the tumour turning malignant
one week before brain surgery she found out she was pregnant
jordan henderson in action against aston villa at wembley on sunday has agreed a new liverpool deal
the club s vice captain puts pen to paper on a deal which will keep him at liverpool until
rodgers will sider henderson for the role of club captain after steven gerrard moves to la galaxy at the end of the campaign but for now the england international is delighted to have agreed terms on a contract that will take him through the peak years of his career
table example outputs from cnndm by cobert
strikethrough indicates discarded edus
originates from missing or improper or missing anaphora resolution
in this example johnny is believed to have but actually he is the police say
only selecting the ond edu yields a sentence actually he is ne which is not clear who is he mentioned here
summaries generated by our model have retained the quality of the original text
related work human evaluation we sampled documents from the test set of cnndm and for each sample we asked two turkers to grade three summaries from to
results are shown in table
bert model the original bertsum model lects sentences from the document hence providing the best overall readability coherence and maticality
in some cases reference summaries are just long phrases so the scores are slightly lower than those from the sentence model
discobert model is slightly worse than sent bert model but is fully comparable to the other two variants
examples analysis we show some examples of model output in table
we notice that a decent amount of irrelevant details are removed from the extracted summary
despite the success we further conducted ror analysis and found that the errors mostly inated from the rst dependency resolution and the upstream parsing error of the discourse parser
the misclassication of rst dependencies and the hand crafted rules for dependency resolution hurted the grammaticality and coherence of the generated outputs
common punctuation issues include extra or missing commas as well as ing quotation marks
some of the coherence issue neural extractive summarization neural works have been widely used in extractive rization
various decoding approaches including ranking narayan et al
index prediction zhou et al
and sequential labelling lapati et al
zhang et al
dong et al
have been applied to content selection
our model uses a similar conguration to encode the document with bert as liu and lapata did but we use discourse graph structure and graph encoder to handle the long range dependency issue
neural compressive summarization text summarization with compression and deletion has been explored in some recent work
xu and durrett presented a two stage neural model for selection and compression based on constituency tree pruning
dong et al
presented a neural sentence compression model with discrete operations including deletion and addition
different from these studies as we use edus as minimal selection basis sentence compression is achieved automatically in our model
discourse summarization the use of course theory for text summarization has been plored before
louis et al
examined the benet of graph structure provided by discourse lations for text summarization
hirao et al
yoshida et al
formulated the tion problem as the trimming of the document course tree
durrett et al
presented a system of sentence extraction and compression with ilp methods using discourse structure
li et al
demonstrated that using edus as units of content selection leads to stronger summarization mance
compared with them our proposed method is the rst neural end to end summarization model using edus as the selection basis
graph based summarization graph approach has been explored in text summarization over decades
lexrank introduced a stochastic based method for computing relative importance of textual units erkan and radev
sunaga et al
employed a gcn on the lation graphs with sentence embeddings obtained from rnn
tan et al
also proposed based attention in abstractive summarization model
fernandes et al
developed a framework to reason long distance relationships for text rization
conclusion in this paper we present discobert which uses discourse unit as the minimal selection basis to reduce summarization redundancy and leverages two types of discourse graphs as inductive bias to capture long range dependencies among discourse units
we validate the proposed approach on two popular summarization datasets and observe sistent improvement over baseline models
for future work we will explore better graph encoding methods and apply discourse graphs to other tasks that require long document encoding
acknowledgement thanks to junyi jessy li greg durrett yen chun chen and to the other members of the microsoft dynamics ai research team for the reading feedback and suggestions
references ziqiang cao furu wei wenjie li and sujian li

faithful to the original fact aware neural tive summarization
in aaai conference on cial intelligence
lynn carlson daniel marcu and mary ellen okurovsky

building a discourse tagged pus in the framework of rhetorical structure theory
in proceedings of the second sigdial workshop on discourse and dialogue
asli celikyilmaz antoine bosselut xiaodong he and yejin choi

deep communicating agents for in proceedings of the abstractive summarization
conference of the north american chapter of the association for computational linguistics man language technologies volume long pers pages new orleans louisiana
association for computational linguistics
yen chun chen and mohit bansal

fast tive summarization with reinforce selected tence rewriting
in proceedings of the annual meeting of the association for computational guistics volume long papers pages
association for computational linguistics
jianpeng cheng and mirella lapata

neural marization by extracting sentences and words
in proceedings of the annual meeting of the sociation for computational linguistics volume long papers pages berlin germany
sociation for computational linguistics
jacob devlin ming wei chang kenton lee and kristina toutanova

bert pre training of deep bidirectional transformers for language in proceedings of the conference standing
of the north american chapter of the association for computational linguistics human language technologies volume long and short papers pages minneapolis minnesota
ation for computational linguistics
yue dong zichao li mehdi rezagholizadeh and jackie chi kit cheung

editnts an neural programmer interpreter model for sentence in proceedings of cation through explicit editing
the annual meeting of the association for putational linguistics pages florence italy
association for computational linguistics
yue dong yikang shen eric crawford herke van hoof and jackie chi kit cheung

banditsum extractive summarization as a contextual bandit
in proceedings of the conference on cal methods in natural language processing pages
association for computational tics
greg durrett taylor berg kirkpatrick and dan klein

learning based single document rization with compression and anaphoricity in proceedings of the annual straints
ing of the association for computational linguistics volume long papers pages
ation for computational linguistics
gunes erkan and dragomir r radev

lexrank graph based lexical centrality as salience in text summarization
journal of articial intelligence search
dialogue pages
association for tional linguistics
patrick fernandes miltiadis allamanis and marc brockschmidt

structured neural tion
arxiv preprint

matt gardner joel grus mark neumann oyvind tafjord pradeep dasigi nelson f
liu matthew ters michael schmitz and luke zettlemoyer

a deep semantic natural language in proceedings of workshop for cessing platform
nlp open source software nlp oss pages melbourne australia
association for tional linguistics
sebastian gehrmann yuntian deng and alexander rush

bottom up abstractive summarization
in proceedings of the conference on cal methods in natural language processing pages
association for computational tics
karl moritz hermann tomas kocisky edward stette lasse espeholt will kay mustafa man and phil blunsom

teaching machines to read and comprehend
in c
cortes n
d
lawrence d
d
lee m
sugiyama and r
nett editors advances in neural information cessing systems pages
curran ciates inc
tsutomu hirao yasuhisa yoshida masaaki nishino norihito yasuda and masaaki nagata

document summarization as a tree knapsack in proceedings of the conference on lem
empirical methods in natural language processing seattle washington usa
yangfeng ji and jacob eisenstein

tion learning for text level discourse parsing
in ceedings of the annual meeting of the tion for computational linguistics volume long papers pages baltimore maryland
ation for computational linguistics
chris kedzie kathleen mckeown and hal daume iii

content selection in deep learning models of in proceedings of the summarization
ference on empirical methods in natural language processing pages
thomas n kipf and max welling

supervised classication with graph convolutional networks
in proceedings of iclr
kenton lee luheng he mike lewis and luke moyer

end to end neural coreference in proceedings of the conference on lution
empirical methods in natural language processing pages copenhagen denmark
association for computational linguistics
junyi jessy li kapil thadani and amanda stent

the role of discourse units in near extractive in proceedings of the annual rization
ing of the special interest group on discourse and chin yew lin

rouge a package for matic evaluation of summaries
in text tion branches out
yang liu and mirella lapata

text in proceedings of tion with pretrained encoders
the conference on empirical methods in ural language processing and the international joint conference on natural language processing emnlp ijcnlp pages hong kong china
association for computational linguistics
annie louis aravind joshi and ani nenkova

discourse indicators for content selection in rization
in proceedings of the sigdial ference pages tokyo japan
association for computational linguistics
william c mann and sandra a thompson

rhetorical structure theory toward a functional ory of text organization
text interdisciplinary nal for the study of discourse
christopher manning mihai surdeanu john bauer jenny finkel steven bethard and david mcclosky

the stanford corenlp natural language processing toolkit
in proceedings of annual meeting of the association for computational guistics system demonstrations pages
sociation for computational linguistics
eleni miltsakaki rashmi prasad aravind joshi and bonnie webber

the penn discourse in proceedings of the fourth international bank
conference on language resources and evaluation
ramesh nallapati feifei zhai and bowen zhou

summarunner a recurrent neural work based sequence model for extractive rization of documents
in aaai conference on cial intelligence
shashi narayan shay b
cohen and mirella lapata

ranking sentences for extractive tion with reinforcement learning
in proceedings of the conference of the north american ter of the association for computational linguistics human language technologies volume long pers pages
association for tional linguistics
ani nenkova kathleen mckeown al

matic summarization
foundations and trends in information retrieval
colin raffel noam shazeer adam roberts katherine lee sharan narang michael matena yanqi zhou wei li and peter j liu

exploring the limits of transfer learning with a unied text to text former
arxiv preprint

alexander m
rush sumit chopra and jason weston

a neural attention model for abstractive in proceedings of the sentence summarization
conference on empirical methods in natural language processing pages
association for computational linguistics
evan sandhaus

the new york times annotated corpus
linguistic data consortium philadelphia
abigail see peter j
liu and christopher d
ning

get to the point summarization with pointer generator networks
in proceedings of the annual meeting of the association for tational linguistics volume long papers pages
association for computational tics
eva sharma luyang huang zhe hu and lu wang

an entity driven framework for abstractive in proceedings of the summarization
ference on empirical methods in natural language processing and the international joint ence on natural language processing ijcnlp pages
jiwei tan xiaojun wan and jianguo xiao

abstractive document summarization with a in proceedings based attentional neural model
of the annual meeting of the association for computational linguistics volume long papers pages vancouver canada
association for computational linguistics
ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan n gomez ukasz kaiser and illia polosukhin

attention is all in advances in neural information you need
cessing systems pages
minjie wang lingfan yu da zheng quan gan yu gai zihao ye mufei li jinjing zhou qi huang chao ma ziyue huang qipeng guo hao zhang haibin lin junbo zhao jinyang li alexander j smola and zheng zhang

deep graph brary towards efcient and scalable deep iclr workshop on representation ing on graphs
learning on graphs and manifolds
yizhong wang sujian li and jingfeng yang

toward fast and accurate neural discourse in proceedings of the conference on tation
empirical methods in natural language processing pages brussels belgium
association for computational linguistics
florian wolf and edward gibson

representing discourse coherence a corpus based study
tational linguistics
jiacheng xu danlu chen xipeng qiu and xuanjing huang

cached long short term memory ral networks for document level sentiment in proceedings of the conference on cation
empirical methods in natural language ing pages austin texas
association for computational linguistics
jiacheng xu and greg durrett

neural tive text summarization with syntactic compression
in proceedings of the conference on cal methods in natural language processing hong kong china
association for computational guistics
jin ge yao xiaojun wan and jianguo xiao

cent advances in document summarization
edge and information systems
michihiro yasunaga rui zhang kshitijh meelu ayush pareek krishnan srinivasan and dragomir radev

graph based neural multi document summarization
in proceedings of the ence on computational natural language learning conll pages vancouver canada
association for computational linguistics
yasuhisa yoshida jun suzuki tsutomu hirao and masaaki nagata

dependency based course parser for single document summarization
in proceedings of the conference on pirical methods in natural language processing emnlp pages doha qatar
ation for computational linguistics
xingxing zhang mirella lapata furu wei and ming zhou

neural latent extractive document in proceedings of the summarization
ference on empirical methods in natural language processing pages
association for tational linguistics
xingxing zhang furu wei and ming zhou

bert document level pre training of hierarchical bidirectional transformers for document in proceedings of the annual meeting tion
of the association for computational linguistics pages florence italy
association for computational linguistics
ming zhong pengfei liu danqing wang xipeng qiu and xuanjing huang

searching for tive neural extractive summarization what works and what s next
in proceedings of the annual meeting of the association for computational guistics pages florence italy
tion for computational linguistics
qingyu zhou nan yang furu wei shaohan huang ming zhou and tiejun zhao

neural ument summarization by jointly learning to score and select sentences
in proceedings of the nual meeting of the association for computational linguistics volume long papers pages
association for computational linguistics
a appendix figure provides three sets of examples of the constructed graphs from cnndm
specically gc is strictly symmetric and self loop is added to all the nodes to prevent the graph from growing too sparse
on the other hand all of the on diagonal entries in gr are zero because the node from rst graph never points to itself
figure examples of the adjacent matrix of ence graphs gc and rst graphs gr

