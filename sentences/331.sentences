multi fact correction in abstractive text summarization yue shuohang zhe yu jackie chi kit jingjing mcgill university dynamics ai research yue

mcgill
ca shuowa zhe
gan yu
cheng jingjl
com t c o l c
s c v
v i x r a abstract pre trained neural abstractive summarization systems have dominated extractive gies on news summarization performance at least in terms of rouge
however generated abstractive summaries often face the pitfall of factual inconsistency generating correct facts with respect to the source text
to address this challenge we propose fact a suite of two factual correction models that leverages knowledge learned from tion answering models to make corrections in system generated summaries via span tion
our models employ single or masking strategies to either iteratively or regressively replace entities in order to ensure the source text semantic consistency w

t
while retaining the syntactic structure of maries generated by abstractive tion models
experiments show that our els signicantly boost the factual consistency of system generated summaries without cing summary quality in terms of both matic metrics and human evaluation
introduction informative text summarization aims to shorten a long piece of text while preserving its main message
existing systems can be divided into two main types extractive and abstractive
tractive strategies directly copy text snippets from the source to form summaries while abstractive strategies generate summaries containing novel sentences not found in the source
despite the fact that extractive strategies are simpler and less pensive and can generate summaries that are more grammatically and semantically correct tive strategies are becoming increasingly popular thanks to its exibility coherency and vocabulary diversity zhang et al

of this work was done when the rst author was an intern at microsoft
cnn about a quarter of a million tralian homes and businesses have no power after a once in a decade storm battered ney and nearby areas
about people have been isolated by ood waters as the roads are cut off and we wo nt be able to reach them for a few days


a quarter of a million australian homes and businesses have no power after a decade
about a quarter of a million australian homes and businesses have no power after a once in a decade storm
all the victims including killed and injured have been identied as senior high school students of the second senior high school of ruzhou city central china s henan province local police said friday
killed injured in central china school shooting
killed injured in central china school shooting
st clare s catholic primary school in ingham has met with equality leaders at the city council to discuss a complaint from the pupil s family
the council is supporting the school to ensure its policies are appropriate


a muslim school has been accused of ing the equality act by refusing to wear scarves
a catholic school has been accused of ing the equality act by refusing to wear scarves
cnndm source bottom up summary corrected by spanfact gigaword source generator summary corrected by spanfact xsum source bertabs summary corrected by spanfact table examples of factual error correction on ent summarization datasets
factual errors are marked in red
corrections made by the proposed spanfact models are marked in orange
recently with the advent of transformer based models vaswani et al
pre trained using self supervised objectives on large text corpora devlin et al
radford et al
lewis et al
raffel et al
abstractive marization models are surpassing extractive ones on automatic evaluation metrics such as rouge lin
however several studies falke et al
goodrich et al
kryscinski et al
wang et al
durmus et al
maynez et al
observe that despite high rouge scores system generated abstractive summaries are often factually inconsistent with spect to the source text
factual inconsistency is a well known problem for conditional text tion which requires models to generate readable text that is faithful to the input document
quently sequence to sequence generation models need to learn to balance signals between the source for faithfulness and the learned language eling prior for uency kryscinski et al

the dual objectives render abstractive tion models highly prone to hallucinating content that is factually inconsistent with the source ments maynez et al

prior work has pushed the frontier of teeing factual consistency in abstractive rization systems
most focus on proposing tion metrics that are specic to factual consistency as multiple human evaluations have shown that rouge or bertscore zhang et al
relates poorly with faithfulness kryscinski et al
maynez et al

these evaluation models range from using fact triples goodrich et al
textual entailment predictions falke et al
adversarially pre trained classiers kryscinski et al
to question answering qa systems wang et al
durmus et al

it is worth noting that qa based evaluation metrics show surprisingly high correlations with human judgment on factuality wang et al
indicating that qa models are robust in capturing facts that can benet summarization tasks
on the other hand some work focuses on model design to incorporate factual triples cao et al
zhu et al
or textual entailment li et al
falke et al
to boost factual consistency in generated summaries
such models are efcient in boosting factual scores but often at the expense of signicantly lowering rouge scores of the generated summaries
this happens because the models struggle between generating pivotal content while retaining true facts often with an eventual propensity to sacricing mativeness for the sake of correctness of the mary
in addition these models inherit the bone of generative models that suffer from cination despite the regularization from complex knowledge graphs or text entailment signals
in this work we propose spanfact a suite of two neural based factual correctors that improve summary factual correctness without sacricing informativeness
to ensure the retention of tic meaning in the original documents while ing the syntactic structures generated by advanced summarization models we focus on factual edits on entities only a major source of hallucinated rors in abstractive summarization systems in tice kryscinski et al
maynez et al

the proposed model is inspired by the observation that fact checking qa model is a reliable medium in assessing whether an entity should be included in a summary as a fact wang et al
mus et al

to our knowledge we are the rst to adapt qa knowledge to enhance tive summarization
compared to sequential eration models that incorporate complex edge graph and nli mechanisms to boost ity our approach is lightweight and can be ily applied to any system generated summaries without retraining the model
empirical results on multiple summarization datasets show that the proposed approach signicantly improves rization quality over multiple factuality measures without sacricing rouge scores
our contributions are summarized as follows
i we propose spanfact a new factual correction framework that focuses on correcting erroneous facts in generated summaries generalizable to any summarization system
we propose two ods to solve multi fact correction problem with single or multi span selection in an iterative or auto regressive manner respectively
imental results on multiple summarization marks demonstrate that our approach can cantly improve multiple factuality measurements without a huge drop on rouge scores
related work the general neural based encoder decoder ture for abstractive summarization is rst posed by rush et al

later work proves this structure with better encoders such as lstms chopra et al
and grus pati et al
that are able to capture range dependencies as well as with reinforcement learning methods that directly optimize rization evaluation scores paulus et al

one drawback of the earlier neural based rization models is the inability to produce out figure training example created for the qa span prediction model upper right and the auto regressive fact correction model bottom right
vocabulary words as the model can only ate whole words based on a xed vocabulary
see et al
proposes a pointer generator work that can copy words directly from the source through a pointer network vinyals et al
in addition to the traditional sequence to sequence generation model
abstractive summarization starts to shine with the advent of self supervised algorithms which low deeper and more complicated neural networks such as transformers vaswani et al
to learn diverse language priors from large scale pora
models such as bert devlin et al
gpt radford et al
and bart lewis et al
have achieved new state of the art performances on abstractive summarization liu and lapata lewis et al
zhang et al
shi et al
fabbri et al

these models often netune pre trained transformers with supervised summarization datasets that tain pairs of source and summary
inconsistency however encoder decoder architectures widely used in abstractive summarization systems are inherently difcult to control and prone to lucination vinyals and le koehn and knowles lee et al
and often leads to factual the system generated summary is uent but unfaithful to the source cao et al

studies have shown that to system generated abstractive summaries have factual errors falke et al
kryscinski et al
that can not be discovered by rouge scores
recent studies have proposed new ods to ensure factual consistency in tion
cao et al
zhu et al
pose rnn based and transformer based decoders that attend to both source and extracted edge triples respectively
li et al
pose an entailment reward augmented likelihood training objective and falke et al
proposes to rerank beam results based on entailment scores to the source
our fact correction models are inherently ferent from these models as we focus on correcting summaries generated by any model
our models are trained with the objective of dicting masked entities identied for fact tion figure and learn to ll in the entity masks of any system generated summaries with single or multi span selection mechanism figure
the most similar work to ours is proposed concurrently by meng et al
where they ne tune a bart lewis et al
model on distant vision examples and use it as a post editing model for factual error correction
multi fact correction models in this section we describe two models proposed for factual error correction qa span fact rection model and auto regressive fact rection model
as both methods rely on span selection with different masking and prediction strategies we call them spanfact collectively

problem formulation let y be a document summary pair where


xm is the source sequence with m tokens and y


yn is the target sequence with n tokens
an abstractive marization model aims to model the conditional figure model architecture left qa span fact correction model
right auto regressive fact correction model
likelihood which can be factorized into a product



where



denote the preceding tokens before tion t
the conditional maximum likelihood jective ideally requires summarization models to not only optimize for informativeness but also rectness
however in reality this often fails as the models have a high propensity for leaning towards informativeness than correctness li et al

suppose a summarization system generates a sequence of tokens n to form a summary
our factual correction models aim to edit an informative yet incorrect summary into


k such that


where f is a metric measuring factual consistency between the source and system summary

span selection dataset our fact correction models are inspired by the span selection task which is often used in ing comprehension tasks such as question ing
figure shows examples of the span lection datasets we created for training our span and auto regressive fact correction models respectively
the query is a reference summary masked with one or all and the passage is the corresponding source document to be marized
if an entity appears multiple times in the source document we rank them based on the fuzzy string matching scores a variation of enshtein distance between the query sentence and this work we use spacy ner tagger honnibal and montani to identify entities for data construction
the source sentence containing the entity
our models explicitly learn to predict the span of the masked entity rather than pointing to a specic ken as in pointer network vinyals et al
because the original tokens and replaced tokens often have different lengths
our qa span fact correction model iteratively mask and replace one entity at a time while the auto regressive model masks all the entities taneously and replace them in an auto regressive fashion from left to right
figure shows an overview of our models
comparing the two els the qa span fact correction model works ter when only a few errors exist in the draft mary as the prediction of each mask is relatively independent of each other
on the other hand the auto regressive fact correction model starts with a skeleton summary that has all the entities masked which is often more robust when summaries tain many factual errors

qa span fact correction model in the iterative setting our model aims to conduct entity correction by answering a query that tains only one mask at a time
suppose a system summary has t entities
at time step i we mask the i th entity and use this masked sequence as the query to our qa span model
the prediction is placed into the masked slot in the query to ate an updated system summary to be used in the next step






given the source text and a masked query q m our iterative rection model aims to predict the answer span via modeling start and end
for span selection we use the model which adds two separate non linear ers on top of transformers as pointers to the start and end token position for the answer
we ize the fact correction model from a pre trained bert model devlin et al
and perform netuning with the span selection datasets we ated from the summarization datasets figure
the input to the bert model is a tion of two segments the masked query q and the source separated by special delimiter markers as q sep
each token in the quence is assigned with three embeddings token embedding position embedding and tion embedding
these embeddings are summed into a single vector and fed to the multi layer transformer model l h hl l l where are the input vectors and l represents the depth of stacked layers
ln and mhatt are layer normalization and multi head attention operations vaswani et al

the top layer provides the hidden states for the input tokens with rich textual information
the start s and end e of the answer span are predicted as astart i aend i e i j j qs i s hi bs e hi be where h is the number of encoder s hidden states ws we rd and bs be r are trainable rameters
the nal span is selected based on the argmax of eqn
and with the constraint of pstart pend and pend pstart

auto regressive fact correction model one disadvantage of the qa style span prediction strategy is that if the sequence contains too many factual errors masking out one entity at a time may lead to highly erroneous skeleton summary
com huggingface transformers segmentation embedding is used to distinguish the query with two special tokens cls and sep and the source in our models
to start with
the model might be making tions on top of wrong entities from later in the quence
masking one entity at a time is essentially a greedy local method that is prone to error mulation
to alleviate this issue we propose a new sequential fact correction model to handle errors in a more global manner with beam search
cally we mask out all the entities simultaneously and use a novel auto regressive span selection coder to predict llers for the multiple masks quentially
by doing this we assume dependency between the masks the earlier predicted entities will be used as corrected context for better tions in the later steps
given a source text


xn and


a draft summary m
our model rst masks out all the entities with t masks and leaves a skeleton summary as the query q m
then we concatenate the query q with the source ilar to section
as inputs to the encoder
the inputs are fed into bert to obtain contextual den representations









maskt we then select the encoder s hidden states for the t masks


as partial input to an auto regressive transformer based decoder
unlike generation tasks that require an eos ken to indicate the end of decoding our decoder runs t steps to predict the answer spans for these t masks
at step t we rst fuse the hidden sentation rd of the t th mask ken and previously predicted entity representation rd sent zt sent where w sent the sentation of cls token and denotes vector concatenation
the input zt is then fed to the transformer coder as in eqn
and to generate the coder s hidden state t at time step t
based on t we use a two pointer network to predict the start and end positions of the answer entity in the source encoder s hidden states
this is achieved with cross attention of t w

t
the encoder s den states similar to eqn and
this tion results in two distributions over the encoder s hidden states for the start and end span positions
the nal prediction of the start and end positions for mask t is obtained by taking the over argmax is used for selecting the start and end indexes the pointer position distributions pstart arg pend arg


astart


aend m m under the constraint that pstart pend and pend pstart
based on the start and end positions for the dicted entity we can obtain the predicted entity representation at time step t as the mean over the in span encoder s hidden states sent t mean hpend which is used as the input for the next step of it is worth noting that although the decoding
argmax operations in eqn
and are differentiable the model is trained based on the start and end positions of the ground truth answer w

t
the start and end logits in eqn
and which makes the gradient back propagates to the encoder
meanwhile the encoder s hidden states used to compose sent in eqn
also carry the gradients
during inference beam search is used to nd the best sequence of predicted spans in the source to replace the masks
i compared to the conventional pointer network vinyals et al
see et al
that only points to one token at a time our sequential span selection decoder has the exibility to replace a mask by any number of entity tokens which is ten required in summary factual correction
experiment in this section we present our results on using spanfact for multiple summarization datasets

experimental setup training data for our fact correction models are generated as described in section
on cnn dailymail hermann et al
xsum narayan et al
and gigaword graff et al
rush et al

the statistics of these three dataset are provided in table
during ing if an entity does not have a corresponding span in the source we point the answer span to the cls token
during inference if the swer span predicted is the cls token we place back the original masked entity
for the answer span and the softmax is used for computing the loss for back propagation
datasets docs train val test doc len
summ
len
mask cnn dailymail xsum gigaword











table comparison of summarization datasets on train validation test set splits average document and summary length numbers of words
we also report the average number of entity masks on the reference summary for each dataset
our fact correction models are implemented via the huggingface transformers library wolf et al
in pytorch paszke et al

we initialize all encoder models with the point of an uncased large bert model trained on english data and squad for all periments
both source and target texts were kenized with bert s sub words tokenizer
the max sequence length is set to for the encoder
we use a shallow transformer decoder for the auto regressive span selection decoder as the pre trained bert large encoder is already robust for selecting right spans in the single span tion task with only two pointers section

the transformer decoder has hidden units and the feed forward intermediate size for all layers is
all models were netuned on our span tion data for epochs with batch size
adamw optimizer loshchilov and hutter with and an initial learning rate is used for training
our learning rate schedule follows a linear decay scheduler with
ing inference we use beam search with and k constraint for the distance between the start and end pointer
the best model checkpoints are chosen based on performance on the validation set
experiments are conducted using quadro rtx gpus with gb of memory

evaluation metrics we use three automatic evaluation metrics to uate our models
the rst is rouge lin the standard summarization quality metric which has high correlation with summary ness in the news domain kryscinski et al

since rouge has been criticized for its poor correlation with factual consistency kryscinski et al
wang et al
we use two ditional automatic metrics that specically focus on factual consistency factcc kryscinski et al
qgqa factcc rouge qgqa factcc rouge datasets bottom up split encoders qa span auto regressive split encoders qa span auto regressive split encoders qa span auto regressive transformerabs split encoders qa span auto regressive















sent















































l















table factual correctness scores and rouge scores on cnn dailymail test set
and qags wang et al

factcc is a pre trained binary classier that evaluates the tuality of a system generated summary by ing whether it is consistent or inconsistent w

t
the source
this classier was trained on sarial examples obtained by heuristically injecting noise into reference summaries
in addition very recent work proposed based models for factuality evaluation wang et al
durmus et al
maynez et al
and wang et al
showed that their evaluation models have higher correlation with human judgements on factuality when compared with factcc kryscinski et al

we thus include our re implementation of a question eration and question answering model qgqa following wang et al
as an evaluation metric for factuality
this model generates a set of questions based on the system generated summary and then answers these questions ing either the source or the summary to obtain two sets of answers
the answers are compared against each other using an answer similarity ric token level and the averaged similarity metric over all questions is used as the qgqa were not able to obtain any of the qa evaluation model or code from wang et al
durmus et al
maynez et al
as the authors are still in the stage of making the code public
we used pre trained unilm model for question generation qg and ing model for question answering qa
the qg model is ne tuned on newsqa trischler et al
with answer conditional task wang et al
and the qa model is pre trained on squad
rajpurkar et al

datasets split encoders qa span auto regressive split encoders qa span auto regressive transformerabs split encoders qa span auto regressive











sent



































l











table factual correctness scores and rouge scores on xsum test set
score
answers generated from a highly faithful system summary should be similar to those ated from the source

baselines the following abstractive we compare against summarization baselines
on cnndm and xsum we use bertsumabs and transformerabs liu and lapata
in tion we also compare with bottom up gehrmann et al

on gigaword we use the generator see et al
base and full parse models song et al
for comparison
for the factual correction baseline we compare with the two encoder pointer split encoder shah et al
which employs a similar setting to ours for masking entities w

t
the source and uses dual encoders to copy and generate from both the source and the masked query for fact update
compared to our span tion models that can ll in the mask with any ber of tokens their models aim to regenerate the mask query based on the source
in other words their decoder regenerates the whole sequence ken by token with a pointer generator which herits the backbone of generative models that fer from hallucination

experimental results tables and summarize the results on the cnn dailymail xsum and gigaword datasets respectively
each block in the tables compares the original summarization model s output with
com split qgqa factcc rouge datasets genparse base split encoders qa span auto regressive genparse full split encoders qa span auto regressive pointer generator split encoders qa span auto regressive











sent



































l











bertabs better worse same qa span vs
original auto regressive vs
original qa span vs
auto regressive






transformerabs better worse same qa span vs
original auto regressive vs
original qa span vs
auto regressive





bottom up better worse same qa span vs
original auto regressive vs
original qa span vs
auto regressive




table factual correctness scores and rouge scores on gigaword test set
table human evaluation results on pairwise ison of factual correctness on randomly sampled articles
the corrected outputs obtained by our baseline and proposed models
on cnn dailymail table our correction models signicantly boost factual consistency measures qgqa and factcc by large margins with only small drops on rouge
this shows our models have the ability to improve the correctness of system generated summaries without ing informativeness
when comparing our two proposed models we observe that the qa span model performs better than the auto regressive model
this is expected as cnn dailymail erence summaries tend to be more extractive see et al
and summarization models tend to make few errors per summary narayan et al

thus the iterative procedure of the span model is more robust with high precision as it has more correct context from the query with only minimum negative inuence from other current errors
this is also reected in the high scores of qgqa and factcc across all the models we tested
since qgqa and factcc are based on the comparing system generated summary w

t
source text high score means high semantic larity between system summary to the source
on xsum table and gigaword table both of our correction models boost factual sistency measures by large margins with a slight drop in rouge
to
on average
this is still encouraging as abstractive summarization models that use complex factual controlling ponents for generation often have drops of rouge points zhu et al

we also notice that the qgqa and factcc scores of all summarization models are lower than that on cnn dailymail
the scores are especially low on xsum
this is likely due to the data struction protocol of xsum where the rst tence of a source document is used as the mary and the remainder of the article is used as the source
as a result many entities that appear in the reference summary never appear in the source which may cause abstractive summarization els to hallucinate severely with many factual errors maynez et al

as the system summaries often contain many errors our qa span model that relies on answering a single mask query often has the wrong context to condition on at each step which negatively affects the performance of this model
in contrast the strategy of masking all the entities would provide the auto regressive model a better query for entity replacement
we can serve in table that the auto regressive model forms better than the qa span model on xsum

human evaluation to provide qualitative analysis of the proposed models we conduct human evaluation on wise comparison of cnn dailymail summaries enhanced by different correction strategies
we select three state of the art abstractive tion models as the backbones and collect three sets of pairwise summaries for each setting i original vs
qa span corrected original vs
auto regressive corrected qa span rected vs
auto regressive corrected
nine sets of randomly selected samples total ples are labeled by amt tuckers
for each pair in anonymized order three annotators from amazon mechanical turk amt are asked to judge which is more factually correct based on the factcc dataset factcc score qaqg human eval before corr
qa span auto regressive








the reviewers for their valuable comments and cial thanks to yuwei fang and other members of the microsoft dynamics ai research team for the feedback and suggestions
table test results on the human annotated dataset provided by factcc kryscinski et al

we show the performance comparisons of the original summaries and the summaries corrected by spanfact
references source document
as shown in table summaries from our two models are chosen more frequently as the factually correct one compared to the nal
between the two correction models the erences are comparable
in addition we also test our fact tion models on the factcc test set provided by kryscinski et al
and manually checked the outputs
table shows the results of the inal summaries and the summaries corrected by our models in terms of automatic fact evaluation and our manual evaluation
among generated summary sentences were incorrect
the qa span model was able to correct out of right and the auto regressive model was able to correct out of
among the sentences that are labeled as correct by the annotators in kryscinski et al
our two models made and wrong changes in the entities while keeping most of the entities unchanged or changed with equivalent entities
conclusion we present spanfact a suite of two factual rection models that use span selection mechanisms to replace one or multiple entity masks at a time
spanfact can be used for fact correction on any stractive summaries
empirical results show that our models improve the factuality of summaries generated by state of the art abstractive rization systems without a huge drop on rouge scores
for future work we plan to apply our method for other type of spans such as noun phrases verbs and clauses
acknowledgments this research was supported in part by microsoft dynamics ai research and the canada far ai chair program
we would like to thank ziqiang cao furu wei wenjie li and sujian li

faithful to the original fact aware neural tive summarization
in thirty second aaai ence on articial intelligence
sumit chopra michael auli and alexander m rush

abstractive sentence summarization with tentive recurrent neural networks
in proceedings of the conference of the north american ter of the association for computational linguistics human language technologies pages
jacob devlin ming wei chang kenton lee and kristina toutanova

bert pre training of deep bidirectional transformers for language standing
in proceedings of the conference of the north american chapter of the association for computational linguistics human language nologies volume long and short papers pages
esin durmus he he and mona diab

feqa a question answering evaluation framework for fulness assessment in abstractive summarization
in proceedings of the annual meeting of the sociation for computational linguistics
alexander fabbri irene li tianwei she suyi li and dragomir radev

multi news a large scale multi document summarization dataset and tive hierarchical model
in proceedings of the annual meeting of the association for tional linguistics pages florence italy
association for computational linguistics
tobias falke leonardo fr ribeiro prasetya ajie utama ido dagan and iryna gurevych

ranking generated summaries by correctness an interesting but challenging application for natural language inference
in proceedings of the nual meeting of the association for computational linguistics pages
sebastian gehrmann yuntian deng and alexander m rush

bottom up abstractive summarization
in proceedings of the conference on cal methods in natural language processing pages
ben goodrich vinay rao peter j liu and mad saleh

assessing the factual accuracy of generated text
in proceedings of the acm sigkdd international conference on knowledge discovery data mining pages
excludes the cases where the model would change a person s full name by last name or break the uency due to spacy ner errors
david graff junbo kong ke chen and kazuaki maeda

english gigaword
linguistic data consortium philadelphia
karl moritz hermann tomas kocisky edward grefenstette lasse espeholt will kay mustafa leyman and phil blunsom

teaching in advances in chines to read and comprehend
neural information processing systems pages
matthew honnibal and ines montani

spacy natural language understanding with bloom dings convolutional neural networks and tal parsing
to appear
philipp koehn and rebecca knowles

six in lenges for neural machine translation
ceedings of the first workshop on neural machine translation pages
the conference on empirical methods in ural language processing emnlp
ramesh nallapati bowen zhou cicero dos santos caglar gulcehre and bing xiang

stractive text summarization using sequence sequence rnns and beyond
in proceedings of the signll conference on computational ral language learning pages
shashi narayan shay b cohen and mirella lapata

do nt give me the details just the summary topic aware convolutional neural networks for in proceedings of the treme summarization
conference on empirical methods in natural guage processing pages
wojciech kryscinski bryan mccann caiming xiong and richard socher

evaluating the factual consistency of abstractive text summarization
arxiv preprint

adam paszke sam gross soumith chintala gory chanan edward yang zachary devito ing lin alban desmaison luca antiga and adam lerer

automatic differentiation in pytorch
katherine lee orhan firat ashish agarwal clara fannjiang and david sussillo

hallucinations in neural machine translation
mike lewis yinhan liu naman goyal jan ghazvininejad abdelrahman mohamed omer levy veselin stoyanov and luke zettlemoyer

bart denoising sequence to sequence training for natural language generation translation and comprehension
in proceedings of the nual meeting of the association for computational linguistics
haoran li junnan zhu jiajun zhang and chengqing zong

ensure the correctness of the mary incorporate entailment knowledge into stractive sentence summarization
in proceedings of the international conference on computational linguistics pages
chin yew lin

rouge a package for matic evaluation of summaries
in text tion branches out pages
yang liu and mirella lapata

text in proceedings of tion with pretrained encoders
the conference on empirical methods in ural language processing and the international joint conference on natural language processing emnlp ijcnlp pages
ilya loshchilov and frank hutter

pled weight decay regularization
arxiv preprint

joshua maynez shashi narayan bernd bohnet and ryan mcdonald

on faithfulness and ality in abstractive summarization
in proceedings of the annual meeting of the association for computational linguistics
cao meng yue cheung dong jiapeng wu and jackie chi kit

factual error correction for stractive summarization models
in proceedings of romain paulus caiming xiong and richard socher

a deep reinforced model for abstractive marization
in international conference on ing representations
alec radford karthik narasimhan tim salimans and improving language ilya sutskever

standing by generative pre training
colin raffel noam shazeer adam roberts katherine lee sharan narang michael matena yanqi zhou wei li and peter j liu

exploring the limits of transfer learning with a unied text to text journal of machine learning research former

pranav rajpurkar robin jia and percy liang

know what you do nt know unanswerable tions for squad
in proceedings of the annual meeting of the association for computational guistics volume short papers pages
alexander m rush sumit chopra and jason weston

a neural attention model for abstractive in proceedings of the tence summarization
conference on empirical methods in natural guage processing pages
abigail see peter j liu and christopher d manning

get to the point summarization with generator networks
in proceedings of the nual meeting of the association for computational linguistics volume long papers pages
darsh j shah tal schuster and regina barzilay

in automatic fact guided sentence modication
proceedings of the thirty fourth aaai conference on articial intelligence
tian shi ping wang and chandan k
reddy

leafnats an open source toolkit and live demo system for neural abstractive text summarization
in proceedings of the conference of the north american chapter of the association for tational linguistics demonstrations pages minneapolis minnesota
association for tional linguistics
kaiqiang song logan lebanoff qipeng guo xipeng qiu xiangyang xue chen li dong yu and fei liu

joint parsing and generation for tive summarization
in proceedings of the fourth aaai conference on articial intelligence pages
adam trischler tong wang xingdi yuan justin ris alessandro sordoni philip bachman and heer suleman

newsqa a machine in proceedings of the hension dataset
shop on representation learning for nlp pages
ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan n gomez ukasz kaiser and illia polosukhin

attention is all you need
in advances in neural information cessing systems pages
oriol vinyals meire fortunato and navdeep jaitly

pointer networks
in advances in neural formation processing systems pages
oriol vinyals and quoc le

a neural tional model
arxiv preprint

alex wang kyunghyun cho and mike lewis

asking and answering questions to evaluate the in proceedings of tual consistency of summaries
the annual meeting of the association for putational linguistics
thomas wolf lysandre debut victor sanh julien chaumond clement delangue anthony moi ric cistac tim rault remi louf morgan icz and jamie brew

huggingface s formers state of the art natural language ing
arxiv

jingqing zhang yao zhao mohammad saleh and ter j liu

pegasus pre training with tracted gap sentences for abstractive summarization
in thirty seventh international conference on chine learning icml
tianyi zhang varsha kishore felix wu kilian q
weinberger and yoav artzi

bertscore in evaluating text generation with bert
tional conference on learning representations
chenguang zhu william hinthorn ruochen xu qingkai zeng michael zeng xuedong huang and meng jiang

boosting factual correctness of abstractive summarization with knowledge graph
arxiv preprint

jerusalem ame of remembrance burns in jerusalem and a song of memory haunts valerie braham as it never has before
this year israel s memorial day ration is for bereaved family members such as braham
now i truly understand everyone who has lost a loved one braham said
her husband philippe braham was one of people killed in january s terror attacks in paris
he was in a kosher supermarket when a gunman stormed in killing four people all of them jewish
france s memorial day commemoration is for bereaved family members as braham
valerie braham was one of people killed in january s terror attacks in paris
israel s memorial day commemoration is for bereaved family members as braham
philippe braham was one of people killed in january s terror attacks in paris
i had to describe the u
s
relationship in one word it would be matched



america is alienating some of our closest allies because of the iran deal and iran is picking up new ones and bolstering relations with old ones who are growing more dependent because they see irans power rising


iran is alienating some of our closest allies because of the iran deal and iran is picking up new ones
america is alienating some of our closest allies because of the iran deal and iran is picking up new ones
north pacic gray whale has earned a spot in the record books after completing the longest migration of a mammal ever recorded
the whale named varvara swam nearly miles kilometers according to a release from oregon state university whose scientists helped conduct the whale tracking study
varvara which is russian for barbara left her primary feeding ground off russias sakhalin island to cross the pacic ocean and down the west coast of the united states to baja mexico


a north pacic gray whale swam nearly miles from oregon state university
a north pacic gray whale swam nearly miles from russias sakhalin island
sanaa yemen airstrikes over yemen have resumed once again two days after saudi arabia announced the end of its air campaign
the airstrikes thursday targeted rebel houthi militant positions in three parts of sanaa two yemeni defense ministry ofcials said
the attacks lasted four hours



the saudi led coalition said a new initiative was underway operation renewal of hope focused on the political process
but less than hours later after rebel forces attacked a yemeni military brigade the airstrikes resumed security sources in taiz said
the attacks lasted four hours two days after rebel forces attacked yemeni military troops

the attacks lasted four hours less than hours after rebel forces attacked yemeni military troops
boston the bomb went off steve woolfenden thought he was still standing
that was because as he lay on the ground he was still holding the handles of his son s stroller
he pulled back the stroller s cover and saw that his son leo was conscious but bleeding from the left side of his head
woolfenden checked leo for other injuries and thought let s get out of here



steve woolfenden was conscious but bleeding from the left side of his head
leo was conscious but bleeding from the left side of his head
cnndm source system summary corrected by spanfact cnndm source system summary corrected by spanfact cnndm source system summary corrected by spanfact cnndm source system summary corrected by spanfact cnndm source system summary corrected by spanfact table examples of factual error correction on factcc dataset a human annotated subset from cnndm obtained by kryscinski et al

factual errors by abstractive summarization system are marked in red
corrections made by the proposed spanfact models are marked in orange

