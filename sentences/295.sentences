mlsum the multilingual summarization corpus thomas paul alexis sylvain lamprier benjamin jacopo cnrs france sorbonne universite cnrs paris france recital paris france thomas jacopo paul
ai sylvain
lamprier benjamin

fr r a l c
s c v
v i x r a abstract we present mlsum the rst large scale tilingual summarization dataset
obtained from online newspapers it contains
ticle summary pairs in ve different languages namely french german spanish russian turkish
together with english newspapers from the popular cnn daily mail dataset the collected data form a large scale multilingual dataset which can enable new research tions for the text summarization community
we report cross lingual comparative analyses based on state of the art systems
these light existing biases which motivate the use of a multi lingual dataset
introduction the document summarization task requires eral complex language abilities understanding a long document discriminating what is relevant and writing a short synthesis
over the last few years advances in deep learning applied to nlp have contributed to the rising popularity of this task among the research community see et al
kryscinski et al
scialom et al

as with other nlp tasks the great majority of available datasets for summarization are in english and thus most research efforts focus on the glish language
the lack of multilingual data is partially countered by the application of transfer learning techniques enabled by the availability of pre trained multilingual language models
this proach has recently established itself as the facto paradigm in nlp guzman et al

under this paradigm for encoder decoder tasks a language model can rst be pre trained on a large corpus of texts in multiple languages
then the model is ne tuned in one or more pivot languages for which the task specic data are available
at inference it can still be applied to the different guages seen during the pre training
because of the dominance of english for large scale corpora english naturally established itself as a pivot for other languages
the availability of multilingual pre trained models such as bert multilingual bert allows to build models for target languages different from training data
however previous works reported a signicant performance gap tween english and the target language e

for classication conneau et al
and question answering lewis et al
tasks
a similar approach has been recently proposed for rization chi et al
obtaining again a lower performance than for english
for specic nlp tasks recent research efforts have produced evaluation datasets in several target languages allowing to evaluate the progress of the eld in zero shot scenarios
nonetheless those proaches are still bound to using training data in a pivot language for which a large amount of tated data is available usually english
this vents investigating for instance whether a given model is as tted for a specic language as for any other
answers to such research questions sent valuable information to improve model mance for low resource languages
in this work we aim to ll this gap for the matic summarization task by proposing a scale multilingual summarization mlsum dataset
the dataset is built from online news lets and contains over
m article summary pairs in languages french german spanish sian and turkish which complement an already established summarization dataset in english
the contributions of this paper can be rized as follows
we release the rst large scale multilingual summarization dataset
we provide strong baselines from multilingual abstractive text generation models
we report a comparative cross lingual ysis of the results obtained by different proaches
related work
multilingual text summarization over the last two decades several research works have focused on multilingual text summarization
radev et al
developed mead a document summarizer that works for both english and chinese
litvak et al
proposed to improve multilingual summarization using a netic algorithm
a community driven initiative multiling giannakopoulos et al
marked summarization systems on multilingual data
while the multiling benchmark covers languages it provides relatively few examples in the release
most proposed approaches so far have used an extractive approach given the lack of a multilingual corpus to train abstractive models duan et al

more recently with the rapid progress in matic translation and text generation abstractive methods for multilingual summarization have been developed
ouyang et al
proposed to learn summarization models for three low resource guages somali swahili and tagalog by using an automated translation of the new york times dataset

although this showed only slight ments over a baseline which considers translated outputs of an english summarizer results remain still far from human performance
summarization models from translated data usually under perform as translation biases add to the difculty of rization
following the recent trend of using multi lingual pre trained models for nlp tasks such as gual bert m bert pires et al
or xlm lample and conneau chi et al
posed to ne tune the models for summarization on english training data
the assumption is that the summarization skills learned from english data can transfer to other languages on which the model has been pre trained
however a signicant mance gap between english and the target language is observed following this process
this sizes the crucial need of multilingual training data for summarization

existing multilingual datasets the research community has produced several tilingual datasets for tasks other than tion
we report two recent efforts below noting that both rely on human translations and only provide evaluation data
the cross lingual nli corpus the snli pus bowman et al
is a large scale dataset for natural language inference nli
it is posed of a collection of human written glish sentence pairs associated with their label entailment contradiction or neutral
the genre natural language inference multinli pus is an extension of snli comparable in size but including a more diverse range of text
conneau et al
introduced the cross lingual nli pus xnli to evaluate transfer learning from glish to other languages based on multinli a collection of test and dev pairs were translated by humans in languages
mlqa given a paragraph and a question the question answering qa task consists in ing the correct answer
large scale datasets such as rajpurkar et al
choi et al
trischler et al
have driven fast progress
however these datasets are only in english
to assess how well models perform on other languages lewis et al
recently proposed mlqa an ation dataset for cross lingual extractive qa posed of k qa instances in languages
xtreme the cross lingual transfer tion of multilingual encoders benchmark covers languages over tasks
the summarization task is not included in the benchmark
xglue in order to train and evaluate their formance across a diverse set of cross lingual tasks liang et al
recently released xglue ering both natural language understanding and generation scenarios
while no summarization task is included it comprises a news title ation task the data is crawled from a commercial news website and provided in form of article title pairs for languages german english french spanish and russian

com google instance see the squad leaderboard rajpurkar
bert blob master multilingual
github
io squad
existing summarization datasets we describe here the main available corpora for text summarization
document understanding conference several small and high quality summarization datasets in english harman and over dang have been produced in the context of the ment understanding conference duc
they are built by associating newswire articles with sponding human summaries
a distinctive feature of the duc datasets is the availability of ple reference summaries this is a valuable acteristic since as found by rankel et al
the correlation between qualitative and automatic metrics such as rouge lin decreases signicantly when only a single reference is given
however due to the small number of training data available duc datasets are often used in a domain adaptation setup for models rst trained on larger datasets such as gigaword cnn dm nallapati et al
see et al
or with unsupervised methods dorr et al
mihalcea and tarau barrios al

gigaword again using newswire as source data the english gigaword napoles et al
rush et al
chopra et al
corpus is acterized by its large size and the high diversity in terms of sources
since the samples are not sociated with human summaries prior works on summarization have trained models to generate the headlines of an article given its incipit which duces various biases for learning models
new york times corpus this large corpus for summarization consists of hundreds of thousand of articles from the new york spanning over years
the articles are paired with summaries written by library scientists
although grusky et al
found indications of bias towards extractive approaches several search efforts have used this dataset for tion hong and nenkova durrett et al
paulus et al

cnn daily mail one of the most commonly used dataset for summarization nallapati et al
see et al
paulus et al
dong et al
although originally built for tion answering tasks hermann et al

it consists of english articles from the cnn and the
nist
daily mail associated with bullet point highlights from the article
when used for summarization the bullet points are typically concatenated into a single summary
newsroom composed of
m articles grusky et al
and featuring high diversity in terms of publishers the summaries associated with english news articles were extracted from the web pages metadata they were originally written to be used in search engines and social media
bigpatent sharma et al
collected
lion u
s
patent documents across several nological areas using the google patents public datasets
the patents abstracts are used as target summaries
lcsts the large scale chinese short text summarization dataset hu et al
is built from million short texts from the sina weibo microblogging platform
they are paired with summaries given by the author of each text
the dataset includes summaries which were ally scored by human for their relevance
mlsum as described above the vast majority of rization datasets are in english
for arabic there exist the essex arabic summaries corpus easc el haj et al
and kalimat el haj and koulali those comprise circa and samples respectively
pontes et al
posed a corpus of few hundred samples for spanish portuguese and french summaries
to our edge the only large scale non english tion dataset is the chinese lcsts hu et al

with the increasing interest for cross lingual els the nlp community have recently released multilingual evaluation datasets targeting cation xnli and qa lewis et al
tasks as described in
though still no large scale dataset is avaulable for document summarization
to ll this gap we introduce mlsum the rst large scale multilingual summarization corpus
our corpus provides more than
millions articles in french fr german de spanish es turkish tr and russian ru
being similarly built from news articles and providing a similar amount of training samples per language except for russian as the previously mentioned cnn daily mail it can effectively serve as a multilingual extension of the cnn daily mail dataset
in the following we rst describe the ology used to build the corpus
we then report the corpus statistics and nally interpret the mances of baselines and state of the art models

collecting the corpus the cnn daily mail cnn dm dataset see tion
is arguably the most used large scale dataset for summarization
following the same methodology we consider news articles as the text input and their paired highlights description as the summary
for each language we selected an online newspaper which met the following requirements
being a generalist newspaper ensuring that a broad range of topics is represented for each language allows to minimize the risk of ing topic specic models a fact which would hinder comparative cross lingual analyses of the models

having a large number of articles in their lic online archive

providing written human lights summaries that for can be extracted from the html code of the web page
articles the after a careful preliminary exploration we lected the online version of the following pers le french german el spanish moskovskij russian internet turkish for each outlet we crawled archived articles from to
we applied one simple ter all the articles shorter than words or maries shorter than words are discarded so as to avoid articles containing mostly audiovisual tent
each article was archived on the wayback allowing interested research to re build
lemonde
fr
sueddeutsche

elpais
com
mk
ru
internethaber
com
archive
org or extend mlsum
we distribute the dataset as a list of immutable snapshot urls of the articles along with the accompanying corpus construction allowing to replicate the parsing and processing procedures we employed
this is due to legal reasons the content of the articles is righted and redistribution might be seen as ing of publishing rights
nonetheless we make available upon request an exact copy of the dataset used in this work
a similar approach has been adopted for several dataset releases in the recent past such as question answering corpus mann et al
or xsum narayan et al

we provide further recommended train validation test splits following a ical ordering based on the articles publication dates
in our experiments below we train evaluate the models on the training test splits obtained in this manner
specically we use data from to included for training data for of the dataset for validation up to may and test may december
while this choice is arguably more challenging due to the possible emergence of new topics over time we consider it as the realistic scenario a successful summarization system should be able to deal with
incidentally this also bring the advantage of excluding most cases of leakage across languages it prevents a model for instance from seeing a training sample describing an important event in one language and then being submitted for inference a similar article in another language published around the same time and dealing with the same event

dataset statistics we report statistics for each language in sum in table including those computed on the cnn daily mail dataset english for quick parison
mlsum provides a comparable amount of data for all languages with the exception of sian with ten times less training samples
important characteristics for summarization datasets are the length of articles and summaries the vocabulary size and a proxy for abstractiveness namely the percentage of novel n grams between the article and its human summary
from table we observe that russian summaries are the shortest as well as the most abstractive
com agude wayback machine archiver
com recitalai mlsum using
dataset size training set size mean article length mean summary length compression ratio novelty gram total vocabulary size occurring times fr



de



es



ru



tr



en



table statistics for the different languages
en refers to cnn daily mail and is reported for comparison purposes
article and summary lengths are computed in words
compression ratio is computed as the ratio between article and summary length
novelty is the percentage of words in the summary that were not in the paired article
total vocabulary is the total number of different words and occurring the total number of words occurring times
coupled with the signicantly lower amount of articles available from its online source the task can be seen as more challenging for russian than for the other languages in mlsum
conversely similar characteristics are shared among other guages for instance french and german

topic shift with the exception of turkish the article urls in mlsum allow to identify a category for a given article
in figure we show the shift over gories among time
in particular we plot the most frequent categories per language
models we experimented on mlsum with the established models and baselines described below
those clude supervised and unsupervised methods tractive and abstractive models
for all the iments we train models on a per language basis
we used the recommended hyperparameters for all languages in order to facilitate assessing the robustness of the models
we also tried to train one model with all the languages mixed together but we did not see any signicant difference of performance

extractive summarization models random in order to elaborate and compare the performances of the different models across guages it is useful to include an unbiased model as a point of reference
to that purpose we dene a simple random extractive model that randomly extracts n words from the source document with n xed as the average length of the summary
simply selects the three rst sentences from the input text
sharma et al
among others showed that this is a robust baseline for several summarization datasets such as cnn dm nyt and bigpatent
textrank an unsupervised algorithm proposed by mihalcea and tarau
it consists in puting the co similarities between all the sentences in the input text
then the most central to the ment are extracted and considered as the summary
we used the implementation provided by barrios et al


abstractive summarization models most of the models for abstractive summarization are neural sequence to sequence models sutskever et al
composed of an encoder that encodes the input text and a decoder that generates the mary
oracle extracts the sentences within the input text that maximise a given metric in our ments rouge l given the reference summary
it is an indication of the maximum one could in this achieve with extractive summarization
work we rely on the implementation of narayan et al

pointer generator see et al
proposed the addition of the copy mechanism vinyals et al
on top of a sequence to sequence lstm model
this mechanism allows to efciently copy out of vocabulary tokens leveraging tion bahdanau et al
over the input
we used the publicly available opennmt figure distribution of topics for german top left spanish top right french bottom left and russian bottom right grouped per year
the shaded area for highlights validation and test data
with the default hyper parameters
ever to avoid biases we limited the preprocessing as much as possible and did not use any sentence separators as recommended for cnn dm
this plains the relatively lower reported rouge pared to the model with the full preprocessing
m bert encoder decoder transformer tectures are a very popular choice for text ation
recent research efforts have adapted large pretrained self attention based models for text eration peters et al
radford et al
devlin et al

in particular liu and lapata added a domly initialized decoder on top of bert
ing the use of a decoder dong et al
posed to instead add a decoder like mask during the pre training to unify the language models for both encoding and generating
both these approaches achieved sota results for summarization
in this paper we only report results obtained following dong et al
as in preliminary experiments we observed that a simple multilingual bert bert with no modication obtained comparable performance on the summarization task

net opennmt py summarization
html evaluation metrics rouge arguably the most often reported set of metrics in summarization tasks the oriented understudy for gisting evaluation lin computes the number of n grams similar between the evaluated summary and the human reference summary
meteor the metric for evaluation of lation with explicit ordering banerjee and lavie was designed for the evaluation of machine it is based on the harmonic translation output
mean of unigram precision and recall with recall weighted higher than precision
meteor is often reported in summarization papers see et al
dong et al
in addition to rouge
novelty because of their use of copy nisms some abstractive models have been reported to rely too much on extraction see et al
kryscinski et al

hence it became a mon practice to report the percentage of novel grams produced within the generated summaries
neural metrics several approaches based on neural models have been recently proposed
recent works eyal et al
scialom et al
have proposed to evaluate summaries with qa based methods the rationale is that a good summary should answer the most relevant questions about the oracle random textrank lead pointer generator m bert oracle random textrank lead pointer generator m bert fr





fr





de





de





es





es





ru





ru





tr





tr





en





en





table rouge l top and meteor bottom results obtainedby the models described in
on the different proposed datasets
article
further kryscinski et al
proposed a discriminator trained to measure the factualness of the summary
while bohm et al
learned a metric from human annotation
all these models were only trained on english datasets preventing us to report them in this paper
the availability of mlsum will enable future works to build such metrics in a multilingual fashion
results and discussion the results presented below allow us to compare the models across languages and investigate or pothesize where their performance variations may come from
we can distinguish the following tors to explain differences in the results
differences in the data independently from the language such as the structure of the cle the abstractiveness of the summaries or the quantity of data
differences due to the language itself either due to metric biases e

due to a different morphological type or to biases inherent to the model
while the rst fold of differences have more to do with domain adaptation the second fold motivates further the development of multilingual datasets since they are the only mean to study such phenomenon
turning to the observed results we report in ble the rouge l and meteor scores obtained by each model for all languages
we note that the overall order of systems for each language is preserved when using either metric modulo some swaps between lead and pointer generator but with relatively close scores
russian the low resource language in mlsum for all experimental setups the performance on russian is comparatively low
this can be explained by at least two factors
first the corpus is the most abstractive see table limiting the performance gures obtained for the extractive models random and oracle
second one order of magnitude less training data is available for russian than for the other mlsum languages a fact which can explain the impressive improvement of performance in terms of rouge l see table between a not pretrained model pointer generator and a pretrained model m bert

how abstractive are the models we report the novelty i
e
the percentage of novel words in the summary in figure
as previous works reported see et al
pointer generator networks are poorly abstractive relying too much on their copy mechanism
it is particularly true for russian the lack of data probably makes it easier to learn to copy than to cope with natural language generation
as expected pretrained guage models such as m bert are consistently more abstractive and by a large margin since they are exposed to other texts during pretraining
figure percentage of novel n grams for different abstractive models neural and human for the datasets

model biases toward languages consistency among rouge scores the dom model obtains comparable rouge l scores across all the languages except for russian
this can be explained by the aforementioned russian corpus characteristics highest novelty shortest summaries and longest input documents see ble
thus in the following for pair wise based comparisons we focus only on scores tained by the different models on french german spanish and turkish since we can not draw ingful interpretations over russian as compared to other languages
abstractiveness of the datasets the oracle formance can be considered as the upper limit for an extractive model since it extracts the sentences that provide the best rouge l
we can observe that while being similar for english and german and to some extent turkish the oracle performance is lower for french or spanish
however as described in gure the percentage of novel words are similar for german
french
and spanish

this may indicate that the relevant information to extract from the article is more spread among sentences for spanish and french than for german
this is conrmed with the results of german and english have a much higher rouge l
and
than french or spanish
and

the case of textrank the textrank mance varies widely across the different languages fr de es ru tr cnn dm en cnn dm en full preprocessing duc en newsroom en t p








b p





table ratios of rouge l t p is the ratio of trank to pointer generator and b p is the ratio of bert to pointer generator
the results for cnn full preprocessing duc and newsroom datasets are those reported in table of grusky et al
pointer c in their paper is our pointer generator
regardless oracle
it is particularly surprising to see the low performance on german whereas for this language has a comparatively higher performance
on the other hand the performance on english is remarkably high the rouge l is higher than for turkish higher than for french and higher than for spanish
we pect that the textrank parameters might actually overt english
in table we report the performance ratio tween textrank and pointer generator on our pus as well as on cnn dm and two other english corpora duc and newsroom
textrank has a performance close to the pointer generator on glish corpora ratio between
to
but not in other languages ratio between
to

that are novelfrpointer generatorbert that are noveldepointer generatorbert that are novelespointer generatorbert that are novelrupointer generatorbert that are noveltrpointer generatorbert that are novelenpointer generatorbert mhuman we thus hypothesise that self attention plays an portant role for german but has a limited impact for french
this could nd an explanation in the morphology of the two languages in statistical parsing tsarfaty et al
considered german to be very sensitive to word order due to its rich morphology as opposed to french
among other reasons the exibility of its syntactic ordering is mentioned
this corroborates the hypothesis that self attention might help preserving information for languages with higher degrees of word order freedom

possible derivative usages of mlsum multilingual question answering originally cnn dm was a question answering dataset mann et al

the hypothesis is that the information in the summary is also contained in the pair article
hence questions can be ated from the summary sentences by masking the named entities contained therein
the masked entities represent the answers and thus a masked question should be answerable given the source article
so far no multilingual training dataset has been proposed for question answering
this methodology could be thus applied on sum as a rst step toward a large scale gual question answering corpus
incidentally this would also allow progressing towards multilingual question generation a crucial component to ploy the neural summarization metrics mentioned in section
news title generation while the release of mlsum hereby described covers only summary pairs the archived news articles also clude the corresponding titles
the accompanying code for parsing the articles allows to easily trieve the titles and thus use them for news title generation
topic detection a topic category can be ciated with each article summary pair by simply parsing the corresponding url
a natural tion of this data for summarization would be for template based summarization perez beltrachini et al
using it as additional features
ever it can also be a useful multilingual resource for topic detection
figure improvement rates from textrank to oracle in abscissa against rates from pointer generator to bert in ordinate
this suggests that this model despite its generic and unsupervised nature might be highly biased towards english
the benets of pretraining we hypothesize that the closer an unsupervised model performance to its maximum limit the less improvement would come from pretraining
in figure we plot the improvement rate from textrank to oracle against that of pointer generator to m bert
looking at the correlation emerging from the plot the hypothesis appears to hold true for all guages including russian not plotted for scaling reasons y with the exception of english
this exception is probably due to the aforementioned bias of textrank towards the glish language
pointer generator and m bert finally we observe in our results that m bert always forms the pointer generator
however the ratio is not homogeneous across the different languages as reported in table
in particular the improvement for german is much more important than the one for french
interestingly this observation is in line with the results reported for machine translation the transformer vaswani et al
outperforms signicantly gehring et al
for english to german but obtains comparable results for english to french see table in vaswani et al

neither model is pretrained nor based on lstm hochreiter and schmidhuber and they both use bpe tokenization shibata et al

fore the main difference is represented by the attention mechanism introduced in the transformer while used only source to target attention
textrank pointer generator bert conclusion we presented mlsum the rst large scale lingual summarization dataset comprising over
m article summary pairs in french german russian spanish and turkish
we detailed its construction and its complementary nature to the cnn dm summarization dataset for english
we reported extensive preliminary experiments lighting biases observed in existing summarization models as well as analyzing and investigating the relative performances across languages of state the art approaches
in future work we plan to add other languages including arabic and hindi and to investigate the adaptation of neural metrics to multilingual summarization
references dzmitry bahdanau kyunghyun cho and yoshua gio

neural machine translation by jointly arxiv preprint learning to align and translate


satanjeev banerjee and alon lavie

meteor an automatic metric for mt evaluation with improved correlation with human judgments
in proceedings of the acl workshop on intrinsic and extrinsic ation measures for machine translation marization pages
federico barrios federico lopez luis argerich and rosa wachenchauzer

variations of the larity function of textrank for automated tion
arxiv preprint

federico barrios federico lopez luis argerich and rosa wachenchauzer

variations of the larity function of textrank for automated tion
corr

florian bohm yang gao christian m
meyer ori shapira ido dagan and iryna gurevych

ter rewards yield better summaries learning to in proceedings of the marise without references
conference on empirical methods in ral language processing and the international joint conference on natural language processing emnlp ijcnlp pages hong kong china
association for computational linguistics
samuel r bowman gabor angeli christopher potts and christopher d manning

a large tated corpus for learning natural language inference
arxiv preprint

zewen chi li dong furu wei wenhui wang ling mao and heyan huang

cross lingual natural language generation via pre training
arxiv preprint

eunsol choi he he mohit iyyer mark yatskar tau yih yejin choi percy liang and luke moyer

quac question answering in context
arxiv preprint

sumit chopra michael auli and alexander m rush

abstractive sentence summarization with tentive recurrent neural networks
in proceedings of the conference of the north american ter of the association for computational linguistics human language technologies pages
alexis conneau ruty rinott guillaume lample ina williams samuel r
bowman holger schwenk and veselin stoyanov

xnli evaluating lingual sentence representations
in proceedings of the conference on empirical methods in ral language processing
association for tional linguistics
hoa trang dang

duc evaluation of in question focused summarization systems
ceedings of the workshop on task focused rization and question answering pages
sociation for computational linguistics
jacob devlin ming wei chang kenton lee and kristina toutanova

bert pre training of deep bidirectional transformers for language standing
in proceedings of the conference of the north american chapter of the association for computational linguistics human language nologies volume long and short papers pages
li dong nan yang wenhui wang furu wei xiaodong liu yu wang jianfeng gao ming zhou and hsiao wuen hon

unied language model pre training for natural language arxiv preprint understanding and generation


bonnie dorr david zajic and richard schwartz

hedge trimmer a parse and trim approach to headline generation
in proceedings of the naacl on text summarization workshop volume pages
association for computational guistics
xiangyu duan mingming yin min zhang boxing chen and weihua luo

zero shot lingual abstractive sentence summarization through teaching generation and attention
in proceedings of the annual meeting of the association for putational linguistics pages
greg durrett taylor berg kirkpatrick and dan klein

learning based single document tion with compression and anaphoricity constraints
in proceedings of the annual meeting of the association for computational linguistics volume long papers pages
m el haj u kruschwitz and c fox

using chanical turk to create a corpus of arabic summaries
mahmoud el haj and rim koulali

kalimat a multipurpose arabic corpus
in second workshop on arabic corpus linguistics pages
matan eyal tal baumel and michael elhadad

question answering as an automatic evaluation in ric for news article summarization
ings of the conference of the north american chapter of the association for computational guistics human language technologies volume long and short papers pages
jonas gehring michael auli david grangier denis yarats and yann n dauphin

convolutional in proceedings sequence to sequence learning
of the international conference on machine learning volume pages
jmlr
org
george giannakopoulos jeff kubina john conroy josef steinberger benoit favre mijail kabadjov udo kruschwitz and massimo poesio

ling multilingual summarization of single and multi documents on line fora and call center versations
in proceedings of the annual ing of the special interest group on discourse and dialogue pages
max grusky mor naaman and yoav artzi

newsroom a dataset of
million summaries with diverse extractive strategies
in proceedings of the conference of the north american chapter of the association for computational linguistics man language technologies volume long pers pages
francisco guzman peng jen chen myle ott juan pino guillaume lample philipp koehn vishrav chaudhary and marcaurelio ranzato

the ores evaluation datasets for low resource machine translation nepali english and sinhala english
in proceedings of the conference on empirical methods in natural language processing and the international joint conference on natural guage processing emnlp ijcnlp pages
donna harman and paul over

the effects of man variation in duc summarization evaluation
in text summarization branches out pages
karl moritz hermann tomas kocisky edward stette lasse espeholt will kay mustafa suleyman and phil blunsom

teaching machines to read and comprehend
in advances in neural mation processing systems pages
karl moritz hermann tomas kocisky edward stette lasse espeholt will kay mustafa suleyman and phil blunsom

teaching machines to read and comprehend
in advances in neural mation processing systems nips
kai hong and ani nenkova

improving the estimation of word importance for news in proceedings of the document summarization
conference of the european chapter of the sociation for computational linguistics pages
baotian hu qingcai chen and fangze zhu

sts a large scale chinese short text summarization dataset
in proceedings of the conference on empirical methods in natural language processing pages
wojciech kryscinski bryan mccann caiming xiong and richard socher

evaluating the factual consistency of abstractive text summarization
arxiv preprint

wojciech kryscinski romain paulus caiming xiong and richard socher

improving abstraction in text summarization
in proceedings of the ference on empirical methods in natural language processing pages
guillaume lample and alexis conneau

lingual language model pretraining
arxiv preprint

patrick lewis barlas oguz ruty rinott sebastian riedel and holger schwenk

mlqa uating cross lingual extractive question answering
arxiv preprint

yaobo liang nan duan yeyun gong ning wu fei guo weizhen qi ming gong linjun shou daxin jiang guihong cao al

xglue a new benchmark dataset for cross lingual arxiv training understanding and generation
preprint

chin yew lin

rouge a package for automatic in text summarization evaluation of summaries
branches out pages
marina litvak mark last and menahem friedman

a new approach to improving multilingual in summarization using a genetic algorithm
ceedings of the annual meeting of the ation for computational linguistics pages
association for computational linguistics
yang liu and mirella lapata

text in proceedings of tion with pretrained encoders
the conference on empirical methods in ural language processing and the international joint conference on natural language processing emnlp ijcnlp pages
rada mihalcea and paul tarau

textrank ing order into text
in proceedings of the ference on empirical methods in natural language processing pages
sepp hochreiter and jurgen schmidhuber

neural computation long short term memory

ramesh nallapati bowen zhou cicero dos santos caglar gulcehre and bing xiang

tive text summarization using sequence to sequence in proceedings of the rnns and beyond
signll conference on computational natural guage learning pages
courtney napoles matthew gormley and benjamin in van durme

annotated gigaword
ceedings of the joint workshop on automatic edge base construction and web scale knowledge extraction pages
association for tional linguistics
shashi narayan shay b
cohen and mirella lapata

do nt give me the details just the summary topic aware convolutional neural networks for in proceedings of the treme summarization
conference on empirical methods in natural guage processing brussels belgium
shashi narayan shay b
cohen and mirella lapata

ranking sentences for extractive tion with reinforcement learning
in proceedings of the conference of the north american ter of the association for computational linguistics human language technologies volume long pers pages new orleans louisiana
association for computational linguistics
jessica ouyang boya song and kathleen mckeown

a robust abstractive system for cross lingual summarization
in proceedings of the ence of the north american chapter of the ation for computational linguistics human guage technologies volume long and short pers pages
romain paulus caiming xiong and richard socher

a deep reinforced model for abstractive marization
arxiv preprint

laura perez beltrachini yang liu and mirella ata

generating summaries with topic plates and structured convolutional decoders
arxiv preprint

matthew e peters mark neumann mohit iyyer matt gardner christopher clark kenton lee and luke zettlemoyer

deep contextualized word resentations
in proceedings of naacl hlt pages
telmo pires eva schlinger and dan garrette

arxiv how multilingual is multilingual bert preprint

elvys linhares pontes juan manuel torres moreno stephane huet and andrea carneiro linhares

a new annotated portuguese spanish corpus for the multi sentence compression task
in proceedings of the eleventh international conference on language resources and evaluation lrec
dragomir radev simone teufel horacio saggion wai lam john blitzer arda celebi hong qi liott drabek and danyu liu

evaluation of text summarization in a cross lingual information trieval framework
center for language and speech processing johns hopkins university baltimore md tech
rep
alec radford karthik narasimhan tim salimans and ilya sutskever

improving language understanding by generative pre training
url com us
assets researchcovers languageunsupervised language understanding paper
pdf
amazonaws
pranav rajpurkar jian zhang konstantin lopyrev and squad questions percy liang

for machine comprehension of text
arxiv preprint

peter a
rankel john m
conroy hoa trang dang and ani nenkova

a decade of automatic tent evaluation of news summaries reassessing the state of the art
in proceedings of the annual meeting of the association for computational guistics volume short papers pages soa bulgaria
association for computational guistics
alexander m rush sumit chopra and jason weston

a neural attention model for abstractive in proceedings of the tence summarization
conference on empirical methods in natural guage processing pages
evan sandhaus

the new york times annotated corpus
linguistic data consortium philadelphia
thomas scialom sylvain lamprier benjamin wowarski and jacopo staiano

answers unite unsupervised metrics for reinforced in proceedings of the rization models
ference on empirical methods in natural language processing and the international joint ence on natural language processing ijcnlp pages hong kong china
sociation for computational linguistics
abigail see peter j liu and christopher d manning

get to the point summarization with generator networks
in proceedings of the nual meeting of the association for computational linguistics volume long papers pages
eva sharma chen li and lu wang

bigpatent a large scale dataset for abstractive and coherent summarization
arxiv preprint

yusuxke shibata takuya kida shuichi fukamachi masayuki takeda ayumi shinohara takeshi hara and setsuo arikawa

byte pair encoding a text compression scheme that accelerates pattern matching
technical report technical report department of informatics kyushu sity
i sutskever o vinyals and qv le

sequence to sequence learning with neural networks
advances in nips
adam trischler tong wang xingdi yuan justin ris alessandro sordoni philip bachman and heer suleman

newsqa a machine hension dataset
arxiv preprint

reut tsarfaty seddah yoav goldberg sandra kubler marie candito jennifer foster yannick sley ines rehbein and lamia tounsi

tistical parsing of morphologically rich languages spmrl what how and whither
in proceedings of the naacl hlt first workshop on statistical parsing of morphologically rich languages pages
association for computational linguistics
ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan n gomez ukasz kaiser and illia polosukhin

attention is all in advances in neural information you need
cessing systems pages
oriol vinyals meire fortunato and navdeep jaitly

pointer networks
in advances in neural formation processing systems pages
a samples french summary terre dorigine du clan karza ville mridionale de kandahar un bastion historique talibans mollah omar a vcu et conserv profondes racines
cest sur cette terre pachtoune plus qu kaboul que lavenir long terme du pays pourrait dcider
body lorsque lon parle lafghanistan yeux monde sont rivs sur capitale kaboul
cest l que concentrent pouvoir et dtermine principe son avenir
cest aussi que sont runis les commandements forces civiles militaires internationales envoyes sur sol afghan pour lutter contre linsurrection et aider pays reconstruire
mais y regarder plus prs kaboul nest quune faade
face un etat inexistant une structure pouvoir afghan encore clanique tribus restes puissantes face une dmocratie articielle importe vraie lgitimit ne vient pas kaboul
la gographie pouvoir afghan aujourdhui oblige dire quune bonne partie cls la population afghane trouve au sud pachtoune dans une cit hostile aux trangers foyer talibans kandahar
kandahar terre dorigine du clan karza tribu popalza
hamid karza prsident afghan tient son pouvoir poids de son clan dans la rgion
mi novembre dans la grande son frre wali kandahar pressaient chefs de venus de tout lafghanistan piliers de son rseau
lobjet rencontre bilan post lectoral aprs la rlection conteste son frre la pays
parfois dcri pour ses liens supposs avec cia traquants wali karza joue un rle politique mconnu
il a organis campagne son frre jour l kandahar jouait sous sa houlette qui avaient soutenu ou au contraire refus leur soutien hamid
chef dorchestre charg du clan prsident wali personnalit forte du pays
les karza adossent leur inuence celle de kandahar dans lhistoire
lorsque ahmad shah fondateur du pays en conquit en capitale
jusquen lors linvasion sovitique kandahar a incarn afghan les kandaharis considrent quils ont un droit divin diriger pays rsume mariam abou zahab experte du monde pachtoune
kandahar cest lafghanistan explique ceux qui linterrogent tooryala wesa la province
la politique sy fait et encore aujourdhui la politique sera dicte par les vnements qui sy drouleront
cette emprise de kandahar svalue aux places prises au sein du gouvernement par ceux sud
la composition du nouveau gouvernement dcembre na pas chang la donne
dautant moins que karza dans le sud ou ailleurs nont pas russi renforcer au cours du dernier mandat du prsident
lautre terre pachtoune grand paktia dans est pays frontire avec pakistan qui a fourni tant rois ne dispose plus de ses relais dans la capitale
kandahar pse aussi pays car sy trouve linsurrection qui menace place
lotan de depuis huit ans na cess du terrain dans le sud insurgs contrlent des zones entires
les provinces du helmand et de kandahar sont les zones les plus meurtrires pour la coalition et lotan semble dpourvue cohrente
kandahar natale talibans
ils sont ns dans campagnes du helmand et de kandahar mouvement taliban sest constitu dans la ville kandahar o vivait leur chef spirituel mollah omar et o il a conserv profondes racines
la pression sur vie quotidienne afghans est croissante
les talibans supplent mme gouvernement dans domaines tels que la justice quotidienne
ceux qui collaborent avec trangers sont stigmatiss menacs voire tus
en guise de premier avertissement les talibans collent nuit lettres sur portes collabos
la progression talibane fait dans le sud relate alex strick van linschoten unique spcialiste occidental la rgion mouvement taliban vivre kandahar sans protection
linscurit labsence travail poussent vers kaboul ceux qui ont peu dducation comptence seuls restent pauvres et ceux qui veulent largent
en raction cette dtrioration les amricains ont dcid sans lassumer ouvertement reprendre contrle situations cones ofciellement par lotan aux britanniques dans le helmand et aux canadiens dans la province de kandahar
le mouvement a t progressif mais depuis un an etats unis nont cess denvoyer renforts amricains au point dexercer aujourdhui direction oprations dans cette rgion
une tendance qui se renforcera encore avec troupes supplmentaires promises par barack obama
lhistoire a montr que pour gagner en afghanistan il fallait tenir campagnes de kandahar
les britanniques lont expriment faon cuisante lors la seconde guerre anglo afghane la xixe sicle sovitiques nen sont jamais venus bout
on sait comment cela sest termin pour eux on essayer dviter les mmes erreurs observait mi novembre optimiste un ofcier suprieur amricain
german summary die wurzeln des elends liegen in der vergangenheit
haiti bezahlt immer noch fr seine befreiung vor jahren
auch damals nahmen die wichtigen der welt den insel staat nicht ernst
body das portrait von zeigt haitis nationalhelden franois dominique toussaint louverture
er war einer der anfhrer der revolution in haiti und autor der ersten verfassung
die wurzeln des elends liegen in der vergangenheit
haiti bezahlt immer noch fr seine befreiung vor jahren
auch damals nahmen die wichtigen der welt den insel staat nicht ernst
am vergangenen wochenende schickte der britische architekt und grnder der organisation architecture for humanity eine atemlose verzweifelte e mail an seine freunde und untersttzer
nicht erdbeben sondern gebude tten menschen schrieb er in die betreffzeile
damit brachte er auf den punkt was auch der geologe und autor simon winchester oder der urbanist mike davis immer wieder geschrieben haben es gibt keine naturkatastrophen
es gibt nur gewaltige naturereignisse die folgen haben
die konsequenz aus dieser schlussfolgerung ist die schuldfrage
einfach lsst sie sich beantworten gier und korruption sind fast immer die auslser einer katastrophe
in haiti aber liegen die wurzeln der tragdie tief in der geschichte des landes
diese begann nach europischer rechnung i m jahre als christopher kolumbus auf der insel landete die ihre ureinwohner ayt nannten
kolumbus benannte die insel in hispaniola um und grndete mit den trmmern der gestrandeten santa maria die erste spanische kolonie in der neuen welt
ende
jahrhunderts besetzten franzsische siedler den westen der insel den frankreich zur franzsischen kolonie sainte domingue erklrte
ideale der franzsischen revolution gut hundert jahre whrte die herrschaft der beiden kolonialherren ber die geteilte insel
saint domingue war die reichste europische kolonie in den amerikas schrieb der historiker hans schmidt
kam fast die hlfte weltweit produzierten zuckers aus der franzsischen kolonie die auch in der produktion von kaffee baumwolle und indigo weltmarktfhrer war
sklaven arbeiteten auf den plantagen und sie erfuhren bald vom neuen geist ihrer herren
die franzsische revolution brachte die ideale von freiheit gleichheit und brderlichkeit in die karibik
i m august war so weit
der voodoo priester dutty boukman rief whrend einer messe zum aufstand
einer der erfolgreichsten kommandeure der rebellion war der ehemalige sklave franois dominique toussaint louverture nach dem heute der flughafen von port au prince benannt ist
gab toussaint dem land seine erste verfassung die gleichzeitig eine unabhngigkeitserklrung war
fr napoleon sollte haiti eine schmach bleiben
daraufhin sandte napoleon bonaparte kriegsschiffe und soldaten
toussaint wurde verhaftet und nach frankreich gebracht wo er i m kerker starb
doch als napoleon i m jahr darauf die sklaverei wieder einfhren wollte kam erneut zum aufstand
verzweifelt baten die franzsischen truppen i m sommer um verstrkung
da aber hatte napoleon schon das interesse an der neuen welt verloren
i m april hatte er seine kolonie louisiana an die nordamerikaner verkauft ein gebiet das rund ein viertel des staatsgebietes der heutigen usa umfasste
fr napoleon sollte haiti eine schmach bleiben
am
januar erklrte der rebellenfhrer jean jacques dessalines die ehemalige kolonie heie nun haiti und sei eine freie republik
der erste und bis zur abschaffung der sklaverei einzige erfolgreiche sklavenaufstand der neuen welt war ein schock fr die gromchte der kolonialra die ihren reichtum auf der sklaverei gegrndet hatten
ein handel der die geschichte haitis bis heute bestimmt die freiheit hatte ihren preis
ein groteil der plantagen war zerstrt ein drittel der bevlkerung haitis den kmpfen zum opfer gefallen
vor allem aber wollte keine kolonialmacht die junge republik anerkennen
i m gegenteil meisten lnder untersttzten das embargo der insel und die forderungen franzsischer sklavenherren nach reparationszahlungen
in der hoffnung als freie nation zugang zu den weltmrkten zu erhalten lie sich die neue machtelite haitis auf einen handel ein der die geschichte der insel bis heute bestimmt
mehr als zwei jahrzehnte nach dem sieg der rebellen entsandte knig karl x
seine kriegsschiffe nach haiti
ein emissr stellte die regierung vor die wahl haiti sollte fr die anerkennung als staat millionen francs bezahlen
sonst wrde man einmarschieren und die bevlkerung erneut versklaven
haiti nahm schulden auf und bezahlte
bis zum jahre lhmte die schuldenlast die haitianische wirtschaft und legte den grundstein fr armut und korruption
lie der damalige haitianische prsident jean bertrand aristide errechnen was diese reparationszahlungen fr haiti bedeuteten
rund milliarden amerikanische dollar rckzahlung forderten seine anwlte damals von der franzsischen regierung
vergebens
lesen sie auf der nchsten seite wie haiti von den akteuren der weltbhne geschnitten wurde
spanish summary el aeropuerto ha estado hasta las
con slo dos pistas por ausencia controladores areos
varias aerolneas han denunciado hasta minutos los pasajeros embarcados body el espacio har un repaso cronolgico vida esteban desde momento en el que una completa desconocida comenz a aparecer medios en como jesuln de ubrique hasta llegar a hoy en convertida la princesa del pueblo en concreto del popular madrileo distrito de san blas donde vive tal y como algunos han calicado y protagonista revistas diarios y portales web aparecer incluso entre los personajes ms populares google
junto a mara teresa campos estarn en el plat patricia prez presentadora del programa matinal sbados en telecinco vulveme loca quien ha conducido las campanadas en cuatro ocasiones y los comentaristas maribel escalona emilio pineda y jos manuel parada
los vuelos han venido registrando este viernes importantes retrasos en barajas a de que desde las
el aeropuerto opera con las cuatro pistas segn han informado fuentes de aena mientras las compaas han denunciado por parte controladores hasta minutos los pasajeros embarcados
segn los datos facilitados por aena ausencia de controladores que estaban programados en el turno torre control de barajas oblig a cerrar dos pistas del aeropuerto lo que gener retrasos medios de minutos
turkish summary atamas yaplmayan retmenler miting yapt
retmen adaylarna muharrem nce ve tekel iileri destek verdi
body yetersiz alan kadrolar nedeniyle atamas yaplamayan retmen adaylar ankarada miting yapt
tekel iilerinin de destek verdii retmen adaylarnn mitinginde retmen kkenli chp milletvekili muharrem nce de hazr bulundu
trkiyenin eitli illerinden gelen atamas yaplmayan retmenler platformu yesi szlemeli retmenler saatlerinde abdi peki parknda topland
milletvekillii iin kpss getirilsin kadrolu retmen cretli retmen ve cretli kle olmayacaz yazl dvizler tayan ve ayn ierikli sloganlar atan retmenlerin dzenledii mitinge baz siyasi parti sivil toplum kuruluu temsilcileri ve tekel destek verdi
chp yalova milletvekili muharrem nce okullarda derslerin bo getiini ne srerek okullar retmensiz retmenler ise isiz dedi
hkmetin bu genlerin sesini gerektiini belirten nce bu lkenin bin eitim fakltesi mezunu genci i bekliyorsa bu hkmetin ve lkenin aybdr
eitim sorununu zememi bir hkmet bu lkenin hibir sorununu zememi demektir
bu kadar nemli bir soruna kulaklarn tkayamaz diye konutu
ankarann gbeinde derslerin bo getiini ileri sren nce bu lkede zik ve matematik retmeni atanmyor ama bunlarn kat din dersi retmeni atanyor dedi
platform adna yaplan aklamada trkiyede her yl niversite bitirerek diplomasn alan retmenlerin eitim alanndaki yetersizlik dolaysyla isizler kervanna katld ifade edildi
talep edilen haklarn insancl ve makul olduu belirtilen aklamada retmenlerin haklarn vermeyenlerin niyetli olduu ne srld
aklamada hkmetin eitim politikas eletirilerek szlemeli retmenlerin kadrolu atamalarnn yaplmas retmen yetitiren fakltelere retmen ihtiyac kadar retmen aday alnmas ve kpss yerine daha effaf bir atama sistemi getirilmesi istendi
lm orucu balatacaklar eitli sivil toplum kuruluu mitingde kadrolu atamalar yaplmad takdirde i brakma eylemi ve lm orucu yaplaca duyuruldu

