noname manuscript no
will be inserted by the editor machine reading comprehension a literature review xin zhang an yang sujian li yizhong wang n u j l c
s c v
v i x r a received date accepted date abstract machine reading comprehension aims to teach machines to understand a text like a human and is a new challenging direction in articial intelligence
this article summarizes recent advances in mrc mainly focusing on two aspects i
e
corpus and techniques
the specic characteristics of various mrc corpus are listed and compared
the main idea of some typical mrc techniques are also described
keywords machine reading comprehension natural language processing more introduction over past decades there has been a growing interest in making the machine understand human languages
and recently great progress has been made in machine reading comprehension mrc
in one view the recent tasks titled mrc can also be seen as the extended tasks of question answering qa
as early as simmons had summarized a dozen of qa systems proposed over the preceding years in his review
the survey by hirschman and gaizauskas classies those qa model into three categories namely the natural language front ends to the database the dialogue interactive advisory systems and the question answering and story comprehension
for qa systems in the rst category like the baseball and the lunar system they usually transform the natural language questions into a query against a structured database based on linguistic xin zhang peking university tel
e mail
edu
an yang peking university sujian li peking university yizhong wang peking university xin zhang et al
knowledge
although performing fairly well on certain tasks they suered from the constraints of the narrow domain of the database
as about the dialogue interactive advisory systems including the shrdlu and the gus early models also used the database as their knowledge source
problems like ellipsis and anaphora in the conservation which those systems struggled in dealing with still remain as a challenge even for nowadays models
the last category can be seen as the origin of modern mrc tasks
wendy lehnert rst proposed that the qa systems should consider both the story and the question and answer the question after necessary interpretation and inference
lehnert also designed a system called qualm according to her theory
the past decade has witnessed a huge development in the mrc eld including the soar of numbers of corpus and great progress in techniques
as about mrc corpus plenty of datasets in dierent domains and styles have been released in recent years
in mctest was released as a choice reading comprehension dataset which was of high quality whereas too small to train neural models
in cnn daily mail and cbt were released
these two datasets were generated automatically from dierentdomains and much larger than previous datasets
in squad was shown up as the rst scale dataset with questions and answers written by the human
many techniques have been proposed along with the competition on this dataset
in the same year the ms marco was released with the emphasis on narrative answers
quently newsqa and narrativeqa were constructed in similar paradigm with squad and ms marco respectively
and both datasets were crowdsourced with the expectation for high quality
next various datasets sourced from dierent domains sprung up in the following two years including race cloth and arc that were collected from exams that were based on trivias and mcscript primarily focused on scripts
released in wikihop aimed at examing systems ability of multi hop reasoning and coqa were proposed to test conversation ability of models
the appearance of large scale datasets above makes training an end to end neural mrc model possible
when competing on the leaderboard many models and niques were developed in an attempt to conquer a certain dataset
from word resentations attention mechanisms to high level architectures neural models evolve rapidly and even surpass human performance in some tasks
in this article we aim to make an extensive review on recent datasets and niques for mrc
in section we categorize the mrc datasets into three types and describe them briey
in section we introduce the traditional non neural methods neural network based models and attention mechanism which have been used in the mrc tasks
finally section concludes our review
mrc corpus the fast development of the mrc eld is driven by various large and realistic datasets released in recent years
each dataset is usually composed of documents and questions for testing the document understanding ability
the answers for the raised questions can be obtained through seeking from the documents or selecting the preseted options
here according to the formats of answers we classify the datasets into three types namely datasets with extractive answers with descriptive answers machine reading comprehension a literature review and with multiple choice answers and introduce them respectively in the following subsections
in parallel to this survey there are also new datasets steadily coming out with more diverse task formulations and testing more complicated derstanding and reasoning abilities

datasets with extractive answers to test a system s ability of reading comprehension this kind of datasets which originates from cloze style questions rstly provide the system with a large amount of documents or passages and then feed it with questions whose answers are segments of corresponding passages
a good system should select a correct text span from a given context
such comprehension tests are appealing because they are objectively gradable and may measure a range of important abilities from basic understanding to complex inference
either sourced by crowdworkers or generated automatically from dierent corpus these datasets all use a text span in the document as the answer to the proposed question
many of them released in recent years are large enough for training strong neural models
these datasets include squad cnn daily mail cbt newsqa triviaqa wikihop which are described briey below
squad one of the most famous dataset of this kind is stanford question ing dataset squad
the stanford question answering dataset
squad
consists of questions posed by crowdworkers on a set of wikipedia articles where the answer to each question is a segment of text or span from the sponding reading passage
squad
contains question answer pairs from articles which is much larger than previous manually labeled rc datasets
we quote some example question answer pairs as in fig
where each answer is a span of the document
in meteorology precipitation is any product of the condensation of atmospheric water vapor that falls under gravity
the main forms of precipitation include drizzle rain sleet snow graupel and hail


precipitation forms as smaller droplets coalesce via collision with other rain drops or ice crystals within a cloud
short intense periods of rain in scattered locations are called showers
what causes precipitation to fall gravity graupel within a cloud what is another main form of precipitation besides drizzle rain snow sleet and hail where do water droplets collide with ice crystals to form precipitation fig
question answer pairs for a sample passage in the squad
qa
com xin zhang et al
answer type percentage example date other numeric person location other entity common noun phrase adjective phrase verb phrase clause other









october thomas coke germany abc sports property damage second largest returned to earth to avoid trivialization quietly table answer type distribution in squad article endangered species act paragraph


other legislation followed including the migratory bird conservation act of a treaty prohibiting the hunting of right and gray whales and the bald eagle protection act of
these later laws had a low cost to society the species were relatively rare and little opposition was raised
question which laws faced signicant opposition plausible answer later laws question what was the name of the treaty plausible answer bald eagle protection act fig
unanswerable question examples with plausible but incorrect answers in squad
the answers belong to dierent categories as shown in table
as we can see common noun phrases make up
of the whole data proper noun phrases make up
of the data and the rest one third consists of date numbers adjective phrase verb phrase clauses and so on
this indicates that the answers of squad
displays reasonable diversity
as about the reasoning skills of squad
to answer the questions the authors show that all examples at least have some lexical or syntactic divergence between the question and the answer in the passage through manually annotating some examples
later squad
was released with emphasis on unanswerable questions
this new version of squad adds over unanswerable questions which were created adversarially by crowdworkers according to the original ones
in order to challenge the existing models which tend to make unreliable guesses on questions whose answers are not stated in context newly added questions are highly similar to corresponding context and have plausible but incorrect answers in context
we also quote some examples as shown in fig

the unanswerable questions in squad
are posed by humans and exhibit much more diversity and delity than those in other automatic constructed datasets
in such cases simple heuristics which are based on overlapping or entity type recognition are not able to distinguish answerable from unanswerable questions
cnn daily mail cnn and daily mail dataset which was released by google deepmind and university of oxford in is the rst large scale reading consisting of person location and other entities machine reading comprehension a literature review original version context the bbc producer allegedly struck by jeremy clarkson will not press charges against the top gear host his lawyer said friday
clarkson who hosted one of the most watched television shows in the world was dropped by the bbc day after an internal investigation by the british broadcaster found he had subjected producer oisin tymon to an unprovoked physical and verbal attack



query producer x will not press charges against jeremy clarkson his lawyer says
anonymised version the producer allegedly struck by will not press charges against the host his lawyer said friday
who hosted one of the most watched television shows in the world was dropped by the wednesday ter an internal investigation by the broadcaster found he had subjected ducer to an unprovoked physical and verbal attack



producer x will not press charges against his lawyer says
answer oisin tymon table an example data point quoted from hension dataset constructed from natural language materials
unlike most relevant work which uses templates or syntactic semantic rules to extract document answer triples this work collects articles from the and articles from the daily as the source text
since each article comes along with a number of bullet points to summarize the article this work converts these bullet points into document query answer triples with the cloze style questions
to exclusively examine a system s ability of reading comprehension rather than using world knowledge or co occurrence further modications are implemented on those triples to construct an anonymized version
that is each entity is anonymized by using an abstract entity marker which is not easily predicted by using world knowledge or n gram language model
an example data point and its anonymized version is shown in table
some basic statistics of cnn and daily mail are shown in table
we also quote the percentages of the right answers appearing in the top n most frequent entities in an given document as in table illustrating the diculty degree of the questions to some extent
cbt the children s book test is a part of babi project of facebook ai which aims at researching automatic text understanding and reasoning
dren books are chosen because they ensure a clear narrative structure which aids this task
the children stories used in cbt come from books freely available from project
questions are formed by enumerating consecutive sentences from chapters in books of which the rst sentences serve as context and the last one as query after removing one word
candidates are selected from words appearing in either context or query
an example question is given in fig
and the dataset size is shown in table
www
cnn
com www
dailymail
co

fb
com downloads
gutenberg
org xin zhang et al
cnn daily mail top n cumulative train valid test train valid test cnn daily mail months documents queries max entities avg entities avg tokens vocab size





table corpus statistics of cnn and daily mail









table percentage of rect answers contained in the top n most frequent entities in a given document quoted from
fig
an cbt example quoted from in cbt four distinct types of word named entities common nouns verbs and are removed respectively to form classes of questions
for each class of questions the nine wrong candidates are selected randomly from words which have the same type as the answer options in the corresponding context and query
compared to human performance on this dataset the state of art models like recurrent neural networks rnns with long short term memory lstm performed much worse when predicting nouns or named entities whereas they did great job in predicting prepostions and verbs
this may probably be explained by the fact that these models are almost based exclusively on local contexts
in contrast memory networks can exploit a wider context and outperform the conventional models when predicting nouns or named entities
thus this corpus encourages the use of world knowledge in comparison with cnn daily mail and therefore focuses less on paraphrasing parts of a context
newsqa based on news articles from news the newsqa dataset contains question answer pairs generated by crowdworkers
similar to squad based on output from the pos tagger and named entity recognizer in the stanford core nlp toolkit
www
cnn
com machine reading comprehension a literature review training validation test number of books number of questions average words in contexts average words in queries distinct candidates vocabulary size table corpus statistics of cbt answer type example proportion date time numeric person location other entity common noun phr
adjective phr
verb phr
clause phr
prepositional phr
other march
million ludwig van beethoven torrance california pew hispanic center federal prosecutors hour suered minor damage trampling on human rights in the attack nearly half table answer types distribution of newsqa










the answer to each question is a text span of arbitrary length in the ing article a null span is also included
cnn articles are chosen as source materials because in the authors view machine comprehension systems are particularly suited to high volume rapidly changing information sources like news
the major ferences between cnn daily mail and newsqa are that the answers of newsqa are not necessarily entities and therefore no anonymization procedure is considered in the generation of newsqa
the statistics of answer types in newsqa is shown in table

as can be seen in the table the variety of answer types is ensured
furthermore the authors sampled examples from newsqa and squad respectively and analyzed the possible reasoning skills to answer the questions
the results indicate that compared to squad a larger proportion of questions in newsqa require high level reasoning skills including inference and synthesis
besides while simple skills like word ing and paraphrasing can solve most questions in both datasets newsqa tends to require more complex reasoning skills than squad
the detailed comparison result is given in table
triviaqa instead of relying on crowdworkers to create question answer pairs from selected passages like newsqa and squad over k triviaqa answer evidence triples are generated through automatic procedures
firstly a huge amount of question answer pairs from trivia and quiz league websites are ered and ltered
then the evidence documents for each question answer pair are collected from either web search results or wikipedia articles
finally a clean free and human annotated subset of triples from triviaqa is given and an triple example is shown in fig

the basic statistics of triviaqa is given in table
by sampling examples from the dataset and annotating them manually it turns out that the wikipedia titles including person organization location and miscellaneous consists of over of reasoning example word matching paraphrasing inference synthesis q when were the ndings published s both sets of research ndings were lished thursday


the struggle between in q who is rwanda s the struggle pits ethnic tutsis ported by rwanda against ethnic hutu backed by congo
q who drew inspiration from presidents s rudy ruiz says the lives of us presidents can make them positive role models for dents
q where is brittanee drexel from s the mother of a year old rochester new york high school student


says she did not give her daughter permission to go on the trip
brittanee marie drexel s mom says


house s


barack obama s mother in law marian robinson will join the obamas at the family s private quarters at vania avenue
michelle is never mentioned xin zhang et al
proportion newsqa squad







ambiguous insucient q whose mother is moving to the white

table reasoning skills used in newsqa and squad and their corresponding proportions question the dodecanese campaign of wwii that was an attempt by the allied forces to capture islands in the aegean sea was the inspiration for which acclaimed commando lm answer the guns of navarone excerpt the dodecanese campaign of world war ii was an attempt by allied forces to capture the italian held dodecanese islands in the aegean sea following the der of italy in september and use them as bases against the german controlled balkans
the failed campaign and in particular the battle of leros inspired the novel the guns of navarone and the successful movie of the same name
question american callan pinckney s eponymously named system became a selling book video franchise in what genre answer fitness excerpt callan pinckney was an american tness professional
she achieved precedented success with her callanetics exercises
her books all became tional best sellers and the video series that followed went on to sell over million copies
pinckney s rst video release callanetics years younger in hours outsold every other tness video in the us
fig
example question answer evidence triples in triviaqa quoted from all answer and the rest small percentage of answers mainly belong to numerical and free text type
the average number of entities per question and the percentages of certain types of questions are also shown in table
wikihop wikihop was released for the purpose of evaluating a system s ability of multi hop reasoning across multiple documents in
in most existing datasets the information needed to answer a question is usually contained in only one machine reading comprehension a literature review total number of qa pairs number of unique answers number of evidence documents avg
question length word avg
document length word table corpus statistics of triviaqa
property example annotation statistics avg
entities question fine grained answer type what fragrant essential oil is obtained from damask
of questions coarse grained answer typewho won the nobel peace prize in
of questions time frame comparisons what was photographed for the rst time in october of questions what is the appropriate name of the largest type of frog which politician won the nobel peace prize in
per question of questions table properties of questions on sampled examples
the boldfaced words mean the presence of the corresponding properties
wikihop medhop train dev test total table dataset sizes of wikihop and medhop
sentence which makes current mrc models pay much attention on simple reasoning skills like locating matching or aligning information between query and support text
for example in squad the sentence which has the highest lexical similarity with the question contains the answer about of the time and a simple binary word in query indicator feature boosted the relative accuracy of a baseline model by

to move beyond this the authors dene a novel mrc task in which a model needs to combine evidences in dierent documents to answer the questions
a sample in wikihop which displays such characteristics is shown in fig

to construct wikihop the authors collect s r o triples with subject entity s relation r and object entity o from wikidata
then wikipedia articles associated with the entities are added as candidate evidence documents d
the triple becomes a query after removing answer from it that is q s r and a o
to reach the goal of multi hop reasoning bipartite graphs are constructed for the help of construction
as shown in fig
vertices on two sides respectively correspond to the entities and the documents from the knowledge base and edges denote the entities appear in the corresponding documents
for a given q a pair the answer candidates cq and support documents sq d are identied by traversing the bipartite graph using search the documents visited will become the support documents sq
another dataset medhop is constructed in the same way as wikihop with the focus on the medicine area
some basic statistics of wikihop and medhop are shown in table and table
table lists the proportions of dierent types of answer samples which indicates that to perform well on wikihop one system needs to be good at multi step reasoning
xin zhang et al
the hanging gardens in also known as pherozeshah mehta gardens are terraced gardens


they provide sunset views over the arabian sea


also known as bombay the ocial name until is the capital city of the indian state of maharashtra
it is the most populous city in india


the arabian sea is a region of the northern indian ocean bounded on the north by pakistan and iran on the west by northeastern somalia and the arabian peninsula and on the east by india


question hanging gardens of mumbai country options iran india pakistan somalia


fig
a sample of wikihop quoted from which displays the necessity of multi hop reasoning across several documents
fig
a bipartite graph given in paper connecting entities and documents mentioning them
bold edges are those traversed for the rst fact in the small kb on the right yellow highlighting indicates documents in sq and candidates in cq
check and cross indicate correct and false candidates

descriptive answer datasets instead of text spans or entities obtained from candidate documents descriptive answers are whole stand alone sentences which exhibit more uency and integrity
in addition in real world many questions may not be answered simply by a text span or an entity
what s more presenting answers with their supporting evidence and examples is preferred by human
so in light of these reasons some descriptive answer datasets are released in recent years
next we mainly introduce two of them in detail namely ms marco and narrativeqa
ms marco ms marco microsoft machine reading comprehension is a large dataset released by microsoft in
this dataset aims to address questions and documents in the real world
sourced from real anonymized queries issued through r r machine reading comprehension a literature review cand
wh docs
wh tok wh cand
mh docs
mh tok mh min max avg





median table corpus statistics of wikihop and medhop
wh wikihop mh medhop
unique multi step answer
likely multi step unique answer
multiple plausible answers
ambiguity due to hypernymy
only single document required
answer does not follow
wikidata wikipedia discrepancy
table qualitiative analysis of sampled answers of wikihop or and the corresponding searching results from bing search engine ms marco can well reproduce qa situations in real world
for each question in the dataset a crowdworker is asked to answer it in the form of a complete sentence using passages provided by bing
the unanswerable questions are also kept in the dataset for the purpose of encouraging one system to judge whether a question is answerable due to scanty or conicting materials
the rst version of ms marco released in has about questions and the latest version
released in has over questions
both are now available at
msmarco
org
the dataset compositions of ms marco are shown in table
and the tribution of dierent types of questions are shown in table

from this table we can see that not all of them contain interrogatives because the queries come from real users
we can also see that the interrogative what is contained in
of the queries and description questions account for the major question type
generally interrogative distribution in questions shows reasonable diversity
is another dataset with descriptive answers released by deepmind and university of oxford in
narrativeqa is specically designed to examine how well a system can capture the underlying narrative elements to answer those questions which can not be answered by simple pattern recognition or global salience
from an example of question answer pair shown in fig
we can see that relatively high level abstraction or reasoning is required to answer the question
the stories used in narrativeqa consist of books from project and movie scripts from relative
each story as well as its plot summary is nally provided to crowdworkers to create question answer pairs
because the www
bing
com
microsoft
com en us cortana
gutenberg
mainly from
imsdb
and also from
dailyscript
and
awesomelm

xin zhang et al
field description query a question query issued to bing
passages top passages from web documents as retrieved by bing
the passages are presented in ranked order to human editors
the passage that the editor uses to compose the answer is annotated as is selected
document urls urls of the top ranked documents for the question from bing
the passages are extracted from these documents
answers composed by human editors for the question matically extracted passages and their corresponding ments
well formed well formed answer rewritten by human editors and the inal answer
segment qa classication
e

tallest mountain in south america longs to the entity segment because the answer is an entity aconcagua
table the ms marco dataset composition
percentage of question question segment question types yesno what how where when why who which other question classication description numeric entity location person













table distribution of dierent question types in ms marco crowdworkers never see the full text it s less likely for them to create questions and answers solely based on localized context
the answers can be full sentences which exhibit more articial intelligence when asked about factual information
some basic statistics are shown in table and the distribution of dierent types of questions and answers are shown in table and table
according to the original paper less than of answers appear as text segments of the stories which decreases the possibility of answering questions with simple skills for a system as before
machine reading comprehension a literature review title ghostbusters ii question how is oscar related to dana answer her son summary snippet


peter s former girlfriend dana barrett has had a son oscar


story snippet dana setting the wheel brakes on the buggy thank you frank
i ll get the hang of this eventually
she continues digging in her purse while frank leans over the buggy and makes funny faces at the baby oscar a very cute nine month old boy
frank to the baby hiya oscar
what do you say slugger that s a good looking kid you got there ms
barrett
frank to dana fig
an example question answer pair of narrativeqa given in paper documents


books


movie scripts question answer pairs avg
tok
in summaries max tok
in summaries avg
tok
in stories max tok
in stories avg
tok
in questions avg
tok
in answers train

valid

test

table narrativeqa dataset statistics first token frequency category frequency what who why how where which how many much when in other









person description location why reason how method event entity object numeric duration relation










table frequency of rst token of the question in the training set of narrativeqa
table question categories on a sample of questions from the validation set of
xin zhang et al
a q e v i a r r a n o c r a m s m p o h k w i i a q a i v i r t a q s w e n t b c n n c l i a m y l i a d d a u q s
v d a u q s
v v e v i a r r a n s t p i r c s w o r c e n i g n e h c r a e s l a r u t a n y r e u q e c r u o s i a m o t u a r e w s n a e v i a r r a n e v i t c a r t e e v i t c a r t e e v i t c a r t e e v i t c a r t e e v i t c a r t e e v i t c a r t e e v i t c a r t e a i e p i i w a i v i r t s o o b s w e n a i e p i i w a i e p i i w i a m o t u a l a r u t a n i a m o t u a i a m o t u a w o r c w o r c e c r u o s e c r u o s s w e n w o r c e c r u o s e s a e l e r e t a e y t n i a m o d n o i t s e u q e c r u o s



a



m e
f
m e
f n a m u h e c n a m r o r e







m e
f
e n
n c
m e
f
m e
f
m e
f a t o s s t e s a t a e v i a r r a n d n a e v i t c a r t e l l a o o n i i s a b e l b a t n i a t n o c e l b a r e w s n a n u n o i t s e u q
k s a t n o i a r e n e g e g u a g n a l l a r u t a n a q n o u e l b l e g u o r
l e g u o r r o e t e m u e l b u e l b a m o i i w


m o b e w m o i i w m o b e w
f

m e
e n
n c
b v

r p machine reading comprehension a literature review a q e v i a r r a n o c r a m s m p o h k w i i a q a i v i r t a q s w e n t b c l i a m y l i a d
v
v n n c d a u q s d a u q s e h g g a a w a r t n e m u c o g g t n e m u c o d e b m u n
















e g a r e v a h t g n e l t n e m u c o d
r e u q e b m u n



g a r e v a h t g n e l r e u q c g
i
xin zhang et al

s t e s a t a e v i a r r a n d n a e v i t c a r t e l l a o n o i a m o n i s c i t s i a t s e l b a t








j



g a r e v a h t g n e l e w s n a
s r e p a p l a n i g i r o g n i n o p s e r r o c m o r e m o c s c i t s i a t s r e h t o e i c e s s s e l n u
s e v l e s r u o y b e t n u o c e r a h t i w s c i t s i a t s
e i c e s s s e l n u h t g n e l g n i a l u c l a c n e h w r e b m u n r o w e s u
r e r a m y t i n e n a s i e w s n a e h t n o i s r e v e s i m y n o n a
t s e t v e n i a r t s e l c i t r a a i e p i i w
t s e t v e n i a r t s w e n s h t n o m
e l b a l i a v a n u s i a t a g n i n o p s e r r o c
t s e t v e n i a r t r e b m u n
s e l c i t r a s o o b c s w e n
t s e t v e n i a r t s h p a r g a r a p g
t s e t v e n i a r t s e i r o t s s t n e m u c o d b e w l l u f
m o r t l u s e r
s e g a s s a p h a e i j t l u a e d e h t machine reading comprehension a literature review james the turtle was always getting in trouble
sometimes he d reach into the freezer and empty out all the food
other times he sled on the deck and get a splinter
his aunt jane tried as hard as she could to keep him out of trouble but he was sneaky and got into lots of trouble behind her back
one day james thought he would go into town and see what kind of trouble he could get into
he went to the grocery store and pulled all the pudding o the shelves and ate two jars
then he walked to the fast food restaurant and ordered bags of fries
he nt pay and instead headed home
his aunt was waiting for him in his room
she told james that she loved him but he would have to start acting like a well behaved turtle
after about a month and after getting into lots of trouble james nally made up his mind to be a better turtle
what is the name of the trouble making turtle a fries b pudding c james d jane what did james pull o of the shelves in the grocery store a pudding b fries c food d splinters where did james go after he went to the grocery store a his deck b his freezer c a fast food restaurant d his room what did james do after he ordered the fries a went to the grocery store b went home without paying c ate them d made up his mind to be a better turtle fig
a sample of mctest given in paper
multiple choice datasets with descriptive answers are relatively dicult to evaluate the system performance precisely and objectively
nevertheless multiple choice question which has long been used for testing students reading comprehension ability can be jectively gradable
generally this kind of questions can extensively examine one s reasoning skills including simple pattern recognition clausal inference and sentence reasoning of a given passage
in light of this many datasets in this format are released and listed as follows
mctest mctest a high quality dataset consisting of stories and tions about ction stories was released in by microsoft with the same format as race
targeting at year old children passages and questions used in mctest are quite easy and understandable which reduces the world knowledge requisite
for mctest many answers can only be found in the story since the stories are ctional
the main drawback of mctest is that its size is too small to train a well performed model
a sample of mctest is shown in fig

race race contains passages and questions that are collected from english exams for middle and high school chinese students
considering that those passages and questions are specically designed by english teachers and experts to evaluate reading comprehension ability of students this dataset is promising in developing and testing mrc systems
because the questions are created with high quality by human experts there are few noises in race
what s more passages in race cover a wide range of topics xin zhang et al
release date type race cloth mctest mcscript arc coqa multiple choice multiple choice domain exam exam question source natural natural multiple choice fiction stories crowd multiple choice script scenarios crowd multiple choice multiple choice science widea natural crowd human performance

b sota




c




d




contain unanswerable question test common sense specically raw document document number average length of document query number average length of query average length of answer






g





e

k

a children s stories literature mid high school exams news wikipedia science reddit b race m race h c total middle high e in domain out of domain f race m race h g scenarios h science related sentences i stories j texts k passages table basic information and statistics of all multiple choice datasets
machine reading comprehension a literature review t i wanted to plant a tree
i went to the home and garden store and picked a nice oak
afterwards i planted it in my garden
what was used to dig the hole a
a shovel b
his bare hands when did he plant the tree a
after watering it
after taking it home fig
example questions of mcscript
overcoming the topic bias problem that commonly exists in other datasets like news articles for cnn daily mail and wikipedia articles for squad
a sample of race is shown in table
the dataset rstly provides dents systems with a passage to read then presents several questions with didate answers
words in the questions and candidate answers may not appear in the passage so simple context matching techniques will not aid as much as in other datasets
analysis in the paper shows that reasoning skill is indispensable to answering most questions of race correctly
race is divided into two subsets namely race m and race h for middle school and high school respectively
some basic statistics of race is given in table and table
distributions of dierent reasoning types required to answer certain questions are illustrated in table denoting that over half of the questions in race requires reasoning skill
cloth cloth cloze test by teachers was constructed with the format of cloze questions
it is also composed of english tests for chinese middle school and high school
one example is shown in table
in cloth the missing blanks in the questions were carefully designed by teachers to test dierent aspects of language knowledge
the candidate answers usually have subtle dierences making the tions dicult to answer even for human
similar to race cloth is also divided into two parts cloth m for middle school and cloth h for high school ones
some basic statistics of this corpus are shown in table
through experiments on cloth the authors came to the conclusion that the performance gap between human and a system mainly results from the ability of using a long term context or multiple sentence reasoning
mcscript mcscript focuses on questions that need reasoning using sense knowledge
released in march this new dataset provides stories describing people s daily activities in which ambiguity and implicitness can be resolved easily by commonsense with crowdworkers to generate questions
the correct answers to the questions may not appear in the given text as is shown in the examples in fig

it consists of about
k texts and k questions
according to statistical analysis
of all the questions in mcscript require commonsense knowledge to answer
thus this dataset can literally examine systems commonsense inference ability
all questions in the dataset are answerable
the distribution of the questions types in mcscript is shown in fig

xin zhang et al
passage in a small village in england about years ago a mail coach was standing on the street
it did nt come to that village often
people had to pay a lot to get a letter
the person who sent the letter did nt have to pay the postage while the receiver had to
here s a letter for miss alice brown said the mailman
i m alice brown a girl of about said in a low voice
alice looked at the envelope for a minute and then handed it back to the mailman
i m sorry i ca nt take it i do nt have enough money to pay it she said
a gentleman standing around were very sorry for her
then he came up and paid the postage for her
when the gentleman gave the letter to her she said with a smile thank you very much this letter is from tom
i m going to marry him
he went to london to look for work
i ve waited a long time for this letter but now i do nt need it there is nothing in it
really how do you know that the gentleman said in surprise
he told me that he would put some signs on the envelope
look sir this cross in the corner means that he is well and this circle means he has found work
that s good news
the gentleman was sir rowland hill
he did nt forgot alice and her letter
the postage to be paid by the receiver has to be changed he said to himself and had a good plan
the postage has to be much lower what about a penny and the person who sends the letter pays the postage
he has to buy a stamp and put it on the envelope
he said
the government accepted his plan
then the rst stamp was put out in
it was called the penny black
it had a picture of the queen on it
questions the rst postage stamp was made
a
in england b
in america c
by alice d
in
the girl handed the letter back to the mailman because a
she did nt know whose letter it was b
she had no money to pay the postage c
she received the letter but she did nt want to open it d
she had already known what was written in the letter we can know from alice s words that
a
tom had told her what the signs meant before leaving b
alice was clever and could guess the meaning of the signs c
alice had put the signs on the lope herself d
tom had put the signs as alice had told him to table a sample of race quoted from
the idea of using stamps was thought of by
a
the government b
sir rowland hill c
alice brown d
tom
from the passage we know the high postage made a
people never send each other letters b
lovers almost lose every touch with each other c
people try their best to avoid paying it d
receivers refuse to pay the coming letters answer adabc machine reading comprehension a literature review race m dataset subset passages questions race h train dev test train dev test train dev test race all table the basic statistics of the training development and test sets of race m race h and race dataset passage len question len option len vocab size race m


race h


race


table statistics of race where len denotes length and vocab denotes vocabulary
dataset word matching paraphrasing single sentence reasoning multi sentence reasoning ambiguous insucient race m race h race cnn squad newsqa





























table distribution of reasoning type in race and other datasets
denotes quoting based on samples per dataset and quoting
fig
distribution of question types in mcscript
arc reasoning challenge makes use of standardized tests whose questions are objectively gradable and exhibit the variety in diculty which can be a grand challenge for ai
arc consists about
k questions
the authors of arc also designe two baselines namely a retrieval based gorithm and a word co occurrence algorithm
the challenge set a subset of arc how many how long when where why who what y n xin zhang et al
passage nancy had just got a job as a secretary in a company
monday was the rst day she went to work so she was very and arrived early
she the door open and found nobody there
i am the to arrive
she thought and came to her desk
she was surprised to nd a bunch of on it
they were fresh
she them and they were sweet
she looked around for a to put them in
somebody has sent me owers the very rst day she thought
but who could it be she began to
the day passed quickly and nancy did everything with interest
for the following days of the the rst thing nancy did was to change water for the followers and then set about her work
then came another monday
she came near her desk she was overjoyed to see bunch of owers there
she quickly put them in the vase the old ones
the same thing happened again the next monday
nancy began to think of ways to nd out the
on tuesday afternoon she was sent to hand in a plan to the
she waited for his directives at his secretary s
she happened to see on the desk a half opened notebook which in order to keep the secretaries in high spirits the company has decided that every monday morning a bunch of fresh owers should be put on each secretarys desk
later she was told that their general manager was a business management psychologist
questions b
pushed b
second b
grapes d
forced d
rst d
bananas d
held d
bottle
a
depressed b
encouraged c
excited d
surprised
a
turned
a
last
a
keys
a
smelled b
ate
a
vase
a
angrily
a
seek
a
low
a
month
a
unless
a
old
a
covering b
demanding c
replacing d
forbidding
a
sender b
receiver
a
assistant b
colleague c
employee d
manager
a
notebook b
desk
a
said c
knocked c
third c
owers c
took c
glass c
strangely d
happily c
work c
great c
year c
since c
blue b
room b
quietly b
wonder b
little b
period b
when b
red d
ask d
general d
week d
before d
new c
secretary d
waiter c
oce c
printed d
house d
signed b
written table a sample passage of cloth
bold faces highlight the correct answers
there is only one best answer among four candidates although several candidates may seem correct
dataset cloth m cloth h cloth train dev test train dev test train dev test passages questions vocab
size avg
sentence avg
words





table the statistics of the training development and test sets of cloth and two subsets from paper
containing about
k questions is created by gathering questions that are answered incorrectly by both of these two baselines
the easy set is composed of the ing
k questions
several state of the art models are tested on the challenge set but none of them are able to signicantly outperform a random baseline which reects the diculty of the challenge set
two example questions of the challenge set questions are as follows machine reading comprehension a literature review train dev test total challenge easy total table number of questions in arc grade challenge qns easy qns
qns table grade level distribution of arc questions
qns










property question words question sentences answer option words answer options min average max challenge



easy



table properties of the arc dataset in which property of a mineral can be determined just by looking at it a ter correct b mass c weight d hardness a student riding a bicycle observes that it moves faster on a smooth road than on a rough road
this happens because the smooth road has a less gravity b more gravity c less friction correct d more friction for example the rst question is dicult in that the ground truth luster can be determined by looking at something only appears as a stand alone sentence in the web text
however the incorrect candidate hardness has a strong correlation with mineral in the text
the arc corpus a scientic text which contains m science related sentences and mentions of the knowledge related to the challenge set questions according to a sample analysis is released along with the arc questions set
the use of the corpus is optional
some statistics of arc is shown in table table and table
coqa question answering systems is a conversational style datasets which consists of questions sourced from conversations in dierent domains
answers of questions are in free form
the motivation of coqa is that in daily life human usually get information by asking questions in conversations and so it is desirable for a machine to be capable of answering such questions
coqa xin zhang et al
jessica went to sit in her rocking chair
today was her birthday and she was turning
her granddaughter annie was coming over in the afternoon and jessica was very excited to see her
her daughter melanie and melanie s husband josh were coming as well
jessica had


who had a birthday jessica jessica went to sit in her rocking chair
today was her birthday and she was turning
how old would she be she was turning did she plan to have any visitors yes her granddaughter annie was coming over how many three her granddaughter annie was coming over in the afternoon and jessica was very excited to see her
her daughter melanie and melanie s husband josh were coming as well
who annie melanie and josh her granddaughter annie was coming over in the afternoon and jessica was very excited to see her
her daughter melanie and melanie s husband josh were coming as well
fig
a conversation example from the coqa
each turn contains a question qi an answer ai and a rationale ri that supports the answer
rstly provides models with a text passage to understand and then presents a series of questions that appear in a conversation
one example is given in fig

the key challenge of coqa is that a system must handle conversation history properly to tackle problems like resolving the coreference
among domains of the passages from which the questions are collected are used for cross domain uation and are used for in domain evaluation
the distribution of domains are shown in table
some linguistic phenomena statistics are given in table
the coreference and pragmatics are unique and challenging linguistic phenomena that do not appeare in other datasets
in this section we will introduce dierent techniques employed in mrc
mrc techniques
non neural method before the neural networks came into fashion many mrc systems were oped based on dierent non neural techniques which now mostly serve as baselines for comparison
next we will introduce the techniques including tf idf sliding window logistic regression and boosted method
machine reading comprehension a literature review domain passages passage length turns per passage q a pairs






out of domain children s sto
literature mid high sch
news wikipedia science reddit total lexical match paraphrasing pragmatics no coref
explicit coref
implicit coref
table distribution of domains in coqa in
phenomenon example percentage relationship between a question and its passage q who had to rescue her a the coast guard r outen was rescued by the coast guard q did the wild dog approach a yes r he drew cautiously closer q is joey a male or female a male r it looked like a stick man so she kept him
she named her new noodle friend joey q what is ifl q who had bashti forgotten a the puppy q what was his name q when will sirisena be sworn in a p
m local time q where relationship between a question and its conversation history table linguistic phenomena in coqa questions given by paper














tf idf the tf idf term frequency inverse document frequency technique is widely used in the information retrieval area and nds a place in the mrc tasks later
as validated before if candidate answers are presented retrieval based models can serve as a strong baseline
this kind of baseline is widely used in document datasets such as wikihop
by solely exploiting lexical correlation tween the concatenation of a candidate answer and the query and a given document this kind of algorithm can predict the candidate with the highest similarity score among all documents
because the inter document information is usually ignored by tf idf this baseline can not detect how much a question rely on cross document reasoning
xin zhang et al
sliding window the sliding window algorithm is constructed as a baseline in the dataset mctest
it predicts an answer based on simple lexical information in a sliding window
inspired by tf idf this algorithm uses inverse word count as weight of each word and maximize the bag of word similarity between the answer and the sliding window in the given passage
logistic regression this baseline method is proposed in squad
it extracts a large mount of features from the candidates including lengths bigram frequencies word frequencies span pos tags lexical features dependency tree path features
and predicts whether a text span is the nal answer based on all those information
boosting method this model is proposed as a conventional feature based baseline for cnn daily mail dataset
since the task can be seen as a ranking problem making the score of the predicted answer rank top among all the candidates the authors turn to the implementation of lambdamart in ranklib a highly successful ranking algorithm using forests of boosting decision trees
through feature engineering features are chosen to form a feature vector which represents a candidate and the weight vector will be learnt so that the correct answer will be ranked the highest

neural based method with the popularity of neural networks end to end models have produced ing results on some mrc tasks
these models do not need to design complex manually devised features that traditional approaches relied on and perform much better than them
next we will introduce several end to end models mainly in chronological order
match network as the rst end to end neural architecture posed for squad this model combines the match lstm which is used to get a query aware representation of passage and the pointer network which aims to construct an answer so that every token within it comes from the input text
an overall picture of the model architecture is given in fig

match lstm is originally designed for predicting textual entailment
in that task a premise and a hypothesis are given and the match lstm encodes the pothesis in a premise aware way
for every token in hypothesis this model uses soft attention mechanism which will be discussed later in sect

to get a weighted vector representation of premise
this weighted vector is concatenated with a vector representation of the according token and both are fed into an lstm namely the match lstm
in this paper the authors replace the premise and hypothesis with the query and passage to get a query aware representation of the given passage
two preprocessing lstms are employed respectively to encode the query and the passage
and a bidirectional match lstm is employed to obtain the query aware representation of the passage

net p lemur wiki
the details can be found in the paper machine reading comprehension a literature review fig
the overview of two models in after getting the query aware representation of the passage a pointer net is employed to generate answers by selecting tokens from the input passage
at each inference step ptr net uses soft attention mechanism to get a probability tribution of the input sequence and selects the token with the largest possibility as the output symbol
moreover two dierent strategies are proposed for constructing the answer
the sequence model assumes that every word in the answer can appear in any position in the passage and the length of the answer is not xed
in order to tell the model to stop generating tokens after getting the whole answer a special symbol is placed at the end of the passage the prediction of this symbol indicates the termination of the answer generating
the boundary model works dierently from the sequence model in that it only predicts the start indice as and the end indice ae in other word it s based on the assumption that the answer appears as a continuous segment of the passage
the test result shows an advantage of the boundary model over the other one
bi directional attention flow proposed by the bi directional attention flow has two key features at the context encoding stage
first this model takes dierent levels of granularity as input including character level word level and contextualized embeddings
second it uses bi directional attention ow namely a passage to query attention and a query to passage attention to get a query aware passage tation
the detailed description is given as follows
as is shown in fig
the bidaf model has six layers
the character ding layer and the word embedding layer map each each word into the vector space based respectively on character level cnns and the pre trained glove embedding
the concatenation of these two word embeddings is passed to a two layer highway networks the output of which is provided to a bi directional lstm in the contextual embedding layer to rene the word embedding using xin zhang et al
fig
overview of bidaf architecture given in
the context information
these rst three layers are applied to both the query and the passage
the attention flow layer is where the information from the query and the passage mixed and interacted
instead of summarizing the passage and the query into a xed vector like most attention mechanisms do this layer grants raw information including attention vectors and the embeddings from previous layers owing to the subsequent layer which reduces the information loss
the attentions are computed in two directions from passage to query and from query to passage
the detailed information of the attention flow layer will be given in sect


the modeling layer takes in the query aware representation of context words and used two bi directional lstm to capture the interactions among the passage words according to the query
the last output layer is task specic which gives the prediction of the answer
gated attention gated attention reader targets at realizing multi hop ing in answering cloze style questions over documents
a multiplicative interaction between the query and the hidden state of the document is employed in its tion mechanism
the multi hop architecture of the model imitates the multi step reasoning of human in reading comprehension
the overview of the model is given in fig

the model reads the document and the query iteratively in a row of k layers
in kth layer the model uses bidirectional gated recurrent gru to transform the x dings of document passed from the last layer to get
then a layer specic query representation is transformed by another bi gru to get
gru k d gru k q y modeling layeroutput layerattention flow layercontextual embed layerword embed softmaxdense and cnncharacter embed machine reading comprehension a literature review fig
gated attention architecture given in
then both and are fed to a gated attention module the result of which x k will be passed to the next layer
for each token in the gated attention module uses soft attention to get a token specied representation of query
finally we get the new embeddings of this token xi by applying a element wise multiplication for qi and
i softmax qi qi qi at the last stage the decoder employs a softmax layer to the inner product between outputs of last layer to get the possibility distribution of the predict answers



xq n dcn dynamic coattention introduces coattention mechanism to combine co dependent representations of query and the document and dynamic iteration to avoid been trapped in local maxima corresponding to incorrect answers like previous single pass models
the dynamic pointer decoder takes in the output of coattention encoder and generates the nal predictions
detailed procedures is given as follows
xq xq


m let in the document and question encoder the vector representations of the ment and the query are fed into lstm respectively and the hidden states at each step are combined to form the encoding matrix d


dmd and


qnq
sentinel vector d and q is appended to the encoding matrix to enable the model to map some unrelated words that exclusively appear in either the query or the document to this void vector
to allow for some variation between the document encoding space and the query encoding space a for those in document
the the details of dcn are as follows
denote the sequence of embeddings of words in query and xin zhang et al
non linear projection q tanh applied to
the nal representations of the document and the query are d and q
the coattention encoder takes in d and q and outputs coattention encoding matrix u


um which is the input to the dynamic pointing decoder
the details of coattention encoder will be discussed in sec


the overview of dynamic pointing decoder is given in fig

to enable the model to recover from local maxima the highway maxout network hmn is posed to predict the start point and the end point iteratively
hmn is composed of highway networks which is characterized by the skip connect that passes gradient eectively through deep networks and maxout networks a learnable activation function that has strong empirical performance
during the iteration the hidden state of the decoder is updated according to eq

hi lst m dec where and are the coattention representations of according start and end words predicted by iteration
given hi and the possibility of the tth word to be the start or the end point is calculated by eq

t hmn hi the word with the maximum possibility is selected as the prediction at current step
the architecture of hmn is given in fig

the mathematical description of hmn is given as follows hmn hi i w h t t max r tanh d t max ut t w t max where r is a non linear projection of the current state
fastqa fastqa achieved competitive performance with simple architecture which questions the necessity of improving complexity of qa systems
unlike many systems that employed a complex interaction layer to catch the interaction between the query and the context fastqa only makes use of computable features on word levels
the overview of fastqa architecture is given in fig

the binary word in feature indicates whether a token in passages appears in the corresponding query
wiqb j i i the weighted which is dened as below takes the term frequency and the similarity between query and context into account
simi j vwiq xj qi x wiqw j softmax vwiq rn machine reading comprehension a literature review fig
architecture of dynamic decoder from paper
blue denotes the variables and functions related to estimating the start position whereas red denotes the variables and tions related to estimating the end position
fig
architecture of highway maxout network given in
the concatenation of these two features and the original representation of each words is fed into a bi lstm to get the nal hidden state
the answer layer is composed of a simple layer feed forward network along with a beam search
r net the r net was proposed in by msra and achieved state of art results on squad and ms marco
an overview of its architecture is shown in fig

given the word level and character level embeddings r net rstly employs a bi directional gru to encode the questions and passages
then it uses a gated attention based recurrent network to fuse the information from the question and hmnargmax u xin zhang et al
fig
overview of fastqa architecture from
passage
later a self matching layer is used to ne tune and get the nal tation of the passage
the output layer is based on pointer networks similar to that in match lstm to predict the boundary of the answer
the initial hidden vectors of the pointer network are computed by an attention pooling over the nal passage representations
the gated attention based recurrent network adds another gate to normal based recurrent networks
this gate gives the weight of certain passage information according to the question
inspired by the sentence pair representations are obtained as follows t t ct t ct vp t rnn vp gt t ct j vt tanh st i exp at i ct m u uq u up j w p w q exp j i t w p v vp where gt sigmoid original representations of the passage and the question
is the added gate t ct t and om n uq t are to exploit information from the whole passage for each token a self matching attention is applied to get the nal representation of the passage hp
the details of self matching attention is given in sec


the output layer uses pointer networks to predict the start and end position of the answer
the initial hidden vector for the pointer network is an pooling over the question representation hp
the objective function is the sum of the negative log probabilities of the ground truth start and end position by the predicted distributions
reasonet unlike previous models which have xed number of turns during reading or reasoning regardless of the complexity of queries and passages the reasonet makes use of reinforcement learning to dynamically determine the reading and





x machine reading comprehension a literature review fig
overview of the r net architecture from paper reasoning depth
the intuition of this work comes from that the diculty of dierent questions can vary a lot in the same dataset and the fact that human usually revisit important part of passage and question to answer the question better
an overview of reasonet structure is given in fig

the external memory m is usually the word embeddings encoded by a bi rnn
the internal state s is updated according to rnn st xt s where xt is the attention vector fatt st m x
the termination gate determines when to stop updating states above and predict the answers according to the binary variable tt tt st tg
in this way the reasonet can mimic the inference process of human exploit the passages and answer the questions better
qanet most of the models above are primarily based on rnns with attention therefore are often slow for both training and inference due to the sequential nature of rnns
to make machine comprehension fast the qanet are proposed without rnns in its architecture
an overview of qanet structure is given in fig

the key dierence between qanet and the previous models is that qanet only use convolutional and self attention mechanism in its embedding and modeling encoders discarding the commonly used rnns
the depthwise separable tions can capture the local structure of the text and the multi head attention mechanism will model global interactions within the whole passages
a query to context attention similar to that in dcn is applied afterwards
the qanet achieved state of the art accuracy while achieving up to speedup in training and per training iteration compared to the rnn counterparts
xin zhang et al
fig
overview of reasonet structure from

attention the attention mechanisms have shown great power in selecting important mation aligning and capturing similarity between dierent part of input
next we will introduce several representative attention mechanism primarily based on time order
hard attention was proposed in image caption task in as the stochastic hard attention
let a


al ai rd denote the feature vectors captured by cnn each corresponding to a part of the image
when deciding which one of all features is to feed to the decoder lstm to generate caption a one hot variable st i is dened
the indicator st i is set to if the i th vector of a is the one used to extract visual features at current step t
if we denote the input of decoder lstm as zt the paper assigns a multinoulli distribution parametrized by t i and view zt as a random variable x zt st iai i p st i t a t i eti fatt ai ti pl truememory m machine reading comprehension a literature review fig
overview of the qanet architecture left which has several encoder blocks
all coder blocks are the same except that the number of convolutional layers for each varies
from
where fatt is a multilayer perceptron
after dening the objective function ls as below ls log a log a x s x s log and approximate its gradient by a monte carlo method the nal learning rule for the model is then ls w n n x log a w a b log p w e h w where the r and e are two hyperparameters set by crossvalidation
although hard attention is tricky and troublesome in training once trained well it can perform better than soft attention for the sharp focus on memory provided
xin zhang et al
soft attention here we will rst introduce the basic form of soft attention in neural machine translation task then we will talk about its variants in other tasks like natural language and mrc
dierent to hard attention soft attention calculates a weight distribution among all the input representations and use the weighted sum of them as the input to the decoder
for example in let htx denote the encoder s output sequence and ij denote the weight of each which indicates to what extent is hj related to the current output token ti
then the input to the decoder ci is ci ijhj txx the weights are calculated and learn through a feedforward neural network a
ij exp eij exp eik ptx eij a hj in nli task input has two components namely a premise and a hypothesis
and attention is used to exploit the interaction relation between these two parts
take the match lstm as example we denote hs k as the resulting hidden states of the encoder lstm separately for premise and hypothesis
when predicting the label of the hypothesis an attention weighted combinations of the hidden states of the premise is computed through a match lstm j and ht ak pm kjhs j kj p ekj we tanh j wtht k wmhm where ak is the attention vector stated above we ws wt wm is the parameters to be learned and hm is the hidden state of match lstm at position k
finally ak is concatenated with ht k for predicting the result
in mrc task we can regard the question as a premise and the passage as a hypothesis as it likes in the model match network
by applying the attention mechanism we can get additional query information for each token in the passage which will improve the model performance
compared to hard attention soft attention s advantage is that it is dierentiabile thus easy to train and fast in training and inference
bi directional attention was proposed in bidaf
compared to the attention scribed above it considers attention in two directions or query to attention and context to attention
take bidaf as example given h and u the concatenation of the outputs of the lstms in contextual embedding layer the similarity matrix s is computed stj h u j u w u h u machine reading comprehension a literature review where w compute the attention weights and the attended query vectors by s is trainable parameters is elementwise multiplication
then we can similarily the attention weights and attended context vectors are at softmax st u p j atju j softmax h p t t finally two attention vectors above are combined together with the original tual embeddings h through a vector fusing function the result of which serve as the base for future modeling or prediction
the bi directional attention adds more information in the attention part compared to normal attention mechanism
however as shown in the ablation study of the attention in this direction is less useful than the standard squad dev set
the reason is that the query is usually short and the added information is relatively small than that of the other one
coattention is proposed in
the architecture of the coattention encoder in dcn is shown in fig

in the coattention encoder the anity matrix l d q is calculated and normalized row wise and column wise to obtain aq the attention weights matrix across the document for each word of query and ad the attention weights matrix across the query for each word of document
then the attention contexts for question are computed c q daq and concatenated with q to obtain the nal document representation c d c ad
at the last step d c d is fed to a bidirectional lstm ut bi lstm cd t the result serves as the foundation for predicting the answer
the hidden states form coattention encoding matrix u


um
similarly to bi directional attention the coattention mechanism utilizes tion information in two directions while in a dierent way
it successively computes the attention contexts for the question and the document and fuses them to get a co dependent representation of document
self matching attention is proposed in r net introduced before
because many useful information exist in the passage context while they can not be captured by the traditional mainly exploits information in words surrounding window so the self matching attention mechanism is proposed to address this problem
it collects evidence for each token vt from the whole passage and its according question information
and the result hp is the nal passage representation hp t birnn t ct gt hp t ct t ct xin zhang et al
fig
architecture of co attention encoder from
ct here refers to an attention pooling vector of the whole passage st j vt tanh i exp st at ct n j w p v vp w p v vp t exp i ivp i j and gt is the gate dene in sec


uniquely self matching attention captures long distance information from the passage itself
this helps r net in dealing with problems like coreference

pre trained word representations how to eciently represent words as vectors which serve as the base of most of the modern mrc systems is a problem that concerns researchers very much
previously one hot representation and n gram model were popular however those simple techniques met their limits in many tasks
to address this problem many technologies have been proposed
according to the time of occurrence we introduce them as follows
moving further from feedforward neural net language and recurrent neural net language this paper proposed two novel models to learn the distributed representations of words namely the tinuous bag of words and the continuous skip gram model
the architectures of these two models are given in fig

the cbow model uses several history words and future words as input and maximizes the probability of correctly predicting the current word
by contrast the skip gram model uses current word as input and tries to predict words within a certain range before and after the current word
the result word vectors of both models achieved state of the art performance on several tests
aqaddocumentproductconcatproductbi lstmbi lstmbi lstmbi lstmbi q cqcdutu machine reading comprehension a literature review fig
architectures of cbow model and skip gram model from
probability and ratio k solid


k gas


k water


fashion


fig
from
a ratio much greater than means word k correlate well with ice and a ratio much greater than means word k correlate well with stream
glove the method belongs to local context window methods those ods can capture ne grained semantic and syntactic regularities of words eciently
however they can not exploit global statistical information like latent semantic which belongs to global matrix factorization methods
glove combines the advantages of these two family of methods
glove takes the co occurrence probabilities of words into consideration and use the ratio of probabilities to reect the relations of dierent words
for example if we denote the probability that word j appear in the context of word j as pij then the ratio pik pjk can tell the correlation between certain words
an example is given in fig

the glove model f takes the below form according to above phenomenon
f wi wj wk pik pjk where w rd are word vectors
f varies according to dierent constrains
elmo one disadvantages of word vectors generated by above methods is that they are static thus are independent of application linguistic contexts
this may lead input projection input projection cbow skip gram xin zhang et al
to poor performance when it comes to polysemy
in light of this elmo was proposed to addresses this problem
elmo s model employs a bi lstm with character convolutions on the input
then it jointly maximizes the log likelihood of the forward and backward directions and record the internal states
p


tn p


p


tn p


tn n y n y log


x lst m s n x log


tn x lst m s finally a task specic linear combination of those internal states are used to obtain the elmo representation
in this way elmo can capture context dependent aspects of word meaning as well as syntax information for each token
if ne tuned on domain specic data the model usually performs better
gpt compared to elmo gpt uses a variant of transformer instead of lstm to better capture the long term linguistic structure
the overview of this work is given in fig

given a corpus u


un a standard language model with a multi layer transformer decoder is used log p


x i u we wp hl transformer block i n p u softmax t e where k is the context window size u uk


is the context vectors of tokens n is the number of layers we is the token embedding matrix and wp is the position embedding matrix
all the parameters are trained using stochastic gradient descent
the nal transformer blocks activation is denoted as hm l
a supervised ne tuning can be applied in dierent down stream tasks
as for some tasks like text classication only a linear output layer with parameters wy is needed to predict p


xm softmax hm wy more recently its successor is released which is a scale up of gpt while with much larger volume
has
billion parameters and claimed to achieve state of the art performance on many language modeling
however its code have not been released by the time this paper is written
machine reading comprehension a literature review fig
graph comes from paper
left is transformer architecture and training tives used in this work
right is input transformations for ne tuning on dierent tasks
all structured inputs are converted into token sequences to be processed by gpt followed by a layer
fig
model architectures of bert gpt and elmo quoted from bert as shown in fig
both elmo and gpt models only use unidirectional language models to learn the representation of tokens
bert points out that this restriction has severely limited the eciency of the pre trained representation
to address this problem two new prediction tasks are proposed to pre train bert in two direction namely the masked language model and the next sentence prediction
inspired by the cloze task the masked language model is to predict the randomly masked tokens i d based on their context in the input
in other words both the left and the right context will be taken into consideration when computing representations
and to capture sentence level information and relationship a rized next sentence prediction task is to predict whether a sentence a is the next sentence of b
the wordpiece embeddings are used in the input layer along with the ment embeddings and the position embeddings
the input embeddings is the sum of above three embeddings as shown in fig

the main architecture of bert s model is a multi layer bidirectional transformer encoder almost identical to the inal one
similar to gpt when ne tuned on down steam tasks only an additional output layer with a minimal number of parameters is needed as shown in fig

bert advanced state of the art results on nlp tasks
a comparison of size of bert and gpt is given in table
bert











openai gptlstmelmolstmlstmlstmlstmlstmlstmlstmlstmlstmlstmlstm tn














en





en


tn


en


xin zhang et al
fig
bert input representation
fig
task specic models overview from paper












sep


tok ntok


tnsingle sentence





berttok tok tok n


tnsingle sentence b peroo





label











tmstart end spanclass labelberttok tok tok n


sep


tok ntok


tokmsentence


sentence machine reading comprehension a literature review model gp t bertbase bertlarge gp t parameters layers hidden size m m m m table hyperparameter comparison among similar models
layers means the number of the transformer blocks
conclusion in this paper we summarized advances in mrc eld in recent years
in we briey introduced the history of mrc tasks and some early mrc systems
in section we introduced recent datasets in three categories i
e
squad cnn daily mail cbt newsqa triviaqa and cloth in extractive format ms marco and narrative qa in narrative format and wikihop mctest race mcscript and arc in multiple choice format
the coqa a novel dataset focuses on conversational questions is also included
in section we rst go through several non neural methods including sliding window logistic regression tf idf and boosted method then more importantly the neural based models like dcn ga bidaf fastqa rnet reasonet and qanet
afterwards we discussed and compared two important positions of these models namely the pre training technology and attention anism in detail
we covered glove elmo and bert in section
and hard attention soft attention bi directional attention coattention and self attention mechanisms in section

all together we reviewed the major progress that has been made in recent years in mrc eld
however the mrc direction is developing very fast and it is dicult to include all the newly proposed mrc work in this survey
we hope this review will ease the reference to recent mrc advences and encourage more researchers to work on mrc eld
references
bahdanau d
cho k
bengio y
neural machine translation by jointly learning to align and translate
corr

bengio y
ducharme r
vincent p
jauvin c
a neural probabilistic language model
journal of machine learning research
bobrow d
g
kaplan r
m
kay m
norman d
a
thompson h
winograd t
gus a frame driven dialog system
articial intelligence
chen d
bolton j
manning c
d
a thorough examination of the cnn daily mail ing comprehension task
arxiv preprint

cho k
van merrienboer b
gulcehre c
bahdanau d
bougares f
schwenk h
bengio y
learning phrase representations using rnn encoder decoder for statistical chine translation
arxiv preprint

chollet f
xception deep learning with depthwise separable convolutions
arxiv preprint pp

clark p
cowhey i
etzioni o
khot t
sabharwal a
schoenick c
tafjord o
think you have solved question answering try arc the reasoning challenge
arxiv preprint
xin zhang et al

clark p
cowhey i
etzioni o
khot t
sabharwal a
schoenick c
tafjord o
think you have solved question answering try arc the reasoning challenge
arxiv preprint

clark p
etzioni o
my computer is an honor studentbut how intelligent is it standardized tests as a measure of ai
ai magazine
clark p
etzioni o
khot t
sabharwal a
tafjord o
turney p
d
khashabi d
combining retrieval statistics and inference to answer elementary science questions
in aaai pp

deerwester s
dumais s
t
furnas g
w
landauer t
k
harshman r
indexing by latent semantic analysis
journal of the american society for information science
devlin j
chang m
w
lee k
toutanova k
bert pre training of deep bidirectional transformers for language understanding
arxiv preprint

dhingra b
liu h
yang z
cohen w
w
salakhutdinov r
gated attention readers for text comprehension
arxiv preprint

goodfellow i
j
warde farley d
mirza m
courville a
bengio y
maxout networks
arxiv preprint

green jr b
f
wolf a
k
chomsky c
laughery k
baseball an automatic in papers presented at the may western joint ire aiee acm answerer
computer conference pp

acm
hermann k
m
kocisky t
grefenstette e
espeholt l
kay w
suleyman m
som p
teaching machines to read and comprehend
in advances in neural information processing systems pp

hill f
bordes a
chopra s
weston j
the goldilocks principle reading children s books with explicit memory representations
arxiv preprint

hirschman l
gaizauskas r
natural language question answering the view from here
natural language engineering
hochreiter s
schmidhuber j
long short term memory
neural computation
jia r
liang p
adversarial examples for evaluating reading comprehension systems
arxiv preprint

joshi m
choi e
weld d
s
zettlemoyer l
triviaqa a large scale distantly pervised challenge dataset for reading comprehension
arxiv preprint

kaiser l
gomez a
n
chollet f
depthwise separable convolutions for neural machine translation
arxiv preprint

kim y
convolutional neural networks for sentence classication
arxiv preprint

t
schwarz j
blunsom p
dyer c
hermann k
m
melis g
grefenstette e
the narrativeqa reading comprehension challenge
transactions of the association of computational linguistics
lai g
xie q
liu h
yang y
hovy e
race large scale reading comprehension dataset from examinations
arxiv preprint

lehnert w
g
a conceptual theory of question answering
in proceedings of the international joint conference on articial intelligence volume pp

morgan kaufmann publishers inc

levy o
seo m
choi e
zettlemoyer l
zero shot relation extraction via reading comprehension
arxiv preprint

liu p
j
saleh m
pot e
goodrich b
sepassi r
kaiser l
shazeer n
generating wikipedia by summarizing long sequences
arxiv preprint

manning c
surdeanu m
bauer j
finkel j
bethard s
mcclosky d
the stanford corenlp natural language processing toolkit
in proceedings of annual meeting of the association for computational linguistics system demonstrations pp

merity s
xiong c
bradbury j
socher r
pointer sentinel mixture models
arxiv preprint

mikolov t
chen k
corrado g
dean j
ecient estimation of word representations in vector space
arxiv preprint

nguyen t
rosenberg m
song x
gao j
tiwary s
majumder r
deng l
ms marco a human generated machine reading comprehension dataset
arxiv preprint
machine reading comprehension a literature review
ostermann s
modi a
roth m
thater s
pinkal m
mcscript a novel dataset for assessing machine comprehension using script knowledge
arxiv preprint

pennington j
socher r
manning c
glove global vectors for word representation
in proceedings of the conference on empirical methods in natural language processing emnlp pp

peters m
e
neumann m
iyyer m
gardner m
clark c
lee k
zettlemoyer l
deep contextualized word representations
arxiv preprint

radford a
narasimhan k
salimans t
sutskever i
improving language ing with unsupervised learning
tech
rep
technical report openai
rajpurkar p
jia r
liang p
know what you do nt know unanswerable questions for squad
arxiv preprint

rajpurkar p
zhang j
lopyrev k
liang p
squad questions for machine comprehension of text
arxiv preprint

reddy s
chen d
manning c
d
coqa a conversational question answering challenge
arxiv preprint

richardson m
burges c
j
renshaw e
mctest a challenge dataset for the domain machine comprehension of text
in proceedings of the conference on pirical methods in natural language processing pp

richardson m
burges c
j
renshaw e
mctest a challenge dataset for the domain machine comprehension of text
in proceedings of the conference on pirical methods in natural language processing pp

robbins h
monro s
a stochastic approximation method
in herbert robbins selected papers pp

springer
rocktaschel t
grefenstette e
hermann k
m
t
blunsom p
reasoning about entailment with neural attention
arxiv preprint

seo m
kembhavi a
farhadi a
hajishirzi h
bidirectional attention ow for machine comprehension
arxiv preprint

shankar s
garg s
sarawagi s
surprisingly easy hard attention for sequence to
shankar s
sarawagi s
label organized memory augmented neural network
corr quence learning
in emnlp

shen y
huang p
s
gao j
chen w
reasonet learning to stop reading in machine comprehension
in proceedings of the acm sigkdd international conference on knowledge discovery and data mining pp

acm
simmons r
f
answering english questions by computer a survey
tech
rep
system development corp santa monica calif
srivastava r
k
gre k
schmidhuber j
highway networks
arxiv preprint
bulletin
taylor w
l
a new tool for measuring readability
journalism
trischler a
wang t
yuan x
harris j
sordoni a
bachman p
suleman k
newsqa a machine comprehension dataset
arxiv preprint

vaswani a
shazeer n
parmar n
uszkoreit j
jones l
gomez a
n
kaiser l
polosukhin i
attention is all you need
in advances in neural information processing systems pp

vinyals o
fortunato m
jaitly n
pointer networks
arxiv e prints

vinyals o
fortunato m
jaitly n
pointer networks
in advances in neural tion processing systems pp

vrandecic d
wikidata a new platform for collaborative data collection
in proceedings of the international conference on world wide web pp

acm
wadhwa s
embar v
grabmair m
nyberg e
towards inference oriented reading comprehension parallelqa
arxiv preprint

wang s
jiang j
learning natural language inference with lstm
arxiv preprint
wang s
jiang j
machine comprehension using match lstm and answer pointer
arxiv
preprint

wang w
yang n
wei f
chang b
zhou m
gated self matching networks for reading comprehension and question answering
in proceedings of the annual ing of the association for computational linguistics volume long papers vol
pp
xin zhang et al

weissenborn d
wiese g
seie l
fastqa a simple and ecient neural architecture for question answering
corr

weissenborn d
wiese g
seie l
making neural qa as simple as possible but not simpler
arxiv preprint

weissenborn d
wiese g
seie l
making neural qa as simple as possible but not simpler
arxiv preprint

welbl j
stenetorp p
riedel s
constructing datasets for multi hop reading hension across documents
transactions of the association of computational linguistics
weston j
chopra s
bordes a
memory networks
corr

winograd t
understanding natural language
cognitive psychology
woods w
a
progress in natural language understanding an application to lunar geology
in proceedings of the june national computer conference and exposition pp

acm
wu q
burges c
j
svore k
m
gao j
adapting boosting for information retrieval measures
information retrieval
wu y
schuster m
chen z
le q
v
norouzi m
macherey w
krikun m
cao y
gao q
macherey k
al
google s neural machine translation system bridging the gap between human and machine translation
arxiv preprint

xie q
lai g
dai z
hovy e
large scale cloze test dataset designed by teachers

xiong c
zhong v
socher r
dynamic coattention networks for question answering
arxiv preprint
arxiv preprint

xu k
ba j
kiros r
cho k
courville a
salakhudinov r
zemel r
bengio y
show attend and tell neural image caption generation with visual attention
in international conference on machine learning pp

yih w
t
chang m
w
meek c
pastusiak a
question answering using enhanced lexical semantic models
in proceedings of the annual meeting of the association for computational linguistics volume long papers vol
pp

yu a
w
dohan d
luong m
t
zhao r
chen k
norouzi m
le q
v
qanet combining local convolution with global self attention for reading comprehension
arxiv preprint

