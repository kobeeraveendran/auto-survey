g u a l c
s c v
v i x r a abstractive summarization improved by wordnet based extractive sentences niantao sujian huiling and qibin moe key laboratory of computational linguistics peking university china institute of medical information chinese academy of medical sciences moe information security lab school of software microelectronics peking university china xieniantao
edu
cn

ac
cn
pku
edu
abstract
recently the abstractive summarization models have achieved good results on the cnn daily mail dataset
still how to prove abstractive methods with extractive methods is a good research direction since extractive methods have their potentials of exploiting various ecient features for extracting important sentences in one text
in this paper in order to improve the semantic relevance of abstractive summaries we adopt the wordnet based sentence ranking algorithm to extract the sentences which are most semantically to one text
then we design a dual attentional framework to generate summaries with consideration of the extracted information
at the same time we bine pointer generator and coverage mechanisms to solve the problems of out of vocabulary oov words and duplicate words which exist in the abstractive models
experiments on the cnn daily mail dataset show that our models achieve competitive performance with the state of art rouge scores
human evaluations also show that the summaries generated by our models have high semantic relevance to the original text
keywords abstractive summarization model dual tion extractive summarization wordnet
introduction for automatic summarization there are two main methods extractive and stractive
extractive methods use certain scoring rules or ranking methods to select a certain number of important sentences from the source texts
for ample proposed to make use of convolutional neural networks cnn to represent queries and sentences as well as adopted a greedy algorithm combined with pair wise ranking algorithm for extraction
based on recurrent neural works rnn constructed a sequence classier and obtained the highest extractive scores on the cnn daily mail corpus set
at the same time the stractive summarization models attempt to simulate the process of how human niantao xie et al
beings write summaries and need to analyze paraphrase and reorganize the source texts
it is known that there exist two main problems called oov words and duplicate words by means of abstraction
proposed an improved pointer mechanism named pointer generator to solve the oov words as well as came up with a variant of coverage vector called coverage to deal with the duplicate words
created the diverse cell structures to handle duplicate words problem based on query based summarization
for the rst time a reinforcement learning method based neural network model was raised and obtained the state of the art scores on the cnn daily mail
both extractive and abstractive methods have their merits
in this paper we employ the combination of extractive and abstractive methods at the sentence level
in the extractive process we nd that there are some ambiguous words in the source texts
the dierent meanings of each word can be acquired through the synonym dictionary called wordnet
first wordnet based lesk algorithm is utilized to analyze the word semantics
then we apply the modied sentence ranking algorithm to extract a specied number of sentences according to the sentence syntactic information
during the abstractive part based on model we add a new encoder which is derived from the extractive sentences and put the dual attention mechanism for decoding operations
as far as we know it is the rst time that joint training of sentence level extractive and abstractive models has been conducted
additionally we combine the pointer generator and coverage mechanisms to handle the oov words and duplicate words
our contributions in this paper are mainly summarized as follows considering the semantics of words and sentences we improve the sentence ranking algorithm based on the wordnet based simplied lesk algorithm to obtain important sentences from the source texts
we construct two parallel encoders from the extracted sentences and source texts separately and make use of dual attentional model for joint training
we adopt the pointer generator and coverage mechanisms to deal with oov words and duplicate words problems
our results are competitive compared with the state of the art scores
our method our method is based on the attentional model which is implemented with reference to and the attention distribution t is calculated as in
here we show the architecture of our model which is composed of eight parts as in figure
we construct two encoders based on the source texts and extracted sentences as well as take advantage of a dual attentional decoder to generate summaries
finally we combine the pointer generator and coverage mechanisms to manage oov and duplicate words problems
abstractive summarization with extractive methods fig

a dual attentional encoders decoder model with pointer generator network

dual attentional model encoders decoder model
referring to we use two single layer tional long short term memory bilstm encoders including source and tractive encoders and a single layer unidirectional lstm unilstm decoder in our model as shown in figure
for encoding time i the source texts and the i and we extracted information respectively input the word embeddings ws i into h e h s two encoders
meanwhile the corresponding hidden layer states i are i and generated
at decoding step t the decoder will receive the word embedding from the step t which is obtained according to the previous word in the reference summary during training or provided by the decoder itself when testing
next we acquire the state st and produce the vocabulary distribution p yt
here we are supposed to calculate i by the following formulas h s also h e i could be obtained as follows h s h s h e h e h s h s i lst m ws i lst m ws i h s h s i h s i h e h e i lst m we i lst m we i h e h e i h i context attention based sentence ranking niantao xie et al
dual attention mechanism
at the tth step we need not only the previous hidden state but also the context vector cs t obtained by the corresponding attention distribution to gain state st and vocabulary distribution p yt
ce cs ce firstly for source encoder we calculate the context vector cs bs are learnable parameters ws way vs ws t in the following i t vst st ws h s i bs s i t i t t cs t s i t h s t e i t ee i t ee t ce t e i t h e t secondly for extractive encoder we utilize the identical method to compute the context vector ce ve we we be are learnable parameters i t vet ee st we h e i be thirdly we get the gated context vector cg t and ce of context vectors the concatenation of cs shown as below is sigmoid function wg bg are learnable parameters by calculating the weighted sum t where the weight is the gate network obtained by t via multi layer perceptron mlp
details are and ce gt t ce bg cg t gt cs t gt ce t in the same way we can obtain the hidden state st and predicte the bout are learnable bin wout win wout ity distribution p yt at time t win parameters
st lst m win win bin p t sof st wout t bout abstractive summarization with extractive methods
wordnet based sentence ranking algorithm to extract the important sentences we adopt a wordnet based sentence ranking algorithm
is a lexical database for the english language which groups english words into sets of synonyms called synsets and provides short denitions and usage examples
used the simplied lesk approach based on wordnet to extract abstracts
we refer to its algorithm and set up our sentence ranking algorithm so as to construct the extractive encoder
for sentence


xn after ltering out the stop words and ambiguous tokens through wordnet we obtain a reserved subsequence


xim
since some words contain too many dierent senses which may result in too much calculation we set a window size nwin default value in descending order according to the number of senses of is and sort words as well as keep the rst nsav nsav nwin words left to get
next we count the common number of senses of each word as word weight
finally we get the sum weights of each sentence and acquire an average sentence weight



xsnsav taking a sentence for instance we make an assumption that has two senses ma and mb has two senses mc and md while has two senses me
currently considering as the keyword we measure the number of common words between a pair of sentences which describe the word senses of and another word
table shows all possible matches of the senses of
for the two senses of we can separately obtain the sum of co occurrence word pairs for each meaning
for ma we obtain countma countac countad countae countaf for mb we gain countmb countbc countbd countbe countbf
the signicance corresponding to the higher score countma or countmb is assigned to the the keyword
table
the number of common words between a pair of sentences
pair of sentences ma and mc ma and md mb and mc mb and md ma and me ma and mf mb and me mb and common words in sense description countac countad countbc countbd countae countaf countbe countbf
nltk
org howto wordnet
html niantao xie et al
in this way we re capable of acquiring the average weight of sentence
weightavg countxi nsav let s assume that document d


xn which contains a total of n sentences
we sort them in descending order according to the average weights of sentences and then extract the top ntop sentences default value is

pointer generator and coverage mechanisms pointer generator network
pointer generator is an eective method to solve the problem of oov words and its structure has been expanded in figure
we borrow the method improved by
pgen is dened as a switch to decide to generate a word from the vocabulary or copy a word from the source encoder attention distribution
we maintain an extended vocabulary including the cabulary and all words in the source texts
for the decoding step t and decoder input xt we dene pgen as pgen t wp st wp pvocab p t xt bp p wt pgen s i t i wi where wt is the value of xt and wp wp wp bp are learnable parameters
coverage mechanism
duplicate words are a critical problem in the model and even more serious when generating long texts like multi sentence texts
made some minor modications to the coverage model which is also displayed in figure
first we calculate the sum of attention distributions from previous decoder steps


t to get a coverage vector covt covs t s then we make use of coverage vector covt to update the attention tion i t vst finally we dene the coverage loss function covlosst for the sake of penalizing st ws i t bs i ws covs the duplicate words appearing at decoding time t and renew the total loss h s covlosst i t covs i t i losst w where w t covlosst t is the target word at tth step w is the primary loss for timestep t during training hyperparameter default value is
is the weight for covlosst ws bs are learnable parameters
ws ws abstractive summarization with extractive methods experiments
dataset cnn daily mail is widely used in the public automatic summarization evaluation which contains online news articles tokens on average paired with multi sentence summaries tokens on average
provided the data processing script and we take advantage of it to obtain the non anonymized sion of the the data including training pairs validation pairs and test pairs though used the anonymized version
during training steps we nd that of articles are empty so we utilize the remaining pairs for training
then we perform the splitting preprocessing for the data pairs with the help of stanford corenlp and convert them into binary les as well as get the vocab le for the convenience of reading data

implementation model parameters conguration
the corresponding parameters of trolled experimental models are described as follows
for all models we have set the word embeddings and rnn hidden states to be dimensional and dimensional respectively for source encoders extractive encoders and decoders
contrary to we learn the word embeddings from scratch during training because our training dataset is large enough
we apply the optimization nique adagrad with learning rate
and an initial accumulator value of
as well as employ the gradient clipping with a maximum gradient norm of
for the one encoder models we set up the vocabulary size to be for source encoder and target decoder simultaneously
we try to adjust the vocabulary size to be then discover that when the model is trained to converge the time cost is doubled but the test dataset scores have slightly dropped
in our analysis the models parameters have increased excessively when the vocabulary enlarges leading to overtting during the training process
meanwhile for the models with two encoders we adjust the vocabulary size to be
each pair of the dataset consists of an article and a multi sentence summary
we truncate the article to tokens and limit the summary to tokens for both training and testing time
during decoding mode we generate at least words with beam search algorithm
data truncation operations not only reduce memory consumption speed up training and testing but also improve the imental results
the reason is that the vital information of news texts is mainly concentrated in the rst half part
we train on a single geforce gtx gpu with a memory of mib and the batch size is set to be as well as the beam size is for beam search in decoding mode
for the dual attentional models without generator we trained them for about two days
models with pointer generator
nyu

github
io niantao xie et al
expedite the training the time cost is reduced to about one day
when we add coverage the coverage loss weight is set to
and the model needs about one hour for training
in order to gure out how each part of our models controlled experiments
contributes to the test results based on the released of tensorow we have implemented all the models and done a series of experiments
the baseline model is a general attentional model the encoder sists of a bilstm and the decoder is made up of an unilstm
the second baseline model is our encoders decoder dual attention model which contains two bilstm encoders and one unilstm decoder
this model combines the tive and generative methods to perform joint training eectively through a dual attention mechanism
for the above two basic models in order to explain how the oov and cate words are treated we lead into the pointer generator and coverage anism step by step
for the second baseline the two tricks are only related to the source encoder because we think that the source encoder already covers all the tokens in the extractive encoder
for the extractive encoder we adopt two methods for extraction
one is the leading three sentences technique which is simple but indeed a strong baseline
the other is the modied sentence ranking algorithm based on wordnet that we explain in details in section
it considers semantic relations in words and sentences from source texts

results rouge is a set of metrics with a software package used for evaluating matic summarization and machine translation results
it counts the number of overlapping basic units including n grams longest common subsequences lcs
we use a python wrapper to gain and l scores and list the scores in table
we carry out the experiments based on original dataset i
e
non anonymized version of data
for the top three models in table their rouge scores are slightly higher than those executed by except for the rouge l score of attn pgn which is
points lower than the former result
for the fourth model we did not reproduce the results of and rouge l decreased by an average of
points
for the four models in the middle we apply the dual attention mechanism to integrate extraction with abstraction for joint training and decoding
these model variants own a single pgn or pgn together with cov achieve better results than the corresponding vulgaris attentional models simultaneously
we conclude that the extractive encoders play a role among which we obtained higher and scores based on the dual attn
com tensorflow models tree master research textsum
org project

abstractive summarization with extractive methods table
rouge scores on cnn daily mail non anonymized testing dataset for all the controlled experiment models mentioned above
according to the ocial rouge usage description all our rouge scores have a condence interval of at most

pgn cov ml rl are abbreviations for pointer generator coverage objective learning and reinforcement learning
models with subscript a were trained and tested on the anonymized cnn daily mail dataset as well as with are the state of the art extractive and abstractive summarization models on the anonymized dataset by now
models attn attn attn pgn attn pgn cov dual attn pgn wordnet dual attn pgn dual attn pgn cov wordnet dual attn pgn cov summarunner a rl intra attn a ml rl intra attn rouge scores












l

























pgn cov model and achieve a better rouge l score on wordnet attn pgn cov model
let s take a look at the ve models at the bottom two of which give the of the art scores for the extractive and generative methods
our scores are already comparable to them
it is worthy to mention that based on the dual attention our models related to both and wordnet with pgn and cov have exceeded the previous best scores
when in fact previous summarunner rl related models are based on anonymized dataset these dierences may cause some deviations in the comparison of experimental results
we give some generated summaries of dierent models for one selected test article
from figure we can see that the red words represent key information about who what where and when
we can match the corresponding keywords in the remaining seven summaries to nd out whether they cover all the signicant points and check if they are expressed in a concise and coherent way
it can be discovered from figure that most of the models have lost several vital points and the model dual attn pgn has undergone fairly serious repetition
our model wordnet dual attn pgn cov holds the main key information as well as has better readability and semantic correctness reliably
niantao xie et al
fig

summaries for all the models of one test article example
related work up to now automatic summarization with extractive and abstractive methods are under fervent research
on the one hand the extractive techniques extract the topic related keywords and signicant sentences from the source texts to stitute summaries
proposed a model with a hierarchical encoder and attentional decoder to solve extractive summarization tasks at the word and tence levels
currently put forward summarunner a rnn based sequence model for extractive summarization and it achieves the previous state of art performance
on the other hand abstractive methods establish an intrinsic semantic representation and use natural language generation techniques to duce summaries which are closer to what human beings express
applied the combination of model and attention mechanism to machine translation reference six people including one civilian are killed when a car bombexplodes near a police station
six others are killed when their armored vehicle is attacked on a highway in northern sinai
ansarbeital maqdis an isisaffiliate claims responsibility
attn an attack on a police station a guard is killed in a car bomb
the attack comes hours after another operation that the group claims responsibility
the attacks come as the military announced a departure of several senior military positions
pgn six people were killed when a car bomb exploded near the police station in al arish capital of north sinai
a first lieutenant a sergeant and four conscripts were killed when their armored vehicle was attacked on the highway from al arishto sheikh zuweidin northern sinai
ansarbeital maqdishas claimed many attacks against the army and police in sinai
cov six people were killed when a car bomb exploded near the police station in al arish capital of north sinai
ansarbeital maqdis a sergeant and four conscripts were killed when their armored vehicle was attacked on the highway from al arishto sheikh zuweidin northern sinai
a third attack sundayon a checkpoint in rafahleft three security personnel injured after unknown assailants opened fire
wordnet dual pgn six people killed when a car bomb exploded near the police station in al arish capital of north sinai
a first lieutenant a sergeant and four conscripts were killed when their armored vehicle was attacked on the highway
a third attack sundayon a checkpoint in rafahleft three security personnel injured
dual pgn six people including one civilian were killed when a car bomb exploded near the police station in al arish capital of north sinai
six people including one civilian were killed when a car bomb exploded near the police station in al arish capital of north sinai authorities say
wordnet dual cov six people including one civilian were killed when a car bomb exploded near the police station
a first lieutenant a sergeant and four conscripts were killed when their armored vehicle was attacked on the highway
ansarbeital maqdishas claimed many attacks against the army and police in sinai
dual cov six people including one civilian were killed when a car bomb exploded near the police station
ansarbeital maqdis an isisaffiliate claimed responsibility for the attack
ansarbeital maqdishas claimed many attacks against the army and police
abstractive summarization with extractive methods tasks for the rst time
exploited model to sentence compression to lay the groundwork for subsequent summarization with dierent granularities
used encoder decoder with attention method to generate news headlines
added a selective gate network to the basic model in order to control which part of the information owed from encoder to decoder
raised a model based on graph and attention mechanism to strengthen the positioning of vital tion of source texts
so as to solve rare and unseen words proposed the copynet model and pointing mechanism created read again and copy mechanisms
made a combination of the basic model with large vocabulary trick lvt feature rich encoder pointer generator and hierarchical attention
in addition to pointer generator other tricks of this paper also contributed to the experiment results
presented an updated version of pointer generator which proved to be better
as for duplicate words for sake of solving problems of over or missing translation came up with a coverage mechanism to avail oneself of historical information for attention calculation while provided a progressive version
introduced a series of diverse cell structures to solve the duplicate words
so far few papers have considered about the structural or sementic issues at the language level in the eld of summarization
presented a novel vised method that made use of a pruned dependency tree to acquire the sentence compression
based on a chinese short text summary dataset lcsts and the attentional model proposed to enhance the semantic relevance by calculating the cos similarities of summaries and source texts
conclusion in our paper we construct a dual attentional model comprising source and extractive encoders to generate summaries
in addition we put forward the modied sentence ranking algorithm to extract a specic number of high weighted sentences for the purpose of strengthening the semantic representation of the extractive encoder
furthermore we introduce the pointer generator and coverage mechanisms in our models so as to solve the problems of oov and duplicate words
in the non anonymized cnn daily mail dataset our results are close to the state of the art rouge scores
moreover we get the highest abstractive scores as well as obtain such summaries that have better readability and higher semantic accuracies
in our future work we plan to unify the reinforcement learning method with our abstractive models
acknowledgments we thank the anonymous reviewers for their insightful comments on this paper
this work was partially supported by national natural science foundation of china and
the correspondence author is sujian li
niantao xie et al
references
bahdanau d
cho k
bengio y
neural machine translation by jointly learning to align and translate
arxiv preprint

cao z
li w
li s
wei f
li y
attsum joint learning of focusing and summarization with neural attention
arxiv preprint

cheng j
lapata m
neural summarization by extracting sentences and words
arxiv preprint

filippova k
strube m
dependency tree based sentence compression
in ceedings of the fifth international natural language generation conference
pp

association for computational linguistics
gu j
lu z
li h
li v
o
incorporating copying mechanism in sequence sequence learning
arxiv preprint

gulcehre c
ahn s
nallapati r
zhou b
bengio y
pointing the unknown words
arxiv preprint

lin c
y
rouge a package for automatic evaluation of summaries
text
lopyrev k
generating news headlines with recurrent neural networks
arxiv rization branches out preprint

ma s
sun x
xu j
wang h
li w
su q
improving semantic relevance for sequence to sequence learning of chinese social media text summarization
arxiv preprint

nallapati r
zhai f
zhou b
summarunner a recurrent neural network based sequence model for extractive summarization of documents
in aaai
pp

nallapati r
zhou b
gulcehre c
xiang b
al
abstractive text tion using sequence to sequence rnns and beyond
arxiv preprint

nema p
khapra m
laha a
ravindran b
diversity driven attention model for query based abstractive summarization
arxiv preprint

pal a
r
saha d
an approach to automatic text summarization using wordnet
in advance computing conference iacc ieee international
pp

ieee
paulus r
xiong c
socher r
a deep reinforced model for abstractive rization
arxiv preprint

rush a
m
chopra s
weston j
a neural attention model for abstractive sentence summarization
arxiv preprint

see a
liu p
j
manning c
d
get to the point summarization with generator networks
arxiv preprint

tan j
wan x
xiao j
abstractive document summarization with a based attentional neural model
in proceedings of the annual meeting of the association for computational linguistics volume long papers
vol
pp

tu z
lu z
liu y
liu x
li h
modeling coverage for neural machine translation
arxiv preprint

zeng w
luo w
fidler s
urtasun r
ecient summarization with read again and copy mechanism
arxiv preprint

zhou q
yang n
wei f
zhou m
selective encoding for abstractive sentence summarization
arxiv preprint

