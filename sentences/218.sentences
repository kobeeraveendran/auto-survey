g u a l c
s c v
v i x r a generating an overview report over many documents jingwen hao cheng wenjing liqun jie of massachusetts lowell ma corporation cambridge ma wang hao zhang cheng zhang wenjing
uml
edu

com
uml
edu abstract how to efciently generate an accurate well structured overview report orpt over thousands of related ments is challenging
a well structured orpt consists of sections of multiple levels e

sections and tions
none of the existing multi document summarization mds algorithms is directed toward this task
to overcome this obstacle we present ndorgs numerous documents overview report generation scheme that integrates text ltering keyword scoring single document summarization sds topic modeling mds and title generation to erate a coherent well structured orpt
we then devise a multi criteria evaluation method using techniques of text mining and multi attribute decision making on a tion of human judgments running time information erage and topic diversity
we evaluate orpts generated by ndorgs on two large corpora of documents where one is classied and the other unclassied
we show that using saaty s pairwise comparison point scale and under topsis the orpts generated on sds s with the length of of the original documents are the best overall on both datasets
introduction it is a challenging task to generate an accurate overview report orpt over a large corpus of related documents in reasonable time
this task arises from the need of analyzing sentiments contained in a large number of documents
by large we mean several thousand and by reasonable time we mean a few hours of cpu time on a commonplace puter
moreover an should be of a reasonable size to save reading time
for a corpus of about ments for instance an orpt should not exceed pages
moreover an opt should be well structured organized in two or three levels for easier reading
it should also include gures to highlight frequencies and trends of interesting tities such as names of people and organizations contained in the documents
most of the early multi document summarization mds systems are designed to produce a summary for a handful of documents
for example the mds systems are designed to handle the sizes of duc datasets
the and datasets each consists of only about ve hundred documents and provide bench marking summaries for mds tasks each consisting of or fewer documents
directly applying these mds systems to ating an orpt for a corpus containing thousands of ments could possibly generate a proportionately longer and disorganized summary
more recently t cmda was devised to generate an english wikipedia article on a cic topic over thousands of related documents
however t cmda is still not suitable for generating a well organized orpt for a large corpus of documents containing multiple topics
to accomplish our task we devise a document generation scheme called ndorgs which stands for numerous documents overview report generation scheme
ndorgs is capable of generating a coherent and well structured orpt over a large corpus of input ments
in particular ndorgs rst lters out noisy data and then uses a suitable sds algorithm to generate document summaries for each document with a length tio e



or
of the original document
for simplicity we refer to a summary with a length ratio of the original document as a summary
ndorgs then uses a suitable clustering algorithm to partition the corpus of summaries into two level clusters
for each cluster of summaries ndorgs generates a section or a subsection using a suitable mds algorithm and uses a suitable title generation algorithm to generate a title for that cluster
nally ndorgs generates an orpt by integrating sections and corresponding subsections in the order of descending salience of clusters
we devise an evaluation method using human tions and text mining techniques to measure the quality of orpts generated by ndorgs and use the running time to measure efciency
we combine the following criteria in descending order of importance human evaluations ning time coverage and diversity and use multi attribute decision making techniques to nd the best value of
human evaluations follow the standard criteria given in while coverage and diversity are evaluated by comparing respectively keyword scores in an orpt and the original corpus of documents and the symmetric ference of clusters of keywords
we use saaty s pairwise comparison point scale and topsis to determine the best overview
we apply ndorgs to a corpus of ed bbc news articles thereafter referred to as bbc news and a corpus of unclassied articles extracted from factiva under the keyword search of marxism from the year of to the year of hereafter referred to as factiva marx for a project of analyzing public sentiments
we show that for both datasets using semantic wordrank to generate single document summaries latent let allocation lda to cluster summaries gflow to generate a summary over a cluster of summaries and dtatg to generate a title for a section or tion the orpts generated on
summaries provide the best overall orpts
sensitivity analysis shows that this sult is stable
the rest of this paper is organized as follows in section we describe related work and in section we present the architecture of ndorgs and the algorithms
we present experiment and evaluation results in section and trending methods and sample gures in section
we conclude the paper in section
related work we discuss related work on topic modeling text rization and title generation

topic modeling topic modeling partitions documents into topic clusters with each cluster representing a unique topic
latent let allocation lda and spectral clustering sc are popular topic clustering methods
lda treats each ument in a corpus as a mixture of latent topics that generate words for each hidden topic with certain probabilities
as a probabilistic algorithm lda may produce somewhat ferent clusters on the same corpus of documents on ent runs
it also needs to predetermine the number of topics
sc is a deterministic and faster clustering algorithm which also needs to preset the number of topics
it uses ues of an afnity matrix to reduce dimensions
it then uses k means to generate clusters over eigenvectors ing to the k smallest eigenvalues
other clustering methods such as pw lda let multinomial mixture and neural network models are targeted at corpora of short documents such as stracts of scientic papers which are not suited for our task
the documents we are dealing with are much longer
even if we use summaries to represent the original documents a summary may still be signicantly longer than a typical stract
we note that the k nn graph model may also be used for topic clustering
methods for keyword phrase extraction include k means tf idf textrank rake and tophrase

text summarization text summarization includes sds mds hierarchical summarization and structural summarization


single document summarization sds algorithms have been studied intensively and sively for several decades for example see
as it is today unsupervised extractive algorithms are still more efcient more accurate and more exible than supervised or abstractive summarization
among unsupervised extractive algorithms the tic wordrank swr algorithm is currently the best in terms of both accuracy and efciency
built on a weighted word graph with semantic and co occurrence edges swr scores sentences using an article structure biased pagerank algorithm with a softplus function elevation ment and promotes topic diversity using spectral subtopic clustering under the word movers distance metric
swr outperforms all previous algorithms supervised and supervised over under the standard and rouge measures
over the bank dataset swr outperforms each of the three human annotators and compares favorably with the combined formance of the three annotators


multi document summarization
an mds algorithm takes several documents as input and generates a summary as output
most mds algorithms are algorithms of selecting sentences
sentences may be ranked using features of term frequencies sentence positions and keyword co occurrences among a few other things
sentences may be selected using algorithms such as based lexical centrality lexrank centroid based tering support vector regression syntactic ages between text and integer linear programming
selected sentences may be reordered to improve coherence using probabilistic methods
among all the mds algorithms the glfow algorithm is focused on sentence coherency
gflow is an pervised graph based method that extracts a coherent mary from multiple documents by selecting and reordering sentences to balance coherence and salience over an imate discourse graph adg of sentences
an adg siders sentence discourse relations among documents based on relations such as deverbal noun reference event entity continuation discourse markers sentence inference and co reference mentions
in an adg graph each node resents a sentence
two nodes are connected if they have one of the aforementioned sentence relations
edge weight is calculated based on the number of sentence relations tween two sentences
based on gflow yasunaga et al
devised a pervised neural network model that combines personalized discourse graph pdg gated recurrent units gru and graph convolutional network gcn to rank and select sentences
tcsum is another neural network model that leverages text classication to improve the ity of multi document summaries
however neural network methods require large scale training data to obtain a good result
more recently liu et al
devised a large scale marization method named t dmca to generate an english wikipedia article
t dmca combines extractive rizations and abstractive summarizations trained on a scale wikipedia dataset to summarize the text
while cmda is capable of creating summaries with specied ics as wikipedia article it can hardly generate an overview report for a large corpus of documents containing multiple topics
moreover t dmca fails on ranking its sub topics based on their salience


hierarchical and structural summarization
hierarchical summarization and structural summarization are two approaches to enhancing sds and mds algorithms
buyukkokten et al
and otterbacher et al
devised algorithms for generating a hierarchical summary of a gle document
a hierarchical summarizer for twitter tweets based on twitter lda summarizes the news tweets into a exible topic oriented hierarchy
summa is a system that creates coherent summaries hierarchically in the order of time locations or events
these methods focus on single documents or short texts or require documents be written with a certain predened structure template making them unsuited for our task
structured summarization algorithms rst identify ics of the input documents
sauper et al
presented an overview generation system that uses a high level structure of human authored documents to generate a topic structure multi paragraph overview with domain specic templates
li et al
developed a summary template generation system based on an entity aspect lda model to cluster sentences and words and generate sentence patterns to resent topics
autopedia is a wikipedia article ation framework that selects wikipedia templates as article structures

title generation generating an appropriate title for a block of text is an important task for generating an overview report of tilevel structure as sections and subsections each needs a title
alexander et al
attempted to generate an stractive sentence summary as title for a given document
ramesh et al
devised an attentional encoder decoder neural network model used for machine translation to generate a title for short input sequences one or two tences
shao and wang devised dtatg for ically generating an adequate title for a given block of text using dependency tree pruning
in particular dtatg rst extracts a few critical sentences that convey the main ings of the text and are more suitable to be converted into a title
each critical sentence is given a numerical ranking score
it then constructs a dependency tree for each of these sentences and trims certain non critical branches from the tree to form a new sentence that retains the original ings and is still grammatically sound
dtatg uses a title test consisting of a number of rules to determine if a tence is suited for a title
if a trimmed sentence passes the title test then it becomes a title candidate
dtatg selects the title candidate with the highest ranking score as the nal title
evaluations by human judges showed that dtatg can generate adequate titles
description of ndorgs ndorgs see fig
consists of ve modules processing
hierarchical topic clustering
cluster summarizing
cluster titling
report generating
step
preprocessing pp the pp module performs text ltering and sds
in particular it rst determines what guage an input document is written in eliminates irrelevant text such as non english articles urls and duplicates and extracts for each document the title and subtitles if any publication time publication source and the main content
it also removes any interview type of articles cause contents in interviews are too subjective and the ture of an interview is essentially a different genre
it then generates a summary of appropriate length for each ment
extracting summaries is necessary for speeding up figure
ndorgs architecture the process and is sufcient for generating a good overview because only a small part of the most important content of an article will ultimately contribute to the overview report also noted in
step
hierarchical topic clustering htc multiple topics are expected over a large corpus of documents where each topic may further contain subtopics
fig
is an ample of a hierarchical topic subtree over bbc news
in this subtree each node is a topic with six most frequent words under the topic
the root illustrates the most quent words of the corpus
the rst level topic cluster tains two subtopics entertainment and technology
when words such as series comedy and episode under topic entertainment are discovered for a substantial ber of times a subtopic of tv series may be detected
the hierarchically structured topic clusters provide detail mation about topic relationships contained in a large corpus of documents
htc is designed to capture topic relations
in lar the htc module partitions a corpus of documents into multilevel topic clusters and ranks the topic clusters based on their salience
we present a two level topic clustering algorithm in this paper three or more level topic clustering is similarly designed
the input can be either the original documents or the summaries generated in step
note that clustering on the original documents and clustering on the summaries of the documents may result in different tions see section

for simplicity we use documents to denote both
in particular htc rst partitions documents into k ters denoted by c


ck using a clustering method such as lda or sc
these are referred to as the level clusters
for each top level cluster ci if n where denotes the number of documents contained in figure
a subtree of topic clusters learned from bbc news dataset ci and n is a preset number for example n then htc further partitions ci into ki sub clusters as the second level clusters where ki
if in this new clustering all but one cluster are empty then this means that documents in ci can not be further divided into sub topics
in this case we sort the documents in ci in scending order of document scores and split ci evenly into ki clusters except the last one
for a second level cluster cij of ci if n we may further create a third level sub clustering by clustering cij or simply splits cij evenly into kij clusters except the last one still at the second level
note that at each level of clustering it is possible to have empty clusters
assume that cluster c consists of n documents denoted by where i


n
let pi denote the probability that belongs to cluster c
such a probability can be easily computed using lda
we dene the score of cluster c using the following empirical formula sc n j i
step
cluster summarizing csum the csum ule generates a coherent summary of an appropriate length for all sds summaries in a given cluster using an mds gorithm
this cluster is typically at the second level except when a rst level cluster does not have a second level ter in this case the cluster is at the rst level
in particular for each second level cluster csum takes its corresponding preprocessinghierarchical topic clusteringcluster summarizingndorgs systemcluster titlingreport generatingdocumentsoverview reportpeople government film music bbc game film director actor award actress star mobiletechnology digital software computer online entertainmenttechnologystar series channel comedy audience episode best film director actress awards oscar ballet musical theatre films original young tv seriesoscarmusicalbroadband digital internet service online access games video sony play gamers xboxsystem software apple industry peer to peer internetgamecompanybbc news figure
an example of ndorgs processing sds summaries as input and uses an appropriate mds to produce a summary of a suitable length
step
cluster titling ct a series of good section headings is one of the most important components in a clear and well organized overview reports
since an overview port contains multiple topics section headings help readers identify the main point of each section in the overview port
the ct module creates appropriate titles for both level and second level clusters using a title generation rithm
step
report generating rg as mentioned in step each cluster has a topic score and each document mary has a corresponding probability score
a higher ter score represents a more signicant topic
the rg ule reorders clusters at the same level according to cluster scores in descending order and generates an overview report of up to k top level sections where each top level section may also contain up to ki second level sections
each tion and subsection has a section title and each subsection contains an mds summary
example fig
is an example of the processes of ndorgs with an input of documents
the pp module determines what language an input document is written in eliminates evant text such as non english articles urls and cates and extracts information including the title and the main content
it then generates an sds summary for each main content
the htc module groups the fteen maries into two level hierarchical topic clusters with two top level clusters where the cluster entertainment consists of ve document summaries and the cluster business sists of ten summaries
both of the top level clusters have three second level clusters where each second level cluster consists of a number of sds summaries and is labeled with a unique color
in step extracts a coherent mds summary for each second level cluster summaries and the rst level cluster if there are no second level clusters
in step ndorgs produces a section title for each cluster and sub cluster
finally the rg module rearranges the maries and section titles based on their salience to generate an overview report with a multilevel structure
evaluations
datasets we describe datasets selections of algorithms and rameter setting for evaluating ndorgs
we use the following two types of corpora
the bbc news dataset of classied cles stemmed from bbc news in the years of and labeled in business entertainment politics sports and technology

the factiva marx of ed articles extracted from factiva under the keyword search of marxism
the statistics of these two datasets are shown in table
factiva marx dataset is available at
ndorg
net
process ppprocess htcprocess csummds summaries process ctprocess summariestitleshierarchical clustersbusinessentertainmentoscartv seriesmusicalfinancereal estatecompanycluster titlesore costs hit global steel firmsgm pays to evade fiat buyoutvera drake leads uk oscar hopes



house prices show slight increase





ore costs hit global steel firms



steel firms have dropped worldwide amidconcerns that higher iron ore costs will hit profitgrowth





gm pays to evade fiat buyout



fiat claims that gm is legally obliged to buy of the car unit it does not already own





house prices show slight increase



uk housing market is slowing after interest rateincreases





vera drake leads uk oscar hopes



best male and female film actors boosting theiroscars hopes this month







overview report table
size comparisons between different datasets dataset factiva marx bbc news of docs avg
of docs task of tokens

avg
of tokens doc vocabulary size


selection of algorithms we use the state of the art semantic wordrank as the sds algorithm and gflow as the mds algorithm for ndorgs
in particular gflow needs to solve the lowing ilp problem maximize subject to b xix xi xj x xj where variable x is a summary is the number of tences in the given summary is the salience score of x is the coherence score of x xj is a measure between two sentences xi and xj and is the length of sentence xi
parameters and are learned using the dataset
we investigate lda and sc and determine that lda is more appropriate
we then use dtatg to erate a title for each section and subsection

in the csum module ndorgs uses gflow to duce cluster summaries
the length l of an mds mary of nonempty cluster c is determined by l if if

in the ct module ndorgs applies dtatg to erate a title for each cluster and sub cluster

in the rg module ndorgs reorders clusters at the same level according to cluster scores sc dened in section
for each level of clusters if a cluster tains less than documents then we consider this cluster a minor topic
ndorgs merges such cluster mds s into a section under the title of other topics where the mds s are each listed as a bullet item in descending order of cluster scores
to achieve the best performance of ndorgs it is ical to determine the sds length ratio and the top level number k of clusters
in section
and section
we describe experiments for deciding these parameters

parameter settings
text clustering evaluations for deciding k we determine empirically the setting of parameters that lead to the best overall performance for both bbc news and factiva marx
for each dataset ndorgs produces three corresponding to three summaries where



the parameter settings are listed below
in the pp module ndorgs generates document summaries for each document with the length ratio

and


in the htc module ndorgs creates two level topic clusters using lda
to achieve a higher topic ing accuracy we set the number k of the top level clusters to k as suggested in section
see fig

to generate the second level clusters we set n to determine if a sub cluster should be ther divided recall that if a cluster contains more than n documents a further division will be performed
the number of second level clusters is automatically determined by ndorgs using the method mentioned in step from section
six orpts generated by ndorgs are available at
ndorg
net
let d be a corpus of text documents
suppose that we have a gold standard partition of d into k clusters c


ck and a clustering algorithm generates k clusters denoted by a


ak
we rearrange these clusters so that the symmetric difference of ci and ai denoted by ai is minimum where y y y
that is for all i k ai aj
we dene csd score for a and c as follows where csd stands for clusters symmetric difference c ci k k ci ai ci p ai ci ci with p and r being precision and recall dened by p ai ci
clearly f c and f c is the best possible
and ci let hlda d hsc d hlda s and hsc s denote spectively the algorithms of applying hlda and hsc on original documents and
summaries generated by mantic wordrank
comparisons of clustering quality fig
compares the csd scores of hlda d hlda s hsc d and hsc s over the labeled corpus of bbc news articles
we can see that hlda d is better than hlda s which is better than hsc d and hsc d is better than hsc s
all of these rithms have the highest csd scores when the number of top level topics k
this is in line with a general rience that the number of top level sections in an overview report should not exceed
csd scores on bbc news figure
csd scores on the labeled bbc news corpus we observe that hlda d offers the best accuracy
thus we will use the clustering generated by hlda d as the baseline for comparing csd scores
fig
and fig
depict the comparison results of hlda s hsc d and hsc s against hlda d on bbc news and factiva marx
comparisons of clustering running time fig
depicts the running time of clustering the bbc news and marx datasets by different algorithms into two level clusters on a dell desktop with a quad core intel xeon
ghz processor and gb ram
we choose the top level ters numbers k
we can see that for both corpora hsc s is the fastest hsc d is slightly slower hlda d is the slowest and hlda s is in between hlda d and hsc d
we note that this result is expected due to the following two facts generating clusters over shorter documents is more time cient than over longer documents
spectral clustering sc is much faster than topic word distribution clustering lda
we also see that when the number of top level ters is small the two level clustering running time is high
this is because for a given corpus a smaller number of top level clusters would mean a larger number of level clusters requiring signicantly more time to compute
csd scores on factiva marx figure
csd scores against hlda d the turning points are around k
on the other hand when the number of top level clusters is larger the number of second level clusters is smaller which implies a lower time complexity
however if the number of sections at the same level is too large it would make a report harder to read
thus we need to nd a balance

overview reports evaluations human judgments we recruited human annotators from amazon mechanical turk amt to evaluate six orpts generated by ndorgs on respectively

and
summaries of documents in the two corpora where each port was evaluated by four human annotators
human ments are based on the following seven categories ence uselesstext redundancy referents overlyexplicit grammatical and formatting
an orpt has a high quality if the following seven gories all have good ratings sentences in the report are coherent
the report does not include useless or of top level







scoreshlda dhlda shsc dhsc of top level







scoreshlda shsc dhsc of top level




scoreshlda shsc dhsc s length ratio may help ndorgs generate a better report on factiva marx
the reason is likely that the factiva marx corpus contains almost three times more documents than the bbc news corpus and each document in factiva marx contains on average over ten times larger number of tokens than that in a document from bbc news
this indicates that for a larger corpus we may want to use summaries of a smaller length ratio for ndorgs to generate a human preferred overview report
running time of clustering the bbc news corpus human evaluation on bbc news reports running time of clustering the factiva marx corpus figure
comparisons of clustering running time the report does not contain redundancy fusion text
information
common nouns proper nouns and nouns are well referenced in the report
the entity mentions are not overly explicit
grammars are correct
in particular we asked the report is well formatted
amt annotators to follow the evaluation schema in their evaluations
the evaluation scores are provided in appendix table
fig
depicts the average scores of human tors using a point system with being the best
for the bbc news corpus fig
shows that the report generated on
summaries outperforms reports generated on
summaries and
summaries in all categories shows that the report cept overlyexplicit
generated on
summaries is better than reports ated on
summaries and
summaries in most of the categories they areuselesstext referents explicit grammatical and formatting
note that a larger length ratio of summaries would help ndorgs generate a better report on bbc news while a smaller fig
human evaluations on marx reports
figure
human evaluations

and
reports with summary length ratio

and
over bbc news


and
reports with summary length ratio

and
over factiva marx
time efciency fig
illustrates the running time of ndorgs on bbc news and factiva marx with the lowing results
ndorgs incurs respectively about and more time to generate an overview report on
of top level time dhlda shsc dhsc of top level time dhlda shsc dhsc





summaries and
summaries than
summaries see fig


ndorgs incurs respectively over times and
times longer to generate an overview report on
summaries and
summaries than it does on
summaries see fig


ndorgs achieves the best time efciency on
summaries and the summary length ratios have cantly impacts on time efciency working on a larger summary length ratio incurs a longer running time
running time of ndorgs on bbc news information coverage we evaluate information coverage via comparison of the top words in an overview report and the top words in the corresponding corpus see section a
in appendix
listed below are the summary of the parison results
for bbc news over of the top words in the pus are also top words in the three overview reports combined over one third of the top words in the pus are top words in the report on
summaries over one half of the top words in the corpus are top words in the report on
summaries as well as in the report on
summaries and over of the top words in the corpus are top words in each report

for factiva marx of the top words in the pus are also top words in the three overview reports combined of the top words in the corpus are top words in the report on
summaries as well as in the report on
summaries of the top words in the corpus are top words in the report on
summaries and the top words in the corpus are top words in each summary
these results indicate that ndorgs is capable of capturing important information of a large corpus
let a be a set of top k words from the original corpus and b a set of top k words from a generated report
let b denote the information coverage score of a and b
then b with b being the best possible score
the information coverage scores for reports over bbc news and factiva marx are listed in table
we can see that the report generated on
summaries achieves the highest information coverage score over bbc news and the report generated on
summaries or
summaries achieves the highest tion coverage score over factiva marx
table
information coverage scores bbc news factiva marx








running time of ndorgs on factiva marx figure
comparisons of running time the seven evaluation criteria do not include information coverage and topic diversity for it is formidable for a man annotator to read through several thousand documents and summarize information coverage and topic diversity of these documents
to overcome this obstable we use text mining techniques described in the following two tions
topic diversity we generate lda clusters for the nal corpus and for the report by treating each sentence as a document in the latter case
we then evaluate the top words among these clusters using csd scores to measure topic diversity see table
we can see that regarding topic diversity reports generated on
summaries outperform reports generated on
summaries and
summaries for both bbc news and factiva marx



time in


time in secondssummaryhldamdsreport weight of criteria of human evaluation time age and diversity
thus the decision made by topsis is stable over both bbc news and factiva marx
trending graphs in addition to generating the text component of the overview which is the major part it is also much desirable to generate trending graphs to provide the reader with an easy visual on name entities of interests including nizations persons and geopolitical entities
in particular it uses a name entity recognition tool such as nltk
org to tag name entities and compute their frequencies
figs
and are the statistics graphs over the factiva marx dataset
for a specic name entity of interests we also generate a tfidf score in addition to its frequency
the tfidf score of each category is the summation of the tfidf score of each document with respect to the corpus of articles in that year which measures its signicance
fig
depicts such a trending graph for south korea
it is interesting to note that while south korea was both mentioned times in and its signicance was much higher in than
fig
is the trending graph of america over bbc news with respect to the six categories of news classications
it is interesting to note that while america has one more count in entertainment than in politics the importance of america in politics is about twice as much as that in entertainment
table
topic diversity scores bbc news factiva marx








overall performance we evaluate the overall mance of overview reports using the following criteria listed in the order of preference human evaluation time efciency information coverage and topic diversity
we then use saaty s pairwise comparison point scale and the technique for order preference by similarity to an ideal solution topsis to determine which length ratio of summaries produces the best overview report
let the three reports for the same corpus be the three alternatives denoted by
let the human uation mean score running time information coverage score and topics diversity score be four criteria denoted by
next we use saatys pairwise son point scale to determine weights for each criterion
a weight vector w is then computed ing the analytic hierarchy process ahp procedure where wi is the weight for criterion ci
a weighted normalization decision matrix t is then generated from the normalized matrix r and the weight vector w
the alternatives and are ranked using euclidean distance and a similarity method see table
we can see that the overview report generated on
summaries achieves the best overall formance on both bbc news and factiva marx
table
overall performance coverage





human eval






time model





rank diversity





sensitivity analysis a stable decision made by topsis is not easily changed when adjusting the weight of the criteria
to evaluate how stable the decision topsis has made we carry out sensitivity analyses to measure the sensitivity of weights
for criterion ci we vary wi with a small increment c by i wi
we then adjust the weights for other criteria cj by j wi
we recompute the ranking until another alternative is ranked number one
fig
depicts the sensitivity ses results
in both fig
and fig
reports generated on
summaries keep the highest rank while adjusting the sensitivity analysis of topsis over bbc news sensitivity analysis of topsis over factiva marx figure
the axis indicates the weight of corresponding criterion in increments decrements of
each time and the y axis shows the new topsis values





































figure
the most frequent organizations in factiva marx figure
the most frequent persons in factiva marx figure
the most frequent geopolitical entities in factiva marx figure
the trend of south korea in factiva marx figure
the trend of america in bbc news




score conclusion we presented ndorgs for generating a coherent structured orpt over a large corpus of documents with ceptable efciency and accuracy
our experiments show that the orpts generated on
summaries are the best overall with respect to human evaluations running time formation coverage and topic diversity
sensitivity analysis shows that this result is stable
acknowledgment we thank dr
leif tang and dr
ting wang for making the factiva marx dataset available to us
we are grateful to dr
peilong li for helping some of the experiments
references d
bahdanau k
cho and y
bengio
neural machine translation by jointly learning to align and translate
corr

d
m
blei a
y
ng and m
i
jordan
latent dirichlet allocation
in journal of machine learning research
o
buyukkokten h
garcia molina and a
paepcke
seeing the whole in parts text summarization for in proceedings web browsing on handheld devices
of the international conference on world wide web www pages
acm
z
cao w
li s
li and f
wei
document summarization via text classication
aaai pages
improving in z
cao w
li s
li f
wei and y
li
attsum joint learning of focusing and summarization with neural attention
in proceedings of coling the international conference on computational tics technical papers pages
duc
ence
projects duc
html
nlpir
nist
understanding document g
erkan and d
r
radev
lexrank graph based ical centrality as salience in text summarization
nal of articial intelligence research
factiva marx
marx dataset

ndorg
net
w
gao p
li and k
darwish
joint topic ing for event summarization across news and social in proceedings of the acm media streams
ternational conference on information and knowledge management pages
acm
d
gillick and b
favre
a scalable global model for summarization
in proceedings of the workshop on teger linear programming for natural langauge cessing pages
association for computational linguistics
d
greene and p
cunningham
practical solutions to the problem of diagonal dominance in kernel ment clustering
in proc
international ence on machine learning pages
acm press
j
a
hartigan and m
a
wong
algorithm as a k means clustering algorithm
journal of the royal statistical society
series c applied statistics
k
hong j
m
conroy b
favre a
kulesza h
lin and a
nenkova
a repository of state of the art and competitive baseline summaries for generic news summarization
in lrec pages
c

hwang and k
yoon
methods for multiple tribute decision making
in multiple attribute decision making pages
springer
j
christensen mausam s
soderland and o
etzioni
towards coherent multi document summarization
in hlt naacl
d
jones
factiva global news database
www
dowjones
com products
j
christensen s
soderland g
bansal et al
archical summarization scaling up multi document in proceedings of the annual summarization
meeting of the association for computational tics volume pages
duc
quality questions

nist
gov quality
questions
txt
t
n
kipf and m
welling
semi supervised sication with graph convolutional networks
arxiv preprint

m
lapata
probabilistic text structuring experiments in proceedings of the with sentence ordering
annual meeting on association for computational linguistics volume pages
association for computational linguistics
c
li y
lu j
wu y
zhang z
xia t
wang d
yu x
chen p
liu and j
guo
lda meets a novel model for academic abstract in companion of the the web conference tering
on the web conference pages
international world wide web conferences steering committee
c
li x
qian and y
liu
using supervised in based ilp for extractive summarization
ings of the annual meeting of the association for computational linguistics volume long papers volume pages
p
li j
jiang and y
wang
generating templates of entity summaries with an entity aspect model and tern mining
in proceedings of the annual ing of the association for computational linguistics acl pages
association for tional linguistics
s
li y
ouyang w
wang and b
sun
document summarization using support vector sion
in proceedings of duc
citeseer
p
j
liu m
saleh e
pot b
goodrich r
sepassi l
kaiser and n
shazeer
generating wikipedia by summarizing long sequences
corr

a
lulli t
debatty m
dellamico p
michiardi and l
ricci
scalable k nn based text clustering
in big data big data ieee international conference on pages
ieee
r
mihalcea and p
tarau
textrank bringing der into texts
in proceedings of the conference on empirical methods in natural language processing
r
nallapati b
xiang and b
zhou
to sequence rnns for text summarization


corr r
nallapati f
zhai and b
zhou
summarunner a recurrent neural network based sequence model for in aaai extractive summarization of documents
pages
in neural information processing systems pages
j
otterbacher d
radev and o
kareem
news to go hierarchical text summarization for mobile devices
in proc
of acm sigir pages
l
page s
brin r
motwani and t
winograd
the pagerank citation ranking bringing order to the web
technical report stanford infolab
d
r
radev h
jing m
sty and d
tam
based summarization of multiple documents
inf
cess
manage

s
j
rose d
engel n
cramer and w
cowley
matic keyword extraction from individual documents
in in book text mining applications and theory pp

a
m
rush s
chopra and j
weston
a neural tention model for abstractive sentence summarization
corr

g
salton and c
buckley
term weighting approaches inf
process
manage
in automatic text retrieval

c
sauper and r
barzilay
automatically generating wikipedia articles a structure aware approach
in proceedings of the joint conference of the nual meeting of the acl and the international joint conference on natural language processing of the afnlp volume acl pages
sociation for computational linguistics
j
shang j
liu m
jiang x
ren c
r
voss and j
han
automated phrase mining from massive text corpora
ieee transactions on knowledge and data engineering
l
shao and j
wang
dtatg an automatic title erator based on dependency trees
in proceedings of the international joint conference on knowledge covery knowledge engineering and knowledge agement k pages portugal
scitepress science and technology publications lda
m
t
nayeem and y
chali
extract with order for coherent multi document summarization
in ings of the workshop on graph based methods for natural language processing pages
association for computational linguistics
x
wang m
nishino t
hirao k
sudoh and m
gata
exploring text links for coherent multi document summarization
in proceedings of coling the international conference on computational guistics technical papers pages
a
y
ng m
i
jordan and y
weiss
on spectral in advances clustering analysis and an algorithm
j
xu p
wang g
tian b
xu j
zhao f
wang and h
hao
short text clustering via convolutional neural table
human evaluation scores where


represent respectively ence uselesstext redundancy referents overlyexplicit grammatical and formatting corpus report a appendix a
human evaluation scores a
comparisons of top words human evaluation score



bbc news
factiva marx
networks
in proceedings of naacl hlt pages
c
yao x
jia s
shou s
feng f
zhou and h
liu
autopedia automatic domain independent wikipedia in proceedings of the article generation
national conference companion on world wide web www pages
acm
m
yasunaga r
zhang k
meelu a
pareek k
srinivasan and d
r
radev
graph based neural multi document summarization
in conll
j
yin and j
wang
a dirichlet multinomial mixture in model based approach for short text clustering
proceedings of the acm sigkdd international conference on knowledge discovery and data mining pages
acm
d
yogatama f
liu and n
a
smith
extractive marization by maximizing semantic volume
in ceedings of the conference on empirical ods in natural language processing pages
h
zhang and j
wang
semantic wordrank ating finer single document summarizations
arxiv e prints sept

top words are listed below in the corpus of bbc news and the corpus of factiva marx respectively for isons where the rst item depicts the top words in the original corpus and the second third and fourth items depict respectively the top words in the report ated on
summaries
summaries and
summaries listed in descending order of keyword scores
the words in bold are the common top words that occur across all four rows
the words with underlines are the top words that cur in the rst row and two of the other three rows
the words in italics are the top words that occur in the rst row and just one of the other three rows
top word comparisons for bbc news
people told best government time year ber three lm music bbc set game going years labour good well top british european win ket won company public second play mobile work rm blair games minister expected england chief technology party sales news plans including help election digital players director economic big
people best number government lm year three game howard music london british face biggest net action rm deal rise national foreign singer michael leader oil blair dollar stock star cup line future games work won list tional coach win mark tory labour brown general prices market car help users
year people number three best british lm company won labour music net bbc government leader shares european earlier chart third games state win coach expected second months cal house economic game years team start ester england election chief international michael prot champion award star announced service ture rm top news
people england year lm labour boss rm spite number three wales british nations best company music blair set record oil time years won prices plans net online including lms bbc court games game brown david government expected club action beat total group unit rms rules mobile second analysts future computer top word comparisons for factiva marx
party chinese china political people communist economic national state government years cial great time rights development international president central war north university power foreign global military united work country history south marxism human western soviet well system mao american news public cultural long states countries three left media british cluding
party china chinese communist political years economic rights human president people tional year state leaders government central countries news social country leader time foreign power north nuclear top marxism ideological led media war beijing western united development soviet mao states history university capitalism cial market ofcials march korea democracy south
china party communist chinese political nomic years rights president people central state social united north beijing western news media mao cpc war human anniversary public leader states members country ment south marxism democratic national power year foreign american education international july nuclear day book leadership committee leaders copyright study jinping
china party communist chinese economic years people political human news state social ernment central national leader president dia cultural rights mao power development year international university leaders history united jing copyright socialist global great top nation universities western revolution nuclear foreign public agency marxism time members congress war change north
