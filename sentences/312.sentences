evaluation of text generation a survey asli celikyilmaz microsoft research
com elizabeth clark university of washington
washington
edu jianfeng gao microsoft research
com abstract the paper surveys evaluation methods of natural language generation nlg tems that have been developed in the last few years
we group nlg evaluation methods into three categories human centric evaluation metrics automatic metrics that require no training and machine learned metrics
for each egory we discuss the progress that has been made and the challenges still being faced with a focus on the evaluation of recently proposed nlg tasks and neural nlg models
we then present two case studies of automatic text summarization and long text generation and conclude the paper by proposing future research directions
n u j l c
s c v
v i x r a equal contribution
are grateful for the following people rahul jha sudha rao ricky lyond for their helpful comments and suggestions on earlier versions of this paper
we would like to thank the authors of the papers who gave us permission to use their gures tables and examples in our survey paper to summarize the related work
contents introduction
evolution of natural language generation






















why a survey on evaluation on natural language generation











outline of the survey





































































human centric evaluation methods
intrinsic evaluation
































extrinsic evaluation
































the evaluators


































inter evaluator agreement































percent agreement































cohen s

































fleiss


































krippendorff s






























untrained automatic evaluation metrics
n gram overlap metrics for content selection





















f score

































bleu

































nist


































rouge
































meteor
































hlepor
































ribes

































cider










































distance based evaluation metrics for content selection















edit distance based metrics


























vector similarity based evaluation metrics

















n gram based diversity metrics




























type token ratio ttr




























self bleu



































measure of textual lexical diversity




















explicit semantic content match metrics























pyramid

































spice


































spider

































semantic similarity models used as evaluation metrics





















syntactic similarity based metrics

























machine learned evaluation metrics
sentence semantic similarity based evaluation



















evaluating factual correctness




























regression based evaluation





























evaluation models with human judgments





















bert based evaluation































composite metric scores






























two case studies of task specic nlg evaluation
case study automatic document summarization evaluation











intrinsic methods
































extrinsic summarization evaluation methods
















case study long text generation evaluation



















evaluation via discourse structure























evaluation via lexical cohesion
























evaluation via writing style


























evaluation with multiple references




















conclusions and future directions chapter introduction natural language generation nlg a of natural language processing nlp deals with building software systems that can produce coherent and readable text
nlg can be applied to a broad range of nlp tasks such as generating responses to user questions in a chatbot translating a sentence or a document from one language into another offering suggestions to help write a story or generating summaries of time intensive data analysis
nlg evaluation is challenging mainly because many nlg tasks are open ended
for example a dialog system can generate multiple plausible responses for the same user input
a document can be summarized in different ways
therefore human evaluation remains the gold standard for almost all nlg tasks
however human evaluation is expensive and researchers often resort to automatic metrics for quantifying day to day progress and for performing automatic system optimization
recent advancements in deep learning have yielded tremendous improvements in many nlp tasks
this in turn presents a need for evaluating these deep neural network dnn models for nlg
in this paper we provide a comprehensive survey of nlg evaluation methods with a focus on uating neural nlg systems
we group evaluation methods into three categories human centric evaluation metrics automatic metrics that require no training and machine learned metrics
for each category we discuss the progress that has been made the challenges still being faced and proposals for new directions in nlg evaluation

evolution of natural language generation nlg is dened as the task of building software systems that can write i
e
producing explanations summaries narratives
in english and other human
just as people communicate ideas through writing or speech nlg systems are designed to produce natural language text or speech that conveys ideas to its readers in a clear and useful way
nlg systems have been used to generate text for many real world applications such as generating weather forecasts carrying interactive conversations with humans in spoken dialog systems chatbots captioning images or visual scenes translating text from one language to another and generating stories and news articles
nlg techniques range from simple template based systems that generate natural language text ing rules and templates to machine learned systems that have a complex understanding of human grammar
the rst generation of automatic nlg systems uses rule based or data driven pipeline methods
in their seminal paper reiter dale present a classical three stage nlg ture as shown in figure

the rst stage is document planning in which the content and its order are determined and a text plan that outlines the structure of messages is generated
the second is the micro planning stage in which referring expressions that identify objects like entities or places are generated along with the choice of words to be used and how they are aggregated
collating similar sentences to improve readability with a natural ow also occurs in this stage
the last stage is realization in which the actual text is generated using linguistic knowledge about morphology syntax semantics
earlier work has focused on modeling discourse structures and learning representations of relations between text units for text generation mckeown marcu ehud reiter s blog reiter
figure
the three stages of the traditional nlg process reiter dale
ono et al
stede umbach for example using rhetorical structure theory mann thompson or discourse representation theory lascarides asher
there is a large body of work that is based on template based models and have used statistical methods to improve generation by introducing new methods such as sentence compression reordering cal paraphrasing and syntactic transformation to name a few sporleder steinberger knight clarke lapata quirk et al

these earlier text generation approaches and their extensions play an important role in the lution of nlg research
the same is true for the nlg research in the last decade in which we witness a paradigm shift towards learning representations from large textual corpora in an vised manner using deep neural network dnn models
recent nlg models are built by training dnn models typically on very large corpora of human written texts
the paradigm shift starts with the use of recurrent neural networks graves e

long short term memory networks lstm hochreiter schmidhuber gated recurrent units grus cho et al

for learning language representations such as mikolov et al
and glove et al
and later sequence to sequence learning sutskever et al
which opens up a new chapter characterised by the wide application of the encoder decoder architecture
though sequence to sequence models were originally developed for machine translation they were soon shown to improve performance across many nlg tasks
these models weakness of capturing long span dependencies in long word sequences motivates the development of attention networks bahdanau et al
and pointer networks vinyals et al

the transformer architecture vaswani et al
which incorporates an encoder and a decoder both implemented using the self attention mechanism is being adopted by new state of the art nlg systems
there has been a large body of research in recent years that focuses on improving the performance of nlg using large scale pre trained language models for contextual word embeddings peters et al
devlin et al
sun et al
dong et al
using better sampling methods to reduce eration in decoding zellers et al
holtzman et al
and learning to generate text with better discourse structures and narrative ow yao et al
fan et al
dathathri et al
rashkin et al

neural models have been applied to many nlg tasks which we will discuss in this paper including summarization common tasks include single or multi document tasks query focused or generic summarization and summarization of news meetings screen plays social blogs
machine translation or document level
dialog response generation goal oriented or chit chat dialogs
paraphrasing question generation long text generation most common tasks are story news or poem generation
data to text generation

table summarization
caption generation from non text input input can be tables images or sequences of video frames e

in visual storytelling to name a few

why a survey on evaluation on natural language generation the question we are interested in in this paper is how to measure the quality of text generated from nlg models
text generation is a key component of language translation chatbots question answering marization and several other applications that people interact with everyday
building language models using traditional approaches is a complicated task that needs to take into account multiple aspects of language including linguistic structure grammar word usage and perception and thus requires non trivial data labeling efforts
recently transformer based neural language models have shown very effective in leveraging large amounts of raw text corpora from online sources such as wikipedia search results blogs reddit posts

for example one of most advanced neural language models radford et al
can generate long texts that are almost able from human generated texts zellers et al

empathetic social chatbots such as xiaoice zhou et al
seem to understand human dialog well and can generate interpersonal responses to establish long term emotional connections with users
nevertheless training a powerful language model relies on evaluation metrics that can measure the model quality from different perspectives
for instance it is imperative to build evaluation methods that can determine whether a text is generated by a human or a machine to prevent any potential harm
similarly evaluating the generated text based on factual consistency has recently drawn tention in the nlg eld
it is concerning that neural language models can generate open ended texts that are uent but not grounded in real world knowledge or facts such as fake news
the situation is particularly alarming if the generated reports or news are related to the well being of humankind such as summaries of health reports zhang et al

thus in addition to mainstream nlg evaluation methods our survey also discusses recently proposed metrics to address human facing issues such as the metrics that evaluate the factual consistency of a generated summary or the pathy level of a chatbot s response
many nlg surveys have been published in the last few years gatt krahmer zhu et al
zhang et al

others survey specic nlg tasks or nlg models such as image captioning kilickaya et al
hossain et al
li et al
bai an machine translation dabre al
han wong wong kit summarization deriu et al
shi et al
question generation pan et al
extractive key phrase generation c ano bojar deep generative models pelsmaeker aziz kim et al
text to image synthesis agnese et al
and dialog response generation liu et al
novikova et al
deriu et al
dusek et al
gao et al
to name a few
there are only a few published papers that review evaluation methods for specic nlg tasks such as image captioning kilickaya et al
machine translation goutte online review generation garbacea et al
interactive systems hastie belz and conversational dialog systems deriu et al
and for human centric evaluations lee et al
amidei et al

the closest to our paper is the nlg survey paper of gkatzia mahamood which includes a chapter on nlg evaluation metrics
different from this work our survey is dedicated to nlg evaluation with a focus on the evaluation metrics developed recently for neural text generation systems and provides an in depth analysis of existing metrics to date
to the best of our knowledge our paper is the most extensive and up to date survey on nlg evaluation

outline of the survey we review nlg evaluation methods in three categories in chapters human centric evaluation
the most natural way to evaluate the quality of a text ator is to involve humans as judges
naive or expert subjects are asked to rate or compare texts generated by different nlg systems or to perform a turing test turing to distinguish machine generated texts from human generated texts
most human evaluations are task specic and thus need to be designed and implemented differently for the outputs of different tasks
for example the human evaluation for image captioning is different from one for text summarization
untrained automatic metrics
this category also known as automatic metrics is the most commonly used in the research community
these evaluation methods compare machine generated texts to human generated texts references from the same input data using metrics that do not require machine learning but are simply based on string overlap content overlap string distance or lexical diversity such as n gram match and distribution similarity
for most nlg tasks it is critical to select the right automatic metric that sures the aspects of the generated text that are consistent with the original design goals of the nlg system
machine learned metrics
these metrics are often based on machine learned models which are used to measure the similarity between two machine generated texts or between machine generated and human generated texts
these models can be viewed as digital judges that simulate human judges
we investigate the differences among these evaluations and shed light on the potential factors that contribute to these differences
in chapter we present two case studies of evaluation methods developed for two tasks automatic document summarization and long text generation e

story or review generation respectively
we choose these tasks because they have attracted a lot of attention in the nlg research community and the task specic evaluation metrics they used can be adopted for other nlg tasks
we then provide general guidelines in building evaluation metrics that correlate well with human judgements
lastly we conclude the paper with future research directions for nlg evaluation
chapter human centric evaluation methods whether a system is generating an answer to a user s query a justication for a classication model s decision or a short story the ultimate goal in nlg is to generate text that is valuable to people
for this reason human evaluations are typically viewed as the most important form of evaluation for nlg systems and are held as the gold standard when developing new automatic metrics
since automatic metrics still fall short of replicating human decisions reiter belz krahmer theune reiter many nlg papers include some form of human evaluation
for example hashimoto et al
report that out of generation papers published at present human evaluation results
while human evaluations give the best insight into how well a model performs in a task it is worth noting that human evaluations also pose several challenges
first human evaluations can be sive and time consuming to run especially for the tasks that require extensive domain expertise
while online crowd sourcing platforms such as amazon mechanical turk have enabled researchers to run experiments on a larger scale at a lower cost they come with their own problems such as maintaining quality control ipeirotis et al
mitra et al

furthermore even with a large group of annotators there are some dimensions of generated text that are not well suited to human evaluations such as diversity hashimoto et al

there is also a lack of consistency in how human evaluations are run which prevents researchers from reproducing experiments and ing results across systems
this inconsistency in evaluation methods is made worse by inconsistent reporting on methods details on how the human evaluations were run are often incomplete or vague
for example van der lee et al
nd that in a sample of nlg papers from acl and inlg only of papers report the number of participants in their human evaluations
in this chapter we describe common approaches researchers take when evaluating generated text using only human judgments grouped into intrinsic
evaluations belz
and extrinsic reiter
however there are other ways to incorporate human subjects into the evaluation process such as training models on human judgments which will be discussed in chapter

intrinsic evaluation an intrinsic evaluation asks people to evaluate the quality of generated text either overall or along some specic dimension e

uency coherence correctness

this is typically done by erating several samples of text from a model and asking human evaluators to score their quality
the simplest way to get this type of evaluation is to show the evaluators the generated texts one at a time and have them judge their quality individually
they are asked to vote whether the text is good or bad or to make more ne grained decisions by marking the quality along a likert or sliding scale see figure

however judgments in this format can be inconsistent and comparing these results is not straightforward amidei et al
nd that analysis on nlg evaluations in this format is often done incorrectly or with little justication for the chosen methods
to more directly compare a model s output against baselines model variants or human generated text intrinsic evaluations can also be performed by having people choose which of two generated likert scale question rankme style question figure
two different methods for obtaining intrinsic evaluations of text generated from a meaning representation
image source novikova et al

com rankme texts they prefer or more generally rank a set of generated texts
this comparative approach has been found to produce higher inter annotator agreement callison burch et al
in some cases
however while it captures models relative quality it does not give a sense of the absolute quality of the generated text
one way to address this is to use a method like rankme novikova et al
which adds magnitude estimation bard et al
to the ranking task asking evaluators to indicate how much better their chosen text is over the see figure

based approaches can become prohibitively costly by requiring lots of head to head comparisons or complex by requiring participants to rank long lists of output when there are many models to compare though there are methods to help in these cases
for example best worst scaling louviere et al
has been used in nlg tasks kiritchenko mohammad koncel kedziorski et al
to simplify comparative evaluations best worst scaling asks participants to choose the best and worst elements from a set of candidates a simpler task than fully ranking the set that still provides reliable results
almost all the text generation tasks today are evaluated with intrinsic human evaluations
machine translation is one of the text generation tasks in which intrinsic human evaluations have made a huge impact on the development of more reliable and accurate translation systems as automatic metrics are validated through correlation with human judgments
one metric that is most commonly used to judge translated output by humans is measuring its adequacy which is dened by the linguistic data consortium as how much of the meaning expressed in the gold standard translation or source is also expressed in the target translation

the annotators must be bilingual in both the source and target languages in order to judge whether the information is preserved across translation
another dimension of text quality commonly considered in machine translation is uency which measures the quality of the generated text only e

the target translated sentence without taking the source into account
it accounts for criteria such as grammar spelling choice of words and style
a ical scale used to measure uency is based on the question is the language in the output uent
fluency is also adopted in several text generation tasks including document summarization ilmaz et al
narayan et al
recipe generation bosselut et al
image captioning lan et al
video description generation park et al
and question generation du et al
to name a few
while uency and adequacy have become standard dimensions of human evaluation for machine translation not all text generation tasks have an established set of dimensions that researchers use
nevertheless there are several dimensions that are common in human evaluations for generated text
as with adequacy many of these dimensions focus on the contents of the generated text
factuality is important in tasks that require the generated text to accurately reect facts described in the context
for example in tasks like data to text generation or summarization the information in the output should not contradict the information in the input data table or news article
this is a challenge to many neural nlg models which are known to hallucinate information holtzman et al
welleck et al
maynez et al
nd that over of generated sentence summaries contained hallucinations a nding that held across several different modeling approaches
even if there is no explicit set of facts to adhere to researchers may want to know how well the generated text follows rules of commonsense or how logical it is
for generation tasks that involve extending a text researchers may ask evaluators to gauge the coherence or consistency of a text how well it ts the provided context
for example in story generation do the same characters appear throughout the generated text and do the sequence of actions make sense given the plot so far other dimensions focus not on what the generated text is saying but how it is being said
as with uency these dimensions can often be evaluated without showing evaluators any context
this can be something as basic as checking for simple language errors by asking evaluators to rate how grammatical the generated text is
it can also involve asking about the overall style formality or tone of the generated text which is particularly important in style transfer tasks or in multi task settings
hashimoto et al
ask evaluators about the typicality of generated text in other words how often do you expect to see text that looks like this these dimensions may also focus on how efciently the generated text communicates its point by asking evaluators how repetitive or redundant it is

ldc
upenn
edu docs
pdf note that while these dimensions are common they may be referred to by other names explained to evaluators in different terms or measured in different ways van der lee et al

more consistency in how user evaluations are run especially for well dened generation tasks would be useful for producing comparable results and for focused efforts for improving performance in a given generation task
one way to enforce this consistency is by handing over the task of human evaluation from the individual researchers to an evaluation platform usually run by people hosting a shared task or leaderboard
in this setting researchers submit their models or model outputs to the evaluation platform which organizes and runs all the human evaluations
for example chateval is an evaluation platform for open domain chatbots based on both human and automatic metrics sedoc et al
and turingadvice zellers et al
tests models language understanding capabilities by having people read and rate the models ability to generate advice
of course as with all leaderboards and evaluation platforms with uniformity and consistency come rigidity and the possibility of overtting to the wrong objectives
thus how to standardize human evaluations should take this into account
a person s goal when producing text can be nuanced and diverse and the ways of evaluating text should reect that

extrinsic evaluation an extrinsic evaluation has people evaluate a system s performance on the task for which it was designed
extrinsic evaluations are the most meaningful evaluation as they show how a system actually performs in a downstream task but they can also be expensive and difcult to run reiter belz
for this reason intrinsic evaluations are more common than extrinsic evaluations gkatzia mahamood van der lee et al
and have become increasingly so which van der lee et al
attribute to a recent shift in focus on nlg subtasks rather than full systems
extrinsic methods measure how successful the system is in a downstream task
this success can be measured from two different perspectives a user s success in a task and the system s success in fullling its purpose hastie belz
extrinsic methods that measure a user s success at a task look at what the user is able to take away from the system e

improved decision making higher comprehension accuracy
gkatzia mahamood
for example young which reiter belz point to as one of the rst examples of extrinsic evaluation of generated text evaluate automatically generated instructions by the number of mistakes subjects made when they followed them
system success extrinsic evaluations on the other hand measure an nlg system s ability to complete the task for which it has been designed
for example reiter et al
generate personalized smoking cessation letters and report how many recipients actually gave up smoking
extrinsic human evaluations are commonly used in evaluating the performance of dialog deriu et al
and have made an impact on the development of the dialog modeling systems
various approaches have been used to measure the system s performance when talking to people such as measuring the conversation length or asking people to rate the system
the feedback is collected by real users of the dialog system black et al
lamel et al
zhou et al
at the end of the conversation
the alexa follows a similar strategy by letting real users interact with operational systems and gathering the user feedback over a span of several months
however the most commonly used human evaluations of dialog systems is still via crowd sourcing platforms such as amazon mechanical turk amt serban et al
peng et al
li et al
zhou et al

jurccek et al
suggest that using enough crowd sourced users can yield a good quality metric which is also comparable to the human evaluations in which subjects interact with the system and evaluate afterwards

the evaluators for many nlg evaluation tasks no specic expertise is required of the evaluators other than a prociency in the language of the generated text
this is especially true when uency related aspects of the generated text are the focus of the evaluation
often the target audience of an nlg system is broad e

a summarization system may want to generate text for anyone who is interested in
amazon
com alexaprize reading news articles or a chatbot needs to carry a conversation with anyone who could access it
in these cases human evaluations benet from being performed on as wide a population as possible
typically evaluations in these settings are performed either in person or online
an in person ation could simply be performed by the authors or a group of evaluators recruited by the researchers to come to the lab and participate in the study
the benets of in person evaluation are that it is easier to train and interact with participants and that it is easier to get detailed feedback about the study and adapt it as needed
researchers also have more certainty and control over who is ipating in their study which is especially important when trying to work with a more targeted set of evaluators
however in person studies can also be expensive and time consuming to run
for these reasons in person evaluations tend to include fewer participants and the set of people in imity to the research group may not accurately reect the full set of potential users of the system
in person evaluations may also be more susceptible to response biases adjusting their decisions to match what they believe to be the researchers preferences or expectations nichols maner orne
to mitigate some of the drawbacks of in person studies online evaluations of generated texts have become increasingly popular
while researchers could independently recruit participants online to work on their tasks it is common to use crowdsourcing platforms that have their own users whom researchers can recruit to participate in their task either by paying them a fee e

amazon chanical or rewarding them by some other means e

which provides ticipants with personalized feedback or information based on their task results
these platforms allow researchers to perform large scale evaluations in a time efcient manner and they are usually less expensive or even free to run
they also allow researchers to reach a wider range of evaluators than they would be able to recruit in person e

more geographical diversity
however taining quality control online can be an issue ipeirotis et al
oppenheimer et al
and the demographics of the evaluators may be heavily skewed depending on user base of the platform difallah et al
reinecke gajos
furthermore there may be a disconnect between what evaluators online being paid to complete a task would want out of a nlg system and what the people who would be using the end product would want
not all nlg evaluation tasks can be performed by any subset of speakers of a given language
some tasks may not transfer well to platforms like amazon mechanical turk where workers are more customed to dealing with large batches of microtasks
specialized groups of evaluators can be useful when testing a system for a particular set of users as in extrinsic evaluation settings
researchers can recruit people who would be potential users of the system e

students for educational tools or doctors for bionlp systems
other cases that may require more specialized human evaluation are projects where evaluator expertise is important for the task or when the source texts or the generated texts consist of long documents or a collection of documents
consider the task of citation tion luu et al
given two scientic documents a and b the task is to generate a sentence in document a that appropriately cites document b
to rate the generated citations the evaluator must be able to read and understand two different scientic documents and have general expert knowledge about the style and conventions of academic writing
for these reasons luu et al
choose to run human evaluations with expert annotators in this case nlp researchers rather than regular crowdworkers

inter evaluator agreement while evaluators often undergo training to standardize their evaluations evaluating generated natural language will always include some degree of subjectivity
evaluators may disagree in their ratings and the level of disagreement can be a useful measure to researchers
high levels of inter evaluator agreement generally mean that the task is well dened and the differences in the generated text are consistently noticeable to evaluators while low agreement can indicate a poorly dened task or that there are not reliable differences in the generated text
nevertheless measures of inter evaluator agreement are not frequently included in nlg papers
only of the generation papers reviewed in amidei et al
include agreement analysis though on a positive note it was more common in the most recent papers they studied
when
mturk

labinthewild
ment measures are included agreement is usually low in generated text evaluation tasks lower than what is typically considered acceptable on most agreement scales amidei et al

however as amidei et al
point out given the richness and variety of natural language ing for the highest possible inter annotator agreement may not be the right choice when it comes to nlg evaluation
while there are many ways to capture the agreement between annotators banerjee et al
we highlight the most common approaches used in nlg evaluation
for an in depth look at annotator agreement measures in natural language processing refer to artstein poesio


percent agreement a simple way to measure agreement is to report the percent of cases in which the evaluators agree with each other
if you are evaluating a set of generated texts x by having people assign a score to each text xi then let ai be the agreement in the scores for xi where ai if the evaluators agree and ai if they do nt
then the percent agreement for the task is pa ai x so pa means the evaluators did not agree on their scores for any generated text while pa means they agreed on all of them
however while this is a common way people evaluate agreement in nlg evaluations amidei et al
it does not take into account the fact that the evaluators may agree purely by chance particularly in cases where the number of scoring categories are low or some scoring categories are much more likely than others artstein poesio
we need a more complex agreement measure to capture this


cohen s cohen s cohen is an agreement measure that can capture evaluator agreements that may happen by chance
in addition to pa we now consider pc the probability that the evaluators agree by chance
so for example if two evaluators and are scoring texts x with a score from the set s then pc would be the odds of them both scoring a text the same pc ss p s p s for cohen s p s ei is estimated using the frequency with which evaluator ei assigned each of the scores across the task
so for example if there are two scores and and assigns scores as and scores as and assigns and then pc
once we have both pa and pc cohen s can then be calculated as








fleiss as seen in equation
cohen s measures the agreement between two annotators but often many evaluators have scored the generated texts particularly in tasks that are run on crowdsourcing platforms
fleiss fleiss can measure agreement between multiple evaluators
this is done by still looking at how often pairs of evaluators agree but now considering all possible pairs are other related agreement measures e

scott s scott that only differ from cohen s in how to estimate p
these are well described in artstein poesio but we do not discuss these here as they are not commonly used for nlg evaluations amidei et al

pa pc pc of evaluators
so now ai which we dened earlier to be the agreement in the scores for a generated text xi is calculated across all evaluator pairs ai ss of evaluator pairs who score xi as s total of evaluator pairs then we can once again dene pa the overall agreement probability as it is dened in equation
the average agreement across all the texts
to calculate pc we estimate the probability of a judgment p s ei by the frequency of the score across all annotators and assuming each annotator is equally likely to draw randomly from this distribution
so if rs is the proportion of judgments that assigned a score s then the likelihood of two annotators assigning score s by chance is rs s
then our overall probability of chance agreement is rs pc ss s with these values for pa and pc we can use equation
to calculate fleiss


krippendorff s each of the above measures treats all evaluator disagreements as equally bad but in some cases we may wish to penalize some disagreements more harshly than others
krippendorff s pendorff which is technically a measure of evaluator disagreement rather than agreement allows different levels of disagreement to be taken into account
like the measures above we again use the frequency of evaluator agreements and the odds of them agreeing by chance
however we will now state everything in terms of disagreement
first we nd the probability of disagreement across all the different possible score pairs sm sn which are weighted by whatever value wm n we assign the pair
so pd wm n of evaluator pairs that assign xi as sm sn total of evaluator pairs
note that when m n i
e
the pair of annotators agree wm n should be
next to calculate the expected disagreement we make a similar assumption as in fleiss the random likelihood of an evaluator assigning a score si can be estimated from the overall frequency of si
if rm n is the proportion of all evaluation pairs that assign scores sm and sn then we can treat it as the probability of two evaluators assigning scores sm and sn to a generated text at random
so pc is now finally we can calculate krippendorff s as



that there are other measures that permit evaluator disagreements to be weighted differently
for example weighted cohen extends cohen s by adding weights to each possible pair of score ments
in nlg evaluation though krippendorff s is the most common of these weighted measures in the set of nlg papers surveyed in amidei et al
only used weighted
pc wm nrm n pd pc chapter untrained automatic evaluation metrics with the increase of the numbers of nlg applications and their benchmark datasets evaluation of nlg systems has become increasingly important
today the best evaluation for automatic nlg system output is human based evaluation
however human evaluation is costly and time consuming to design and run and more importantly the results are not always repeatable belz reiter
thus automatic evaluation metrics are employed as an alternative in both developing new models and comparing them against state of the art
in this survey we group automatic metrics into two categories untrained automatic metrics that do not require training this chapter and learned evaluation metrics that are based on machine learned models chapter
in this chapter we review untrained automatic metrics used in different nlg applications and discuss their advantages and drawbacks in comparison with other approaches
untrained automatic metrics for nlg evaluation are used to measure the effectiveness of the models that generate text such as in machine translation image captioning or question generation
these metrics compute a score that indicates the similarity or dissimilarity between an automatically generated text and written reference gold standard text
untrained automatic evaluation metrics are fast and efcient and are widely used to quantify day to day progress of model development e

comparing model training with different hyperparameters
we group the untrained automatic evaluation methods as in table
into ve categories gram overlap metrics distance based metrics diversity metrics content overlap metrics grammatical feature based metrics
n gram overlap metrics for content selection n gram overlap metrics are commonly used for evaluating nlg systems and measure the degree of matching between machine generated and human authored ground truth texts
in this section we present several n gram match features and the nlg tasks they are used to evaluate
p a l r e v o m a r g n metric bleu nist f score wer rouge meteor hlepor ribes cider edit dist
ter wmd smd p pyramid a l spice r e v spider o e s a e c n a t s i d t n e t n o c property n gram precision n gram precision precision and recall of insert delete replace n gram recall n gram synonym matching unigrams harmonic mean unigrams harmonic mean tf idf weighted n gram similarity cosine similarity translation edit rate earth mover distance on words earth mover distance on sentences mt scene graph similarity scene graph similarity ic sr sum dg qg rg table
untrained automatic and retrieval based metrics based on string match string distance or context overlap
the acronyms for some of the nlp sub research elds that each metric is commonly used to evaluate text generation are mt machine translation qg question tion sum summarization rg dialog response generation dg document or story generation visual story generation
ic image captioning


f score the f score also called the score or f measure is a measure of accuracy
the f score ances the generated text s precision and recall by measuring the harmonic mean of the two measures
f score is dened as precision recall precision recall
precision specicity also called the positive predictive value is the fraction of n grams in the model generated hypothesis text that are present in the reference human or gold text
recall also called sensitivity is the fraction of the n grams in the reference text that are present in the candidate text
the f score reaches the best value indicating perfect precision and recall at a value of
the worst f score which means lowest precision and lowest recall would be a value of
in text generation tasks such as machine translation or summarization f score gives an tion as to the quality of the generated sequence that a model will produce melamed et al
aliguliyev
specically for machine translation f score based metrics have been shown to be effective in evaluating translation quality
one of these metrics is the chrf character n gram f score which uses character n grams instead of word n grams to compare the machine translation model output with the reference translations popovic
they use character n grams because it helps to better match the morphological variations in words
in recent work by mathur et al
it was empirically shown that chrf has high correlation with human judgments compared to commonly used n gram based evaluation metrics


bleu the bilingual evaluation understudy bleu is one of the rst metrics used to measure the larity between two sentences papineni et al

originally proposed for machine translation it compares a candidate translation of text to one or more reference translations
bleu is a weighted geometric mean of n gram precision scores dened as precn s y y s y
where y is the hypothesis sequence y is the ground truth sequence s is an n gram sequence of y and y is the number of times s appears in y
the bleu score is then is the brevity penalty to penalize sequences that are too short and often calculated as bp bleu exp wn log precn bp n bp c t t t t

where t and t are the prediction and gold sequence length is the length of the candidate generated sequence and r is the effective reference corpus length
n is the total number of n gram precision scores to use and wn is the weight for each precision score which is often set to be n
it has been argued that although bleu has signicant advantages it may not be the ultimate sure for improved machine translation quality callison burch osborne
earlier work has reported that bleu correlates well with human judgments lee przybocki denoual age
more recent work argues that although it can be a good metric for the machine translation task zhang et al
for which it is designed it does nt correlate well with human judgments for other generation tasks outside of machine translation such as image captioning or dialog response generation
reiter report that there s not good evidence to support that bleu is the best metric for evaluating nlg systems other than machine translation
in caccia et al
it was empirically demonstrated that when used to sample from the model outputs that has perfect bleu with the corpus the generated sentences were grammatically correct but lacked semantic global coherence concluding that the generated text has poor information content
outside of machine translation bleu has been used for other text generation tasks such as ment summarization graham image captioning vinyals et al
human machine versation gao et al
and language generation semeniuta et al

in graham it was concluded that bleu achieves strongest correlation with human assessment but does not nicantly outperform the best performing rouge variant
on the other hand a more recent study has demonstrated that n gram matching scores such as bleu can be insufcient and potentially less accurate metric for unsupervised language generation semeniuta et al

text generation research especially when focused on short text generation like sentence based chine translation or question generation has successfully used bleu for benchmark analysis with models since it is fast easy to calculate and enables a comparison with other models on the same task
however bleu has some drawbacks for nlg tasks where contextual understanding and soning is the key e

story generation fan et al
martin et al
or long form question answering fan et al

it considers neither semantic meaning nor sentence structure
it does not handle morphologically rich languages well nor does it map well to human judgments tatman
recent work by mathur et al
investigated how sensitive the machine translation evaluation metrics are to outliers
they found that when there are outliers in tasks like machine translation metrics like bleu lead to high correlations yielding false conclusions about reliability of these metrics
they report that when the outliers are removed these metrics do not correlate as well as before which adds evidence to the unreliablity of bleu
we will present other metrics that address some of these shortcomings throughout this paper


nist proposed by the us national institute of standards and technology nist martin przybocki is a method similar to bleu for evaluating the quality of text
unlike bleu which treats each n gram equally nist heavily weights n grams that occur less frequently as co occurrences of these n grams are more informative than common n grams doddington
information weights are computed using n gram counts over the set of reference translations according to the following equation wn wn
where wi are n grams in the reference text and c nist score is indicates count
the formula for calculating the nist n


that co occur in hyp
sequence wn exp br log min lhyp lref
where lhyp is the average number of words in a hypothesis generated translation averaged over all reference translations and lref is the number of words in the translation being scored
different from bleu the brevity penalty is chosen to be
when the number of words in the reference output is two thirds of the average number of words in the reference translation
this change is made to minimize the impact on the score when there is a small variation in the translation length
the goal is to preserve the original motivation of using the brevity penalty while reducing the contributions of length variations on the score when there are small variations
it has shown that the nist metric is often superior to bleu in terms of reliability and quality doddington
even though this metric has several merits in evaluating machine translation it has not been adopted by recent neural nlg research as much as the bleu metric


rouge recall oriented understudy for gisting evaluation rouge lin is a set of metrics for evaluating automatic summarization of long texts consisting of multiple sentences or paragraphs
although mainly designed for evaluating or multi document summarization it has also been used for evaluating short text generation such as machine translation lin och image captioning cui et al
and question generation nema khapra dong et al

rouge includes a large number of distinct variants including eight different n gram counting ods to measure n gram overlap between the generated and the ground truth human written text
simplifying the notation in the original paper lin rouge n can be dened as rouge n n match r s count gramn r
where n sums over all n grams of length n e

if n the formula measures the number of times a matching bigram is found in the hypothesis model generated and the reference generated text
if there is more than one reference summary the outer summation r repeats the process over all reference summaries
we explain commonly used rouge metrics lin below measures the overlap of unigrams single tokens between the reference and hypothesis text e
g
summaries
measures the overlap of bigrams between the reference and hypothesis text
measures the overlap of trigrams between the reference and hypothesis text
measures the overlap of four grams between the reference and hypothesis text
rouge l measures the longest matching sequence of words using longest common sequence lcs
this metric does nt require consecutive matches but it requires sequence matches that indicate sentence level word order
the n gram length does not need to be predened since rouge l automatically includes the longest common n grams shared by the reference and hypothesis text
rouge w less commonly used measures weighted lcs based statistics that favor secutive lcss
rouge s less commonly used measures skip based co occurrence statistics
any pair of skip words in the sentence order is considered a skip bigram
skip gram huang et al
is a type of n gram in which tokens e

words do nt need to be consecutive but in order in the sentence where there can be gaps between the tokens that are skipped over
in nlp research they are used to overcome data sparsity issues
rouge su less commonly used measures skip bigram and unigram based co occurrence statistics
rouge also includes a setting for word stemming of summaries and an option to remove or retain stop words
additional congurations include the use of precision rouge p recall rouge r or f score rouge f to compute individual summary scores
it also provides options for computation of the overall score for a system by computing the mean or median of the generated hypothesis text score distribution which is not found in bleu scores
in total rouge can provide possible system level measure variants
compared to bleu rouge focuses on recall rather than precision and is more interpretable than bleu callison burch osborne
additionally rouge includes the mean or median score from individual output text which allows for a signicance test of differences in system level rouge scores while this is restricted in bleu graham baldwin graham
rouge evaluates the adequacy of the generated output text by counting how many n grams in the generated output text matches the n grams in the reference human generated output text
this is ered a bottleneck of this measure especially for long text generation tasks kilickaya et al
because it does nt provide information about the narrative ow grammar or topical ow of the erated text nor does it evaluate the factual correctness of the summary compared to the corpus it is generated from


meteor the metric for evaluation of translation with explicit ordering meteor lavie et al
banerjee lavie is a metric designed to address some of the issues found in bleu and has been widely used for evaluating machine translation models and other models such as image captioning kilickaya et al
question generation nema khapra du et al
and summarization see et al
chen bansal yan et al

compared to bleu which only measures the precision meteor is based on the harmonic mean of the unigram precision and recall in which recall is weighted higher than precision
several metrics support this property since it yields high correlation with human judgments lavie agarwal
the meteor score between a reference and hypothesis text is measured as follows
let resent the unigrams found between the hypothesis and reference text be the total number of unigrams in the hypothesis text and be the total number of unigrams in the reference text r
the mean f score is computed using the unigram recall and precision fmean precision precision recall recall then the alignment between the hypothesis and reference is calculated as follows meteor fmean penalty the penalty term which is called the fragmentation penalty determines the extent to which the matched unigrams in both hypothesis and reference are well ordered and is measured as follows penalty frag in the above the sequence of matched unigrams between the two texts is divided into the fewest possible number of chunks such that the matched unigrams in each chunk are adjacent and identical in word order
the number of chunks ch and the number of matches m are then used to calculate a fragmentation fraction rag ch m
determines the maximum penalty and determines the functional relation between the fragmentation and the penalty
meteor scores range between and
meteor has several variants that extend exact word matching that most of the metrics in this gory do not include such as stemming and synonym matching
these variants address the problem of reference translation variability allowing for morphological variants and synonyms to be ognized as valid translations
the metric has been found to produce good correlation with human


judgments at the sentence or segment level agarwal lavie
this differs from bleu in that meteor is explicitly designed to compare at the sentence level rather than the level
harmonic mean of enhanced length penalty precision n gram position difference penalty and call hlepor initially proposed for machine translation is a metric designed for morphologically complex languages like turkish or czech han et al

among other factors hlepor utilizes part of speech noun verb
tags similarity to capture syntactic information


hlepor

ribes rank based intuitive bilingual evaluation score ribes isozaki et al
is another un trained automatic evaluation metric for machine translation
it was developed by ntt communication ence labs and designed to be more informative for asian languageslike japanese and chinese since it does nt rely on word boundaries
specically ribes is based on how the words in generated text are ordered
it uses the rank correlation coefcients measured based on the word order from the hypothesis model generated translation and the reference translation
some of the correlation efcients used in ribes are spearman s which is based on the distance of difference in the ranks or kendall s which is based on the direction of the difference in rank
however earlier work on evaluating the correlation of automatic metrics with human judgments has shown that ribes tends to show lower correlation with human evaluation scores indicating that higher ribes does nt necessary yield better translations tan et al



cider consensus based image description evaluation cider is an automatic metric for measuring the similarity of a generated sentence against a set of human written sentences using a consensus based protocol
originally proposed for image captioning vedantam et al
cider shows high agreement with consensus as assessed by humans
it enables a comparison of text generation els based on their human likeness without having to create arbitrary calls on weighing content grammar saliency
with respect to each other
the cider metric presents three explanations about what a hypothesis sentence should contain
n grams in the hypothesis sentence should also occur in the reference sentences

if an n gram does not occur in a reference sentence it should nt be in the hypothesis tence

n grams that commonly occur across all image caption pairs in the dataset should be signed lower weights since they are potentially less informative
given these intuitions a term frequency inverse document frequency tf idf weight is lated for each n gram
specically given an image i and a list of the reference descriptive sentences about the image sij where and represent the numbers of times an n gram wk occurs in a reference sentence sij and a hypothesis model generated sentence ci respectively
the tf idf score is calculated as follows


sim si wl log i ipi q where i is the set of all images and the vocabulary of all n grams
then the cidern score for a particular n gram is calculated as the cosine similarity between the generated candidate sentence and the reference sentence as

si m j where is a vector of containing all n grams of length n and measures the magnitude of the vector
to capture richer semantics and grammatical properties cider can also use higher order longer n grams as si si
n vedantam et al
nd that uniform weights wn n work the best where n
a recent study kilickaya et al
shows that among all the untrained automatic metrics for image captioning evaluation cider is the most robust and correlates the best with human judgments

distance based evaluation metrics for content selection a distance based metric in nlg applications uses a distance function to measure the similarity between two text units e

words sentences
first we represent two text units using vectors
then we compute the distance between the vectors
the smaller the distance the more similar the two text units are
this section reviews distance based similarity measures where text vectors can be constructed using discrete tokens such as bag of words


we note that even though the embeddings that are used by these metrics to represent the text vectors are pre trained these metrics are not trained to mimic the human judgments as in machine learned metrics that we summarize in chapter


or embedding vectors

edit distance based metrics edit distance one of the most commonly used evaluation metrics in natural language processing measures how dissimilar two text units are based on the minimum number of operations required to transform one text into the other
we summarize some of the well known edit distance measures below
wer word error rate wer originally designed for measuring the performance of speech nition systems is also commonly used to evaluate the quality of machine translation systems tomas et al

specically wer is the percentage of words that need to be inserted deleted or placed in the translated sentence to obtain the reference sentence i
e
the edit distance between the reference and hypothesis sentences
it is calculated as wer substitutions insertions deletions reference sentence length
where a substitution replaces one word with another an insertion adds one new word and a deletion drops one word
the main drawback of wer is its dependency on the reference sentences
in machine translation there may exist multiple correct translations for the same input
but this metric only considers one to be correct
other variations of wer such as sentence error rate ser measure the percentage of sentences whose translations do not exactly match the reference sequence
multi reference word error rate mwer ali et al
calculates the edit distance between several references for each sentence and chooses the smallest one nieen et al

one drawback of this approach is that it requires human effort to obtain multiple references but it has been found to be an effective measure
reference word error rate awer tomas et al
measures the number of words to be inserted deleted or replaced in the sentence under evaluation in order to obtain a correct translation
awer can be considered a version of mwer which takes into account all possible references not just one reference as in wer
wer has some limitations
for instance while its value is lower bounded by zero which indicates a perfect match between the hypothesis and reference text its value is not upper bounded making it hard to evaluate in an absolute manner mccowan et al

it is also reported to suffer from weak correlation with human evaluation
for example in the task of spoken document retrieval the wer of an automatic speech recognition system is reported to poorly correlate with the retrieval system performance kae huenerfauth
med the minimum edit distance med between two text strings is the minimum number of ing operations i
e
insertion deletion and substitutions required to transform one string into the other
for two strings and y of length n and m respectively we dene a distance metric j which will be the edit distance between j i
e
the rst j characters of string y
now the distance between the entire two strings and y will be m
the med can be applied to sentences or longer text using words as units rather than characters
in machine translation med is the minimum number of insertions deletions and substitutions of words that are required in order to make a system translation equivalent in meaning to that of a reference translation
both wer and med are based on levenshtein distance
while med is used mainly to measure two text strings wer is for both text and speech
i i
e
the rst i characters of string and ter translation edit rate ter snover et al
is dened as the minimum number of edits needed to change a generated text so that it exactly matches one of the references normalized by the average length of the references
in terms of minimum number of edits ter measures the number of edits to the closest reference as ter number of edits average number of reference words
ter considers the insertion deletion and substitution of single words and shifts of words as possible edits
the word shifting moves a contiguous sequence of words within the hypothesis to another location within the hypothesis
the metric assigns all edits an equal cost
while ter has been shown to correlate well with human judgments in evaluating machine lation quality it suffers from some limitations
for example it can only capture similarity in a narrow sense as it only uses a single reference translation and considers only exact word matches between the hypothesis and the reference
this issue can be partly addressed by constructing a lattice of reference translations a technique that has been used to combine the output of multiple translation systems rosti et al

many variants have been proposed to improve the original ter
uses phrasal substitutions using automatically generated paraphrases stemming onyms relaxed shifting constraints and other improvements
iter panja naskar adds stem matching and normalization on top of ter
cder leusch et al
models block ordering as an edit operation
per tillmann et al
computes position independent error rate
two recent variants character wang et al
a character based translation edit distance measure and eed stanchev et al
an extension of levenshtein distance have shown to correlate better with human judgments on some languages


vector similarity based evaluation metrics in nlp embedding based similarity measures are commonly used in addition to n gram based ilarity metrics
embeddings are real valued vector representations of character or lexical units such as word tokens or n grams that allow tokens with similar meanings to have similar representations
even though the embedding vectors are learned using supervised or unsupervised neural network models the vector similarity metrics we summarize below assume the embeddings are pre trained and simply used as input to calculate the metric
meant
the vector based similarity measure meant uses word embeddings and shallow mantic parses to compute lexical and structural similarity lo
it evaluates translation quacy by measuring the similarity of the semantic frames and their role llers between the human references and the machine translations
inspired by the meant score lo is proposed to evaluate the accuracy of yisi chine translation model outputs
it is based on the weighted distributional lexical semantic similarity as well as shallow semantic structures
specically it extracts the longest common character string from the hypothesis and reference translations to measure the lexical similarity
terp is named after the university of maryland mascot the terrapin
is the romanization of the cantonese word which translates as meaning in english
word mover s distance wmd earth mover s distance emd also known as the wasserstein metric rubner et al
is a measure of the distance between two probability distributions
word mover s distance wmd kusner et al
is a discrete version of emd that calculates the distance between two sequences e

sentences paragraphs
each represented with relative word frequencies
it combines item on bag of word bow histogram representations of text goldberg et al
with word embedding similarity
in short wmd has several intriguing properties it is hyperparameter free and easy to use
it is highly interpretable as the distance between two documents can be broken down and explained as the sparse distances between few individual words
it uses the knowledge encoded within the word embedding space which leads to high retrieval accuracy
for any two documents a and b we dene the wmd as the minimum cost of transforming one document into the other
each document is represented by the relative frequencies of the words it contains i
e
for the ith word type in equation
a is the total word count of document a and db i is dened in the same way
da i a
rm i
e
an m length we dene distances between representing the ith word by vi the ith and jth words as j
v is the vocabulary size
kusner et al
use the euclidean distance j vj
vi figure
left illustration of word mover s distance wmd
picture source kusner et al
right illustration of sentence mover s distance smd
picture source clark et al

the wmd is then calculated by nding the solution to the linear program b min j s
t



rv v is a non negative matrix where each j denotes how much of word i across all its t tokens in a is assigned to word j in b and the constraints ensure the ow of a given word can not similarity can be dened as cosine jaccard euclidean
could use pre trained type based or contextual word embeddings
i j da i j db j exceed its weight
specically wmd ensures that the entire outgoing ow from word i equals da i i
e
j da i
additionally the amount of incoming ow to word j must match db i
e
j i j db j
empirically wmd has been instrumental to the improvement of many nlg tasks specically sentence level tasks such as image caption generation kilickaya et al
and natural language inference sulea
however while wmd works well for short texts its cost grows prohibitively as the length of the documents increases and the bow approach can be problematic when ments become large as the relation between sentences is lost
by only measuring word distances the metric can not capture information conveyed in the group of words for which we need higher level document representations dai et al
wu et al

sentence mover s distance smd sentence mover s distance smd is an automatic metric based on wmd to evaluate text in a continuous space using sentence embeddings clark et al
zhao et al

smd has been used to compare the generated texts to reference texts in tasks like machine translation and summarization and is found to be correlated with human evaluation
smd represents each document as a collection of sentences or of both words and sentences as seen in figure
where each sentence embedding is weighted according to its length
the bag of words and sentences representing document a is normalized by a so that da i a i a if i is a word if i is a sentence
like wmd smd also tries to solve the same linear program in eq


unlike wmd smd measures the cumulative distance of moving both the words in a document and the sentences to match another document
the vocabulary is dened as a set of sentences and words in the documents
on a summarization task smd is found to correlate better with human judgments than rouge clark et al

recently zhao et al
propose a new version of smd that attains higher correlation with human judgments
similar to smd they use word and sentence embeddings by taking the average of the token based embeddings before the mover s distance is calculated
they also investigate different contextual embeddings models including elmo and bert by taking power mean which is an embedding aggregation method of their embeddings at each layer of the encoding model
frechet inception distance extending inception score heusel et al
propose a new metric called frechet inception distance fid to score the similarity between generated images and real ones
it measures the distance between two multivariate gaussians f id r g t r
r g r g g r where the samples xr g are the hidden layer activations of the inception for real and generated samples respectively
the authors assume that the features extracted by a classier are normally distributed
semeniuta et al
adapt fid to nlg uation by using infersent text embedding model conneau et al
to compute the sentence embeddings
infersent is a supervised model of bidirectional lstm with max pooling
r and xg n n
n gram based diversity metrics the lexical diversity score measures the breadth and variety of the word usage in writing inspector
consider two pieces of texts about in class teaching
the rst repeatedly uses the same words such as teacher reads and asks
the second one avoids repetition by using different words or expressions
g lecturer instructor delivers teaches questions explains
the second text is more lexically diverse which is more desirable in many nlg tasks such as conversational bots li et al
story generation rashkin et al
question generation du et al
pan et al
and abstractive question answering fan et al

in this section we review some of the metrics designed to measure the quality of the generated text in terms of lexical diversity


type token ratio ttr type token ratio ttr is a measure of lexical diversity richards mostly used in linguistics to determine the richness of a writer s or speaker s vocabulary
it is computed as the number of unique words types divided by the total number of words tokens in a given segment of language distinct tokens total tokens
although intuitive and easy to use ttr has a major problem it is sensitive to text length
the longer the document the lower the prospect that a replacement token will be a new type
this eventually causes the ttr to drop as more words are added
to remedy this issue several other lexical diversity measures have been proposed and we discuss them below
measuring diversity using n gram repetitions is a more generalized version of ttr which has been use for text generation evaluation
li et al
has shown that modeling mutual information between source and targets signicantly decreases the chance of generating bland responses and improves the diversity of responses
they use bleu and distinct word unigram and bigram counts to evaluate the proposed diversity promoting objective function for dialog response generation


self bleu zhu et al
propose self bleu as a diversity evaluation metric by measuring the differences between generated sentences and references or other generated texts
in a sense it is the opposite of bleu which assesses how similar two sentences are
taking a generated sentence to be evaluated as the hypothesis and the other sentences as references self bleu calculates a bleu score for every generated sentence and denes the average of these bleu scores as the self bleu score of the to be evaluated text
a lower self bleu score implies higher diversity
several nlg papers have reported that self bleu achieves good generation diversity zhu et al
chen et al
lu et al

however others have reported some weakness of the metric in generating diverse output caccia et al
or detecting mode collapse semeniuta et al
in text generation with gan goodfellow et al
models
even though self bleu is mainly used for evaluating the diversity of generated sentences people are exploring a better evaluation metric that evaluates both quality and diversity montahaei et al



measure of textual lexical diversity as we noted earlier in this chapter the ttr metric is sensitive to the length of the text
to remedy this a new diversity metric hd d hyper geometric distribution function is proposed to compare texts of different lengths mccarthy jarvis
mccarthy jarvis argue that the probabilities of word occurrence can be modeled using the hyper geometric distribution hd
the hd is a discrete probability distribution that expresses the probability of k successes after drawing n items from a nite population of size n containing m successes without replacement
hd is used to measure lexical diversity entitled hd d
d assumes that if a text sample consists of many tokens of a specic word then there is a high probability of drawing a text sample that contains at least one token of that word
this measure does not require a minimum tokens to be estimated
the hd d and its variants mccarthy jarvis have been used to measure the diversity in story generation mccarthy jarvis and summarization tasks crossley et al


explicit semantic content match metrics semantic content matching metrics dene the similarity between human written and generated text by extracting explicit semantic information units from text beyond n grams
these metrics operate on semantic and conceptual levels and are shown to correlate well with human judgments
we summarize some of them below


pyramid the pyramid method is a semi automatic evaluation method nenkova passonneau for evaluating the performance of document summarization models
like other untrained automatic metrics that require references this untrained metric also requires human annotations
it identies summarization content units scus to compare information in a human generated reference mary to the model generated summary
to create a pyramid annotators begin with model generated summaries of the same source texts and select sets of text spans that express the same meaning across summaries
each set is referred to as a scu and receives a label for mnemonic purposes
an scu has a weight corresponding to the number of summaries that express the scu s meaning
scus are extracted from a corpus of summaries by annotators and are not longer than a clause see table

the annotation starts with identifying similar sentences and then proceeds with more ne grained inspection that identies related sub parts
the scus that appear in human generated summaries more often get higher weights
so a pyramid is formed after the scu annotation of human generated summaries
the scus that appear in most of the summaries appear at the top of the pyramid and get the greatest weights
the lower in the pyramid an scu appears the lower its weight is because it occurs in fewer summaries
the scus in peer summary are then checked against an existing pyramid to measure how much information agrees between the model generated and human generated summaries
scu the cause of an airline crash over nova scotia has not been determined a
the cause of the sept
crash has not been determined

searched for clues as to a but refrained from naming one

the cause has not been determined
the specic cause of the tragedy was never determined e
but investigators remain unsure of its cause

a nal determination of the crashes cause is still far off
table
overlay of matching scus from two annotators and from summaries a through
boldface indicates text selected by both annotators
text spans in italics are labeled or to indicate which annotator selected them
table source passonneau the pyramid metric relies on manual human labeling effort which makes it difcult to automate
in a recent study the peak pyramid evaluation via automated knowledge extraction yang et al
is presented as a fully automated variant of pyramid model which can automatically assign the pyramid weights
first the peak identies relation triples subject predicate object triples using open information extraction del corro gemulla
then these triplets are combined into a hypergraph based on a semantic similarity approach in which the nodes become the triplets
salient nodes on the graph which are later assigned as potential scus are determined based on novel similarity metrics dened on the graph
the peak metric not only generates a pyramid entirely automatically but also is shown to correlate well with human judgments yang et al



spice sm semantic propositional image caption evaluation spice anderson et al
is an image tioning metric that measures the similarity between a list of reference human written captions of an image and a hypothesis caption c generated by a model
instead of s rectly comparing a generated caption to a set of references in terms of syntactic agreement spice parses each reference to derive an abstract scene graph representation
the generated caption is also parsed and compared to the scene graph to capture the semantic similarity
spice has shown to have a strong correlation with human ratings
a scene graph schuster et al
encodes objects attributes and relationships detected in image captions representing an image in a skeleton form as shown in figure

a two stage process is typically used to parse an image caption into a scene graph schuster et al
lin et al

first syntactic dependencies between words in the generated and reference captions are extracted using a dependency parser klein manning
second the extracted dependency tree is figure
illustration of scene graph extraction for measuring the spice metric
a scene graph right is parsed from a set of reference image captions on the left
picture source anderson et al

mapped to a scene graph using a rule based system schuster et al

spice then computes the f score using the hypothesis and reference scene graphs over the conjunction of logical tuples representing semantic propositions in the scene graph
the semantic relations in a scene graph g are represented as a conjunction of logical propositions namely tuples t dened as t each tuple contains up to three elements indicating objects o relations r and attributes a
a then returns the matching tuples in two scene graphs
the spice metric binary matching operator is dened as s s p c s s p c s s where p c s and s are the precision and recall
as discussed in the previous subsections most evaluation metrics are based on n gram matching such as bleu and meteor
however a higher number of matched n grams does nt always cate a higher generation quality because two sentences with a lot of words in common can be very different semantically
in comparison spice is more desirable because it measures the semantic similarity between a hypothesis and a reference text using scene graphs
even though spice lates well with human evaluations a major drawback is that it ignores the uency of the generated captions sharif et al





spider liu et al
propose spider which is a linear combination of spice and cider
they show that optimizing spice alone often results in captions that are wordy and repetitive although scene graph similarity is good at measuring the semantic similarity between captions it does not take into account the syntactical aspects of texts
thus a combination of semantic graph similarity like spice and gram similarity measure like cider yields a more complete quality evaluation metric
however the correlation of spider and human evaluation is not reported


semantic similarity models used as evaluation metrics other text generation work has used the condence scores obtained from semantic similarity ods as an evaluation metric
such models can evaluate a reference and a hypothesis text based on their task level semantics
the most commonly used methods based on the sentence level similarity are as follows semantic textual similarity sts is concerned with the degree of equivalence in the underlying semantics of paired text agirre et al

the sts is used as an evaluation metric in text generation tasks such as machine translation summarization and dialogue sponse generation in conversational systems
the ofcial score is based on weighted son correlation between predicted similarity and human annotated similarity
the higher the score the better the the similarity prediction result from the algorithm maharjan et al
cer et al

paraphrase identication pi considers if two sentences express the same meaning dolan brockett barzilay lee
pi is used as a text generation ation score based on the textual similarity kauchak barzilay of reference and hypothesis by nding a paraphrase of the reference sentence that is closer in wording to the hypothesis output
for instance given the pair of sentences reference however israel s reply failed to completely clear the u
s
suspicions
hypothesis however israeli answer unable to fully remove the doubts
pi is concerned with learning to transform the reference sentence into paraphrase however israel s answer failed to completely remove the u
s
suspicions
which is closer in wording to the hypothesis
in jiang et al
a new paraphrasing evaluation metric tiger is used for image caption generation evaluation
similarly sidering image captioning liu et al
introduce different strategies to select useful visual paraphrase pairs for training by designing a variety of scoring functions
textual entailment te is concerned with whether a hypothesis can be inferred from a premise requiring understanding of the semantic similarity between the hypothesis and the premise dagan et al
bowman et al

it has been used to evaluate several text generation tasks including machine translation pado et al
document rization long et al
language modeling liu et al
and video captioning pasunuru bansal
machine comprehension mc is concerned with the sentence matching between a sage and a question pointing out the text region that contains the answer rajpurkar et al

mc has been used for tasks like improving question generation yuan et al
du et al
and document summarization hermann et al


syntactic similarity based metrics a syntactic similarity metric captures the similarity between a reference and a hypothesis text at a structural level to capture the overall grammatical or sentence structure similarity
in corpus linguistics part of speech pos tagging is the process of assigning a part of speech tag e

verb noun adjective adverb and preposition
to each word in a sentence based on its context morphological behaviour and syntax
pos tags have been commonly used in machine translation evaluation to evaluate the quality of the generated translations
while tesla dahlmeier et al
is introduced as an evaluation metric to combine the synonyms of bilingual phrase tables and pos tags others use pos n grams together with a combination of morphemes and lexicon abilities to compare the target and source translations popovic et al
han et al

pos tag information has been used for other text generation tasks such as story generation agirrezabal et al
summarization suneetha fatima and question generation zerr
syntactic analysis studies the arrangement of words and phrases in well formed sentences
for ample a dependency parser extracts a dependency tree of a sentence to represent its grammatical structure
several text generation tasks have enriched their evaluation criteria by leveraging tic analysis
in machine translation liu gildea use constituent labels and head modier dependencies to extract structural information from sentences for evaluation while others use low parsers lo et al
or a dependency parser yu et al

yoshida et al
combine a sequential decoder with a tree based decoder in a neural architecture for abstractive text summarization
chapter machine learned evaluation metrics many of the untrained evaluation metrics described in chapter assume that the generated text has signicant word or gram overlap with the ground truth text
however this assumption does not hold for many nlg tasks such as a social chatbot which permit signicant diversity and allow multiple plausible outputs for a given input
table
shows two examples from the dialog response generation and image captioning tasks respectively
in both tasks the model generated outputs are plausible given the input but they do not share any words with the ground truth output
one solution to this problem is to use embedding based metrics which measure semantic ity rather than word overlap as in section


but embedding based methods can not help in situations when the generated output is semantically different from the reference as in the dialog example
in these cases we can build machine learned models trained on human judgment data to mimic human judges to measure many quality metrics of output such as factual correctness uralness uency coherence
in this chapter we survey the nlg evaluation metrics that are computed using machine learned models with a focus on recent neural models
context dialog response generation speaker a hey john what do you want to do tonight speaker b why do nt we go see a movie image captioning ground truth response nah i hate that stuff let s do something active
model distorted output response oh sure heard the lm about turing is out rouge bleu

wmd
a man wearing a red life caption jacket is sitting in a canoe on a lake caption a guy wearing a life vest is in a small boat on a lake


table
demonstration of issues with using automatic evaluation metrics that rely on n gram overlap using two short text generation tasks dialog response generation and image captioning
the examples are adapted from liu et al
and kilickaya et al


sentence semantic similarity based evaluation neural approaches to sentence representation learning seek to capturing semantic and syntactic meanings of sentences from different perspectives and topics and to map a sentence onto an bedding vector using dnn models
as with word embeddings nlg models can be evaluated by embedding each sentence in the generated and reference texts
figure
illustration of skip thoughts vectors model for sentence representation learning image source kiros et al

extending mikolov et al
to produce word or phrase embeddings one of the liest sentence embeddings models deep semantic similarity model dssm huang et al
introduces a series of latent semantic models with a deep structure that projects two or more text streams such as a query and multiple documents into a common low dimensional space where the relevance of one text towards the other text can be computed via vector distance
the thought vectors model kiros et al
exploits the encoder decoder architecture to predict context sentences in an unsupervised manner
skip thought vectors allow us to encode rich tual information by taking into account the surrounding context but are slow to train
fastsent hill et al
makes training efcient by representing a sentence as the sum of its word dings but also dropping any knowledge of word order
a simpler weighted sum of word vectors arora et al
weighs each word vector by a factor similar to the tf idf score where more frequent terms are weighted less
similar to fastsent it ignores word order and surrounding tences
extending dssm models infersent conneau et al
is an effective model which uses lstm based siamese networks with two additional advantages over the fastsent
it encodes word order and is trained on a high quality sentence inference dataset
on the other hand thought logeswaran lee is based on an unsupervised model of universal sentence embeddings trained on consecutive sentences
given an input sentence and its context a classier is trained to distinguish a context sentence from other contrastive sentences based on their embeddings
the recent large scale pre trained language models plms such as elmo and bert use tualized word embeddings to represent sentences
even though these plms outperform the lier models such as dssms they are more computationally expensive to use for evaluating nlg systems
for example the transformer based bert model devlin et al
and its extension roberta liu et al
are designed to learn textual similarities on sentence pairs using cosine similarities similar to dssm
but both are much more computationally expensive than dssm due to the fact that they use a much deeper nn architecture and need to ne tuned for different tasks
to remedy this reimers gurevych propose to use sentbert which is a ne tuned bert on a general task to optimize the bert parameters so that a cosine similarity between two erated sentence embeddings is strongly related to the semantic similarity of the two sentences
then the ne tuned model can be used to evaluate various nlg tasks
a recent study focusing on chine translation task esim also computes sentence representations from bert embeddings with no ne tuning and later computes the similarity between the translated text and its reference using metrics such as average recall of its reference
chen et al
mathur et al


evaluating factual correctness figure
illustration of the training strategy of the factually correct summarization model
image source zhang et al

latexit severecardiomegalyisseen
nsubj pass background patientwithchestpain findings persistentlowlungvolumeswithenlargedheart
latexit
latexit ki radiographsshowseverecardiomegalywithpluraleffusions
y latexit severecardiomegalyisseen
y latexit
latexit zhang et al
propose a way to tackle the problem of factual correctness in summarization models
focusing on summarizing radiology reports they extend pointer networks for abstractive summarization by introducing a reward based optimization that trains the generators to obtain more rewards when they generate summaries that are factually aligned with the original document
ically they design a fact extractor module so that the factual accuracy of a generated summary can be measured and directly optimized as a reward using policy gradient as shown in figure

this fact extractor is based on an information extraction module and extracts and represents the facts from generated and reference summaries in a structured format
the summarization model is dated via reinforcement learning using a combination of the nll negative log likelihood loss a rouge based loss and a factual correctness based loss act
their work suggests that for domains in which generating factually correct text is crucial a carefully plemented information extraction system can be used to improve the factual correctness of neural summarization models via reinforcement learning
n l to evaluate the factual consistency of the text generation models eyal et al
present a question answering based parametric evaluation model named answering performance for tion of summaries apes
their evaluation model is designed to evaluate document summarization and is based on the hypothesis that the quality of a generated summary is associated with the number of questions from a set of relevant ones that can be answered by reading the summary
figure
apes evaluation ow
image source hashimoto et al

to build such an evaluator to assess the quality of generated summaries they introduce two nents a set of relevant questions for each source document and a question answering system
they rst generate questions from each reference summary by masking each of the named entities present in the reference based on the method described in hermann et al

for each ence summary this results in several triplets in the form generated summary question answer where question refers to the sentence containing the masked entity answer refers to the masked entity and the generated summary is generated by their summarization model
thus for each erated summary metrics can be derived based on the accuracy of the question answering system in retrieving the correct answers from each of the associated triplets
this metric is useful for rizing documents for domains that contain lots of named entities such as biomedical or news article summarization

regression based evaluation shimanaka et al
propose a segment level machine translation evaluation metric named ruse
they treat the evaluation task as a regression problem to predict a scalar value to indicate the quality of translating a machine translated hypothesis t to a reference translation r
they rst do a forward pass on the gru gated recurrent unit based an encoder to generate t and represent r as a dimensional vector
then they apply different matching methods to extract relations between t and r by concatenating getting the element wise product computing the see figure

ruse is demonstrated to be an efcient absolute element wise distance metric in machine translation shared tasks in both segment level how well the metric correlates with human judgements of segment quality and system level how well a given metric correlates with the machine translation workshop ofcial manual ranking metrics
figure
the sketch of the ruse metric
image source logeswaran lee

evaluation models with human judgments for more creative and open ended text generation tasks such as chit chat dialog story generation or online review generation current evaluation methods are only useful to some degree
as we mentioned in the beginning of this section word overlap metrics are ineffective as there are often many plausible references in these scenarios and collecting all is impossible
even though human evaluation methods are useful in these scenarios for evaluating aspects like coherency naturalness or uency aspects like diversity or creativity may be difcult for human judges to assess as they have no knowledge about the dataset that the model is trained on
language models can learn to copy from the training dataset and generate samples that a human judge will rate as high in quality but will fail in generating diverse samples i
e
samples that are very different from training samples as has been observed in social chatbots li et al
zhou et al

as we discussed in the previous sections a language model optimized only for perplexity generates coherent but bland responses
such behaviours are observed when generic pre trained language models are used for downstream tasks as is without ne tuning on in domain datasets of related downstream tasks
a commonly overlooked issue is that conducting human evaluation for every new generation task can be expensive and not easily generalizable
to calibrate human judgments and automatic evaluation metrics model based approaches that use human judgments as attributes or labels have been proposed
lowe et al
introduce a based evaluation metric adem which is learned from human judgments for dialog system tion specically response generation in a chatbot setting
they collect human judgment scores on chitchat dialog using the appropriateness metric which they say is satisfactory to evaluate chitchat dialog model responses as most systems generate inappropriate responses
they train the evaluation model using twitter each tweet response is a reference and its previous dialog turns are its context
then they use different models such as rnns retrieval based methods or other human responses to generate different responses and ask humans to judge the appropriateness of the generated sponse given the context
for evaluation they use a higher quality labeled twitter dataset ritter et al
which contains dialogs on a variety of topics
figure
the adem evaluation model
image source lowe et al

using this score labeled dataset the adem evaluation model is trained as follows first a latent variational recurrent encoder decoder model vhred serban et al
is pre trained on a dialog dataset to learn to represent the context of a dialog
vhred encodes the dialog context into a vector representation from which the model generates samples of initial vectors to condition the decoder model to generate the next response
using the pre trained vhred model as the encoder they train adem as follows
first the dialog context c the model generated response r and the reference response r are fed to vhred to get their embedding vectors c r and r
then each embedding is linearly projected so that the model response r can be mapped onto the spaces of the dialog context and the reference response to calculate a similarity score
the similarity score measures how close the model responses are to the context and the reference response after the projection as follows r r ct mr rt nr
adem is optimized for squared error loss between the predicted score and the human judgment score with regularization in an end to end fashion
the trained evaluation model is shown to correlate well with human judgments
adem is also found to be conservative and give lower scores to plausible responses
with the motivation that a good evaluation metric should capture both the quality and the diversity of the generated text hashimoto et al
propose a new evaluation metric named human unied with statistical evaluation huse which focuses on more creative and open ended text generation tasks such as dialogue and story generation
different from the adem metric which relies on human judgments for training the model huse combines statistical evaluation and human evaluation metrics in one model as shown in figure

figure
huse can identify samples with defects in quality sharon has stroke for stroke and diversity cleared coach facing
image source hashimoto et al

huse considers the conditional generation task that given a context sampled from a prior tribution outputs a distribution over possible sentences
the evaluation metric is designed to determine the similarity of the output distribution pmodel and a human generation reference distribution pref
this similarity is scored using an optimal discriminator that determines whether a sample comes from the reference or hypothesis model distribution figure

for instance a low quality text is likely to be sampled from the model distribution
the discriminator is implemented approximately using two probability measures i the probability of a sentence under the model which can be estimated using the text generation model and the probability under the reference distribution which can be estimated based on human judgment scores
on summarization and chitchat dialog tasks huse has been shown to be effective to detect low diverse generations that humans fail to detect

bert based evaluation given the strong performance of bert devlin et al
across many tasks there has been work that uses bert or similar pre trained language models for evaluating nlg tasks such as tion and dialog response generation
here we summarize some of the recent work that ne tunes bert to use as evaluation metrics for downstream text generation tasks
one of the bert based models for semantic evaluation is bertscore zhang et al

as illustrated in figure
it leverages the pre trained contextual embeddings from bert and matches referencemodel probability bows out of australian openagassi withdraws from australian opensharon has stroke for strokecleared coach facing another grilling from british swim bossesmodel generationsreferencehuman judgment figure
illustration of bertscore metric
image source zhang et al

words in candidate and reference sentences by cosine similarity
it has been shown to correlate well with human judgments on sentence level and system level evaluations
moreover bertscore computes precision recall and measures which are useful for evaluating a range of nlg tasks
kane et al
present a new bert based evaluation method called roberta sts to detect sentences that are logically contradictory or unrelated regardless whether they are grammatically plausible
using roberta liu et al
as a pre trained language model roberta sts is ne tuned on the sts b dataset to learn the similarity of sentence pairs on a likert scale
another evaluation model is ne tuned on the multi genre natural language inference corpus in a similar way to learn to predict logical inference of one sentence given the other
both model based tors have been shown to be more robust and correlate better with human evaluation than automatic evaluation metrics such as bleu and rouge
figure
agreement between bleurt and human ratings for different skew factors in train and test
image source sellam et al
another recent bert based machine learned evaluation metric is bleurt sellam et al
which is proposed to evaluate various nlg systems
the evaluation model is trained as follows a checkpoint from bert is taken and ne tuned on synthetically generated sentence pairs using matic evaluation scores such as bleu or rouge and then further ne tuned on system generated outputs and human written references using human ratings and automatic metrics as labels
the ne tuning of bleurt on synthetic pairs is an important step because it improves the robustness to quality drifts of generation systems
as shown in the plots in figure
as the nlg task gets more difcult the ratings get closer as it is easier to discriminate between good and bad systems than to rank good systems
to ensure the robustness of their metric they investigate with training datasets with different characteristics such as when the training data is highly skewed or out of domain
they report that the training skew has a disastrous effect on bleurt without pre training this pre training makes bleurt signicantly more robust to quality drifts
llllllllllllllllllllllllllllllllllllllllllllllllllbleurt no pretrain
bleurt w




set skewkendall tau w
human ratingslllllbertscorebleutrain sk
sk

sk

sk

sk

figure
composite metrics model architecture
image source sharif al

as discussed in chapter humans can efciently evaluate performance of two models side side and most embedding based similarity metrics reviewed in the previous sections are based on this idea
inspired by this the comparator evaluator zhou xu is proposed to evaluate nlg models by learning to compare a pair of generated sentences by ne tuning bert
a text pair relation classier is trained to compare the task specic quality of a sample hypothesis and reference based on the win loss rate
using the trained model a skill rating system is built
this system is similar to the player vs player games in which the players are evaluated by observing a record of wins and losses of multiple players
then for each player the system infers the value of a latent unobserved skill variable that indicates the records of wins and losses
on story generation and open domain dialogue response generation tasks the comparator evaluator metric demonstrates high correlation with human evaluation

composite metric scores the quality of many nlg models like machine translation and image captioning can be evaluated for multiple aspects such as adequacy uency and diversity
many composite metrics have been proposed to capture a multi dimensional sense of quality
sharif et al
present a machine learned composite metric for evaluating image captions
the metric incorporates a set of existing metrics such as meteor wmd and spice to measure both adequacy and uency
li chen propose a composite reward function to evaluate the performance of image captions
the approach is based on rened adversarial inverse reinforcement learning rairl which eases the reward ambiguity common in reward based generation models by decoupling the reward for each word in a sentence
the proposed composite reward is shown on ms coco data to achieve state of the art performance on image captioning
some examples generated from this model that uses the composite reward function are shown in figure
top four images example image captions using different learning objectives mle maximum likelihood learning gan generative adversarial networks rl reward based reinforcement learning
tom image example generations from adversarial inverse reinforcement learning rairl
image source li chen
chapter two case studies of task specic nlg evaluation in the previous chapters we have reviewed a wide range of nlg evaluation metrics individually
this chapter presents how these metrics are used jointly to evaluate nlg systems for real world applications
we choose two nlg tasks automatic document summarization and long text ation as case studies
these tasks are sophisticated enough that multiple metrics are required to gauge different aspects of the nlg quality

case study automatic document summarization evaluation a text summarization system aims to extract useful content from a reference document and generate a short summary that is coherent uent readable concise and consistent with the reference ment
there are different types of summarization approaches which can be grouped by their tasks into i generic text summarization for broad topics ii topic focused such as scientic article conversation or meeting summarization and query focused such that the summary answers a posed query
these approaches can also be grouped by their method i extractive where a mary is composed of a subset of sentences or words in the input document and abstractive where a summary is generated on and often contains text units that do not occur in the input document
depending on the number of documents to be summarized these approaches can be grouped into single document or multi document summarization
evaluation of text summarization regardless of its type measures the system s ability to generate a summary based on i a set of criteria that are not related to references dusek et al
ii a set of criteria that measure its closeness to the reference document or a set of criteria that measure its closeness to the reference summary
figure
shows the taxonomy of evaluation metrics steinberger jezek in two categories intrinsic and extrinsic


intrinsic methods intrinsic evaluation of generated summaries can focus on the generated text s content text quality and factual consistency each discussed below
content
content evaluation compares a generated summary to a reference summary using matic metrics
the most widely used metric for summarization is rouge though other metrics such as bleu and f score are also used
although rouge has been shown to correlate well with human judgments for generic text summarization the correlation is lower for topic focused marization like extractive meeting summarization liu liu
meetings are transcripts of spontaneous speech and thus usually contain disuencies such as pauses e

um uh
discourse markers e

you know i mean
repetitions
liu liu nd that after such disuencies are cleaned the rouge score is improved
they even observed fair amounts of figure
taxonomy of summarization evaluation methods
extended from steinberger jezek
improvement in the correlation between the rouge score and human judgments when they include the speaker information of the extracted sentences from the source meeting to form the summary
quality
evaluating generated summaries based on quality has been one of the challenging tasks for summarization researchers
as basic as it sounds since the denition of a good quality mary has not been established nding the most suitable metrics to evaluate quality remains an open research area
below are some criteria of text which are used in recent papers as human evaluation metrics to evaluate the quality of generated text in comparison to the reference text
coherence measures how clearly the ideas are expressed in the summary lapata lay
readability and fluency associated with non redundancy are linguistic quality metrics used to measure how repetitive the generated summary is and how many spelling and mar errors there are in the generated summary
lapata
focus indicates how many of the main ideas of the document are captured while avoiding superuous details
informativeness which is mostly used to evaluate question focused summarization sures how well the summary answers a question
auto regressive generation models trained to generate a short summary text given a longer may yield shorter summaries due to reasons relating to bias in the training data or type of the decoding method e

beam search can yield more coherent text compared to top k decoding but can yield shorter text if a large beam size is used
huang et al

thus in comparing different model generations the summary text length has also been used as an informativeness measure since a shorter text typically preserves less information singh jin
these quality criterion are widely used as evaluation metrics for human evaluation in document summarization
they can be used to compare a system generated summary to a source text a human generated summary or to another system generated summary
factual consistency
one thing that is usually overlooked in document summarization tasks is evaluating the generated summaries based on how well they can convey factual correctness
as discussed in the introduction section fact checking has been a mainstream evaluation strategy for automatic text generation due to the emergence of powerful language models zellers et al

however since they are not trained to be factually consistent and can write about anything related to the prompt they frequently generate factually incorrect text
table
shows a sample marization model output in which the claims made are not consistent with the source document kryscinski et al

it is imperative that the summarization models are factually consistent and that any conicts tween a source document and its generated summary can be easily measured especially for specic summarization tasks like patient doctor conversation summarization or business meeting source article fragments cnn the mother of a quadriplegic man who police say was left in the woods for days can not be dited to face charges in philadelphia until she pletes an unspecied treatment maryland police said monday
the montgomery county maryland department of police took nyia parler into tody sunday


model generated claims quadriplegic man nyia parler left in woods for days can not be extradited
cnn the classic video game space invaders was developed in japan back in the late s and now their real life counterparts are the topic of an earnest political discussion in japan s corridors of power
luckily japanese can sleep soundly in their beds tonight as the government s top military ofcial earnestly revealed that


video game space invaders was developed in japan back in
table
examples of factually incorrect claims output by summarization models
green text highlights the support in the source documents for the generated claims red text highlights the errors made by summarization models
table source kryscinski et al

summarization
as a result factual consistency aware text generation research has drawn a lot of attention in the community in recent years kryscinski et al
gunel et al
kryscinski et al
zhang et al
wang et al

a common approach is to use a model based approach in which a separate component is built on top of a summarization engine that can evaluate the generated summary based on factual consistency
in section
we have discussed some of these parametric fact checking models


extrinsic summarization evaluation methods extrinsic evaluation metrics test the generated summary text by how it impacts the performance of downstream tasks such as relevance assessment reading comprehension and question answering
cohan goharian propose a new metric sera summarization evaluation by relevance analysis for summarization evaluation based on the content relevance of the generated summary and the human written summary
they nd that this metric yields higher correlation with human judgments compared to rouge especially on the task of scientic article summarization
eyal et al
wang et al
measure the performance of a summary by using it to answer a set of questions regarding the salient entities in the source document

case study long text generation evaluation a long text generation system aims to generate multi sentence text such as a single paragraph or a multi paragraph document
common applications of long form text generation are document level machine translation story generation news article generation poem generation summarization and image description generation to name a few
this research area presents a particular challenge to state of the art sota approaches that are based on statistical neural models which are proven to be insufcient to generate coherent long text
for example one of the sota neural language models radford et al
can generate remarkably uent sentences and even paragraphs for a given topic or a prompt
however as more sentences are generated and the text gets longer it starts to wander switching to unrelated topics and becoming incoherent rashkin et al

evaluating long text generation by itself is a challenging task
new criteria need to be implemented to measure the quality of long generated text such as inter sentence or inter paragraph coherence in language style and semantics
although human evaluation methods are commonly used we focus our discussion on automatic evaluation methods in this case study


evaluation via discourse structure long text consists of groups of sentences structured together by linguistic elements known as course jurafsky martin
considering the discourse structure of the generated text is then crucial in evaluating the system
especially in open ended text generation the model needs to termine the topical ow structure of entities and events and their relations in a narrative ow that is coherent and uent
one of the major tasks in which discourse plays an important role is level machine translation gong et al

hajlaoui popescu belis present a new metric called accuracy of connective translation act meyer et al
that uses a combination of rules and automatic metrics to compare the discourse connection between the source and target uments
joty et al
on the other hand compare the source and target documents based on the similarity of their discourse trees


evaluation via lexical cohesion lexical cohesion is a surface property of text and refers to the way textual units are linked together grammatically or lexically
lexical similarity lapata barzilay is one of the most monly used metrics in story generation
roemmele et al
lter the n grams based on lexical semantics and only use adjectives adverbs interjections nouns pronouns proper nouns and verbs for lexical similarity measure
other commonly used metrics compare reference and source text on mikolov et al
or sentence level kiros et al
embedding similarity averaged over the entire document
entity co reference is another metric that has been used to measure herence elsner charniak
an entity should be referred to properly in the text and should not be used before introduced
roemmele et al
capture the proportion of the entities in the generated sentence that are co referred to an entity in the corresponding context as a metric of entity co reference in which a higher co reference score indicates higher coherence
in machine translation wong kit introduce a feature that can identify lexical cohesion at the sentence level via word level clustering using wordnet miller and stemming to obtain a score for each word token which is averaged over the sentence
they nd that this new score improves correlation of bleu and ter with human judgments
other work such as gong et al
uses topic modeling together with automatic metrics like bleu and meteor to evaluate lexical cohesion in machine translation of long text
chow et al
investigate the position of the word tokens in evaluating the uency of the generated text
they modify wmd by adding a fragmentation penalty to measure the uency of a translation for evaluating machine translation systems


evaluation via writing style gamon show that an author writing is consistent in style across a particular work
based on this nding roemmele et al
propose to measure the quality of generated text based on whether it presents a consistent writing style
they capture the category distribution of individual words between the story context and the generated following sentence using their part of speech tags of words e
g adverbs adjectives conjunctions determiners nouns

text style transfer reects the creativity of the generation model in generating new content
style transfer can help re write a text in a different style which is useful in creative writing such as poetry generation ghazvininejad et al

one metric that is commonly used in style transfer is the classication score obtained from a pre trained style transfer model fu et al

this metric measures whether a generated sentence has the same style as its context


evaluation with multiple references one issue of evaluating text generation systems is the diversity of generation especially when the text to evaluate is long
the generated text can be uent valid given the input and informative for the user but it still may not have lexical overlap with the reference text or the prompt that was used to constrain the generation
this issue has been investigated extensively li et al
haei et al
holtzman et al
welleck et al
gao et al

using multiple references that cover as many plausible outputs as possible is an effective solution to improving the correlation of automatic evaluation metrics such as adequacy and uency with human judgments as demonstrated in machine translation han laubli et al
and other nlg tasks
chapter conclusions and future directions text generation is central to many nlp tasks including machine translation dialog response eration document summarization
with the recent advances in neural language models the research community has made signicant progress in developing new nlg models and systems for challenging tasks like multi paragraph document generation or visual story generation
with every new system or model comes a new challenge of evaluation
this paper surveys the nlg evaluation methods in three categories human centric evaluation
human evaluation is the most important for developing nlg systems and is considered the gold standard when developing automatic metrics
but it is expensive to execute and the evaluation results are difcult to reproduce
untrained automatic metrics
untrained automatic evaluation metrics are widely used to monitor the progress of system development
a good automatic metric needs to correlate well with human judgments
for many nlg tasks it is desirable to use multiple metrics to gauge different aspects of the system s quality
machine learned evaluation metrics
in the cases where the reference outputs are not complete we can train an evaluation model to mimic human judges
however as pointed out in gao et al
any machine learned metrics might lead to potential problems such as overtting and gaming of the metric
we conclude this paper by summarizing some of the challenges of evaluating nlg systems detecting machine generated text and fake news
as language models get stronger by learning from increasingly larger corpora of human written text they can generate text that is not easily distinguishable from human authored text
due to this new systems and evaluation methods have been developed to detect if a piece of text is or generated
a recent study schuster et al
reports the results of a fact verication system to identify inherent bias in training datasets that cause fact checking issues
in an attempt to combat fake news vo lee present an extensive analysis of tweets and a new tweet generation method to identify fact checking tweets among many tweets which were originally produced to persuade posters to stop tweeting fake news
other research focuses on factually correct text generation with a goal of providing users with accurate information
massarelli et al
introduce a new approach for generating text that is factually consistent with the knowledge source
kryscinski et al
investigate methods of checking the consistency of a generated summary against the document from which the summary is generated
making evaluation explainable
explainable ai refers to ai and machine learning ods that can provide human understandable justications for their behaviour ehsan et al

evaluation systems that can provide reasons for their decisions are benecial in many ways
for instance the explanation could help system developers to identify the root causes of the system s quality problems such as unintentional bias repetition or factual consistency
the eld of explainable ai is growing particularly in generating explanations of classier predictions in nlp tasks ribeiro et al
thorne et al

text generation systems that use evaluation methods that can provide justication or tion for their decisions will be more trusted by their users
future nlg evaluation research should focus on developing easy to use robust and explainable evaluation tools
improving corpus quality
creating high quality datasets with multiple reference texts is essential for not only improving the reliability of evaluation but also allowing the ment of new automatic metrics that correlate well with human judgments belz reiter
standardizing evaluation methods
most untrained automatic evaluation metrics are standardized using open source platforms like natural language toolkit or
such platforms can signicantly simplify the process of benchmarking different models
however there are still many nlg tasks that use task specic evaluation rics such as metrics to evaluate the contextual quality or informativeness of generated text
there are also no standard criteria for human evaluation methods for different nlg tasks
it is important for the research community to collaborate more closely to standardize the evaluation metrics for nlg tasks that are pursued by many research teams
one effective way to achieve this is to organize challenges or shared tasks such as the evaluating natural language generation and the shared task on nlg
developing effective human evaluations
for most nlg tasks there is little consensus on how human evaluations should be conducted
furthermore papers often leave out portant details on how the human evaluations were run such as who the evaluators are and how many people evaluated the text van der lee et al

clear reporting of human evaluations is very important especially for replicability purposes
we encourage nlg researchers to design their human evaluations carefully paying tention to best practices described in nlg and crowdsourcing research and to include the details of the studies and data collected from human evaluations where possible in their papers
this will allow new research to be consistent with previous work and enable more direct comparisons between nlg results
human evaluation based shared tasks and evaluation platforms can also provide evaluation consistency and help researchers directly compare how people perceive and interact with different nlg systems
evaluating ethical issues
there is still a lack of systematic methods for evaluating how effectively an nlg system can avoid generating improper or offensive language
the lem is particularly challenging when the nlg system is based on neural language models whose output is not always predictable
as a result many social chatbots such as aoice zhou et al
resort to hand crafted policies and editorial responses to make the system s behavior predictable
however as pointed out by zhou et al
even a completely deterministic function can lead to unpredictable behavior
for example a simple answer yes could be perceived as offensive in a given context
we encourage researchers working in nlg and nlg evaluation to focus on these challenges moving forward as they will help sustain and broaden the progress we have seen in nlg so far

org
io
org sympa info eval
gen
chal
com evanmiltenburg shared task on nlg evaluation bibliography abhaya agarwal and alon lavie
meteor m bleu and m ter evaluation metrics for high correlation with human rankings of machine translation output
in proceedings of the third workshop on statistical machine translation statmt pp
usa
association for tational linguistics
isbn
eneko agirre aitor gonzalez agirre inigo lopez gazpio montse maritxalar german rigau and larraitz uria
task interpretable semantic textual similarity
pp

manex agirrezabal bertol arrieta aitzol astigarraga and mans hulden
pos tag based poetry generation with wordnet
in proceedings of the european workshop on natural language generation pp
soa bulgaria august
association for computational tics
url
aclweb
org anthology
jorge agnese jonathan herrera haicheng tao and xingquan zhu
a survey and taxonomy of adversarial neural networks for text to image synthesis
wires data mining and knowledge discovery feb
issn

widm

url


widm

ahmed ali walid magdy peter bell and steve renais
multi reference wer for evaluating asr for languages with no orthographic rules
pp


asru


ramiz aliguliyev
using the measure as similarity measure for automatic text summarization
vychislitelnye tekhnologii
jacopo amidei paul piwek and alistair willis
rethinking the agreement in human evaluation tasks
in coling
jacopo amidei paul piwek and alistair willis
agreement is overrated a plea for correlation to assess human evaluation reliability
in inlg
jacopo amidei paul piwek and alistair willis
the use of rating and likert scales in natural inlg language generation human evaluation tasks a review and some recommendations

url

com assets
pdf
peter anderson basura fernando mark johnson and stephen gould
spice semantic tional image caption evaluation
eccv
url
org

erion c ano and ondrej bojar
keyphrase generation a multi aspect survey

sanjeev arora yingyu liang and tengyu ma
a simple but tough to beat baseline for sentence embeddings
january
international conference on learning representations iclr conference date through
ron artstein and massimo poesio
inter coder agreement for computational linguistics
tional linguistics
dzmitry bahdanau kyunghyun cho and yoshua bengio
neural machine translation by jointly learning to align and translate
in iclr
shuang bai and shan an
va survey on automatic image caption generation
neuro computing pp

mousumi banerjee michelle hopkins capozzoli laura a
mcsweeney and debajyoti sinha
yond kappa a review of interrater agreement measures

satanjeev banerjee and alon lavie
meteor an automatic metric for mt evaluation with in proceedings of the acl workshop on improved correlation with human judgments
trinsic and extrinsic evaluation measures for machine translation summarization pp
ann arbor michigan june
association for computational linguistics
url
aclweb
org anthology
ellen gurman bard dan robertson and antonella sorace
magnitude estimation of linguistic acceptability
language
issn
url
jstor
org
regina barzilay and lillian lee
learning to paraphrase an unsupervised approach using sequence alignment
in hlt naacl main proceedings pp

anja belz and ehud reiter
comparing automatic and human evaluation of nlg systems
in conference of the european chapter of the association for computational linguistics trento italy april
association for computational linguistics
url
aclweb
anthology
alan black susanne burger alistair conkie helen hastie simon keizer oliver lemon nicolas merigaud gabriel parent gabriel schubiner blaise thomson jason williams kai yu steve young and maxine eskenazi
spoken dialog challenge comparison of live and control test results
pp

antoine bosselut asli celikyilmaz xiaodong he jianfeng gao po sen huang and yejin choi
discourse aware neural rewards for coherent text generation
in conference of the north american chapter of the association for computational linguistics human language nologies naacl hlt july
massimo caccia lucas caccia william fedus hugo larochelle joelle pineau and laurent lin
language gans falling short
corr

url
org

chris callison burch and miles osborne
re evaluating the role of bleu in machine translation research
in in eacl pp

chris callison burch cameron fordyce philipp koehn christof monz and josh schroeder
evaluation of machine translation
in proceedings of the second workshop on statistical machine translation pp
prague czech republic june
association for computational linguistics
url
aclweb
org anthology
asli celikyilmaz antoine bosselut xiaodong he and yejin choi
deep communicating agents for abstractive summarization
july
daniel m
cer mona t
diab eneko agirre inigo lopez gazpio and lucia specia
task semantic textual similarity multilingual and cross lingual focused evaluation
corr

url
org

liqun chen shuyang dai chenyang tao haichao zhang zhe gan dinghan shen yizhe zhang guoyin wang ruiyi zhang and lawrence carin
adversarial text generation via feature mover s distance
in s
bengio h
wallach h
larochelle k
grauman n
bianchi and r
garnett eds
advances in neural information processing systems pp

curran associates inc

url
nips
cc adversarial text generation via feature movers distance
pdf
qian chen xiaodan zhu zhen hua ling si wei hui jiang and diana inkpen
enhanced lstm for natural language inference
in proceedings of the annual meeting of the association for computational linguistics volume long papers pp
vancouver canada july
association for computational linguistics


url https
aclweb
org anthology
yen chun chen and mohit bansal
fast abstractive summarization with reinforce selected sentence rewriting
in proceedings of the annual meeting of the association for computational guistics volume long papers pp
melbourne australia july
association for computational linguistics


url
aclweb
anthology
kyunghyun cho bart van merrienboer caglar gulcehre dzmitry bahdanau fethi bougares ger schwenk and yoshua bengio
learning phrase representations using rnn encoder decoder for statistical machine translation
in proceedings of the conference on empirical methods in natural language processing emnlp pp
doha qatar october
tion for computational linguistics
url
aclweb
org anthology
julian chow lucia specia and pranava madhyastha
wmdo fluency based word mover s tance for machine translation evaluation
in proceedings of the fourth conference on machine translation volume shared task papers day pp
florence italy august
association for computational linguistics
url
aclweb
org
elizabeth clark asli celikyilmaz and noah a
smith
sentence mover s similarity automatic evaluation for multi sentence texts
in acl
j
clarke and m
lapata
global inference for sentence compression an integer linear programming approach
journal of articial intelligence research
arman cohan and nazli goharian
revisiting summarization evaluation for scientic articles
corr

url
org

jacob cohen
a coefcient of agreement for nominal scales
educational and psychological surement
jacob cohen
weighted kappa nominal scale agreement provision for scaled disagreement or partial credit
psychological bulletin


url


alexis conneau douwe kiela holger schwenk loc barrault and antoine bordes
supervised learning of universal sentence representations from natural language inference data
in ings of the conference on empirical methods in natural language processing pp
copenhagen denmark september
association for computational linguistics
url
aclweb
org anthology
scott a
crossley minkyung kim laura allen and danielle mcnamara
automated in articial intelligence in tion evaluation ase using natural language processing tools
ucation international conference aied proceedings lecture notes in computer science including subseries lecture notes in articial intelligence and lecture notes in formatics pp

springer verlag
yin cui guandao yang andreas veit xun huang and serge j
belongie
learning to evaluate image captioning
corr

url
org

raj dabre chenhui chu and anoop kunchukuttan
a comprehensive survey of multilingual neural machine translation
tunable metric
daniel dahlmeier chang liu and hwee tou ng
tesla at wmt translation evaluation and andrew m
dai christopher olah and quoc v
le
document embedding with paragraph vectors
in neurips deep learning workshop
sumanth dathathri andrea madotto janice lan jane hung eric frank piero molino jason ski and rosanne liu
plug and play language models a simple approach to controlled text generation
in iclr
luciano del corro and rainer gemulla
clausie clause based open information extraction
in proceedings of the international conference on world wide web www pp
new york ny usa
association for computing machinery
isbn
doi


url



etienne denoual and yves lepage
bleu in characters towards automatic mt evaluation in guages without word delimiters
in companion volume to the proceedings of conference ing posters demos and tutorial abstracts
url
aclweb
org
jan deriu alvaro rodrigo arantxa otegi guillermo echegoyen sophie rosset eneko agirre and mark cieliebak
evaluation metrics for text summarization
computing and informatics
url
cai
sk ojs index
php cai article
jan deriu alvaro rodrigo arantxa otegi guillermo echegoyen sophie rosset eneko agirre and mark cieliebak
survey on evaluation methods for dialogue systems
corr

url
org

jacob devlin ming wei chang kenton lee and kristina toutanova
bert pre training of deep bidirectional transformers for language understanding
corr

url http
org

djellel eddine difallah elena filatova and panagiotis g
ipeirotis
demographics and dynamics of mechanical turk workers
proceedings of the eleventh acm international conference on web search and data mining
george doddington
automatic evaluation of machine translation quality using n gram occurrence statistics
pp

and chris brockett
in paraphrases

asia bill dolan sentential ing
automatically constructing a corpus of sentential
of january
microsoft
com en us research constructing international workshop automatically of natural processing federation language third a on url li dong nan yang wenhui wang furu wei xiaodong liu yu wang jianfeng gao ming zhou and hsiao wuen hon
unied language model pre training for natural language understanding and generation
corr

url
org

xinya du junru shao and claire cardie
learning to ask neural question generation for reading comprehension
corr

url
org

ondrej dusek jekaterina novikova and verena rieser
referenceless quality estimation for ral language generation
corr

url
org

ondrej dusek jekaterina novikova and verena rieser
evaluating the state of the art of end end natural language generation the nlg challenge
corr

url
org

upol ehsan pradyumna tambwekar larry chan brent harrison and mark o
riedl
automated rationale generation a technique for explainable ai and its effects on human perceptions
ceedings of the international conference on intelligent user interfaces mar
doi


url




m
elsner and e
charniak
coreference inspired coherence modeling
proceedings of the annual meeting of the association for computational linguistics on human language gies short papers
matan eyal tal baumel and michael elhadad
question answering as an automatic evaluation metric for news article summarization
proceedings of the conference of the north
url



matan eyal tal baumel and michael elhadad
question answering as an automatic evaluation ric for news article summarization
in proceedings of the conference of the north american chapter of the association for computational linguistics human language technologies ume long and short papers pp
minneapolis minnesota june
tion for computational linguistics
url
aclweb
org anthology
angela fan mike lewis and yann n
dauphin
hierarchical neural story generation
corr

url
org

angela fan yacine jernite ethan perez david grangier jason weston and michael auli
long form question answering
corr

url
org

angela fan mike lewis and yann n
dauphin
strategies for structuring story generation
corr

url
org

jl fleiss
measuring nominal scale agreement among many raters
psychological bulletin november
issn


url


zhenxin fu xiaoye tan nanyun peng dongyan zhao and rui yan
style transfer in text ploration and evaluation
thirty second aaai conference on articial intelligence
michael gamon
linguistic correlates of style authorship classication with deep linguistic analysis features
association for computational linguistics
jianfeng gao michel galley and lihong li
neural approaches to conversational ai
foundations and trends r in information retrieval
cristina garbacea samuel carton shiyan yan and qiaozhu mei
judge the judges a large scale evaluation study of neural language models for online review generation
corr

url
org

albert gatt and emiel krahmer
survey of the state of the art in natural language generation core tasks applications and evaluation
j
artif
intell
res

m
ghazvininejad x
shi y
choi and k
knight
generating topical poetry
emnlp
dimitra gkatzia and saad mahamood
a snapshot of nlg evaluation practices
in proceedings of the european workshop on natural language generation enlg pp
brighton uk september
association for computational linguistics
url https
aclweb
org anthology
yoav goldberg graeme hirst yang liu and meng zhang
neural network methods for natural language processing
in computational linguistics volume
zhengxian gong min zhang and guodong zhou
document level machine translation evaluation with gist consistency and text cohesion
pp

ian j
goodfellow jean pouget abadie mehdi mirza bing xu david warde farley sherjil ozair aaron courville and yoshua bengio
generative adversarial networks
arxiv

cyril goutte
automatic evaluation of machine translation quality

yvette graham
re evaluating automatic summarization with bleu and shades of rouge
in proceedings of the conference on empirical methods in natural language processing pp
lisbon portugal september
association for computational linguistics
url
aclweb
org anthology
yvette graham and timothy baldwin
testing for signicance of increased correlation with human judgment
in proceedings of the conference on empirical methods in natural language processing emnlp pp
doha qatar october
association for computational linguistics
url
aclweb
org anthology
alex graves
generating sequences with recurrent neural networks
corr

url
org

beliz gunel chenguang zhu michael zeng and xuedong huang
mind the facts boosted coherent abstractive text summarization
knowledge representation and reasoning meets machine learning workshop at neurips
najeh hajlaoui and andrei popescu belis
assessing the accuracy of discourse connective lations validation of an automatic metric
proceedings of the international conference on computational linguistics and intelligent text processing
aaron l
han and derek wong
machine translation evaluation a survey

org

aaron l
han derek wong lidia chao liangye he and yi lu
unsupervised quality estimation model for english to german translation and its application in extensive supervised evaluation
the scientic world journal
aaron l
han derek wong lidia chao liangye he yi lu junwen xing and xiaodong zeng
mt
language independent model for machine translation evaluation with reinforced factors

lifeng han
machine translation evaluation resources and methods a survey
ireland postgraduate research conference
tatsunori hashimoto hugh zhang and percy liang
unifying human and statistical evaluation for natural language generation
in proceedings of the conference of the north american ter of the association for computational linguistics human language technologies volume long and short papers pp
minneapolis minnesota june
association for computational linguistics
url
aclweb
org anthology
helen hastie and anja belz
a comparative evaluation methodology for nlg in interactive systems
in proceedings of the ninth international conference on language resources and evaluation pp
reykjavik iceland may
european language resources sociation elra
url
lrec conf
org proceedings paper
pdf
helen f
hastie and anja belz
a comparative evaluation methodology for nlg in interactive systems
in lrec
karl moritz hermann tomas kocisky edward grefenstette lasse espeholt will kay mustafa leyman and phil blunsom
teaching machines to read and comprehend
corr

url
org

martin heusel hubert ramsauer thomas unterthiner bernhard nessler gunter klambauer and sepp hochreiter
gans trained by a two time scale update rule converge to a nash equilibrium
corr

url
org

felix hill kyunghyun cho and anna korhonen
learning distributed representations of sentences from unlabelled data
corr

url
org

sepp hochreiter and jurgen schmidhuber
long short term memory
neural computation
ari holtzman jan buys maxwell forbes and yejin choi
the curious case of neural text ation
iclr

md
zakir hossain ferdous sohel mohd fairuz shiratuddin and hamid laga
a comprehensive survey of deep learning for image captioning
corr

url arxiv
org

liang huang kai zhao and mingbo ma
when to nish optimal beam search for neural text generation modulo beam size
proceedings of the conference on empirical methods in natural language processing


url



po sen huang xiaodong he jianfeng gao li deng alex acero and larry heck
acm cikm url
microsoft
com en us research ing deep structured semantic models for web search using click through data
international conference on information and knowledge management tober
learning deep structured semantic models for web search using clickthrough
xuedong huang fileno alleva hsiao wuen hon mei yuh hwang and ronald rosenfeld
the sphinx ii speech recognition system an overview
computer speech and language
text inspector
measure lexical diversity
url
com lexical
turk
in hcomp
panagiotis g
ipeirotis f
provost and jing wang
quality management on amazon mechanical hideki isozaki tsutomu hirao kevin duh katsuhito sudoh and hajime tsukada
automatic in proceedings of the evaluation of translation quality for distant language pairs
ference on empirical methods in natural language processing pp
cambridge ma october
association for computational linguistics
url
aclweb
anthology
ming jiang qiuyuan huang lei zhang xin wang pengchuan zhang zhe gan jana diesner and jianfeng gao
tiger text to image grounding for image caption evaluation
in emnlp november
shaq joty francisco guzman lluis marquez and preslav nakov
discourse structure in machine translation evaluation
computational linguistics
daniel jurafsky and james h
martin
asking and answering questions to evaluate the factual sistency of summaries

filip jurccek simon keizer milica gasic francois mairesse blaise thomson kai yu and steve young
real user evaluation of spoken dialogue systems using amazon mechanical turk
pp

sushant kae and matt huenerfauth
evaluating the usability of automatically generated captions for people who are deaf or hard of hearing
in proceedings of the international acm cess conference on computers and accessibility pp
new york ny usa
association for computing machinery
isbn
hassan kane yusuf kocyigit pelkins ajanoh ali abdalla and mohamed coulibali
towards neural language evaluators
neurips document intelligence workshop
david kauchak and regina barzilay
paraphrasing for automatic evaluation
in human language technology conference of the north american chapter of the association of computational guistics
mert kilickaya aykut erdem nazli ikizler cinbis and erkut erdem
re evaluating automatic metrics for image captioning
proceedings of the conference of the european chapter of the association for computational linguistics volume long papers


url



yoon kim sam wiseman and alexander m
rush
a tutorial on deep latent variable models of natural language
corr

url
org

svetlana kiritchenko and saif m
mohammad
capturing reliable ne grained sentiment tions by crowdsourcing and best worst scaling
in proceedings of the conference of the north american chapter of the association for computational linguistics human language technologies pp
san diego california june
association for computational linguistics
url
aclweb
org anthology
ryan kiros yukun zhu ruslan salakhutdinov richard s
zemel antonio torralba raquel tasun and sanja fidler
skip thought vectors
corr

url http
org

dan klein and christopher d
manning
accurate unlexicalized parsing
in proceedings of the annual meeting of the association for computational linguistics pp
sapporo japan july
association for computational linguistics
url
aclweb
anthology
d
knight k
marcu
statistics based summarization step one sentence compression
in ceeding of the national conference of the american association for articial intelligence pp

rik koncel kedziorski dhanush bekal yi luan mirella lapata and hannaneh hajishirzi
text generation from knowledge graphs with graph transformers
arxiv

e
krahmer and m
theune
empirical methods in natural language generation data oriented methods and empirical evaluation
lncs sublibrary articial intelligence
springer
isbn
url
google
com
klaus krippendorff
estimating the reliability systematic error and random error of interval data
educational and psychological measurement
wojciech kryscinski bryan mccann caiming xiong and richard socher
evaluating the factual consistency of abstractive text summarization

wojciech kryscinski nitish shirish keskar bryan mccann caiming xiong and richard socher
in proceedings of the conference on neural text summarization a critical evaluation
empirical methods in natural language processing and the international joint ence on natural language processing emnlp ijcnlp pp
hong kong china november
association for computational linguistics
url
aclweb
anthology
m
j
kusner y
sun n
i
kolkin and k
q
weinberger
from word embeddings to document distances
in icml
lori lamel sophie rosset jean luc gauvain samir bennacef matine garnier rizet and bernard prouts
the limsi arise system
in speech communication pp

weiyu lan xirong li and jianfeng dong
fluency guided cross lingual image captioning
acl multimedia

url
org

mirealla lapata
probabilistic text structuring experiments with sentence ordering
proceedings of the annual meeting of the association for computational linguistics the association of tational linguistics
mirealla lapata and regina barzilay
automatic evaluation of text coherence models and sentations
in kaelbling l
p
safotti a
eds
ijcai professional book center
alex lascarides and nicholas asher
discourse relations and defeasible knowledge
in nual meeting of the association for computational linguistics pp
berkeley california usa june
association for computational linguistics
url
aclweb
anthology
alon lavie and abhaya agarwal
meteor an automatic metric for mt evaluation with high levels of correlation with human judgments
in proceedings of the second workshop on statistical machine translation statmt pp
usa
association for computational linguistics
alon lavie kenji sagae and shyamsundar jayaraman
the signicance of recall in automatic metrics for mt evaluation
in amta
audrey j
lee and mark a
przybocki
nist machine translation evaluation ofcial results

chris van der lee albert gatt emiel van miltenburg sander wubben and emiel krahmer
best practices for the human evaluation of automatically generated text
inlg
url https

com assets
pdf
gregor leusch nicola uefng and hermann ney
cder efcient mt evaluation using block movements
in conference of the european chapter of the association for computational linguistics trento italy april
association for computational linguistics
url https
aclweb
org anthology
jinchao li qi zhu baolin peng lars liden runze liang ryuichi takanobu shahin shayandeh swadheen shukla zheng zhang minlie huang and jianfeng gao
multi domain task oriented dialog challenge ii

jiwei li michel galley chris brockett jianfeng gao and bill dolan
a diversity promoting objective function for neural conversation models
in proceedings of the conference of the north american chapter of the association for computational linguistics human language technologies pp
san diego california june
association for computational linguistics
url
aclweb
org anthology
nannan li and zhenzhong chen
learning compact reward for image captioning
arxiv

sheng li zhiqiang tao and yun fu
visual to text survey of image and video captioning
ieee transactions on emerging topics in computational intelligence

tetci


zhongyang li xiao ding and ting liu
generating reasonable and diversied story ending ing sequence to sequence model with adversarial training
in proceedings of the tional conference on computational linguistics pp
santa fe new mexico usa august
association for computational linguistics
url
aclweb
anthology
chin yew lin
rouge a package for automatic evaluation of summaries
in text summarization branches out pp
barcelona spain july
association for computational tics
url
aclweb
org anthology
chin yew lin and franz josef och
automatic evaluation of machine translation quality using in proceedings of the annual longest common subsequence and skip bigram statistics
meeting of the association for computational linguistics pp
barcelona spain july
url
aclweb
org anthology
dahua lin sanja fidler and chen kong raquel urtasun
visual semantic search retrieving videos via complex textual queries
in in cvpr
chia wei liu ryan lowe iulian serban mike noseworthy laurent charlin and joelle pineau
how not to evaluate your dialogue system an empirical study of unsupervised evaluation rics for dialogue response generation
in proceedings of the conference on empirical ods in natural language processing pp
austin texas november
association for computational linguistics
url
aclweb
org anthology
ding liu and daniel gildea
syntactic features for evaluation of machine translation
in proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation summarization pp
ann arbor michigan june
association for tional linguistics
url
aclweb
org anthology
feifan liu and yang liu
correlation between rouge and human evaluation of extractive meeting in proceedings of the annual meeting of the association for computational summaries
linguistics on human language technologies short papers hlt short pp
usa
association for computational linguistics
lixin liu jiajun tang xiaojun wan and zongming guo
generating diverse and descriptive image captions using visual paraphrases

siqi liu zhenhai zhu ning ye sergio guadarrama and kevin murphy
improved image xiaodong liu ing via policy gradient optimization of spider
pp

pengcheng he weizhu chen multi task june for natural
microsoft
com en us research deep neural networks
multi task deep neural networks for natural language
yinhan liu myle ott naman goyal jingfei du mandar joshi danqi chen omer levy mike lewis luke zettlemoyer and veselin stoyanov
roberta a robustly optimized bert pretraining approach
corr

url
org

language understanding
in acl jianfeng gao
url and chi kiu lo
meant
accurate semantic mt evaluation for any output language
in wmt
chi kiu lo
yisi a unied semantic mt quality evaluation and estimation metric for languages with different levels of available resources
in proceedings of the fourth conference on machine translation volume shared task papers day pp
florence italy august
association for computational linguistics


url
aclweb
org anthology
chi kiu lo anand karthik tumuluru and dekai wu
fully automatic semantic mt evaluation
in hlt
lajanugen logeswaran and honglak lee
an efcient framework for learning sentence in international conference on learning representations
url https sentations

net
dang hoang long minh tien nguyen ngo xuan bach le minh nguyen and tu minh phuong
an entailment based scoring method for content selection in document summarization
in ceedings of the ninth international symposium on information and communication technology pp
new york ny usa
association for computing machinery
jordan j
louviere terry n
flynn and a
a
j
marley
best worst scaling theory methods and applications
cambridge university press
ryan lowe michael noseworthy iulian vlad serban nicolas angelard gontier yoshua bengio and joelle pineau
towards an automatic turing test learning to evaluate dialogue responses
acl
url
org

sidi lu yaoming zhu weinan zhang jun wang and yong yu
neural text generation past present and beyond
corr

url
org

samuel laubli sheila castilho graham neubig rico sennrich qinlan shen and antonio toral
a set of recommendations for assessing human machine parity in language translation
journal of articial intelligence research
kelvin luu rik koncel kedziorski kyle lo isabel cachola and noah a
smith
citation text generation
arxiv

nabin maharjan rajendra banjade dipesh gautam lasang j
tamang and vasile rus
dt team at task semantic similarity using alignments sentence level embeddings and gaussian mixture model output
in proceedings of the international workshop on semantic evaluation vancouver canada august
association for computational linguistics
url
aclweb
org anthology
w
c
mann and s
a
thompson
rhetorical structure theory description and construction of text in kempen g
eds natural language generation
nato asi series series e structures
applied sciences
daniel marcu
from discourse structures to text summaries
proceedings of workshop on intelligent scalable text summarization pp

a
martin and m
przybocki
the nist speaker recognition evaluation an overview
lara j
martin prithviraj ammanabrolu william hancock shruti singh brent harrison and mark o
riedl
event representations for automated story generation with deep neural nets
corr

url
org

luca massarelli fabio petroni aleksandra piktus myle ott tim rocktaschel vassilis plachouras fabrizio silvestri and sebastian riedel
how decoding strategies affect the veriability of erated text
nitika mathur timothy baldwin and trevor cohn
putting evaluation in context in proceedings of the annual tual embeddings improve machine translation evaluation
meeting of the association for computational linguistics pp
florence italy july
association for computational linguistics


url https
aclweb
org anthology
nitika mathur timothy baldwin and trevor cohn
tangled up in bleu reevaluating the evaluation of automatic machine translation evaluation metrics
association for computational linguistics acl
joshua maynez shashi narayan bernd bohnet and ryan t
mcdonald
on faithfulness and tuality in abstractive summarization
arxiv

p
m
mccarthy and s
jarvis
mtld vocd d and hd d a validation study of sophisticated approaces in behaviour research methods volume pp
to lexical diversity assessment

url
springer
com
brm



iain mccowan darren moore john dines daniel gatica perez mike flynn pierre wellner and herve bourlard
on the use of information retrieval measures for speech recognition evaluation

kathleen r
mckeown
text generation
using discourse strategies and focus constraints to generate natural language text
studies in natural language processing
i
melamed ryan green and joseph turian
precision and recall of machine translation

thomas meyer andrei popescu belis najeh hajlaoui and andrea gesmundo
machine translation of labeled discourse connectives
proceedings of the tenth conference of the association for machine translation in the americas
tomas mikolov ilya sutskever kai chen greg corrado and jeffrey dean
distributed sentations of words and phrases and their compositionality
corr

url
org

george a
miller
wordnet a lexical database for english
association for computing machinery november
tanushree mitra clayton j
hutto and eric gilbert
comparing and process centric gies for obtaining quality data on amazon mechanical turk
proceedings of the annual acm conference on human factors in computing systems
ehsan montahaei danial alihosseini and mahdieh soleymani baghshah
jointly measuring versity and quality in text generation models
corr

url http
org

ehsan montahaei danial alihosseini and mahdieh soleymani baghshah
jointly measuring sity and quality in text generation models
neuralgen workshop at naacl

url
org

shashi narayan shay b
cohen and mirella lapata
ranking sentences for extractive tion with reinforcement learning
july
preksha nema and mitesh m
khapra
towards a better metric for evaluating question generation systems
corr

url
org

ani nenkova and rebecca passonneau
evaluating content selection in summarization the pyramid method
pp

austin lee nichols and jon k
maner
the good subject effect investigating participant demand characteristics
the journal of general psychology
sonja nieen franz josef och gregor leusch and hermann ney
an evaluation tool for in proceedings of the second chine translation fast evaluation for mt research
tional conference on language resources and evaluation athens greece may
european language resources association elra
url
lrec conf
org proceedings
pdf
jekaterina novikova ondrej dusek amanda cercas curry and verena rieser
why we need new evaluation metrics for nlg
in proceedings of the conference on empirical methods in ural language processing pp
copenhagen denmark september
association for computational linguistics
url
aclweb
org anthology
jekaterina novikova ondrej dusek and verena rieser
rankme reliable human ratings for ral language generation
in proceedings of the conference of the north american chapter of the association for computational linguistics human language technologies volume short papers pp
new orleans louisiana june
association for computational tics
url
aclweb
org anthology
kenji ono kazuo sumita and seiji miike
abstract generation based on rhetorical structure tion
proceedings of the international conference on computational linguistics
daniel m oppenheimer tom meyvis and nicolas davidenko
instructional manipulation checks detecting satiscing to increase statistical power
journal of experimental social psychology
martin t
orne
on the social psychology of the psychological experiment with particular reference to demand characteristics and their implications

sebastian pado michel galley dan jurafsky and christoper manning
textual entailment features for machine translation evaluation
pp

liangming pan wenqiang lei tat seng chua and min yen kan
recent advances in neural tion generation

url
org

joybrata panja and sudip kumar naskar
iter improving translation edit rate through optimizable edit costs
in proceedings of the third conference on machine translation shared task papers pp

association for computational linguistics
url
aclweb
org anthology
kishore papineni salim roukos todd ward and wei jing zhu
bleu a method for automatic evaluation of machine translation
in proceedings of the annual meeting of the association for computational linguistics
association for computational linguistics
jae sung park marcus rohrbach trevor darrell and anna rohrbach
adversarial inference for multi sentence video description
corr

url


rebecca passonneau
measuring agreement on set valued items masi for semantic and matic annotation
in proceedings of the fifth international conference on language resources and evaluation genoa italy may
european language resources association elra
url
lrec conf
org proceedings
pdf
ramakanth pasunuru and mohit bansal
multi task video captioning with video and entailment generation
corr

url
org

tom pelsmaeker and wilker aziz
effective estimation of deep generative language models
corr

url
org

baolin peng chenguang zhu chunyuan li xiujun li and jianfeng gao
february
few shot natural language generation for task oriented
jinchao li michael zeng task oriented dialog
language generation for url
microsoft
com en us research few shot natural jeffrey pennington richard socher and christopher d
manning
glove global vectors for word representation
in empirical methods in natural language processing emnlp pp

url
aclweb
org anthology
matthew e
peters mark neumann mohit iyyer matt gardner christopher clark kenton lee and luke zettlemoyer
deep contextualized word representations
in proc
of naacl
maja popovic
chrf character n gram score for automatic mt evaluation
in proceedings of the tenth workshop on statistical machine translation pp
lisbon portugal september
association for computational linguistics


url https
aclweb
org anthology
maja popovic david vilar eleftherios avramidis and aljoscha burchardt
evaluation without references scores as evaluation metrics
pp

chris quirk chris brockett and william dolan
monolingual machine translation for paraphrase generation
emnlp
alec radford karthik narasimhan tim salimans and ilya sutskever
improving language derstanding by generative pre training
url us
amazonaws
com assets researchcovers languageunsupervised language understanding paper
pdf
alec radford jeff wu rewon child david luan dario amodei and ilya sutskever
language models are unsupervised multitask learners

pranav rajpurkar jian zhang konstantin lopyrev and percy liang
squad questions for machine comprehension of text
in proceedings of the conference on empirical methods in natural language processing emnlp
association for computational linguistics
hannah rashkin asli celikyilmaz yejin choi and jianfeng gao
plotmachines conditioned generation with dynamic plot state tracking
in arxiv
nils reimers and iryna gurevych
sentence bert sentence embeddings using siamese networks
katharina reinecke and krzysztof z
gajos
labinthewild conducting large scale online ments with uncompensated samples
in cscw
ehud reiter
a structured review of the validity of bleu
computational linguistics september
url
aclweb
org anthology
ehud reiter
ehud reiter s blog
url
com blog
ehud reiter and anja belz
an investigation into the validity of some metrics for automatically evaluating natural language generation systems
computational linguistics
ehud reiter and anja belz
an investigation into the validity of some metrics for automatically evaluating natural language generation systems
comput
linguist
december
url

coli




ehud reiter and robert dale
building applied natural language generation systems
cambridge university press cambridge uk
ehud reiter roma robertson and liesl osman
lessons from a failure generating tailored ing cessation letters
artif
intell

marco tulio ribeiro sameer singh and carlos guestrin
why should i trust you explaining the predictions of any classier
corr

url
org

marco tulio ribeiro sameer singh and carlos guestrin
anchors high precision model agnostic explanations
in aaai

brian richards
type token ratios what do they really tell us journal of child language alan ritter colin cherry cial media
uary
data driven response generation in social
data driven response generation in in empirical methods in natural language processing emnlp url
microsoft
com en us research and bill dolan
m
roemmele a
gordon and r
swanson
evaluating story generation systems using automated linguistic analyses
workshop on machine learning for creativity at the sigkdd ence on knowledge discovery and data mining kdd
antti veikko i
rosti spyros matsoukas and richard schwartz
improved word level system bination for machine translation
in proc
of acl pp

y
rubner c
tomasi and l
j
guibas
a metric for distributions with applications to image databases
in ieee
sebastian schuster ranjay krishna angel chang li fei fei and christopher d
manning
erating semantically precise scene graphs from textual descriptions for improved image retrieval
in proceedings of the fourth workshop on vision and language pp
lisbon portugal september
association for computational linguistics
url
aclweb
anthology
tal schuster darsh shah yun jie serene yeo daniel roberto filizzola ortiz enrico santus and regina barzilay
towards debiasing fact verication models
proceedings of the conference on empirical methods in natural language processing and the international joint ence on natural language processing emnlp ijcnlp


url



william a
scott
reliability of content analysis the case of nominal scale coding
the issn
url http public opinion quarterly

jstor
org
joao sedoc daphne ippolito arun kirubarajan jai thirani lyle ungar and chris callison burch
chateval a tool for chatbot evaluation

abigail see peter j
liu and christopher d
manning
get to the point summarization with in proceedings of the annual meeting of the association pointer generator networks
for computational linguistics volume long papers pp
vancouver canada july
association for computational linguistics
url
aclweb
anthology
thibault sellam dipanjan das and ankur p
parikh
bleurt learning robust metrics for text generation
stanislau semeniuta aliaksei severyn and sylvain gelly
on accurate evaluation of gans for guage generation
corr

url
org

stanislau semeniuta aliaksei severyn and sylvain gelly
on accurate evaluation of gans for language generation
iclr
url
net
iulian vlad serban tim klinger gerald tesauro kartik talamadupula bowen zhou yoshua gio and aaron c
courville
multiresolution recurrent neural networks an application to alogue response generation
corr

url
org

iulian vlad serban alessandro sordoni ryan lowe laurent charlin joelle pineau aaron c
courville and yoshua bengio
a hierarchical latent variable encoder decoder model for ing dialogues
corr

url
org

naeha sharif lyndon white mohammed bennamoun and syed afaq ali shah
learning based composite metrics for improved caption evaluation
in proceedings of acl student search workshop pp
melbourne australia july
association for computational linguistics
url
aclweb
org anthology
tian shi yaser keneshloo naren ramakrishnan and chandan k
reddy
neural abstractive text summarization with sequence to sequence models
corr

url http
org

hiroki shimanaka tomoyuki kajiwara and mamoru komachi
ruse regressor using sentence embeddings for automatic machine translation evaluation
in proceedings of the third conference on machine translation shared task papers pp
belgium brussels october
association for computational linguistics
url
aclweb
org
abhisek singh and wei jin
ranking summaries for informativeness and coherence withouth erence summaries
proceedings of the twenty ninth international florida artical intelligent research society conference
matthew snover bonnie dorr richard schwartz linnea micciulla and john makhoul
a study of translation edit rate with targeted human annotation
pp

m
sporleder c
lapata
discourse chunking and its application to sentence compression
ceedings of hlt emnlp pp

peter stanchev weiyue wang and hermann ney
eed extended edit distance measure for machine translation
in proceedings of the fourth conference on machine translation volume shared task papers day pp
florence italy august
association for computational linguistics
url
aclweb
org anthology
manfred stede and carla umbach
dimlex a lexicon of discourse markers for text generation and understanding
in proceedings of the annual meeting of the association for computational linguistics and international conference on computational linguistics volume acl coling pp
usa
association for computational linguistics
josef steinberger and karel jezek
evaluation measures for text summarization
computing and informatics
k
steinberger j
jezek
sentence compression for the lsa based summarizer
proceedings of the international conference on information systems implementation and modelling pp

octavia maria sulea
recognizing textual entailment in twitter using word embeddings
in workshop on evaluating vector space representations for nlp
yu sun shuohuan wang yu kun li shikun feng hao tian hua wu and haifeng wang
ernie
a continual pre training framework for language understanding
corr

url
org

manne suneetha and sheerin fatima
extraction based automatic text summarization system with hmm tagger
international journal of soft computing and engineering issn
ilya sutskever oriol vinyals and quoc v
le
sequence to sequence learning with neural networks
corr

url
org

liling tan jon dehdari and josef van genabith
an awkward disparity between bleu ribes scores and human judgements in machine translation
in proceedings of the workshop on asian translation pp
kyoto japan october
workshop on asian lation
url
aclweb
org anthology
rachel own evaluating text output in nlp bleu at your own risk
your
tatman
risk evaluating output
url bleu nlp text in at james thorne andreas vlachos christos christodoulopoulos and arpit mittal
generating level explanations for natural language inference
corr

url http
org

c
tillmann s
vogel h
ney a
zubiaga and h
sawaf
accelerated dp based search for statistical translation
in in european conf
on speech communication and technology pp

jesus tomas josep angel mas and francisco casacuberta
a quantitative method for machine in proceedings of the eacl workshop on evaluation initiatives translation evaluation
in natural language processing are evaluation methods metrics and resources reusable pp
columbus ohio april
association for computational linguistics
url https
aclweb
org anthology
a
m
turing
computing machinary and intelligence
mind
url

mind lix


chris van der lee albert gatt emiel van miltenburg sander wubben and emiel krahmer
best practices for the human evaluation of automatically generated text
in proceedings of the international conference on natural language generation
url
aclweb
org anthology
ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan n gomez in neurips pp
ukasz kaiser and illia polosukhin
attention is all you need

ramakrishna vedantam c
lawrence zitnick and devi parikh
cider consensus based image description evaluation
corr

url
org

oriol vinyals alexander toshev samy bengio and dumitru erhan
show and tell a neural image caption generator
corr

url
org

oriol vinyals meire fortunato and navdeep jaitly
pointer networks

nguyen vo and kyumin lee
learning from fact checkers analysis and generation of fact checking language
in proceedings of the international acm sigir conference on research and velopment in information retrieval pp
new york ny usa
ation for computing machinery
isbn



url



alex wang kyunghyun cho and mike lewis
asking and answering questions to evaluate the factual consistency of summaries

weiyue wang jan thorsten peter hendrik rosendahl and hermann ney
character translation in proceedings of the first conference on machine translation edit rate on character level
volume shared task papers pp
berlin germany august
association for computational linguistics
url
aclweb
org anthology
sean welleck ilia kulikov stephen roller emily dinan kyunghyun cho and jason weston
neural text generation with unlikelihood training

billy t
m
wong and chunyu kit
extending machine translation evaluation metrics with lexical cohesion to document level
pp

lingfei wu ian en hsu yen kun xu fangli xu avinash balakrishnan pin yu chen pradeep ravikumar and michael j
witbrock
word mover s embedding from to document embedding
in emnlp
yu yan weizhen qi yeyun gong dayiheng liu nan duan jiusheng chen ruofei zhang and ming zhou
prophetnet predicting future n gram for sequence to sequence pre training
arxiv

qian yang rebecca j
passonneau and gerard de melo
peak pyramid evaluation via automated knowledge extraction
in proceedings of the thirtieth aaai conference on articial intelligence
lili yao nanyun peng ralph m
weischedel kevin knight dongyan zhao and rui yan
and write towards better automatic storytelling
corr

url http
org

yasuhisa yoshida jun suzuki tsutomu hirao and masaaki nagata
dependency based in proceedings of the conference on course parser for single document summarization
empirical methods in natural language processing emnlp pp
doha qatar october
association for computational linguistics
url
aclweb
anthology
r
michael young
using grice s maxim of quantity to select the content of plan descriptions
artif
intell

hui yu xiaofeng wu jun xie wenbin jiang qun liu and shouxun lin
red a reference in proceedings of coling the dependency based mt evaluation metric
tional conference on computational linguistics technical papers pp
dublin land august
dublin city university and association for computational linguistics
url
aclweb
org anthology
hui yu xiaofeng wu wenbin jiang qun liu and shouxun lin
an automatic machine translation evaluation metric based on dependency parsing model

xingdi yuan tong wang caglar gulcehre alessandro sordoni philip bachman saizheng zhang sandeep subramanian and adam trischler
machine comprehension by text to text neural tion generation
in proceedings of the workshop on representation learning for nlp pp

association for computational linguistics august
url
aclweb
org anthology
rowan zellers ari holtzman hannah rashkin yonatan bisk ali farhadi franziska roesner and yejin choi
defending against neural fake news
in neurips
rowan zellers ari holtzman elizabeth clark lianhui qin ali farhadi and yejin choi
evaluating machines by their real world language use
arxiv

jacob zerr
question generation using part of speech information
final report for reu program at uccs
url
uccs
work reu finalpapers zerr
pdf
qiuyun zhang bin guo hao wang yunji liang shaoyang hao and zhiwen yu
ai powered text generation for harmonious human machine interaction current state and future directions
corr

url
org

tianyi zhang varsha kishore felix wu kilian q
weinberger and yoav artzi
bertscore ating text generation with bert
in international conference on learning representations
url
net
ying zhang stephan vogel and alex waibel
interpreting bleu nist scores how much ment do we need to have a better system
in in proceedings of proceedings of language resources and evaluation pp

yuhao zhang derek merck emily bao tsai christopher d
manning and curtis p
langlotz
optimizing the factual correctness of a summary a study of summarizing radiology reports
arxiv

wei zhao maxime peyrard fei liu yang gao christian m
meyer and steffen eger
moverscore text generation evaluating with contextualized embeddings and earth mover distance
in emnlp
li zhou jianfeng gao di li and heung yeung shum
the design and implementation of xiaoice an empathetic social chatbot
computational linguistics
wangchunshu zhou and ke xu
learning to compare for better training and evaluation of open domain natural language generation models
yaoming zhu sidi lu lei zheng jiaxian guo weinan zhang jun wang and yong yu
texygen a benchmarking platform for text generation models
corr

url http
org


