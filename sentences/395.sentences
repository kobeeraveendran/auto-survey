v o n l c
s c v s c v i x r a machine learning of generic and user focused summarization inderjeet mani and eric bloedorn the mitre corporation sunset hills road reston va usa
org abstract a key problem in text summarization is ing a salience function which determines what information in the source should be included in the summary
this paper describes the use of machine learning on a training corpus of uments and their abstracts to discover salience functions which describe what combination of features is optimal for a given summarization task
the method addresses both generic and user focused summaries
with the mushrooming of the quantity of on line text information triggered in part by the growth of the world wide web it is especially useful to have tools which can help users digest information content
text summarization attempts to address this need by ing a partially structured source text extracting formation content from it and presenting the most important content to the user in a manner sensitive to the user s or application s needs
the end result is a condensed version of the original
a key problem in summarization is determining what information in the source should be included in the summary
this mination of the salience of information in the source i
e
a salience function for the text depends on a including the nature number of interacting factors and genre of the source text the desired compression summary length as a percentage of source length and the application s information needs
these mation needs include the reader s interests and tise suggesting a distinction between user focused versus generic summaries and the use to which the summary is being put for example whether it is intended to alert the user as to the source tent the indicative function or to stand in place of the source the informative function or even to oer a critique of the source the evaluative function sparck jones
american association for articial intelligence www
aaai
org
all rights reserved
a considerable body of research over the last forty years has explored dierent levels of analysis of text to help determine what information in the text is salient for a given summarization task
the salience functions are usually sentence lters i
e
methods for scoring sentences in the source text based on the contribution of dierent features
these features have included for example location edmundson paice and jones statistical measures of term prominence luhn brandow mitze and rau ical structure miike et al
marcu ilarity between sentences skorokhodko ence or absence of certain syntactic features pollock and zamora presence of proper names kupiec pedersen and chen and measures of nence of certain semantic concepts and relationships paice and jones maybury fum guida and tasso
in general it appears that a number of features drawn from dierent levels of analysis may combine together to contribute to salience
further the importance of a particular feature can of course vary with the genre of text
consider for example the feature of text location
in newswire texts the most common narrative style volves lead in text which oers a summary of the main news item
as a result for most varieties of newswire summarization methods which use leading text alone tend to outperform other methods brandow mitze and rau
however even within these varieties of newswire more anecdotal lead ins or multi topic articles do not fare well with a leading text approach in other genres brandow mitze and rau
other locations are salient for scientic and cal articles both introduction and conclusion sections might contain pre summarized material in tv news broadcasts one nds segments which contain trailing information summarizing a forthcoming segment
viously if we wish to develop a summarization system that could adapt to dierent genres it is important to have an automatic way of nding out what location values are useful for that genre and how it should instead of be combined with other features
ing and combining these features in an adhoc manner which would require re adjustment for each new genre of text a natural suggestion would be to use machine learning on a training corpus of documents and their abstracts to discover salience functions which describe what combination of features is optimal for a given summarization task
this is the basis for the able approach to summarization
now if the training corpus contains generic stracts i
e
abstracts written by their authors or by professional abstractors with the goal of dissemination to a particular usually broad readership nity the salience function discovered would be one which describes a feature combination for generic maries
likewise if the training corpus contains focused abstracts i
e
abstracts relating information in the document to a particular user interest which could change over time then then we learn a function for user focused summaries
while generic abstracts have traditionally served as surrogates for full text as our computing environments continue to date increased full text searching browsing and sonalized information ltering user focused abstracts have assumed increased importance
thus algorithms which can learn both kinds of summaries are highly relevant to current information needs
of course it would be of interest to nd out what sort of overlap exists between the features learnt in the two cases
in this paper we describe a machine learning proach which learns both generic summaries and focused ones
our focus is on machine learning pects in particular performance level comparison tween dierent learning methods stability of the ing under dierent compression rates and ships between rules learnt in the generic and the focused case
overall approach in our approach a summary is treated as a tation of the user s information need in other words as a query
the training procedure assumes we are provided with training data consisting of a collection of texts and their abstracts
the training procedure rst assigns each source sentence a relevance score dicating how relevant it is to the query
in the basic boolean labeling form of this procedure all source sentences above a particular relevance threshold are treated as summary sentences
the source sentences are represented in terms of their feature descriptions with summary sentences being labeled as positive examples
the training sentences positive and tive examples are fed to machine learning algorithms which construct a rule or function which labels any new sentence s feature vector as a summary vector or in the generic summary case the training not
stracts are generic in our corpus they are written abstracts of the articles
in the user focused case the training abstract for each document is erated automatically from a specication of a user formation need
it is worth distinguishing this approach from other previous work in trainable summarization in ular that of kupiec pedersen and chen at xerox parc referred to henceforth as parc an proach which has since been followed by teufel and moens
first our goal is to learn rules which can be easily edited by humans
second our proach is aimed at both generic summaries as well as user focused summaries thereby extending the generic summary orientation of the parc work
third by treating the abstract as a query we match the tire abstract to each sentence in the source instead of matching individual sentences in the abstract to one or more sentences in the source
this tactic seems sensible since the distribution of the ideas in the stract across sentences of the abstract is not of sic interest
further it completely avoids the rather tricky problem of sentence alignment including sideration of cases where more than one sentence in the source may match a sentence in the abstract which the parc approach has to deal with
also we do not make strong assumptions of independence of features which the parc based work which uses bayes rule does assume
other trainable approaches include lin and hovy in that approach what is learnt from training is a series of sentence positions
in our case we learn rules dened over a variety of features lowing for a more abstract characterization of rization
finally the learning process does not require any manual tagging of text for generic summaries it requires that generic abstracts be available and for user focused abstracts we require only that the user select documents that match her interests
features the set of features studied here are encoded as in ble where they are grouped into three classes
cation features exploit the structure of the text at ferent shallow levels of analysis
consider the matic
the feature based on proper names is extracted using sra s nametag krupka a system
we also use a feature based on the standard tf
idf metric the weight k l of term in document i given corpus l is given by k l tfik
where tfik frequency of term in document i vided by the maximum frequency of any term in ment i dfk number of documents in l in which term k occurs n total number of documents in l
while sorts all the sentences in the document by the feature in question
it assigns to the current sentence i it belongs in top c of the scored sentences where pression rate
as it turns out removing this discretization lter completely to use raw scores for each feature merely increases the complexity of learnt rules without improving performance feature sent loc para para loc section sent special section depth sent section values description sentence occurs in rst middle or last third of para sentence occurs in rst middle or last third of section if sentence occurs in introduction if in conclusion if in other if sentence is a top level section if sentence is a subsubsubsection location features sent in highest tf sent in highest tf
idf sent in highest sent in highest title sent in highest pname sent in highest syn sent in highest co occ thematic features average tf score filter average tf
idf score filter average score filter number of section heading or title term mentions and filter number of name mentions filter cohesion features number of unique sentences with a synonym link to sentence filter number of unique sentences with a co occurrence link to sentence filter table text features the tf
idf metric is standard there are some tics that are perhaps better suited for small data sets dunning
the statistic indicates the lihood that the frequency of a term in a document is greater than what would be expected from its quency in the corpus given the relative sizes of the document and the corpus
the version we use here based on cohen uses the raw frequency of a term in a document its frequency in the corpus the number of terms in the document and the sum of all term counts in the corpus
we now turn to features based on cohesion
text cohesion halliday and hasan involves relations between words or referring expressions which mine how tightly connected the text is
cohesion is brought about by linguistic devices such as repetition synonymy anaphora and ellipsis
models of text hesion have been explored in application to tion retrieval salton et al
where paragraphs which are similar above some threshold to many other paragraphs i
e
bushy nodes are considered likely to be salient
text cohesion has also been applied to the explication of discourse structure morris and hirst hearst and has been the focus of newed interest in text summarization boguraev and kennedy mani and bloedorn and elhadad
in our work we use two based features synonymy and co occurrence based on bigram statistics
to compute synonyms the gorithm uses wordnet miller comparing tentful nouns their contentfulness determined by a function word stoplist as to whether they have a synset in common nouns are extracted by the bic part of speech tagger aberdeen et al

occurrence scores between contentful words up to words apart are computed using a standard mutual information metric fano church and hanks the mutual information between terms j and in document i is mutinf k i ln nitfji ki tfjitfki where tfji ki maximum frequency of bigram jk in document i tfji frequency of term j in document i ni total number of terms in document i
the document in question is the entire cmp lg corpus
the co occurrence table only stores scores for tf counts greater than and mutinfo scores greater than
training data we use a training corpus of computational tics texts
these are full text articles and for the generic summarizer their author supplied stracts all from the computation and language print archive cmp lg provided in sgml form by the university of edinburgh
the articles are tween and pages in length and have gures captions references and cross references replaced by place holders
the average compression rate for stracts in this corpus is
once the sentences in each text extracted using a sentence tagger aberdeen et al
are coded as feature vectors they are labeled with respect to evance to the text s abstract
the labeling function uses the following similarity metric n
p
p qp where is the tf
idf weight of word i in sentence is the number of words in common between and and is the total number of words in and
in labeling the top where c is the compression rate of the relevance ranked sentences for a document are then picked as positive examples for that ment
this resulted in training vectors with considerably redundancy among them which when moved yielded unique vectors since the learning implementations we used ignore duplicate vectors of which were positive and the others negative
the positive vectors along with a random subset of metric predictive accuracy number of testing examples classied correctly denition precision recall balanced f score total number of test examples
number of positive examples classied correctly number of examples classied positive during testing number of positive examples classied correctly number known positive during testing recision recision recall table metrics used to measure learning performance negative were collected together to form balanced training data of examples
the labeled vectors are then input to the learning methods
some preliminary data analysis on the generic training data indicates that except for the two sion features there is a signicant dierence between the summary and non summary counts for some ture value of each feature test

this suggests that this is a reasonable set of features for the problem even though dierent learning methods may disagree on importance of individual features
generating user focused training abstracts the overall information need for a user is dened by a set of documents
here a subject was told to pick a sample of documents from the cmp lg which matched his interests
the top content words were tracted from each document scored by the score with the cmp lg corpus as the background corpus
then a centroid vector for the user interest uments was generated as follows
words for all the documents were sorted by their scores scores were averaged for words occurring in multiple documents
all words more than
standard deviations above the mean of these words scores were treated as a sentation of the user s interest or topic there were such words
next the topic was used in a ing activation algorithm based on mani and bloedorn to discover in each document in the cmp lg pus words related to the topic
once the words in each of the corpus documents have been reweighted by spreading activation each sentence is weighted based on the average of its word weights
the top where c is the compression rate of these sentences are then picked as positive examples for each document together constituting a user focused abstract or extract for that ment
further to allow for user focused features to be learnt each sentence s vector is extended with two additional user interest specic features the number of reweighted words called keywords in the sentence as well as the number of keywords per contentful word in the
note that the keywords while ing terms in the user focused abstract include many do nt use specic keywords as features as we would prefer to learn rules which could transfer across interests
other related terms as well
learning methods we use three dierent learning algorithms ized canonical discriminant function scdf sis spss
rules quinlan and wnek et al

scdf is a multiple sion technique which creates a linear function that maximally discriminates between summary and summary examples
while this method unlike the other two does nt have the advantage of generating logical rules that can easily be edited by a user it fers a relatively simple method of telling us to what extent the data is linearly separable
results the metrics for the learning algorithms used are shown in table
in table we show results averaged over ten runs of training and test where the test data across runs is
interestingly in the
learning of generic maries on this corpus the thematic and cohesion tures are referenced mostly in rules for the negative class while the location and tf features are referenced mostly in rules for the positive class
in the focused summary learning the number of keywords in the sentence is the single feature responsible for the dramatic improvement in learning performance pared to generic summary learning here the rules have this feature alone or in combination with tests on cational features
user focused interests tend to use a subset of the locational features found in generic terests along with user specic keyword features
now scdf does almost as well as
rules for the user focused case
this is because the keywords feature is most inuential in rules learnt by either algorithm
however not all the positive user focused examples which have signicant values for the keywords feature are linearly separable from the negative ones in cases where they are nt the other algorithms yield useful rules which include keywords along with other features
in the generic case the positive examples are linearly separable to a much lesser extent
overall although our gures are higher the ported by parc their performance metric is based on uses a holdout of document
method scdf generic scdf user focused aq generic aq user focused
rules generic pruned
rules user focused pruned predictive accuracy





precision recall











f score





table accuracy of learning algorithms at compression overlap between positively labeled sentences and vidual sentences in the abstract whereas ours is based on overlap with the abstract as a whole making it cult to compare
it is worth noting that the most eective features in our generic learning are a subset of the parc features with the cohesion features tributing little to overall performance
however note that unlike the parc work we do not avail of tor phrases which are known to be genre dependent
in recent work using a similar overlap metric teufel and moens reports that the indicator phrase ture is the single most important feature for accurate learning performance in a sentence extraction task ing this corpus it is striking that we get good learning performance without exploiting this feature
analysis of
rules learning curves generated at compression reveal some learning improvement in the generic case

predictive accuracy and

f score whereas the user focused case reaches a plateau very early

predictive curacy and f score
this again may be attributed to the relative dominance of the keyword feature
we also found surprisingly little change in learning mance as we move from to compression
these latter results suggests that this approach maintains high performance over a certain spectrum of summary sizes
inspection of the rules shows that the learning system is learning similar rather than dierent rules across compression rates
some example rules are as follows if the sentence is in the conclusion and it is a high tf
idf sentence then it is a summary sentence

generic rule run compression
if the sentence is in a section of depth and the number of keywords is between and and the keyword to content word ratio is between
and
inclusive then it is a summary sentence
aq user focused rule run compression
as can be seen the learnt rules are highly ble and thus are easily edited by humans if desired in contrast with approaches such as scdf or naive bayes which learn a mathematical function
in tice this becomes useful because a human may use the learning methods to generate an initial set of rules whose performance may then be evaluated on the data as well as against intuition leading to improved formance
conclusion we have described a based machine learning approach to produce generic and user specic maries
this approach shows encouraging learning performance
the rules learnt for user focused ests tend to use a subset of the locational features found in rules for generic interests along with specic keyword features
the rules are intelligible making them suitable for human use
the approach is widely applicable as it does not require manual ging or sentence level text alignment
in the future we expect to also investigate the use of regression niques based on a continuous rather than boolean beling function
of course since learning the ing function does nt tell us anything about how useful the summaries themselves are we plan to carry out a task based evaluation of the summaries
finally we intend to apply this approach to other genres of text as well as languages such as thai and chinese
acknowledgments we are indebted to simone teufel marc moens and byron georgantopoulos university of edinburgh for providing us with the cmp lg corpus and to barbara gates mitre for helping with the co occurrence data
references aberdeen j
burger j
day d
hirschman l
robinson p
and vilain m
mitre description of the alembic system used for
in ings of the sixth message understanding conference columbia maryland november
barzilay r
and elhadad m
using lexical chains for text summarization in mani i
and bury m
eds
proceedings of the acl workshop on intelligent scalable text tion madrid spain pp

salience based boguraev b
and kennedy c
content characterization of text documents in mani i
and maybury m
eds
proceedings of the acl workshop on intelligent scalable text summarization madrid spain pp

maybury m
generating summaries from event data information processing and management
miike s
itoh e
ono k
and sumita k
a text retrieval system with a dynamic abstract eration function proceedings of acm dublin ireland
miller g
wordnet a lexical database for english
in communications of the acm
morris j
and hirst g
lexical cohesion puted by thesaural relations as an indicator of the structure of text computational linguistics pp

paice c
and jones p
the identication of portant concepts in highly structured technical pers proceedings of acm pittsburgh pa
pollock j
and zamora a
automatic abstracting research at chemical abstracts service journal of chemical information and computer sciences
quinlan j

programs for machine learning morgan kaufmann san mateo ca
salton g
allan j
buckley c
and singhal a
tomatic analysis theme generation and rization of machine readable texts science june pp

skorokhodko e
f
adaptive method of automatic abstracting and indexing ifip congress ljubljana yugoslavia pp

sparck jones k
summarizing where are we now where should we go in mani i
and maybury m
eds
proceedings of the acl workshop on intelligent scalable text summarization madrid spain
spss base
applications guide spss inc
chicago
wnek k
bloedorn e
and michalski r
tive inductive learning method the method and user s guide
machine learning and inference laboratory report george mason sity fairfax virginia
teufel s
and moens m sentence extraction and rhetorical classication for flexible abstracts in working notes of the aaai spring symposium on intelligent text summarization spring nical report aaai
brandow r
mitze k
and rau l
automatic condensation of electronic publications by sentence selection
information processing and management
computation

gov cmp
and language e print archive kenneth church and patrick hanks
word ciation norms mutual information and phy
in proceedings of vancouver british columbia june
cohen j
d
hilights and independent automatic indexing terms for stracting journal of the american society for formation science
see also vol
for a very important erratum
dunning t
accurate methods for the statistics of surprise and coincidence computational tics
edmundson h
p
new methods in automatic stracting journal of the association for computing machinery pp

fano r
transmission of information mit press
fum d
guida g
and tasso c
evaluating portance a step towards text summarization ceedings of ijcai pp

halliday m
and hasan r
cohesion in text don longmans
hearst m
texttiling segmenting text into multi paragraph subtopic passages computational linguistics
george krupka
sra description of the sra system as used for
in proceedings of the sixth sage understanding conference columbia maryland november
julian kupiec jan pedersen and francine chen
a trainable document summarizer
in proceedings of acm seattle wa
lin c
y
and hovy e
h
identifying topics by sition proceedings of the applied natural language processing conference
luhn h
p
the automatic creation of literature stracts ibm journal of research and development
mani i
and bloedorn e
multi document rization by graph search and merging proceedings of the fourteenth national conference on articial intelligence providence ri pp

marcu d
from discourse structures to text maries in mani i
and maybury m
eds
ceedings of the acl workshop on ligent scalable text summarization madrid spain pp


