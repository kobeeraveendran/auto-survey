r a m l m

t a t s
v
v i x r a published as a conference paper at iclr maskgan better text generation via filling
in the william fedus ian goodfellow and andrew dai google brain goodfellow
abstract
neural text generation models are often autoregressive language models or models
these models generate text by sampling words sequentially with each word conditioned on the previous word and are state of the art for several machine translation and summarization benchmarks
these benchmarks are often dened by validation perplexity even though this is not a direct measure of the quality of the generated text
additionally these models are typically trained via mum likelihood and teacher forcing
these methods are well suited to optimizing perplexity but can result in poor sample quality since generating text requires tioning on sequences of words that may have never been observed at training time

we propose to improve sample quality using generative adversarial networks
gans which explicitly train the generator to produce high quality samples and have shown a lot of success in image generation
gans were originally designed to output differentiable values so discrete language generation is challenging for them
we claim that validation perplexity alone is not indicative of the quality of text generated by a model
we introduce an actor critic conditional gan that lls in missing text conditioned on the surrounding context
we show tively and quantitatively evidence that this produces more realistic conditional and unconditional text samples compared to a maximum likelihood trained model
introduction
recurrent neural networks rnns graves al are the most common generative model for sequences as well as for sequence labeling tasks
they have shown impressive results in language modeling mikolov et al machine translation wu et al and text classication miyato et al
text is typically generated from these models by sampling from a distribution that is conditioned on the previous word and a hidden state that consists of a representation of the words generated so far
these are typically trained with maximum likelihood in an approach known as teacher forcing where ground truth words are fed back into the model to be conditioned on for generating the following parts of the sentence
this causes problems when during sample generation the model is often forced to condition on sequences that were never conditioned on at training time

this leads to unpredictable dynamics in the hidden state of the rnn
methods such as professor forcing lamb et al and scheduled sampling bengio et al have been proposed to solve this issue
these approaches work indirectly by either causing the hidden state dynamics to become predictable professor forcing or by randomly conditioning on sampled words at training time however they do not directly specify a cost function on the output of the rnn that encourages high sample quality
our proposed method does so
generative adversarial networks gans goodfellow et al are a framework for training generative models in an adversarial setup with a generator generating images that is trying to fool a discriminator that is trained to discriminate between real and synthetic images
gans have had a lot of success in producing more realistic images than other approaches but they have only seen limited use for text sequences
this is due to the discrete nature of text making it infeasible to propagate the gradient from the discriminator back to the generator as in standard gan training
we overcome this by using reinforcement learning rl to train the generator while the discriminator is still trained via maximum likelihood and stochastic gradient descent
gans also commonly suffer from issues such published as a conference paper at iclr as training instability and mode dropping both of which are exacerbated in a textual setting
mode dropping occurs when certain modalities in the training set are rarely generated by the generator for example leading all generated images of a volcano to be multiple variants of the same volcano
this becomes a signicant problem in text generation since there are many complex modes in the data ranging from bigrams to short phrases to longer idioms
training stability is also an issue since unlike image generation text is generated autoregressively and thus the loss from the discriminator is only observed after a complete sentence has been generated
this problem compounds when generating longer and longer sentences
we reduce the impact of these problems by training our model on a text ll in the blank or task
this is similar to the task proposed in bowman et al

but we use a more robust setup
in this task portions of a body of text are deleted or redacted
the goal of the model is to then inll the missing portions of text so that it is indistinguishable from the original data
while text the model operates autoregressively over the tokens it has thus far lled in as in standard language modeling while conditioning on the true known context
if the entire body of text is redacted then this reduces to language modeling

designing error attribution per time step has been noted to be important in prior natural language gan research yu et al li et al
the text inlling task naturally achieves this consideration since our discriminator will evaluate each token and thus provide a ne grained supervision signal to the generator
consider for instance if the generator produces a sequence perfectly matching the data distribution over the rst t time steps but then produces an outlier token yt
despite
the entire sequence now being clearly synthetic as a result of the errant token a discriminative model that produces a high loss signal to the outlier token but not to the others will likely yield a more informative error signal to the generator
this research also opens further inquiry of conditional gan models in the context of natural language

in the following sections we introduce a text generation model trained on maskgan
consider the actor critic architecture in extremely large action spaces
consider new evaluation metrics and the generation of synthetic training data
related work
research into reliably extending gan training to discrete spaces and discrete sequences has been a highly active area
gan training in a continuous setting allows for fully differentiable computations permitting gradients to be passed through the discriminator to the generator
discrete elements break this differentiability leading researchers to either avoid the issue and reformulate the problem work in the continuous domain or to consider rl methods
seqgan
yu et al trains a language model by using policy gradients to train the generator to fool a cnn based discriminator that discriminates between real and synthetic text
both the generator and discriminator are pretrained on real and fake data before the phase of training with policy gradients
during training they then do monte carlo rollouts in order to get a useful loss signal per word
follow up work then demonstrated text generation without pretraining with rnns press et al
additionally zhang et al produced results with an rnn generator by matching
dimensional latent representations
professor forcing lamb et al is an alternative to training an rnn with teacher forcing by using a discriminator to discriminate the hidden states of a generator rnn that is conditioned on real and synthetic samples
since the discriminator only operates on hidden states gradients can be passed through to the generator so that the hidden state dynamics at inference time follow those at training time
gans have been applied to dialogue generation li et al showing improvements in adversarial evaluation and good results with human evaluation compared to a maximum likelihood trained baseline
their method applies reinforce with monte carlo sampling on the generator
published as a conference paper at iclr
replacing the non differentiable sampling operations with efcient gradient approximators jang al
not yet shown strong results with discrete gans
recent unbiased and low variance gradient estimate techniques such as tucker et al
may prove more effective
wgan gp gulrajani al avoids the issue of dealing with backpropagating through discrete nodes by generating text in a one shot manner using a convolutional network
hjelm et al

proposes an algorithmic solution and uses a boundary seeking gan objective along with importance sampling to generate text
in rajeswar et al
the discriminator operates directly on the continuous probabilistic output of the generator
however to accomplish this they recast the traditional autoregressive sampling of the text since the inputs to the rnn are predetermined
che al
instead optimize a lower variance objective using the discriminator s output rather than the standard gan objective
reinforcement learning methods have been explored successfully in natural language
using a reinforce and cross entropy hybrid mixer ranzato et al directly optimized bleu score and demonstrated improvements over baselines
more recently actor critic methods in natural language were explored in bahdanau et al
where instead of having rewards supplied by a discriminator in an adversarial setting the rewards are task specic scores such as bleu
conditional text generation via gan training has been explored in rajeswar et al

li et al


our work is distinct in that we employ an actor critic training procedure on a task designed to provide rewards at every time step li et al
we believe the may mitigate the problem of severe mode collapse
this task is also harder for the discriminator which reduces the risk of the generator contending with a near perfect discriminator
the critic in our method helps the generator converge more rapidly by reducing the high variance of the gradient updates in an extremely high action space environment when operating at word level in natural language
maskgan notation architecture let yt
denote pairs of input and target tokens
let m denote a masked token where the original token is replaced with a hidden token and let xt denote the lled in token
finally xt is a lled in token passed to the discriminator which may be either real or fake
the task of imputing missing tokens requires that our maskgan architecture condition on information from both the past and the future
we choose to use a sutskever et al architecture

our generator consists of an encoding module and decoding module
for a discrete sequence xt a binary mask is generated deterministically or stochastically of the same length mt where each mt selects which tokens will remain
the token at time t xt is then replaced with a special mask token m if the mask is and remains unchanged if the mask is

the encoder reads in the masked sequence which we denote as where the mask is applied element wise
the encoder provides access to future context for the maskgan during decoding

as in standard language modeling the decoder lls in the missing tokens auto regressively however it is now conditioned on both the masked text as well as what it has lled in up to that point

the generator decomposes the distribution over the sequence into an ordered conditional sequence
p xt


p

p



the discriminator has an identical architecture to the except that the output is a scalar probability at each time point rather than a distribution over the vocabulary size
the nator is given the lled in sequence from the generator but importantly it is given the original also tried cnn based discriminators but found that lstms performed the best published as a conference paper at iclr figure generator architecture
blue boxes represent known tokens and purple boxes are imputed tokens
we demonstrate a sampling operation via the dotted line
the encoder reads in a masked sequence where masked tokens are denoted by an underscore and then the decoder imputes the missing tokens by using the encoder hidden states
in this example the generator should ll in the alphabetical ordering a b c d e
real context
we give the discriminator the true context otherwise this algorithm has a critical failure mode
for instance without this context if the discriminator is given the lled in sequence the director director guided the series it will fail to reliably identify the director director bigram as fake text despite this bigram potentially never appearing in the training corpus aside from an errant typo
the reason is that it is ambiguous which of the two currences of director is fake the associate director guided the series or the director expertly guided the series are both potentially valid sequences

without the context of which words are real the discriminator was found to assign equal probability to both words
the result of course is an inaccurate learning signal to the generator which will not be correctly penalized for producing these bigrams
to prevent this our discriminator
d computes the probability of each token xt being real given the true context of the masked sequence
t

p xt xreal t t in our formulation the logarithm of the discriminator estimates are regarded as the reward rt log t
our third network is the critic network which is implemented as an additional head off the inator
the critic estimates the value function which is the discounted total return of the lled in sequence
rt s t srs where is the discount factor at each position in the training our model is not fully differentiable due to the sampling operations on the generator s probability distribution to produce the next token
therefore to train the generator we estimate the gradient with respect to its parameters via policy gradients sutton et al
reinforcement learning was rst employed to gans for language modeling in yu et al

analogously here the generator seeks to maximize the cumulative total reward r

rt
we optimize the parameters of the generator by performing gradient ascent on
using one of the reinforce family of algorithms we can nd an unbiased estimator of this as
rt log g xt
the variance of this gradient estimator may be reduced by using the learned value function as a baseline bt v t which is produced by the critic
this results in the generator gradient contribution for a single token xt
rt log g xt
in the nomenclature of rl the quantity rt bt may be interpreted as an estimate of the advantage st st
v st
here the action at is the token chosen by the generator at xt and source code available at master research maskgan published as a conference paper at iclr the state st are the current tokens produced up to that point st
this approach is an actor critic architecture where g determines the policy and the baseline bt is the critic sutton barto degris et al
for this task we design rewards at each time step for a single sequence in order to aid with credit assignment li et al
as a result a token generated at time step t will inuence the rewards received at that time step and subsequent time steps
our gradient for the generator will include contributions for each token lled in order to maximize the discounted total return r
rt

the full generator gradient is given by equation extg rt extg srs bt t t
t s t
intuitively this shows that the gradient to the generator associated with producing xt will depend on all the discounted future rewards s t assigned by the discriminator
for a non zero discount factor the generator will be penalized for greedily selecting a token that earns a high reward at that time step alone
then for one full sequence we sum over all generated words xt for t t

finally as in conventional gan training our discriminator will be updated according to the gradient m m

alternative approaches for long sequences and large vocabularies as an aside for other avenues we explored we highlight two particular problems of this task and plausible remedies
this task becomes more difcult with long sequences and with large vocabularies

to address the issue of extended sequence length we modify the core algorithm with a dynamic task

we apply our algorithm up to a maximum sequence length t however upon satisfying a convergence criterion we then increment the maximum sequence length to t and continue training
this allows the model to build up an ability to capture dependencies over shorter sequences before moving to longer dependencies as a form of curriculum learning

in order to alleviate issues of variance with reinforce methods in a large vocabulary size we consider a simple modication
at each time step instead of generating a reward only on the sampled token we instead seek to use the full information of the generator distribution
before sampling the generator produces a probability distribution over all tokens v
we compute the reward for each possible token v conditioned on what had been generated before
this incurs a computational penalty since the discriminator must now be used to predict over all tokens but if performed efciently the potential reduction in variance could be benecial
method details
prior to training we rst perform pretraining
first we train a language model using standard maximum likelihood training
we then use the pretrained language model weights for the encoder and decoder modules
with these language models we now pretrain the model on the task using maximum likelihood in particular the attention parameters as described in luong et al

we select the model producing the lowest validation perplexity on the masked task via a hyperparameter sweep over runs
initial algorithms did not include a critic but we found that the inclusion of the critic decreased the variance of our gradient estimates by an order of magnitude which substantially improved training
published as a conference paper at iclr evaluation
evaluation of generative models continues to be an open ended research question
we seek heuristic metrics that we believe will be correlated with human evaluation
bleu score papineni et al is used extensively in machine translation where one can compare the quality of candidate translations from the reference
motivated by this metric we compute the number of unique n grams produced by the generator that occur in the validation corpus for small
then we compute the geometric average over these metrics to get a unied view of the performance of the generator

from our maximum likelihood trained benchmark we were able to nd gan hyperparameter congurations that led to small decreases in validation perplexity on
however we found that these models did not yield considerable improvements to the sample quality so we abandoned trying to reduce validation perplexity
one of the biggest advantages of gan trained nlp models is that the generator can produce alternative yet realistic language samples but not be unfairly penalized by not producing with high likelihood the single correct sequence
as the generator explores off manifold in the free running mode it may nd alternative options that are valid but do not maximize the probability of the underlying sequence
we therefore choose not to focus on architectures or hyperparameter congurations that led to small reductions in validation perplexity but rather searched for those that improved our heuristic evaluation metrics
experiments
we present both conditional and unconditional samples generated on the ptb and imdb data sets at word level
maskgan refers to our gan trained variant and maskmle refers to our likelihood trained variant
additional samples are supplied in appendix the penn treebank ptb the penn treebank dataset marcus et al has a vocabulary of unique words
the training set contains words the validation set contains words and the test set contains words
for our experiments we train on the training partition

we rst pretrain the commonly used variational lstm language model with parameter dimensions common to maskgan following gal ghahramani to a validation perplexity of
after then loading the weights from the language model into the maskgan generator we further pretrain with a masking rate of half the text blanked to a validation perplexity of
finally we then pretrain the discriminator on the samples produced from the current generator and real training text
conditional samples

we produce samples conditioned on surrounding text in table
underlined sections of text are missing and have been lled in via either the maskgan or maskmle algorithm
ground truth the next day s show interactive telephone technology has taken a new leap in unk and television programmers are maskgan
the next day s show interactive telephone technology has taken a new leap in its retail business eos a the next day s show interactive telephone technology has taken a new leap in the complicate case of the table conditional samples from ptb for both maskgan and maskmle models
published as a conference paper at iclr
language model unconditional samples

we may also run maskgan in an unconditional mode where the entire context is blanked out thus making it equivalent to a language model
we present a language model sample in table and additional samples are included in the appendix
maskgan oct
n as the end of the year the resignations were approved eos the march n n unk was down table language model unconditional sample from ptb for maskgan

imdb movie dataset
the imdb dataset maas al
consists of movie reviews taken from imdb
each review may contain several sentences
the dataset is divided into labeled training instances labeled test instances and unlabeled training instances
the label indicates the sentiment of the review and may be either positive or negative
we use the rst words of each review in the training set to train our models which leads to a dataset of million words
identical to the training process in ptb we pretrain a language model to a validation perplexity of
after then loading the weights from the language model into the maskgan generator we further pretrain with masking rate of half the text blanked to a validation perplexity of

finally we then pretrain the discriminator on the samples produced from the current generator and real training text
conditional samples
here we compare maskgan and maskmle conditional language generation ability for the imdb dataset
ground truth pitch black was a complete shock to me when i rst saw it back in
in the previous years i maskgan
pitch black was a complete shock to me when i rst saw it back in
i was really looking forward black was a complete shock to me when i rst saw it back in i live in new zealand table
conditional samples from imdb for both maskgan and maskmle models
language model unconditional
samples
as in the case with ptb we generate imdb samples unconditionally equivalent to a language model

we present a sample in table and additional samples are included in the appendix

maskgan positive follow the good earth movie linked vacation is a comedy that credited against the modern day era yarns which has helpful something to the modern day s best it is an interesting drama based on a story of the famed table language model unconditional sample from imdb for maskgan
perplexity of generated samples
as of this date gan training has not achieved state of the art word level validation perplexity on the penn treebank dataset
rather the top performing models are still maximum likelihood trained published as a conference paper at iclr model perplexity of imdb samples under a pretrained lm
maskgan table the perplexity is calculated using a pre trained language model that is equivalent to the decoder in terms of architecture and size used in the maskmle and maskgan models
this language model was used to initialize both models
models such as the recent architectures found via neural architecture search in zoph le

an extensive hyperparameter search with maskgan further supported that gan training does not improve the validation perplexity results set via state of the art models
however we instead seek to understand the quality of the sample generation
as highlighted earlier a fundamental problem of generating in free running mode potentially leads to off manifold sequences which can result in poor sample quality for teacher forced models
we seek to quantitatively evaluate this dynamic present only during sampling
this is commonly done with bleu but as shown by wu et al
bleu is not necessarily correlated with sample quality
we believe the correlation may be even less in the task since there are many potential valid and bleu would penalize valid ones

instead we calculate the perplexity of the generated samples by maskgan and maskmle by using the language model that was used to initialize maskgan and maskmle
both maskgan and produce samples autoregressively free running mode building upon the previously sampled tokens to produce the distribution over the next

the maskgan model produces samples which are more likely under the initial model than the maskmle model
the maskmle model generates improbable sentences as assessed by the initial language model during inference as compounding sampling errors result in a recurrent hidden states that are never seen during teacher forcing lamb et al
conversely the maskgan model operates in a free running mode while training and this supports that it is more robust to these sampling perturbations
mode collapse
in contrast to image generation mode collapse can be measured by directly calculating certain n gram statistics
in this instance we measure mode collapse by the percentage of unique n grams in a set of generated imdb movie reviews
we unconditionally generate each sample consisting of words
this results in almost k total bi tri quad grams
model

unique bigrams unique trigrams unique quadgrams lm


maskgan table diversity statistics within unconditional samples of ptb news snippets words each
the results in table show that maskgan does show some mode collapse evidenced by the reduced number of unique quadgrams
however all complete samples taken as a sequence for all the models are still unique
we also observed during rl training an initial small drop in perplexity on the ground truth validation set but then a steady increase in perplexity as training progressed
despite
this sample quality remained relatively consistent
the nal samples were generated from a model that had a perplexity on the ground truth of
we hypothesize that mode dropping is occurring near the tail end of sequences since generated samples are unlikely to generate all the previous words correctly in order to properly model the distribution over words at the tail
theis et al
also shows how validation perplexity does not necessarily correlate with sample quality
published as a conference paper at iclr human evaluation
ultimately the evaluation of generative models is still best measured by unbiased human evaluation

therefore we evaluate the quality of the generated samples of our initial language model lm the maskmle model and the maskgan model in a blind heads up comparison using amazon mechanical turk
note that these models have the same number of parameters at inference time
we pay raters to compare the quality of two extracts along axes grammaticality topicality and overall quality
they are asked if the rst extract second extract or neither is higher quality
preferred model grammaticality topicality
overall table a mechanical turk blind heads up evaluation between pairs of models trained on imdb reviews
reviews each words long from each model are unconditionally sampled and randomized
raters are asked which sample is preferred between each pair
ratings were obtained for each model pair comparison

preferred model grammaticality topicality
overall
lm
maskgan lm
maskgan

real samples lm

real samples maskgan lm
maskgan lm
maskgan

seqgan

seqgan
maskgan table
a mechanical turk blind heads up evaluation between pairs of models trained on ptb
news snippets each words long from each model are unconditionally sampled and randomized

raters are asked which sample is preferred between each pair
ratings were obtained for each model pair comparison

the mechanical turk results show that maskgan generates superior human looking samples to maskmle on the imdb dataset
however on the smaller ptb dataset with word instead of word samples the results are closer
we also show results with seqgan trained with the same network size and vocabulary size as maskgan which show that maskgan produces superior samples to seqgan
published as a conference paper at iclr discussion
our work further supports the case for matching the training and inference procedures in order to produce higher quality language samples
the maskgan algorithm directly achieves this through gan training and improved the generated samples as assessed by human evaluators
in our experiments we generally found training where contiguous blocks of words were masked produced better samples
one conjecture is that this allows the generator an opportunity to explore longer sequences in a free running mode in comparison a random mask generally has shorter sequences of blanks to ll in so the gain of gan training is not as substantial
we found that policy gradient methods were effective in conjunction with a learned critic but the highly active research on training with discrete nodes may present even more stable training procedures
we also found the use of attention was important for the words to be sufciently conditioned on the input context
without attention the would ll in reasonable subsequences that became implausible in the context of the adjacent surrounding words
given this we suspect another promising avenue would be to consider gan training with attention only models as in vaswani et al

in general we think the proposed contiguous task is a good approach to reduce mode collapse and help with training stability for textual gans
we show that maskgan samples on a larger dataset imdb reviews is signicantly better than the corresponding tuned maskmle model as shown by human evaluation
we also show we can produce high quality samples despite the maskgan model having much higher perplexity on the ground truth test set
acknowledgements
we would like to thank george tucker jascha sohl dickstein jon shlens ryan sepassi jasmine collins irwan bello barret zoph gabe pereyra eric jang and the google brain team particularly the rst year residents who humored us listening and commenting on almost every conceivable variation of this core idea
published as a conference paper at iclr references dzmitry bahdanau philemon brakel kelvin xu anirudh goyal ryan lowe joelle pineau aaron courville and yoshua bengio
an actor critic algorithm for sequence prediction
in international conference on learning representations
samy bengio oriol vinyals navdeep jaitly and noam shazeer
scheduled sampling for sequence prediction with recurrent neural networks
in advances in neural information processing systems pp
yoshua bengio rejean ducharme pascal vincent and christian jauvin
a neural probabilistic language model
journal of machine learning research
samuel r bowman luke vilnis oriol vinyals andrew m dai rafal jozefowicz and samy bengio

generating sentences from a continuous space
in signll conference on computational natural language learning conll
tong che yanran li ruixiang zhang r devon hjelm wenjie li yangqiu song and yoshua bengio
maximum likelihood augmented discrete generative adversarial networks
arxiv preprint
thomas degris patrick m pilarski and richard s sutton
model free reinforcement learning with continuous action in practice
in american control conference acc pp


ieee

yarin gal and zoubin ghahramani
a theoretically grounded application of dropout in recurrent neural networks
in advances in neural information processing systems pp

ian goodfellow jean pouget abadie mehdi mirza bing xu david warde farley sherjil ozair aaron courville and yoshua bengio
generative adversarial nets
in advances in neural tion processing systems pp

alex graves et al
supervised sequence labelling with recurrent neural networks volume

springer
ishaan gulrajani faruk ahmed martin arjovsky vincent dumoulin and aaron courville
improved training of wasserstein gans
arxiv preprint
r devon hjelm athul paul jacob tong che kyunghyun cho and yoshua bengio
seeking generative adversarial networks
arxiv preprint
hakan inan khashayar khosravi and richard socher
tying word vectors and word classiers a loss framework for language modeling
in international conference on learning representations
eric jang shixiang gu and ben poole
categorical reparameterization with gumbel softmax
in international conference on learning representations

diederik kingma and jimmy ba
adam a method for stochastic optimization
in international conference on learning representations
alex lamb anirudh goyal ying zhang saizheng zhang aaron courville and yoshua bengio

in advances
in neural professor forcing a new algorithm for training recurrent networks

information processing systems pp

jiwei li will monroe tianlin shi alan ritter and dan jurafsky
adversarial learning for neural dialogue generation
in conference on empirical methods in natural language processing

minh thang luong hieu pham and christopher d manning
effective approaches to attention based neural machine translation
in conference on empirical methods in natural language processing pp
published as a conference paper at iclr andrew l maas raymond e daly peter t pham dan huang andrew y ng and christopher
potts
learning word vectors for sentiment analysis
in proceedings of the annual meeting of the association for computational linguistics human language technologies volume pp


association for computational linguistics

mitchell p marcus mary ann marcinkiewicz and beatrice santorini
building a large annotated corpus of english the penn treebank
computational linguistics

tomas mikolov martin karaat lukas burget jan and sanjeev khudanpur
recurrent neural network based language model
in interspeech volume pp

takeru miyato andrew m dai and ian goodfellow
virtual adversarial training for semi supervised text classication
in international conference on learning representations volume pp


kishore papineni salim roukos todd ward and wei jing zhu
bleu a method for automatic evaluation of machine translation
in proceedings of the annual meeting on association for computational linguistics pp

association for computational linguistics

or press and lior wolf
using the output embedding to improve language models

in
conference of the european chapter of the association for computational linguistics pp


or press amir bar ben bogin jonathan berant and lior wolf
language generation with recurrent generative adversarial networks without pre training
arxiv preprint

sai rajeswar sandeep subramanian francis dutil christopher pal and aaron courville
adversarial generation of natural language
in workshop on representation learning for nlp

marcaurelio ranzato sumit chopra michael auli and wojciech zaremba
sequence level training with recurrent neural networks
arxiv preprint
ilya sutskever oriol vinyals and quoc v le
sequence to sequence learning with neural networks

in advances in neural information processing systems pp

richard s sutton and andrew g barto
reinforcement learning an introduction volume
mit press cambridge
richard s sutton david a mcallester satinder p singh and yishay mansour
policy gradient ods for reinforcement learning with function approximation
in advances in neural information processing systems pp

lucas theis aaron van den oord and matthias bethge
a note on the evaluation of generative models
in international conference on learning representations
george tucker andriy mnih chris j maddison dieterich lawson and jascha sohl dickstein

rebar low variance unbiased gradient estimates for discrete latent variable models
in
conference on neural information processing systems nips
ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan n gomez lukasz kaiser and illia polosukhin
attention is all you need
in conference on neural information processing systems nips
yonghui wu mike schuster zhifeng chen quoc le mohammad norouzi wolfgang macherey maxim krikun yuan cao qin gao klaus macherey jeff klingner apurva shah melvin johnson xiaobing liu lukasz kaiser stephan gouws yoshikiyo kato taku kudo hideto kazawa keith stevens george kurian nishant patil wei wang cliff young jason smith jason riesa alex rudnick oriol vinyals greg corrado macduff hughes and jeffrey dean
google s neural machine translation system bridging the gap between human and machine translation
corr
url
lantao yu weinan zhang jun wang and yong yu
seqgan sequence generative adversarial nets with policy gradient
in association for the advancement of articial intelligence pp

published as a conference paper at iclr yizhe zhang zhe gan kai fan zhi chen ricardo henao dinghan shen and lawrence carin
adversarial feature matching for text generation
arxiv preprint
barret zoph and quoc v le
neural architecture search with reinforcement learning
in international conference on learning representations
published as a conference paper at iclr a training details
our model was trained with the adam method for stochastic optimization kingma ba with the default tensorow exponential decay rates of and
our model uses layers of unit lstms for both the generator and discriminator dimensional word embeddings variational dropout
we used bayesian hyperparameter tuning to tune the variational dropout rate and learning rates for the generator discriminator and critic
we perform gradient descent steps on the discriminator for every step on the generator and critic

we share the embedding and softmax weights of the generator as proposed in bengio et al

press wolf inan et al

furthermore to improve convergence speed we share the embeddings of the generator and the discriminator
additionally as noted in our architectural section our critic shares all of the discriminator parameters with the exception of the separate output head to estimate the value
both our generator and discriminator use variational recurrent dropout gal ghahramani
b additional samples the penn treebank ptb

we present additional samples on ptb here
conditional samples ground truth the next day s show interactive telephone technology has taken a new leap in unk and television programmers are maskgan
the next day s show interactive telephone technology has taken a new leap in its retail business eos a
the next day s show interactive telephone technology has long dominated the unk of the nation s largest economic
the next day s show interactive telephone technology has exercised a n n stake in the and france the next day s show interactive telephone technology has taken a new leap in the complicate case of the
the next day s show interactive telephone technology has been unk in a number of clients estimates mountain bike the next day s show interactive telephone technology has instituted a week of unk by unk unk wis
auto we also consider lling in on non continguous masks below
ground truth president of the united states ronald reagan delivered his unk address to the nation president reagan addressed several sues maskgan president of the united states and congress delivered his unk address to the nation mr
reagan addressed several issues president of the united states have been delivered his unk address to the nation mr
reagan addressed several issues
language model unconditional samples
we present additional language model unconditional samples on ptb here
we modied seqgan to train and generate ptb samples using the same size architecture for the generator as in the maskgan generator and present samples here with maskgan samples
published as a conference paper at iclr maskgan
seqgan a unk basis despite the huge after tax interest income unk from n million eos in west germany n n the world s most corrupt organizations act as a multibillion dollar unk atmosphere or the metropolitan zone historic array with their are removed eos another takeover target lin s directors attempted through october
unk and british airways is allowed three funds cineplex odeon corp
shares made fresh out of the group purchase one part of a revised class of unk british
there are unk unk and unk about the unk seed eos
they use pcs are unk and their performance eos
imdb movie dataset we present additional samples on imdb here

conditional samples ground truth pitch black was a complete shock to me when i rst saw it back in
in the previous years i maskgan
pitch black was a complete shock to me when i rst saw it back in
i was really looking forward pitch black was a complete shock to me when i rst saw it back in
the promos were very well pitch black was a complete shock to me when i rst saw it back in the days when i was a black was a complete shock to me when i rst saw it back in i live in new zealand pitch black was a complete shock to me when i rst saw it back in
it was funny all interiors pitch black was a complete shock to me when i rst saw it back in the day
and i was in
language model unconditional samples
we present additional language model unconditional samples from maskgan on imdb here

positive follow the good earth movie linked vacation is a comedy that credited against the modern day era yarns which has helpful something to the modern day s
best it is an interesting drama based on a story of the famed
negative i really can t understand what this movie falls like i was seeing it i m sorry to say that the only reason i watched it was because of the casting of the
emperor i was not expecting anything as negative that s about so much time in time a lm that persevered to become cast in a very good way i didn t realize that the book was made during the the story was manhattan the allies were to c failure modes
here we explore various failure modes of the maskgan model which show up under certain bad hyperparameter settings
published as a conference paper at iclr mode collapse as widely witnessed in gan training we also nd a common failure of mode collapse across various n gram levels
the mode collapse may not be as extreme to collapse at a gram level ddddddd as described by gulrajani et al
but it may manifest as grammatical albeit inanely repetitive phrases for example
it is a very funny lm that is very funny
it s a very funny movie and it s charming
it
of course the discriminator may discern this as an out of distribution sample however in certain failure modes we observed the generator to move between common modes frequently present in the text
matching syntax at boundaries
we notice that the maskgan architecture often struggles to produce syntactically correct sequences when there is a hard boundary where it must end
this is also a relatively challenging task for humans because the lled in text must not only be contextual but also match syntactically at the boundary between the blank and where the text is present over a xed number of words
cartoon is one of those lms me when i rst saw it back in as noted in this failure mode the intersection between the lled in text and the present text is non grammatical
loss of global context
similar to failure modes present in gan image generation the produced samples often can lose global coherence despite being sensible locally
we expect a larger capacity model can mitigate some of these issues
this movie is terrible
the plot is ludicrous
the title is not more interesting and original
this is a great movie
lord of the rings was a great movie john travolta is brilliant published as a conference paper at iclr n gram metrics may be misleading proxies
in the absence of a global scalar objective to optimize while training we monitor various n gram language statistics to assess performance
however these only are crude proxies of the quality of the produced samples
gram perplexity figure particular failure mode succeeding in the optimization of a gram metric at the extreme expense of validation perplexity
the resulting samples are shown below

for instance maskgan models that led to improvements of a particular n gram metric at the extreme expense of validation perplexity as seen in figure could devolve to a generator of very low sample diversity
below we produce several samples from this particular model which despite the dramatically improved gram metric has lost diversity
it is a great movie it s just a tragic story of a man who has been working on a home
it s a great lm that has a great premise but it s not funny
it s just a silly lm
it s not the best movie i have seen in the series the story is simple and very clever but it capturing the complexities of natural language with these metrics alone is clearly insufcient

