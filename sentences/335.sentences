zest zero shot learning from text descriptions using textual similarity and visual summarization tzuf paz yuval gal and reut ilan university research institute for articial intelligence tzufar reut

com t c o l c
s c v
v i x r a abstract we study the problem of recognizing visual entities from the textual descriptions of their classes
specically given birds images with free text descriptions of their species we learn to classify images of previously unseen species based on specie descriptions
this setup has been studied in the vision nity under the name zero shot learning from text focusing on learning to transfer edge about visual aspects of birds from seen classes to previously unseen ones
here we suggest focusing on the textual description and distilling from the description the most vant information to effectively match visual features to the parts of the text that discuss them
specically we propose to age the similarity between species reected in the similarity between text descriptions of the species
we derive visual summaries of the texts i
e
extractive summaries that focus on the visual features that tend to be reected in images
we propose a simple attention based model augmented with the similarity and sual summaries components
our empirical sults consistently and signicantly outperform the state of the art on the largest benchmarks for text based zero shot learning illustrating the critical importance of texts for zero shot image recognition
introduction in computer vision zero shot learning zsl for image classication is the problem of classifying images given auxiliary information
an image sication model is trained to classify images from a pre dened set of classes
at test time images from new classes are given and the task is to fer knowledge learned from seen classes during training to unseen test classes
figure an illustration of textual similarity and ally relevant descriptions in wikipedia articles we aim to leverage the similarity within texts red via ument clustering bottom box we aim to extract similar red and dissimilar black visual descriptions and remove non visually relevant blue one
a common setup for zsl assumes that the iliary information is a set of semantically ingful properties called attributes describing the class e

black beak long tail wah et al
farhadi et al

a different zsl setup uses age captions as auxiliary information reed et al
felix et al

typically this auxiliary information is manually collected by human raters for each image test and train alike and averaged across images
a more realistic approach relies on available online text descriptions of classes e

wikipedia elhoseiny et al

it avoids pensive annotation and exposure to test images
by a ratio improvement of up to

in this work we classify bird species according to wikipedia descriptions
this task raises many challenges differences between the birds are very small which makes it a ne grained cation task this is an expert task and the text contains terminology that is unlikely to be familiar to a layman and on top of that the text scriptions of the classes are long containing few visually relevant sentences
as opposed to previous work on text based zsl employing textual descriptions zhu et al
elhoseiny et al
that focused on the visual modality here we focus on the text modality and address a key question in zsl how can we identify text components that are visual in nature to get an intuition about the task setup and our proposed solution consider the following situation
imagine you have never seen a zebra but have seen a horse
what if you were given a text describing a zebra zebras have hooves mane tail pointed ears and white and black stripes
this description would probably be very close to a description of a horse having hooves mane tail pointed ears and you would probably be looking for an image that reminds you of a horse but has white and black stripes
so even without ever seeing a zebra using text descriptions of the zebra and knowledge already acquired about horses one can correctly classify unknown classes like a zebra
our proposed solution has two phases
first based on the intuition that similar objects or ages thereof tend to have similar texts we encode a similarity feature that enhances text descriptions separability
in addition we leverage the intuition that the differences between text descriptions of species would be their most salient visual features and extract visually relevant descriptions from the text
our experiments empirically demonstrate both the efcacy and generalization capacity of our posed solution
on two large zsl datasets in both the easy and hard scenarios the similarity method obtains a ratio improvement of up to

with the addition of extracting visually relevant scriptions we obtain a ratio improvement of up to
over the state of the art
we further show that our visual summarization method alizes from the cub dataset wah et al
to the nab dataset van horn et al
and we demonstrate its contribution to additional models the contributions of this paper are threefold
first to the best of our knowledge we are the rst to showcase the critical importance of the text sentation in zero shot image recognition scenarios and we present two concrete text based processing methods that vastly improve the results
second we demonstrate the efcacy and generalizability of our proposed methods by applying them to both the zero shot and generalized zero shot tasks forming all previously reported results on the cub and nab benchmarks
finally we show that visual aspects learned from one dataset can be transferred effectively to another dataset without the need to obtain dataset specic captions
the efcacy of our proposed solution on these benchmarks illustrates that purposefully exposing the visual features in texts is indispensable for tasks that learn to align the vision and language modalities
background and related work zero shot learning zsl aims at overcoming the need to label massive datasets for new categories by learning the connections between images and prior auxiliary knowledge about their classes
at test time this auxiliary information compensates for the lack of previously attained visual tion about the new categories
text based zsl is a specic multimodal ation of this learning task that uses natural language descriptions as the auxiliary information
models for text based zsl are typically composed of three parts the text representation the image resentation a compatibility function between the two
while most previous work focused mainly on the latter two components here we focus on the text
most zsl studies for object recognition are aimed at processing the image modality
for ample xu et al
lei ba et al
qiao et al
akata et al
rely on visual features extracted using convolutional neural work cnn
more recent studies use object tion to detect the semantic parts of the object and extract visual features at the part level elhoseiny et al
zhu et al
zhang et al

this approach makes the image more compatible with the text as it enables text terms such as crest to be linked to the visual representation of parts like head
the auxiliary information provided to zsl tasks may be of various kinds ranging from pre dened semantic attributes lampert et al
pinyo et al
atzmon and chechik to captions xian et al
sariyildiz and cinbis to wikipedia article describing the species elhoseiny et al

here we assume the ter scenario
zsl studies that rely on wikipedia articles as auxiliary information improve the visual representation and the compatibility function and use text representations such as bag of words and tf idf without further text processing
lei ba et al
elhoseiny et al
zhu et al

qiao et al
used a simple bow and a norm objective to suppress the noisy signal in the text
however this basic ment of the text is problematic as it misses crucial information for detecting the correct class
recent studies lu et al
tan and bansal have shown improved performance on tiple vision and language tasks using pre trained bert based models that jointly learn a tation for vision and language
however they are tuned on relatively short texts and are not optimal for classifying long textual descriptions
in this work we proceed in a different yet plementary direction to previous work aiming to purposefully model the contribution of the textual modality to zsl
we aim to establish the tance of adequately processing the text into a sound representation of visually salient features in order to increase the vision and language compatibility which can then be effectively learned in an end end manner
strong baseline model the basic architecture which term zestvanilla is a simple multiplicative attention mechanism ong et al
inspired by romera paredes and torr
we model the problem using an attention based model where the image is queried against a set of candidate documents



xs formally let xs m be image feature i rm
the set tors from a training set where xs of m training images corresponds to a set of l seen classes
each class has a single class description which is a document written by experts in free guage e

wikipedia
we denote ds l as a i r m
set of l document feature vectors where ds likewise let xu n be the image feature i rm
the set of test tors from a test set where xu images corresponds to a set of k unseen classes



xu


ds likewise each class has a single class


du tion
we denote du k as a sets of i r m
finally ment feature vectors where xu w rm m is our learned matrix
at inference the label assignment of an image xu is dened as i y arg max i w du k k


k for an image representation xs j an indicator function i and a text sentation ds i ds j puts if image xs i corresponds to the class scribed by ds j and otherwise
the matrix w is then learned by minimizing the categorical entropy loss l i ds i w ds j t image encoding the image encoder s goal is to transform the image into a vector sentation of the most salient visual features for the classication
we adopt the image encoder for text based zsl of zhang et al
zhu it is et al
elhoseiny et al

based on a fast r cnn with girshick a backbone for object detection to detect seven semantic parts in the cub dataset
each visual part s encoded features are then catenated into a feature vector that functions as the image representation for the text based zsl
text processing our basic encoder processes the text into a feature vector
similar to previous ies we employ a tf idf representation salton and buckley
we preprocess the text to kenize words remove stop words and stem the remaining words
then we extract a feature vector using tf idf
this processing procedure is similar to the text processing presented by zhu et al

the dimensionalities of tf idf features for cub and nab are and respectively
the proposed approach our solution s key idea is to replace the general class s text representation with a text tion focusing on the most salient features for the visual recognition task
to do so we employ two different complementary methods i induce a similarity measure used for clustering and tract visually relevant text descriptions
both ods are incorporated in our proposed end to end figure our model with the similarity component and visually relevant summaries vrs


nearest neighbor similarity nns figure presents our nearest neighbor similarity nns method which aims to reconstruct the allel similarity links between the vision and text latent spaces
the algorithm is as follows
given an image xu from an unseen class in the zero shot phase we rst look for the nearest neighbor image in the set of training images using cosine similarity
the closest image from the training set xs corresponds to a document from training ds
we then look for k the nearest neighbor text in from test set du y and predict the corresponding class label


zestsimilarity a different way to incorporate textual similarity into the classication is to embed it into our model zestvanilla to benet from it in the learning cedure
to this aim we want to add on top of our text feature vector a representation of the text s similarity to its neighbors
the basic encoder captures similarities and ferences at the word level
however to nd larities at the document level we add to this vector our similarity component which applies pervised clustering to all class descriptions in the training and test texts
we use two different ing methods that capture different aspects of text similarities
the cluster indexes are then embedded as a bow hence cluster embedding
we hypothesize that the similarity component will work well on the easy scenario where closely related birds are seen during training and their text can cluster together to indicate these ilarities
figure the nearest neighbor similarity nns model links images and texts through in modality ilarities vision and language classication architecture sented in figure
in what follows we describe the similarity component
and the extraction of visually relevant summaries


the importance of being similar our proposed method leverages the similarities tween images and texts
that is when the images look similar the texts describing their classes are also similar and vice versa
here we propose to reconstruct this similarity link
to this end we propose two models a strong baseline based on two nearest neighbors which create a link between images and texts adding a similarity component to our model zestvanilla
for both models we use the image encoder section to process the images and the text processing section to process documents d

the importance of being seen here we extract visually relevant features from the text making the texts that enter the classication more compatible with the salient visual information typically reected in images
while the similarity method takes advantage of the similarity between objects seen in training and objects seen at test time here we want to address the harder scenario where similar objects are served together during test time only e

zebras and mules and they may be very different from those observed during training
to differentiate between classes in the test set we need to emphasize the parts that are different both in the image and the text and these are typically their most salient visual features


visually relevant summaries vrs our method for enhancing the textual description is based on visually relevant extractive summaries
extractive summarization is the task of extracting a small number of sentences that summarize a given document
in this work we dene visually vant extractive summarization vrs as the task of extracting only sentences that represent visually relevant language
the term visually relevant guage vrl was coined by winn et al
to indicate sentences which are visually descriptive with respect to the object i
e
bird species
a nave approach for vrs would be to extract sentences with parts that we know are visually salient in our domain e

the parts employed by the vision recognition representation
ever this nave approach has several drawbacks
first bird parts can be described using many ferent terms and paraphrases additionally a bird can be described by its property values e

black without any mention of the attribute e

beak
stead we propose to use the similarity of sentences in the documents and compare them to naturally curring sentences in the wild containing vrl
note that we can not rely on descriptions of ticular species due to the zero shot setup
we must do with descriptions of objects in the general main of objects we are interested in classifying
we propose to use a set of l bird captions to create an unsupervised classier
the classier will receive a set of sentences assembled as a ument and for each sentence the classier will predict whether the sentence is relevant that is whether it contains descriptions that can be seen in a bird image
for each document we propose to calculate the pairwise similarity between captions and sentences in the wikipedia description and based on this similarity assign a vrs score to each sentence
we calculate the vrs score of a sentence sj to a caption by computing the cosine similarity of the embeddings of both the captions l and sentences m in the document
for a xed size sentence embeddings we use a pre trained and triplet network reimers and gurevych schroff et al
on top of a pre trained bert network devlin et al

the vrs score of sentence sj with respect to all available captions l is thus dened to be l l ci sj we then take the highest k scoring sentences from k to be the visually relevant extractive summary of the document
we can then nate the similarity embedding to the vrs summary of the text and perform the multiplicative attention on this revised encoding of the documents and the same image encoding as before
a bird s eye overview of our overall architecture is presented in figure
the text that enters the similarity clustering component is the original wikipedia document not the document s vrs mary
documents contain many non visual tions that are unobserved in the images
however these non visual descriptions might still be tial to capture the similarity between documents
for example similar looking birds are likely to be in the same habitat
thus the vrl sentence traction and the similarity enhancement operate in parallel on the original document
experiments

one way to obtain naturally occurring descriptions of birds is from captions that describe bird ages
critically these captions need not be from our dataset they can describe any bird image

experiment setting datasets we evaluate our method on the tech ucsd cub wah et al
and the north america s birds dataset code can be found at
com tzuf zest
nab van horn et al
using class scriptions obtained from wikipedia and the laboutbirds website collected by elhoseiny et al

both are ne grained datasets of birds but from different species
the cub dataset contains images of bird species and the nab is a larger dataset of birds with images of
the texts of both cub and nab are long containing non visual information
cub has an average of tokens and sentences in class documents
nab has an average of tokens and sentences in class documents
two split settings we use the two splits sented by elhoseiny et al
super category shared scs also referred to as the easy split and super category exclusive sce also referred to as the hard split
in the scs for each class in the test set at least one class in the training set belongs to the same category categories are organized taxonomically
for ample in figure the rufous hummingbird and the ruby throated hummingbird are both from the hummingbird category
in the sce all classes in a category are in the same set
namely if a class is in the test set then other classes from the same egory are also in the test set and will never be seen during training
intuitively classes from the same category have high similarity in both images and texts so while in scs similar images have been seen during training in the sce a class from an entirely new category is seen for the rst time
training details the parameters of our model include cluster parameters
we use two clustering methods density based spatial clustering of applications with noise dbscan ester et al
hierarchical dbscan mcinnes et al

the dbscan algorithm takes two ters minimal cluster the number of samples in a neighborhood for a point to be considered as a core point max distance the maximum tance between two samples for one to be considered as in the neighborhood of the other
the minimal cluster is chosen to be two as two birds are the imal similarity we want similar to the nns model
the max distance parameter we optimize on idation sets of data according to the two splits
in addition the similarity model includes
allaboutbirds
org et al
merged the original classes according to the subtle division of classes
a threshold for performing the similarity nent also optimized over the validation set
the vrs algorithm includes a sentence score threshold for the number of sentences to be extracted
this threshold was chosen on the validation set
the weights w were initialized with normalized initialization glorot and bengio
the entropy loss function was optimized with adam optimizer kingma and ba
human summarization to evaluate our posed vrs extraction method we designed an oracle experiment using ground truth visually evant summarization
to this end two dent human experts manually annotated the cub dataset by reading each sentence in the document and marking the sentence as vrl
we set guidelines to resolve disagreements e

lings descriptions were marked as not vrl
on average only
of the sentences were found to include vrl
image captions to create visual summaries we use image captions of birds from the cub train set provided by reed et al

each image in the cub dataset has been annotated with ten grained captions
these captions describe only the birds visual appearance while avoiding ing the names of the bird species
e

this bird has a long beak a creamy breast and body with brown wings
in this work we use the rst ve captions of each image
to showcase this approach s generality we use these captions in both in domain cub and of domain nab scenarios
in all cases we avoid using captions of unseen test bird classes
in nab we effectively use captions from cub to tract vrs for entirely different species presented in nab
note that only models that include the vrs component vrs employ these image captions
we report the accuracy achieved per the number of captions used in the vrs to indicate the number of captions that are realistically needed
baselines our approach is compared asainst ten leading algorithms see table mczsl akata et al
wac linear elhoseiny et al
wac kernel elhoseiny et al
zsl romera paredes and torr sje akata et al
syncf ast changpinyo et al
syncov o changpinyo et al
zslns qiao et al
and gazsl zhu et al

methods mczsl akata et al
wac linear elhoseiny et al
wac kernel elhoseiny et al
eszsl romera paredes and torr sje akata et al
zslns qiao et al
syncf ast changpinyo et al
syncov o changpinyo et al
zslpp elhoseiny et al
gazsl zhu et al
nearest neighbor similarity nns zestvanilla zestsimilarity cub nab scs













sce











scs









sce









table accuracy on cub and nab datasets with two split settings
we report the mean over three random initializations
the standard deviation for zestsimilarity for the cub is
and
for nab
and
for the scs and sce splits cordingly
methods gazsl summarization vrs zestvanilla vrs zestsimilarity summarization vrs cub nab scs











sce











scs








sce








table visually relevant summarization vrs with gazsl zestvanilla and zestsimilarity
generalized zero shot learning the tional zero shot learning task considers only seen classes during the zero shot phase
however in a realistic scenario seen objects might also pear chao et al

in generalized zero shot learning gzsl test data might also come from seen classes and the labeling space is the union of both types of seen and unseen classes
gzsl is thus considered a more challenging problem ting than zsl due to the model s bias towards the seen classes
we follow the metric present by chao et al
to evaluate our models on the gzsl task
we evaluate the accuracy of a seen unseen accuracy curve suc and use area under suc to measure the general capability of zsl methods

results table presents the accuracy for each of the models
the table is divided into four sections which are from top to bottom previous work our baselines our models with previous setup for comparison to previous work our methods zestvanilla category zestsimilarity only cluster zestsimilarity full cluster cub nab scs scs







table zest model with different similarity methods cub nab methods scs
eszsl
zslns
wackernal
waclinear
syncf ast
syncovo
zslpp
gazsl
zestsimilarity

sce










scs









sce







table generalized zero shot learning auc of seen unseen curve
model with additional data captions
nns model according to table the nns model achieves competitive results on the scs
and
on cub and nab spondingly
the high scores on the scs where similar birds have been seen during training is expected as this method relies on similarities within texts and images
in contrast the nns fers from low accuracy on the sce where different categories of birds have been seen during training
as the nns model relies on text and image larities it is intuitively appealing that low accuracy on the sce stems from the fact that birds from different categories are less likely to look alike
zestvanilla in contrast to the very sophisticated approaches of zhu et al
the vanilla entropy based approach outperforms all previous methods on the sce split on both cub
ratio of improvement and nab
ratio of improvement
as the sce split is a more ing split this sheds light on the strength as well as limitations of this simple framework
zestsimilarity we then combined strengths of zestvanilla and nns models over the two ent scenarios hard and easy respectively
the zestsimilarity model adds the cluster index embedding to the tf idf representation only if a signicant percentage of the documents from the human vrs model sentence after nesting north american birds move in ocks further north along the coasts returning to warmer waters for winter
red foxes and coyotes readily predate colonies that they can access the later being the only known species to hunt adult pelicans which are too large for most bird predators to subdue
when foraging they dive like a kingsher often submerging completely below the surface momentarily as they snap up prey
it is one of only three pelican species found in the western hemisphere
due to their small size they are vulnerable to insect eating birds and animals
hummingbirds show a slight preference for red tubular owers as a nectar source
the head is white but often gets a yellowish wash in adult birds
table qualitative analysis showing seven sentences from three randomly selected summaries
the table shows human and vrs model markings of the sentence as yes vrl
test set are clustered with documents from the train set
the threshold picked over the validation set is a
thus in the case of the sce split no or few similarities are found and the zestsimilarity forms at the same level as the zestvanilla model
the threshold parameter was optimized on the dation set
the two clustering algorithms we applied nd real similarities achieving high accuracy when tested on predicting the correct label according to the ground truth taxonomical category
the scan and dbscan achieved and
accuracy on the cub and
and
on the nab accordingly
interestingly though different clustering nd ferent sources of similarities that are essentially additive
in table we can see a comparison tween different similarity enhancing methods
the category method is a bow of the bird category added to the original text ding and then passed as before to a zestvanilla model
the use of two clusters that capture ent similarities performs better than embedding the bird category in the text representation by a ratio improvement of up to

this suggests that our zestsimilarity method captures similarities that are beyond the bird category
finally in table we present the results of zestsimilarity in the gzsl setup
on both datasets and splits the zestsimilarity achieves state of the art results with up to
ratio provement
and use the captions from training images in the cub in order to generate visually relevant extractive summaries of the original wikipedia documents
we test the summarized representation on the zestvanilla model the zestsimilarity and the gazsl zhu et al
model
in table we show the experimental results
we compare the models before and after the use of the visually evant extractive summarization component
we see an improvement in accuracy in both models on both datasets and on both splits
in contrast to the zestsimilarity the gazsl does not have a component that embeds ities
the vrs reduces similarity by removing non vrl that might be similar between documents
the human summary is an especially lean mary with only
sentences extracted
thus the similarity between texts of similar objects minishes
the in the scs split performs poorly due to the diminished similarity
in contrast the vrs adds the similarity that was lost and the performance improves
to assess the quality of the vrs tion performance we treat human tion as the ground truth
the vrs method succeeds in removing
of the sentences in the cub dataset with
recall and
precision
for comparison removing
of the sentences randomly produces a recall of
and a sion of

table shows a qualitative analysis of our vrs results
in sentences the vrs model correctly marked the sentences as non vrl sentence is a typical case of non visually relevant language describing birds migration in sentence the vrs model correctly marks the sentence as non vrl spite the mention of color red since the color does not refer to the object to be classied the bird in sentence the vrs model correctly marks ing textual similarity and visually relevant maries lead to signicant improvements across models splits and datasets and illustrate that quate text processing is essential in text based zsl tasks
we conjecture that text processing methods will be essential in a range of vision and based tasks and hope this work will assist future research in better representing the language ity in various multi modal tasks
acknowledgments the research of the rst and last author is funded by the european research council erc grant and the israel science foundation isf grant and the research of the third thor is also funded by the israel science foundation isf grant for which we are grateful
references zeynep akata mateusz malinowski mario fritz and bernt schiele

multi cue zero shot learning with strong supervision
in proceedings of the ieee conference on computer vision and pattern nition pages
zeynep akata scott reed daniel walter honglak lee and bernt schiele

evaluation of output embeddings for ne grained image classication
in proceedings of the ieee conference on computer vision and pattern recognition pages
yuval atzmon and gal chechik

probabilistic and or attribute grouping for zero shot learning
in proceedings of the thirty forth conference on certainty in articial intelligence
soravit changpinyo wei lun chao boqing gong and fei sha

synthesized classiers for in proceedings of the ieee shot learning
ence on computer vision and pattern recognition pages
soravit changpinyo wei lun chao boqing gong and fei sha

classier and exemplar sis for zero shot learning
international journal of computer vision
wei lun chao soravit changpinyo boqing gong and fei sha

an empirical study and analysis of generalized zero shot learning for object tion in the wild
in european conference on puter vision pages
springer
jacob devlin ming wei chang kenton lee and kristina toutanova

bert pre training of deep bidirectional transformers for language in proceedings of the conference standing
of the north american chapter of the association for computational linguistics human language figure accuracy per number of captions used to cus summarization measured on the hard sce split of cub
showing that as little as captions in total are sufcient to focus the summarization process
the sentence as non vrl despite the mention of a body part bill since that description it is not sually relevant in that particular context
sentences show examples of false positive predictions of the vrs model
e

in sentence the vrs model incorrectly predicted vrl which we attribute to the mention their small size
in sentence the vrs model incorrectly marks the sentence as vrl a mistake we attribute to the mention of the ower s red color
we then compare both zestsimilarity and the gazsl to the use of human summarization in the cub dataset and see additional improvement in both models on the two splits
the gap between the performance on the vrs and the human marization indicates that improvement in the marization of documents will improve the models performance and is therefore a promising path for text based zero shot learning research
finally we experiment to assess the number of captions that are realistically needed for the vrs method
the results presented in table show that only a few sentences captions from arbitrary birds are needed to achieve the maximum accuracy with this method
testing the vrs with ve arbitrary captions from cub dataset on the nab dataset with scs split we achieved a
accuracy
for comparison reed et al
showed that their model needed at least captions per class to achieve the maximum accuracy i
e
had it used all the captions available
conclusion this work aims to establish a better way to sent the language modality in text based zsl for image classication
our approach only relies on semantic information about visual features and not on the visual features themselves
specically our two orthogonal text processing methods technologies volume long and short papers pages minneapolis minnesota
ation for computational linguistics
mohamed elhoseiny ahmed elgammal and babak saleh

write a classier predicting visual ieee classiers from unstructured text
tions on pattern analysis and machine intelligence
mohamed elhoseiny babak saleh and ahmed mal

write a classier zero shot learning in proceedings of ing purely textual descriptions
the ieee international conference on computer sion pages
mohamed elhoseiny yizhe zhu han zhang and ahmed elgammal

link the head to the beak zero shot learning from noisy text tion at part precision
in ieee conference on computer vision and pattern recognition cvpr pages
ieee
martin ester hans peter kriegel jorg sander xiaowei xu al

a density based algorithm for discovering clusters in large spatial databases with noise
in kdd volume pages
ali farhadi ian endres derek hoiem and david forsyth

describing objects by their attributes
in ieee conference on computer vision and pattern recognition pages
ieee
rafael felix vijay b
g
kumar ian reid and gustavo carneiro

multi modal cycle consistent alized zero shot learning
in the european ence on computer vision eccv
ross girshick

fast r cnn
in proceedings of the ieee international conference on computer vision pages
xavier glorot and yoshua bengio

ing the difculty of training deep feedforward neural networks
in proceedings of the thirteenth tional conference on articial intelligence and tics pages
diederik p
kingma and jimmy ba

adam a in method for stochastic optimization
national conference on learning representations iclr san diego ca usa may conference track proceedings
christoph h lampert hannes nickisch and stefan harmeling

learning to detect unseen object classes by between class attribute transfer
in ieee conference on computer vision and pattern recognition pages
ieee
jimmy lei ba kevin swersky sanja fidler al

predicting deep zero shot convolutional neural works using textual descriptions
in proceedings of the ieee international conference on computer sion pages
jiasen lu dhruv batra devi parikh and stefan lee

vilbert pretraining task agnostic olinguistic representations for vision and language tasks
in advances in neural information ing systems pages
thang luong hieu pham and christopher d
ning

effective approaches to attention based in proceedings of the neural machine translation
conference on empirical methods in ral language processing pages bon portugal
association for computational guistics
leland mcinnes john healy and steve astels

hdbscan hierarchical density based clustering
journal of open source software
ruizhi qiao lingqiao liu chunhua shen and anton van den hengel

less is more zero shot learning from online textual documents with noise in proceedings of the ieee suppression
ence on computer vision and pattern recognition pages
scott reed zeynep akata honglak lee and bernt schiele

learning deep representations of ne grained visual descriptions
in proceedings of the ieee conference on computer vision and tern recognition pages
nils reimers and iryna gurevych

bert sentence embeddings using siamese networks
in proceedings of the conference on empirical methods in natural language processing
association for computational linguistics
bernardino romera paredes and philip torr

an embarrassingly simple approach to zero shot ing
in international conference on machine ing pages
gerard salton and christopher buckley

weighting approaches in automatic text retrieval
formation processing management
mert bulent sariyildiz and ramazan gokberk cinbis

gradient matching generative networks for in the ieee conference on zero shot learning
computer vision and pattern recognition cvpr
florian schroff dmitry kalenichenko and james philbin

facenet a unied embedding for face recognition and clustering
in the ieee ference on computer vision and pattern recognition cvpr
hao tan and mohit bansal

lxmert learning cross modality encoder representations from formers
arxiv preprint

grant van horn steve branson ryan farrell scott haber jessie barry panos ipeirotis pietro perona and serge belongie

building a bird nition app and large scale dataset with citizen entists the ne print in ne grained dataset in proceedings of the ieee conference lection
on computer vision and pattern recognition pages
c
wah s
branson p
welinder p
perona and s
longie

the caltech ucsd dataset
technical report cns fornia institute of technology
olivia winn madhavan kavanur kidambi and smaranda muresan

detecting visually evant sentences for ne grained classication
in proceedings of the workshop on vision and guage pages
yongqin xian tobias lorenz bernt schiele and zeynep akata

feature generating networks for zero shot learning
in the ieee conference on computer vision and pattern recognition cvpr
tao xu pengchuan zhang qiuyuan huang han zhang zhe gan xiaolei huang and xiaodong he

attngan fine grained text to image tion with attentional generative adversarial networks
in proceedings of the ieee conference on computer vision and pattern recognition pages
han zhang tao xu mohamed elhoseiny xiaolei huang shaoting zhang ahmed elgammal and dimitris metaxas

spda cnn unifying semantic part detection and abstraction for in proceedings of the ieee grained recognition
conference on computer vision and pattern nition pages
yizhe zhu mohamed elhoseiny bingchen liu xi peng and ahmed elgammal

a tive adversarial approach for zero shot learning from noisy texts
in proceedings of the ieee conference on computer vision and pattern recognition pages

