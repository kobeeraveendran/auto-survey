abstractive and extractive text summarization using document context vector and recurrent neural networks chandra khatri amazon
sunnyvale california
gyanit singh ebay
san jose california
nish parikh google mountain view california
l u
j l c
s
c
v
v i x r a abstract sequence to sequence learning has recently been used for abstractive and extractive summarization
in current study models have been used for ebay product description tion
we propose a novel document context based els using rnns for abstractive and extractive summarizations

tuitively this is similar to humans reading the title abstract or any other contextual information before reading the document
this gives humans a high level idea of what the document is about
we use this idea and propose that models should be started with contextual information at the rst time step of the input to tain better summaries
in this manner the output summaries are more document centric than being generic overcoming one of the major hurdles of using generative models
we generate context from user behavior and seller provided information
we train and evaluate our models on human extracted golden summaries

the document contextual models outperform standard models
moreover generating human extracted summaries is hibitively expensive to scale we therefore propose a semi supervised technique for extracting approximate summaries and using it for training models at scale
semi supervised models are uated against human extracted summaries and are found to be of similar ecacy
we provide side by side comparison for tive and extractive summarizers contextual and non contextual on same evaluation dataset
overall we provide methodologies to use and evaluate the proposed techniques for large document marization
furthermore we found these techniques to be highly eective which is not the case with existing techniques
ccs concepts information systems summarization
computing ologies neural networks semi supervised learning settings keywords
text summarization recurrent neural networks natural language processing information retrieval extraction abstraction language modeling topic signature deep learning commerce work was done while the author was at ebay work was done while the author was at ebay permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for prot or commercial advantage and that copies bear this notice and the full citation on the rst page
copyrights for third party components of this work must be honored

for all other uses contact the owner

deep learning day london uk

copyright held by the owner
figure snapshot of a product snippet as it appears on the ebay mobile app for product titled mens travel hiking
itary tactical army camo sling backpack chest shoulder bag
the full html description is hidden behind the click row on item description making the current page easy to consume and user friendly
introduction
document summarization has its applications in almost all the mains of the internet
search engines provide query and context specic summary snippets as a part of search experience news websites use summaries to brief the articles social media use them for content targeting while e commerce websites use summaries for better browse experience through item or product highlights

in this paper we leverage data sets from a popular and ebay
presenting users with product summary decreases user s
nitive load to evaluate product s relevance to their purchase intent leading to higher engagement and better browsing experience

thermore as the trac on mobile sites and applications is ing item summaries become much more relevant
due to limited real estate available for mobile sites and design point of view it is more relevant to show item summaries than the entire html ments
figure depicts the picture of the snippet on ebay mobile application
text summarization techniques are either extractive or tive
in extraction key sentences and objects are extracted without modifying the objects themselves
this is obtained by key phrase or ad sentence extraction keeping the sentences intact

while abstraction involves paraphrasing the context aware tences after understanding the language
abstractive niques generally requires large scale data documents and sponding summaries for training the models for example
news titles can be considered as summaries and the articles can be sidered as the document
in many instances while summarizing deep learning day august london uk chandra khatri gyanit singh and nish parikh content generated by third party web systems have a legal straints on modication of the content
in these cases summaries are extracted than being generated
since majority of the content written for products in marketplace is provided by the sellers the marketplace also has legal constraint about not revising the tent

in this work we propose a technique for generation and corporating document context in based generative els for abstractive and extractive summarization
using context is akin to humans reading title and abstract to know the key details before delving into the full document
in this paper we describe techniques for generating context around documents ing user behavior and other information provided by the document creators
we show that rnns for both abstraction and extraction both benet from feeding document context at the rst time step of sequence to sequence learning
we evaluate the summaries erated by this methodology and found them to be more contextual to the document and preferred by humans
human extracted summaries are used for training extractive models like extractive rnns
however this approach for training is not scalable since extracting summaries for millions hundreds of thousands of documents is prohibitively expensive
sequence to quence model generally perform best with large scale data


fore we proposed a novel approach using document context for extracting approximate summaries in a semi supervised fashion which is used for large scale training
abstractive sequence to sequence models are generally trained on titles and subtitles
we adopt a similar approach with context which helps us scaling the training
furthermore we form a heuristics by using the trained model for re ranking all the sentences within the document based on likelihood of a sentence in a document being a summary sentence during the inference
we adopted the idea from a state of the art techniques for automatic speech recognition wherein an rnn is used for re ranking the potential outcomes from an n gram based language models
following are the main contributions of our
obtaining the context vectors from documents which can be used for extractive and abstractive summarization tasks

automatically extracting approximate summaries and ating training data to enable large scale semi supervised learning for extractive summarization
this is shown to be competitive to supervised learning techniques
using rnn and cnn rnn for extractive summarization


abstractive summarization for large documents


using document context vectors for improving the learning for text summarization
this novel approach is shown to beat the state of the art in a similar settings


comparing abstractive to extractive summarization under same setting of sequence to sequence learning
previous work
several summarization techniques have been explored over the past decades following are some of the popular techniques surface level approaches consider title words and cue words
important best for extracting relevant sentences

corpus based approaches leverage structural distribution of words using internal or external corpus wordnet
for summarization

cohesion based approaches considers cohesive relations tween the concepts within the text such as antonyms repetitions synonyms using lexical chains graph based approaches are some of the most popular text summarization techniques
each sentence in the text is represented as a vertex and a graph is constructed around all the sentences where the edges correspond to the inter connections between the sentences
lexrank and textrank are two such techniques
machine learning based approaches document tion can be converted to a supervised or semi supervised learning problem
in supervised learning approaches hints or clues such as key phrases topic words blacklist words are used to label the tences as positive or negative classes or the sentences are manually tagged which is not scalable
once the labels are established a nary classier can be trained for obtaining the scores or likelihood score pertaining to each sentence
several tion techniques have been explored in the literature in this regard
however classication based approaches generalize well however they are not ecient in extracting document specic maries
training data for machine learning approach contains bel of the sentences irrespective of the document
if the document level information is not provided then these approaches provide same prediction irrespective of the document
providing document context in the models alleviates this problem which is what is one of the contributions of this paper
abstractive summarization techniques are less prevalent in the literature than the extractive ones
it is much harder because it involves re writing the sentences which if performed manually is not scalable and requires natural language generation techniques

the two common abstraction techniques are structured and tic both of which mostly are either graph tree based or ontology and rule template based
all the approaches mentioned above work well however they either face challenge towards scalability or large scale evaluation or do not generalize well
due to complexity constraints research to date has focused primarily on extractive methods but due to advancements in and natural language generation niques is making it possible to generate reasonable summaries for very short descriptions using abstraction
used reinforcement learning while used cnns for abstractive summarization
most of the current advancements are around short summaries from short documents
furthermore there does not exist any work where extractive and abstractive techniques have been compared in the similar setting using approaches
techniques based approaches have been used to
ciently map the input sequences description document to map output sequence summary however they require large amounts data
with several examples model tends to learn the mapping tween input sequence and output sequence and generate more cient summaries corresponding to the input document
moreover it is found that models currently work well for smaller ument summarizations one two lines of the document mapping to abstractive and extractive text summarization deep learning day august london uk headlines phrase representation

even though els are providing benchmark results in machine translation and speech recognition tasks they have not yet performed well for summarization tasks dialog systems and evaluation of alog systems and are facing many challenges marizing long documents
for long sequence depicted that reversing the source sentence provides better results

architecturally our abstractive models are similar to with a change in encoder being novel document contextual lstm
stead of simple attention based mechanism
document
vector is described in section
furthermore unlike related ous work models in current study are summarizing full large uments longer than characters to generate relatively large summaries characters using beam search and vocabulary strictions constraints
furthermore we have not found any ture where based abstractive summarization is compared with related rnn cnn variations for extractive summarization which is novel addition of this paper
khatri et
al
describes summarization for ecommerce setting
user actions to infer document context
websites which host user generated documents for consumption by other users provide myriads of ways for document discovery

for example users on ecommerce website discover relevant ument to their intent via search recommendation modules or ious topical pages
also sellers or content creators while creating the document or product pages provide lot of metadata on the ument for example title tags categorical
in this section we describe how we use historical information from document creator and document consumer to generate the document context
this context is then later combined with word embeddings obtained via
skip gram model with negative sampling sgns
to generate what we call document context vectors or dcv
we also use
ment context to score sentences in the document for algorithmic labeling for large scale semi supervised learning to obtain imate summaries
stop words and high frequency words in ulary are not considered while obtaining the context words in the following subsections
let v be the vocabulary of the corpora
we assume that stop words have been removed from the corpora to generate the cabulary
also let n be the vocabulary size n
for the document d we generate three unit vectors namely s s denotes
cd seller b b denotes browse
all of these vectors have dimension n but for document d most of the c sions are
we later combine these vectors with sgns vectors to make dcv
for brevity we drop the d from superscript is rest of the section as vectors are being generated per document only
q q denotes queries cd cd c c document creators
when a seller lists a product for sale on ebay they create a document for it which includes title key value meta data on xed dimensions like condition brand size color
they also type in verbose description containing images videos and html
in tion to this they select a leaf in the ebay taxonomy c
ln figure a snapshot of taxonomy used in a ebay
here the top taxa collectibles and art are expanded into lower taxa for example antiques
where li is taxon used in the taxonomy
the taxonomy used in ebay is a laminar family
that is given two taxa and either or or
seller has to attach a leaf in this taxonomy tree to the document
figure shows a small snapshot of ebay taxonomy
this
omy tree is maintained and generated by domain experts and is of high quality
for example the document titled mens travel hiking military tactical army camo sling backpack chest shoulder bag is chosen to be put in the following leaf sporting goods outdoor sports camping hiking hiking backpacks day packs by the seller
context vector cs is induced from seller provided metadatams title subtitle taxon other key valued metadata such as brand color

for w v let cs w be the value of the dimension w in cs
it is dened as follows cs w in ms if w seller metadata otherwise we then normlaize the vector cs to a unit vector cs
we use the weight obtained for each context word later on to obtain the document context vector for each document giving relevance to words based on weights
this technique is not limited to ebay or any ecommerce based pages only
such an approach can be easily extended to the general textual documents available online in the form of webpages title metadata or articles title sub titles abstract keywords category
c document consumers
buyers discover relevant inventory via search recommendations made to them or on topical pages
hiking backpacks

or from external sources like advertising
these discovery browse paths whether they are search trails or click trails that ends in a document are used to extract relevant words for the ument
one such discovery path is search
for example when the user searches with the queries like tactical sling backpack tary backpack and lands on the document titled mens travel
ing military tactical army camo sling backpack chest shoulder
bag the words of the queries contains what the user thought was the most descriptive information from the document
when such information is aggregated statistically cross large number of users it can provide great context for the document
let qsetd queries used to discover document d and browsed titles of documents via which user discovered document via deep learning day august london uk chandra khatri gyanit singh and nish parikh recommendation or topical page
for the dimension associated with word w values in both vectors are dened as in qsetd if w qsetd otherwise cb w in browsed if w browsed otherwise
cq cb


we then normalize the vector cb cq to a unit vector ilar to the weights obtained for the context words from document creators we obtain the weights for words provided by the ment consumers readers
basically there are signicant tion provided by the readers based on their experience and ior
we can leverage the information provided by the readers to ther obtain more context words and corresponding weights which is later on used to obtain the document context vector

c document context vectors
for a document d we dened three unit vectors above s s denotes
cd seller b b denotes browse
we combine these three to form a cumulative context unit vector cd by adding c c these three vector
q q denotes queries cd cd c cd q cd s q b cd s cd

we used s q and b
these parameters may be ne tuned based on historical demand which may be estimated by ratio of trac volumes in dierent channels or by expert set existing priors for importance of various channels of trac
we then re weight value for each word dimension in cd by its idf to make cd i d
more precisely value for dimension for word w is dened as i d f w
cd w i d for word w cd
we combine context with word embeddings obtained via sgns to generate a vector per document that we call vd
let msg n s be the matrix of dimension n xk where k is the dimension of word embeddings and n is the vocabulary size of the corpora
row for word w in msg n s is the word embedding of the word
we dene as vd
cd i d msg n s note that dimension of vd is just like word embeddings
scoring sentences using document context
set of
for a document let s say it contains sentnecesd tences in d
we describe how to use document context to score these sentences
these score are used for generating algorithmic labels for large scale semi supervised learning to obtain mate summaries
for more details see section
for a sentence s sentnecesd where s
wk
sscor e vd
w s

the above score corresponds to the weighted sum for the words in the sentence
the weight for the words incorporate frequencies in seller provided metadata buyer s search history browse tory leading to discovery of the said product and also the inverse document frequence idf score of that word
the idf score for
each word is simply the inverse of documents containing this word across all the documents at ebay
the idf score corresponds to the topical or document level relevance for each word in the lary
highly common words such as stop words the and have nearly zero idf score
using the score above we obtain the score of each sentence
we use the sentence score to rank each tence within the document and select top
till we reach acters sentences as the summary for a document product
we did an a b test to see if the summaries obtained using this technique correlates with user expectations and we observed statistically
nicant lift in sales and user engagement
this implies that the summaries obtained using this semi supervised approach are ful
we use this technique to generate the training data for vised extraction based summarization techniques rnn
models
this work primarily focuses on adding context as the initial state to rnns for abstractive and extractive text summarizations and paring it with various state of the art techniques
for extraction we use labeled supervised and semi supervised data
for tion we use titles and subtitles for training the models
nomenclature and base model recurrent neural network is a type of neural network which is an extension to feed forward nn with at least one feed back tion so activations can ow round in a loop
essentially information from prior observation along with the current vations are used to make predictions
the notations are borrowed from sutskever et al

rnn putes an output sequence

yt for a given input sequence

xt
corresponding to the following equations ht hx xt w hh
ht yt
w yh ht this framework works when there is an alignment between put and output sequences that is the size of the input is same as size of the output
when the size of input and output sequences are dierent then two rnns with encoding decoding mechanisms are used cho et al

theoretically this framework should work however it is found that rnns with encoding decoding nisms nd diculties in mapping long sequences or when there are long term dependencies
gated recurrent units grus and long short term memory lstms
solve this problem by troducing gates into the network to prevent vanishing gradient problem associated with rnns with long term dependencies

in current study lstms are used for extractive and tive summarizations
in standard lstm sequence of xed length is passed as an input to be encoded into a xed dimension vector v which is then decoded into the output sequence of words
to summarize lstm estimates the following abstractive and extractive text summarization deep learning day august london uk t t
yt

xt


in abstractive summarization document can be fed as an input during training and the summaries can be fed as output
however extractive rnn can be trained using standard supervised cation setup by performing soft max on the encoded layer

contextual recurrent neural network abstractive contextual rnn ac rnn
it is an rnn architecture wherein a document context vector as described in section is passed as an input at rst time step along with the ument sequence in the encoder
the idea is that if a pre learned document context vector vd is passed as an input at the
ning of the encoding stage then the model not only converges faster but also learns the summaries corresponding to the ment and not just the generic sequences
the basic idea is that if a reader is aware of the title of a document or abstract for a cation then it provides a better understanding and a high level
terpretation of the document which makes the model to be able to provide more specic summaries corresponding to the documents

therefore document context vector vd at the rst time step sentially changes the encoding vector
an lstm decoder with similar architecture mentioned in
ing have been used however the input at time to the decoder is the vector obtained from encoder
unlike other decoding mechanisms where output at time t can be any word from the cabulary the output from the document vocabulary is considered during the time of prediction making the inference faster

sutskever et al
and proposed beam search to obtain the most likely sentence in the machine translation task
however we perform a heuristics by using the trained model for re ranking all the sentences within the document based on likelihood of a sentence being a summary sentence during the inference
where likelihood of any sentence is dened as likelihood of decoding the sentence given the encoded input
so the abstractive model is used for extraction during the inference
we adopted the idea from a state of the art techniques for automatic speech tion wherein an rnn is used for re ranking the potential outcomes from an n gram based language model
we do this to address several issues avoid generic and short output issues with quence to sequence models
obtaining grammatically correct sentences for ebay users to avoid poor customer experience and c avoiding legal push backs from the sellers
extractive contextual rnn ec rnn
it consists of encoder only
the encoder used in ec rnn is the replica of coder used in ac rnn with document context vector as an input at time and embedding representation of the words is passed as input to the model
however the output of encoder is used for binary classication sentence being a summary sentence or a not using softmax
note that each sentence starts with context vector therefore the classication of sentence happens given the context of the document and not just the sentence alone
in this way same sentence may be classied as summary sentence for table description distribution of a ebay products
total vocabulary size median document length
median number of words median sentence length
median nbr of words in sentence characters characters one document but not for others
furthermore given a context vector vd it is the extra information provided by the tence which dierentiates it from the other sentences in the ument which is a major drawback of other state of the art cation approaches wherein some sentences are always classied as a summary sentence
non contextual rnn architectures
in current setup rnns trained without document context vector are termed as non contextual rnns
recently several architectures have been proposed in this regards however three main models which are explored in current study are
abstractive rnn a rnn
abstractive rnn is the ditional sequence to sequence model using lstm suggested by

the model is exactly similar to ac rnn without the text as input at time
the input at time in a rnn is a token start
a xed sized input and output sequences are used for ing by either curtailing or padding
extractive rnn e rnn
rnn has been used for sication tasks and it generates state of the art results
extractive rnn is non context version of ec rnn proposed in section

as mentioned before embeddings for words are pre calculated ing
skip gram with negative sampling sgns technique and are used as input corresponding to each word in encoding layer
tures are extracted using embedding for classication task convolutional rnn cnn rnn
convolution based
lstm has performed extremely well in text classication tasks

furthermore convolution attention based encoder has been used for short summarization tasks
cnn lstm is used to sify the sentences with the same technique as e rnn however the dierence is that cnn is used to extract sequences of level phrase representations
as suggested by zhou et al

lstm is able to capture both local features of phrases global and temporal sentence semantics
cnns with multiple lters max pooling and dropout are used to extract high level phrase representations and then passed to lstm for classication using softmax
datasets table describes the distribution of a ebay description
vocabulary size of our dataset is k words
median document length is and words
datasets
there are two kinds of datasets which are used in current study
deep learning day august london uk chandra khatri gyanit singh and nish parikh human extracted snippets items documents and responding details titles url description were provided to mans for extracting the summaries
the task was to extract and rank the sentences from the descriptions given ebay item url
items out of items were used for evaluations golden set and items were used for training dierent models
semi supervised large scale summarization tion sentences from item descriptions were extracted and ranked based on relevance towards the document context
section provides information about how to rank a sentence in the order of relevance given the document and it s contextual details

eral techniques have been proposed by shen et al lin et al
and
based on query thematic similarity topic signature and tent semantic analysis
expanded the topics to wikipedia and obtained best results on duc summarization task
similar to approaches mentioned above we obtain the imate summaries for ebay item descriptions
after an evaluation from a ebay reviewers it is found that summaries generated ing document context based approach are of high quality and can be used for training the models
given the quality of these maries ebay launched this feature on mobile applications and site
on an a b test it is found that showing summaries using this approach have a high monetary value when compared to not ing the summary snippet
models trained using semi supervised approach will be evaluated on golden test set to identify the vance of this technique
data generation for classication task ec rnn c rnn cnn rnn and other classication based
tive summarization techniques need the labeled data for training

for classication tasks sentences which have blacklist terms are labeled as non summary class while the sentences scoring high on document context metric are considered as positive sentences

blacklist terms are the words and phrases which do not contain item document level information and are frequently used at ebay such as returns me stars
we obtained terms using human curation and with statistical analysis of ebay item descriptions
for each description in items tences are tagged as positive or negative
sentences which were neither scoring high on document context metric nor having list terms were left out from being tagged
since the context sentence ranking is a new approach and is yet to be uated except using a b test in production the data is tagged for high precision
this work is a step towards evaluating context based sentence ranking as well
architecture details and experimentation training details and model architectures
abstractive context rnn ac rnn and abstractive rnn
a rnn were trained using deep lstm with layers as described in sutskever et al
with cells and dimension word embeddings
since we wanted to nd the relative dierence ter adding the context in rnns for summarization task we kept the same parameters for both the models
parameters settings and table
parameter setting for abstractive approaches like ac rnn and a rnn
parameters input description length output summary length optimization method learning rate
batch size
lstm parameters value
words words
stochastic gradient descent with momentum reduced to half after every third epoch

uniform distribution from table parameter setting for extractive approaches like rnn and e rnn
parameters maximum sentence length optimization method
learning rate batch size lstm parameters value words adam uniform distribution from table parameter setting for convolutional rnn
parameters maximum sentence length

dropout keep probability learning rate
filter sizes for convolution batch size cnn and lstm parameters max pool size value words
random normal centered at with standard deviation model details which worked the best in our case are mentioned in table
extractive context rnn ec rnn and
extractive rnn
e rnn were trained using two lstm layers with cells and dimension word embedding
since we wanted to nd the tive dierence after adding the context in rnns for summarization task we kept the same parameters for both the models
parameters settings and model details which worked the best in our case are mentioned in table

convolutional rnn cnn rnn consists of two neural works
cnn for high level phrase representations and then lstms for obtaining the temporal and sequential nature of the text
we used single layer convolution with lter size equal to and a gle layer lstm with cells and dimension word embeddings were used
parameters settings and model details which worked the best in our case are mentioned in table
abstractive and extractive text summarization deep learning day august london uk table dierent type of experiments done and metrics used for those experiments

evaluation setting classication similarity ranking description
given a sentence classify whether
it is summary sentence or not
given golden summaries nd the similarity score
given sentences rank them by in the order of relevance towards summary evaluation metric accuracy precision recall f score rouge rouge l
bleu tf idf cosine similarity topic similarity ndcg mean average precision experiments and evaluation metrics
we split our k human extracted summaries dataset into k and k parts
we use k for all evaluations
we also used the data tained using semi supervised technique for training and ing the models
supervised we train our models on k human extracted summaries
these models are trained with three dierent settings of target summary lengths
we use sentence sentence and sentence summary lengths

semi supervised we use k documents and approximate maries generated via document context for training models at scale

we use k human extracted summaries for evaluation purposes

we also baseline with a fuzzy summarization strategy where dom sentences are picked in the summary output
results for supervised setting
in this section we present the result for supervised models for marization
for this puproses we trained our models on k human extracted summaries and evaluated on k human extracted maries
table compiles the performance result for all summarization strategies
it is clear that rnn s as a whole outperform other niques
naive bayes svm lsa lexrank textrank on rouge bleu and token similarity
in both abstractive and extractive rnn s adding document context improves all metrics
for example and bleu for abstractive rnn is and respectively
when document context is added ac rnn then and bleu creases to and respectively
similarly for extractive rnn
e rnn vs extractive contextual rnn ec rnn changes from to and bleu changes from to

for large target summaries which are sentence long tive contextual rnns perform the best followed by abstractive textual rnns
for example for ec rnn is and for ac rnn is
similarly bleu is compared to for rnn
for small target summaries sentence long we observe that abstractive context rnn ac rnn outperform extractive
rnn ec rnn
where is for ec rnn compared to for ac rnn
for target summaries which are sentences long results are shared in table
as the target summary length creases ecacy of extractive contextual rnn increases over stractive contextual rnns

in table we present classication metrics for extractive els
all models are doing a good job of separating summary sentences from non summary sentences
for this task ec rnn outperforms other methodologies as well
table results on supervised task using human extracted summaries k for training and k for evaluation
tions with target summary a sentence long and tences long
abstractive context rnn ac rnn performs best in all metrics for short summaries
extractive context rnn ec rnn performs best for longer summaries
adding context as a whole created improvements in rnn models
bleu
topic sim
model fuzzy e rnn ec rnn
cnn rnn
a rnn
ac rnn
nb svm lsa lexrank textrank fuzzy e rnn ec rnn
cnn rnn
a rnn
ac rnn
nb svm lsa lexrank textrank rouge

token
sim

target snippet length sentence

target snippet length sentence








table describe ranking metrics for summarization models
we assign summary sentences as relevance of and other sentences relevance score of
the task is to generate ranking of sentences in a way that picks the summary sentences before non summary tences
for this task as well context aware rnns win out
tive context rnn ec rnn have the highest and
deep learning day august london uk chandra khatri gyanit singh and nish parikh table results on supervised task using human extracted summaries k for training and k for evaluation
ations with target summary when target summaries are extremely long sentence
extractive context rnn rnn performs best
adding context as a whole created provements in rnn models
bleu
topic sim
model
fuzzy v rnn
c rnn cnn rnn
a rnn
ac rnn
nb svm lsa lexrank textrank rouge
token
sim

target snippet length sentence

table classication for extractive supervised model
classes non summary
training k human judged k human judged
extractive context rnn rnn shows best performance
note no classication for stractive models
model
accuracy nb svm e rnn cnn rnn
ec rnn precision
recall
f score
table supervised model ranking evaluation
training k human judged k human judged
tive context rnn ec rnn shows best performance



model
fuzzy e rnn ec rnn
cnn rnn
a rnn
ac rnn nb svm lsa lexrank textrank









results for semi supervised setting
in this section we share the result of training with algorithmically generated approximate summaries
these approximate summaries are generated by using document context
for extractive rnn and other classication approaches like svm nb human labeled and table classication for semi supervised extractive
pervised model classes
non summary
training k algorithmically labeled data k human judged
ec rnn shows best performance
model
accuracy e rnn
cnn rnn
ec rnn e rnn
cnn rnn
ec rnn
precision recall
semi supervised
supervised f score algorithmically labeled data was used for training
whereas for stractive rnn we use title and subtitles for learning
table compares the classication metric for extractive els when trained with human extracted summaries to cally labeled approximate summaries
it can be seen that training models on large scale approximate summaries does not lead to any drop in precision recall and accuracy
table compares the summarization results for all rnn els abstractive vs extractive
training on k approximate maries does not lead to drop in metric
for example drops from ec rnn supervised to
ec rnn semi supervised

bleu also remains comparable with for dataset and for k dataset

ranking metrics are compared in table
as the size of data increases for training abstractive methods performance increases tremendously
for example ac rnn have of vs for k vs k documents respectively
same result hold for map as well where increase from to when data is increased from k to k for ac rnn
for extractive rnn which are trained on approximate summaries we do nt see a drop in ranking metrics between k human extracted vs k approximate summaries
remains in both cases

overall it can be seen that abstractive rnns are improving with more data
extractive rnn are able to generate near similar formance with large scale approximate summaries as with small scale human extracted summaries
and adding document context to rnn with approximate summaries further boost the performance for both abstractive and extractive rnn
conclusion
we proposed a novel document context model for
stractive and extractive summarization
we have shown that rnns and other models are powerful and beat state of the art summarization approaches in e commerce setting
the idea of adding contextual information at the rst time step during the encoding of the input to output sequence label mapping aligns with humans since generally humans tend to read title abstract and gather other contextual information before reading the entire document articles

this gives humans high level understanding of the document which abstractive and extractive text summarization deep learning day august london uk table result of summarization for semi supervised
tractive supervised model
training k algorithmically labeled data k human judged
extractive text rnn ec rnn shows best performance
abstractive contextual rnns show signicant improvements with large training data
model

token
sim



semi supervised rouge bleu
topic sim
e rnn ec rnn cnn rnn

a rnn
ac rnn
e rnn ec rnn
cnn rnn a rnn
ac rnn

supervised table ranking metrics for semi supervised extractive supervised model
training k algorithmically labeled data k human judged
abstractive contextual rnns show most improvements with large training data


tractive contextual rnns also show improvements
model


semi supervised supervised e rnn
ec rnn
cnn rnn
a rnn
ac rnn if incorporated in model will generate much richer ment specic summaries
training is performed in a human tagged supervised setting as well as with large scale semi supervised tracted summaries
it is found that based rnn
tion techniques out performs other state of the art summarization techniques
within rnns contextual rnns outperform non rnns on most of the similarity and ranking measures

contextual rnns are found to be best performing followed by contextual rnns for large summaries however for shorter maries abstractive contextual rnns outperform all other techniques followed by extractive rnns with attention and more cated setting it can be possible to further improve abstractive niques
we have also depicted that abstractive rnns contextual contextual can be used for extraction tasks and still beat the tractive systems
it is found that large scale semi supervised data for training improves the performance of the models on the evaluation dataset
hence training the extractive models with proximate summaries leads to better results compared to relatively smaller human tagged supervised data
we think that advantage of large scale training outperforms the noise in approximating maries
we recommend other researchers to incorporate context in other tasks machine translation task
references
text summarization with tensorow

accessed

badry eldin and
elzanfally
text summarization within the international journal latent semantic analysis framework comparative study
of computer applications
barzilay and elhadad
using lexical chains for text summarization
vances in automatic text summarization pages
baumel eyal and elhadad
query focused abstractive summarization
incorporating query relevance multi document coverage and summary length constraints into models
arxiv preprint

cheng and lapata
neural summarization by extracting sentences and words
arxiv preprint
cho van merrinboer gulcehre bahdanau bougares schwenk and bengio
learning phrase representations using rnn encoder decoder for statistical machine translation
arxiv preprint
chopra auli rush and harvard
abstractive sentence rization with attentive recurrent neural networks
proceedings of naacl pages
chung gulcehre cho and bengio
empirical evaluation of gated current neural networks on sequence modeling
arxiv preprint

das and martins
a survey on automatic text summarization
literature survey for the language and statistics ii course at cmu

elman
finding structure in time
cognitive science
erkan and radev
lexrank graph based lexical centrality as salience in text summarization
journal of articial intelligence research

gehring auli grangier yarats and dauphin
convolutional quence to sequence learning
arxiv preprint
gong and liu
generic text summarization using relevance measure and latent semantic analysis
in proceedings of the annual international acm sigir conference on research and development in information retrieval pages acm

guo metallinou khatri al
topic based evaluation for conversational bots
arxiv preprint

khandelwal
neural text summarization
khatri parikh solanki al
snippet extractor recurrent neural works for text summarization at industry scale
us patent
khatri parikh solanki al
snippet generation and item description summarizer
us patent
khatri voleti veeraraghavan parikh islam mahmood garg
in big data big and singh
algorithmic content generation for products

data ieee international conference on pages
ieee

li lam bing and wang
deep recurrent generative decoder for abstractive text summarization
arxiv preprint

lin
rouge
a package for automatic evaluation of summaries

in text summarization branches out proceedings of the workshop volume

barcelona spain
lopyrev
generating news headlines with recurrent neural networks
arxiv preprint

luhn
the automatic creation of literature abstracts
ibm journal of search and development

mihalcea and tarau
textrank
bringing order into texts
association for computational linguistics

mikolov sutskever chen corrado and dean
distributed resentations of words and phrases and their compositionality
in advances in neural information processing systems pages

miller
wordnet
a lexical database for english
commun
acm
moawad and aref
semantic graph reduction approach for abstractive text summarization
in computer engineering systems icces seventh international conference on pages
ieee
nallapati zhai and zhou
summarunner a recurrent neural network arxiv based sequence model for extractive summarization of documents
preprint

nallapati zhou gulcehre xiang al
abstractive text arxiv preprint marization using sequence to sequence rnns and beyond

nastase
topic driven multi document summarization with encyclopedic knowledge and spreading activation

in proceedings of the conference on pirical methods in natural language processing pages
association for computational linguistics
paulus xiong and socher
a deep reinforced model for abstractive summarization
arxiv preprint
ram prasad khatri and venkatesh
conversational ai the science behind the alexa prize
arxiv preprint
deep learning day august london uk chandra khatri gyanit singh and nish parikh rush chopra and weston
a neural attention model for abstractive sentence summarization arxiv preprint

singh parikh and sundaresn
user behavior in zero recall ecommerce
in proceedings of the international acm sigir conference on queries
search and development in information retrieval pages acm
song yang and wang
training excitatory inhibitory recurrent neural networks for cognitive tasks a simple and exible framework
plos comput biol
steinberger and jezek
evaluation measures for text summarization
puting and informatics
sutskever martens and hinton
generating text with recurrent
in proceedings of the international conference on machine ral networks

learning pages

sutskever vinyals and le
sequence to sequence learning with neural
in advances in neural information processing systems pages networks



venkatesh khatri ram guo al
on evaluating and comparing conversational agents
arxiv preprint
wiseman and rush
sequence to sequence learning as beam search optimization
arxiv preprint

xiong wu alleva droppo huang and stolcke
the microsoft conversational speech recognition system arxiv preprint
xu lau baldwin and cohn
decoupling encoder and decoder works for abstractive document summarization
aclweb
zhou sun liu and lau
a c lstm neural network for text tion
arxiv preprint

