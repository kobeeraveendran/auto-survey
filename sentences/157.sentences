abstractive and extractive text summarization using document context vector and recurrent neural networks chandra khatri amazon sunnyvale california
com gyanit singh ebay inc
san jose california
com nish parikh google mountain view california
com l u j l c
s c v
v i x r a abstract sequence to sequence learning has recently been used for abstractive and extractive summarization
in current study models have been used for ebay product description tion
we propose a novel document context based els using rnns for abstractive and extractive summarizations
tuitively this is similar to humans reading the title abstract or any other contextual information before reading the document
this gives humans a high level idea of what the document is about
we use this idea and propose that models should be started with contextual information at the rst time step of the input to tain better summaries
in this manner the output summaries are more document centric than being generic overcoming one of the major hurdles of using generative models
we generate context from user behavior and seller provided information
we train and evaluate our models on human extracted golden summaries
the document contextual models outperform standard models
moreover generating human extracted summaries is hibitively expensive to scale we therefore propose a semi supervised technique for extracting approximate summaries and using it for training models at scale
semi supervised models are uated against human extracted summaries and are found to be of similar ecacy
we provide side by side comparison for tive and extractive summarizers contextual and non contextual on same evaluation dataset
overall we provide methodologies to use and evaluate the proposed techniques for large document marization
furthermore we found these techniques to be highly eective which is not the case with existing techniques
ccs concepts information systems summarization computing ologies neural networks semi supervised learning settings keywords text summarization recurrent neural networks natural language processing information retrieval extraction abstraction language modeling topic signature deep learning commerce work was done while the author was at ebay inc
work was done while the author was at ebay inc
permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for prot or commercial advantage and that copies bear this notice and the full citation on the rst page
copyrights for third party components of this work must be honored
for all other uses contact the owner
deep learning day london uk copyright held by the owner
figure snapshot of a product snippet as it appears on the ebay mobile app for product titled mens travel hiking itary tactical army camo sling backpack chest shoulder bag
the full html description is hidden behind the click row on item description making the current page easy to consume and user friendly
introduction document summarization has its applications in almost all the mains of the internet
search engines provide query and context specic summary snippets as a part of search experience news websites use summaries to brief the articles social media use them for content targeting while e commerce websites use summaries for better browse experience through item or product highlights
in this paper we leverage data sets from a popular and ebay
presenting users with product summary decreases user s nitive load to evaluate product s relevance to their purchase intent leading to higher engagement and better browsing experience
thermore as the trac on mobile sites and applications is ing item summaries become much more relevant
due to limited real estate available for mobile sites and design point of view it is more relevant to show item summaries than the entire html ments
figure depicts the picture of the snippet on ebay mobile application
text summarization techniques are either extractive or tive
in extraction key sentences and objects are extracted without modifying the objects themselves
this is obtained by key phrase or ad sentence extraction keeping the sentences intact
while abstraction involves paraphrasing the context aware tences after understanding the language
abstractive niques generally requires large scale data documents and sponding summaries for training the models for example news titles can be considered as summaries and the articles can be sidered as the document
in many instances while summarizing deep learning day august london uk chandra khatri gyanit singh and nish parikh content generated by third party web systems have a legal straints on modication of the content
in these cases summaries are extracted than being generated
since majority of the content written for products in marketplace is provided by the sellers the marketplace also has legal constraint about not revising the tent
in this work we propose a technique for generation and corporating document context in based generative els for abstractive and extractive summarization
using context is akin to humans reading title and abstract to know the key details before delving into the full document
in this paper we describe techniques for generating context around documents ing user behavior and other information provided by the document creators
we show that rnns for both abstraction and extraction both benet from feeding document context at the rst time step of sequence to sequence learning
we evaluate the summaries erated by this methodology and found them to be more contextual to the document and preferred by humans
human extracted summaries are used for training extractive models like extractive rnns
however this approach for training is not scalable since extracting summaries for millions hundreds of thousands of documents is prohibitively expensive
sequence to quence model generally perform best with large scale data
fore we proposed a novel approach using document context for extracting approximate summaries in a semi supervised fashion which is used for large scale training
abstractive sequence to sequence models are generally trained on titles and subtitles
we adopt a similar approach with context which helps us scaling the training
furthermore we form a heuristics by using the trained model for re ranking all the sentences within the document based on likelihood of a sentence in a document being a summary sentence during the inference
we adopted the idea from a state of the art techniques for automatic speech recognition wherein an rnn is used for re ranking the potential outcomes from an n gram based language models
following are the main contributions of our obtaining the context vectors from documents which can be used for extractive and abstractive summarization tasks
automatically extracting approximate summaries and ating training data to enable large scale semi supervised learning for extractive summarization
this is shown to be competitive to supervised learning techniques
using rnn and cnn rnn for extractive summarization
abstractive summarization for large documents
using document context vectors for improving the learning for text summarization
this novel approach is shown to beat the state of the art in a similar settings
comparing abstractive to extractive summarization under same setting of sequence to sequence learning
previous work several summarization techniques have been explored over the past decades following are some of the popular techniques surface level approaches consider title words and cue words e

important best
for extracting relevant sentences
corpus based approaches leverage structural distribution of words using internal or external corpus e

wordnet for summarization
cohesion based approaches considers cohesive relations tween the concepts within the text such as antonyms repetitions synonyms
using lexical chains graph based approaches are some of the most popular text summarization techniques
each sentence in the text is represented as a vertex and a graph is constructed around all the sentences where the edges correspond to the inter connections between the sentences
lexrank and textrank are two such techniques
machine learning based approaches document tion can be converted to a supervised or semi supervised learning problem
in supervised learning approaches hints or clues such as key phrases topic words blacklist words are used to label the tences as positive or negative classes or the sentences are manually tagged which is not scalable
once the labels are established a nary classier can be trained for obtaining the scores or likelihood score pertaining to each sentence
several tion techniques have been explored in the literature in this regard
however classication based approaches generalize well however they are not ecient in extracting document specic maries
training data for machine learning approach contains bel of the sentences irrespective of the document
if the document level information is not provided then these approaches provide same prediction irrespective of the document
providing document context in the models alleviates this problem which is what is one of the contributions of this paper
abstractive summarization techniques are less prevalent in the literature than the extractive ones
it is much harder because it involves re writing the sentences which if performed manually is not scalable and requires natural language generation techniques
the two common abstraction techniques are structured and tic both of which mostly are either graph tree based or ontology and rule e

template based
all the approaches mentioned above work well however they either face challenge towards scalability or large scale evaluation or do not generalize well
due to complexity constraints research to date has focused primarily on extractive methods but due to advancements in and natural language generation niques is making it possible to generate reasonable summaries for very short descriptions using abstraction
used reinforcement learning while used cnns for abstractive summarization
most of the current advancements are around short summaries from short documents
furthermore there does not exist any work where extractive and abstractive techniques have been compared in the similar setting using approaches
techniques based approaches have been used to ciently map the input sequences description document to map output sequence summary however they require large amounts data
with several examples model tends to learn the mapping tween input sequence and output sequence and generate more cient summaries corresponding to the input document
moreover it is found that models currently work well for smaller ument summarizations one two lines of the document mapping to abstractive and extractive text summarization deep learning day august london uk headlines phrase representation
even though els are providing benchmark results in machine translation and speech recognition tasks they have not yet performed well for summarization tasks dialog systems and evaluation of alog systems and are facing many challenges e

marizing long documents
for long sequence depicted that reversing the source sentence provides better results
architecturally our abstractive models are similar to with a change in encoder being novel document contextual lstm stead of simple attention based mechanism
document vector is described in section
furthermore unlike related ous work models in current study are summarizing full large uments longer than characters to generate relatively large summaries characters using beam search and vocabulary strictions constraints
furthermore we have not found any ture where based abstractive summarization is compared with related rnn cnn variations for extractive summarization which is novel addition of this paper
khatri et
al
describes summarization for ecommerce setting
user actions to infer document context websites which host user generated documents for consumption by other users provide myriads of ways for document discovery
for example users on ecommerce website discover relevant ument to their intent via search recommendation modules or ious topical pages
also sellers or content creators while creating the document or product pages provide lot of metadata on the ument for example title tags categorical
in this section we describe how we use historical information from document creator and document consumer to generate the document context
this context is then later combined with word embeddings obtained via skip gram model with negative sampling sgns to generate what we call document context vectors or dcv
we also use ment context to score sentences in the document for algorithmic labeling for large scale semi supervised learning to obtain imate summaries
stop words and high frequency words in ulary are not considered while obtaining the context words in the following subsections
let v be the vocabulary of the corpora
we assume that stop words have been removed from the corpora to generate the cabulary
also let n be the vocabulary size i
e
n
for the document d we generate three unit vectors namely s s denotes cd seller b b denotes browse
all of these vectors have dimension n but for document d most of the c sions are
we later combine these vectors with sgns vectors to make dcv
for brevity we drop the d from superscript is rest of the section as vectors are being generated per document only
q q denotes queries cd cd c c
document creators when a seller lists a product for sale on ebay they create a document for it which includes title key value meta data on xed dimensions like condition brand size color
they also type in verbose description containing images videos and html
in tion to this they select a leaf in the ebay taxonomy c


ln figure a snapshot of taxonomy used in a ebay
here the top taxa collectibles and art are expanded into lower taxa for example antiques
where li is taxon used in the taxonomy
the taxonomy used in ebay is a laminar family
that is given two taxa and either or or
seller has to attach a leaf in this taxonomy tree to the document
figure shows a small snapshot of ebay taxonomy
this omy tree is maintained and generated by domain experts and is of high quality
for example the document titled mens travel hiking military tactical army camo sling backpack chest shoulder bag is chosen to be put in the following leaf sporting goods outdoor sports camping hiking hiking backpacks day packs by the seller
context vector cs is induced from seller provided metadatams title subtitle taxon other key valued metadata such as brand color

for w v let cs w be the value of the dimension w in cs
it is dened as follows cs w in ms if w seller metadata otherwise we then normlaize the vector cs to a unit vector cs
we use the weight obtained for each context word later on to obtain the document context vector for each document giving relevance to words based on weights
this technique is not limited to ebay or any ecommerce based pages only
such an approach can be easily extended to the general textual documents available online in the form of webpages e

title metadata
or articles e

title sub titles abstract keywords category
c
document consumers buyers discover relevant inventory via search recommendations made to them or on topical pages e


ebay
com hiking backpacks i
html
amazon
com hiking backpacks and bags or from external sources like advertising
these discovery browse paths whether they are search trails or click trails that ends in a document are used to extract relevant words for the ument
one such discovery path is search
for example when the user searches with the queries like tactical sling backpack tary backpack and lands on the document titled mens travel ing military tactical army camo sling backpack chest shoulder bag the words of the queries contains what the user thought was the most descriptive information from the document
when such information is aggregated statistically cross large number of users it can provide great context for the document
let qsetd queries used to discover document d and browsed titles of documents via which user discovered document via deep learning day august london uk chandra khatri gyanit singh and nish parikh recommendation or topical page
for the dimension associated with word w values in both vectors are dened as in qsetd if w qsetd otherwise cb w in browsed if w browsed otherwise cq cb
we then normalize the vector cb cq to a unit vector ilar to the weights obtained for the context words from document creators we obtain the weights for words provided by the ment consumers readers
basically there are signicant tion provided by the readers based on their experience and ior
we can leverage the information provided by the readers to ther obtain more context words and corresponding weights which is later on used to obtain the document context vector
c
document context vectors for a document d we dened three unit vectors above s s denotes cd seller b b denotes browse
we combine these three to form a cumulative context unit vector cd by adding c c these three vector
q q denotes queries cd cd c cd q cd s q b cd s cd we used s q and b
these parameters may be ne tuned based on historical demand which may be estimated by ratio of trac volumes in dierent channels or by expert set existing priors for importance of various channels of trac
we then re weight value for each word dimension in cd by its idf to make cd i d
more precisely value for dimension for word w is dened as i d f w cd w i d for word w cd we combine context with word embeddings obtained via sgns to generate a vector per document that we call vd
let msg n s be the matrix of dimension n xk where k is the dimension of word embeddings and n is the vocabulary size of the corpora
row for word w in msg n s is the word embedding of the word w
we dene as vd cd i d msg n s note that dimension of vd is just like word embeddings

scoring sentences using document context set of for a document let s say it contains sentnecesd tences in d
we describe how to use document context to score these sentences
these score are used for generating algorithmic labels for large scale semi supervised learning to obtain mate summaries
for more details see section

for a sentence s sentnecesd where s


wk
sscor e vd w s the above score corresponds to the weighted sum for the words in the sentence
the weight for the words incorporate frequencies in seller provided metadata buyer s search history browse tory leading to discovery of the said product and also the inverse document frequence idf score of that word
the idf score for each word is simply the inverse of documents containing this word across all the documents at ebay
the idf score corresponds to the topical or document level relevance for each word in the lary
highly common words such as stop words the and
have nearly zero idf score
using the score above we obtain the score of each sentence
we use the sentence score to rank each tence within the document and select top till we reach acters sentences as the summary for a document product
we did an a b test to see if the summaries obtained using this technique correlates with user expectations and we observed statistically nicant lift in sales and user engagement
this implies that the summaries obtained using this semi supervised approach are ful
we use this technique to generate the training data for vised extraction based summarization techniques e

rnn
models this work primarily focuses on adding context as the initial state to rnns for abstractive and extractive text summarizations and paring it with various state of the art techniques
for extraction we use labeled supervised and semi supervised data
for tion we use titles and subtitles for training the models

nomenclature and base model recurrent neural network is a type of neural network which is an extension to feed forward nn with at least one feed back tion so activations can ow round in a loop
essentially information from prior observation along with the current vations are used to make predictions
the notations are borrowed from sutskever et al

rnn putes an output sequence


yt for a given input sequence


xt corresponding to the following equations ht hx xt w hh ht yt w yh ht this framework works when there is an alignment between put and output sequences that is the size of the input is same as size of the output
when the size of input and output sequences are dierent then two rnns with encoding decoding mechanisms are used cho et al

theoretically this framework should work however it is found that rnns with encoding decoding nisms nd diculties in mapping long sequences or when there are long term dependencies
gated recurrent units grus and long short term memory lstms solve this problem by troducing gates into the network to prevent vanishing gradient problem associated with rnns with long term dependencies
in current study lstms are used for extractive and tive summarizations
in standard lstm sequence of xed length is passed as an input to be encoded into a xed dimension vector v which is then decoded into the output sequence of words
to summarize lstm estimates the following abstractive and extractive text summarization deep learning day august london uk t t


yt


xt


in abstractive summarization document can be fed as an input during training and the summaries can be fed as output
however extractive rnn can be trained using standard supervised cation setup by performing soft max on the encoded layer


contextual recurrent neural network

abstractive contextual rnn ac rnn
it is an rnn architecture wherein a document context vector as described in section is passed as an input at rst time step along with the ument sequence in the encoder
the idea is that if a pre learned document context vector vd is passed as an input at the ning of the encoding stage then the model not only converges faster but also learns the summaries corresponding to the ment and not just the generic sequences
the basic idea is that if a reader is aware of the title of a document or abstract for a cation then it provides a better understanding and a high level terpretation of the document which makes the model to be able to provide more specic summaries corresponding to the documents
therefore document context vector vd at the rst time step sentially changes the encoding vector
an lstm decoder with similar architecture mentioned in ing have been used however the input at time to the decoder is the vector obtained from encoder
unlike other decoding mechanisms where output at time t can be any word from the cabulary the output from the document vocabulary is considered during the time of prediction making the inference faster
sutskever et al
and proposed beam search to obtain the most likely sentence in the machine translation task
however we perform a heuristics by using the trained model for re ranking all the sentences within the document based on likelihood of a sentence being a summary sentence during the inference
where likelihood of any sentence is dened as likelihood of decoding the sentence given the encoded input
so the abstractive model is used for extraction during the inference
we adopted the idea from a state of the art techniques for automatic speech tion wherein an rnn is used for re ranking the potential outcomes from an n gram based language model
we do this to address several issues avoid generic and short output issues with quence to sequence models obtaining grammatically correct sentences for ebay users to avoid poor customer experience and c avoiding legal push backs from the sellers


extractive contextual rnn ec rnn
it consists of encoder only
the encoder used in ec rnn is the replica of coder used in ac rnn with document context vector as an input at time and embedding representation of the words is passed as input to the model
however the output of encoder is used for binary classication sentence being a summary sentence or a not using softmax
note that each sentence starts with context vector therefore the classication of sentence happens given the context of the document and not just the sentence alone
in this way same sentence may be classied as summary sentence for table description distribution of a ebay products
total vocabulary size median document length median number of words median sentence length median nbr of words in sentence characters characters one document but not for others
furthermore given a context vector vd it is the extra information provided by the tence which dierentiates it from the other sentences in the ument which is a major drawback of other state of the art cation approaches wherein some sentences are always classied as a summary sentence

non contextual rnn architectures in current setup rnns trained without document context vector are termed as non contextual rnns
recently several architectures have been proposed in this regards however three main models which are explored in current study are

abstractive rnn a rnn
abstractive rnn is the ditional sequence to sequence model using lstm suggested by
the model is exactly similar to ac rnn without the text as input at time
the input at time in a rnn is a token start
a xed sized input and output sequences are used for ing by either curtailing or padding


extractive rnn e rnn
rnn has been used for sication tasks and it generates state of the art results
extractive rnn is non context version of ec rnn proposed in section


as mentioned before embeddings for words are pre calculated ing skip gram with negative sampling sgns technique and are used as input corresponding to each word in encoding layer
tures are extracted using embedding for classication task


convolutional rnn cnn rnn
convolution based lstm has performed extremely well in text classication tasks
furthermore convolution attention based encoder has been used for short summarization tasks
cnn lstm is used to sify the sentences with the same technique as e rnn however the dierence is that cnn is used to extract sequences of level phrase representations
as suggested by zhou et al
lstm is able to capture both local features of phrases global and temporal sentence semantics
cnns with multiple lters max pooling and dropout are used to extract high level phrase representations and then passed to lstm for classication using softmax
datasets table describes the distribution of a ebay description
vocabulary size of our dataset is k words
median document length is and words

datasets there are two kinds of datasets which are used in current study
deep learning day august london uk chandra khatri gyanit singh and nish parikh human extracted snippets items documents and responding details titles url description were provided to mans for extracting the summaries
the task was to extract and rank the sentences from the descriptions given ebay item url
items out of items were used for evaluations golden set and items were used for training dierent models
semi supervised large scale summarization tion sentences from item descriptions were extracted and ranked based on relevance towards the document context
section
provides information about how to rank a sentence in the order of relevance given the document and it s contextual details
eral techniques have been proposed by shen et al
lin et al
and based on query thematic similarity topic signature and tent semantic analysis
expanded the topics to wikipedia and obtained best results on duc summarization task
similar to approaches mentioned above we obtain the imate summaries for ebay item descriptions
after an evaluation from a ebay reviewers it is found that summaries generated ing document context based approach are of high quality and can be used for training the models
given the quality of these maries ebay launched this feature on mobile applications and site
on an a b test it is found that showing summaries using this approach have a high monetary value when compared to not ing the summary snippet
models trained using semi supervised approach will be evaluated on golden test set to identify the vance of this technique

data generation for classication task ec rnn c rnn cnn rnn and other classication based tive summarization techniques need the labeled data for training
for classication tasks sentences which have blacklist terms are labeled as non summary class while the sentences scoring high on document context metric are considered as positive sentences
blacklist terms are the words and phrases which do not contain item document level information and are frequently used at ebay such as returns me stars
we obtained terms using human curation and with statistical analysis of ebay item descriptions
for each description in items tences are tagged as positive or negative
sentences which were neither scoring high on document context metric nor having list terms were left out from being tagged
since the context sentence ranking is a new approach and is yet to be uated except using a b test in production the data is tagged for high precision
this work is a step towards evaluating context based sentence ranking as well
architecture details and experimentation
training details and model architectures abstractive context rnn ac rnn and abstractive rnn a rnn were trained using deep lstm with layers as described in sutskever et al
with cells and dimension word embeddings
since we wanted to nd the relative dierence ter adding the context in rnns for summarization task we kept the same parameters for both the models
parameters settings and table parameter setting for abstractive approaches like ac rnn and a rnn
parameters input description length output summary length optimization method learning rate batch size lstm parameters value words words stochastic gradient descent with momentum
reduced to half after every third epoch
uniform distribution from

table parameter setting for extractive approaches like rnn and e rnn
parameters maximum sentence length optimization method learning rate batch size lstm parameters value words adam
uniform distribution from

table parameter setting for convolutional rnn
parameters maximum sentence length dropout keep probability learning rate filter sizes for convolution batch size cnn and lstm parameters max pool size value words

random normal centered at with standard deviation
model details which worked the best in our case are mentioned in table
extractive context rnn ec rnn and extractive rnn e rnn were trained using two lstm layers with cells and dimension word embedding
since we wanted to nd the tive dierence after adding the context in rnns for summarization task we kept the same parameters for both the models
parameters settings and model details which worked the best in our case are mentioned in table
convolutional rnn cnn rnn consists of two neural works
cnn for high level phrase representations and then lstms for obtaining the temporal and sequential nature of the text
we used single layer convolution with lter size equal to and a gle layer lstm with cells and dimension word embeddings were used
parameters settings and model details which worked the best in our case are mentioned in table
abstractive and extractive text summarization deep learning day august london uk table dierent type of experiments done and metrics used for those experiments
evaluation setting classication similarity ranking description given a sentence classify whether it is summary sentence or not
given golden summaries nd the similarity score given sentences rank them by in the order of relevance towards summary evaluation metric accuracy precision recall f score rouge rouge l bleu tf idf cosine similarity topic similarity ndcg mean average precision
experiments and evaluation metrics we split our k human extracted summaries dataset into k and k parts
we use k for all evaluations
we also used the data tained using semi supervised technique for training and ing the models
supervised we train our models on k human extracted summaries
these models are trained with three dierent settings of target summary lengths
we use sentence sentence and sentence summary lengths
semi supervised we use k documents and approximate maries generated via document context for training models at scale
we use k human extracted summaries for evaluation purposes
we also baseline with a fuzzy summarization strategy where dom sentences are picked in the summary output

results for supervised setting in this section we present the result for supervised models for marization
for this puproses we trained our models on k human extracted summaries and evaluated on k human extracted maries
table compiles the performance result for all summarization strategies
it is clear that rnn s as a whole outperform other niques naive bayes svm lsa lexrank textrank on rouge bleu and token similarity
in both abstractive and extractive rnn s adding document context improves all metrics
for example and bleu for abstractive rnn is
and
respectively
when document context is added ac rnn then and bleu creases to
and
respectively
similarly for extractive rnn e rnn vs extractive contextual rnn ec rnn changes from
to
and bleu changes from
to

for large target summaries which are sentence long tive contextual rnns perform the best followed by abstractive textual rnns
for example for ec rnn is
and for ac rnn is

similarly bleu is
compared to
for rnn
for small target summaries sentence long we observe that abstractive context rnn ac rnn outperform extractive rnn ec rnn
where is
for ec rnn compared to
for ac rnn
for target summaries which are sentences long results are shared in table
as the target summary length creases ecacy of extractive contextual rnn increases over stractive contextual rnns
in table we present classication metrics for extractive els
all models are doing a good job of separating summary sentences from non summary sentences
for this task ec rnn outperforms other methodologies as well
table results on supervised task using human extracted summaries k for training and k for evaluation
tions with target summary a sentence long and tences long
abstractive context rnn ac rnn performs best in all metrics for short summaries
extractive context rnn ec rnn performs best for longer summaries
adding context as a whole created improvements in rnn models
bleu topic sim
model fuzzy e rnn ec rnn cnn rnn a rnn ac rnn nb svm lsa lexrank textrank fuzzy e rnn ec rnn cnn rnn a rnn ac rnn nb svm lsa lexrank textrank rouge
































token sim
target snippet length sentence










target snippet length sentence























































































table describe ranking metrics for summarization models
we assign summary sentences as relevance of and other sentences relevance score of
the task is to generate ranking of sentences in a way that picks the summary sentences before non summary tences
for this task as well context aware rnns win out
tive context rnn ec rnn have the highest and
deep learning day august london uk chandra khatri gyanit singh and nish parikh table results on supervised task using human extracted summaries k for training and k for evaluation
ations with target summary when target summaries are extremely long sentence
extractive context rnn rnn performs best
adding context as a whole created provements in rnn models
bleu topic sim
model fuzzy v rnn c rnn cnn rnn a rnn ac rnn nb svm lsa lexrank textrank rouge token sim
target snippet length sentence

































































table classication for extractive supervised model classes non summary
training k human judged k human judged
extractive context rnn rnn shows best performance
note no classication for stractive models
model accuracy nb svm e rnn cnn rnn ec rnn




precision








recall







f score









table supervised model ranking evaluation
training k human judged k human judged
tive context rnn ec rnn shows best performance
model fuzzy e rnn ec rnn cnn rnn a rnn ac rnn nb svm lsa lexrank textrank












































results for semi supervised setting in this section we share the result of training with algorithmically generated approximate summaries
these approximate summaries are generated by using document context
for extractive rnn and other classication approaches like svm nb human labeled and table classication for semi supervised extractive pervised model classes non summary
training k algorithmically labeled data k human judged
ec rnn shows best performance
model accuracy e rnn cnn rnn ec rnn e rnn cnn rnn ec rnn





precision recall semi supervised











supervised f score























algorithmically labeled data was used for training
whereas for stractive rnn we use title and subtitles for learning
table compares the classication metric for extractive els when trained with human extracted summaries to cally labeled approximate summaries
it can be seen that training models on large scale approximate summaries does not lead to any drop in precision recall and accuracy
table compares the summarization results for all rnn els abstractive vs extractive
training on k approximate maries does not lead to drop in metric
for example drops from
ec rnn supervised to
ec rnn semi supervised
bleu also remains comparable with
for dataset and
for k dataset
ranking metrics are compared in table
as the size of data increases for training abstractive methods performance increases tremendously
for example ac rnn have of
vs
for k vs k documents respectively
same result hold for map as well where increase from
to
when data is increased from k to k for ac rnn
for extractive rnn which are trained on approximate summaries we do nt see a drop in ranking metrics between k human extracted vs k approximate summaries
remains
in both cases
overall it can be seen that abstractive rnns are improving with more data
extractive rnn are able to generate near similar formance with large scale approximate summaries as with small scale human extracted summaries
and adding document context to rnn with approximate summaries further boost the performance for both abstractive and extractive rnn
conclusion we proposed a novel document context model for stractive and extractive summarization
we have shown that rnns and other models are powerful and beat state of the art summarization approaches in e commerce setting
the idea of adding contextual information at the rst time step during the encoding of the input to output sequence label mapping aligns with humans since generally humans tend to read title abstract and gather other contextual information before reading the entire document articles
this gives humans high level understanding of the document which abstractive and extractive text summarization deep learning day august london uk table result of summarization for semi supervised tractive supervised model
training k algorithmically labeled data k human judged
extractive text rnn ec rnn shows best performance
abstractive contextual rnns show signicant improvements with large training data
model token sim
semi supervised rouge bleu topic sim

e rnn ec rnn
cnn rnn


a rnn ac rnn
e rnn ec rnn
cnn rnn


a rnn ac rnn









supervised







































table ranking metrics for semi supervised extractive supervised model
training k algorithmically labeled data k human judged
abstractive contextual rnns show most improvements with large training data
tractive contextual rnns also show improvements
model semi supervised supervised e rnn



ec rnn





cnn rnn













a rnn ac rnn















if incorporated in model will generate much richer ment specic summaries
training is performed in a human tagged supervised setting as well as with large scale semi supervised tracted summaries
it is found that based rnn tion techniques out performs other state of the art summarization techniques
within rnns contextual rnns outperform non rnns on most of the similarity and ranking measures
contextual rnns are found to be best performing followed by contextual rnns for large summaries however for shorter maries abstractive contextual rnns outperform all other techniques followed by extractive rnns with attention and more cated setting it can be possible to further improve abstractive niques
we have also depicted that abstractive rnns contextual contextual can be used for extraction tasks and still beat the tractive systems
it is found that large scale semi supervised data for training improves the performance of the models on the evaluation dataset
hence training the extractive models with proximate summaries leads to better results compared to relatively smaller human tagged supervised data
we think that advantage of large scale training outperforms the noise in approximating maries
we recommend other researchers to incorporate context in other tasks e

machine translation task
references text summarization with tensorow

googleblog
text summarization with tensorow
html
accessed
r
m
badry a
s
eldin and d
s
elzanfally
text summarization within the international journal latent semantic analysis framework comparative study
of computer applications
r
barzilay and m
elhadad
using lexical chains for text summarization
vances in automatic text summarization pages
t
baumel m
eyal and m
elhadad
query focused abstractive summarization incorporating query relevance multi document coverage and summary length constraints into models
arxiv preprint

j
cheng and m
lapata
neural summarization by extracting sentences and words
arxiv preprint

k
cho b
van merrinboer c
gulcehre d
bahdanau f
bougares h
schwenk and y
bengio
learning phrase representations using rnn encoder decoder for statistical machine translation
arxiv preprint

s
chopra m
auli a
m
rush and s
harvard
abstractive sentence rization with attentive recurrent neural networks
proceedings of naacl pages
j
chung c
gulcehre k
cho and y
bengio
empirical evaluation of gated current neural networks on sequence modeling
arxiv preprint

d
das and a
f
martins
a survey on automatic text summarization
literature survey for the language and statistics ii course at cmu
j
l
elman
finding structure in time
cognitive science
g
erkan and d
r
radev
lexrank graph based lexical centrality as salience in text summarization
journal of articial intelligence research
j
gehring m
auli d
grangier d
yarats and y
dauphin
convolutional quence to sequence learning
arxiv preprint

y
gong and x
liu
generic text summarization using relevance measure and latent semantic analysis
in proceedings of the annual international acm sigir conference on research and development in information retrieval pages
acm
f
guo a
metallinou c
khatri al
topic based evaluation for conversational bots
arxiv preprint

u
khandelwal
neural text summarization
c
khatri n
parikh s
solanki al
snippet extractor recurrent neural works for text summarization at industry scale
us patent
c
khatri n
parikh s
solanki al
snippet generation and item description summarizer
us patent
c
khatri s
voleti s
veeraraghavan n
parikh a
islam s
mahmood n
garg in big data big and v
singh
algorithmic content generation for products
data ieee international conference on pages
ieee
p
li w
lam l
bing and z
wang
deep recurrent generative decoder for abstractive text summarization
arxiv preprint

c

lin
rouge a package for automatic evaluation of summaries
in text summarization branches out proceedings of the workshop volume
barcelona spain
k
lopyrev
generating news headlines with recurrent neural networks
arxiv preprint

h
p
luhn
the automatic creation of literature abstracts
ibm journal of search and development
r
mihalcea and p
tarau
textrank bringing order into texts
association for computational linguistics
t
mikolov i
sutskever k
chen g
s
corrado and j
dean
distributed resentations of words and phrases and their compositionality
in advances in neural information processing systems pages
g
a
miller
wordnet a lexical database for english
commun
acm nov

i
f
moawad and m
aref
semantic graph reduction approach for abstractive text summarization
in computer engineering systems icces seventh international conference on pages
ieee
r
nallapati f
zhai and zhou
summarunner a recurrent neural network arxiv based sequence model for extractive summarization of documents
preprint

r
nallapati b
zhou c
gulcehre b
xiang al
abstractive text arxiv preprint marization using sequence to sequence rnns and beyond


v
nastase
topic driven multi document summarization with encyclopedic knowledge and spreading activation
in proceedings of the conference on pirical methods in natural language processing pages
association for computational linguistics
r
paulus c
xiong and r
socher
a deep reinforced model for abstractive summarization
arxiv preprint

a
ram r
prasad c
khatri and a
venkatesh
conversational ai the science behind the alexa prize
arxiv preprint

deep learning day august london uk chandra khatri gyanit singh and nish parikh a
m
rush s
chopra and j
weston
a neural attention model for abstractive sentence summarization
arxiv preprint

g
singh n
parikh and n
sundaresn
user behavior in zero recall ecommerce in proceedings of the international acm sigir conference on queries
search and development in information retrieval pages
acm
h
f
song g
r
yang and x

wang
training excitatory inhibitory recurrent neural networks for cognitive tasks a simple and exible framework
plos comput biol
j
steinberger and k
jezek
evaluation measures for text summarization
puting and informatics
i
sutskever j
martens and g
e
hinton
generating text with recurrent in proceedings of the international conference on machine ral networks
learning pages
i
sutskever o
vinyals and q
v
le
sequence to sequence learning with neural in advances in neural information processing systems pages networks

a
venkatesh c
khatri a
ram f
guo et al
on evaluating and comparing conversational agents
arxiv preprint

s
wiseman and a
m
rush
sequence to sequence learning as beam search optimization
arxiv preprint

w
xiong wu f
alleva j
droppo x
huang and a
stolcke
the microsoft conversational speech recognition system
arxiv preprint

y
xu j
lau t
baldwin and t
cohn
decoupling encoder and decoder works for abstractive document summarization
aclweb

c
zhou c
sun z
liu and f
lau
a c lstm neural network for text tion
arxiv preprint


