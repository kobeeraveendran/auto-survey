how to train your agent to read and write li mengge guanghui mingkui qi school of software engineering south china university of technology pazhou laboratory university of adelaide key laboratory of big data and intelligent robot ministry of education seliushiya semenggehe
scut
edu
cn
edu
cn qi

edu
au n a j l c
s c v
v i x r a abstract reading and writing research papers is one of the most leged abilities that a qualied researcher should master
ever it is difcult for new researchers e

students to fully grasp this ability
it would be fascinating if we could train an intelligent agent to help people read and summarize pers and perhaps even discover and exploit the potential knowledge clues to write novel papers
although there have been existing works focusing on summarizing i
e
reading the knowledge in a given text or generating i
e
writing a text based on the given knowledge the ability of ously reading and writing is still under development
cally this requires an agent to fully understand the knowledge from the given text materials and generate correct and ent novel paragraphs which is very challenging in practice
in this paper we propose a deep reader writer draw network which consists of a reader that can extract edge graphs kgs from input paragraphs and discover tial knowledge a graph to text writer that generates a novel paragraph and a reviewer that reviews the generated graph from three different aspects
extensive experiments show that our draw network outperforms considered lines and several state of the art methods on agenda and m agenda datasets
our code and supplementary are leased at
com menggehe draw
introduction currently hundreds of papers are published online every day even on small topics
however a study wang et al
shows that us scientists can only read papers per year on average
thus researchers are exhausted by following the sharply increased numbers of papers much less to standing the research and coming up with new ideas to write novel papers gopen and ja buenz
in practice writing novel papers requires not only the abilities of reading and reasoning but also the ability of creative thinking which is nontrivial for most fresh researchers xiao et al

it would be fantastic if an agent could help people especially authors contributed equally
corresponding author
copyright association for the advancement of articial intelligence www
aaai
org
all rights reserved
figure an intuitive understanding of our draw network
first the draw network reads multiple related works and discovers potential knowledge among them
and then it writes a new paragraph based on knowledge graph
last it reviews the output and uses feedback rewards to improve the quality of writing
new researchers to read and write
however building such an agent encounters several challenges
first to understand multiple related works the agent needs to capture complex logic in the related works which is trivial
several knowledge extraction methods min et al
gerber and chai yoshikawa et al
achieve it by identifying entities in the texts extracting the ships between these entities and representing them as a knowledge graph kg
however they have trouble in covering potential connections among these entities which hampers a comprehensive understanding of related works
second after generating a kg the agent is then required to decode a uent novel paragraph from the kg
in practice however how to evaluate the quality of the generated texts accurately is still an open problem
existing methods kedziorski et al
wang et al
adopt the forcing scheme that aims to match the tokens in the generated texts to the tokens in the target texts
however these methods related workswriteknowledge graphnew paragraphscores only focus on token level matching while ignoring level and graph level evaluation of the generated texts
in this paper we propose a method named deep writer draw
our draw network is able to read multiple texts discover potential knowledge and then write a novel paragraph
from figure the draw network consists of i
e
reader writer and reviewer
three modules cally the reader rst extracts kgs from the research texts and discovers potential knowledge to enrich the kgs
the reader considers the multi hop neighborhood to predict new links among conceptual nodes
then the writer writes a novel paragraph to describe the main idea of the enriched kgs using a graph attention network which aggregates the global and local graph information
inspired by the review process of research papers we further propose a reviewer module to evaluate the quality of the generated paragraphs and return rewards as feedback signals to rene the writer
to be specic given a generated paragraph the reviewer will output three feedback signals including a quality reward which reects the metric scores of the generated paragraph an adversarial reward which denotes the probability of the generated paragraph passing the turing test and an alignment reward which represents the matching score tween the generated paragraphs and the enriched kgs
in this way the writer is able to write better paragraphs that clearly represent the key idea of the enriched kgs
in summary our main contributions are threefold we propose a deep reader writer draw network that reads multiple research texts and then discovers potential knowledge to write a novel paragraph covering the key idea of the source inputs
we propose a feedback mechanism to review whether the generated paragraph is consistent with the enriched kg and whether the generated paragraph is human written thereby greatly improving the quality of paragraph tion
extensive experiments show that our writer reviewer leads to signicant improvements in the kgs to text eration task and outperforms the state of the art methods
related work automatic writing
paperrobot wang et al
forms as an automatic research assistant to incrementally write to chemical related research datasets
it enriches kgs by predicting links of input papers kgs
according to a given title it then selects several entities that are related to the title in enriched kgs to generate texts
however robot neglects to consider the multi hop neighborhood to predict links which is very important for capturing potential relationships
in addition the generated texts do not closely align with the kgs
to address this we use a graph attention network to consider the multi hop neighborhood capturing the complex and hidden information that is inherently plicit in the neighborhood
moreover we design a reviewer to measure the quality of the generated text from different mensions to effectively align with the kgs
in particular our draw network is different from the multi document mary ling and hui which compresses the lengthy document content into several relatively short paragraphs
we not only extract important knowledge but also discover potential knowledge from multiple paragraphs by predicting links and writing a novel paragraph
link prediction
some translation based approaches al
zhen et al
lin et al
are widely used in link prediction but result in poor representation ability
recently cnn based models dettmers et al
nguyen et al
have been proposed for relation prediction
these methods only focus on the entity and its neighborhood while not considering the relationships among these nodes
other methods kipf and welling schlichtkrull et al
take the relationships among the entities and their hop neighbors into consideration
however they still omit the information from multi hop neighborhood
instead we pose a reader module to capture semantic information of the multi hop neighborhood in the kg
graph to text task
graph to text is an active research area
some works generate texts based on structured edge trisedya et al
xu et al
feng et al
while several neural graph to text models use different coders based on gnn ribeiro gardent and gurevych zhijiang et al
huang et al
and former vaswani et al
architectures to learn graph representations
koncel kedziorski et al
proposes a novel graph transformer encoder which leverages the topological structure of kgs to generate texts
however it ignores the global graph information which is important for text tion
to solve this ribeiro et al
introduce a novel architecture that aggregates both global and local graph information to generate texts
however such an encoder decoder framework presents some problems such as word repetition and lack of diversity
to solve these issues we propose a reviewer module to review the generated paragraphs and rene the quality of paragraphs using feedback rewards
our reviewer consists of three modules to review and evaluate whether the generated paragraphs are real and to align with the given kgs in order to improve the text generation ability
proposed method in this paper we focus on generating novel paragraphs via reading multiple ai related paragraphs
to this end we pose a deep reader writer draw network that consists of three modules namely reader writer and reviewer as shown in figure
to understand and sort out the textual logic of given paragraphs the reader rst reads and extracts knowledge graphs kgs from them
and then considering the multi hop neighborhood the reader predicts new links between conceptual nodes namely potential knowledge to enrich the kgs
the writer adopts a graph encoder to encode the rich semantic information in kgs and delivers it to a text encoder to generate a novel paragraph
inspired by the adversarial learning cao et al
wang et al
cao et al
chen et al
we also devise a reviewer to evaluate the quality of the generated paragraph which serves as a feedback signal to rene the writer
we relate the details of these modules in the following sections
figure an overview of our deep reader writer draw network
the draw network consists of three modules namely reader writer and reviewer
given multiple related works the reader rst extracts knowledge to construct initial knowledge graphs kgs and performs link prediction to enrich kgs
based on the enriched kgs the writer captures global and local topology information using a graph encoder and generates a novel paragraph with a text decoder
in particular the reviewer employs three feedback modules to measure the quality of the generated paragraph

reader text to graph to extract the textual logic from the given related paragraphs we use the standard sciie luan et al
a science domain information extraction system to constrcu edge graphs specically the output of the sciie system is a list of triplets where each triplet consists of two entities and the corresponding relation
the knowledge graph noted as gi v r where v is the node set r i is the edge set and n represents the number of nodes
v and r represent the extracted entities and the relations respectively
however the initial knowledge graph gi does not exploit potential knowledge
to address this we perform a link prediction to predict new links between entities based on the initial kgs
link prediction
given kgs gi we obtain the entity bedding rd and relation embedding rij rd with two separated embedding layers where is the feature sion
formally given entity embedding vj and relation embedding rij between them the triplet is represented by rij
to aggregate more information we introduce auxiliary edges between one entity and its n hop hood
for the entity and its n hop neighborhood we sum the embeddings of all the relations in the path between them as the auxiliary relation embedding
we apply a linear formation to update the entity representation rd by rij where is a trainable parameter and denotes the catenation operation
a particular entity vi may be involved in multiple triplets and its neighborhood can be denoted as i denotes the k th neighborhood of the i th entity
to learn the importance of each triplet for the entity we apply a self attention to calculate attention weights as i where follows exp exp with the help of the attention weights we update feature vi rd by fusing the information from its neighborhood i
e
i i vi k i where is a trainable parameter and is the sigmoid function
based on the original relation feature rij we apply a ear transformation to obtain the updated relation embedding rij rd
after updating the node and relation embeddings we need to determine whether there is a relationship between two given entities
an intuitive way is to calculate the ability for each triple
following convkb nguyen et al
we train a scoring function to perform the relation prediction as follows sm rm where denotes a convolution operation is a set of convolution lters and fc is a linear transformation layer
following nathani et al
we assign a score sm to the triplet vi rm vj in eqn
which indicates the probability that the triplet holds
for each entity we rst traverse all entities and relationships to construct triples and then we select the triplet with the highest score as the new link
in this way the reader can capture potential relations between different nodes and derive a new graph gp
finally we denote the enriched knowledge graph as g gi gp

writer graph to text based on the enriched graph g with n entities we propose a writer to generate novel paragraphs which consists of a bcedafhggraph encoderwriter related workstext decoderbcdaafefhgknowledge graphreaderrewardrevieweralignmentmoduleadversarialmodulequalitymodulenew paragraphreading and writing researchpapers recently we consider the task


in this paper we propose anew in this paper we consider the graph encoder and a text decoder
specically the writer rst uses the graph encoder to extract the knowledge resentations and then writes a new paragraph with the text decoder vaswani et al

graph encoder
a comprehensive understanding of a kg g is the rst step to generate the desired paragraph
however it is difcult to directly capture rich semantic information in the knowledge graph g
to address this we extract the edge representations within two sub encoders i
e
graph encoder and local graph encoder
following cge lw ribeiro et al
we integrate global context information and local topology information to generate new paragraphs
to aggregate global context information we rst nate all of the node features v and feed them into the graph encoder as follows





vn where is a standard transformer encoder vaswani et al
which contains multi head self attention layers and feed forward networks
in the global graph encoder we treat the knowledge graphs g as a fully connected graph without labeled edges
based on the self attention mechanism the global graph encoder is suitable for discovering the global correlation between nodes
each node rd has the ability to capture all nodes information
to better represent the interaction between nodes we need to build local relations between each node and its hood
however the global graph encoder does not explicitly consider such graph topology information
to address this we use the local graph encoder to model the local relations
for each node we rst calculate attention weights for its adjacent nodes since the different types of relationships have considerable discrepancies in impact when fusing tion
based on the attention weights we obtain the hidden node features by aij rij where jni jni aij
here denotes the model parameters and denotes the hidden features which encode the local interaction between the i th node and its neighborhood
ni denotes the bourhood of the i th node
we also perform the multi head attention operation to learn structural information from ferent perspectives
we employ a gru cho et al
to merge local information between different layers as follows hi where the nal node representation hi rd
text decoder
based on the node representations h we use the standard transformer decoder vaswani et al
to generate a novel paragraph with t words in an auto regression manner
at each step t the text decoder consumes the previously generated tokens as additional input and outputs a probability pt over candidate vocabularies
we train the writer with supervised learning as follows lsl t where yt is the ground truth one hot vector at step t and generates words by selecting the element with the highest score at this step
in practice the text decoder also can use other sequence generation models such as lstm hochreiter and schmidhuber and so on

reviewer feedback rewards the encoder decoder framework has made great progress in many sequence generation tasks including text rization and image captioning
nevertheless it suffers from some problems
for each training sample such a framework tends to use only one word as ground truth at each generation step even if other candidate words are also reasonable
this leads to a lack of diversity in the generated text
moreover the language is so complex that it requires us to evaluate the quality of the generated paragraph from different dimensions such as grammatical correctness topic relevance language and logic coherence
inspired by the review process of a research paper we propose a reviewer module to review the generated paragraph from different dimensions
the output of reviewer can be used as an auxiliary training signal to optimize the writer which is similar to researchers further polishing the paper based on reviews
specically we design three feedback rewards in the viewer
first we use the metric scores of the generated graph as a reward to meet the rules of these metrics
second we train a turing test discriminator to determine whether the paragraph is generated by an agent or written by a human which draws on the idea of adversarial training and requires the paragraph to conform to the natural language cation
third we design an alignment module to align the generated paragraphs and the corresponding enriched edge graphs which ensures the correctness and completeness of the generated texts
different from teacher forcing ods the reviewer focuses on sentence level and graph level alignment
given a generated paragraph however the above evaluation processes are non differentiable
as discussed in seqgan yu et al
the discrete signals are limited in passing the gradient update from the reviewer to the writer
to address this we denote the outputs of reviewer as rewards r and maximize expectation rewards e via reinforcement learning
formally the goal of reviewer can be represented by max ep where denotes the trainable rameters of our model and is the paragraph generated by the writer based on the generation probability p w

t

specically the reward function is denoted as ar m r where and correspond to the three modules of the reviewer
ar and m r control the contribution of the corresponding reward
following policy gradient ods williams schulman et al
we can solve the above problem in batch training as follows lrl ep logp logp b b b where b is the training batch size
now we introduce these reward modules in detail
quality reward
given a generated paragraph we can calculate some quantitative metrics for it such as et al
meteor denkowski and lavie cider vedantam zitnick and parikh
directly using these metrics as the training reward can boost the sentence generation quality
in this paper we simply adopt the bleu score as the reward since the bleu score is one of the most popular automated and inexpensive metrics
in practice the bleu can be replaced with any metric that needs to be optimized
adversarial reward
based on a paragraph this ule acts as a discriminator to determine whether is manual annotation real or generated by the machine fake
ing yu et al
we use convolutional neural network cnn to extract text features since it can capture sequence information and has shown exhibited high performance in the complicated sequence classication task zhang and cun
specically given a generated paragraph we rst concatenate the token embedding as the text tion
we then use different numbers of kernels with different window sizes to extract different features over the text sentation and produce a new feature map
after applying a max pooling operation we perform a fully connected layer with sigmoid activation to output a probability which notes the probability that the input text is real
the calculation can be formulated as
inspired by adversarial training cao et al
this module aims to minimize the performance gap between humans and the writer
alignment reward
a paragraph is supposed to align its enriched kg g since is generated by writer according to the g
in this sense we propose to compute the similarity between and g based on the attention mechanism
given an abstract with t words we rst use long short term memory lstm to extract text representation c where rd t


t
following attngan xu et al
we obtain the hidden representation as follows qt softmax wv h d where wq wk and wv are trainable parameters d is a scaling factor and h rdn are node features obtained from the writer
with the help of the self attention nism vaswani et al
the hidden feature qt rd not only fuses the text representations but also merges graph formation
then we calculate the cosine similarity as ing score as follows t t ct
thus far we can obtain the rewards and from above the reviewer modules
finally to train our draw network we dene the overall training loss as follows l lsl rl lrl where rl is a trade off parameter
lsl trains the draw network within supervised learning while lrl allows the draw network to explore diverse generation via ment learning and evaluate the generation from multiple entations
experiments
datasets agenda dataset
agenda is one of the most popular kgs to text datasets which concludes pair samples collected from the proceedings of top ai conferences
each sample consists of a title an abstract and the sponding kg which is extracted by the sciie system
the kg is composed of recognized scientic terms and their tionships
in particular the types of scientic terms include task metric method material and other
the types of tionships include used for conjunction feature of part of compare evaluate for and hyponym of
m agenda dataset
to further demonstrate the tiveness of our draw network we create a new dataset called m agenda
specically we rst calculate the sine similarity between each abstract and the others in the agenda dataset
we select two most related instances for each one and combine these three as a new data example in the m agenda dataset

experimental settings implementation details
our draw network consists of three well design modules i
e
reader writer and reviewer
we rst train our reader writer and reviewer on agenda dataset
then we use the trained reader and writer model on the m agenda to generate novel paragraphs
to speed up convergence early in training we adopt different ing strategies for each module
for the reader we rst use bordes et al
to train entity and relation dings
we then aggregate information passed from a hop neighborhood to update the embedding of each node
ing nathani al
we use adam optimization with an initial learning rate of

for the writer we pre train for epochs with early stopping
following ribeiro et al
we use adam optimization with an initial learning rate of

to ensure the generation effect we set the maximum generation length to
for the reviewer we pre train the adversarial module with sgd optimization and initialize a learning rate of

when pre training the graph encoder of the alignment module we use the same model and rameters of writer
in addition we systematically adjust the values of ar and m r to conduct several ablation studies
we nd that the experimental results of different coefcient combinations uctuate only around
causing little effect on the results
writer reviewer obtains the best results with ar m r
we set the trade off parameter rl
we implement our method with pytorch
model graphwriter graformer cge lw writer reviewer ours bleu meteor cider












model writer writer reviewer ours bleu meteor cider














table quantitative evaluations of generation systems on the agenda dataset higher is better
table ablation study for modules used in the reviewer on the agenda dataset
paragraph turing test results human machine written by human written by draw table quantitative results of turing test
model paperrobot cge lw draw ours grammar coherence





informativeness


table automatic evaluations results higher is better
evaluation metrics
to demonstrate the quality of the erated paragraphs we report both quantitative results and human study results
we divide our evaluation into two parts kgs to text evaluation and overall performance evaluation
for kgs to text evaluations we adopt three general titative evaluation metrics i
e
bleu papineni et al
meteor denkowski and lavie and cider tam zitnick and parikh to evaluate our reviewer
in addition to demonstrate the realness of the paragraphs generated by our model we also set up a ing test
specically we randomly select abstracts and shufe them to nd an evaluation set where half of the stracts are written by authors and the rest are generated by our writer reviewer
after that we test the turkers on amazon mechanical turk amt to determine whether the paragraphs in the evaluation set are written by humans
for overall performance evaluation we set up a human study to rate the abstracts generated by draw network cge lw and paperrobot
for each model we randomly select generated paragraphs and score them in terms of grammar informativeness and coherence on amazon mechanical turk amt
specically the metric grammar measures the paragraphs written in well formed english
the metric informativeness denotes whether the paragraphs make use of appropriate scientic terms
the metric herence denotes that the generated text conforms to general specications
for example a complete abstract should clude a brief introduction to a task describe the solution analyze and discuss the results and so on
each metric scribed above contains levels with rankings from to from bad to good
following the relation prediction task nathani et al
we evaluate our link prediction method of reader on the proportion of correct entities in the top n ranks for and

kgs to text evaluation on agenda dataset to verify our model on kgs to text task we compare our writer reviewer against several state of the art models cluding graphwriter koncel kedziorski et al
an graformer schmitt al
and cge lw ribeiro et al
on the agenda dataset
results
we report the results of our method and other compared models with respect to three quantitative evaluation metrics in table
as shown in table our writer reviewer achieves better performance than all the compared models in three quantitative evaluation metrics
specically our reviewer outperforms the state of the art method cge lw by
points in bleu
points in meteor and
points in cider
these results demonstrate the superiority of our writer reviewer in the kgs to text task
in addition we carry out a human evaluation to strate the effectiveness of our writer reviewer
to be specic for each paragraph in the evaluation set we ask the human to choose whether these paragraphs are written by authors
from these results in table nearly half of the paragraphs generated by our writer reviewer are reviewed as written by humans
more critically of the paragraphs written by humans are chosen as written by the ai system
these results demonstrate that our writer reviewer can erate realistic paragraphs similar to those written by humans
ablation studies in reviewer
to investigate the effect of different modules in reviewer we conduct an ablation study
as shown in table writer combined with one of the ules in reviewer arbitrarily obtains better performance than writer which demonstrates the effectiveness of the modules in reviewer
writer combined with all the modules in viewer namely writer reviewer achieves best performance

evaluation on m agenda dataset to show the effectiveness of our draw network we duct experiments on the m agenda dataset
since the agenda dataset does not provide ground truth we conduct human study instead of quantitative evaluations
specically for each metric in the human study we average the scores of the paragraphs rated by the humans as the nal score
results of draw
we report the experimental results of our draw network and other compared methods in table
from these results our draw network achieves the best performance in terms of grammar coherence and formativeness
specically paperrobot wang et al
initial kgs paperrobot entities relations global scene level contextual information part of spatial context recurrent convnet model wikipedia used for multilingual ner systems local image de scriptors tion spatial congurations


in this paper we propose a novel approach for multilingual named entity recognition tasks
the proposed method is based on semantic similarity measure that can be used to improve word retrieval performance by using wikipedia type of words from text documents and then build an efcient query language model which allows users with similar information between entities as clusters across different domains part of speech tags are generated through each user s document representation our knowledge base system was evaluated over state of the art approaches trained object


covering entities
cge lw draw in this paper


we propose a spatial context recurrent convnet model to incorporate global scene level contextual information into a spatial context recurrent convnet model for object retrieval



and the contextual information from candidate boxes is used for object retrieval
a positional language model that captures contextual information from candidate boxes for object retrieval
the proposed system is evaluated on the tac kbp data and the experimental results show that the proposed system can signicantly improve the entity linking performance


covering entities
in this paper we propose a novel approach to entity based on statistical language model based information which exploits both local contexts and global world to improve the entity performance



we propose a spatial context recurrent convnet model to integrate global context features with local image de spatial congurations and global scene level contextual into a spatial context recurrent convnet model


and a recurrent network with local and global information to guide the search for candidate boxes for object retrieval


covering entities
table example outputs of various models
to better visualize the generated text we omit information irrelevant to the comparisons
repetitive words are represented in red and entities included in kgs are represented in orange
the potential knowledge is represented in blue with the corresponding superscript
method paperrobot our





table accuracy of the link prediction on the m agenda dataset
values are in percentage
obtains poor performance due to the neglect of the logical structure between entities
cge lw ribeiro et al
takes advantage of the graph information effectively and achieves

and
points in terms of three metrics but it also ignores the fact that the generated graphs are supposed to match the kgs
different from the methods above our draw network not only performs link prediction with multi hop information in the reader but also matches the graphs and the generated paragraphs and thus achieves the best performance
more ablation experiments about reader can be found in the supplementary material
results of reader
as shown in table we report the perimental results of the link prediction method of our reader and paperrobot
our method achieves the scores of

and
outperforming the paperrobot by

and
points respectively
it demonstrates the effectiveness of our link prediction method
visualization analysis
as shown in table we visualize a generated paragraph of our draw network
more ization results can be found in the supplementary material
we see that our draw network has the ability to cover more entities represented in orange while paperrobot mentions less entities in the given kg
in addition cge lw tends to repeat unrelated entities sentences represented in red
with the help of reviewer the generated text of draw network is uent and grammatically correct
moreover our draw work is able to discover the potential relationships between entities represented in blue superscript
conclusions and future work in this paper we propose a deep reader writer draw work that reads multiple ai related abstracts and then writes a new paragraph to represent enriched knowledge combining the potential knowledge covering the topics mentioned in the source abstracts
inspired by the review process we propose a reviewer to rate the quality of the generated texts from ent dimensions which serve as feedback signals to rene our draw network
ablation experiments demonstrate the tiveness of our method
moreover writer reviewer achieves state of the art results on kgs to text generation task
in terms of human study some generations of our draw work successfully pass the turing test and confuse the turkers
in future study we will extend the draw network to write a complete paper in an iterative manner and develop more niques to discover novel ideas such as creating new entities
acknowledgments this work was partially supported by key area search and development program of guangdong province national natural science foundation of china nsfc key project program for dong introducing innovative and entrepreneurial teams international cooperation open project of state key laboratory of subtropical building science south china university of technology fundamental research funds for the central universities
references an b

repulsive bayesian sampling for diversied attention modeling
in workshop of neurips
bordes a
usunier n
garcia duran a
weston j
and yakhnenko o

translating embeddings for modeling multi relational data
in neurips
buenz e
j

essential elements for high impact tic writing
nature doi

cao j
guo y
wu q
shen c
huang j
and tan m

adversarial learning with local coordinate coding
in icml
cao j
guo y
wu q
shen c
huang j
and tan m

improving generative adversarial networks with local coordinate coding
ieee transactions on pattern analysis and machine intelligence
cao j
mo l
zhang y
jia k
shen c
and tan m

multi marginal wasserstein gan
in neurips
chen p
zhang y
tan m
xiao h
huang d
and gan c

generating visually aligned sound from videos
ieee trans
image process

cho k
merrienboer b
v
aglar glehre bahdanau d
bougares f
schwenk h
and bengio y

learning phrase representations using rnn encoder decoder for statistical machine translation
in emnlp
denkowski m
j
and lavie a

meteor universal language specic translation evaluation for any target language
in acl
dettmers t
minervini p
stenetorp p
and riedel s

convolutional knowledge graph embeddings
in aaai
feng n
jinpeng w
jin ge y
rong p
and chin yew l

operation guided neural networks for high fidelity data to text generation
in emnlp
gerber m
and chai j

beyond nombank a study of implicit arguments for nominal predicates
in acl
gopen g
d
and ja s

the science of scientic writing
american scientist
hochreiter s
and schmidhuber j

long short term memory
neural computation
huang d
chen p
zeng r
du q
tan m
and gan c

location aware graph convolutional networks for video question answering
in aaai
kipf t
and welling m

semi supervised tion with graph convolutional networks
in iclr
koncel kedziorski r
bekal d
luan y
lapata m
and hajishirzi h

text generation from knowledge graphs with graph transformers
in naacl hlt
lin y
liu z
sun m
liu y
and zhu x

ing entity and relation embeddings for knowledge graph completion
in aaai
ling f
u
and hui z

multi document summary using lda and spectral clustering
computer engineering applications
luan y
he l
ostendorf m
and hajishirzi h

multi task identication of entities relations and erence for scientic knowledge graph construction
in emnlp
min z
jie z
jian s
and guodong z

a ite kernel to extract relations between entities with both flat and structured features
in acl
nathani d
chauhan j
sharma c
and kaul m

learning attention based embeddings for relation tion in knowledge graphs
in acl
nguyen d
q
nguyen t
nguyen d
q
and phung d
q

a novel embedding model for knowledge base completion based on convolutional neural network
in naacl hlt
papineni k
roukos s
ward t
and zhu w


bleu a method for automatic evaluation of machine translation
in acl
ribeiro l
gardent c
and gurevych i

enhancing amr to text generation with dual graph representations
in emnlp ijcnlp
ribeiro l
f
r
zhang y
gardent c
and gurevych i

modeling global and local node contexts for text generation from knowledge graphs
transactions of the association for computational linguistics
schlichtkrull m
kipf t
bloem p
berg r
titov i
and welling m

modeling relational data with graph convolutional networks
in eswc
schmitt m
ribeiro l
f
r
dufter p
gurevych i
and schutze h

modeling graph structure via relative position for better text generation from knowledge graphs
arxiv

schulman j
wolski f
dhariwal p
radford a
and klimov o

proximal policy optimization algorithms
arxiv

trisedya b
jianzhong q
rui z
and wei w

lstm a triple encoder for sentence generation from rdf data
in acl
vaswani a
shazeer n
parmar n
uszkoreit j
jones l
gomez a
n
kaiser l
u
and polosukhin i

attention is all you need
in neurips
vedantam r
zitnick c
l
and parikh d

cider consensus based image description evaluation
in cvpr
wang h
wang j
wang j
zhao m
zhang w
zhang f
xie x
and guo m

graphgan graph tion learning with generative adversarial nets
in aaai
wang q
huang l
jiang z
knight k
ji h
bansal m
and luan y

paperrobot incremental draft eration of scientic ideas
in acl
williams r
j

simple statistical gradient following algorithms for connectionist reinforcement learning
machine learning
xiao l
wang l
he h
and jin y

copy or rewrite hybrid summarization with hierarchical ment learning
in aaai
xu k
wu l
guo wang z
yu m
chen l
and sheinin v

sql to text generation with graph to sequence model
in emnlp
xu t
zhang p
huang q
zhang h
gan z
huang x
and he x

fine grained text to age generation with attentional generative adversarial works
in cvpr
yoshikawa k
riedel s
hirao t
asahara m
and sumoto y

coreference based event argument relation extraction on biomedical text
journal of biomedical tics
yu l
zhang w
wang j
and yu y

seqgan sequence generative adversarial nets with policy gradient
in aaai
zhang x
and lecun y

text understanding from scratch
arxiv

zhen w
jianwen z
jianlin f
and zheng c

knowledge graph embedding by translating on planes
in aaai
zhijiang g
yan z
zhiyang t
and wei l

densely connected graph convolutional networks for graph sequence learning
transactions of the association for computational linguistics
references an b

repulsive bayesian sampling for diversied attention modeling
in workshop of neurips
bordes a
usunier n
garcia duran a
weston j
and yakhnenko o

translating embeddings for modeling multi relational data
in neurips
buenz e
j

essential elements for high impact tic writing
nature doi

cao j
guo y
wu q
shen c
huang j
and tan m

adversarial learning with local coordinate coding
in icml
cao j
guo y
wu q
shen c
huang j
and tan m

improving generative adversarial networks with local coordinate coding
ieee transactions on pattern analysis and machine intelligence
cao j
mo l
zhang y
jia k
shen c
and tan m

multi marginal wasserstein gan
in neurips
chen p
zhang y
tan m
xiao h
huang d
and gan c

generating visually aligned sound from videos
ieee trans
image process

cho k
merrienboer b
v
aglar glehre bahdanau d
bougares f
schwenk h
and bengio y

learning phrase representations using rnn encoder decoder for statistical machine translation
in emnlp
denkowski m
j
and lavie a

meteor universal language specic translation evaluation for any target language
in acl
dettmers t
minervini p
stenetorp p
and riedel s

convolutional knowledge graph embeddings
in aaai
feng n
jinpeng w
jin ge y
rong p
and chin yew l

operation guided neural networks for high fidelity data to text generation
in emnlp
gerber m
and chai j

beyond nombank a study of implicit arguments for nominal predicates
in acl
gopen g
d
and ja s

the science of scientic writing
american scientist
hochreiter s
and schmidhuber j

long short term memory
neural computation
huang d
chen p
zeng r
du q
tan m
and gan c

location aware graph convolutional networks for video question answering
in aaai
kipf t
and welling m

semi supervised tion with graph convolutional networks
in iclr
koncel kedziorski r
bekal d
luan y
lapata m
and hajishirzi h

text generation from knowledge graphs with graph transformers
in naacl hlt
lin y
liu z
sun m
liu y
and zhu x

ing entity and relation embeddings for knowledge graph completion
in aaai
ling f
u
and hui z

multi document summary using lda and spectral clustering
computer engineering applications
luan y
he l
ostendorf m
and hajishirzi h

multi task identication of entities relations and in erence for scientic knowledge graph construction
emnlp
min z
jie z
jian s
and guodong z

a ite kernel to extract relations between entities with both flat and structured features
in acl
nathani d
chauhan j
sharma c
and kaul m

learning attention based embeddings for relation tion in knowledge graphs
in acl
nguyen d
q
nguyen t
nguyen d
q
and phung d
q

a novel embedding model for knowledge base completion based on convolutional neural network
in naacl hlt
papineni k
roukos s
ward t
and zhu w


bleu a method for automatic evaluation of machine translation
in acl
zhen w
jianwen z
jianlin f
and zheng c

knowledge graph embedding by translating on planes
in aaai
zhijiang g
yan z
zhiyang t
and wei l

densely connected graph convolutional networks for graph sequence learning
transactions of the association for computational linguistics
ribeiro l
gardent c
and gurevych i

enhancing amr to text generation with dual graph representations
in emnlp ijcnlp
ribeiro l
f
r
zhang y
gardent c
and gurevych i

modeling global and local node contexts for text generation from knowledge graphs
transactions of the association for computational linguistics
schlichtkrull m
kipf t
bloem p
berg r
titov i
and welling m

modeling relational data with graph convolutional networks
in eswc
schmitt m
ribeiro l
f
r
dufter p
gurevych i
and schutze h

modeling graph structure via relative position for better text generation from knowledge graphs
arxiv

schulman j
wolski f
dhariwal p
radford a
and klimov o

proximal policy optimization algorithms
arxiv

trisedya b
jianzhong q
rui z
and wei w

lstm a triple encoder for sentence generation from rdf data
in acl
vaswani a
shazeer n
parmar n
uszkoreit j
jones l
gomez a
n
kaiser l
u
and polosukhin i

attention is all you need
in neurips
vedantam r
zitnick c
l
and parikh d

cider consensus based image description evaluation
in cvpr
wang h
wang j
wang j
zhao m
zhang w
zhang f
xie x
and guo m

graphgan graph tion learning with generative adversarial nets
in aaai
wang q
huang l
jiang z
knight k
ji h
bansal m
and luan y

paperrobot incremental draft eration of scientic ideas
in acl
williams r
j

simple statistical gradient following algorithms for connectionist reinforcement learning
machine learning
xiao l
wang l
he h
and jin y

copy or rewrite hybrid summarization with hierarchical ment learning
in aaai
xu k
wu l
guo wang z
yu m
chen l
and sheinin v

sql to text generation with graph to sequence model
in emnlp
xu t
zhang p
huang q
zhang h
gan z
huang x
and he x

fine grained text to age generation with attentional generative adversarial works
in cvpr
yoshikawa k
riedel s
hirao t
asahara m
and sumoto y

coreference based event argument relation extraction on biomedical text
journal of biomedical tics
yu l
zhang w
wang j
and yu y

seqgan sequence generative adversarial nets with policy gradient
in aaai
zhang x
and lecun y

text understanding from scratch
arxiv


