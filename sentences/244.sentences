unsupervised opinion summarization as copycat review generation arthur mirella and ivan ilcc university of edinburgh illc university of amsterdam
ac
mlap ititov
ed
ac
r a l c
s c v
v i x r a abstract opinion summarization is the task of cally creating summaries that reect subjective information expressed in multiple documents such as product reviews
while the majority of previous work has focused on the tive setting i
e
selecting fragments from put reviews to produce a summary we let the model generate novel sentences and hence duce abstractive summaries
recent progress in summarization has seen the development of supervised models which rely on large ties of document summary pairs
since such training data is expensive to acquire we stead consider the unsupervised setting in other words we do not use any summaries in training
we dene a generative model for a review collection which capitalizes on the ition that when generating a new review given a set of other reviews of a product we should be able to control the amount of novelty ing into the new review or equivalently vary the extent to which it deviates from the input
at test time when generating summaries we force the novelty to be minimal and produce a text reecting consensus opinions
we capture this intuition by dening a hierarchical tional autoencoder model
both individual views and the products they correspond to are associated with stochastic latent codes and the review generator decoder has direct access to the text of input reviews through the generator mechanism
experiments on zon and yelp datasets show that setting at test time the review s latent code to its mean lows the model to produce uent and coherent summaries reecting common opinions
introduction summarization of user opinions expressed in line resources such as blogs reviews social media or internet forums has drawn much attention due to its potential for various information access cations such as creating digests search and report summary reviews this restaurant is a hidden gem in toronto
the food is delicious and the service is peccable
highly recommend for anyone who likes french bistro
we got the steak frites and the chicken frites both of which were very good


great service


i really love this place


cote de boeuf


a jewel in the big city


french jewel of spadina and adelaide jules


they are super accommodating


moules and frites are delicious


food came with tons of greens and fries along with my main course thumbs uppp


chef has a very cool and fun attitude


great little french bistro spot


go if you want french bistro food classics


great place


the steak frites and it was ing


best steak frites


in downtown toronto


favourite french spot in the city


brule for dessert table a summary produced by our model colors encode its alignment to the input reviews
the reviews are truncated and delimited with the symbol
generation hu and liu angelidis and ata medhat et al

although there has been signicant progress recently in summarizing non subjective context rush et al
nallapati et al
paulus et al
see et al
liu et al
modern deep learning methods rely on large amounts of annotated data that are not readily available in the opinion summarization domain and expensive to produce
moreover notation efforts would have to be undertaken for multiple domains as online reviews are inherently multi domain blitzer al
and rization systems highly domain sensitive isonuma et al

thus perhaps unsurprisingly there is a long history of applying unsupervised and weakly supervised methods to opinion tion e

mei et al
titov and mcdonald angelidis and lapata however these approaches have primarily focused on extractive summarization i
e
producing summaries by ing parts of the input reviews
in this work we instead consider abstractive summarization which involves generating new phrases possibly rephrasing or using words that were not in the original text
abstractive summaries are often preferable to extractive ones as they can synthesize content across documents avoiding dundancy barzilay et al
carenini and ung di fabbrizio et al

in addition we focus on the unsupervised setting and do not use any summaries for training
unlike aspect based summarization liu which rewards the versity of opinions we aim to generate summaries that represent consensus i
e
dominant opinons in reviews
we argue that such summaries can be useful for quick decision making and to get an overall feel for a product or business see the example in table
more specically we assume we are provided with a large collection of reviews for various ucts and businesses and dene a generative model of this collection
intuitively we want to design such a model that when generating a review for a relying on a set of other reviews we can control the amount of novelty going into the new review or equivalently vary the extent to which it deviates from the input
at test time we can force the novelty to be minimal and generate summaries representing consensus opinions
we capture this intuition by dening a chical variational autoencoder vae model
both products and individual reviews are associated with latent representations
product representations can store for example overall sentiment common ics and opinions expressed about the product
in contrast latent representations of reviews depend on the product representations and capture the tent of individual reviews
while at training time the latent representations are random variables we x them to their respective means at test time
as desired for summarization these average or copycat reviews differ in writing style from a ical review
for example they do not contain evant details that are common in customer reviews such as mentioning the occasion or saying how many family members accompanied the reviewer
in order to encourage the summaries to include cic details the review generator decoder has direct access to the text of input reviews through the pointer generator mechanism see et al

in the example in table the model included cic information about the restaurant type and its location in the generated summary
as we will see in ablation experiments without this ing model performance drops substantially as the summaries become more generic
we evaluate our approach on two datasets zon product reviews and yelp reviews of businesses
the only previous method dealing with vised multi document opinion summarization as far as we are aware of is meansum chu and liu
similarly to our work they generate sus summaries and consider the yelp benchmark
whereas we rely on continuous latent tions they treat the summary itself as a discrete tent representation of a product
although this tures the intuition that a summary should relay key information about a product using discrete latent sequences makes optimization challenging miao and blunsom baziotis et al
chu and liu all have to use an extra training loss term and biased gradient estimators
our contributions can be summarized as follows we introduce a simple end to end approach to unsupervised abstractive summarization we demonstrate that the approach tially outperforms the previous method both when measured with automatic metrics and in human evaluation we provide a dataset of abstractive summaries for amazon products
model and estimation as discussed above we approach the tion task from a generative modeling perspective
we start with a high level description of our model then in sections
and
we describe how we estimate the model and provide extra technical tails
in section we explain how we use the model to generate summaries

overview of the generative model our text collection consists of groups of reviews with each group corresponding to a single product
simplicity we refer to both products e

iphone x and businesses e

a specic starbucks branch as products
and code
com copycat abstractive opinion summarizer
conditional independence of the reviews given the group representation c
the ri s decoder accesses other reviews of the group





rn
figure unfolded graphical representation of the model
our latent summarization model which we call copycat captures this hierarchical organization and can be regarded as an extension of the vanilla text vae model bowman et al

copycat uses two sets of latent variables as shown in ure
namely we associate each review group equivalently each product with a continuous able c which captures the group s latent tics
in addition we associate each individual review ri with a continuous variable zi ing the semantics of that review
the information zi to stored in zi is used by the decoder duce review text ri
the marginal log likelihood of one group of reviews n


rn is given by log n n log dc where we marginalize over variables c and
when generating a new review ri given the set of previous reviews i the information about these reviews has to be conveyed through the latent representations c and zi
this bottleneck is sirable as it will make it hard for the model to pass ne grain information
for example at generation time the model should be reusing named entities e

product names or technical characteristics from other reviews rather than hallucinating or avoiding generating them at all resulting in generic and non informative text
we alleviate this issue by letting the decoder directly access other reviews
we can formulate this as an autoregressive model n


c
n as we discuss in section
the conditioning is instantiated using the pointer generator anism see et al
and thus will specically help in generating rare words e

named entities
we want our summarizer to equally rely on every review without imposing any order e

temporal on the generation process
instead as shown in figure when generating ri we let the decoder access all other reviews within a group ri





rn
this is closely lated to pseudolikelihood estimation besag or skip thought s objective kiros et al

the nal objective that we maximize for each group of reviews n log n zi r dc we will conrm in ablation experiments that both hierarchical modeling i
e
using c and the direct conditioning on other reviews are benecial

model estimation as standard with vaes and variational inference in general kingma and welling instead of directly maximizing the intractable marginal hood in equation we maximize its lower l n n e e c log zi r i n dkl ri c dkl n
the derivations in appendix a

latexit latexit latexit latexit latexit ouilqmipwun latexit ouilqmipwun great italian restaurant with authentic food and great we ordered pasta and it was very tasty
would recommend this place to anyone
we visited this place last week
the waiters were friendly and the food was latexit zi latexit latexit latexit ri latexit ri latexit rn latexit rn latexit latexit latexit latexit latexit latexit ouilqmipwun latexit ouilqmipwun great italian restaurant with authentic food and great we ordered pasta and it was very tasty
would recommend this place to anyone
we visited this place last week
the waiters were friendly and the food was latexit zi latexit latexit latexit ri latexit ri latexit rn latexit rn latexit the full architecture used to produce the latent codes c and zi is shown in figure
we make gaussian assumptions for all distributions i
e
teriors and priors
as in kingma and welling we use separate linear projections lps to compute the means and diagonal log covariances


prior and posterior n we set the prior over group latent codes to the standard normal distribution c i
in order to compute the approximate posterior n we rst predict the contribution portance of each word in each review t i to the code of the group n t mt i mk where ti is the length of ri and f is a feed forward neural network which takes as input catenated word embeddings and hidden states of the gru encoder mt i ht i and returns a scalar
i wt next we compute the intermediate tion with the weighted sum h imt i
finally we compute the gaussian s parameters ing the afne projections t n lh bl log n gh bg n ri c and posterior

prior to compute the prior on the review code zi zi we linearly project the product code
similarly to pute the parameters of the approximate posterior c we catenate the last encoder s state i of the review ri and c and perform afne transformations
ri c n zi r i

decoder zi r i we use to compute the distribution an auto regressive gru decoder with the attention mechanism bahdanau et al
and a generator network
we compute the context vector ct i h by attending to all the encoder s hidden states h i of the other reviews r i of the group where the decoder s hidden state st i is used as a query
the use ffnns with the tanh non linearity in several model components
whenever a ffnn is mentioned in the subsequent discussion this architecture is assumed
figure production of latent code for review rn
the lower bound includes two inference ri c which are n and works ral networks parameterized with and will be discussed in detail in section

they imate the corresponding posterior distributions of the model
the rst term is the reconstruction error it encourages the quality reconstruction of the views
the other two terms are regularizers
they control the amount of information encoded in the latent representation by penalizing the deviation of the estimated posteriors from the ing priors the deviation is measured in terms of the kullback leibler kl divergence
the bound is maximized with respect to both the generative model s parameters and inference networks rameters
due to gaussian assumptions the kullback leibler kl divergence terms are able in closed form while we rely on the eterization trick kingma and welling to compute gradients of the reconstruction term
the inference network predicting the posterior for a review specic variable ri c is needed only in training and is discarded afterwards
in contrast we will exploit the inference network n when generating summaries as cussed in section

design of model components

text representations a gru encoder cho et al
embeds review words w to obtain hidden states h
those sentations are reused across the system e

in the inference networks and the decoder
concat review embeddings gru hidden state of the decoder is computed using the gru cell as st i i i
the cell inputs the previous hidden state well as concatenated word embedding wt vector and latent code zi
i as i i context finally we compute the word distributions using the pointer generator network g zi r i ct st i wt i r i t the pointer generator network computes two internal word distributions that are hierarchically aggregated into one distribution morin and bengio
one distribution assigns probabilities to words being generated using a xed vocabulary and another one probabilities to be copied directly from the other reviews r i
in our case the network helps to preserve details and especially to generate rare tokens
summary generation given reviews n we generate a summary that reects common information using trained nents of the model
formally we could sample a new review from n e e n
as we argued in the introduction and will visit in experiments a summary or summarizing review should be generated relying on the mean of the reviews latent code
consequently instead of sampling z from c we set it to
we also found benecial in terms of evaluation metrics not to sample c but stead to rely on the mean predicted by the inference network experimental setup n
n
datasets our experiments were conducted on business tomer reviews from the yelp dataset challenge and amazon product reviews he and mcauley
these were pre processed similarly to chu and liu and the corresponding data statistics are dataset yelp amazon training validation table data statistics after pre processing
the format in the cells is businesses reviews and ucts reviews for yelp and amazon respectively
shown in table
details of the pre processing are available in appendix a

these datasets present different challenges to abstractive summarization systems
yelp reviews contain much personal information and irrelevant details which one may nd unnecessary in a mary
our summarizer therefore needs to distill important information in reviews while abstracting away from details such as a listing of all items on the menu or mentions of specic dates or sions upon which customers visited a restaurant
on the contrary in amazon reviews we observed that users tend to provide more objective tion and specic details that are useful for decision making e

the version of an electronic product its battery life its dimensions
in this case it would be desirable for our summarizer to preserve this information in the output summary
for evaluation we used the same created yelp summaries released by chu and liu
these were generated by amazon ical turk amt workers who summarized input reviews
we created a new test for amazon reviews following a similar procedure see appendix a
for details
we sampled products and reviews for each product and they were shown to amt workers who were asked to write a summary
we collected three summaries per product products were used for development and for testing

experimental details we used grus cho et al
for tial encoding and decoding we used grus
we randomly initialized word embeddings that were shared across the model as a form of regularization press and wolf
further optimization was performed using adam kingma and ba
in order to overcome the posterior collapse man et al
both for our model and the vanilla vae baseline we applied cyclical annealing fu et al

the reported rouge scores are based on see appendix a
for details on perparameters

copycat meansum

lexrank
opinosis
vae
clustroid
lead
random
oracle








rl









copycat meansum

lexrank
opinosis
vae
clustroid
lead
random
oracle








rl








table rouge scores on the yelp test set
table rouge scores on the amazon test set

baseline models opinosis is a graph based abstractive summarizer ganesan et al
designed to generate short opinions based on highly redundant texts
though it is referred to as abstractive it can only select words from the reviews
lexrank is an unsupervised algorithm which lects sentences to appear in the summary based on graph centrality sentences represent nodes in a graph whose edges have weights denoting ity computed with tf idf
a node s centrality can be measured by running a ranking algorithm such as pagerank page et al

is the unsupervised abstractive marization model chu and liu discussed in the introduction
ri
we also trained a vanilla text vae model man et al
with our gru encoder and coder
when generating a summary for


rn we averaged the means of finally we used a number of simple tion baselines
we computed the clustroid review for each group as follows
we took each review from a group and computed rouge l with spect to all other reviews
the review with the highest rouge score was selected as the clustroid review
furthermore we sampled a random review from each group as the summary and constructed the summary by selecting the leading sentences from each review of a group
additionally as an upper bound we report the performance of an oracle review i
e
the highest scoring review in a group when computing rouge l against reference summaries
experiments on yelp we used the checkpoint vided by the authors as we obtained very similar rouge scores when retraining the model
evaluation results
automatic evaluation as can be seen in tables and our model cat yields the highest scores on both yelp and amazon datasets
we observe large gains over the vanila vae
we conjecture that the vanilla vae struggles to properly represent the variety of categories under a single prior
for example reviews about a sweater can result in a summary about socks see example summmaries in appendix
this contrasts with our model which allows each group to have c and access to other reviews its own prior during decoding
the gains are especially large on the amazon dataset which is very broad in terms of product categories
our model also substantially outperforms sum
as we will conrm in human evaluation meansum s summaries are relatively uent at the sentence level but often contain hallucinations i
e
information not present in the input reviews

human evaluation best worst scaling we performed human uation using the amt platform
we sampled businesses from the human annotated yelp test set and used all test products from the zon set
we recruited workers to evaluate each tuple containing summaries from meansum our model lexrank and human annotators
the views and summaries were presented to the ers in random order and were judged using worst scaling louviere and woodworth louviere et al

bws has been shown to produce more reliable results than ranking scales kiritchenko and mohammad
ers were asked to judge summaries according to the fluency coherence non red
opinion cons
overall

copycat
meansum


lexrank

gold











table human evaluation results in terms of the best worst scaling on the yelp dataset
fluency coherence non red
opinion cons
overall

copycat
meansum


lexrank

gold











table human evaluation results in terms of the best worst scaling on the amazon dataset
criteria listed below we show an abridged version below the full set of instructions is given in pendix a

the non redundancy and coherence criteria were taken from dang
fluency the summary sentences should be matically correct easy to read and understand herence the summary should be well structured and well organized non redundancy there should be no unnecessary repetition in the summary ion consensus the summary should reect mon opinions expressed in the reviews overall based on your own criteria judgment please select the best and the worst summary of the reviews
for every criterion a system s score is computed as the percentage of times it was selected as best minus the percentage of times it was selected as worst orme
the scores range from unanimously worst to unanimously best
on yelp as shown in table our model scores higher than the other models according to most criteria including overall quality
the differences with other systems are statistically signicant for all the criteria at p
using post hoc hd tukey tests
the difference in uency between our system and gold summaries is not statistically signicant
the results on amazon are shown in table
our system outperforms other methods in terms of uency coherence and non redundancy
as with yelp it trails lexrank according to the ion consensus criterion
additionally lexrank is slightly preferable overall
all pairwise differences between our model and comparison systems are statistically signicant at p

tures the coverage of common opinions and it seems to play a different role in the two datasets
on yelp lexrank has better coverage compared to our model as indicated by the higher oc score but is not preferred overall
in contrast on amazon while the oc score is on the same par lexrank is preferred overall
we suspect that presenting a breadth of exact details on amazon is more tant than on yelp
moreover lexrank tends to produce summaries that are about tokens longer than ours resulting in better coverage of input tails
content support the rouge metric relies on unweighted n gram overlap and can be tive to hallucinating facts and entities falke et al

for example referring to a burger joint as a veggie restaurant is highly problematic from a user perspective but yields only marginal differences in rouge
to investigate how well the content of the summaries is supported by the input reviews we performed a second study
we used the same sets as in the human evaluation in section
and split meansum and our system s summaries into sentences
then for each summary sentence we assigned amt workers to assess how well the sentence is supported by the reviews
workers were advised to read the reviews and rate sentences using one of the following three options
full support all the content is reected in the reviews partial support only some content is reected in the views no support content is not reected in the reviews
the results in table indicate that our model is opinion consensus oc is a criterion that better at preserving information than meansum
yelp amazon copycat meansum copycat meansum
full partial

no








table content support on yelp and amazon datasets percentages
r i c z sampling full









rl




table ablations rouge scores on amazon
analysis ablations to investigate the importance of the model s individual components we performed lations by removing the latent variables zi and c one at a time and attention over the other reviews
the models were re trained on the amazon dataset
the results are shown in table
they indicate that all components play a role yet the most signicant drop in rouge was achieved when the variable z was removed and only c remained
summaries obtained from the latter system were wordier and looked more similar to reviews
dropping the tention r results in more generic summaries as the model can not copy details from the input
nally the smallest quality drop in terms of l was observed when the variable c was removed
in the introduction we hypothesized that using the mean of latent variables would result in more grounded summaries reecting the content of the input reviews whereas sampling would yield texts with many novel and potentially irrelevant details
to empirically test this hypothesis we sampled the latent variables during summary generation as opposed to using mean values see section
we indeed observed that the summaries were wordier less uent and less aligned to the input reviews as is also reected in the rouge scores table
copy mechanism finally we analyzed which words are copied by the full model during summary generation
generally the model copies around tokens per summary
we observed a tendency to copy product type specic words e

shoes as well as brands and names
related work extractive weakly supervised opinion tion has been an active area of research
a recent example is angelidis and lapata
first they learn to assign sentiment polarity to review segments in a weakly supervised fashion
then they induce aspect labels for segments relying on a small sample of gold summaries
finally they use a heuristic to construct a summary of segments
opinosis ganesan et al
does not use any supervision
the model relies on redundancies in opinionated text and pos tags in order to generate short opinions
this approach is not well suited for the generation of coherent long summaries and though it can recombine fragments of input text it can not generate novel words and phrases
lexrank erkan and radev is an unsupervised tractive approach which builds a graph in order to determine the importance of sentences and then selects the most representative ones as a summary
isonuma et al
introduce an unsupervised approach for single review summarization where they rely on latent discourse trees
other earlier approaches gerani et al
di fabbrizio et al
relied on text planners and templates while our approach does not require rules and can duce uent and varied text
finally conceptually related methods were applied to unsupervised gle sentence compression west et al
otis et al
miao and blunsom
the most related approach to ours is meansum chu and liu which treats a summary as a crete latent state of an autoencoder
in contrast we dene a hierarchical model of a review collection and use continuous latent codes
conclusions in this work we presented an abstractive rizer of opinions which does not use any maries in training and is trained end to end on a large collection of reviews
the model compares favorably to the competitors especially to the only other unsupervised abstractive multi review marization system
furthermore human evaluation of the generated summaries by considering their alignment with the reviews shows that those ated by our model better reect the content of the input
acknowledgments we would like to thank the anonymous reviewers for their valuable comments
also stefanos gelidis for help with the data as well as jonathan mallinson serhii havrylov and other members of edinburgh nlp group for discussion
we fully acknowledge the support of the european research council titov erc stg broadsem lapata erc cog transmodal and the dutch national science foundation nwo vidi


references stefanos angelidis and mirella lapata

marizing opinions aspect extraction meets ment prediction and they are both weakly supervised
in proceedings of the conference on cal methods in natural language processing pages
dzmitry bahdanau kyunghyun cho and yoshua gio

neural machine translation by jointly in proceedings of learning to align and translate
international conference on learning tions iclr
regina barzilay kathleen r mckeown and michael elhadad

information fusion in the context of multi document summarization
in proceedings of the annual meeting of the association for putational linguistics pages
christos baziotis ion androutsopoulos ioannis stas and alexandros potamianos

ferentiable sequence to sequence to sequence toencoder for unsupervised abstractive sentence compression
in proceedings of the association for computational linguistics pages
julian besag

statistical analysis of non lattice data
journal of the royal statistical society series d the statistician
john blitzer mark dredze and fernando pereira

biographies bollywood boom boxes and blenders in domain adaptation for sentiment classication
proceedings of the annual meeting of the ciation of computational linguistics pages
samuel bowman luke vilnis oriol vinyals drew m dai rafal jozefowicz and samy gio

generating sentences from a ous space
in proceedings of the twentieth ence on computational natural language learning conll
giuseppe carenini and jackie chi kit cheung

extractive vs
nlg based abstractive summarization of evaluative text the effect of corpus ity
in proceedings of the fifth international ral language generation conference pages
association for computational linguistics
kyunghyun cho bart van merrienboer caglar cehre dzmitry bahdanau fethi bougares holger schwenk and yoshua bengio

learning phrase representations using rnn encoder decoder for statistical machine translation
in proceedings of the conference on empirical methods in ural language processing emnlp pages
eric chu and peter liu

meansum a ral model for unsupervised multi document tive summarization
in proceedings of international conference on machine learning icml pages
hoa trang dang

overview of duc
in ceedings of the document understanding conference volume pages
giuseppe di fabbrizio amanda stent and robert gaizauskas

a hybrid approach to document summarization of opinions in reviews
pages
gunes erkan and dragomir r radev

lexrank graph based lexical centrality as salience in text summarization
journal of articial intelligence search
tobias falke leonardo fr ribeiro prasetya ajie utama ido dagan and iryna gurevych

ranking generated summaries by correctness an teresting but challenging application for natural guage inference
in proceedings of the annual meeting of the association for computational guistics pages
hao fu chunyuan li xiaodong liu jianfeng gao asli celikyilmaz and lawrence carin

cal annealing schedule a simple approach to in proceedings of the igating kl vanishing
conference of the north american chapter of the sociation for computational linguistics pages
kavita ganesan chengxiang zhai and jiawei han

opinosis a graph based approach to tive summarization of highly redundant opinions
in proceedings of the international conference on computational linguistics coling pages
shima gerani yashar mehdad giuseppe carenini raymond t ng and bita nejat

abstractive summarization of product reviews using discourse structure
in proceedings of the conference on empirical methods in natural language processing emnlp pages
xavier glorot and yoshua bengio

ing the difculty of training deep feedforward neural networks
in proceedings of the thirteenth tional conference on articial intelligence and tics pages
ruining he and julian mcauley

ups and downs modeling the visual evolution of fashion trends with one class collaborative ltering
in proceedings of the international conference on world wide web pages
international world wide web conferences steering committee
ari holtzman jan buys maxwell forbes and yejin choi

the curious case of neural text ation
arxiv preprint

minqing hu and bing liu

mining and rizing customer reviews
in proceedings of the tenth acm sigkdd international conference on edge discovery and data mining pages
acm
masaru isonuma toru fujino junichiro mori yutaka matsuo and ichiro sakata

extractive marization using multi task learning with document in proceedings of the classication
ence on empirical methods in natural language processing pages
masaru isonuma junichiro mori and ichiro sakata

unsupervised neural single document marization of reviews via learning latent discourse structure and its ranking
in proceedings of acl
diederik p kingma and jimmy ba

adam a method for stochastic optimization
arxiv preprint

diederik p kingma and max welling

arxiv preprint encoding variational bayes


svetlana kiritchenko and saif m mohammad

capturing reliable ne grained sentiment tions by crowdsourcing and best worst scaling
in proceedings of the conference of the north american chapter of the association for tional linguistics human language technologies pages
ryan kiros yukun zhu ruslan r salakhutdinov richard zemel raquel urtasun antonio torralba in and sanja fidler

skip thought vectors
advances in neural information processing systems pages
philipp koehn hieu hoang alexandra birch chris callison burch marcello federico nicola bertoldi brooke cowan wade shen christine moran richard zens al

moses open source in toolkit for statistical machine translation
ceedings of the annual meeting of the ation for computational linguistics companion ume proceedings of the demo and poster sessions pages
bing liu

sentiment analysis and opinion ing
synthesis lectures on human language gies
peter j liu mohammad saleh etienne pot ben goodrich ryan sepassi lukasz kaiser and noam shazeer

generating wikipedia by ing long sequences
in proceedings of international conference on learning representations iclr
jordan j louviere terry n flynn and anthony fred john marley

best worst scaling ory methods and applications
cambridge sity press
jordan j louviere and george g woodworth

best worst scaling a model for the largest ence judgments
university of alberta working per
walaa medhat ahmed hassan and hoda korashy

sentiment analysis algorithms and tions a survey
ain shams engineering journal
qiaozhu mei xu ling matthew wondra hang su and chengxiang zhai

topic sentiment ture modeling facets and opinions in weblogs
in proceedings of the international conference on world wide web pages
acm
yishu miao and phil blunsom

language as a latent variable discrete generative models for tence compression
in proceedings of the ference on empirical methods in natural language processing pages
frederic morin and yoshua bengio

hierarchical probabilistic neural network language model
tats
ramesh nallapati bowen zhou cicero dos santos caglar gulcehre and bing xiang

tive text summarization using sequence to sequence in proceedings of the rnns and beyond
signll conference on computational natural guage learning pages
bryan orme

maxdiff analysis simple counting individual level logit and hb
sequim wa tooth software
lawrence page sergey brin rajeev motwani and terry winograd

the pagerank citation ing bringing order to the web
technical report stanford infolab
romain paulus caiming xiong and richard socher

a deep reinforced model for abstractive marization
arxiv preprint

or press and lior wolf

using the output in bedding to improve language models
ings of the conference of the european ter of the association for computational linguistics pages
alexander m rush sumit chopra and jason weston

a neural attention model for abstractive in proceedings of the tence summarization
conference on empirical methods in natural guage processing pages
abigail see peter j liu and christopher d manning

get to the point summarization with generator networks
in proceedings of association for computational linguistics acl
ivan titov and ryan mcdonald

modeling line reviews with multi grain topic models
in ceedings of the international conference on world wide web pages
acm
peter west ari holtzman jan buys and yejin choi

bottlesum unsupervised and self supervised sentence summarization using the information tleneck principle
arxiv preprint

a appendices a
derivation of the lower bound to make the notation below less cluttered we make a couple of simplications n and ri c
n log log dc n r i n log dz dc log e n n e log e c r i i c r i i e log r i e log r i e dkl n e dkl n e n dkl i dkl a
dataset pre processing we selected only businesses and products with a minimum of reviews and thee minimum and maximum length of and words respectively popular groups above the percentile were moved
and each group was set to contain views during training
from the amazon dataset we selected categories electronics clothing shoes and jewelry home and kitchen health and personal care
a
hyperparameters for sequential encoding and decoding we used grus cho et al
with dimensional den states
the word embeddings dimension was set to and they were shared across the model press and wolf
the vocabulary size was set to most frequent words and an extra were allowed in the extended vocabulary the words were lower cased
we used the moses koehn et al
reversible tokenizer and caser
xavier uniform initialization glorot and bengio of weights was used and weights were initialized with the scaled normal noise

we used the adam optimizer kingma and ba and set the learning rate to
and
on yelp and amazon tively
for summary decoding we used normalized beam search of size and relied on latent code means
in order to overcome rior collapse bowman et al
we applied cycling annealing fu et al
with r
for both the z and c related kl terms with a new cycle over approximately every epochs over the training set
the maximum annealing scalar was set to for related kl term in on both datasets and
and
for c related kl term on yelp and amazon respectively
the reported rouge scores are based on
the dimensions of the variables c and z were set to and the c posterior scoring neural network had a dimensional hidden layer and the tanh non linearity
the decoder s attention mechanism used a gle layer neural network with a dimensional hidden layer and the tanh non linearity
the copy gate in the pointer generator network was puted with a dimensional single hidden layer network with the same non linearity
a
human evaluation setup to perform the human evaluation experiments scribed in sections
and
we combined both tasks into single human intelligence tasks hits
namely the workers needed to mark sentences as described in section
and then proceed to the task in section

we explicitly asked then to re read the reviews before each task
for worker requirements we set approval rate hits location usa uk canada and the maximum score on a qualication test that we designed
the test was asking if the workers are native english speakers and verifying that they correctly understand the instructions of both tasks by completing a mini version of the actual hit
a
full human evaluation instructions fluency the summary sentences should be grammatically correct easy to read and stand
coherence the summary should be well structured and well organized
the summary should not just be a heap of related tion but should build from sentence to tence to a coherent body of information about a topic
non redundancy there should be no essary repetition in the summary
sary repetition might take the form of whole sentences that are repeated or repeated facts or the repeated use of a noun or noun phrase e

bill clinton when a pronoun he would sufce
opinion consensus the summary should ect common opinions expressed in the views
for example if many reviewers plain about a musty smell in the hotel s rooms the summary should include this information
overall based on your own criteria ment please select the best and the worst summary of the reviews
a
amazon summaries creation first we sampled products from each of the amazon review categories electronics clothing shoes and jewelry home and kitchen health and personal care
then we selected reviews from each product to be summaries
we used the same requirements for workers as for human evaluation in a

we assigned workers to each product and instructed them to read the reviews and duce a summary text
we followed the instructions provided in chu and liu and used the lowing points in our instructions the summary should reect common ions about the product expressed in the views
try to preserve the common sentiment of the opinions and their details e

what exactly the users like or dislike
for ple if most reviews are negative about the sound quality then also write negatively about it
please make the summary coherent and uent in terms of sentence and information structure
iterate over the written summary the decoder essentially becomes a uncoditional guage model for which beam search was shown to lead to generation of repetitions holtzman et al

multiple times to improve it and re read the reviews whenever necessary
please write your summary as if it were a review itself e

this place is expensive instead of users thought this place was pensive
keep the length of the summary reasonably close to the average length of the reviews
please try to write the summary using your own words instead of copying text directly from the reviews
using the exact words from the reviews is allowed but do not copy more than consecutive words from a review
a
latent codes analysis we performed a qualitative analysis of the latent variable z to shed additional light on what it stores and sensitivity of the decoder with respect to its input
specically we computed the mean value for the variable c using the approximate posterior


rn and then sampled z from the prior c
first we observed that the summaries produced using the mean of z are more uent
for example in table the based summary states the ture quality is very good but it does nt work aswell as the picture
where the second phrase could be rewritten in a more uent matter
also we found that mean based summaries contain less details that are partially or not supported by the reviews
for example in the table based summary mentions kindle fire hd
while the dimension is never mentioned in the reviews
finally different ples were observed to result in texts that contain different details about the reviews
for example sample results in the summary that captures the picture quality while that the item is good for its price
overall we observed that the latent variable z stores content based information that results in syntactically diverse texts yet reecting tion about the same businesses or product
a
repetitions we observed an increase in the amount of ated repetitions both in the reconstructed reviews and summaries when the related kl term is low and beam search is used
intuitively the initial put to the decoder becomes less informative and it starts relying on learned local statistics to perform reconstruction
when the kld vanishes to zero mean z rev rev rev rev rev rev rev rev bought this for my kindle fire hd and it works great
i have had no problems with it
i would recommend it to anyone looking for a good quality cable
works ne with my kindle fire hd

the picture quality is very good but it does nt work as well as the picture
i m not sure how long it will last but i am very disappointed
this is a great product
i bought it to use with my kindle fire hd and it works great
i would recommend it to anyone who is looking for a good quality cable for the price
good product does what it is supposed to do
i would recommend it to anyone looking for a hdmi cable
love this hdmi cable but it only works with hd kindle and not the hdx kindle which makes me kinda crazy
i have both kinds of kindles but the hdx is newer and i can t get a cable for the new one
i guess my hd kindle will be my amazon prime kindle
it works great i got a kindle for christmas
i had no idea how to work one
i discovered you can stream movies to your tv and this is the exact cable for it
works great and seems like its good quality
a bit long though
this is great for watching movies from kindle to tv
now the whole family can enjoy rather than one person at a time
picture quality isn t amazing but it s good
i just received this wire in the mail and it does not work in the slightest
i am very displeased with this product
works great now i can watch netix on my tv with my kindle fire hd


i love it and so will you works awesome
great item for the price
got it very quickly
was as described in the ad
exactly what i was looking for
i plugged it into my kindle re hd and into the tv and works perfectly
have had no problems with it this is just what i was looking for to connect my kindle fire to view on our tv great price too table amazon summaries of the full model with sampled and mean assignment to z
the assignment to c was xed and was the mean value based on the approximate posterior


rn
ours meansum lexrank gold rev rev rev rev rev rev rev rev this place is the best mexican restaurant i have ever been to
the food was delicious and the staff was very friendly and helpful
our server was very attentive and made sure we were taken care of
we ll be back for sure
a little on the pricey side but i was pleasantly surprised
we went there for a late lunch and it was packed with a great atmosphere food was delicious and the staff was super friendly
very friendly staff
we had the enchiladas with a few extra veggies and they were delicious will be back for sure we will denitely be going back for more great food everything we had so far was great
the staff was great and so nice good food great atmosphere this place is simply amazing its the best mexican spot in town
their tacos are delicious and full of avor
they also have chips and salsa that is to die for the salsa is just delectable it has a sweet tangy avor that you ca nt nd anywhere else
i highly recommend classic style mexican food done nicely yummy crispy cheese crisp with a limey margarita will will win my heart any day of the week the classic frozen with a chambord oat is my favorite and they do it well here
the salad carbon was off the served on a big platter and worked for me as full dinners
for delicious mexican food in north phoenix try la pinata
this was our visit here and we were so stunned by the speed in which our food was prepared that we were sure it was meant for another table
the food was hot and fresh and well within our budget
my husband got a beef chimichanga and i got bean and cheese burrito which we both enjoyed
chips and salsa arrived immediately the salsa tastes sweeter than most and is equally avorful
we will be back good food great atmosphere great patio
staff was super friendly and accommodating we will denately return this place was very delicious i got the ranchero burro and it was so good
the plate could feed at least two people
the staff was great and so nice i also got the fried ice cream it was good
i would recommend this place to all my friends
we arrive for the rst time greeted immediately with a smile and seated promptly
our server was fantastic he was funny and fast
gave great suggestions on the menu and we both were very pleased with the food avors speed and accuracy of our orders
we will denitely be going back for more great food well was very disappointed to see out favorite ice cream parlor closed but delightfully surprised at how much we like this was fantastic top notch taco was great lots of cheese
freshly deep fried shell not like so many phoenix mex restaurants use enchilada was very good
my wife really enjoyed her chimichanga
my moms chilli reanno was great too
everything we had so far was great
we will return
highly recommended
i m only on the salsa and it s just as fabulous as always
i love the new location and the decor is beautiful
open days and the place is standing room only
to the previous negative commentor they are way took busy to ll an order for beans
go across the street



you ll be angry lol
i just tried to make a reservation for people in march at am on a tuesday and was informed by a very rude female
she said we do not take reservations and i asked if they would for people and she said i told you we do nt take reservations and hung up on me
is that the way you run a business very poor customer service and i have no intentions of ever coming there or recommending it to my friends
table yelp summaries produced by different models
ours meansum lexrank gold rev rev rev rev rev rev rev rev this place is the worst service i ve ever had
the food was mediocre at best
the service was slow and the waiter was very rude
i would not recommend this place to anyone who wants to have a good time at this location
i love the decor but the food was mediocre
service is slow and we had to ask for rells
they were not able to do anything and not even charge me for it
it was a very disappointing experience and the service was not good at all
i had to ask for a salad for a few minutes and the waitress said he did nt know what he was talking about
all i can say is that the staff was nice and attentive
i would have given stars if i could
food was just okay server was just okay
the atmosphere was great friendly server
it took a bit long to get a server to come over and then it took our server a while to get our bread and drinks
however there was complementary bread served
the pizza i ordered was undercooked and had very little sauce
macaroni grill has unfortunately taken a dive
went to dinner with others and had another bad experience at the macaroni grill
i m really not a fan of macaroni grill well at least this macaroni grill
the staff is slow and really does nt seem to car about providing quality service
it took well over minutes to get my food and the place was nt even packed with people
i ordered pizza and it did nt taste right
i think it was nt fully cooked
i wo nt be coming back
was the date of our visit
food was just okay server was just okay
the manager climbed up on the food prep counter to x a light
we felt like that was the most unsanitary thing anyone could do he could have just come from the restroom for all we knew
needless to say lackluster service mediocre food and lack of concern for the cleanliness of the food prep area will guarantee we will never return
we like the food and prices are reasonable
our biggest complaint is the service
it took a bit long to get a server to come over and then it took our server a while to get our bread and drinks
they really need to develop a better sense of teamwork
while waiting for things there were numerous servers standing around gabbing
it really gave us the impression of not my table
not my problem
only other complaint is they need to get some rinse aid for the dishwasher
i had to dry our bread plates when the hostess gave them to us
not enough staff is on hand the two times i have been in to properly pay attention to paying customers
i agree that the portions have shrunk over the years and the effort is no longer there
it is convenient to have nearby but not worth my time when other great restaurants are around
wish i could rate it better but it s just not that good at all
went to dinner with others and had another bad experience at the macaroni grill
when will we ever learn the server was not only inattentive but p od when we asked to be moved to another table
when the food came it was at best luke warm
they had run out of one of our ordered dishes but did nt inform us until minutes after we had ordered
running out at
m
really more delay and no apologies
there is no excuse for a cold meal and poor service
we will not go back since the grill seems not to care and there are plenty of other restaurants which do
the service is kind and friendly
however there was complementary bread served
the pizza i ordered was undercooked and had very little sauce
macaroni grill has unfortunately taken a dive
best to avoid the place or at the very least this location
i know this is a chain but between this and olive garden i would def pick this place
service was great at this location and food not bad at all although not excellent i think it still deserves a good stars i had a for
express dinner coupon so we order up dinners to go
the deal was min or its free it took but since i was getting meals for
i did not make a fuss
the actual pasta was ne and amount was fair but it had maybe a of a chicken breast
the chicken tasted like it came from taco bell very processed
the sauce straight from a can
i have had much better frozen dinners
my husband and i used to like macaroni grill it sad too see its food go so down hill
the atmosphere was great friendly server
although the food i think is served from frozen
i ordered mama trio
the two of three items were great
plate came out hot could nt touch it
went to eat lasagna and was ice cold in the center nit even warm
the server apologized about it offered new one or reheat this one
i chose a new one to go
i saw her go tell manager
the manager did nt even come over and say anything
i was not even acknowledged on my way out and walked past people
i will not be going back
over priced for frozen food
table yelp summaries produced by different models
ours meansum lexrank gold rev rev rev rev rev rev rev rev my wife and i have been here several times now and have never had a bad meal
the service is impeccable and the food is delicious
we had the steak and lobster which was delicious
i would highly recommend this place to anyone looking for a good meal
our rst time here the restaurant is very clean and has a great ambiance
i had the let mignon with a side of mashed potatoes
they were both tasty and lling
i ve had better at a chain restaurant but this is a great place to go for a nice dinner or a snack
have eaten at the restaurant several times and have never had a bad meal here
had the let


really enjoyed my let and slobster
in addition to excellent drinks they offer free prime let steak sandwiches
i have had their let mignon which is pretty good calamari which is ok scallops which are nt really my thing sour dough bread which was fantastic amazing stuffed mushrooms
very good steak house
the steak is the must have dish at this restaurant
one small problem with the steak is that you want to order it cooked less than you would at a normal restaurant
they have the habit of going a bit over on the steak
the drinks are excellent and the stuffed mushrooms as appetizers were amazing
this is a classy place that is also romantic
the staff pays good attention to you here
the ambiance is relaxing yet rened
the service is always good
the steak was good although not cooked to the correct temperature which is surprising for a steakhouse
i would recommend ordering for a lesser cook than what you normally order
i typically order medium but at donovan would get medium rare
the side dish menu was somewhat limited but we chose the creamed spinach and asparagus both were good
of course you have to try the creme brulee yum had nt been there in several years and after this visit i remember why i do nt like onions or shallots in my macaroni and cheese
the food is good but not worth the price just a very disappointing experience and i probably wo nt go back my wife and i come here every year for our anniversary literally every year we have been married
the service is exceptional and the food quality is top notch
furthermore the happy hour is one of the best in the valley
in addition to excellent drinks they offer free prime let steak sandwiches
i highly recommend this place for celebrations or a nice dinner out
i get to go here about once a month for educational dinners
i have never paid so do nt ask about pricing
i have had their let mignon which is pretty good calamari which is ok scallops which are nt really my thing sour dough bread which was fantastic amazing stuffed mushrooms
the vegetables are perfectly cooked and the mashed potatoes are great
at the end we get the chocolate mousse cake that really ends the night well
i have enjoyed every meal i have eaten there
very good steak house
steaks are high quality and the service was very professional
attentive but not hovering
classic menus and atmosphere for this kind of restaurant
no surprises
a solid option but not a clear favorite compared to other restaurants in this category
had a wonderful experience here last night for restaurant week
had the let


which was amazing and cooked perfectly with their yummy mashed potatoes and veggies
the bottle of red wine they offered for an additional paired perfectly with the dinner
the staff were extremely friendly and attentive
ca nt wait to go back the seafood tower must change in selection of seafood which is good which is also why mine last night was so fresh fresh delicious
its good to know that you can get top rate seafood in phoenix
bacon wrapped scallops were very good and i sacricied a full steak opting for the let medallion to try the scallops
i asked for medium rare steak but maybe should ve asked for rare


my cousin had the ribeye and could not have been any happier than he was yum for fancy steak houses
its an ultra romantic place to fyi
the wait staff is very attentive
donovans how can you go wrong
had some guests in town and some fantastic steaks paired with some great cabernets
really enjoyed my let and lobster
table yelp summaries produced by different models
ours meansum lexrank gold rev rev rev rev rev rev rev rev i love this tank
it ts well and is comfortable to wear
i wish it was a little bit longer but i m sure it will shrink after washing
i would recommend this to anyone
i normally wear a large so it was not what i expected
it s a bit large but i think it s a good thing
i m and the waist ts well
i m and this is a bit big
i m and this tank ts like a normal tank top not any longer
the only reason i m rating this at two stars is because it is listed as a long tank top and the photo even shows it going well past the models hips however i m short and the tank top is just a normal length
i bought this tank to wear under shirts when it is colder out
i was trying to nd a tank that would cover past my hips so i could wear it with leggings
great tank top to wear under my other shirts as i liking layering and the material has a good feel
there was a good choice of colors to pick from
although the top is a thin material i do nt mind since i wear it under something else
the description say it long


not so it is average
that s why i purchased it because it said it was long
this is a basic tank
i washed it and it did nt warp but did shrink a little
nothing to brag about
i m and this tank ts like a normal tank top not any longer
i was trying to nd a tank that would cover past my hips so i could wear it with leggings
do nt order if you re expecting tunic length
this shirt is ok if you are layering for sure
it is thin and runs small
i usually wear a small and read the reviews and ordered a medium
it ts tight and is not long like in the picture
glad i only purchased one
the tank t very well and was comfortbale to wear
the material was thinner than i expected and i felt it was probably a little over priced
i ve bought much higher quality tanks for at a local store
the only reason i m rating this at two stars is because it is listed as a long tank top and the photo even shows it going well past the models hips however i m short and the tank top is just a normal length
i usually get them someplace out but they no longer carry them
i thought i would give these a try
i received them fast although i did order a brown and got a black which i also needed a black anyway
they were a lot thinner than i like but they are okay
every women should own one in every color
they wash well perfect under everything
perfect alone
as i write i m waiting on another of the same style to arrive
just feels quality i do nt know how else to explain it but i m sure you get it ladies i bought this tank to wear under shirts when it is colder out
i bought one in white and one in an aqua blue color
they are long enough that the color peeks out from under my tops
looks cute
i do wish that the neck line was a bit higher cut to provide more modest coverage of my chest
table amazon summaries produced by different models
ours meansum lexrank gold rev rev rev rev rev rev rev rev this is the best acupressure mat i have ever used
i use it for my back pain and it helps to relieve my back pain
i have used it for several months now and it seems to work well
i would recommend it to anyone
i have used this for years and it works great
i have trouble with my knee pain but it does help me to get the best of my feet
i have had no problems with this product
i have had many compliments on it and is still in great shape
i ordered this acupressure mat to see if it would help relieve my back pain and at rst it seemed like it was nt doing much but once you use it for a second or third time you can feel the pain relief and it also helps you relax
its great to lay on to relax you after a long day at work
i really like the acupressure mat
i usually toss and turn a lot when i sleep now i use this before i go to bed and it helps relax my body so that i can sleep more sound without all the tossing and turning
these acupressure mats are used to increase circulation and reduce body aches and pains and are most effective when you can fully relax
consistence is key to receive the full relaxing benets of the product
however if you are using this product after surgery it is responsible to always consult with your physician to ensure it is right for your situation
always consult with your doctor before purchasing any circulation product after surgery
i had ankle surgery and this product is useful for blood circulation in the foot
this increase in circulation has assisted with my ability to feel comfortable stepping down on the foot only after doc said wait bearing was okay
i use it sitting down barefoot
i really like the acupressure mat
i usually toss and turn a lot when i sleep now i use this before i go to bed and it helps relax my body so that i can sleep more sound without all the tossing and turning
i used the mat the rst night after it arrived and every other night since
after ten minute sessions i am sold
i have slept much better at night i think it puts me in a more relaxed state making it easier to fall asleep
a rather inexpensive option to relieving tension in my neck upper back and shoulders
this is the best thing you can use socks if your feet are tender to walk on it or bare foot if you can take it
i use it every morning to walk across to jump start my body
when i think about it i will lay on it it feels wonderful
i love these spike mats and have recommended them to everyone that has had any kind of body ache
its great to lay on to relax you after a long day at work
helps with pain in my back and pain in my legs
its not a cure but it sure helps with the healing process
i wish i had nt purchased this item
i just ca nt get use to it it s not comfortable
i have not seen any benets from using it but that could be because i do nt relax or use it for long enough
i run an alternative health center and use acupressure pin mats from different sources to treat my patients but this product is the patients choice they are asking allways for this mat against other brands so i changed all of them for britta moreover the s h was outstanding and really fast
i ordered this acupressure mat to see if it would help relieve my back pain and at rst it seemed like it was nt doing much but once you use it for a second or third time you can feel the pain relief and it also helps you relax
i use it almost everyday now and it really helps
i recommed this product and this seller
table amazon summaries produced by different models

