learning syntactic and dynamic selective encoding for document summarization haiyang xu yahao he kun han junwen chen and xiangang li ai labs didi chuxing co
ltd
beijing china email xuhaiyangsnow heyahao kunhan chenjunwen
com r a m l c
s c v
v i x r a abstract text summarization aims to generate a headline or a short summary consisting of the major information of the source text
recent studies employ the sequence to sequence framework to encode the input with a neural network and generate abstractive summary
however most studies feed the encoder with the semantic word embedding but ignore the syntactic information of the text
further although previous studies proposed the selective gate to control the information ow from the encoder to the decoder it is static during the decoding and can not differentiate the information based on the decoder states
in this paper we propose a novel neural architecture for document summarization
our approach has the following contributions rst we incorporate syntactic information such as constituency parsing trees into the encoding sequence to learn both the semantic and syntactic information from the document resulting in more accurate summary we propose a dynamic gate network to select the salient information based on the context of the decoder state which is essential to document summarization
the proposed model has been evaluated on cnn daily mail summarization datasets and the experimental results show that the proposed approach outperforms baseline approaches
index terms summarization parse tree dynamic selective gate syntactic attention i
introduction text summarization is a very challenging task of natural guage processing nlp and information retrieval
existing proaches for text summarization are categorized into two major types extractive and abstractive
extractive methods produce summaries by extracting sentences or tokens from the source text which can produce the grammatically correct summaries and preserve the meaning of the original text
however these approaches heavily rely on the text in the original documents and the extracted sentences may contain redundant information or be lack of readability
on the contrary abstractive methods produce the summaries by generating new sentences or tokens which do not necessarily appear in the source text
however abstractive approaches are more difcult in practice because they need to address many nlp problems including document understanding semantic representation and natural language generation which are harder than sentence extraction
the recent neural sequence to sequence proach has achieved tremendous success in many nlp tasks such as machine translation dialogue systems
the essence of based text summarization methods is an encoder decoder framework which rst encodes a input sentence to a low dimensional representation and then fig
the example of text summarization in cnn dataset
the colored text show the source text corresponding maries generated by human written original with attention and the proposed approach respectively
decodes the abstract representation into a output sequence
as the extension of methods attention based models encode a input sequence to a context vector using attention mechanism and dynamically calculate the attentive probability distribution at each generation step
similar to machine translation some researchers have plied neural model to abstractive text summarization
however there is a signicant difference between two tasks in machine translation one aims to capture all the semantic details from the source text while in text rization one only focuses on the salient text information and it is critical to utilize only the key information in the source text rather than the whole text
furthermore the original with attention method does not learn the syntactic structure of the source text which is important to text summarization
in figure a piece of original text is shown at the top
the human written summary is shown in the next box as the target
the third box shows a model generated summary using the original with attention method baseline
the green text in the target and the red text in the baseline show the summary corresponding to the blue text in the original text
as shown in the gure the baseline model incorrectly summarizes that he says he will become the citizens mainly because it is not able to capture the internal syntactic structure of the original text e

applicants is a noun phase and applied to

ceremony is an attributive clause for it
to address these problems we propose a novel syntactic and dynamic selective encoding method for document tion
we incorporate structured linguistic information such as parsing trees to learn more effective sentence representation by serializing parsing trees into a encoder sequence as in
in this way the encoding sequence contains both the semantic words and the syntactic parsing symbols information both of which are fed into the decoder for summary generation
in addition a document may contains hundreds of words and it is hard to directly encode the key information from the whole source text
a selective gate was proposed in previous study to lter out the secondary information
however the salient information varies in different decoding stage so it is better to select the salient information based on the context of decoder states
therefore for each decoding step we take a dynamic selective gate network to control the salient mation ow according to the document information current decoder state and the previous gated encoder state
in this way our approach can learn better representation of the sentences and select the salient information from the long input sequence for summary generation
as an example we show the summary generated by our approach in figure where the text correctly summarize the original sentences from the document
for reference figure shows the constituency parsing tree for the source sentence and figure shows the change of the states of the dynamic selective gates in a input sentence
we also conduct experiments on two large scale cnn daily mail datasets and the experimental results show that our model achieves superiority over baseline tion models
we organize the paper as follows sec
ii introduces the related work
sec
iii describes our proposed method
in sec
iv we present the experiment settings and illustrate experimental results
we conclude the paper in sec
v
ii
related work in general there are two broad approaches to automatic text summarization extractive and abstractive
extractive methods work by selecting important sentences or passages in the original text and reproducing them as summary
in contrast abstractive summarization techniques generate new shorter texts that seldom consist of the identical tences from the document
banko et al
apply statistical models of the term selection and term ordering processes to produce short summaries
bonnie and dorr implement a system using a combination of linguistically motivated sentence compression technique
other notable methods for abstractive summarization include using discriminative tree tree transduction model and quasi synchronous grammar approach utilizing both context free parses and dependency parses
recently researchers have started utilizing deep learning framework in extractive and abstractive summarization
for extractive methods nallapati et al
use recurrent neural networks rnns to read the article and get the representations of the sentences and select important sentences
yasunaga al
combine rnns with graph convolutional networks cnns to compute the salience of each sentence
narayan et al
propose a framework composed of a hierarchical encoder based on cnns and an attention based extractor with attention over external information
more works are published recently on abstractive methods
rush et al
rstly apply modern neural networks to text summarization by using a local attention based model to generate word conditioned on the input sentence
a bunch of work have been proposed to extend this approach which achieving further improvements in performance
chopra et al
use a similar tional attention based encoder and replace the decoder with a conditional rnns
nallapati et al
apply encoder decoder rnns framework with hierarchical attention and feature rich embedding vector
tan et al
propose graph based attention mechanism to summarize the salient information of document
however the above neural models can not emit unseen words since the vocabulary is xed at training time
in order to solve this problem the point network and the copynet have been proposed to allow both copying words from the original text and generating words from a xed vocabulary
hsu et al
combine the strength of extractive and stractive summarization and propose an inconsistency loss
zhou et al
extend general encoder decoder framework with a selective gate network which helps improve encoding effectiveness and release the burden of the decoder
our work has several signicant improvements comparing with previous studies
first to incorporate syntactic tion previous works only use unstructured linguistic mation such as part of speech pos tags and named entity
in this work we utilize a structured syntactic parsing tree to learn a more effective context vector which improves the performance of word prediction and alleviate the repetition problem
second to choose the salient information previous works employ a selective gate network which is static during the decoding stage
we improve the gate network and let the states of the gate dynamically adjust according to the context of the decoder states which is essential to document summarization
iii
methodology in this section we describe the proposed model
the architecture of the syntactic and dynamic selective syntactic encoding model is shown in figure which consists of the syntactic sequence encoder the dynamic selective gates and the pointer generator network with syntactic attention decoder
fig
overall architecture of the proposed syntactic and dynamic selective encoding model
the parsing tree of each sentence is serialized and fed into the encoder to help attain syntactic meanings
in jth decoding stage the decoder benets from the dynamic selective gate to drop out trivial words as well as attention mechanism inuenced by the syntactic vector
a
syntactic sequence encoder previous studies usually treat a document as a sequence of words but ignore the syntactic structure of document
to leverage the syntactic knowledge we design a syntactic sequence encoder to learn document representations
a document d is denoted as a sequence of sentences q d


qn where n is the number of sentences in the document
for each sentence ql we apply a syntactic parser to generate a parsing tree and then adopt a traversal to serialize the parsing tree to a sequence of tokens ql

ekl where kl is the number of tokens in the serialized parsing tree
note that the token is not necessarily a word
in a parsing tree a leaf node represents a word while a non leaf represents a parsing symbol including either a phrase label or a pos tag
then for a document we concatenate all the serialized parsing trees into a long sequence


em
here m is the total number of the tokens in all parsing trees m kl
to model the sequential information we rst use an embedding vector xi to represent the token ei which can be either a word or a symbol in the parsing tree
then we employ a bidirectional long short term memory bilstm to encoder the sequence information i

m i

m h h h i denote the hidden state of the forward where lstm and the backward lstm respectively
the whole representation of ith token is the concatenation of the hidden states from both directions hi h
h i and h i to model the syntactic information we apply the the hidden states corresponding to the pooling over all parsing symbols to produce the syntactic vector ds max s where s is the set of all parsing symbols in the document
as shown in figure the syntactic sequence encoder takes the serialized parsing tree as the input
the bilstm compute the hidden states for both the words e

and the parsing symbols e

as input
the word hidden states are used for further computation while the hidden states of the parsing symbols are max pooled to generate a syntactic vector
b
dynamic selective gates as we discussed in sec
i for document summarization not all the information in the source should be fed into the decoder and it is more important to only select the salient information and remove the unnecessary information from the input
herein we propose a novel dynamic selective gate to model the generation process of the salient information
we use a parameterized gate network to select the useful information for the summary generation
the gate state takes as input from both the state of the source and the previous state of the generated target as well as a low dimensional representation of the whole document dh which is a concatenation of the last state of the forward lstm and the rst state of the backward lstm dh h m h
specically for the ith encoder step and jth decoder step the state of the dynamic selective gate gj i is calculated as g dh g sj g bg gj i hj i gj i hi where sj is the state of the jth step from the decoder lstm which will be discussed in the next subsection is the gated hidden state of the encoder denotes the sigmoid function and denotes element wise multiplication
when j equals gj i is set to vector
wg ug vg bg are trainable parameters
note that previous study has utilized the selective gate to control the information ow but the gate state only depends on the hidden states of the source text and the selective gate is static during the whole decoding stage
but the proposed dynamic selective gate depends on both the encoder and the decoder states suggesting that the gate only open to the information which is useful for the current target output rather than the whole target outputs
this is critical to document summarization because the length of the document is long and a static gate may select much irrelevant information from the source at every decoding step
we will show the effectiveness of the dynamic selective gate in sec
iv e
c
pointer generator network with syntactic attention coder we use the recent proposed pointer generator network for decoding which allows either copying words from the original text via pointers or generating new words based on the source vocabulary to handle the oov problem
specically the attention strength between the ith source step and the jth target step is calculated by the current decoder state sj the current gated encoder hidden state hj i and the document syntactic vector
the context vector cj is calculated by the attention weighted summation of the gated encoding hidden states hj i a ds a sj ba ej a i cj aj i hj i i where wa ua va ba are trainable parameters
an lstm takes as input from the word embedding vector of the previous generated word the previous context vector and the previous decoder hidden state to compute the new decoder state sj and then the current context vector cj and the current decoder hidden state sj are fed into two linear layers and predicts the probability for each word w in the vocabulary using the softmax function pvoc j w v v cj bw bv where wv uv bw bv are trainable parameters
further a pointer generator network produces the switch probability pgen to decide whether generates a word by pvoc or copies a word from the original source text
pgen is calculated the decoder state sj and the from the context vector cj decoder word yj
the nal probability with the word w is calculated based on pgen and the attention distribution pgen cj pgen p sj yj bp aj i w where wp up vp bp are trainable parameters
d
model training to train the model we use the negative log likelihood function log as the loss for each document
we further adopt the coverage loss from see et al
aiming to handle the repetition problem in text summarization
the coverage loss at the decoding step j corresponding to the encoding step i is the summation of attention distributions over all previous decoding step aj i at i
the nal loss function at the decoding step j is log i aj i i iv
experiments in this section we describe the experiment details including datasets implementation details baselines and the results
a
datasets we conduct experiments on cnn daily dataset which comprises multi sentence summaries and has been widely used in automatic text summarization
we use released to obtain the same version of the the data which has training pairs validation pairs and test pairs
the source documents have words spanning sentences on an average while the summaries consist of words and
sentences
the dataset is released in two versions one is anonymized version which has been processed to replace each named entity and the other is the original version consisting of actual entity names
in this work we use the original text since it requires no pre processing and is more challenging because anonymized dataset replaces named entities with unique identier which always are out of vocabulary
in the following experiments all the models are trained and tested with three different datasets separately including cnn corpus daily mail corpus and the combination of cnn and daily mail corpus
table i shows the detail statistics information of experiment datasets
b
implementation for all experiments we use words of the source ulary and stanford constituency to get the syntactic information of the sentences in the corpora which includes phrase labels and pos tags
our model takes dimensional hidden states dimensional word embedding vectors and use adagrad with learning rate
and initialize
com abisee pointer generator
com abisee cnn dailymail
stanford
edu software srparser
html data set cnn daily cnn daily avgdocsents avgdocwords avgsumwords avgsumsents



table i data statistics for cnn and cnn daily mail datasets
avgdocsents is the average sentences number of original documents and avgdocwords is the average sentences length of original documents
avgsumsents is the average sentences number of summaries and avgdocwords is the average sentences length of summaries
the accumulator value with

this was found to work best among stochastic gradient descent adadelta momentum adam and rmsprop
in addition we set the maximum length of sentence on source side to on target side for training and testing to and respectively
to both decode fast and get better results we set the beam size to in our experiments
furthermore we added the coverage mechanism in loss function with coverage loss weighted to
c
baselines we compare our proposed model with several state the art automatic text summarization systems and techniques consisting of extractive and abstractive methods
is a standard extractive baseline which generates summary simply by selecting the leading three tences from source document
nn se utilizes encoder decoder framework which learns the representation of source though encoder and classies sentences of document by decoder
applies encoder decoder rnn tive framework with hierarchical attention and rich embedding vector
treats extractive summarization as a sequence classication problem where a binary decision has been made on each sentence about whether or not it should be included in the summary
summarunner is also an extractive model is trained directly on the ilar to summarunner but abstractive summaries
we use a framework based on uni gru with non hierarchical attention as our baseline model
distraction is an extension of model with distract mechanism to traverse between ferent content of a document to better grasp the overall meaning for summarization
graph based model proposes a novel abstractive graph based attention mechanism in the work which aims to nd salient content from the original document
deeprl proposes a unied framework combining and rl into to improve the quality of summary
pointer improves the standard model with a hybrid pointer generator which can not only produce novel words but also copy words from the source text
selectivegate proposes the encoder decoder work based on a static selective gate network which helps improve encoding effectiveness and release the burden of the decoder
d
experimental results we adopt the widely used by pyrouge for evaluation metric
it measures the similarity of the output summary and the standard reference by computing ping n gram such as unigram bigram and longest common subsequence
in the following experiments we adopt unigram bigram and rouge l longest common subsequence for evaluation
it can be observed from tables ii and iii that the posed approach achieves the best performance on the two datasets
our best model outperforms all baseline extractive and abstractive models on and l
compared with abstractive graph based rl based and leverages the structural summarunner model our model information of document and improves the pointer network with syntactic attention to copy relevant words in semantic and structural aspect from the original text to handle oov problems while graph based rl based and summarunner take the anonymized data which has replaced model all named entity with to alleviate oov problems
thermore unlike graph based rl based and summarunner model we do not pretrain the word embedding vectors
method rl distraction graph based model po o po our syntax our selective our


























table ii comparison results on the cnn test set tively using the full length variants of rouge
baseline model results with mark are taken from the corresponding papers
all our rouge scores have a condence interval of at most
as reported by the ofcial rouge script
we also compare in detail with two similar methods in table ii
for pointer generator with coverage po model results of baselines are incomplete on sub dataset because some other researchers chose to report results on only one sub dataset
python
org pypi

method rl summarunn summarunner graph based model deeprl pointer our model


























table iii comparison results on the cnn daily mail test set using the full length variants of rouge
baseline model results with mark are taken from the corresponding papers
represents vocabulary size of and represents vocabulary size of
all our rouge scores have a condence interval of at most
as reported by the ofcial rouge
we show that with the help of structural information and dynamic selective gate the scores of our best model performs the best over the po model on evaluation metrics

and
rouge l
for static selectivegate model we conduct two experiments with and without po due to its original paper focusing on short text summarization which does not use mechanism to alleviate oov and word repetition problems
the result demonstrates that the static selectivegate improves the performance of po model and the dynamic selectivegate can further improve the rouge scores of static selectivegate model by selecting current important information for decoding in every time step
further to study the different impacts of source syntax and dynamic selective gate on the performance of the proposed model we conduct ablation experiments on the cnn dataset where we train the model with the source syntax encoding only and the dynamic selective gate only respectively
as shown in the last three rows in table ii that either the source syntax encoding or the dynamic selective gate can improve the performance compared with selectivegate approach combining both approach leads to further improvement which achieves the best results as shown in the last row in the table
l rl








table iv comparison results with different document lengths on the cnn dataset respectively using the full length variants of rouge
all our rouge scores have a condence interval of at most
as reported by the ofcial rouge
in addition to study the impact of the lengths of document on the performance of the proposed model we conduct experiments on the cnn test sets between and
table iv clearly shows that the performance of the proposed approach is stable across different lengths of document
e
example analysis in this subsection we use an example to show the tiveness of the syntactic encoding and the dynamic selective mechanism
we choose the same example introduced in figure
figure shows the constituency parsing tree of the sentence
the baseline model in figure generates wrong summary cause it is not able to model the syntactic structure e

applicants is a noun phase and applied to

ceremony is an attributive clause
in our approach the bilstm encoder takes as input from the serialized parsing tree and each word token is surrounded by the parsing symbols
intuitively if two consecutive words do not belong to the same syntactic subtree more parsing symbols will be inserted between them in the bilstm encoder and it will be less likely that these two symbols have strong connection
as shown at the top in figure the generated summary of our model correctly conveys the summary of the source text
fig
a parsing tree corresponding to the blue text in figure
the dashed box shows that applied to

ceremony is an attributive clause for applicants
the upper box shows the our generated summary which correctly summarize the original document
for the dynamic selective gate we use a method in to visualize it
the method denes a highly non linear function to measure the contribution of the source word wi gated by gj i in the jth generation step
as shown in figure the dynamic selective mechanism can select the most important information from the original document in every decoding step
for example at decoding step the selective gate lters out some nonsensical words e

the is he and selects current important words e

jedlicka politician to help the following attention to generate the most important word e

jedlicka
furthermore figure also shows that the word out of source vocabulary can also be generated e

vit jedlicka but the weight of the selected words will decrease in the next decoding steps indicating that our model can address the oov problem and the word repetition problem
fig
visualization of the dynamic selective gates
the gates dynamically adjust the states in different decoding steps



darkness of the blocks indicates the openness of the gates
v
conclusions and future work in this work we propose a novel document summarization model which takes its input from a serialized parsing tree which enables the encoder to learn the inherent syntactic structure from each sentence in the document
further we pose a dynamic selective gate to control the information ow from the encoder to the decoder
this mechanism dynamically control the salient information based on the context of the decoder state which is essential to document summarization
the experimental results on two long text datasets cnn daily mail show the advantage of our model over several baseline approaches
in this work we use the traversal to generate the serialized parsing tree which is not able to contain all structure information from the tree
we will consider better tion to model the hierarchical structure of the parsing tree
on the other hand the proposed dynamic selective gate only apply to the word tokens in the source
we may consider integrating both the word tokens and the parsing symbol to produce better the information ow from the source to the target
references i
sutskever o
vinyals and q
v
le sequence to sequence learning with neural networks in proceedings of the conference on neural information processing systems pp

d
bahdanau k
cho and y
bengio neural machine translation by jointly learning to align and translate arxiv preprint

t
luong h
pham and c
d
manning effective proaches to attention based neural machine translation in proceedings of the conference on empirical methods in natural language processing pp

i
v
serban a
sordoni y
bengio a
courville and j
pineau building end to end dialogue systems ing generative hierarchical neural network models in proceedings of the aaai conference on articial intelligence
aaai press pp

a
bordes y

boureau and j
weston ing end to end goal oriented dialog arxiv preprint

j
gu z
lu h
li and v
o
li incorporating copying mechanism in sequence to sequence learning arxiv preprint

r
nallapati b
zhou c
dos santos c
gulcehre and b
xiang abstractive text summarization using sequence to sequence rnns and beyond in proceedings of the signll conference on computational ural language learning pp

a
see p
j
liu and c
d
manning get to the point summarization with pointer generator networks in ceedings of the annual meeting of the association for computational linguistics volume long papers vol
pp

j
tan x
wan and j
xiao abstractive document summarization with a graph based attentional neural model in proceedings of the annual meeting of the association for computational linguistics volume long papers vol
pp

j
li d
xiong z
tu m
zhu m
zhang and g
zhou modeling source syntax for neural machine translation in proceedings of the annual meeting of the ciation for computational linguistics volume long papers vol
pp

q
zhou n
yang f
wei and m
zhou selective encoding for abstractive sentence summarization in proceedings of the annual meeting of the ciation for computational linguistics volume long papers vol
pp

k

wong m
wu and w
li extractive rization using supervised and semi supervised learning in proceedings of the international conference on computational linguistics volume
association for computational linguistics pp

d
wang s
zhu t
li and y
gong multi document summarization using sentence based topic models in proceedings of the annual meeting of the ciation for computational linguistics
association for computational linguistics pp

a
celikyilmaz and d
hakkani tur a hybrid archical model for multi document summarization in proceedings of the annual meeting of the ation for computational linguistics
association for computational linguistics pp

r
m
alguliev r
m
aliguliyev m
s
hajirahimova and c
a
mehdiyev mcmr maximum coverage and minimum redundant text summarization model expert systems with applications vol
no
pp

g
erkan and d
r
radev lexrank graph based cal centrality as salience in text summarization journal of articial intelligence research vol
pp

r
m
alguliev r
m
aliguliyev and n
r
isazade multiple documents summarization based on ary optimization algorithm expert systems with cations vol
no
pp

summarization using inconsistency loss in proceedings of the annual meeting of the association for putational linguistics volume long papers
a
graves and j
schmidhuber framewise phoneme classication with bidirectional lstm and other neural network architectures neural networks vol
no
pp

j
cheng and m
lapata neural summarization by tracting sentences and words in proceedings of the annual meeting of the association for computational linguistics volume long papers vol
pp

q
chen x
zhu z
ling s
wei and h
jiang distraction based neural networks for modeling ments in proceedings of the international joint conference on articial intelligence
aaai press pp

r
paulus c
xiong and r
socher a deep reinforced model for abstractive summarization arxiv preprint

c

lin rouge a package for automatic evaluation of summaries text summarization branches out
j
li x
chen e
hovy and d
jurafsky visualizing and understanding neural models in nlp arxiv preprint

m
banko v
o
mittal and m
j
witbrock headline generation based on statistical translation in ings of the annual meeting on association for putational linguistics
association for computational linguistics pp

d
z
bonnie and b
dorr bbn umd at ary in proceedings of the document ing conference duc at conference of the north american chapter of the association for computational linguistics human language technologies
citeseer
t
cohn and m
lapata sentence compression beyond word deletion in proceedings of the international conference on computational linguistics volume
sociation for computational linguistics pp

k
woodsend y
feng and m
lapata generation with quasi synchronous grammar in proceedings of the conference on empirical methods in natural language processing
association for computational linguistics pp

r
nallapati f
zhai and b
zhou summarunner a recurrent neural network based sequence model for extractive summarization of documents
in proceedings of the aaai conference on articial intelligence pp

m
yasunaga r
zhang k
meelu a
pareek k
srinivasan and d
radev graph based summarization arxiv preprint ral multi document

s
narayan r
cardenas n
papasarantopoulos s
b
cohen m
lapata j
yu and y
chang document modeling with external attention for sentence extraction in proceedings of the annual meeting of the ciation for computational linguistics volume long papers vol
pp

a
m
rush s
chopra and j
weston a neural attention model for abstractive sentence summarization in proceedings of the conference on empirical methods in natural language processing pp

s
chopra m
auli and a
m
rush abstractive sentence summarization with attentive recurrent neural networks in proceedings of the annual conference of the north american chapter of the association for computational linguistics human language gies pp

o
vinyals m
fortunato and n
jaitly pointer works in proceedings of the twenty ninth conference on neural information processing systems pp

w

hsu c

lin m

lee k
min j
tang and m
sun a unied model for extractive and abstractive
