neural abstractive text summarizer for telugu language mohan bharath aravindh gowtham akhil m
tech iiit allahabad allahabad u
p india b
tech shiv nadar university greater noida u
p india m
tech university of hyderabad hyderabad telangana india
com
edu
in
com abstract
abstractive text summarization is the process of constructing semantically relevant shorter sentences which captures the essence of the overall meaning of the source text
it is actually difficult and very time consuming for humans to summarize manually large documents of text
much of work in abstractive text summarization is being done in english and almost no significant work has been reported in telugu abstractive text summarization
so we would like to propose an abstractive text summarization approach for telugu language using deep learning
in this paper we are proposing an abstractive text summarization deep learning model for telugu language
the proposed architecture is based on encoder decoder sequential models with attention mechanism
we have applied this model on manually created dataset to generate a one sentence summary of the source text and have got good results measured qualitatively
keywords deep learning lstm telugu neural networks nlp summarization introduction textual data is ever increasing in the current internet age
we need some process to condense the text and simultaneously preserving the meaning of the source text
text summarization is creating a short accurate and semantically relevant summary of a given text
it would help in easy and fast retrieval of information
text summarization can be classified into two categories
extractive text summarization methods form summaries by copying from the parts of the source text by taking some measure of importance on the words of the source text and then joining those sentences together to form a summary of the source text
abstractive text summarization methods create new semantically relevant phrases it can also form summaries by rephrasing or by using the words that were not in the source text
abstractive methods are actually harder
for an accurate and semantically relevant summaries the model is expected to comprehend the meaning of the text and then try to express that understanding using the relevant words and phrases
so abstractive models can have capabilities like generalization paraphrasing
significant work is being focused on extractive text summarization methods and especially with english as the source language
there is no reported work for telugu abstractive text summarization using deep learning models and also there are no available datasets for telugu text summarization
our goal is to build a model such that when given the telugu news article it should output semantically relevant sentence as the summary title sentence for the corresponding telugu article
we have proposed a deep learning model using encoder decoder architecture and we have achieved good results measured qualitatively
we have manually created the dataset because of the fact there are no available datasets
training dataset has been created from the telugu news websites by taking the headline as the summary and the main content as the source text and we have created a dataset with telugu news articles with their corresponding summaries which are taken as the headline of the respective article
we have created the dataset in such a way that the articles belonging to the different domains i
politics entertainment sports business national are more or less equally distributed to maintain a balance to the dataset
to create word embeddings for the telugu words we have made use of embeddings by fasttext which has created word embeddings for nearly languages with each word embedding of dimensions
related work as our work is based on abstractive text summarization using deep learning models on telugu language on which there is no reported work
our deep learning models are mainly inspired by these three papers rush et al
used an encoder decoder neural attention model to perform abstractive text summarization on english data and found that it performed very well and beat the previous non deep based approaches konstantin lopyrev proposed a encoder decoder recurrent neural network with attention mechanism to generate headlines for english attention mechanism itself is inspired from bahdanau seminal
approach in this section we provide a brief overview of the model architecture used and its individual components

recurrent neural network encoder decoder

rnns text is a sequential type of data
recurrent neural networks are a type of neural networks used for handling sequential data
rnns can take a variable length input sequence x
and can output sequence
yt
it uses an internal hidden to capture both the current input and the previous hidden state
a simple mathematical representation of recurrent network can be seen below
h whxt bh hidden state at timestep t by output at time step t wh is the weight matrix connecting input layer to hidden states and uh is the weight matrix connecting the hidden states of current time step to the hidden states of previous timestep
bh and by are biases of hidden states and of the output layer
w u and b are parameters and they are the same at each time step
h and y are activation functions
rnns can also be multi layered they are called deep rnns these are layered rnns where each layer extracts information from the previous layer


lstms long short term memory lstm is a type of rnn architecture with complicated hidden unit computations
so by introducing gates which are input forget output and memory cells it allows memorizing and forgetting over a long distance of training and the model can effectively handle the vanishing gradient problem
lstm is a basic unit of our encoder decoder model to perform summarization
fig

lstm unit the motivation behind using an lstm is that it captures long term dependency pretty well and the information in the starting of the sequence is able to traverse down the line
this is done by being selective and restricting the information flow in the lstm unit
there are three gates in an lstm
wf weight matrix xt input at timestep t dot product
forget gate layer ft bf ct ct ft ct cell state at time step t hidden state of previous timestep
forget gate sigmoid activation input gate layer it bi output gate layer ot ot

encoder decoder model ct bc ct ct it it input gate
ct cell state at timestep t
wi weight matrix between previous state and current input
encoder decoder model is based on neural networks which aims at handling the mapping between the highly structured input and output
in the vanilla encoder decoder model the encoder rnn first reads the source text word by word and then encodes the information in a hidden state and passes the information forward
decoder starts from the final hidden state of the encoder and at every timestep it computes a probability distribution on the total words in the vocabulary by taking a softmax function which gives probability values to all the words in the vocabulary and the most probable word is selected for that timestep and this continues until the end of sentence token is selected by the decoder or until the no of timesteps reaches the threshold
all the words generated so far will form the summary sentence be a length t input sequence to the encoder network
y be a length u output sequence the decoder network generates
each encoded representation ht contains information about the input sequence with focus on the tth input of the sequence
be the encoder network output of length t
fig

encoder decoder architecture in the encoder decoder framework the encoder tries to summarize the entire input in a fixed dimension vector ht decoder takes as input the output of the previous step and the hidden state vector from the previous time step
so at every timestep of the decoder the word selected in the previous time step and the hidden state vector of the previous time step is given as input in the current timestep


attention mechanism the basic encoder decoder model performs well on very short sentences but it fails to generalize well for longer sentences paragraphs
the only input to the decoder at the first timestep is a fixed size vector from this fixed size vector may not be able to capture all the relevant information at each step of the decoder only certain parts of the input are relevant to how do we make the model learn what to focus on at each step of the decoder the encoder stage last timestep
of the longer source text
generate an appropriate word
attention attention model calculates the importance of each input encoding for the current step of the decoder by doing a kind of similarity check between decoder output at this timestep and all the input encodings
this similarity check is done by taking dot product between the current hidden state of the decoder and all the input encodings
doing this for all of the input encodings and normalizing we get an importance vector
we then convert it to probabilities by passing through softmax which would give probability distribution
then we form a context vector by multiplying with the encodings
importanceit v battn attention distribution at softmax importanceit context vector ht ei ait context vector is then fed into two layers to generate distribution over the vocabulary from which we sample
for the loss at time step t losst where wt is the target summary word
i
e
negative log probability of the target summary word
loss at all the time steps is summed up
now we can do backpropagation and get all the required gradients and to minimize the loss we can apply gradient descent and learn the parameters of the network

training language python libraries tensorflow keras
dataset which is created manually contains telugu news articles and their corresponding summaries which are taken from the headline of the respective articles
dataset is divided into two parts for training and for testing
model has hidden units
loss function is cross entropy
training is done for epochs
training of encoder decoder architecture is done end to end i
loss is propagated to the encoder as well

evaluation evaluation of the results i
e generated summaries of the source text have been done qualitatively
here a sample of the generated summaries has been given below
original summary generated summary table
results source text



































































we have observed that the generated summaries are more or less semantically relevant to the source text in many cases in the test data despite the limited training data our model did a good job
conclusions we have implemented abstractive text summarization for telugu language using encoder decoder architecture with attention mechanism
specifically there are no datasets for telugu language that have paired human generated summaries
we have got semantically relevant good results on most of the test data measured qualitatively although there are very few generated summaries which were not at all relevant
given the fact the dataset has been created from the scratch and the limitation of the dataset size we have got good results
for text summarization in telugu language one of the difficulties is the lack of quality summaries of large given dataset in future we would like to create a very large dataset corpus for this purpose comparable to the standard english text summarization datasets available we would also work towards a numeric metric specifically to telugu text summarization tasks which would also capture the semantic relevance of the summary to the given source text
in future we shall explore the possibility of using the transformer architecture for telugu text summarization task
we have taken permission from competent authorities to use the images data as given in the paper
in case of any dispute in the future we shall be wholly responsible
references
edouard grave piotr bojanowski prakhar gupta armand joulin tomas mikolov
learning word vectors for languages

cs
cl
alexander m
rush sumit chopra jason weston a neural attention model for abstractive sentence summarization

cs
cl
konstantin lopyrev generating news headlines with recurrent neural networks

cl
dzmitry bahdanau kyughyun cho yoshua bengio neural machine translation by jointly learning to align and translate

cs
cl
