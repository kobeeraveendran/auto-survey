a neural attention model for abstractive sentence summarization alexander m
rush facebook ai research harvard seas
harvard
edu sumit chopra facebook ai research
com jason weston facebook ai research
com p e s l c
s c v
v i x r a abstract summarization based on text extraction is inherently limited but generation style stractive methods have proven in this work we propose ing to build
a fully data driven approach to tive sentence summarization
our method utilizes a local attention based model that generates each word of the summary ditioned on the input sentence
while the model is structurally simple it can ily be trained end to end and scales to a large amount of training data
the model shows signicant performance gains on the shared task compared with several strong baselines
introduction summarization is an important challenge of ral language understanding
the aim is to produce a condensed representation of an input text that captures the core meaning of the original
most successful summarization systems utilize tive approaches that crop out and stitch together portions of the text to produce a condensed in contrast abstractive summarization sion
tempts to produce a bottom up summary aspects of which may not appear as part of the original
we focus on the task of sentence level marization
while much work on this task has looked at deletion based sentence compression techniques knight and marcu among many others studies of human summarizers show that it is common to apply various other operations while condensing such as paraphrasing ization and reordering jing
past work has modeled this abstractive summarization lem either using linguistically inspired constraints dorr et al
zajic et al
or with tactic transformations of the input text cohn and figure example output of the attention based rization abs system
the heatmap represents a soft ment between the input right and the generated summary top
the columns represent the distribution over the input after generating each word
lapata woodsend et al

these proaches are described in more detail in section
we instead explore a fully data driven approach for generating abstractive summaries
inspired by the recent success of neural machine translation we combine a neural language model with a textual input encoder
our encoder is modeled off of the attention based encoder of bahdanau et al
in that it learns a latent soft alignment over the input text to help inform the summary as shown in figure
crucially both the encoder and the generation model are trained jointly on the sentence summarization task
the model is scribed in detail in section
our model also corporates a beam search decoder as well as tional features to model extractive elements these aspects are discussed in sections and
this approach to summarization which we call attention based summarization abs rates less linguistic structure than comparable stractive summarization approaches but can easily input



first sentence of article russian defense minister ivanov called sunday for the creation of a joint front for combating global terrorism output



generated headline russia calls for joint front against terrorism for joint front against figure example input sentence and the generated summary
the score of generating terrorism is based on the context yc for


against as well as the input



note that the summary generated is abstractive which makes it possible to generalize russian defense minister to russia and paraphrase for combating to against in addition to compressing dropping the creation of see jing for a survey of these editing operations
scale to train on a large amount of data
since our system makes no assumptions about the lary of the generated summary it can be trained directly on any document summary pair
this allows us to train a summarization model for headline generation on a corpus of article pairs from gigaword graff et al
consisting of around million articles
an example of tion is given in figure and we discuss the details of this task in section
to test the effectiveness of this approach we run extensive comparisons with multiple tive and extractive baselines including traditional integer linear syntax based systems constrained systems information retrieval style approaches as well as statistical phrase based chine translation
section describes the results of these experiments
our approach outperforms a machine translation system trained on the same large scale dataset and yields a large improvement over the highest scoring system in the competition
background we begin by dening the sentence summarization task
given an input sentence the goal is to duce a condensed summary
let the input sist of a sequence of m words


xm ing from a xed vocabulary v of size v
we will represent each word as an indicator vector xi for i


m sentences as a sequence of indicators and x as the set of ble inputs
furthermore dene the notation j to indicate the sub sequence of elements i j k
a summarizer takes as input and outputs a shortened sentence y of length n m
we will assume that the words in the summary also come from the same vocabulary v and that the output is contrast to a large scale sentence compression tems like filippova and altun which require tonic aligned compressions
a sequence


yn
note that in contrast to related tasks like machine translation we will sume that the output length n is xed and that the system knows the length of the summary fore generation
the problem of consider summaries
next erating set y


as all possible sentences of length n i
e
for all i and y y yi is an indicator
we say a system is abstractive if it tries to nd the optimal sequence from this set y dene the arg max yy y under a scoring function x y r
contrast this to a fully extractive sentence which transfers words from the input arg max


m n


mn or to the related problem of sentence compression that concentrates on deleting words from the input arg max


m n mi


mn
while abstractive summarization poses a more cult generation challenge the lack of hard straints gives the system more freedom in tion and allows it to t with a wider range of ing data
in this work we focus on factored scoring tions s that take into account a xed window of previous words y yc n the evaluation it is actually the number of bytes of the output that is capped
more detail is given in section
the literature is inconsistent on the formal denition of this distinction
some systems self described as abstractive would be extractive under our denition
where we dene yc


i for a window of size c
in particular consider probability of a summary given the input y log
we can write this as the conditional log log yc n where we make a markov assumption on the length of the context as size c and assume for i yi is a special start symbol
with this scoring function in mind our main focus will be on modelling the local conditional distribution yc
the next section denes a parameterization for this distribution in section we return to the question of generation for factored models and in section we introduce a modied factored scoring function
model the distribution of interest yc is a conditional language model based on the put sentence
past work on summarization and compression has used a noisy channel approach to split and independently estimate a language model and a conditional summarization model banko et al
knight and marcu daume iii and marcu i
e
arg max y log arg max log y where and are estimated separately
here we instead follow work in neural machine translation and directly parameterize the original distribution as a neural network
the network tains both a neural probabilistic language model and an encoder which acts as a conditional marization model

neural language model the core of our parameterization is a language model for estimating the contextual probability of the next word
the language model is adapted from a standard feed forward neural network guage model nnlm particularly the class of nnlms described by bengio et al

the full model is yc yc v h u e yc w enc p p c g yc x x f figure a network diagram for the nnlm decoder with additional encoder element
a network diagram for the attention based encoder
the parameters are e u v w where e rdv is a word embedding matrix u v rv h w rv h are weight d is the size of the word embeddings and h is a hidden layer of size h
the black box function enc is a contextual encoder term that turns a vector of size h representing the input and current context we consider several possible ants described subsequently
figure gives a schematic representation of the decoder ture

encoders note that without the encoder term this represents a standard language model
by incorporating in enc and training the two elements jointly we cially can incorporate the input text into tion
we discuss next several possible tions of the encoder
bag of words encoder our most basic model simply uses the bag of words of the input sentence embedded down to size h while ignoring ties of the original order or relationships between neighboring words
we write this model as yc p m


m x



where the input side embedding matrix f rhv is the only new parameter of the encoder and is a uniform distribution over the input words
yc


eyi h
of the weight matrices u v w also has a responding bias term
for readability we omit these terms throughout the paper
for summarization this model can capture the relative importance of words to distinguish tent words from stop words or embellishments
potentially the model can also learn to combine words although it is inherently limited in senting contiguous phrases
convolutional encoder to address some of the modelling issues with bag of words we also sider using a deep convolutional encoder for the input sentence
this architecture improves on the bag of words model by allowing local interactions between words while also not requiring the text yc while encoding the input
we utilize a standard time delay neural network tdnn architecture alternating between ral convolution layers and max pooling layers
j max xl i i


l xl i j xl where g rdv is an embedding of the text p is a new weight matrix rameter mapping between the context embedding and input embedding and q is a smoothing dow
the full model is shown in figure
informally we can think of this model as simply replacing the uniform distribution in bag of words with a learned soft alignment p between the put and the summary
figure shows an ple of this distribution p as a summary is ated
the soft alignment is then used to weight the smoothed version of the input x when structing the representation
for instance if the current context aligns well with position i then the words xiq


are highly weighted by the encoder
together with the nnlm this model can be seen as a stripped down version of the attention based neural machine translation model
i


l xl







training where f is a word embedding matrix and consists of a set of lters for each layer


l
eq
is a temporal tion layer eq
consists of a element temporal max pooling layer and a pointwise non linearity and nal output eq
is a max over time
at each layer x is one half the size of x
for simplicity we assume that the convolution is padded at the boundaries and that m is greater than so that the dimensions are well dened
attention based encoder while the tional encoder has richer capacity than bag words it still is required to produce a single resentation for the entire input sentence
a lar issue in machine translation inspired bahdanau et al
to instead utilize an attention based contextual encoder that constructs a representation based on the generation context
here we note that if we exploit this context we can actually use a rather simple model similar to bag of words yc c p x


fxm


gyi i xi xi q
q the lack of generation constraints makes it sible to train the model on arbitrary input output pairs
once we have dened the local tional model yc we can estimate the parameters to minimize the negative likelihood of a set of summaries
dene this ing set as consisting of j input summary pairs



the negative likelihood conveniently into a term for each token in the summary log j j n log yc
we minimize nll by using mini batch stochastic gradient descent
the details are described further in section
be explicit compared to bahdanau et al
our model uses an nnlm instead of a target side lstm source side windowed averaging instead of a source side directional rnn and a weighted dot product for alignment instead of an alignment mlp
is dependent on using the gold standard contexts
an alternative is to use the predicted context within a structured or reenforcement learning style objective
generating summaries extension extractive tuning we now return to the problem of generating maries
recall from eq
that our goal is to nd y arg max yc
n yy unlike phrase based machine translation where inference is np hard it actually is tractable in ory to compute y
since there is no explicit hard alignment constraint viterbi decoding can be plied and requires v c time to nd an exact solution
in practice though v is large enough to make this difcult
an alternative approach is to approximate the arg max with a strictly greedy or deterministic decoder
a compromise between exact and greedy coding is to use a beam search decoder rithm which maintains the full vocabulary v while limiting itself to k potential hypotheses at each position of the summary
this has been the standard approach for neural mt models danau et al
sutskever et al
luong et al

the beam search algorithm is shown here modied for the feed forward model algorithm beam search input parameters beam size k input output approx
k best summaries s v if abstractive else i for i to n do generate hypotheses n y hypothesis recombination n h n s
t
yc c filter k max k arg max yh end for return yc while we will see that the attention based model is effective at generating summaries it does miss an important aspect seen in the human generated in particular the abstractive model references
does not have the capacity to nd extractive word matches when necessary for example transferring unseen proper noun phrases from the input
lar issues have also been observed in neural lation models particularly in terms of translating rare words luong et al

to address this issue we experiment with tuning a very small set of additional features that off the abstractive extractive tendency of the tem
we do this by modifying our scoring function to directly estimate the probability of a summary using a log linear model as is standard in machine translation yc
n where is a weight vector and f is a ture function
finding the best summary under this distribution corresponds to maximizing a factored scoring function s yc
n where yc yc to isfy eq

the function is dened to combine the local conditional probability with some tional indicator featrues yc log yc

xjk k
xjk k j
yi xk
as with viterbi this beam search algorithm is much simpler than beam search for phrase based mt
because there is no explicit constraint that each source word be used exactly once there is no need to maintain a bit set and we can ply move from left to right generating words
the beam search algorithm requires v time
from a computational perspective though each round of beam search is dominated by computing yc for each of the k hypotheses
these can be computed as a mini batch which in tice greatly reduces the factor of k
these features correspond to indicators of gram bigram and trigram match with the input as well as reordering of input words
note that ting


gives a model identical to standard abs
after training the main neural model we x and tune the parameters
we follow the tical machine translation setup and use error rate training mert to tune for the rization metric on tuning data och
this tuning step is also identical to the one used for the phrase based machine translation baseline
related work abstractive sentence summarization has been ditionally connected to the task of headline ation
our work is similar to early work of banko et al
who developed a statistical machine translation inspired approach for this task using a corpus of headline article pairs
we extend this using a neural summarization approach by model as opposed to a count based noisy channel model training the model on much larger scale k compared to million articles and lowing fully abstractive decoding
this task was standardized around the and competitions over et al

the topiary system zajic et al
performed the best in this task and is described in detail in the next section
we point interested ers to the duc web page
nist
for the full list of systems entered in this shared task
more recently cohn and lapata give a compression method which allows for more bitrary transformations
they extract tree duction rules from aligned parsed texts and learn weights on transfomations using a max margin learning algorithm
woodsend et al
pose a quasi synchronous grammar approach lizing both context free parses and dependency parses to produce legible summaries
both of these approaches differ from ours in that they rectly use the syntax of the input output sentences
the latter system is in our results we tempted to train the former system on this dataset but could not train it at scale
in addition to banko et al
there has been some work using statistical machine translation directly for abstractive summary
wubben et al
utilize moses directly as a method for text simplication
recently filippova and altun developed a strictly extractive system that is trained on a atively large corpora k sentences of title pairs
because their focus is extractive pression the sentences are transformed by a series of heuristics such that the words are in monotonic alignment
our system does not require this ment step but instead uses the text directly
neural mt this work is closely related to cent work on neural network language models nnlm and to work on neural machine tion
the core of our model is a nnlm based on that of bengio et al

recently there have been several papers about models for machine translation kalchbrenner and blunsom cho et al
sutskever et al

of these our model is most closely related to the attention based model of bahdanau et al
which explicitly nds a soft alignment tween the current position and the input source
most of these models utilize recurrent neural works rnns for generation as opposed to forward models
we hope to incorporate an lm in future work
experimental setup we experiment with our attention based sentence summarization model on the task of headline eration
in this section we describe the corpora used for this task the baseline methods we pare with and implementation details of our proach

data set the standard sentence summarization evaluation set is associated with the and shared tasks over et al

the data for this task consists of news cles from the new york times and associated press wire services each paired with different human generated reference summaries not ally headlines capped at bytes
this data set is evaluation only although the similarly sized data set was made available for the task
the expectation is for a summary of roughly words based on the text of a complete cle although we only make use of the rst tence
the full data set is available by request at
nist
gov data
html
for this shared task systems were entered and evaluated using several variants of the oriented rouge metric lin
to make recall only evaluation unbiased to length put of all systems is cut off after characters and no bonus is given for shorter summaries
unlike bleu which interpolates various n gram matches there are several versions of rouge for different match lengths
the duc evaluation uses unigrams bigrams and rouge l longest common substring all of which we report
in addition to the standard ation we also report evaluation on single ence headline generation using a randomly out subset of gigaword
this evaluation is closer to the task the model is trained for and it allows us to use a bigger evaluation set which we will clude in our code release
for this evaluation we tune systems to generate output of the average title length
for training data for both tasks we utilize the annotated gigaword data set graff et al
napoles et al
which consists of standard gigaword preprocessed with stanford corenlp tools manning et al

our model only uses annotations for tokenization and sentence tion although several of the baselines use parsing and tagging as well
gigaword contains around
million news articles sourced from various tic and international news services over the last two decades
for our training set we pair the headline of each article with its rst sentence to create an summary pair
while the model could in theory be trained on any pair gigaword contains many rious headline article pairs
we therefore prune training based on the following heuristic lters are there no non stop words in common does the title contain a byline or other ous editing marks does the title have a tion mark or colon after applying these lters the training set consists of roughly j million title article pairs
we apply a minimal ing step using ptb tokenization lower casing placing all digit characters with and replacing of word types seen less than times with unk
we also remove all articles from the time period of the duc evaluation
release
the complete input training vocabulary consists of million word tokens and k unique word types with an average sentence size of
words
the headline vocabulary consists of million kens and k word types with the average title of length
words note that this is signicantly shorter than the duc summaries
on average there are
overlapping word types between the headline and the input although only
in the rst characters of the input

baselines due to the variety of approaches to the sentence summarization problem we report a broad set of headline generation baselines
from the task we include the fix baseline that simply returns the rst characters of the input as the headline
we also report the winning system on this shared task topiary zajic et al

topiary merges a compression system using motivated transformations of the input dorr et al
with an unsupervised topic detection utd algorithm that appends key phrases from the full article onto the compressed output
woodsend et al
described above also report results on the duc dataset
the duc task also includes a set of manual summaries performed by human summarizers each summarizing half of the test data sentences yielding references per sentence
we report the average inter annotater agreement score as erence
for reference the best human evaluator scores

we also include several baselines that have cess to the same training data as our system
the rst is a sentence compression baseline press clarke and lapata
this model uses the syntactic structure of the original sentence along with a language model trained on the line data to produce a compressed output
the syntax and language model are combined with a set of linguistic constraints and decoding is formed with an ilp solver
to control for memorizing titles from training we implement an information retrieval baseline ir
this baseline indexes the training set and gives the title for the article with highest match to the input see manning et al

finally we use a phrase based statistical chine translation system trained on gigaword to produce summaries koehn et al

to improve the baseline for this task we augment the phrase table with deletion rules mapping each article word to include an tional deletion feature for these rules and allow for an innite distortion limit
we also itly tune the model using mert to target the byte capped rouge score as opposed to standard bleu based tuning
unfortunately one ing issue is that it is non trivial to modify the lation decoder to produce xed length outputs so we tune the system to produce roughly the pected length
model rouge l ext
rouge l gigaword ir prefix compress topiary abs reference














































table experimental results on the main summary tasks on various rouge metrics
baseline models are described in detail in section

we report the percentage of tokens in the summary that also appear in the input for gigaword as ext

implementation for training we use mini batch stochastic gradient descent to minimize negative log likelihood
we use a learning rate of
and split the learning rate by half if validation log likelihood does not improve for an epoch
training is performed with shufed mini batches of size
the minibatches are grouped by input length
after each epoch we renormalize the embedding tables hinton et al

based on the validation set we set parameters as d h c l and q
our implementation uses the torch numerical framework
and will be openly available along with the data pipeline
cially training is performed on gpus and would be intractable or require approximations wise
processing mini batches with d h requires seconds
best tion accuracy is reached after epochs through the data which requires around days of training
additionally as described in section we apply a mert tuning step after training using the data
for this step we use z mert zaidan
we refer to the main model as abs and the tuned model as
results our main results are presented in table
we run experiments both using the uation data set sentences references bytes with all systems and a randomly held out gigaword test set sentences reference
we rst note that the baselines compress and ir do relatively poorly on both datasets indicating that neither just having article information or guage model information alone is sufcient for the task
the prefix baseline actually performs prisingly well on which makes sense given the earlier observed overlap between article and summary
both abs and perform better than topiary particularly on and rouge l in duc
the full model scores the best on these tasks and is signicantly better based on the default rouge condence level than topiary on all metrics and on for duc as well as and rouge l for gigaword
note that the additional extractive features bias the system towards taining more input words which is useful for the underlying metric
next we consider ablations to the model and gorithm structure
table shows experiments for the model with various encoders
for these iments we look at the perplexity of the system as a language model on validation data which trols for the variable of inference and tuning
the nnlm language model with no encoder gives a gain over the standard n gram language model
including even the bag of words encoder reduces perplexity number to below
both the lutional encoder and the attention based encoder further reduce the perplexity with attention giving a value below
we also consider model and decoding ablations on the main summary model shown in table
these experiments compare to the bow encoding models compare beam search and greedy ing as well as restricting the system to be plete extractive
of these features the biggest pact is from using a more powerful encoder tion versus bow as well as using beam search to generate summaries
the abstractive nature of the system helps but for rouge even using pure tractive generation is effective
model encoder perplexity kn smoothed gram feed forward nnlm bag of word convolutional tdnn attention based abs none none




table perplexity results on the gigaword validation set comparing various language models with and to end summarization models
the encoders are dened in section
decoder model cons
greedy beam beam beam abs bow abs ext abs







r l



table rouge scores on development data for various versions of inference
greedy and beam are scribed in section
ext
is a purely extractive version of the system eq
finally we consider example summaries shown in figure
despite improving on the line scores this model is far from human formance on this task
generally the models are good at picking out key words from the input such as names and places
however both models will reorder words in syntactically incorrect ways for instance in sentence both models have the wrong subject
abs often uses more interesting re wording for instance new nz pm after election in sentence but this can also lead to attachment mistakes such a russian oil giant chevron in tence
conclusion we have presented a neural attention based model for abstractive summarization based on recent velopments in neural machine translation
we combine this probabilistic model with a tion algorithm which produces accurate tive summaries
as a next step we would like to further improve the grammaticality of the maries in a data driven way as well as scale this system to generate paragraph level summaries
both pose additional challenges in terms of cient alignment and consistency in generation
references dzmitry bahdanau kyunghyun cho and yoshua
neural machine translation by corr bengio
jointly learning to align and translate


a detained iranian american academic accused of acting against national security has been released from a tehran prison after a hefty bail was posted a to p judiciary ofcial said tuesday
g iranian american academic held in tehran released on bail a detained iranian american academic released from jail after posting bail detained iranian american academic released from prison after hefty bail ministers from the european union and its mediterranean neighbors gathered here under heavy security on monday for an unprecedented conference on economic and political cooperation
g european mediterranean ministers gather for landmark conference by julie bradford a mediterranean neighbors gather for unprecedented conference on heavy security mediterranean neighbors gather under heavy security for dented conference the death toll from a school collapse in a haitian shanty town rose to after rescue workers uncovered a classroom with dead students and their teacher ofcials said saturday
g toll rises to in haiti school unk ofcial a death toll in haiti school accident rises to death toll in haiti school to dead students australian foreign minister stephen smith sunday congratulated new zealand s new prime minister elect john key as he praised ousted leader helen clark as a gutsy and respected politician
g time caught up with nz s gutsy clark says australian fm a australian foreign minister congratulates new nz pm after election australian foreign minister congratulates smith new zealand as leader two drunken south african fans hurled racist abuse at the country s rugby sevens coach after the team were eliminated from the weekend s hong kong tournament reports said tuesday
g rugby union racist taunts mar hong kong sevens report a south african fans hurl racist taunts at rugby sevens south african fans racist abuse at rugby sevens tournament christian conservatives kingmakers in the last two us presidential elections may have less success in getting their pick elected in political observers say
g christian conservatives power diminished ahead of vote a christian conservatives may have less success in election christian conservatives in the last two us presidential elections the white house on thursday warned iran of possible new sanctions after the un nuclear watchdog reported that tehran had begun sensitive nuclear work at a key site in deance of un resolutions
g us warns iran of step backward on nuclear issue a iran warns of possible new sanctions on nuclear work un nuclear watchdog warns iran of possible new sanctions thousands of kashmiris chanting pro pakistan slogans on sunday attended a rally to welcome back a hardline separatist leader who underwent cancer treatment in mumbai
g thousands attend rally for kashmir hardliner a thousands rally in support of hardline kashmiri separatist leader thousands of kashmiris rally to welcome back cancer treatment an explosion in iraq s restive northeastern province of diyala killed two us soldiers and wounded two more the military reported monday
g two us soldiers killed in iraq blast december toll a us two soldiers killed in restive northeast province explosion in restive northeastern province kills two us soldiers russian world no
nikolay davydenko became the fth drawal through injury or illness at the sydney international wednesday retiring from his second round match with a foot injury
g tennis davydenko pulls out of sydney with injury a davydenko pulls out of sydney international with foot injury russian world no
davydenko retires at sydney international russia s gas and oil giant gazprom and us oil major chevron have set up a joint venture based in resource rich northwestern siberia the interfax news agency reported thursday quoting gazprom ofcials
g gazprom chevron set up joint venture a russian oil giant chevron set up siberia joint venture russia s gazprom set up joint venture in siberia figure example sentence summaries produced on gaword
i is the input a is abs and g is the true headline
michele banko vibhu o mittal and michael j brock

headline generation based on tical translation
in proceedings of the annual meeting on association for computational tics pages
association for computational linguistics
yoshua bengio rejean ducharme pascal vincent and christian janvin

a neural probabilistic guage model
the journal of machine learning search
kyunghyun cho bart van merrienboer c aglar gulcehre dzmitry bahdanau fethi bougares ger schwenk and yoshua bengio

learning phrase representations using rnn encoder decoder for statistical machine translation
in proceedings of emnlp pages
james clarke and mirella lapata

global ference for sentence compression an integer linear programming approach
journal of articial gence research pages
trevor cohn and mirella lapata

sentence compression beyond word deletion
in proceedings of the international conference on tional linguistics volume pages
ciation for computational linguistics
hal daume iii and daniel marcu

a channel model for document compression
in ceedings of the annual meeting on association for computational linguistics pages
sociation for computational linguistics
bonnie dorr david zajic and richard schwartz

hedge trimmer a parse and trim approach to headline generation
in proceedings of the naacl on text summarization workshop volume pages
association for computational guistics
katja filippova and yasemin altun

ing the lack of parallel data in sentence compression
in emnlp pages
david graff junbo kong ke chen and kazuaki maeda

english gigaword
linguistic data consortium philadelphia
geoffrey e
hinton nitish srivastava alex krizhevsky ilya sutskever and ruslan improving neural networks by dinov
preventing co adaptation of feature detectors
corr


hongyan jing

using hidden markov modeling to decompose human written summaries
tional linguistics
nal kalchbrenner and phil blunsom

recurrent in emnlp pages continuous translation models

kevin knight and daniel marcu

tion beyond sentence extraction a probabilistic proach to sentence compression
articial gence
philipp koehn hieu hoang alexandra birch chris callison burch marcello federico nicola bertoldi brooke cowan wade shen christine moran richard zens al

moses open source in toolkit for statistical machine translation
ceedings of the annual meeting of the acl on interactive poster and demonstration sessions pages
association for computational tics
chin yew lin

rouge a package for automatic in text summarization evaluation of summaries
branches out proceedings of the shop pages
thang luong ilya sutskever quoc v
le oriol vinyals and wojciech zaremba

ing the rare word problem in neural machine in proceedings of the annual lation
ing of the association for computational tics pages
christopher d manning prabhakar raghavan and introduction to hinrich schutze

tion retrieval volume
cambridge university press cambridge
christopher d manning mihai surdeanu john bauer jenny finkel steven j bethard and david closky

the stanford corenlp natural in proceedings of guage processing toolkit
annual meeting of the association for tional linguistics system demonstrations pages
courtney napoles matthew gormley and benjamin in van durme

annotated gigaword
ceedings of the joint workshop on automatic edge base construction and web scale knowledge extraction pages
association for tational linguistics
franz josef och

minimum error rate training in statistical machine translation
in proceedings of the annual meeting on association for tational linguistics volume pages
sociation for computational linguistics
paul over hoa dang and donna harman

duc in context
information processing management
ilya sutskever oriol vinyals and quoc vv le

sequence to sequence learning with neural works
in advances in neural information ing systems pages
kristian woodsend yansong feng and mirella lapata

generation with quasi synchronous grammar
in proceedings of the conference on empirical methods in natural language processing pages
association for computational linguistics
sander wubben antal van den bosch and emiel krahmer

sentence simplication by lingual machine translation
in proceedings of the annual meeting of the association for tational linguistics long papers volume pages
association for computational tics
omar zaidan

z mert a fully congurable open source tool for minimum error rate training of machine translation systems
the prague bulletin of mathematical linguistics
david zajic bonnie dorr and richard schwartz
in
bbn umd at topiary
ceedings of the hlt naacl document standing workshop boston pages

