n u j l c
s c v
v i x r a taming language gans with cautious sampling strategies thomas paul alexis sylvain lamprier benjamin jacopo cnrs france sorbonne universit cnrs paris france recital paris france thomas jacopo paul
ai sylvain
lamprier benjamin

fr abstract training regimes based on maximum likelihood estimation mle suffer from known limitations often leading to poorly generated text sequences
at the root of these limitations is the mismatch between training and inference i
e
the so called exposure bias exacerbated by considering only the reference texts as correct while in practice several alternative formulations could be as good
generative ial networks gans can mitigate those limitations but the discrete nature of text has hindered their application to language generation the approaches proposed so far based on reinforcement learning have been shown to underperform mle
departing from previous works we analyze the exploration step in gans applied to text generation and show how classical sampling results in unstable training
we propose to consider alternative exploration strategies in a gan framework that we name coldgan s where we force the sampling to be close to the bution modes to get smoother learning dynamics
for the rst time to the best of our knowledge the proposed language gans compare favorably to mle and obtain improvements over the state of the art on three generative tasks namely unconditional text generation question generation and abstractive summarization
introduction deep learning approaches have paved the way for signicant achievements in natural language generation nlg
under the most popular paradigm sequence to sequence models are trained with maximum likelihood estimation mle via teacher forcing
training neural networks under mle does not succeed in modeling sequence probabilities since at inference the model is conditioned on sequences that may have never been observed at training time
indeed generated texts using this approach are often degenerate e

prone to repetition
nonetheless these same architectures when used as discriminators are able to distinguish human from machine generated text with a disconcerting efciency reported values are around for long article generation or abstractive summarization
in the generative architectures the encoder part can reach such performances supporting the hypothesis that generation failures are mostly due to the decoding step under mle training regimes the decoding suffers from exposure bias and lacks a sequence level loss to optimize
to mitigate mle limitations reinforcement learning rl has been applied to text generation tasks considering sequence level metrics such as bleu or rouge as the reward
however such metrics based on n grams similarity are known to poorly correlate with human judgments preprint
under review
and do not preserve meaning
hence when reinforced on them models yield to poorer tions and higher degradation compared to their mle counterparts
to overcome these drawbacks better rewards are thus necessary
to this end ziegler et al
proposed to directly reward systems using human judgment
although this approach performs very well and approximates the best possible reward it is obviously not a viable solution in practice
however it attests that with perfect rewards one can achieve excellent levels of performance
a natural alternative not requiring human judgments is to frame the problem under the generative adversarial network gan paradigm which has been used successfully for image generation
for text modeled as a sequence of discrete symbols a naive computation of the gradients is however intractable
hence language gans are based on gradient estimation via rl based techniques
however the reward in this case can be extremely sparse as discussed in section
yielding to high variance gradient estimation which is known to be challenging for optimization
most previous works have focused on this aspect and proposed denser rewards
unfortunately these attempts to apply gans to text generation obtained limited success and have been found to underperform mle
although known to be crucial exploration is surprisingly understudied when rl is applied to text generation
in this work we propose a new exploration method that aims at sampling more structured rewards and that better suits the gans training dynamics allowing for the rst time to successfully train language gans
our main contributions can be summarized as
we study the discriminators behavior and show that their degree of specialization has important implications on the exploration to stabilize the training process
in particular we nd that reducing the exploration space is essential to successfully train discrete gans

based on these observations we propose coldgan s a gan architecture using alternative sampling strategies that force the sampling to remain closer to the distribution modes

finally we apply our proposed methods on three tasks
we report positive results compared to previous works including gans and mle based models
related work rl for text generation since many metrics of interest in nlp are non differentiable several approaches used rl for text generation
to our knowledge all works based on rl for text generation use standard sampling for policy gradient estimation following the current policy from the generator they dene
apart from text gans they all suffer from the aforementioned limitations of ill dened reward metrics such as bleu or rouge
text gans tackling this problem by implicitly learning the metric via a discriminator adversarial approaches have been proposed for text generation
given the very high dimension of the generative action space and the sparsity of associated rewards provided by the discriminator see section
a large body of works focused on dening denser rewards ranking and comparative discriminators sequential discriminators where the rewards are provided at each time step of the generation or using masked language modeling
the policy is usually learned via vanilla policy gradient reinforce with the exception of maligan which another difculty with gans for discrete sequential data is that discriminators are inaccurate for samples close to the generator distribution modes as those used for training are usually too scattered over the full space to enable specialization on useful difcult areas see section
for preliminary experiments on this
cautious rl standard works in rl proposed ways to avoid catastrophic moves of the policy parameters by enforcing the new policy to stay importance sampling for reinforcement learning in rl is is generally used for sample ciency purposes in off policy policy gradient methods is allows to re use previously sampled sequences more than once
conversely in this work is is employed to improve the stability of rl for text gans
closer to our work maligan proposes to rely on is to consider an estimation of the data distribution as a target via a kl objective
although theoretically appealing its stability relies on very strong assumptions about discriminator guarantees which rarely hold in practice
instead we propose to rely on is to stabilize the generator discriminator min max game via alternative careful sampling strategies
note also that our approach could easily be included in the maligan framework
discriminators and generators interaction
generating and discriminating as text to text tasks generator text generation naturally lends itself to autoregressive modeling
the probability to generate a sequence y composed of n tokens


yn is given by


x n where are the learnable parameters of the generator and x the input sequence
neural networks typically produce class probabilities by using a softmax output layer that converts the logit zi computed for each token of the vocabulary into a probability qi qi exp zi t j exp t where t is a temperature hyper parameter set to unless otherwise specied
the higher the temperature the more uniform the probability distribution over the vocabulary resulting in more diversity but also more mistakes
in the following we note as the distribution dened by the generator with temperature t
discriminator generated texts for each input x as a logistic regression problem in the following we consider a discriminator d learned from sets of human and x y h x y g y y where h is a set of pairs of input x associated with a human written text y from the data distribution and g is a set of pairs with generated outputs y
text to text tasks casting any nlp task as a text to text problem demonstrated state the art results on the established glue benchmark and on its more challenging successor
accordingly we employ the same architecture for both discrimination and generation
this allows for fairer comparisons thereafter as both generator and discriminator have the same architecture pre training and capacity

discriminator generator equilibrium exposure bias as mentioned above a discriminator can easily predict the human or machine nature of a text
one reason for this lies in exposure bias
to quantify this statement we compare the results for a discriminator when trained under the two following generation strategies standard generation suffering from the exposure bias and teacher forcing generation where the truth tokens t are fed to the generator so not to expose the model to its own prediction and only yt is generated by a machine
we show the results in fig

as expected the two discriminators have the same score for t
we observe that both perform well and that the standard generation discriminator obtains consistently larger improvements w

t
the teacher forcing generation discriminator as the length of the sequence increases
this could indicate the presence of the exposure bias for which the errors accumulate over time
still the relatively high accuracy observed under teacher forcing generation suggests that additional factors beyond exposure bias might be involved in the following we show that the extreme specialization of discriminators is among those
figure accuracy of a discriminator model trained under two different generation modes standard subject to the exposure bias and teacher forcing
the axis corresponds to the partial length t of the sequence to discriminate
table probability that a text is human according to various discriminators
dperf ect corresponds to a theoretical perfect discriminator with innite capacity and training data
dt corresponds to a discriminator trained on samples generated with a temperature t
past t and past t correspond to results on samples obtained with the generator weights resumed from a previous stage of the training i
e
a checkpoint one epoch before the nal state see section memory replay
human t t t past t past t evaluated on dt dt dperf ect























discriminator s no free lunch as dened above the temperature t of the generator is a parameter which allows to control the randomness of predictions while sampling by scaling the logits before applying a softmax
thus we can dene various sampling strategies from the same generator
low close to temperatures provide samples close to the sequence sgreedy of a greedy procedure that takes the token with max generator probability at each step the output of a beam search with beam size b
with high temperatures the distribution of sequences tends to the uniform distribution
we experiment with different temperature settings for the same generator trained with mle and use the obtained samples to train and test a discriminator
this allows us to evaluate the impact of differences in sampling temperatures between training and inference on the discriminator performance
in other words how a discriminator trained with samples obtained at a specic temperature performs when faced with samples generated under different sampling setups
we train and evaluate discriminators on samples generated under temperatures t or for a conditional generation task summarization see section
which allows to consider various sequence samples even at low temperatures
we report the results in table
as expected in all but one case discriminators perform better if trained and evaluated with sequences generated under the same temperature no mismatch
however when the training and evaluation samples are generated with different temperatures we observe that the discriminator fails to distinguish human from generated ones
more precisely it considers most sentences to be human generated around
conversely when trained on the different temperatures together t results are more balanced robust across the various temperatures but yielding a drop in accuracy consistently with the well known accuracy robustness trade off
this highlights that individual discriminators are specialized on specic generation pairs machine human
knowing this it is crucial to orient this specialization on useful areas
interestingly when trained from samples issued from the discriminator dt is inaccurate at identifying samples close to sgreedy as generated ones dt equals
on average over these samples
this is particularly bad for a discriminator used as a reward signal of a rl process since such samples lie in the useful area of the output distribution
they correspond to samples close to the modes of the distribution
moreover in many text generation applications generation






accuracystandard generationteacher forcing generation strategies such as beam search target these sequences as prediction outputs
a bad reward function at these locations is likely to lead to bad generation performance
besides the discriminator trained on samples close to the mode of i
e
dt is bad for samples from i
e
t indicating that one can not simply use such samples to train the discriminator while considering standard sampling for generator training as rewards would be very inaccurate
implications for discrete gans holtzman et al
report that for t sampling from the tail of the distribution is expected to happen within the rst three steps of decoding and with a probability superior to
within steps
such unstructured exploration causes a large variance which grows with the number of time steps and perturbs actions too frequently
a less random exploration would thus yield to better structured sequences and lower variance closer to the distribution learned by the discriminator and would likely enable better training dynamics between the discriminator and the generator
models based on the ndings above we seek sampling strategies that allow both the discriminator to train on useful samples and the generator to be trained from reliable rewards from the discriminator within a policy gradient rl scheme where we are interested at maximizing e according to generator parameters
the discriminator is updated at the end of each training epoch via gradient ascent on human machine pairs with new articial sequences resulting from the generator distribution
in order to introduce cautious sampling that focuses more on modes of distributions note that it would be useless to consider the policy gradient e t of a generator distribution with modied temperature t as it would compared to t only imply rescaling the network outputs without altering the learning process
log t e t instead we propose to employ importance sampling for dening our cautious sampling strategies for text gans based on the fact that for any distribution p q x such that whenever p and any function x r we have exp p
in our case this yields the following unbiased policy gradient e log where t v is the t token from sequence and the subsequence of its t rst tokens the generator probability and a modied sampling distribution which enables the generation of any possible sequence of tokens given the vocabulary v
in this work we focus on the exploration stage therefore conversely to previous works we can choose the most sober form of reward if predicted human and otherwise
we show that a sparse reward is not a limitation if the sampling strategy is close to the modes of the distribution provided the initial solution is a good enough bootstrap which according to our experiments is the case
note that d is trained with samples from to avoid any mismatch with the generator training samples which would be problematic otherwise as pointed out in section

coldgans exploration the temperature t plays a major role in moderating exploration
indeed being a scaling factor applied to the generator outputs it directly denes the degree of diversity of the generated sequences
the default exploration is obtained by recursively sampling a sequence of tokens from the model distribution with t
the higher t the more random the sampled sequences regardless of the model s policy
conversely lower temperatures reduce the exploration with t ultimately equivalent to the argmax function
therefore we consider a distribution t with lower colder temperatures t
this allows to explore sequences composed of tokens less likely to be sampled from tail
note that for t whenever
in addition we consider a more sophisticated technique nucleus sampling
coldgansnucleus this decoding method has been shown to produce higher quality texts than previous sampling strategies including those temperature based
sampling from the nucleus of tokens containing the vast majority of the probability mass the approach dynamically truncates the unreliable tail of the probability distribution and hence is an instance of a cautious generative process
however with nucleus sampling many sequences get while invalidating the is
to avoid this we propose to use a mixture combining low temperatures and nucleus policies where is a hyper parameter nucleus rescaled for temperature as described in the previous paragraph
is the probability under nucleus and t the probability importance weight clipping the importance weights can become large causing instability
adapting from see paragraph
of their paper for more details we truncate the importance weights and add a correction term in the computation of e log e max c log where
in the rst term of eq
by clipping the importance weight the variance of the gradient estimate is bounded
the second term of the equation ensures that our estimate is unbiased by re sampling another sequence from the true policy
in our experiments we set c
note that contrary to off policy rl for which such a is clipping was proposed in our case clipping is very rare it only occurs for sequences whose probability from the generator is much higher than the one from the sampling distribution which is designed for sampling close to the mode of
however if this happens this clipping ensures that the corresponding gradient does not explode
memory replay in table we observed that the performance of the discriminators is lower when evaluated on samples generated from the previous checkpoint of the same model i
e
evaluated on past t
we connect this to the failure mode in gans observed by metz et al
where the generator and the discriminator oscillate during training rather than converging to a xed point
in lifelong learning literature it has been shown that of experience replay is sufcient to avoid catastrophic forgetting
inspired by this work we construct a memory buffer which contains samples generated in the last k training steps and replace of the discriminator training examples with samples from the buffer
this allows the discriminator to remain accurate on the samples from the previous state of the generator hence preventing such failure loop during training
experiments due to the computational cost of large parameters we used small m parameters
for all our experiments we used the validation sets for hyperparameter selection
in more detail we evaluated our approach with several learning reporting results for a value of
from the best performing coldgan conguration we perform ablations to assess the impact of memory replay and importance weight clipping
finally we experimented with bart instead of

unconditional language generation most previous works for language gans have been evaluated on unconditional language generation benchmarks
in this task no input is provided and the goal is to generate both meaningful and diverse texts
consistently with we measure these two aspects using respectively bleu and bleu metrics
the to obtain a ner comparison between models caccia et al
proposed to draw the curve of negative bleu vs self bleu by sampling with various temperatures at inference
this allows to measure the trade off between quality and diversity
following we used the news dataset
we report coldgan s results in figure left
notice that
has comparable performance to large but with fewer parameters
in
com deepmind deepmind research tree master scratchgan
statmt
org figure results on the emnlp news dataset for all metrics lower is better
scores for previous works are taken from
figure relative gains obtained with coldgan s over mle grouped by ground truth sequence length on qg
table results on question generation qg and abstractive summarization summ
tasks
qg squad summ
cnn dm params rouge l semqg bertsumabs unilm pegasus large mle small mle gan t coldgan t
coldgannucleus t
coldgannucleus t
coldgannucleus t

memory replay is weight clipping bart mle coldgannucleus t

m m m m m m m m m m m m m


























































previous works did not use self supervised pretrained models while we did with this explains the improvement of our mle baseline over theirs mle scratchgan
as one can not directly compare our performances with those reported from previous works we study the performance variations from the corresponding mle baseline
consistently with previous works we observe that the model under the default exploration i
e
gant performs strictly worse than mle
as a baseline we experimented coldgant where during the training the temperature is randomly sampled between and for each sequence
while it performs better than gant it still does not compare favorably w

t
mle
finally both coldgant
and coldgannucleus obtain better results than mle for the entire curve
to our knowledge this is the rst time that mle falls short w

t
gan based approaches for this task

conditional language generation we evaluate coldgan s on two popular tasks where text inputs are given for conditioning the generation namely question generation and text summarization
these are highly competitive benchmarks with recent state of the art results achieved by mle based on pre trained ers
answer aware question generation qg is the task wherein given a text and a target answer the goal is to generate a relevant question
following previous works we used the squad dataset
automatic summarization aims to produce concise and uent summaries given a longer text
we used the popular cnn dm dataset a corpus containing news articles and the corresponding abstractive summaries
for conditional text generation tasks output sequences are commonly evaluated using bleu for e

machine translation question generation or rouge for e

summarization metrics
in contrast to the unconditioned scenario the diversity is linked to the variety of the inputs and it is common practice to decode through beam search at inference













this work small


previous work mle truth






gain for coldgannucleus table human evaluation on qg
coldgan corresponds to bart trained with coldgannucleus t


tailed t test results are reported for each model compared to human


fluency relevance answerability human bart mle coldgan








figure probability that the generated text is human according to d on cnn dm
results for both tasks we used data and evaluation metrics released by dong et al

the results shown in table are consistent across the two tasks again we observe that exploring under the default temperature yields to poor performances while coldgan s compare favorably to mle
the best performance is achieved with the experiment emphasizing the coldgannucleus exploration the most with
and t

over independent training runs we also observed very stable results for this model with a standard deviation of the average lower than
on the test set
finally we applied this last coldgan s setup to bart achieving a new state of the art on both qg with
and summarization with
rouge l
mitigating the exposure bias in figure we report the relative gain obtained in terms of for small for the best conguration i
e
coldgannucleus
w

t
the corresponding mle baseline
the axis gives the length of considered ground truth target sequences
we observe that the longer the target sequence the more the coldgan outperforms mle
this might indicate that coldgan s can successfully mitigate exposure bias
human evaluation as discussed in section automatic metrics are known to suffer from key limitations
therefore we additionally conducted a human evaluation on the qg task
three professional english speakers were asked to judge on a likert scale to what extent the generated questions were well posed and natural fluency relevant to their context relevance and answerable by looking at their context and answer answerability
the results in table show surprisingly both mle bart and coldgan outperform the ground truth for fluency
a similar result was reported by yoon et al
refer to table in their paper
a plausible explanation is that humans are more inclined to use informal language and make grammar mistakes
for instance the human question about how many yellow cabs operate in new york sounds slightly less formal than the one generated by coldgan how many yellow taxicabs are in manhattan
compared to mle coldgan enables to signicantly improve in term of uency while remaining competitive on other metrics consistently with our experiments on exposure bias
adversarial training curves figure shows the evolution during training and for different setups of the probability of the generated text to be human according to the discriminator
consistently with table coldgannucleus appears to be the most adverse to the discriminator
conversely the regular gan t is less and less adversarial and comparatively more perturbed
conclusion we proposed coldgan s a novel approach able to tame the exploration in language gans allowing to obtain performance improvements on both conditional and unconditional text generation w

to mle based training
our proposed is method makes it compatible with advanced sampling methods such as nucleus or other future decoding methods
in the future we plan to combine coldgan s with orthogonal approaches proposed by previous works such as denser rewards

com microsoft unilm tree master unilm







probability to be




broader impact fluent and reliable natural language generation can have signicant societal impacts
on the one hand we envision several applications benecial for business research or education from automatic summarization of news papers or books to efcient information access from automatic and alized student evaluation tests trough question generation to responsive conversational interfaces
on the other hand malicious actors can use the same technology to build tools detrimental to society e

for creation and propagation of misleading fake news as discussed in impersonation and deceit
nonetheless keeping this research open and under public scrutiny is arguably one of the best ways to defend against such actors
implementation details all models are implemented in pytext
we used a single rtx ti gpu
all our experiments were conducted with small million parameters for both the generator and the discriminator these were rst trained on the corresponding task with mle as in
while small underperforms its larger version the latter has billion parameters
however bart performs as well with only m parameters
hence for each task we chose to train bart following the same dure with the best set of hyper parameters found with small i
e
coldgannucleus t


for and bart in conditional text generation we applied at inference beam search with for and for bart as recommended
one epoch to train coldgan takes hours with and hours with bart
references ahmed aly kushal lakhotia shicong zhao mrinal mohit barlas oguz abhinav arora sonal gupta christopher dewan stef nelson lindall and rushin shah

pytext a seamless path from nlp research to production
arxiv preprint

samy bengio oriol vinyals navdeep jaitly and noam shazeer

scheduled sampling for sequence prediction with recurrent neural networks
in advances in neural information processing systems pages
andrew brock jeff donahue and karen simonyan

large scale gan training for high delity natural image synthesis
arxiv preprint

sbastien bubeck eric price and ilya razenshteyn

adversarial examples from tational constraints
arxiv preprint

massimo caccia lucas caccia william fedus hugo larochelle joelle pineau and rent charlin

language gans falling short
in international conference on learning representations
asli celikyilmaz antoine bosselut xiaodong he and yejin choi

deep communicating agents for abstractive summarization
in proceedings of the conference of the north ican chapter of the association for computational linguistics human language technologies volume long papers pages
tong che yanran li ruixiang zhang r devon hjelm wenjie li yangqiu song and yoshua bengio

maximum likelihood augmented discrete generative adversarial networks
arxiv preprint

nina dethlefs and heriberto cuayhuitl

hierarchical reinforcement learning for tive text generation
in proceedings of the international natural language generation conference pages
association for computational linguistics
li dong nan yang wenhui wang furu wei xiaodong liu yu wang jianfeng gao ming zhou and hsiao wuen hon

unied language model pre training for natural language understanding and generation
in advances in neural information processing systems pages

com google research text to text transfer transformer xinya du junru shao and claire cardie

learning to ask neural question generation for reading comprehension
in proceedings of the annual meeting of the association for computational linguistics volume long papers pages
william fedus ian goodfellow and andrew m
dai

maskgan better text generation via filling in the
in international conference on learning representations
jakob foerster nantas nardelli gregory farquhar triantafyllos afouras philip hs torr pushmeet kohli and shimon whiteson

stabilising experience replay for deep agent reinforcement learning
in proceedings of the international conference on machine learning volume pages
jmlr
org
justin gilmer luke metz fartash faghri samuel s schoenholz maithra raghu martin wattenberg and ian goodfellow

adversarial spheres
ian goodfellow jean pouget abadie mehdi mirza bing xu david warde farley sherjil ozair aaron courville and yoshua bengio

generative adversarial nets
in advances in neural information processing systems pages
jiaxian guo sidi lu han cai weinan zhang yong yu and jun wang

long text generation via adversarial training with leaked information
in thirty second aaai conference on articial intelligence
geoffrey hinton oriol vinyals and jeff dean

distilling the knowledge in a neural network
arxiv preprint

ari holtzman jan buys li du maxwell forbes and yejin choi

the curious case of neural text degeneration
arxiv preprint

jens kober and jan r peters

policy search for motor primitives in robotics
in advances in neural information processing systems pages
mike lewis yinhan liu naman goyal marjan ghazvininejad abdelrahman mohamed omer levy ves stoyanov and luke zettlemoyer

bart denoising sequence to sequence pre training for natural language generation translation and comprehension
arxiv preprint

jiwei li will monroe tianlin shi sbastien jean alan ritter and dan jurafsky

adversarial learning for neural dialogue generation
in proceedings of the conference on empirical methods in natural language processing pages
kevin lin dianqi li xiaodong he zhengyou zhang and ming ting sun

adversarial ranking for language generation
in advances in neural information processing systems pages
yang liu and mirella lapata

text summarization with pretrained encoders
in ings of the conference on empirical methods in natural language processing and the international joint conference on natural language processing emnlp ijcnlp pages
cyprien de masson dautume shakir mohamed mihaela rosca and jack rae

training language gans from scratch
in advances in neural information processing systems pages
cyprien de masson dautume sebastian ruder lingpeng kong and dani yogatama

episodic memory in lifelong language learning
in advances in neural information processing systems pages
luke metz ben poole david pfau and jascha sohl dickstein

unrolled generative adversarial networks
arxiv preprint

ramesh nallapati bowen zhou caglar gulcehre bing xiang al

abstractive text summarization using sequence to sequence rnns and beyond
arxiv preprint

renato negrinho matthew gormley and geoffrey j gordon

learning beam search policies via imitation learning
in advances in neural information processing systems pages
jekaterina novikova ondrej duek amanda cercas curry and verena rieser

why we need new evaluation metrics for nlg
in proceedings of the conference on pirical methods in natural language processing pages copenhagen denmark
association for computational linguistics
kishore papineni salim roukos todd ward and wei jing zhu

bleu a method for automatic evaluation of machine translation
in proceedings of the annual meeting on association for computational linguistics pages
association for computational linguistics
romain paulus caiming xiong and richard socher

a deep reinforced model for abstractive summarization
arxiv preprint

alec radford jeffrey wu rewon child david luan dario amodei and ilya sutskever

language models are unsupervised multitask learners
openai blog
colin raffel noam shazeer adam roberts katherine lee sharan narang michael matena yanqi zhou wei li and peter j liu

exploring the limits of transfer learning with a unied text to text transformer
arxiv preprint

pranav rajpurkar jian zhang konstantin lopyrev and percy liang

squad in proceedings of the conference on questions for machine comprehension of text
empirical methods in natural language processing pages
marcaurelio ranzato sumit chopra michael auli and wojciech zaremba

sequence level training with recurrent neural networks
arxiv preprint

thomas rckstie martin felder and jrgen schmidhuber

state dependent ration for policy gradient methods
in joint european conference on machine learning and knowledge discovery in databases pages
springer
john schulman sergey levine pieter abbeel michael jordan and philipp moritz

trust region policy optimization
in international conference on machine learning pages
john schulman filip wolski prafulla dhariwal alec radford and oleg klimov

mal policy optimization algorithms
arxiv preprint

thomas scialom paul alexis dray sylvain lamprier benjamin piwowarski and jacopo staiano

discriminative adversarial search for abstractive summarization
arxiv preprint

stanislau semeniuta aliaksei severyn and sylvain gelly

on accurate evaluation of gans for language generation
arxiv preprint

elior sulem omri abend and ari rappoport

bleu is not suitable for the evaluation of text simplication
in proceedings of the conference on empirical methods in natural language processing pages
ilya sutskever oriol vinyals and quoc v le

sequence to sequence learning with neural networks
in advances in neural information processing systems pages
richard s sutton and andrew g barto

reinforcement learning an introduction
mit press
guy tevet gavriel habib vered shwartz and jonathan berant

evaluating text gans as language models
arxiv preprint

philip thomas and emma brunskill

data efcient off policy policy evaluation for reinforcement learning
in international conference on machine learning pages
ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan n gomez ukasz kaiser and illia polosukhin

attention is all you need
in advances in neural information processing systems pages
alex wang yada pruksachatkun nikita nangia amanpreet singh julian michael felix hill omer levy and samuel bowman

superglue a stickier benchmark for general purpose language understanding systems
in advances in neural information processing systems pages
alex wang amanpreet singh julian michael felix hill omer levy and samuel bowman

glue a multi task benchmark and analysis platform for natural language understanding
in proceedings of the emnlp workshop blackboxnlp analyzing and interpreting neural networks for nlp pages brussels belgium
association for computational linguistics
ziyu wang victor bapst nicolas heess volodymyr mnih remi munos koray kavukcuoglu and nando de freitas

sample efcient actor critic with experience replay
sean welleck ilia kulikov stephen roller emily dinan kyunghyun cho and jason weston

neural text generation with unlikelihood training
arxiv preprint

ronald j williams

simple statistical gradient following algorithms for connectionist reinforcement learning
machine learning
ronald j williams and david zipser

a learning algorithm for continually running fully recurrent neural networks
neural computation
wonjin yoon yoon sun yeo minbyul jeong bong jun yi and jaewoo kang

learning by semantic similarity makes abstractive summarization better
arxiv preprint

lantao yu weinan zhang jun wang and yong yu seqgan

sequence generative adversarial nets with policy gradient
arxiv e prints page
arxiv preprint

rowan zellers ari holtzman hannah rashkin yonatan bisk ali farhadi franziska roesner and yejin choi

defending against neural fake news
in advances in neural information processing systems pages
jingqing zhang yao zhao mohammad saleh and peter j liu

pegasus pre training with extracted gap sentences for abstractive summarization
arxiv preprint

shiyue zhang and mohit bansal

addressing semantic drift in question generation for semi supervised question answering
arxiv preprint

yizhe zhang zhe gan kai fan zhi chen ricardo henao dinghan shen and lawrence carin

adversarial feature matching for text generation
in proceedings of the international conference on machine learning volume pages
jmlr
org
qingyu zhou nan yang furu wei chuanqi tan hangbo bao and ming zhou

neural question generation from text a preliminary study
in national ccf conference on natural language processing and chinese computing pages
springer
wangchunshu zhou tao ge ke xu furu wei and ming zhou

self adversarial learning with comparative discrimination for text generation
in international conference on learning representations
yaoming zhu sidi lu lei zheng jiaxian guo weinan zhang jun wang and yong yu

texygen a benchmarking platform for text generation models
in the international acm sigir conference on research development in information retrieval pages
daniel m ziegler nisan stiennon jeffrey wu tom b brown alec radford dario amodei paul christiano and geoffrey irving

fine tuning language models from human preferences
arxiv preprint


