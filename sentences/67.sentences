abstractive text summarization using sequence to sequence rnns and beyond ramesh nallapati ibm watson
ibm
com bowen zhou ibm watson
ibm
com cicero dos santos ibm watson
ibm
com aglar g ulehre universit de montral
umontreal
ca bing xiang ibm watson
ibm
com abstract in this work we model abstractive text summarization using attentional decoder recurrent neural networks and show that they achieve state of the art formance on two different corpora
we propose several novel models that address critical problems in summarization that are not adequately modeled by the basic architecture such as modeling key words capturing the hierarchy of sentence word structure and emitting words that are rare or unseen at training time
our work shows that many of our proposed models contribute to further improvement in performance
we also propose a new dataset consisting of multi sentence maries and establish performance marks for further research
introduction abstractive text summarization is the task of erating a headline or a short summary consisting of a few sentences that captures the salient ideas of an article or a passage
we use the adjective stractive to denote a summary that is not a mere selection of a few existing passages or sentences extracted from the source but a compressed phrasing of the main contents of the document potentially using vocabulary unseen in the source document
this task can also be naturally cast as ping an input sequence of words in a source ument to a target sequence of words called mary
in the recent past deep learning based els that map an input sequence into another put sequence called sequence to sequence els have been successful in many problems such as machine translation bahdanau et al
speech recognition bahdanau et al
and video captioning venugopalan et al

in the framework of sequence to sequence models a very relevant model to our task is the tional recurrent neural network rnn decoder model proposed in bahdanau et al
which has produced state of the art formance in machine translation mt which is also a natural language task
despite the similarities abstractive tion is a very different problem from mt
unlike in mt the target summary is typically very short and does not depend very much on the length of the source document in summarization
tionally a key challenge in summarization is to timally compress the original document in a lossy manner such that the key concepts in the original document are preserved whereas in mt the lation is expected to be loss less
in translation there is a strong notion of almost one to one level alignment between source and target but in summarization it is less obvious
we make the following main contributions in this work i we apply the off the shelf tional encoder decoder rnn that was originally developed for machine translation to tion and show that it already outperforms of the art systems on two different english pora
motivated by concrete problems in marization that are not sufciently addressed by the machine translation based model we propose novel models and show that they provide tional improvement in performance
we pose a new dataset for the task of abstractive marization of a document into multiple sentences and establish benchmarks
the rest of the paper is organized as follows
in section we describe each specic problem in abstractive summarization that we aim to solve and present a novel model that addresses it
g u a l c
s c v
v i x r a tion contextualizes our models with respect to closely related work on the topic of abstractive text summarization
we present the results of our periments on three different data sets in section
we also present some qualitative analysis of the output from our models in section before cluding the paper with remarks on our future rection in section
models in this section we rst describe the basic decoder rnn that serves as our baseline and then propose several novel models for summarization each addressing a specic weakness in the line

encoder decoder rnn with attention and large vocabulary trick our baseline model corresponds to the neural chine translation model used in bahdanau et al

the encoder consists of a bidirectional gru rnn chung et al
while the decoder consists of a uni directional gru rnn with the same hidden state size as that of the encoder and an attention mechanism over the source hidden states and a soft max layer over target in the interest of space lary to generate words
we refer the reader to the original paper for a tailed treatment of this model
in addition to the basic model we also adapted to the tion problem the large vocabulary trick lvt described in jean et al

in our approach the decoder vocabulary of each mini batch is stricted to words in the source documents of that batch
in addition the most frequent words in the target dictionary are added until the vocabulary reaches a xed size
the aim of this technique is to reduce the size of the soft max layer of the decoder which is the main computational neck
in addition this technique also speeds up convergence by focusing the modeling effort only on the words that are essential to a given example
this technique is particularly well suited to marization since a large proportion of the words in the summary come from the source document in any case

capturing keywords using feature rich encoder in summarization one of the key challenges is to identify the key concepts and key entities in the document around which the story revolves
in order to accomplish this goal we may need to go beyond the word embeddings based tation of the input document and capture tional linguistic features such as parts of speech tags named entity tags and tf and idf tics of the words
we therefore create additional look up based embedding matrices for the ulary of each tag type similar to the embeddings for words
for continuous features such as tf and idf we convert them into categorical values by discretizing them into a xed number of bins and use one hot representations to indicate the bin number they fall into
this allows us to map them into an embeddings matrix like any other tag type
finally for each word in the source document we simply look up its embeddings from all of its sociated tags and concatenate them into a single long vector as shown in fig

on the target side we continue to use only word based embeddings as the representation
figure feature rich encoder we use one embedding vector each for pos ner tags and discretized tf and idf values which are concatenated together with word based beddings as input to the encoder

modeling rare unseen words using switching generator pointer often times in summarization the keywords or named entities in a test document that are central to the summary may actually be unseen or rare with respect to training data
since the vocabulary of the decoder is xed at training time it can not emit these unseen words
instead a most common way of handling these out of vocabulary oov words is to emit an unk token as a placeholder
however this does not result in legible summaries
in summarization an intuitive way to handle such oov words is to simply point to their location in the source document instead
we model this hidden stateinput layeroutput layerwposnertfidfwposnertfidfwposnertfidfwposnertfidfattention mechanismencoderdecoder tion using our novel switching decoder pointer chitecture which is graphically represented in ure
in this model the decoder is equipped with a switch that decides between using the tor or a pointer at every time step
if the switch is turned on the decoder produces a word from its target vocabulary in the normal fashion
however if the switch is turned off the decoder instead erates a pointer to one of the word positions in the source
the word at the pointer location is then copied into the summary
the switch is modeled as a sigmoid activation function over a linear layer based on the entire available context at each step as shown below
p ws hhi ws ws cci e ws h ws where p is the probability of the switch turning on at the ith time step of the decoder hi is the hidden state is the embedding tor of the emission from the previous time step ci is the attention weighted context vector and c bs and vs are the switch ws ters
we use attention distribution over word tions in the document as the distribution to sample the pointer from
i j wa p a j ba wa p a pi arg max wa i for


nd
e c hd j in the above equation pi is the pointer value at ith word position in the summary sampled from the attention distribution pa i over the document word positions


nd where p a i j is the probability of the ith time step in the decoder pointing to the jth position in the document and hd j is the encoder s hidden state at position j
at training time we provide the model with plicit pointer information whenever the summary word does not exist in the target vocabulary
when the oov word in summary occurs in multiple ument positions we break the tie in favor of its rst occurrence
at training time we optimize the conditional log likelihood shown below with ditional regularization penalties
log p i gi gi p where and are the summary and document words respectively gi is an indicator function that is set to whenever the word at position i in the summary is oov with respect to the decoder cabulary
at test time the model decides ically at each time step whether to generate or to point based on the estimated switch probability p
we simply use the arg max of the rior probability of generation or pointing to ate the best output at each time step
the pointer mechanism may be more robust in handling rare words because it uses the encoder s hidden state representation of rare words to decide which word from the document to point to
since the hidden state depends on the entire context of the word the model is able to accurately point to unseen words although they do not appear in the target vocabulary
figure switching generator pointer model when the switch shows g the traditional generator consisting of the softmax layer is used to produce a word and when it shows p the pointer network is activated to copy the word from one of the source document positions
when the pointer is activated the embedding from the source is used as input for the next time step as shown by the arrow from the encoder to the decoder at the bottom

capturing hierarchical document structure with hierarchical attention in datasets where the source document is very long in addition to identifying the keywords in the document it is also important to identify the key sentences from which the summary can be drawn
this model aims to capture this notion of two levels of importance using two bi directional when the word does not exist in the source lary the pointer model may still be able to identify the correct position of the word in the source since it takes into account the contextual representation of the corresponding unk ken encoded by the rnn
once the position is known the corresponding token from the source document can be played in the summary even when it is not part of the training vocabulary either on the source side or the target side
hidden stateencoderdecoderinput layeroutput layergpggg rnns on the source side one at the word level and the other at the sentence level
the attention mechanism operates at both levels simultaneously
the word level attention is further re weighted by the corresponding sentence level attention and normalized as shown below p p a a p a s a s where p a is the word level attention weight at jth position of the source document and is the id of the sentence at jth word position p a s l is the sentence level attention weight for the lth sentence in the source nd is the number of words in the source document and p is the re scaled attention at the jth word position
the re scaled attention is then used to compute the weighted context vector that goes as input to the hidden state of the decoder
further we also catenate additional positional embeddings to the hidden state of the sentence level rnn to model positional importance of sentences in the ment
this architecture therefore models key tences as well as keywords within those sentences jointly
a graphical representation of this model is displayed in figure
figure hierarchical encoder with hierarchical attention the attention weights at the word level represented by the dashed arrows are re scaled by the corresponding level attention weights represented by the dotted arrows
the dashed boxes at the bottom of the top layer rnn resent sentence level positional embeddings concatenated to the corresponding hidden states
related work a vast majority of past work in summarization has been extractive which consists of ing key sentences or passages in the source ument and reproducing them as summary neto et al
erkan and radev wong et al
filippova and altun colmenares et al
litvak and last k
riedhammer and hakkani tur ricardo ribeiro
humans on the other hand tend to paraphrase the original story in their own words
as such man summaries are abstractive in nature and dom consist of reproduction of original sentences from the document
the task of abstractive marization has been standardized using the and competitions
the data for these tasks consists of news stories from various topics with multiple reference summaries per story generated by humans
the best performing system on the task called topiary zajic et al
used a combination of linguistically motivated compression techniques and an pervised topic detection algorithm that appends keywords extracted from the article onto the pressed output
some of the other notable work in the task of abstractive summarization includes ing traditional phrase table based machine tion approaches banko et al
compression using weighted tree transformation rules cohn and lapata and quasi synchronous mar approaches woodsend et al

with the emergence of deep learning as a viable alternative for many nlp tasks collobert et al
researchers have started considering this framework as an attractive fully data driven in rush et native to abstractive summarization
al
the authors use convolutional models to encode the source and a context sensitive tentional feed forward neural network to generate the summary producing state of the art results on gigaword and duc datasets
in an extension to this work chopra et al
used a similar volutional model for the encoder but replaced the decoder with an rnn producing further ment in performance on both datasets
in another paper that is closely related to our work hu et al
introduce a large dataset for chinese short text summarization
they show promising results on their chinese dataset using an encoder decoder rnn but do not report iments on english corpora
in another very recent work cheng and lapata used rnn based encoder decoder for tractive summarization of documents
this model is not directly comparable to ours since their
nist
hidden stateword layerencoderdecoderinput layeroutput layerhidden statesentence layer eos sentence level attentionword level attention framework is extractive while ours and that of rush et al
hu et al
and chopra et al
is abstractive
our work starts with the same framework as hu et al
where we use rnns for both source and target but we go beyond the standard architecture and propose novel models that dress critical problems in summarization
we also note that this work is an extended version of lapati et al

in addition to performing more extensive experiments compared to that work we also propose a novel dataset for document rization on which we establish benchmark bers too
below we analyze the similarities and ences of our proposed models with related work on summarization
feature rich encoder sec

linguistic tures such as pos tags and named entities as well as tf and idf information were used in many extractive approaches to summarization wong et al
but they are novel in the context of deep learning approaches for abstractive rization to the best of our knowledge
switching generator pointer model sec

this model combines extractive and abstractive approaches to summarization in a single end end framework
rush et al
also used a combination of extractive and abstractive proaches but their extractive model is a rate log linear classier with handcrafted features
pointer networks vinyals et al
have also been used earlier for the problem of rare words in the context of machine translation luong et al
but the novel addition of switch in our model allows it to strike a balance between when to be faithful to the original source e

for named entities and oov and when it is allowed to be ative
we believe such a process arguably how human produces summaries
for a more detailed treatment of this model and experiments on multiple tasks please refer to the parallel work published by some of the authors of this work gulcehre et al

hierarchical attention model sec

viously proposed hierarchical encoder decoder models use attention only at sentence level li et al

the novelty of our approach lies in joint modeling of attention at both sentence and word levels where the word level attention is further uenced by sentence level attention thus ing the notion of important sentences and tant words within those sentences
concatenation of positional embeddings with the hidden state at sentence level is also new
experiments and results
gigaword corpus in this series of we used the tated gigaword corpus as described in rush et al

we used the scripts made available by the authors of this to preprocess the data which resulted in about
m training examples
the script also produces about k validation and test examples but we created a randomly pled subset of examples each for validation and testing purposes on which we report our formance
further we also acquired the exact test sample used in rush et al
to make precise comparison of our models with theirs
we also made small modications to the script to extract not only the tokenized words but also generated parts of speech and named entity tags
training for all the models we discuss below we used dimensional vectors mikolov et al
trained on the same corpus to ize the model embeddings but we allowed them to be updated during training
the hidden state mension of the encoder and decoder was xed at in all our experiments
when we used only the rst sentence of the document as the source as done in rush et al
the encoder lary size was and that of the decoder stood at
we used adadelta zeiler for training with an initial learning rate of

we used a batch size of and randomly shufed the training data at every epoch while sorting every batches according to their lengths to speed up training
we did not use any dropout or ization but applied gradient clipping
we used early stopping based on the validation set and used the best model on the validation set to report all test performance numbers
for all our models we employ the large vocabulary trick where we strict the decoder vocabulary size to cause it cuts down the training time per epoch by nearly three times and helps this and all used kyunghyun cho s code
com kyunghyuncho material as the ing point

com facebook namas values improved performance only marginally but at the cost of much slower training
quent models converge in only of the epochs needed for the model based on full ulary
decoding at decode time we used beam search of size to generate the summary and limited the size of summary to a maximum of words since this is the maximum size we noticed in the pled validation set
we found that the average tem summary length from all our models
to
agrees very closely with that of the ground truth on the validation set about
words out any specic tuning
computational costs we trained all our els on a single tesla gpu
most models took about hours per epoch on an average except the hierarchical attention model which took hours per epoch
all models typically converged within epochs using our early stopping criterion based on the validation cost
the wall clock training time until convergence therefore varies between days depending on the model
generating summaries at test time is reasonably fast with a throughput of about summaries per second on a single gpu using a batch size of
evaluation metrics similar to nallapati et al
and chopra et al
we use the full length variant of to evaluate our tem
although limited length recall was the ferred metric for most previous work one of its disadvantages is choosing the length limit which varies from to corpus making it difcult for researchers to compare performances
length recall on the other hand does not impose a length restriction but unfairly favors longer maries
full length solves this problem since it can penalize longer summaries while not ing a specic length restriction
in addition we also report the percentage of tokens in the system summary that occur in the source which we call src
copy rate in table
we describe all our experiments and results on the gigaword below
words this is the baseline attentional encoder decoder model with the large vocabulary trick
this model is trained only on the rst tence from the source document as done in rush et al

words this model is identical to the model above except for the fact that it is trained
berouge
com pages default
aspx on the rst two sentences from the source
on this corpus adding the additional sentence in the source does seem to aid performance as shown in table
we also tried adding more sentences but the performance dropped which is probably because the latter sentences in this corpus are not pertinent to the summary
words hieratt since we used two tences from source document we trained the erarchical attention model proposed in sec

as shown in table this model improves mance compared to its atter counterpart by ing the relative importance of the rst two tences automatically
feats here we still train on the rst two sentences but we exploit the parts of speech and named entity tags in the annotated gigaword as well as tf idf values to augment the input embeddings on the source side as described in sec

in total our embedding vector grew from the original to and produced mental gains compared to its counterpart as shown in table demonstrating the utility of syntax based features in this task
feats ptr this is the switching ator pointer model described in sec

but in addition we also use feature rich embeddings on the document side as in the above model
our periments indicate that the new model is able to achieve the best performance on our test set by all three rouge variants as shown in table
comparison with state of the art we pared the performance of our model words with state of the art models on the sample created by rush et al
as displayed in the bottom part of table
we also trained another system which we call words which has a larger lvt vocabulary size of but also has much larger source and target vocabularies of k and k respectively
the reason we did not evaluate our best dation models here is that this test set consisted of only sentence from the source document and did not include nlp annotations which are needed in our best models
the table shows that despite this fact our model outperforms the model of rush et al
with statistical signicance
in addition our models exhibit better abstractive ability as shown by the src
copy rate metric in the last column of the table
further our larger model words outperforms the state of the art model of chopra et al
with statistically signicant improvement on
we believe the bidirectional rnn we used to model the source captures richer contextual mation of every word than the bag of embeddings representation used by rush et al
and chopra et al
in their convolutional tional encoders which might explain our superior performance
further explicit modeling of portant information such as multiple source tences word level linguistic features using the switch mechanism to point to source words when needed and hierarchical attention solve specic problems in summarization each boosting mance incrementally

duc corpus the duc comes in two parts the corpus consisting of document summary pairs and the corpus consisting of pairs
since these corpora are too small to train large neural networks on rush et al
trained their models on the gigaword corpus but combined it with an additional log linear extractive rization model with handcrafted features that is trained on the duc corpus
they call the original neural attention model the abs model and the combined model
chopra et al
also report the performance of their elman model on this corpus and is the current state of the art since it outperforms all previously published baselines including non neural network based extractive and abstractive systems as sured by the ofcial duc metric of recall at bytes
in these experiments we use the same ric to evaluate our models too but we omit ing numbers from other systems in the interest of space
in our work we simply run the models trained on gigaword as they are without tuning them on the duc validation set
the only change we made to the decoder is to suppress the model from emitting the end of summary tag and force it to emit exactly words for every summary since the ofcial evaluation on this corpus is based on limited length rouge recall
on this corpus too since we have only a single sentence from source and no nlp annotations we ran just the models words and words
the performance of this model on the test set
nist
gov tasks
html is compared with abs and models elman from chopra et al
as well as iary the top performing system on in table
we note our best model words outperforms ras elman on two of the three ants of rouge while being competitive on
model topiary abs ras elman words words rouge l

















table evaluation of our models using the limited length rouge recall at bytes on duc validation and test sets
our best model although trained exclusively on the gaword corpus consistently outperforms the model which is tuned on the validation corpus in tion to being trained on the gigaword corpus

cnn daily mail corpus the existing abstractive text summarization pora including gigaword and duc consist of only one sentence in each summary
in this section we present a new corpus that comprises sentence summaries
to produce this corpus we modify an existing corpus that has been used for the task of passage based question answering in this work the hermann et al

thors used the human generated abstractive mary bullets from new stories in cnn and daily mail websites as questions with one of the ties hidden and stories as the corresponding sages from which the system is expected to swer the ll in the blank question
the authors leased the scripts that crawl extract and generate pairs of passages and questions from these sites
with a simple modication of the script we restored all the summary bullets of each story in the original order to obtain a multi sentence mary where each bullet is treated as a sentence
in all this corpus has training pairs validation pairs and test pairs as dened by their scripts
the source documents in the ing set have words spanning
sentences on an average while the summaries consist of words and
sentences
the unique istics of this dataset such as long documents and ordered multi sentence summaries present esting challenges and we hope will attract future model name rouge l src
copy rate full length on our internal test set words words words hieratt feats feats ptr rush et al
words ras elman chopra et al
words


























full length on the test set used by rush et al







table performance comparison of various models
indicates statistical signicance of the corresponding model with respect to the baseline model on its dataset as given by the condence interval in the ofcial rouge script
we report statistical signicance only for the best performing models
src
copy rate for the reference data on our validation sample is
please refer to section for explanation of notation
model words words hieratt words temp att rouge l








table performance of various models on cnn daily mail test set using full length rouge metric
bold faced numbers indicate best performing system
researchers to build and test novel models on it
the dataset is released in two versions one consisting of actual entity names and the other in which entity occurrences are replaced with document specic integer ids beginning from
in the since the vocabulary size is smaller anonymized version we used it in all our iments below
we limited the source vocabulary size to k and the target vocabulary to k the source and target lengths to at most and words respectively
we used dimensional embeddings trained on this dataset as input and we xed the model hidden state size at
we also created explicit pointers in the ing data by matching only the anonymized ids between source and target on similar lines as we did for the oov words in gigaword
computational costs we used a single tesla gpu to train our models on this dataset as well
while the at models words and ptr took under hours per epoch the archical attention model was very expensive suming nearly
hours per epoch
convergence of all models is also slower on this dataset pared to gigaword taking nearly epochs for all models
thus the wall clock time for ing until convergence is about days for the at models but nearly days for the hierarchical tention model
decoding is also slower as well with a throughput of examples per second for at models and
examples per second for the hierarchical attention model when run on a single gpu with a batch size of
evaluation we evaluated our models using the full length rouge metric that we employed for the gigaword corpus but with one notable ence in both system and gold summaries we sidered each highlight to be a separate sentence
results results from the basic attention decoder as well as the hierarchical attention model are displayed in table
although this dataset is smaller and more complex than the gigaword pus it is interesting to note that the rouge bers are in the same range
however the archical attention model described in sec

outperforms the baseline attentional decoder only marginally
upon visual inspection of the system output we noticed that on this dataset both these models duced summaries that contain repetitive phrases or even repetitive sentences at times
since the summaries in this dataset involve multiple tences it is likely that the decoder forgets what part of the document was used in producing earlier highlights
to overcome this problem we used the temporal attention model of sankaran et al
that keeps track of past attentional weights of the decoder and expliticly discourages it from attending to the same parts of the document in ture time steps
the model works as shown by the this dataset we used the pyrouge script pypi
python
org pypi

that lows evaluation of each sentence as a separate unit
tional pre processing involves assigning each highlight to its own a tag in the system and gold xml les that go as input to the rouge evaluation script
similar evaluation was also done by cheng and lapata
source document wanted lm director must be eager to shoot footage of golden lassos and invisible jets
eos conrms that is leaving the upcoming movie the hollywood reporter rst broke the story
eos was announced as director of the movie in november
eos obtained a statement from that says given creative differences and have decided not to move forward with plans to develop and direct together
eos and are both owned by
eos the movie starring in the title role of the princess is still set for release on june
eos it s the rst theatrical movie centering around the most popular female superhero
eos will appear beforehand in v
due out march
eos in the meantime will need to nd someone new for the director s chair
eos ground truth summary is no longer set to direct the rst theatrical movie eos left the project over creative differences eos movie is currently set for words conrms that is leaving the upcoming movie eos and have decided not to move forward with plans to develop eos conrms that is leaving the upcoming movie words hieratt is leaving the upcoming movie eos the movie is still set for release on june eos is still set for release on june words temp att conrms that is leaving the upcoming movie eos the movie is the rst lm to around the most popular female actor eos will appear in due out march table comparison of gold truth summary with summaries from various systems
named entities and numbers are anonymized by the preprocessing script
the eos tags represent the boundary between two highlights
the temporal attention model words temp att solves the problem of repetitions in summary as exhibited by the models words and words hieratt by encouraging the attention model to focus on the uncovered portions of the document
following simple equations t k t t t where t is the unnormalized attention weights vector at the tth time step of the decoder
in other words the temporal attention model weights the attention weights at the current time step if the past attention weights are high on the same part of the document
using this strategy the temporal attention model improves performance signicantly over both the baseline model as well as the hierarchical attention model
we have also noticed that there are fewer repetitions of summay highlights duced by this model as shown in the example in table
these results although preliminary should serve as a good baseline for future researchers to compare their models against
qualitative analysis table presents a few high quality and poor ity output on the validation set from feats one of our best performing models
even when the model differs from the target summary its summaries tend to be very meaningful and evant a phenomenon not captured by word phrase matching evaluation metrics such as rouge
on the other hand the model sometimes prets the semantics of the text and generates a summary with a comical interpretation as shown in the poor quality examples in the table
clearly capturing the meaning of complex sentences mains a weakness of these models
our next example output presented in figure displays the sample output from the switching generator pointer model on the gigaword corpus
it is apparent from the examples that the model learns to use pointers very accurately not only for named entities but also for multi word phrases
despite its accuracy the performance ment of the overall model is not signicant
we believe the impact of this model may be more nounced in other settings with a heavier tail bution of rare words
we intend to carry out more experiments with this model in the future
on cnn daily mail data although our models are able to produce good quality multi sentence summaries we notice that the same sentence or good quality summary output s a man charged with the murder last year of a british packer confessed to the slaying on the night he was charged with her killing according to police evidence presented at a court hearing tuesday
ian douglas previte is charged with murdering caroline stuttle of yorkshire england t man charged with british backpacker s death confessed to crime police ofcer claims o man charged with murdering british backpacker fessed to murder s following are the leading scorers in the english premier league after saturday s matches alan shearer newcastle united james beattie
t leading scorers in english premier league o english premier league leading scorers s volume of transactions at the nigerian stock exchange has continued its decline since last week a nse ofcial said thursday
the latest statistics showed that a total of
million shares valued at
million naira about
million us dollars were traded on wednesday in deals
t transactions dip at nigerian stock exchange o transactions at nigerian stock exchange down poor quality summary output s broccoli and broccoli sprouts contain a chemical that kills the bacteria responsible for most stomach cancer say searchers conrming the dietary advice that moms have been handing out for years
in laboratory tests the chemical unk killed helicobacter pylori a bacteria that causes stomach ulcers and often fatal stomach cancers
t for release at unk mom was right broccoli is good for you say cancer researchers o broccoli sprouts contain deadly bacteria s norway delivered a diplomatic protest to russia on day after three norwegian sheries research expeditions were barred from russian waters
the norwegian research ships were to continue an annual program of charting sh resources shared by the two countries in the barents sea gion
t norway protests russia barring sheries research ships o norway grants diplomatic protest to russia s

morgan chase s ability to recover from a slew of recent losses rests largely in the hands of two men who are both looking to restore tarnished reputations and may be considered for the top job someday
geoffrey unk now the co head of j

morgan s investment bank left goldman sachs co
more than a decade ago after executives say he lost out in a bid to lead that rm
t executives to lead j

morgan chase on road to ery o

morgan chase may be considered for top job table examples of generated summaries from our best model on the validation set of gigaword
s source document t target summary o system output
although we displayed equal number of good quality and poor quality summaries in the table the good ones are far more prevalent than the poor ones
of the annual meeting on association for putational linguistics
cheng and jianpeng cheng and mirella lapata

neural summarization by extracting sentences and words
in proceedings of the nual meeting of the association for computational linguistics
cheng et al
jianpeng cheng li dong and long short term corr mirella lapata
memory networks for machine reading



chopra et al
sumit chopra michael auli and alexander m
rush

abstractive sentence summarization with attentive recurrent neural works
in hlt naacl
chung et al
junyoung chung aglar glehre kyunghyun cho and yoshua bengio

pirical evaluation of gated recurrent neural networks on sequence modeling
corr

cohn and trevor cohn and mirella ata

sentence compression beyond word tion
in proceedings of the international ference on computational linguistics volume pages
collobert et al
ronan collobert jason weston lon bottou michael karlen koray kavukcuoglu and pavel p
kuksa
guage processing almost from scratch
corr

natural
colmenares et al
carlos a
colmenares marina litvak amin mantrach and fabrizio silvestri

heads headline generation as sequence diction using an abstract feature rich space
in ceedings of the conference of the north ican chapter of the association for computational linguistics human language technologies pages
erkan and g
erkan and d
r
radev

lexrank graph based lexical centrality as salience in text summarization
journal of articial intelligence research

filippova and katja filippova and yasemin altun
overcoming the lack in of parallel data in sentence compression
the conference on empirical ceedings of methods in natural language processing pages
figure sample output from switching generator pointer networks
an arrow indicates that a pointer to the source sition was used to generate the corresponding summary word
phrase often gets repeated in the summary
we lieve models that incorporate intra attention such as cheng et al
can x this problem by couraging the model to remember the words it has already produced in the past
conclusion in this work we apply the attentional decoder for the task of abstractive summarization with very promising results outperforming of the art results signicantly on two different datasets
each of our proposed novel models dresses a specic problem in abstractive rization yielding further improvement in mance
we also propose a new dataset for sentence summarization and establish benchmark numbers on it
as part of our future work we plan to focus our efforts on this data and build more bust models for summaries consisting of multiple sentences
references bahdanau et al
dzmitry bahdanau kyunghyun cho and yoshua bengio

neural machine translation by jointly learning to align and translate
corr

bahdanau et al
dzmitry bahdanau jan chorowski dmitriy serdyuk philemon brakel and yoshua bengio

end to end based large vocabulary speech recognition
corr

gulcehre et al
caglar gulcehre sungjin ahn ramesh nallapati bowen zhou and yoshua in gio

pointing the unknown words
ceedings of the annual meeting of the tion for computational linguistics
banko et al
michele banko vibhu o
mittal and michael j witbrock

headline tion based on statistical translation
in proceedings hermann et al
karl moritz hermann toms kocisk edward grefenstette lasse espeholt will kay mustafa suleyman and phil blunsom

rush et al
alexander m
rush sumit chopra and jason weston

a neural attention model corr for abstractive sentence summarization


sankaran et al
b
sankaran h
mi y
onaizan and a
ittycheriah

temporal tion model for neural machine translation
arxiv e prints august
venugopalan et al
subhashini venugopalan marcus rohrbach jeff donahue raymond j
mooney trevor darrell and kate saenko

corr sequence to sequence video to text


vinyals et al
o
vinyals m
fortunato and n
jaitly

pointer networks
arxiv e prints june
wong et al
kam fai wong mingli wu and wenjie li

extractive summarization using in supervised and semi supervised learning
ceedings of the international conference on computational linguistics volume pages
wong et al
kam fai wong mingli wu and wenjie li

extractive summarization using in supervised and semi supervised learning
ceedings of the annual meeting of the tion for computational linguistics pages
woodsend et al
kristian woodsend yansong feng and mirella lapata

title generation with quasi synchronous grammar
in proceedings of the conference on empirical methods in ral language processing emnlp pages stroudsburg pa usa
association for putational linguistics
zajic et al
david zajic bonnie j
dorr and richard schwartz

bbn umd at in proceedings of the north american topiary
chapter of the association for computational guistics workshop on document understanding pages
matthew d
zeiler

adadelta corr an adaptive learning rate method


teaching machines to read and comprehend
corr

et al
baotian hu qingcai chen and fangze zhu

lcsts a large scale chinese short text summarization dataset
in proceedings of the conference on empirical methods in natural guage processing pages lisbon gal september
association for computational guistics
jean et al
sbastien jean kyunghyun cho roland memisevic and yoshua bengio

on using very large target vocabulary for neural chine translation
corr

k
riedhammer and hakkani b
favre k
riedhammer and d
hakkani tur

long story short a s global unsupervised models for keyphrase based meeting summarization
in speech communication pages
et al
jiwei li minh thang luong and dan
a hierarchical neural corr for paragraphs and documents
jurafsky
coder

litvak and m
litvak and m
last

for in coling pages graph based document summarization

extraction keyword luong et al
thang luong ilya sutskever quoc v
le oriol vinyals and wojciech zaremba

addressing the rare word problem in neural in proceedings of the machine translation
annual meeting of the association for tional linguistics and the international joint conference on natural language processing of the asian federation of natural language processing pages
mikolov et al
tomas mikolov ilya sutskever kai chen greg corrado and jeffrey dean

distributed representations of words and phrases and their compositionality
corr

nallapati et al
ramesh nallapati bing xiang
sequence to sequence iclr workshop and bowen zhou
rnns for text summarization


neto et al
joel larocca neto alex alves itas and celso a
a
kaestner

automatic text summarization using a machine learning proach
in proceedings of the brazilian posium on articial intelligence advances in cial intelligence pages
ricardo david martins de matos joco p
neto anatole gershman jaime carbonell cardo ribeiro lu s marujo

self ment for important passage retrieval
in national acm sigir conference on research and development in information retrieval pages

