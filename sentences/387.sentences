multichannel lstm cnn for telugu technical domain identication sunil gundapu language technologies research centre kcis iiit hyderabad telangana india sunil

iiit
ac
in radhika mamidi language technologies research centre kcis iiit hyderabad telangana india radhika

ac
in e f l c
s c v
v i x r a abstract with the instantaneous growth of text tion retrieving domain oriented information from the text data has a broad range of tions in information retrieval and natural guage processing
thematic keywords give a compressed representation of the text
usually domain identication plays a signicant role in machine translation text summarization question answering information extraction and sentiment analysis
in this paper we posed the multichannel lstm cnn ology for technical domain identication for telugu
this architecture was used and uated in the context of the icon shared task techdocation task h and our tem got
of the score on the test dataset and
on the validation set
introduction technical domain identication is the task of matically identifying and categorizing a set of beled text passages documents to their ing domain categories from a predened domain category set
the domain category set consists of category labels bio chemistry tion technology computer science management physics and other
these domains can be viewed as a set of text passages and test text data can be treated as a query to the system
domain cation has many applications like machine lation summarization question answering
this task would be the rst step for most stream applications i
e
machine translation
it decides the domain for text data and afterward machine translation can choose its resources as per the identied domain
majority of the research work in the area of text classication and domain identication has been done in english
there has been well below tribution for regional languages especially indian languages
telugu is one of india s old traditional languages and it is categorized as one of the vidian language family
according to the list there are about million native telugu speakers and it ranks sixteenth most spoken guages worldwide
we tried to identify the domain of telugu text data using various supervised machine learning and deep learning techniques in our work
our multichannel lstm cnn method outperforms the other methods on the provided dataset
this proach incorporates the advantages of cnn and self attention based bilstm into one model
the rest of the paper is structured as follows section explains some related works of domain identication section describes the dataset vided in the shared task section addresses the methodology applied in the task section presents the results and error analysis and nally section concludes the paper as well as possible future works
related work several methods for domain identication and text categorization have been done on indian languages and few of the works have been reported on the telugu language
in this section we survey some of the methodologies and approaches used to address domain identication and text categorization
murthy explains the automatic text gorization with special emphasis on telugu
in his research work supervised classication using the naive bayes classier has been applied to ugu news articles for text categorization
swamy et al
work on representing and categorizing indian language text documents using text mining techniques k nearest neighbour naive bayes and decision tree classier

ethnologue
com guides categorization of telugu text documents using language dependent and independent models posed by narala et al

durga and han introduced a model for document sication and text categorization
in their paper described a term frequency ontology based text categorization for telugu documents
combining lstm and cnn s robustness liu et al
posed attention based multichannel convolutional neural network for text classication
in their network bilstm encodes the history and future information of words and cnn capture relations between words
dataset description we used the dataset provided by the organizers of task h of techdocation for training the models
the data for the task consists of text documents for training for validation for testing
for hyperparameter tuning we used the validation set provided by the organizers
the statistics of the dataset are shown in table
and the amount of texts for the dataset can be seen in figure
train data validation data cse phy com tech bio tech mgnt other total table dataset statistics figure number of samples per class proposed approach
data preprocessing the text passages have been originally provided in the telugu script with the corresponding domain tags
the text documents have some noise so fore passing the text to the training stage they are preprocessed using the following procedure acronym mapping dictionary we created an acronym mapping dictionary
expanded the english acronyms using the acronym ping dictionary
find language words sometimes english words are co located with telugu words in the passage
we nd the index of those words to translate into telugu
translate english words translate the glish words into the telugu language which are identied in the rst stage of ing
google s translation was used for this purpose
hindi sentence translation we can serve a few hindi sentences in the dataset
we translated those sentences into telugu using google translation tool
noise removal removed the unnecessary tokens punctuation marks non utf format tokens and single length english tokens from the text data

supervise machine learning algorithms to build the nest system for domain identication we started with supervised machine learning niques then moved to deep learning models
svm multilayer perceptron linear classier gradient boosting methods performed very well on the given training dataset
these supervised models trained on the word level n gram level and character level tf idf vector representations

multichannel lstm cnn architecture we started experiments with individual lstm gru cnn models with different word dings like glove and fasstext
ever ensembling of cnn with self attention lstm model gave better results than individual models
googletrans
readthedocs
io en we develop a multichannel model for domain identication consisting of two main components
the rst component is a long short time ory hochreiter and schmidhuber forth lstm
the advantage of lstm can dle the long term dependencies but does not store the global semantics of foregoing information in a variable sized vector
the second component is a convolutional neural network lecun henceforth cnn
the advantage of cnn can capture the n gram features of text by using volution lters but it restricts the performance due to convolutional lters size
by considering the strengths of these two components we ensemble the lstm and cnn model for domain tion
figure self attention bilstm the forward and backward hidden states
we catenate the forward and backward hidden units to get the merged representations



hi hi the self attention model gives a score ei to each subword i in the sentence s as given by below equation figure multichannel lstm cnn model ei kt i kn

self attention bilstm classier the rst module in architecture is self attention based bilstm classier
we employed this attention xu et al
based bilstm model to extract the semantic and sentiment information from the input text data
self attention is an attention mechanism in which a softmax function gives each subword s weights in the sentence
the outcome of this module is a weighted sum of den representations at each subword
the self attention mechanism is built on stms architecture see gure and it takes input as pre trained embeddings of the subwords
we passed the telugu fasttext grave et al
word embeddings to a bilstm layer to get hidden representation at each timestep which is the input to the self attention component
suppose the input sentence s is given by the subwords


wn
let h represents the h represents the forward hidden state and ward hidden state at ith position in bilstm
the merged representation ki is obtained by combining then we calculate the attention weight ai by normalizing the attention score ai finally we compute the sentence s latent sentation vector h using below equation the latent representation vector h is fed to a fully connected layer followed by a softmax layer to obtain probabilities plstm


convolutional neural network the second component is cnn which consider the ordering of the words and the context in which each word appears in the sentence
we present telugu fasstext subword embeddings bojanowski et al
of a sentence to cnn see gure to generate the required embeddings
figure cnn classier initially we present a d s sentence ding matrix to the convolution layer
each row is a dimension fasstext subword embedding tor of each word and s is sentence length
we perform convolution operations in the convolution layer with three different kernel sizes and
the purpose behind using various kernel sizes was to capture contexts of varying lengths and to extract local features around each word window
the output of convolution layers was passed to responding max pooling layers
the max pooling layer is used to preserve the word order and bring out the important features from the feature map
we change the original max pooling layer in the convolution neural network with the word preserving k max pooling layer to preserve the putted sentences word order
the order persevering max pooling layer reduces the number of features while preserving the order of these words
the max pooling layer output is concatenated together fed to a fully connected layer followed by a softmax layer to obtain softmax probabilities pcnn
the cnn and bilstm models softmax abilities are aggregated see gure using an element wise product to obtain the nal ities pf inal pcnn plstm
we tried various gregation techniques like average maximum imum element wise addition and element wise multiplication to combine lstm and cnn els probabilities
but an element wise product gave better results than other techniques
results and error analysis we rst started our experiments with machine ing algorithms with various kinds of tf idf ture vector representations
svm mlp gave pretty good results on the validation dataset but failed on bio chemistry and management data points
and cnn model with fasstext word embeddings seemed to be confused between the computer nology and cse data points see table
maybe the reason behind this confusion is that both datapoints quite similar at the syntactic level
the self attention based bilstm model forms the cnn model on physics cse and puter technology data points though it performs worse on the bio chemistry and management data points
if we observe the training set of the data samples belong to physics cse and computer technology domains remaining of data owned by the remaining domain labels
so we were suming that an imbalanced training set was also one of the problems for misclassication of chemistry and management domain samples
we tried to handle the this data skewing problem with smote chawla et al
technique but it is does nt work very well
after observing the cnn and bilstm model results we ensemble these two models for better results
the ensemble multichannel lstm cnn model outperforms all our previous models ing a recall of
with the weighted score of
on the development dataset and a recall of
with an score of
on the test dataset see table
validation data test data model accuracy precision recall score accracy precision recall score svm cnn bilstm multichannel organizers system































table comparison between various model results sepp hochreiter and jurgen schmidhuber

long short term memory
neural computation
y
lecun

generalization and network design strategies
zhenyu liu haiwei huang chaohong lu and shengfei lyu

multichannel cnn with tion for text classication
arxiv

kavi murthy

automatic categorization of telugu news articles
swapna narala b
p
rani and k
ramakrishna

telugu text categorization using language models
global journal of computer science and technology
m
n
swamy m
hanumanthappa and n
jyothi

indian language text representation and categorization using supervised learning algorithm
international conference on intelligent puting applications pages
kelvin xu jimmy ba ryan kiros kyunghyun cho aaron c
courville r
salakhutdinov r
zemel and yoshua bengio

show attend and tell neural image caption generation with visual attention
in icml
validation data test data precision recall accuracy







table performance of multichannel system conclusion in this paper we proposed a multichannel approach that integrates the advantages of cnn and lstm
this model captures local global dependencies and sentiment in a sentence
our approach gives better results than individual cnn lstm and supervised machine learning algorithms on the ugu techdocation dataset
as discussed in the previous section we will handle the data imbalance problem efciently in future work
and we will improve the performance of the unambiguous cases in cse and computer nology domains
references piotr bojanowski edouard grave armand joulin and tomas mikolov

enriching word arxiv preprint tors with subword information


nitesh chawla kevin bowyer lawrence hall and w
kegelmeyer

smote synthetic minority over sampling technique
j
artif
intell
res
jair
a
durga and a
govardhan

ontology based text categorization telugu documents
edouard grave piotr bojanowski prakhar gupta mand joulin and tomas mikolov

learning in proceedings word vectors for languages
of the international conference on language sources and evaluation lrec

