improving social media text summarization by learning sentence weight distribution jingjing xu moe key laboratory of computational linguistics peking university school of electronics engineering and computer science peking university
edu
cn t c o l c
s c v
v i x r a abstract recently encoder decoder models are widely used in social media text rization
however these models times select noise words in irrelevant tences as part of a summary by error thus declining the performance
in der to inhibit irrelevant sentences and cus on key information we propose an effective approach by learning sentence in our model we weight distribution
build a multi layer perceptron to predict sentence weights
during training we use the rouge score as an alternative to the estimated sentence weight and try to imize the gap between estimated weights and predicted weights
in this way we courage our model to focus on the key tences which have high relevance with the summary
experimental results show that our approach outperforms baselines on a large scale social media corpus
introduction text summarization is an important task in ral language processing
it aims to understand the key idea of a document and generate a headline or a summary
previous social media text marization systems rush et al
hu et al
are mainly based on abstractive text marization
most of them belong to a family of encoder decoders which have shown effective in many tasks like machine translation cho et al
dzmitry bahdanau and bengio
however these models sometimes select noise words in irrelevant sentences as part of a mary by error
figure gives an example of noise words generated by a state of the art decoder model rnn context
unlike lation which requires encoding all information to ensure the accuracy summarization tries to tract the most important information
more only a small part of sentences convey the key information while the rest of sentences usually are useless
thus these unrelated sentences make it hard for encoder decoder models to extract key information
in order to address this issue we propose a novel method by learning sentence weight tribution to encourage models focus on key tences and ignore unimportant sentences
in our approach we rst design a multi layer tron to predict sentence weights
then ering that rouge is a popular evaluation rion for summarization we estimate the gold tence weights of training data by rouge scores between sentences and summaries
during ing we design an end to end optimization method which minimizes the gap between predicted tence weights and estimated sentence weights
we conduct experiments on a large scale social media dataset
experimental results show that our method outperforms competitive baselines
sides we do not limit our method to any cic neural network it can be extended to any sequence to sequence model
related work summarization approaches can be divided into two typical categories extractive summarization radev et al
aliguliyev woodsend and lapata ferreira et al
cheng and lapata and abstractive tion knight and marcu bing et al
rush et al
hu et al
gu et al

for extractive summarizations most works ally select several sentences from a document as a summary or a headline
for abstractive figure illustration of the negative inuence of noise words
rnn context is the basic decoder model with the attention mechanism
the gold summary shown in blue is that the rate of china economy growth slows down and the investment in real estate is certainly better than
however the key words are unseen in the output of rnn context while words shown in pink in irrelevant sentences are selected as part of a summary
rization most works usually encode a document into an abstractive representation and then ate words in a summary one by one
most cial media summarization systems belong to stractive text summarizaition
generally ing extractive summarization achieves better formance than abstractive summarization for long and normal documents
however extractive marization is not suitable for social media text which are full of noises and very short
neural abstractive text summarization is a newly proposed method and has become a hot research topic in recent years
unlike the ditional summarization systems which consist of many small sub components that are tuned rately knight and marcu erkan and radev moawad and aref neural abstractive text summarization attempts to build and train a single large neural network that reads a document and outputs a correct summary
rush et al
rst introduced the encoder decoder framework with the attention mechanism to abstractive text summarization
bing et al
proposed an abstraction based multi document summarization framework which can construct new sentences by exploring more ne grained syntactic units than sentences
gu et al
proposed a copy mechanism to address the problem of unknown words
nallapati et al
proposed several novel models to address critical problems in marization
proposed model our method is based on the basic encoder decoder framework proposed by cho et al
and sutskever et al

section
introduces how to estimate sentence weight distribution in detail
section
describes how to generate the representation of sentence weight distribution
section
shows how to incorporate estimated sentence weights and dicted sentence weights in training

estimating sentence weight distribution assume we are provided with a summary y and a document d


dn where n is the number of sentences
the rst step of our method is to compute the distribution of sentence weights for training data as w


wi


where wi is computed as wi and ei is computed as ei rou y where rouge is the evaluation metric to judge the quality of predicted summaries
train devlopment test
representation of sentence weight distribution table details of lcsts dataset
the size is given in number of pairs short text summary
in our model we rst produce sentence weight distribution over all sentences
the tion is based on the sentence embeddings s and the position embeddings of sentences as w j lp sj lp where sj denotes vector tion of sj and mlp refers to a layer perceptron
sj is produced as sj where returns all indexes of words which belong to the jth sentence
then the new output of an encoder part is h h


h i


h n models rnn hu et al
swd rnn context hu et al
swd

r l









table rouge scores r l rouge l of the trained els computed on the test and development sets
rnn and rnn context are two baselines
we refer our method as swd
experiments in this section we evaluate our proposed approach on a social media dataset and report the mance of the models
furthermore we use a case to illustrate the improvement achieved by our proach
where n is the number of hidden states and computed as i is
dataset h i w where returns the weight of sentence which belongs to and hi is the output of rnn or bi lstm used in an encoder
then the new output of an encoder h is delivered to a decoder which produces a summary

training given the model parameter and an input text a corresponding summary y and sentence weight distribution s described in section
the loss function is n n m w i where n is the batch size is the ditional probability of the output word yi given source texts xi i j is the predicted sentence weight and is the conditional ability of the sentence weight wi j descirbed in section
given source texts xi
we use the large scale chinese short tion dataset lcsts which is provided by hu et al

this dataset is constructed from a famous chinese social media called sina
based on the statistic data on the training set we set the maximum number of sentences as and the maximum length of a sentence as in this paper

experimental settings following previous works and experimental sults on the development set we set parameters as follows
the character embedding dimension is and the size of hidden state is
the parameter is

all word dings are initialized randomly
we use the layer encoder and the layer decoder in this paper
we use the minibatch stochastic gradient scent sgd algorithm to train our model
each gradient is computed using a minibatch of pairs document summary
best validation curacy is reached after batches which quires around days of training
for evaluation place where a lot of popular chinese media and ganizations post news and information
figure comparisons of the predicted summaries between rnn context and our method
the dicted summary shown in blue of rnn context comes from an unimportant sentence
in contrast the predicted summary of our method covers some key words shown in purple
we use the rouge metric proposed by lin and hovy
unlike bleu which includes ious n gram matches there are several versions of rouge for different match lengths and rouge l
experiments are performed on a commodity bit dell precision workstation with one
ghz core cpu ram and one titan x gpu

models we do not limit our method on specic neural network it can be extended to any sequence sequence model
in this paper we evaluate our method on two types of baselines
rnn we denote rnn as the basic sequence sequence model with a bi lstm encoder and a bi lstm decoder
it is a widely used framework
rnn context rnn context is a sequence sequence framework with the attention nism

results and discussions we compare our approach with baselines ing rnn and rnn context
the main results are shown in table
it can be seen that our proach achieves rouge improvement over both baselines
in particular swd outperforms context by almost points
finally we give an example summary as shown in figure
this example is illustrated in section aimed to show the negative inuence of portant words on extracting key information on rnn context
rnn context chooses some portant words as summary like some fundings from the central government shown in blue
in contrast the outputs of our method contain some key words shown in pink like fan gang the rate of china economy growth slows down
this example shows the effectiveness of our model on handling noise documents which are full of a ber of irrelevant words
conclusion in this paper we propose a novel method by ing sentence weight distribution to improve the performance of abstractive summarization
the target is to make models focus on important tences and ignore irrelevant sentences
the sults on a large scale chinese social media dataset show that our approach outperforms competitive baselines
we also give the example which shows that the summary produced by our method is more relevant to the gold summary
besides our method can be extended to any sequence sequence model
word based systems are potentially helpful to this task cause the words can incorporate more meaningful information
in the future we will try several word segmentation methods sun et al
xu and sun xu et al
to improve the system
references ramiz m aliguliyev

a new sentence similarity measure and sentence based extractive technique for automatic text summarization
expert systems with applications
lidong bing piji li yi liao wai lam weiwei guo and rebecca passonneau

abstractive document summarization via phrase selection and merging
in proceedings of the annual ing of the association for computational linguistics and the international joint conference on ral language processing volume long papers
association for computational linguistics beijing china pages
jianpeng cheng and mirella lapata

neural summarization by extracting sentences and words
in proceedings of the annual meeting of the association for computational linguistics ume long papers
association for tional linguistics berlin germany pages

aclweb
org anthology
kyunghyun cho bart van merrienboer caglar cehre dzmitry bahdanau fethi bougares holger learning schwenk and yoshua bengio

phrase representations using rnn encoder decoder for statistical machine translation
in proceedings of the conference on empirical methods in ural language processing emnlp
association for computational linguistics doha qatar pages
kyunghyun cho dzmitry bahdanau and yoshua gio

neural machine translation by jointly learning to align and translate
in iclr
gunes erkan and dragomir r radev

lexrank graph based lexical centrality as salience in text journal of articial intelligence summarization
research
rafael luciano ferreira souza cabral rafael dueire lins gabriel pereira e silva fred freitas george dc cavalcanti rinaldo lima steven j simske and luciano favaro

assessing sentence scoring techniques for extractive text summarization
expert systems with applications
jiatao gu zhengdong lu hang li and victor o
k
incorporating copying mechanism in li

in proceedings of sequence to sequence learning
the annual meeting of the association for computational linguistics acl august berlin germany volume long papers
baotian hu qingcai chen and fangze zhu

sts a large scale chinese short text summarization dataset
in proceedings of the conference on empirical methods in natural language ing emnlp lisbon portugal september
pages
kevin knight and daniel marcu

tion beyond sentence extraction a probabilistic proach to sentence compression
articial gence
chin yew lin and eduard h
hovy

matic evaluation of summaries using n gram occurrence statistics
in human language ogy conference of the north american chapter of the association for computational linguistics naacl edmonton canada may june
shuming ma and xu sun

a semantic vance based neural network for text summarization and text simplication
corr

shuming ma xu sun jingjing xu houfeng wang wenjie li and qi su

improving semantic relevance for sequence to sequence learning of nese social media text summarization
in
ibrahim f moawad and mostafa aref

mantic graph reduction approach for abstractive text in computer engineering summarization
tems icces seventh international ence on
ieee pages
ramesh nallapati bowen zhou ccero nogueira dos santos c aglar gulcehre and bing xiang

abstractive text summarization using sequence in proceedings of the sequence rnns and beyond
signll conference on computational natural language learning conll berlin germany august
pages
dragomir r radev timothy allison sasha goldensohn john blitzer arda celebi stanko dimitrov elliott drabek ali hakim wai lam danyu liu al

mead a platform for in tidocument multilingual text summarization
lrec
alexander m
rush sumit chopra and jason weston

a neural attention model for abstractive tence summarization
in emnlp
the association for computational linguistics pages
xu sun wenjie li houfeng wang and qin lu

feature frequency adaptive on line training for fast and accurate natural language processing
tational linguistics
xu sun houfeng wang and wenjie li

fast line training with frequency adaptive learning rates for chinese word segmentation and new word tion
in
pages
xu sun bingzhen wei xuancheng ren and shuming ma

label embedding network learning bel representation for soft training of deep networks
corr

xu sun yaozhong zhang takuya matsuzaki masa tsuruoka and junichi tsujii

a criminative latent variable chinese segmenter with hybrid word character information
in human guage technologies conference of the north ican chapter of the association of computational linguistics proceedings may june boulder colorado usa
pages
xu sun yaozhong zhang takuya matsuzaki masa tsuruoka and junichi tsujii

abilistic chinese word segmentation with non local inf
process
information and stochastic training
manage

ilya sutskever oriol vinyals and quoc v le

sequence to sequence learning with neural works
in advances in neural information ing systems
pages
kristian woodsend and mirella lapata

matic generation of story highlights
in proceedings of the annual meeting of the association for computational linguistics
association for tational linguistics pages
jingjing xu shuming ma yi zhang bingzhen wei xiaoyan cai and xu sun

transfer learning for low resource chinese word segmentation with a novel neural network
in the conference on natural language processing and chinese computing
jingjing xu and xu sun

dependency based gated recursive neural network for chinese word mentation
in
pages

