proceedings of the twenty seventh international joint conference on articial intelligence
areinforcedtopic awareconvolutionalsequence to weproposeadeeplearningapproachtotackletheautomaticsummarizationtasksbyincorporatingtopicinformationintotheconvolu tionalsequence to timization
throughjointlyattendingtotopicsandword levelalignment ourapproachcanimprovecoherence diversity andinformativenessofgen eratedsummariesviaabiasedprobabilitygenera tionmechanism
ontheotherhand reinforcementtraining likescst directlyoptimizesthepro posedmodelwithrespecttothenon differentiablemetricrouge whichalsoavoidstheexposurebiasduringinference
wecarryouttheexperimen talevaluationwithstate of the artmethodsoverthegigaword


thekeychallengesinautomatictextsummarizationarecorrectlyevaluatingandselectingimpor tantinformation efcientlylteringredundantcontents andproperlyaggregatingrelatedsegmentsandmakinghuman readablesummaries
comparedtoothernlptasks theau tomaticsummarizationhasitsowndifculties
forexample unlikemachinetranslationtaskswhereinputandoutputse quencesoftensharesimilarlengths summarizationtasksaremorelikelytohaveinputandoutputsequencesgreatlyim balanced
besides machinetranslationtasksusuallyhavesomedirectword levelalignmentbetweeninputandoutputsequences whichislessobviousinsummarization
therearetwogenresofautomaticsummarizationtech niques namely extractionandabstraction
thegoalofex umentandconcatenatingthemverbatim thereforethesum mariescouldbeparaphrasedinmoregeneralterms
otherthanextraction abstractivemethodsshouldbeabletoprop erlyrewritethecoreideasofthesourcedocumentandassurethatthegeneratedsummariesaregrammaticallycorrectandhumanreadable whichisclosetothewayhowhumansdosummarizationandthusisofinteresttousinthispaper
recently deepneuralnetworkmodelshavebeenwidelyusedfornlptasks
inparticular theattentionbasedsequence to

however rnn basedmod elsaremorepronetogradientvanishingduetotheirchainstructureofnon linearitiescomparedtothehierarchicalstruc tureofcnn dition thetemporaldependenceamongthehiddenstatesofrnnspreventsparallelizationovertheelementsofase quence whichmakesthetraininginefcient
inthispa per weproposeanewapproachbasedontheconvolutionalsequence to
awareattentionmechanism
tothebestofourknowledge thisistherstworkforauto maticabstractivesummarizationthatincorporatesthetopicinformation whichcanprovidethemedandcontextualalign mentinformationintodeeplearningarchitectures
inaddi tion erationmechanismtoincorporatethetopicinformationintoanautomaticsummarizationmodel whichintro ducescontextualinformationtohelpthemodelgeneratemorecoherentsummarieswithincreaseddiversity
proceedings of the twenty seventh international joint conference on articial intelligence differentiablesummarizationmetricrouge ourproposedmodelyieldshighaccuracyforabstractivesummarization advancingthestate of the

whichselectimportantcontentsoftextandcombinethemverbatimtoproduceasummary
ontheotherhand abstractivesummarizationmodelsareabletoproduceagrammaticalsummarywithanovelexpression


basedsequence to



veryfewmethodshaveexploredtheperformanceofconvolu tionalstructureonsummarizationtasks
comparedtornns tages includingefcienttrainingbyleveragingparallelcom puting andmitigatingthegradientvanishingproblemduetofewernon
there
basedmodelsinthelanguagemodelingandmachinetranslationtasks
erallimitations
first themodelistrainedbyminimizingamaximum likelihoodlosswhichissometimesinconsistentwiththemetricthatisevaluatedonthesentencelevel zatoetal

moreimportantly levelalignmentwhichmaybeinsufcientforsummariza tionandpronetoincoherentgeneratedsummaries
there fore thehigherlevelalignmentcouldbeapotentialassist
forexample thetopicinformationhasbeenintroducedtoarnn basedsequence to awareconvolutionalsequence to sequencemodelinthissection weproposethereinforcedtopic awarecon volutionalsequence to sequencemodel whichconsistsofaconvolutionalarchitecturewithbothinputwordsandtopics ajointmulti stepattentionmechanism agraphicalillustrationofthetopic awareconvolutionalarchitecture
topicencoderrepresentations
finally andareinforcementlearningprocedure
thegraph icalillustrationofthetopic

inthispaper twoconvolutionalblocksareemployed associatedwiththeword levelandtopic levelembeddings respectively
weintroducetheformerinthissectionandthelatterinnext alongwiththenewjointattentionandthebiasedgenerationmechanism


wealsoaddapositionalembed ding toretaintheor derinformation
thus
convolutionallayerbothencoderanddecodernetworksarebuiltbystackingsev eralconvolutionallayers
supposethatthekernelhaswidthofkandtheinputembeddingdimensionisd
theconvolu proceedings of the twenty seventh international joint conference on articial intelligence namely y

wisemultipli cation andtheoutputofgluisinrd
wedenotetheoutputsofthel
takethedecoderforillustration
theconvolutionunitionthel thlayeriscomputedbyresidualconnectionsashli
multi stepattentiontheattentionmechanismisintroducedtomakethemodelaccesshistoricalinformation
tocomputetheattention werstembedthecurrentdecoderstatehliasdli

mentjiscomputedasadotproductbetweendliandtheout putzuojofthelastencoderblockuo
onceclihasbeencomputed awareattentionmechanismatopicmodelisatypeofstatisticalmodelfordiscoveringtheabstractideasorhiddensemanticstructuresthatoccurinacollectionofsourcearticles
inthispaper weemploythetopicmodeltoacquirelatentknowledgeofdocumentsandincorporateatopic awaremechanismintothemulti stepattention whichisexpectedtobringpriorknowledgefortextsummarization

training weuseldatoassigntopicstotheinputtexts
thetopnnon universalwordswiththehighestproba weembeditasbeforetoattainwi
however wherekisthesizeoftopicvocabulary
theembeddingmatrixdtopicisnor malizedfromthecorrespondingpre trainedtopicdistributionmatrix whoserowisproportionaltothenumberoftimesthateachwordisassignedtoeachtopic
inthiscase theposi tionalembeddingvectorsarealsoaddedtotheencoderanddecoderelements respectively toobtainthenaltopicem
followingthecon volutionallayerintroducedbefore wecanobtaintheconvo lutionunitionthel thlayerinthedecoderoftopiclevelas

duringdecoding levelencoderblockut
thentheconditionalinput cli both cliandcliareaddedtotheoutputofthecorrespondingdecoderlayer hliandareapartoftheinputto
biasedprobabilitygenerationfinally leveldecoderoutputshloandtopic leveldecoderoutputs

proceedings of the twenty seventh international joint conference on articial intelligence wherezisthenormalizer hloiand hltidenotethei thtopdecoderoutputsofwordandtopic respectively andiistheone webiasthegen erationdistributionbythetopicinformation
otherwise weignorethetopicpart
tosomeextent likelihoodlossateachde codingstep namely truthoutputsequence
optimalresultswithrespecttotheevaluationmetrics suchasrougewhichmeasuresthesentence levelaccuracyofthegeneratedsummaries
thesub ingdatainsteadofitsowndistribution
duringthetrainingprocess modelsarefedbyground truthoutputsequencestopredictthenextword whereasduringinferencetheygeneratethenextwordgiventhepredictedwordsasinputs
therefore inthetestprocess theerrorofeachstepaccumulatesandleadstothedeteriorationofperformance
thesecondreasonforsub optimalitycomesfromtheex ibilityofsummaries
themaximum likelihoodobjectivere wardsmodelsthatcanpredictexactlythesamesummariesasreferenceswhilepenalizingthosethatproducedifferenttextseventhoughtheyaresemanticallysimilar
providingmulti plereferencesummariesishelpfulyetinsufcientsincetherearealternativestorephraseagivensummary
therefore min ertyofsummarization
rouge ontheotherhand providesmoreexibleevaluation encouragingmodelstofocusmoreonsemanticmeaningsthanonword levelcorrespondences
inordertoaddresssuchissues weutilizeself criticalse
dientalgorithmforreinforcementlearning todirectlymax imizethenon differentiablerougemetric
duringrein forcementlearning wegeneratetwooutputsequencesgiventheinputsequencex
itydistribution andtheotheroutputsequenceysisgeneratedbysamplingfromthedistribution
afterobtainingrougescoresofbothsequencesasourrewards
withscst wecandirectlyoptimizethediscreteevalua tionmetric
inaddition theself criticaltest
minister talks leader elections ofcials opens poultry free eu army urges world talks foreign investment malaysia thailand meet vietnam testtimeconsistency
sinceduringlearningwesetthebaselineofthereinforcealgorithmastherewardobtainedbythecurrentmodelinthetest timeinference thescstexposesthemodeltoitsowndistribu weconsiderthreedatasetstoevaluatetheper formanceofdifferentmethodsintheabstractivetextsum marizationtask
first
theinputsummarypairsconsistofthehead lineandtherstsentenceofthesourcearticles
ationset
unlikethegigawordcorpus generatedreferencesummaries whichmakestheevaluationmoreobjective

followingthesettingintheoriginalpaper weusetherstpartoflcstsdatasetfortraining summarypairs trainthecorpusfortopicembeddinginitializationandprovidecandidatesforthebiasedprobabilitygenerationpro cess
thetopicembeddingvaluesarenormalizedtoadistri
inthispaper
notethattheuniversalwordsarelteredoutduringpre training

allembeddings proceedings of the twenty seventh international joint conference on articial intelligence






topic
accuracyonthegigawordcorpusintermsofthefull





topic
accuracyontheinternaltestsetofgigawordcorpusintermsofthefull tweenhiddenandembeddingstates
dationrougescorestopsincreasingaftereachepochun sictopic awareconvolutionalmodelwithrespecttoastan dardmaximumlikelihoodobjective andthenswitchtofur
ments
moreover wechoosetherouge lmetricasthereinforcementrewardfunction
nesterovsacceleratedgradi
withthemini
by stepjustication
first awaremodelorreinforcementlearningistested respectively
thenwecombinethetwotoshowtheperfor manceofourreinforced topic wereportexamplesofsummariesd thesrilankangovernmentonwednesdayannouncedtheclosureofgovernmentschoolswithimmediateeffectasamilitarycampaignagainsttamilseparatistsescalatedinthenorthofthecountry
r srilankaclosesschoolsaswarescalatesor srilankaclosesschoolswithimmediateeffectot srilankaclosesschoolsinwakeofmilitaryattacksd
r uscitizenwhospiedforeastgermansgivensuspendedsentenceor usmangetssuspendedjailtermforcommunistspyingot usmanjailedforespionaged malaysianprimeministermahathirmohamadindicatedhewouldsoonrelinquishcontroloftherulingpartytohisdeputyanwaribrahim
r mahathirwantsleadershipchangetobesmoothor malaysiasmahathirtorelinquishcontrolofrulingpartyot malaysiasmahathirtosubmitcontrolofrulingpartyd afrenchcrocodilefarmsaidithadsteppedupeffortstobreedoneoftheworldsmostendangeredspecies theindianunk withthehopeofultimatelyreturninganimalstotheirhabitatinsouthasia
r frenchfarmoffershopeforendangeredasiancrocsunkpictureor frenchcrocodilefarmstepsupeffortstobreedendangeredspeciesot examplesofgeneratedsummariesonthegigawordcor pus
d sourcedocument r referencesummary or outputofthereinforced ot outputofthereinforced topic
thewordsmarkedinbluearetopicwordsnotinthereferencesummaries
ods basedneuralmodelsfortextsumma rization
theras elmanmodelintroducesaconditionalrnn inwhichtheconditionerisprovidedbyaconvolutionalattention basedencoder
thewords basedattentionmodelwhichimplementsalarge vocabularytrick
besides mumrisktrainingstrategywhichdirectlyoptimizesmodelparametersinsentencelevelwithrespecttotheevaluationmetrics
to sequenceframeworkwithaselectiveencodingmodel
theresultshavedemonstratedthatboththetopic awaremoduleandtherein forcementlearningprocesscanimprovetheaccuracyontextsummarization
moreover

wealsoevaluateourproposedmodelonthissetandpresentthere ourproposedmodelachievesthebestperformanceintermsofallthethreerougescores
tofurtherdemonstratetheimprovementofreadabilityanddiversitybythetopicinformation wealsopresentsomequal itativeresultsbyrandomlyextractingseveralsummariesfromtest
wecomparethereferencesummariestothesummariesgeneratedbyourproposedmodelwithorwithouttopic awaremechanism
itcangener atesomeaccuratelydeliveredtopicwordswhicharenotin proceedings of the twenty seventh international joint conference on articial intelligence







topic
itisbelievedthatthejointlearningwithapre trainedtopicmodelcanof onlydataset wetrainthemodelsonthegigawordcorpusrstandthenevaluatetheirperformanceontheducdataset
asthestan dardpractice wereporttherecall lmetricsinthisexperiment topic lmetrics
duetothesimilarityofthetwodatasets wedonotpro
sincethisisalarge scalechinesedataset suitabledatapreprocessingapproachesshouldbeproposedrst
basically therearetwoapproachestopreprocessingthechinesedataset character basedandword based
thefor mertakeseachchinesecharacterastheinput

thecopynet withbothcharacter basedandword basedpreprocessingbyincorporatingthecopyingmechanismintothesequence to sequenceframework
inthiswork weadopttheword basedapproachaswebelievethatinthecaseofchinese wordsaremorerelevanttolatentknowl edgeofdocumentsthancharactersare
directlyemployingthepack agetoevaluatechinesesummarieswouldyieldunderratedresults
inordertoevaluatethesummarizationonthelc stsdataset characterstonumericalids onwhichwethenperformtherougeevaluation
sincenotallpreviousworkexplicitlymentionedwhetherword pages




topic accuracyonthelcstsdatasetintermsofthefull theword levelrougescoresarepresentedontheleftandthecharacter basedrougemetricswerereported weeval uateourproposedmodelwithbothmetricsinordertoob tainacomprehensivecomparison
basedscore character basedscore

wecanalsoobservethatthecharacter basedresultsofourreinforced topic
regardingtoword basedrougescores lmet rics
however lscores
wesuspectthatitmaybepartlycausedbythebiasedprobabilitygenerationmechanismthatinuenceswordorder whichrequiresfurtherstudies
inadditiontorougescores
theex amplesdemonstratethatthetopic weproposeatopic tion
itisdemonstratedthatthenewtopic awareattentionmechanismintroducessomehigh levelcontextualinforma tionforsummarization
theperformanceoftheproposedmodeladvancesstate of the artmethodsonvariousbench markdatasets
inaddition ourmodelcanproducesummarieswithbetterinformativeness coherence anddiversity
notethattheexperimentsinthisworkaremainlybasedonthesentencesummarization
inthefuture weaimtoevalu ateourmodelonthedatasetswherethesourcetextscanbelongparagraphsormulti documents
moreover wealsonotethathowtoevaluatetheperformanceonchinesesummariesremainsanopenproblem
itisalsoofgreatinteresttostudyonthissubjectinthefuture

proceedings of the twenty seventh international joint conference on articial intelligence examplesofsummariesd
accordingtothenoticeonthefurtherpromotionandapplicationofnewenergyvehicles
ot thenationaldevelopmentandreformcommissionissuedapolicyonfurtherpromotionandapplicationofnewenergyvehiclesd inrecentyears theserviceindustryofsoftwareandinformationtechnologyinchengduhasbeengrowingrapidly rankingrstamongthecitiesinmidwestchina
chengduhasbecomechinaswesternsiliconvalley
r chengdumakeseveryefforttobuildthewesternsiliconvalleyor thereportofchengdusoftwareandinformationtechnologyserviceindustrydevelopmenthasbeenreleasedot theserviceindustryofsoftwareandinformationtechnologyinchengdurocketstomakeitthewesternsiliconvalleyd
thereporterlearnedfromthexinjiangdevelopmentandreformcommissionthattheinitialrailwayconstructionprojectfromkorlatogolmudhadbeenontenderingprocedure
thebeltandroadstrategybenetsxinjiang unk unk
therailwayfrom unk
thedaybefore thereportersofcommercialnewslearnedfromtheshanghaiinternationalweddingphotographicequipmentexhibition whichhasbeenleadinganddeningthedomesticweddingindustry generationnewlymarriedcouplesbyself decidedweddingdecoration weddingprocessandforms

r
generationnewlymarriedcouplesor or shanghaiinternationalweddingphotographicequipmentexhibitionwasheldot ot examplesofgeneratedsummariesonthelcstsdataset
d sourcedocument r referencesummary or outputofthereinforced ot outputofthereinforced topic
thewordsmarkedinbluearetopicwordsnotinthereferencesummaries
thewordsmarkedinredaretopicwordsneitherinthereferencesummariesnorinthesourcedocuments
allthetextsarecarefullytranslatedfromchinese
proceedings of the twenty seventh international joint conference on articial intelligence
kyunghyuncho andyoshuabengio
leenrmckeown
sentencefusionformultidocu mentnewssummarization

andrewyng andmichaelijordan
latentdirichletallocation

bartvanmerrienboer caglargulcehre dzmitrybahdanau fethibougares holgerschwenk andyoshuabengio
learningphraserepresentationsusingrnnencoder
michaelauli andalexandermrush
abstractivesentencesummarizationwithattentiverecurrentneuralnetworks
humanlanguagetechnologies
angelafan michaelauli anddavidgrangier

michaelauli davidgrangier denisyarats andyannndauphin
convo
englishgi gawordcorpus
zhengdonglu hangli andvictorokli
incorporatingcopyingmecha nisminsequence to
longshort termmemory

qingcaichen andfangzezhu
lcsts alargescalechineseshorttextsummariza martijnspitters andanettehulth
headlineextractionbasedonacombina tionofuni andmultidocumentsummarizationtechniques inproceedingsoftheaclworkshoponautomaticsum marization yewlin
rouge
apackageforauto maticevaluationofsummaries
intextsummarizationbranchesout vol
bingxiang
sequence to sequencernnsfortextsumma
bowenzhou caglargulcehre bingxiang etal
abstractivetextsum marizationusingsequence to
alexfreitas andcelsokaest ner
automatictextsummarizationusingamachinelearn ingapproach
advancesinarticialintelligence
hoadang
anddonnahar man
ducincontext
samgross andsoumithchintala

caimingxiong andrichardsocher
adeepreinforcedmodelforabstractivesummarization
corr
sumitchopra michaelauli andwojciechzaremba

etiennemarcheret youssefmroueh jarretross andvaibhavagoel
self
sumitchopra andjasonweston
aneuralattentionmodelforab
yuzhao zhiyuanliu maosongsun etal
neuralheadlinegenera tionwithsentence
jamesmartens georgedahl andgeoffreyhinton
ontheimportanceofinitializationandmomentumindeeplearning
ininternationalconferenceonmachinelearning oriolvinyals andquocvle
sequencetosequencelearningwithneuralnetworks
inadvancesinneuralinformationprocessingsystems
alearningalgorithmforcontinuallyrunningfullyrecurrentneuralnetworks

weiwu yuwu jieliu yalouhuang mingzhou andwei yingma
topicawareneuralresponsegeneration
inaaai
nanyang furuwei andmingzhou


