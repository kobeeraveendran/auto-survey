proceedings of the twenty seventh international joint conference on articial intelligence areinforcedtopic awareconvolutionalsequence to
com
ethz
ch y

edu
com
edu
eduabstractinthispaper weproposeadeeplearningapproachtotackletheautomaticsummarizationtasksbyincorporatingtopicinformationintotheconvolu tionalsequence to timization
throughjointlyattendingtotopicsandword levelalignment ourapproachcanimprovecoherence diversity andinformativenessofgen eratedsummariesviaabiasedprobabilitygenera tionmechanism
ontheotherhand reinforcementtraining likescst directlyoptimizesthepro posedmodelwithrespecttothenon differentiablemetricrouge whichalsoavoidstheexposurebiasduringinference
wecarryouttheexperimen talevaluationwithstate of the artmethodsoverthegigaword
theempiricalresultsdemonstratethesuperiorityofourproposedmethodintheabstractivesummarization


itisofinteresttogenerateinformativeandrepresentativenaturallanguagesummarieswhicharecapableofretainingthemainideasofsourcearticles
thekeychallengesinautomatictextsummarizationarecorrectlyevaluatingandselectingimpor tantinformation efcientlylteringredundantcontents andproperlyaggregatingrelatedsegmentsandmakinghuman readablesummaries
comparedtoothernlptasks theau tomaticsummarizationhasitsowndifculties
forexample unlikemachinetranslationtaskswhereinputandoutputse quencesoftensharesimilarlengths summarizationtasksaremorelikelytohaveinputandoutputsequencesgreatlyim balanced
besides machinetranslationtasksusuallyhavesomedirectword levelalignmentbetweeninputandoutputsequences whichislessobviousinsummarization
therearetwogenresofautomaticsummarizationtech niques namely extractionandabstraction
thegoalofex
umentandconcatenatingthemverbatim
thereforethesum mariescouldbeparaphrasedinmoregeneralterms
otherthanextraction abstractivemethodsshouldbeabletoprop erlyrewritethecoreideasofthesourcedocumentandassurethatthegeneratedsummariesaregrammaticallycorrectandhumanreadable whichisclosetothewayhowhumansdosummarizationandthusisofinteresttousinthispaper
recently deepneuralnetworkmodelshavebeenwidelyusedfornlptasks
inparticular theattentionbasedsequence to


however rnn basedmod elsaremorepronetogradientvanishingduetotheirchainstructureofnon linearitiescomparedtothehierarchicalstruc tureofcnn

inad dition thetemporaldependenceamongthehiddenstatesofrnnspreventsparallelizationovertheelementsofase quence whichmakesthetraininginefcient
inthispa per weproposeanewapproachbasedontheconvolutionalsequence to
awareattentionmechanism
tothebestofourknowledge thisistherstworkforauto maticabstractivesummarizationthatincorporatesthetopicinformation whichcanprovidethemedandcontextualalign mentinformationintodeeplearningarchitectures
inaddi tion

themaincontri erationmechanismtoincorporatethetopicinformationintoanautomaticsummarizationmodel whichintro ducescontextualinformationtohelpthemodelgeneratemorecoherentsummarieswithincreaseddiversity
proceedings of the twenty seventh international joint conference on articial intelligence differentiablesummarizationmetricrouge whichalsoremediestheexposurebiasissue
ourproposedmodelyieldshighaccuracyforabstractivesummarization advancingthestate of the artmethods


whichselectimportantcontentsoftextandcombinethemverbatimtoproduceasummary
ontheotherhand abstractivesummarizationmodelsareabletoproduceagrammaticalsummarywithanovelexpression


basedsequence to






however veryfewmethodshaveexploredtheperformanceofconvolu tionalstructureonsummarizationtasks
comparedtornns tages includingefcienttrainingbyleveragingparallelcom puting andmitigatingthegradientvanishingproblemduetofewernon

notably there

basedmodelsinthelanguagemodelingandmachinetranslationtasks

erallimitations
first themodelistrainedbyminimizingamaximum likelihoodlosswhichissometimesinconsistentwiththemetricthatisevaluatedonthesentencelevel e


inaddition zatoetal

moreimportantly levelalignmentwhichmaybeinsufcientforsummariza tionandpronetoincoherentgeneratedsummaries
there fore thehigherlevelalignmentcouldbeapotentialassist
forexample thetopicinformationhasbeenintroducedtoarnn basedsequence to

awareconvolutionalsequence to sequencemodelinthissection weproposethereinforcedtopic awarecon volutionalsequence to sequencemodel whichconsistsofaconvolutionalarchitecturewithbothinputwordsandtopics ajointmulti stepattentionmechanism agraphicalillustrationofthetopic awareconvolutionalarchitecture
tomright
thenwejointlyattendtowordsandtopicsbycomput topicencoderrepresentations
finally weproducethetargetsequencethroughabiasedprobabilitygenerationmechanism
structure andareinforcementlearningprocedure
thegraph icalillustrationofthetopic



inthispaper twoconvolutionalblocksareemployed associatedwiththeword levelandtopic levelembeddings respectively
weintroducetheformerinthissectionandthelatterinnext alongwiththenewjointattentionandthebiasedgenerationmechanism


wealsoaddapositionalembed ding toretaintheor derinformation
thus
similarly
convolutionallayerbothencoderanddecodernetworksarebuiltbystackingsev eralconvolutionallayers
supposethatthekernelhaswidthofkandtheinputembeddingdimensionisd
theconvolu proceedings of the twenty seventh international joint conference on articial intelligence namely y


wisemultipli cation andtheoutputofgluisinrd
wedenotetheoutputsofthel
takethedecoderforillustration
theconvolutionunitionthel thlayeriscomputedbyresidualconnectionsashli
multi stepattentiontheattentionmechanismisintroducedtomakethemodelaccesshistoricalinformation
tocomputetheattention werstembedthecurrentdecoderstatehliasdli

mentjiscomputedasadotproductbetweendliandtheout putzuojofthelastencoderblockuo
onceclihasbeencomputed

awareattentionmechanismatopicmodelisatypeofstatisticalmodelfordiscoveringtheabstractideasorhiddensemanticstructuresthatoccurinacollectionofsourcearticles
inthispaper weemploythetopicmodeltoacquirelatentknowledgeofdocumentsandincorporateatopic awaremechanismintothemulti stepattention whichisexpectedtobringpriorknowledgefortextsummarization



duringpre training weuseldatoassigntopicstotheinputtexts
thetopnnon universalwordswiththehighestproba bilitiesofeachtopicarechosenintothetopicvocabularyk

whilethevocabularyoftextsisdenotedasv
weembeditasbeforetoattainwi
however wherekisthesizeoftopicvocabulary
theembeddingmatrixdtopicisnor malizedfromthecorrespondingpre trainedtopicdistributionmatrix whoserowisproportionaltothenumberoftimesthateachwordisassignedtoeachtopic
inthiscase theposi tionalembeddingvectorsarealsoaddedtotheencoderanddecoderelements respectively toobtainthenaltopicem
jointattentionagainwetakethedecoderforillustration
followingthecon volutionallayerintroducedbefore wecanobtaintheconvo lutionunitionthel thlayerinthedecoderoftopiclevelas

duringdecoding levelencoderblockut
thentheconditionalinput cli both cliandcliareaddedtotheoutputofthecorrespondingdecoderlayer hliandareapartoftheinputto
biasedprobabilitygenerationfinally leveldecoderoutputshloandtopic leveldecoderoutputs
proceedings of the twenty seventh international joint conference on articial intelligence wherezisthenormalizer hloiand hltidenotethei thtopdecoderoutputsofwordandtopic respectively andiistheone
whenthecandidatewordwisatopicword webiasthegen erationdistributionbythetopicinformation
otherwise weignorethetopicpart
tosomeextent thecomplexityofthesearchspaceisreducedbyintroducingthetopicbiassinceimportantwordsaremorelikelytobegenerateddirectly

likelihoodlossateachde codingstep namely truthoutputsequence
minimizingtheobjectiveineq
optimalresultswithrespecttotheevaluationmetrics suchasrougewhichmeasuresthesentence levelaccuracyofthegeneratedsummaries
thesub
ingdatainsteadofitsowndistribution
duringthetrainingprocess modelsarefedbyground truthoutputsequencestopredictthenextword whereasduringinferencetheygeneratethenextwordgiventhepredictedwordsasinputs
therefore inthetestprocess theerrorofeachstepaccumulatesandleadstothedeteriorationofperformance
thesecondreasonforsub optimalitycomesfromtheex ibilityofsummaries
themaximum likelihoodobjectivere wardsmodelsthatcanpredictexactlythesamesummariesasreferenceswhilepenalizingthosethatproducedifferenttextseventhoughtheyaresemanticallysimilar
providingmulti plereferencesummariesishelpfulyetinsufcientsincetherearealternativestorephraseagivensummary
therefore min imizingtheobjectiveineq
ertyofsummarization
rouge ontheotherhand providesmoreexibleevaluation encouragingmodelstofocusmoreonsemanticmeaningsthanonword levelcorrespondences
inordertoaddresssuchissues weutilizeself criticalse
dientalgorithmforreinforcementlearning todirectlymax imizethenon differentiablerougemetric
duringrein forcementlearning wegeneratetwooutputsequencesgiventheinputsequencex
itydistribution andtheotheroutputsequenceysisgeneratedbysamplingfromthedistribution
afterobtainingrougescoresofbothsequencesasourrewards i
e

withscst wecandirectlyoptimizethediscreteevalua tionmetric
inaddition theself criticaltest
minister talks leader elections ofcials opens poultry free eu army urges world talks foreign investment malaysia thailand meet vietnam u
s
examplesoftopicwordsforthegigawordcorpus
andimprovestraining testtimeconsistency
sinceduringlearningwesetthebaselineofthereinforcealgorithmastherewardobtainedbythecurrentmodelinthetest timeinference thescstexposesthemodeltoitsowndistribu avoidingtheexposurebiasissueandthusimprovingthetestperformance

weconsiderthreedatasetstoevaluatetheper formanceofdifferentmethodsintheabstractivetextsum marizationtask
first


theinputsummarypairsconsistofthehead lineandtherstsentenceofthesourcearticles


thedatasetisastandardsummarizationevalu ationset
unlikethegigawordcorpus generatedreferencesummaries whichmakestheevaluationmoreobjective


followingthesettingintheoriginalpaper weusetherstpartoflcstsdatasetfortraining
summarypairs

trainthecorpusfortopicembeddinginitializationandprovidecandidatesforthebiasedprobabilitygenerationpro cess
thetopicembeddingvaluesarenormalizedtoadistri

inthispaper
notethattheuniversalwordsarelteredoutduringpre training



allembeddings
nist
gov data
html proceedings of the twenty seventh international joint conference on articial intelligence





































topic


accuracyonthegigawordcorpusintermsofthefull l
bestperformanceoneachscoreisdisplayedinboldface


























topic


accuracyontheinternaltestsetofgigawordcorpusintermsofthefull l
bestperformanceoneachscoreisdisplayedinboldface
dingandtheoutputproducedbythedecoderbeforethenallinearlayer
wealsoadoptthesamedimensionalityforthesizeoflinearlayermappingbe tweenhiddenandembeddingstates


dationrougescorestopsincreasingaftereachepochun
wersttraintheba sictopic awareconvolutionalmodelwithrespecttoastan dardmaximumlikelihoodobjective andthenswitchtofur
ments
moreover wechoosetherouge lmetricasthereinforcementrewardfunction
nesterovsacceleratedgradi
withthemini




by stepjustication
first awaremodelorreinforcementlearningistested respectively
thenwecombinethetwotoshowtheperfor manceofourreinforced topic
wereportexamplesofsummariesd thesrilankangovernmentonwednesdayannouncedtheclosureofgovernmentschoolswithimmediateeffectasamilitarycampaignagainsttamilseparatistsescalatedinthenorthofthecountry
r srilankaclosesschoolsaswarescalatesor srilankaclosesschoolswithimmediateeffectot srilankaclosesschoolsinwakeofmilitaryattacksd
r uscitizenwhospiedforeastgermansgivensuspendedsentenceor usmangetssuspendedjailtermforcommunistspyingot usmanjailedforespionaged malaysianprimeministermahathirmohamadindicatedhewouldsoonrelinquishcontroloftherulingpartytohisdeputyanwaribrahim
r mahathirwantsleadershipchangetobesmoothor malaysiasmahathirtorelinquishcontrolofrulingpartyot malaysiasmahathirtosubmitcontrolofrulingpartyd afrenchcrocodilefarmsaidithadsteppedupeffortstobreedoneoftheworldsmostendangeredspecies theindianunk withthehopeofultimatelyreturninganimalstotheirhabitatinsouthasia
r frenchfarmoffershopeforendangeredasiancrocsunkpictureor frenchcrocodilefarmstepsupeffortstobreedendangeredspeciesot examplesofgeneratedsummariesonthegigawordcor pus
d sourcedocument r referencesummary or outputofthereinforced ot outputofthereinforced topic
thewordsmarkedinbluearetopicwordsnotinthereferencesummaries
thewordsmarkedinredaretopicwordsneitherinthereferencesummariesnorinthesourcedocuments
thefull ods
basedneuralmodelsfortextsumma rization
theras elmanmodelintroducesaconditionalrnn inwhichtheconditionerisprovidedbyaconvolutionalattention basedencoder
thewords basedattentionmodelwhichimplementsalarge vocabularytrick
besides mumrisktrainingstrategywhichdirectlyoptimizesmodelparametersinsentencelevelwithrespecttotheevaluationmetrics
to sequenceframeworkwithaselectiveencodingmodel
theresultshavedemonstratedthatboththetopic awaremoduleandtherein forcementlearningprocesscanimprovetheaccuracyontextsummarization
moreover l


wealsoevaluateourproposedmodelonthissetandpresentthere
again ourproposedmodelachievesthebestperformanceintermsofallthethreerougescores
tofurtherdemonstratetheimprovementofreadabilityanddiversitybythetopicinformation wealsopresentsomequal itativeresultsbyrandomlyextractingseveralsummariesfromtest
wecomparethereferencesummariestothesummariesgeneratedbyourproposedmodelwithorwithouttopic awaremechanism

wecanobservethatwhenthetopicmodelisadopted itcangener atesomeaccuratelydeliveredtopicwordswhicharenotin proceedings of the twenty seventh international joint conference on articial intelligence





































topic


l
bestperformanceoneachscoreisdisplayedinboldface
thereferencesummariesortheoriginaltexts
itisbelievedthatthejointlearningwithapre trainedtopicmodelcanof fermoreinsightfulinformationandimprovethediversityandreadabilityforthesummarization

onlydataset wetrainthemodelsonthegigawordcorpusrstandthenevaluatetheirperformanceontheducdataset
asthestan dardpractice wereporttherecall lmetricsinthisexperiment
topic lmetrics
duetothesimilarityofthetwodatasets wedonotpro videqualitativesummarizationexamplesinthisexperiment


sincethisisalarge scalechinesedataset suitabledatapreprocessingapproachesshouldbeproposedrst
basically therearetwoapproachestopreprocessingthechinesedataset character basedandword based
thefor mertakeseachchinesecharacterastheinput whilethelattersplitsaninputsentenceintochinesewords
huetal

shenetal

guetal
thecopynet withbothcharacter basedandword basedpreprocessingbyincorporatingthecopyingmechanismintothesequence to sequenceframework
inthiswork weadopttheword basedapproachaswebelievethatinthecaseofchinese wordsaremorerelevanttolatentknowl edgeofdocumentsthancharactersare
directlyemployingthepack agetoevaluatechinesesummarieswouldyieldunderratedresults
inordertoevaluatethesummarizationonthelc stsdataset
characterstonumericalids onwhichwethenperformtherougeevaluation
sincenotallpreviousworkexplicitlymentionedwhetherword
berouge
com pages default




































topic





accuracyonthelcstsdatasetintermsofthefull l
inlastthreerows theword levelrougescoresarepresentedontheleftandthecharacter levelontheright
orcharacter basedrougemetricswerereported weeval uateourproposedmodelwithbothmetricsinordertoob tainacomprehensivecomparison
basedscore character basedscore

wecanalsoobservethatthecharacter basedresultsofourreinforced topic
regardingtoword basedrougescores lmet rics
however lscores
wesuspectthatitmaybepartlycausedbythebiasedprobabilitygenerationmechanismthatinuenceswordorder whichrequiresfurtherstudies
inadditiontorougescores

theex amplesdemonstratethatthetopic awaremechanismcanalsoimprovethediversityinchinesesummarizationtasks
weproposeatopic tion
itisdemonstratedthatthenewtopic awareattentionmechanismintroducessomehigh levelcontextualinforma tionforsummarization
theperformanceoftheproposedmodeladvancesstate of the artmethodsonvariousbench markdatasets
inaddition ourmodelcanproducesummarieswithbetterinformativeness coherence anddiversity
notethattheexperimentsinthisworkaremainlybasedonthesentencesummarization
inthefuture weaimtoevalu ateourmodelonthedatasetswherethesourcetextscanbelongparagraphsormulti documents
moreover wealsonotethathowtoevaluatetheperformanceonchinesesummariesremainsanopenproblem
itisalsoofgreatinteresttostudyonthissubjectinthefuture

proceedings of the twenty seventh international joint conference on articial intelligence examplesofsummariesd accordingtothenoticeonthefurtherpromotionandapplicationofnewenergyvehicles
ot thenationaldevelopmentandreformcommissionissuedapolicyonfurtherpromotionandapplicationofnewenergyvehiclesd





inrecentyears theserviceindustryofsoftwareandinformationtechnologyinchengduhasbeengrowingrapidly rankingrstamongthecitiesinmidwestchina
chengduhasbecomechinaswesternsiliconvalley






r chengdumakeseveryefforttobuildthewesternsiliconvalleyor thereportofchengdusoftwareandinformationtechnologyserviceindustrydevelopmenthasbeenreleasedot theserviceindustryofsoftwareandinformationtechnologyinchengdurocketstomakeitthewesternsiliconvalleyd
thereporterlearnedfromthexinjiangdevelopmentandreformcommissionthattheinitialrailwayconstructionprojectfromkorlatogolmudhadbeenontenderingprocedure

thebeltandroadstrategybenetsxinjiang unk unk therailwayfrom unk thedaybefore thereportersofcommercialnewslearnedfromtheshanghaiinternationalweddingphotographicequipmentexhibition whichhasbeenleadinganddeningthedomesticweddingindustry generationnewlymarriedcouplesbyself decidedweddingdecoration weddingprocessandforms

r generationnewlymarriedcouplesor or shanghaiinternationalweddingphotographicequipmentexhibitionwasheldot ot examplesofgeneratedsummariesonthelcstsdataset
d sourcedocument r referencesummary or outputofthereinforced ot outputofthereinforced topic
thewordsmarkedinbluearetopicwordsnotinthereferencesummaries
thewordsmarkedinredaretopicwordsneitherinthereferencesummariesnorinthesourcedocuments
allthetextsarecarefullytranslatedfromchinese
proceedings of the twenty seventh international joint conference on articial intelligence
kyunghyuncho andyoshuabengio
neuralmachinetranslationbyjointlylearningtoalignandtranslate


leenrmckeown
sentencefusionformultidocu mentnewssummarization

bleietal
andrewyng andmichaelijordan
latentdirichletallocation

choetal
bartvanmerrienboer caglargulcehre dzmitrybahdanau fethibougares holgerschwenk andyoshuabengio
learningphraserepresentationsusingrnnencoder decoderforstatisticalmachinetranslation


chopraetal
michaelauli andalexandermrush
abstractivesentencesummarizationwithattentiverecurrentneuralnetworks
humanlanguagetechnologies
dauphinetal
angelafan michaelauli anddavidgrangier
languagemodelingwithgatedconvolutionalnetworks


gehringetal
michaelauli davidgrangier denisyarats andyannndauphin
convo lutionalsequencetosequencelearning



englishgi gawordcorpus

guetal
zhengdonglu hangli andvictorokli
incorporatingcopyingmecha nisminsequence to sequencelearning



longshort termmemory

huetal
qingcaichen andfangzezhu
lcsts alargescalechineseshorttextsummariza tiondataset


kraaijetal
martijnspitters andanettehulth
headlineextractionbasedonacombina tionofuni andmultidocumentsummarizationtechniques
inproceedingsoftheaclworkshoponautomaticsum marization

yewlin
rouge apackageforauto maticevaluationofsummaries
intextsummarizationbranchesout vol
barcelona
nallapatietal
bingxiang andbowenzhou
sequence to sequencernnsfortextsumma rization

nallapatietal
bowenzhou caglargulcehre bingxiang etal
abstractivetextsum marizationusingsequence to sequencernnsandbeyond


netoetal
alexfreitas andcelsokaest ner
automatictextsummarizationusingamachinelearn ingapproach
advancesinarticialintelligence
overetal
hoadang anddonnahar man
ducincontext

paszkeetal
samgross andsoumithchintala

paulusetal
caimingxiong andrichardsocher
adeepreinforcedmodelforabstractivesummarization
corr

ranzatoetal
sumitchopra michaelauli andwojciechzaremba
sequenceleveltrainingwithrecurrentneuralnetworks


rennieetal
etiennemarcheret youssefmroueh jarretross andvaibhavagoel
self criticalsequencetrainingforimagecaptioning


rushetal
sumitchopra andjasonweston
aneuralattentionmodelforab stractivesentencesummarization


shenetal
yuzhao zhiyuanliu maosongsun etal
neuralheadlinegenera tionwithsentence wiseoptimization


sutskeveretal
jamesmartens georgedahl andgeoffreyhinton
ontheimportanceofinitializationandmomentumindeeplearning
ininternationalconferenceonmachinelearning
sutskeveretal
oriolvinyals andquocvle
sequencetosequencelearningwithneuralnetworks
inadvancesinneuralinformationprocessingsystems

j
williamsandd
zipser
alearningalgorithmforcontinuallyrunningfullyrecurrentneuralnetworks

xingetal
weiwu yuwu jieliu yalouhuang mingzhou andwei yingma
topicawareneuralresponsegeneration
inaaai
zhouetal
nanyang furuwei andmingzhou
selectiveencodingforabstractivesentencesummarization



