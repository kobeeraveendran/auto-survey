knowledge guided unsupervised rhetorical parsing for text summarization shengluan houa ruqian lua c ainstitute of computing technology chinese academy of sciences beijing china buniversity of chinese academy of sciences beijing china cacademy of mathematics and systems sciences key lab of madis chinese academy of sciences beijing china t c o l c
s c v
v i x r a abstract automatic text summarization ats has recently achieved impressive performance thanks to recent advances in deep learning and the availability of large scale corpora
to make the summarization results more faithful this paper presents an unsupervised approach that combines rhetorical structure theory deep neural model and domain knowledge concern for ats
this architecture mainly contains three components domain knowledge base tion based on representation learning attentional encoder decoder model for rhetorical parsing and subroutine based model for text summarization
domain knowledge can be eectively used for unsupervised rhetorical parsing thus rhetorical structure trees for each document can be derived
in the unsupervised rhetorical parsing module the idea of translation was adopted to alleviate the problem of data scarcity
the subroutine based summarization model purely depends on the derived rhetorical structure trees and can generate content balanced results
to evaluate the summary results without golden standard we proposed an unsupervised evaluation metric whose hyper parameters were tuned by supervised learning
experimental results show that on a large scale chinese dataset our proposed approach can obtain comparable performances compared with existing methods
keywords automatic text summarization rhetorical structure theory domain knowledge base attentional encoder decoder natural language processing
introduction automatic text summarization ats aims to produce a condensed representation while keeping the salient ements from one or a group of topic related documents which is a potential research area receiving considerable attentions from academia to industry
with the amounts of data are being generated in the web age ats plays an creasingly important role in addressing the problem of how to acquire information and knowledge in a fast reliable and ecient way
generally ats can be categorized into two types extractive summarization and abstractive counterpart
extractive text summarization approaches directly extract salient words sentences or other granularities of texts to produce the summary
conversely abstractive models paraphrase the salient contents using nlg techniques
abstractive methods concerns the generation of new sentences new phrases while retaining the same meaning as the same source documents have which are more complex than extractive ones
extractive approaches are now the mainstream ones
according to the number of input documents ats can also be classied into single document summarization sds and multi document summarization mds methods
in this paper we focus on extractive sds task
the key to sds is how to score the salience of candidate text summary units i
e
sentences clauses ect
there are lexical chain based approaches classical machine learning approaches graph based unsupervised methods
with the recent advances in deep learning ats has beneted much from these new ideas and gained considerable improvements
these approaches are described in more detail in section
to make the results more faithful and coherent we incorporate discourse structures into summarization generation
corresponding author email addresses
com shengluan hou
ac
cn ruqian lu preprint submitted to elsevier october discourse structure theories involve understanding the part whole nature of textual documents
the task of ical parsing for example involves understanding how two text spans are related to each other in the context
as web mining extracts latent knowledge from the web content rhetorical parsing reveals the meaningful knowledge out of vast amounts of documents and help in improving many nlp applications
the theoretical foundation is rhetorical structure theory rst which a comprehensive theory of discourse organization
rst investigates how clauses sentences and even larger text spans connect together into a whole
rst assumes that discourse is not merely a lection of random utterances but the discourse units connect to each other as a whole in a logical and topological way
rst explains text coherence by postulating a hierarchical connected tree structure denote as rs tree for a given text in which every part has a role a function to play with respect to other parts in the text
rst has been empirically proved useful for improving the performance of nlp tasks that need to combine meanings of larger text units such as single document summarization qa and chabot and text classication
our proposed model can benet from rst for faithful results which mainly consists of three components main knowledge base construction based on representation learning attentional encoder decoder model for rhetorical parsing and subroutine based model for text summarization
the rst component extracts domain keywords based on representation learning
the domain keywords contain three types acting agents major inuence factors and dynamics of a domain
the second component leverages the output of the rst component for rs tree construction which will be fed to the third component for summary generation
our aim is chinese oriented text summarization
to alleviate the problem of data scarcity we leverage the labeled english data rst dt and map texts of english and chinese into the same latent space from which the rhetorical relation between two chinese text spans can be determined
furthermore the last component extracts summary texts from the derived rs trees
the generated summary from our subroutine based text summarization model can be always balanced between nucleus and satellite subtrees
the contributions of this work can be concluded as follows we rst proposed an unsupervised chinese oriented rhetorical parsing method
existing rhetorical parsing methods are english oriented supervised methods that often trained on rst dt a human annotated discourse treebank of wsj articles under the framework of rst
our proposed method leverages the idea of translation and embeds the chinese and english texts in the same latent space
in this way the rhetorical relations between chinese text spans can be determined by the the rhetorical relations in rst dt
domain knowledge was utilized in the rhetorical parsing procedure which was constructed based on tation learning
domain knowledge was used in two aspects one for discourse segmentation and the other one for guiding rhetorical structure inference
furthermore attention mechanism was adopted in rhetorical parsing thus the attention weights enable our model has the ability to focus on relevant and drown out irrelevant parts of the input
dierent from the majority of literature our subroutine based summarization model is purely based on the generated rhetorical structure
the basic processing unit is elementary discourse unit edu which is tive shorter than sentence
thus the generated summary can be more informative
this model is based on importance rst principle each time the currently most important edu from the rhetorical structure will be selected one by one mechanically
the importance rst principle makes the selection of edus alternated between nucleus and satellite subtrees
thus the generated summary can always be balanced
we also proposed an unsupervised summarization evaluation metric
this evaluation metric considers many aspects of how faithful a generated summary is
to make this evaluation metric more eective the parameters were tuned by supervised learning on the golden standard of
the remainder of this paper is organized as follows
section reviews some related works including approaches about domain knowledge rhetorical parsing and automatic text summarization
section is domain knowledge base construction based on representation learning
the large scale chinese dataset and experimental results on it will also be given
the unsupervised rhetorical parsing approach is elaborated in section in which the idea of translation and attention mechanism were adopted
section is about the subroutine based text summarization
an unsupervised summarization evaluation metric and experimental results are shown in section
the paper is concluded with a brief summary and an outlook for further research in section

related works in this section we briey review some related works
in subsection
we will rst discuss works about domain knowledge
subsection
then introduces rhetorical structure theory which is an important theoretical foundation of our work
finally the latest and classical approaches of automatic text summarization will be described in tion



domain knowledge knowledge is power
domain knowledge plays a signicant role in many nlp tasks
for instance the knowledge graph kg is a knowledge base proposed by google to enhance its search engine s results with information gathered from a variety of sources
li and mao proposed an eective way of combing human knowledge and information from data for cnn to achieve better performance
they presented k cnn a knowledge oriented cnn for causal relation extraction
in k cnn the convolutional lters are automatically generated based on wordnet and framenet
the data oriented channel is used to learn other important features of causal relation from the data
lu et al
studied the concepts of big knowledge big knowledge system and big knowledge engineering
ten massiveness characteristics for big knowledge and big knowledge systems are dened and explored
zheng explored how to enable humans to use big knowledge correctly and eectively in biomedical domain
there are also some based text summarization methods we refer to
domain knowledge keyword extraction is dened as the task that automatically identies a set of the terms that best describe the domain of documents
generally domain keyword extraction approaches can be divided into two categories as unsupervised methods and supervised methods
tf idf is one of the simplest unsupervised proaches
the top k high tf idf value words are chosen as keywords
until now tf idf remains a strong vised baseline
textrank is another typical unsupervised method which formulates keyword extraction as recommendation
the supervised methods often take keyword extraction as classication problems
however a number of annotated dataset is needed which is limited for unlabeled data
kong et al
constructed a chinese sentiment lexicon using representation learning
a skip gram model was built to predict word embeddings ing to the context words and their composing characters whose outputs were then fed into a random forest rf classier
words of the same polarity were then grouped together to form the sentiment lexicon
with regard to kg yago is automatically extracted from wikipedia and other sources
contains million facts about
million entities in which an article in wikipedia becomes an entity
dbpedia extracts fact triples from dierent language versions of wikipedia
to tackle the problem of low recall for pattern based approaches angeli et al
leveraged dependency parsing tree for relation triple extraction
they constructed a few patterns for canonically structured sentences and shift the focus to a classier which learns to extract self contained clauses from long sentences
on the other hand the key idea of kg embedding is to embed components of a kg into continuous vector spaces and thus to simplify the manipulation while preserving the inherent structure of the kg
typical methods contain transe transh
kg embedding has been applied to and benets a wide variety of downstream nlp tasks such as kg completion question answering and so on


rhetorical structure theory rhetorical structure theory is a comprehensive theory of text organization
with more and more attentions on this theory rst has been applied to many high level nlp applications since marcu s earlier works on rst parsing and applications on text summarization
rst is now one of the most popular theories for discourse analysis
central to rst is rhetorical relation which exists between two neighboring text units
the interpretation of how text spans are semantically related to each other described by rhetorical relations is crucial to retrieve important information from documents
there are two types of rhetorical relations mononuclear relations and multi nuclear relations
in the former ones one of the text spans is more important than the other one which play the role of nucleus and satellite respectively
one the other hand all text spans are equally salient in multi nuclear relations which are all play the role of nucleus
nucleus and satellite play dierent roles to the writer s purpose
in general what nucleus of a rhetorical relation expresses is more essential than what satellite expresses the nucleus is comprehensible independent of the satellite but not vice versa
according to rst the minimum processing unit is edu
edu acts as a syntactic constituent that has independent semantics
in this sense an edu corresponds to a clause or a simple sentence
rst explains text coherence by postulating a hierarchical connected tree structure i
e
rs tree for a given text
in the rs tree each leaf node corresponds to an edu
each internal node corresponds to a larger text span which captures the rhetorical relation between its two children
rhetorical parsing aims to generate edu sequences and rs trees for given documents
it involves nding roles for every granularity of text spans and rhetorical relations that hold between them
there are rule based methods traditional machine learning methods and deep learning methods
lethanh et al
used syntactic information and cue phrases to segment sentences and integrated constraints about textual adjacency and textual organization to generate best rs trees
toloski et al
presented a syntactic rules and lexical rules based discourse segmenter slseg
soricut and marcu s spade model used two probabilistic models for sentence level analysis one for segmentation and the other for rs tree building
after that most research focused on svm based discourse analysis
they regarded relation identication as classication problem
joty et al
rst used dynamic conditional random field dcrf for sentence level discourse analysis and then proposed a two stage rhetorical parser
recent advances in deep learning led to further progress in rhetorical parsing
dplp is a representation learning method whose main idea is to project lexical features into a latent space
dplp constructs rs trees in a shift reduce way
a multi class linear svm classier was learned to decide whether shift or reduce operation would be taken
li et al
s recursive method contains two components
the rst is to obtain the distributed representation for sentences using recursive convolution based on its syntactic tree
the second component contains two classiers one is used for determining whether two adjacent nodes should be merged
if so the other one selects the appropriate rhetorical relation to the new merged subtree


automatic text summarization automatic text summarization has spurred a surge of research and experimentation since its remarkable eect in modern web age
with the fast development of deep learning technologies many eorts applied encoder decoder models into ats
the usage of attention mechanism into text summarization was rst brought to prominence by rush et al

this attentional encoder decoder abstractive model was trained on large scale dataset
its variants and further improvements include al
neural extractive methods are also popular such as pointer network based models summaarunner swap net
most of the extractive models are trained on cnn dataset
however large scale dataset is necessary for these neural models since they are purely driven
for mds parallel data is scarce and costly to obtain
to tackle this predicament lebano et al
presented pg mmr an adaptation method from single to mds to generate abstract summaries from multiple documents
this method is new but also aected by the data source and data scale
besides the above deep learning based approaches there are also other solutions such as traditional machine learning based methods optimization based methods graph based ones
ats was taken as an optimization lem in the ilp based method did exact inference under a maximum coverage model
other traditional machine learning based methods take tf idf n gram the position and others as features to extract summary sentences
for more details we refer to
graph based methods have become increasingly prevalent and far reaching since their easy implementation and relative good performance such as textrank
another representative of pervised algorithm is summcoder whose summary sentence selection module contains three metrics sentence content relevance is measured by a deep auto encoder network sentence novelty is measured by sentence similarity based on sentence embeddings and sentence position relevance is derived by a hand designed score function
the authors of rst have long speculated that the nuclei in rs tree constitute an adequate summarization of the text
it was rst validated by marcu
louis et al
proved that the structure features i
e
position in the global structure of the whole text of rs tree are the most useful feature to compute the salience of text spans
hirao et al
treated summary generation as a tree knapsack problem
they transformed an rs tree into a based discourse tree dep dt which can be directly used to take tree trimming method for text summarization
for mds to address redundancy problem zahri et al
used rs trees for cluster based mds
they utilized rhetorical relations that exist between two sentences to group similar sentences into multiple clusters to identify themes of common information from which candidate summary sentences were extracted
in this paper we propose further contribution to this approach focusing on unsupervised extractive summarization

ldc
upenn
edu
com deepmind rc data
domain knowledge base construction based on representation learning domain knowledge plays a signicant role in many nlp tasks
at present most of the existing knowledge bases are in the form of knowledge graph such as yago dbpedia which generally consists of entity and relation triples
the knowledge triples in a kg are composed of two entities along with their relation which are in the form of where and are entities that often nouns or noun phrases is the relation between and
however knowledge keywords for a domain are also indispensable
for a domain knowledge keywords can provide a panorama for this domain
in this section we propose a framework of constructing domain knowledge base on the basis of representation learning
our proposed domain knowledge contains three types of keywords acting agents major inuence factors and dynamics of a domain
we dene the domain as denition domain
a domain is a particular area of human knowledge
such as education nance al
for a domain keywords can be regarded as the knowledge generalization of the full text in a corresponding literature and help readers to quickly grasp the core idea core technique or core methodology
in general two dierent domains have dierent knowledge keywords but maybe with some common knowledge keywords
the denition of domain knowledge keyword is given in denition
denition domain knowledge keyword dkk
a domain knowledge keyword is a basic and characteristic ement of this domain which is represented by a word or phrase and is often referred to when talking about some aspects of this domain
dkk can generalize the main topics of domain texts
example
teacher student professor teach learn library course doctoral are dkks of the domain education
two dierent domains may share some of their dkks e

library may be a dkk of some other domains but never share their whole sets of dkks
the less the size of the shared dkks the more are the two domains dierent from each other
we argue that besides nouns verbs and adjectives adverbs also serve as the key components
in the above example teacher is a noun teach is a verb and doctoral is an adjective
in fact these three types of keywords constitute the main types of dkks
for each domain we construct domain knowledge from large scale texts
the dkb in this work is composed of a set of triples containing domain keywords
denition domain knowledge base dkb
for a domain the dkb can be represented by a triple a p t where a denotes nouns and named entities each of which represents the acting agents of this domain p acts as the major inuence factors of this domain which are nouns t denotes the concepts about dynamics of this domain each of which is often adjective or adverb
these three types of keywords constitute a full dkb for a domain
our goal is to construct the dkb for each domain in a fast and ecient manner
traditional methods can obtain high accuracy but with low recall it also need much eorts when used to a new domain
on the other hand the more and more popular word embedding methods have the pros of robust and ecient
it can be trained on large scale dataset without any other extra resource
we leverage the representation learning from word embedding methods for dkb construction
denition domain knowledge base construction dkbc
given a large set of documents that consists of texts for several domains dt dkbc aims to extract a dkb from each domain texts i t the constructed dkb is in the form of
specially for each domain di given the corresponding documents dkbc can automatically generate three types of dkks as dened in denition
all generated dkks can constitute the dkb denote as dkbi in the form of such that if wm dkbi then wm dicti where dicti is the vocabulary taken from di suppose dkbi ai pi ti wp ai wq pi then p q
to obtain better results our model is an integration of three dierent models
the rst one is representation learning based model which we call vwrank
the other two models are tf idf model and textrank model
idf is an important indicator of the word s saliency
textrank is an recommendation strategy for voting salient words


the architecture of vwrank our dkbc model utilizes representation learning from word embedding approaches
we use the improved word representation learning with sememes method called se wrl
the sememe knowledge base they used is hownet
se wrl provides dierent strategies among which se wrl sat achieved the best performance according to their original paper
se wrl sat learns the original word embeddings for context words but sememe embeddings for target words
for each domain we use se wrl sat to learn word representations
then we dene the similarity between two n as consine distance candidate words n and s cw j cwi cw j obviously the following two properties hold s cw j s j cwi s cw j
motivated by textrank the score of candidate keyword cwi can be computed as s d s j s j cwi cw js cwi cwks cw j s j cwk where is a damping factor which has the role of integrating into the model the probability of jumping from a given candidate word to another random candidate word
s cwi and s cw j are two sets of candidate words that cwi and cw j similar with respectively
the similarity between two candidate keywords is computed by
after several iterations the s can converge to a xed value


model integration besides vwrank in a domain we also calculate the tf idf and textrank values for candidate words in each domain
we denote the high score candidate keywords of vwrank tf idf and textrank as cvw cti and ctr
the nal score of a candidate keyword is computed as s cwi cwi cwi where and are harmonic coecients i is the indicator function such that cw if cw c else table excerpts of url and its corresponding domain url domain world
xinhuanet
com
china
com world
sina
com

china

soufun
finance sports house table the statistics of documents in each domain domain sports it military olympic culture house domestic entertainment domain auto finance lady education society world docs avg
sens
avg
words






docs avg
sens
avg
words








health











total





then the nal dkks are composed of candidate keywords that further ltered by the value of the number of documents that cwi presents the number of all documents in this domain finally all selected dkks will be organized in hierarchies by their semantic in hownet


dataset sogouca is a large scale chinese corpus which is crawled and provided by sogou labs from dozens of chinese news websites including news reports and reviews
each document in sogouca contains elds of url docno contenttitle and content
leveraging url information we can categorize documents into corresponding domains
excerpts of url and its corresponding domain are shown in table
after that we collected texts for domains
we did preprocessing including delete empty or very short lines ignore extreme long lines
the statistics of documents in each domain are listed in table from which we can see that most of the domains contain tens of thousands of documents
unlike english to manipulate text at the word level word segmentation is needed for chinese text processing
we used hanlp for chinese word segmentation part of speech tagging and named entity recognition ner which is a chinese natural language processing tool


experimental results of dkbc we have nished the dkbc for domains according to the above methods
the derived dkbs can be used for other nlp applications
the statistics of dkb in each domain are shown in table
fig
shows some dkks of finance domain in chinese
the dkks contains keywords such as
these words can provide a panorama for domain finance

sogou
com labs resource ca
php table the statistics of dkb in each domain domain sports it military olympic culture house domestic entertainment agents phenomenons tendencies domain agents phenomenons tendencies auto finance lady health education society world total fig

the dkks of finance domain on the other hand in the domain of it fig
some dkks are be
these two examples can validate the eectiveness of our method

unsupervised rhetorical parsing rhetorical structure theory was proposed as a way to attribute structure to text which often represents a text as a tree structure
it is characterized by rhetorical relations which reect the semantic and functional judgments about the text spans they connect
we rst give a formal denition of rhetorical structure tree
denition rhetorical structure tree
rhetorical structure tree rs tree is a tree representation of a document under the framework of rst
the leaf nodes of a rs tree are edus
each internal node is characterized by a rhetorical relation and corresponds to a contiguous text span
the siblings are connected via a rhetorical relation such that in most cases one is nucleus and the other is satellite
the siblings are both nucleus when they are connected by a multi nuclear relation
in denition edu is the minimal textual unit of an rs tree which means that it ca nt be split into smaller text spans
edu acts as a syntactic constituent that has independent semantics
in this sense an edu functionally corresponds to a simple sentence or a clause in a complex sentence
denition rhetorical parsing
rhetorical parsing also called rst analysis rst parsing or rhetorical ysis is a procedure of generating edu sequences and deriving rs trees for given texts
it involves segmenting discourse into edus and nding roles for every granularity of text spans edus sentences paragraphs and even larger spans and rhetorical relations that hold between them
as depicted in denition rhetorical parsing contains two steps discourse segmentation and rs tree tion
an example rs tree for a given chinese text is shown in fig

the leaf nodes numbered with digits are four fig

the dkks of it domain fig

an example of rs tree edus
the internal nodes corresponds to text spans are characterized by rhetorical relations such as joint and oration
the arrow from a to b denotes a and b are satellite and nucleus respectively in the sense of that relation
they are both nuclei when a and b have multi nuclear relation
horizontal lines correspond to text spans and vertical lines identify text spans which are nuclei


discourse segmentation based on domain knowledge base leveraging domain knowledge we segment each document in each domain into edu sequence
according to denition edu functionally corresponds to a simple sentence or a clause
we rst segment a text into paragraphs and further sentences by punctuations
then dkb is used for segmenting sentences into edus
concretely for a domain given its dkb kd and domain texts td for each text td i td algorithm is the detailed segmentation algorithm
algorithm discourse segmentation for a domain text input a document td output edu sequences s edu
i dkb kd
segment td i into sentence sequence s by punctuations line break for segmenting into paragraphs period question mark for segmenting into sentences
for each sentence s j in s do scan s j and match their words against the domain keywords in kd if the domain keywords of a clause have the form or then put it into s edu
end for output the edus in s edu according to their order in the original text
fig

the architecture of rhetorical relation identication after algorithm each derived edu is a part of a sentence or clause characterizing the domain relatedness of its elements
moreover most edus have the form of with respect to domain keywords
for the form of we borrow a agent keyword from the nearest neighbor edu to form a complete triple
after that each edu has a dkb triple a p t


rhetorical structure theory discourse treebank for rs tree construction existing models contain classical machine learning based methods and deep based methods almost all of which are supervised methods
these approaches were trained on rhetorical structure theory discourse treebank rst dt
rst dt was developed as a human annotated discourse level corpus with rs trees for english written wall street journal texts
these texts were manually annotated by the professional language analysts grounded in the framework of rst
there are ne grained rhetorical relations that grouped into coarse grained relation categories
in the existing approaches the latter categories are often used for training and testing
since there exist multi nuclear relations non binary relations are often converted into a cascade of right branching binary relations for convenience
in rst dt there are edus and text pairs that are characterized by rhetorical relations


attentional encoder decoder model for rs tree construction the objective of rs tree construction is to nd rhetorical relations between two adjacent text edus
then the rs tree can be constructed in a bottom up way
for our chinese oriented rhetorical parsing work there is no human annotated chinese oriented discourse treebank like rst dt in english
motivated by the basic ideas of recent progress in unsupervised machine translation we propose to leverage rst dt and embed chinese text spans and english text spans into the same latent space
thus the rhetorical relation between two chinese text spans can be derived by the the rhetorical relations in rst dt
our work is unsupervised since there s no labeled chinese dataset is used
the architecture of rhetorical relation identication is shown in fig

the unsupervised rhetorical parsing model we propose is composed of two encoders an decoder and two siers
the translation encoder is responsible for encoding chinese and english texts into a latent space and the dkb encoder is used for representing domain keyword sequence
the attention based decoder and attention based decoder are the same decoder with same parameters whose only dierence is the choice of lookup tables when applying them to dierent languages
the two classiers are used for rhetorical relation identication
in fig
the components in dotted box are to constrain the model can map text pair from chinese english to english chinese
suppose t t is english chinese text pair the output of attention based decoder is chinese english t t which then will be input to the translation encoder
the output of based decoder is english chinese t t
the object of this procedure is to learn a mapping such that translations are close in the same latent space
the translation loss function is ltrans t t t t t t t t where is the sum of token level cross entropy losses
the second objective of our model is to train two classiers
when t t is english text pair we t t as its domain keyword sequence
the concatenation of the two encoders hidden states is fed into two classiers
the attention based span classier is used for determining whether t and t should be merged into a new subtree and if so the attention based relation classier is used to assign which relation and which role should be labeled to the merged node and its two children respectively
the loss function used for classication is also cross entropy loss
in this work we our propsed model is based on the sequence to sequence model with attention
these two encoders are both bidirectional gru which returns a sequence of hidden states whereas the decoder is also an gru which takes as input the previous hidden state the current word and a context vector given by a weighted sum over the encoder states
the nal loss function is l transltrans claslclas where trans and clas are hyper parameters lclas is classication loss
for inference the input is chinese edu text pair along with their domain keywords and the output is whether they can be merged into a subtree if so which rhetorical relation and which role should be labeled to the merged node
then the text of the merged node and its neighboring node s text will form the new input text pair
loop this step until a rs tree for a document has been constructed

subroutine based model for automatic text summarization in this section we present a subroutine based model for automatic text summarization which has been introduce in our previous paper
dierent from the majority of literature our subroutine based summarization model is purely based on the generated rs tree from section
the basic processing unit is edu which is relative shorter than sentence
thus the generated summary can be more informative than summary that composed of sentences
the summarization algorithm is based on importance rst principle each time the currently most important edu from rs tree will be selected one by one mechanically
in this way we can obtain a hierarchy of dierent summarizations level wise from simple to complex by adding one more edu at each level
there two ways of controlling the complexity of summarized result either by specifying the word length limit or the rate of text reduction
when going to produce a summary the summarization model traverses the rs tree in a nucleus preference way
that is a nucleus node is always preferred over its sibling satellite node if node a is preferred over node b then all child nodes of a are preferred over b the selection of edus should be alternated between the left and right subtrees of the root node whenever both subtrees are not empty
whenever a leaf node edu is traversed the text unit represented by it will be put to the nal summary
in our model a nucleus node is always preferred over its sibling satellite node
thus the generated summaries will always content balanced
all details of this subroutine based text summarization algorithm are claried in algorithm
the and is the function with f and mean the nucleus resp
satellite child nodes of
f
d p and s p are pointers pointing to the entry of subroutine d inding resp
s inding
zp
is a formal parameter for storing a pointer
for example after the subroutine call f it is zp in the subroutine body of d inding

experimental results

training details about unsupervised rhetorical parsing the training of our unsupervised rhetorical parsing was carried out on sogouca and rst dt datasets
we used a mini batch stochastic gradient descent sgd algorithm together with adam with initial learning rate
algorithm subroutine based text summarization input the rs tree of a document summary length cadence ratio r j k
output the generated edu sequences redu
if is a leaf node then put into redu and goto step else call f p
subroutine d y zp case w if j then goto step if is a non leaf node then call d y zp else put into redu call index and call zp
y s p case w if k then goto step if y is a leaf node then put y into redu k if j then goto step else call index call zp
y s p else if is non leaf node then call zp else put into redu call index call zp
s p
subroutine u y zp case w if j then goto step else call zp
y s p case w if k then goto step else call zp
s p
subroutine s y zp case w if j then call s inding y s p if then j if k then goto step else call s inding y s p if has been travelled then call u y s p else if is a leaf node then put into redu call index call u y s p else call y s p case if k then call s inding y s p if then k if j then goto step else call s inding y s p if has been travelled then call u y s p else if is a leaf node then put into redu call index call u y s p else call d s p
subroutine index if the word length of redu satises r then goto step
sort the edus in redu according to their order in the original text
return redu
to train this model
in each epoch the training data in each batch are the mixture of chinese and english text pairs
we used textrank for dkbc of rst dt
the size of word embedding for both language and gru hidden state dimensions are set to and respectively
for two decoders texts are generated using greedy decoding


unsupervised quantitative evaluation metric the commonly used evaluation metric for text summarization is rouge
rouge evaluates n gram occurrences between summary pairs
it works by comparing an automatically produced summary against a set of erence summaries
the reference summaries are typically human produced which are expensive and time consuming
it is even more dicult when facing large amounts of texts in the big data age
there is no reference summary as golden standard in our selected dataset i
e
sogouca
to build a quantization standard we propose an unsupervised evaluation metric
we consider that a faithful summary should
overlaps with title in three aspects gram domain knowledge keywords and named entities
contains more domain knowledge keywords than other non summary texts
contains more named entities than other non summary texts
the similarities between two summary texts should be lower in case of redundancy
formally for a document d in domain d whose title is t the faithful score of a generated summary s is computed as s w s s s where w rn is a scalar
b denotes the rouge score between text a and
b denotes the number of domain keywords named entities that a and b both have
denotes the number of domain keywords named entities that has
denotes the ber of edus in s
to make the score more objective the hyper parameters w were learned using linear regression on dataset
in the training step the faithful score for each golden standard is set to
nlpir
nist
gov projects duc
html table evaluation results on sogouca dataset approaches lead textrank ilp summcoder ours





















results and analysis for each generated rs tree we applied algorithm for summary generation
in what follows we present the results using our method and our comparison to previous works
since our model is unsupervised we compare it with existing unsupervised single document summarization methods
the baselines include lead selects the leading sentences in the document until length limit to form a summary which is often used as an ocial baseline of duc
textrank is a graph based text summarization model
it represents the document as a graph in which sentences are nodes and the edges between two sentences are connected based on the similarity between them
ilp is a text summarization technique which utilizes integer linear program ilp for inference under a mum coverage model
summcoder is an unsupervised framework for extracting sentences based on deep auto encoders
we generated four versions of summary word length and the rate of text
table shows the faithful score of our method and baseline approaches
our proposed framework outperforms many of the existing text summarizers on sogouca dataset in terms of our proposed faithful score such as ilp graph based approaches
the nal summaries obtained from a sample sogouca document by each summarizer i
e
lead textrank ilp summcoder and ours with word length limit are shown in table
from the summaries it can be observed that the result generated from our method is more informative than other methods
our result can summarize the results of others
the summary generated by ilp is similar to that generated by summcoder but it is dierent from those generated by textrank

concluding remarks in this paper we proposed a novel unsupervised rhetorical parsing architecture for single document extractive summarization
the proposed approach mainly contains three parts domain knowledge base construction oriented rhetorical parsing and level wise extractive summarization
to the best of our knowledge this is the rst study to adopt translation idea for rhetorical parsing
firstly we proposed a domain knowledge base construction model based on representation learning
the learned dkb can provide a panorama for a domain which has two important roles for rhetorical parsing
one is discourse segmentation and the other one is guiding rhetorical relation identication
in the unsupervised rhetorical parsing model we leveraged the idea of translation and designed a novel attention based sequence to sequence model for rhetorical relation identication
then the subroutine based ats model can accept dierent word length limit or summarization ratio and provide content balanced results based on rs tree
to evaluate our generated summary results in an unsupervised way we presented a faithful score whose hyper parameters were learned on dataset
directions for future work are many and varied
one of challenges left for the future is to further improve the performance of rhetorical parsing
such as introducing attribute grammar into the deep neural model
another important further work would be to utilize rs tree for multi document summarization
table case study with summary title airbus said it is seeking to participate in china s large aircraft project in a cooperative manner
lead xinhuanet tianjin may bolong the president of airbus china said in an exclusive interview with xinhua news agency in tianjin that airbus is negotiating with chinese partners to participate in china s large aircraft project in a cooperative manner
for the china s developing large aircraft project airbus is negotiating with chinese partners to participate in the project in a cooperative manner
textrank airbus china ltd
corporate information department provided as early as airbus and china aviation industry first group signed an agreement to transfer the series aircraft wing manufacturing technology and production line to china in stages with the goal of at the end of china was able to manufacture the complete wing structure of the family of aircraft for airbus plants in broughton and north wales england
ilp airbus is negotiating with chinese partners to participate in the project in a cooperative manner
as the fastest growing country in the world aviation market china has become the focus of competition for airbus and boeing worldwide
through the center china has undertaken a share of the latest aircraft project ocially launched by airbus on october
summcoder airbus is negotiating with chinese partners to participate in the project in a cooperative manner
both parties have also participated in the production of important parts and components on the latest models
bolong holds that china s big plane is dicult and long
airbus has spent nearly years to achieve today s achievements and now we have a strong industrial base
ours airbus is negotiating with chinese partners to participate in china s large aircraft project in a cooperative manner
as the fastest growing country in the world aviation market china has become the focus of competition for airbus and boeing worldwide
both parties have continuously increased procurement investment and technical cooperation in china
both parties have also participated in the production of important parts and components on the latest models
acknowledgments the authors would like to thank the developers of pytorch
this work was supported by the national key research and development program of china under grant and the national natural science dation of china no
and
references references aggarwal c
c

text summarization in machine learning for text
springer pp

angeli g
premkumar m
j
j
manning c
d

leveraging linguistic structure for open domain information extraction in proceedings of the annual meeting of the association for computational linguistics and the international joint conference on natural language processing volume long papers pp

bahdanau d
cho k
bengio y

neural machine translation by jointly learning to align and translate in international conference on learning representations
bharti s
k
babu k
s
pradhan a

automatic keyword extraction for text summarization in multi document e newspapers articles
european journal of advances in engineering and technology
bordes a
usunier n
garcia duran a
weston j
yakhnenko o

translating embeddings for modeling multi relational data in advances in neural information processing systems pp

carlson l
marcu d
okurowski m
e

building a discourse tagged corpus in the framework of rhetorical structure theory in current and new directions in discourse and dialogue
springer pp

cheng j
lapata m

neural summarization by extracting sentences and words in proceedings of the annual meeting of the association for computational linguistics volume long papers pp

das d

signalling of coherence relations in discourse
ph
d
thesis
simon fraser university
feng v
w
hirst g

text level discourse parsing with rich linguistic features in proceedings of the annual meeting of the association for computational linguistics long papers volume association for computational linguistics
pp

ferreira r
de souza cabral l
lins r
d
e silva g
p
freitas f
cavalcanti g
d
lima r
simske s
j
favaro l

assessing sentence scoring techniques for extractive text summarization
expert systems with applications
galitsky b
ilvovsky d

chatbot with a discourse structure driven dialogue management in proceedings of the software strations of the conference of the european chapter of the association for computational linguistics pp

gambhir m
gupta v

recent automatic text summarization techniques a survey
articial intelligence review
gillick d
favre b

a scalable global model for summarization in proceedings of the workshop on integer linear programming goldstein a
shahar y

an automated knowledge based textual summarization system for longitudinal multivariate clinical data
he h

hanlp han language processing
url
com hankcs hanlp
hernault h
prendinger h
ishizuka m
al

hilda a discourse parser using support vector machine classication
dialogue for natural langauge processing pp

journal of biomedical informatics
discourse
hirao t
yoshida y
nishino m
yasuda n
nagata m

single document summarization as a tree knapsack problem in ings of the conference on empirical methods in natural language processing pp

hoart j
suchanek f
m
berberich k
weikum g

a spatially and temporally enhanced knowledge base from wikipedia
articial intelligence
hou s
huang y
fei c
zhang s
lu r

holographic lexical chain and its application in chinese text summarization in asia pacic web apweb and web age information management waim joint conference on web and big data springer
pp

jadhav a
rajan v

extractive summarization with swap net sentences and words from alternating pointer networks in proceedings of the annual meeting of the association for computational linguistics volume long papers pp

ji y
eisenstein j

representation learning for text level discourse parsing in proceedings of the annual meeting of the association for computational linguistics volume long papers pp

ji y
smith n
a

neural discourse structure for text categorization in proceedings of the annual meeting of the association for computational linguistics volume long papers pp

joshi a
fidalgo e
alegre e
fernandez robles l

summcoder an unsupervised framework for extractive text summarization based on deep auto encoders
expert systems with applications
joty s
carenini g
ng r
mehdad y

combining intra and multi sentential rhetorical parsing for document level discourse analysis in proceedings of the annual meeting of the association for computational linguistics volume long papers pp

kong l
li c
ge j
yang y
zhang f
luo b

construction of microblog specic chinese sentiment lexicon based on tation learning in pacic rim international conference on articial intelligence springer
pp

kraus m
feuerriegel s

sentiment analysis based on rhetorical structure theory learning deep neural networks from discourse lample g
conneau a
denoyer l
ranzato m

unsupervised machine translation using monolingual corpora only in trees
expert systems with applications
tional conference on learning representations iclr
lebano l
song k
liu f

adapting the neural encoder decoder framework from single to multi document summarization in proceedings of the conference on empirical methods in natural language processing pp

lehmann j
isele r
jakob m
jentzsch a
kontokostas d
mendes p
n
hellmann s
morsey m
van kleef p
auer s
al

dbpedia a large scale multilingual knowledge base extracted from wikipedia
semantic web
lethanh h
abeysinghe g
huyck c

generating discourse structures for written texts in international conference on tional linguistics p

li j
li r
hovy e

recursive deep models for discourse parsing in proceedings of the conference on empirical methods in li p
mao k

knowledge oriented convolutional neural network for causal relation extraction from natural language texts
expert natural language processing emnlp pp

systems with applications
lin c
y

rouge a package for automatic evaluation of summaries
text summarization branches out
lin j
sun x
ma s
su q

global encoding for abstractive summarization in proceedings of the annual meeting of the association for computational linguistics volume short papers pp

lin y
liu z
sun m
liu y
zhu x

learning entity and relation embeddings for knowledge graph completion in twenty ninth aaai conference on articial intelligence
louis a
joshi a
nenkova a

discourse indicators for content selection in summarization in proceedings of the annual meeting of the special interest group on discourse and dialogue association for computational linguistics
pp

lu r
hou s
wang c
huang y
fei c
zhang s
submitted for publication
attributed rhetorical structure grammar for domain text lu r
jin x
zhang s
qiu m
wu x

a study on big knowledge and its engineering issues
ieee transactions on knowledge mann w
c
thompson s
a

rhetorical structure theory toward a functional theory of text organization
text interdisciplinary summarization
knowledge and information systems
and data engineering
journal for the study of discourse
marcu d

from discourse structures to text summaries
intelligent scalable text summarization
marcu d

the theory and practice of discourse parsing and summarization
mit press
marujo l
ling w
trancoso i
dyer c
black a
w
gershman a
matos d
m
neto j
carbonell j

automatic keyword extraction on twitter in proceedings of the annual meeting of the association for computational linguistics and the international joint conference on natural language processing volume short papers pp

mihalcea r
tarau p

textrank bringing order into text in proceedings of the conference on empirical methods in natural language processing
nallapati r
zhai f
zhou b

summarunner a recurrent neural network based sequence model for extractive summarization of documents in thirty first aaai conference on articial intelligence
niu y
xie r
liu z
sun m

improved word representation learning with sememes in proceedings of the annual meeting of the association for computational linguistics volume long papers pp

onan a
korukoglu s
bulut h

ensemble of keyword extraction methods and classiers in text classication
expert systems with applications
dierentiation in pytorch
paszke a
gross s
chintala s
chanan g
yang e
devito z
lin z
desmaison a
antiga l
lerer a

automatic ruder s

an overview of gradient descent optimization algorithms
arxiv preprint

rush a
m
chopra s
weston j

a neural attention model for abstractive sentence summarization in proceedings of the conference on empirical methods in natural language processing pp

see a
liu p
j
manning c
d

get to the point summarization with pointer generator networks in proceedings of the annual meeting of the association for computational linguistics volume long papers pp

soricut r
marcu d

sentence level discourse parsing using syntactic and lexical information in proceedings of the conference of the north american chapter of the association for computational linguistics on human language technology volume pp

timofeyev a
choi b

building a knowledge based summarization system for text data mining in international cross domain conference for machine learning and knowledge extraction springer
pp

toloski m
brooke j
taboada m

a syntactic and lexical based discourse segmenter in proceedings of the acl ijcnlp conference short papers association for computational linguistics
pp

wang q
mao z
wang b
guo l

knowledge graph embedding a survey of approaches and applications
ieee transactions on wang z
zhang j
feng j
chen z

knowledge graph embedding by translating on hyperplanes in twenty eighth aaai wu y
hu b

learning to extract coherent summary via deep reinforcement learning in thirty second aaai conference on knowledge and data engineering
conference on articial intelligence
articial intelligence
yaqoob i
hashem i
a
t
gani a
mokhtar s
ahmed e
anuar n
b
vasilakos a
v

big data from beginning to future
zahri n
a
h
fukumoto f
suguru m
lynn o
b

exploiting rhetorical relations to multiple documents text summarization
international journal of information management
international journal of network security its applications
zhendong d
qiang d

hownet and the computation of meaning with cd rom
world scientic
zheng l

applications of big knowledge summarization
ph
d
thesis
new jersey institute of technology
zhou q
yang n
wei f
huang s
zhou m
zhao t

neural document summarization by jointly learning to score and select sentences in proceedings of the annual meeting of the association for computational linguistics volume long papers pp


