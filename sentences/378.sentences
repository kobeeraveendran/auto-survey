scientic document summarization for laysumm and longsumm sayar ghosh roy nikhil pinnaparaju risubh jain manish gupta and vasudeva varma information retrieval and extraction lab international institute of information technology hyderabad india sayar
ghosh nikhil

iiit
ac
in risubh

iiit
ac
in manish
gupta
ac
in abstract text summarization has been automatic task in widely studied as an important traditionally natural language processing
various feature engineering and machine learning based systems have been proposed for extractive as well as abstractive text summarization
recently deep learning based specically transformer based systems have been immensely popular
summarization is a cognitively challenging task extracting summary worthy sentences is laborious and expressing semantics in brief when doing abstractive summarization is complicated
in this paper we specically look at the problem of summarizing scientic research papers from multiple domains
we differentiate between two types of summaries namely laysumm a very short summary that captures the essence of the research paper in layman terms restricting overtly specic technical jargon and longsumm a much longer detailed summary aimed at providing specic insights into various ideas touched upon in the paper
while leveraging latest transformer based models our systems are simple intuitive and based on how paper sections contribute to human maries of the two types described above
evaluations against gold standard summaries using rouge lin metrics prove the effectiveness of our approach
on blind test corpora our system ranks rst and third for the longsumm and laysumm tasks respectively
introduction popularity of data science in recent years has led to a massive growth in the number of published papers online
this has generated an epochal change in the way we retrieve analyze and consume tion from these papers
also wider interest in data science implies even lay persons readers outside the author also works as a researcher at microsoft the data science community are signicantly ested in keeping up with the latest developments
the readers have access to a huge amount of such research papers on the web
for a human standing large documents and assimilating crucial information out of them is often a laborious and time consuming task
motivation to make a concise representation of huge text while retaining the core meaning of the original text has led to the ment of various automated summarization systems
these systems provide users ltered high quality and concise content to work with at an dented scale and speed
summarization methods are mainly classied into two categories extractive and abstractive
extractive methods aim to select salient phrases sentences or elements from the text while abstractive techniques focus on generating summaries from scratch without the constraint of reusing phrases from the original text
scientic papers are large complex documents that tend to be geared towards a particular audience
this is a very small percentage of the population while majority of individuals are unable to fully comprehend the contents of long scientic ments
even among the people who are able to understand the material the length of such ments often spanning several pages demand a great deal of time and attention
hence tasks like layman summarization laysumm and long form rization longsumm are of great importance in today s world
typically scientic research papers are fairly structured documents containing standard sections like abstract introduction background related work experiments results discussion conclusion and acknowledgments
thus summarization of such documents should be aware of such sectional structure
an intuitive way is to pick a few tences from each of the sections to be a part of the summary
but how do we decide how many sentences to pick from each section also which n a j l c
s c v
v i x r a sentences to pick can we rewrite sentences so as to obtain a concise abstractive summary we investigate answers to these questions in this paper
multiple survey papers have provided a detailed overview of the automatic text summarization task tas and kiyani nenkova and eown allahyari et al

most of the practically usable summarization systems are tractive in nature
also most summarization ies have focused on summarization of news articles
in this work we mainly focus on two interesting pects of text summarization summarization of scientic research papers and summarization for laymen
cohan et al
propose that section level processing of scientic documents is useful
ther collins et al
conclude that not all sections are equally useful
also recent papers have observed that a hierarchical summarization of scientic documents is highly effective where at the rst level an extractive summary of each section is independently generated and at the ond level the sectional output is abstracted into a brief summary subramanian et al
erera et al

xiao and carenini observe that while summarizing local context is useful but global is not
thus in our approach at the sectional level we use extracted information from only within the section text to obtain a section s extractive summary ignoring the remaining text of the entire paper
for the laysumm task we observe that abstract is the most relevant section of a scientic paper from a layman perspective
we therefore feed the abstract to a transformer based model and ate an abstractive summary for the laysumm task
for the longsumm task we rst perform tive summarization for each section and choose a selected number of sentences from each section into the nal summary
on blind test corpora of and papers for the laysumm and longsumm tasks our proposed system leads to a rouge of
and
respectively
these results helped us bag the top positions on the leaderboards for the two tasks
related work in this section we discuss related areas including text summarization and style transfer

automatic text summarization text summarization focuses on summarizing a given document and obtaining its key information bits
there are two types of text summarization methods extractive summarization and tive summarization


extractive summarization extractive summarization deals with extracting pieces of text directly from the input document
tractive summarization can also be seen as a text classication task where we try to predict whether a given sentence will be part of the summary or not liu
most papers in this area focus on the summarization of news articles
but several others focus on specic domains like tion of medical documents legal documents entic documents
summarization can also be performed in a query sensitive manner or a user centric manner
sentence scoring methods include graph based methods like lexrank erkan and radev or textrank mihalcea and rau machine learning or deep learning techniques and position based methods
recently various deep learning architectures such as ert zhang et al
bertsum liu and lapata summarunner nallapati et al
csti singh et al
and hybrid net singh et al
have been proposed for extractive summarization


abstractive summarization in abstractive summarization the model tries to generate the summary instead of extracting tences or keywords
as compared to tive summarization this is more challenging and requires strong language modeling schemes to achieve good results
traditionally abstractive summarization techniques have focused on erating short text such as headlines or titles
but more recently there have been efforts on ation of longer summaries
older methods have depended on tree transduction rules cohn and pata and quasi synchronous grammar proaches woodsend and lapata for tive abstractive summarization
recently neural summarization approaches have been found to be more effective
effective neural representative language models are very important for text eration tasks
with the recent breakthrough of transformer based vaswani et al
tectures like bert devlin et al
raffel summary
a set of research papers were vided as the blind test data
the laysumm dataset comprises of full text papers with lay summaries in a variety of domains epilepsy archeology and materials engineering and from a number of nals
elsevier made available a collection of lay summaries from a multidisciplinary collection of journals as well as their abstracts and full texts
for a small sample dataset look at laysumm s ofcial github

longsumm dataset the corpus for this task includes a training set that consists of extractive summaries and abstractive summaries of scientic papers in the domains of natural language processing and machine learning
the extractive summaries are based on video talks from associated ences lev et al
while the abstractive maries are blog posts created by nlp and ml searchers
the average gold summary length was tokens
the research papers were parsed ing the science library
a collection of pdfs of research papers served as the blind test set
the longsumm train and test datasets are publicly accessible on longsumm s ofcial github
system overview in this section we present an overview of the posed systems for the laysumm and longsumm tasks

system overview for laysumm we observed that the laysumm summaries in the train set were highly abstractive in nature with a length limit of words
in fig
we alyze how information from each paper section contributes to the nal lay summary by ing the rouge overlap between a paper section and the available gold summary
this analysis is performed for the entire dataset
fig
shows that the abstract was the most signicant section followed by the conclusion
moreover a tively high rouge l overlap indicates some gree of verbatim copying from the abstract onto
com wing scisumm corpus blob master readme laysumm

com figure and rouge l laps between paper sections and laysumm summary figure and rouge l laps between paper sections and longsumm summary al
and bart lewis et al
ing these types of models is crucial for obtaining good textual representations on the target side for neural abstractive summarization

text style transfer neural text style transfer is yet another related area of work where the document in style a is converted to style b without any loss of content or tics syed et al
vadapalli et al

this work leverages transformer encoder decoder els
the text encoder is used to obtain robust latent representations while the decoder generates text with a particular target style
datasets we rst describe the datasets which were provided by the organizers of the workshop on scholarly document processing

laysumm dataset a dataset of research papers and ing gold standard lay summaries were available for training tokens being the average length of a
github
io science parse sharedtasks
html
com guyfe longsumm




scoresmean




scoresmean




scoresmean




scoresmean




scoresmean




scoresmean rl the lay summary
in addition to providing a high rouge overlap the conclusion section was atively shorter in length
this indicates that the conclusion section contains a great degree of ful information in a more condensed fashion
note that we picked the paper sections directly from the paper text without performing any orate conation on section headers
conation in general should not hurt the performance of our models since a particular paper will contain only one form of the section heading e

it will tain either materials and methods or methods
however we plan to explore deeper section wise analysis using improved conation as part of future work
we leveraged pretrained transformer models for conditional generation given a set of individual tions
our results indicate that using abstract as the only sequence for conditional generation is a ter choice as compared to utilizing more sections
therefore the problem at hand is one of ing salient information as one would expect from a summarization task with the additional avor of text style transfer
figure system architecture for longsumm
system overview for longsumm we performed a similar section contribution uation and considered section headings which peared in at least of all papers in our training set for the longsumm task
fig
shows that the introduction is the most important section when it comes to creating summaries followed by related work and results
for our summary generation architecture we considered one section at a time without the global context
as discussed earlier this was guided by isting scientic evidence from xiao and carenini which showed that not considering the global context and focusing purely on the section at hand is marginally better than doing otherwise
based on section contribution evaluations we constructed a budget module to calculate how much weight to assign to a section for the purpose of combining tion summaries into the nal long summary
fig
illustrates the broad architecture of our proposed system
for summarizing each individual section we used summarunner nallapati et al
a ple neural extractive summarizer
we pre trained summarunner on the pubmed cohan et al
dataset to generate paper abstracts from ious paper sections
we show results using tions in our budget module setting various cutoff thresholds for overlap in order for a section to be considered for the summary i
e
we ignore a section if overlap is less than the threshold value
the system performance indicates that even with a fairly simple neural summarizer at the base our architecture is capable of achieving superior results on a blind test dataset
experimental settings we used hugging implementation of and bart
we experimented with various length settings for training and generation
we found that minimum and maximum sequence lengths of and respectively for generation gave us the best results
we used adam optimizer with an tial learning rate of with learn rate scheduling based on values calculated on the dation split
in the hyper parameter tuning phase a repetition penalty of
while generating lay maries provided the most optimal results
we used hpzhao s implementation of with default hyper parameter values
the topk parameter was dynamically adjusted to set the summary length of each section based on the section specic budget
results our system with the team name summaformers ranks rst and third for the and tasks respectively
further details on these tasks can be found in the shared tasks overview paper chandrasekaran et al

in this section we present detailed results


com hpzhao summarunner
draco
res
ibm

codalab



abstractsummarized introductionsummarized conclusion


long summary


budget module method baseline base base base small r l recall recall r l recall

































































table laysumm results best results are highlighted in bold method section cutoff at
section cutoff at
section cutoff at
section cutoff at
post proc r l recall recall r l recall























table longsumm results best results are highlighted in bold
lay summary generation we experimented with the bart large cnn model which is pre trained on the cnn dailymail rization dataset and with base in summarization mode
we ne tuned the conditional generation architectures of these models using the available laysumm train corpus of documents which we split into training and validation splits in a ratio
our initial results proved the superiority of large cnn bartl over
we experimented with various generative sources such as abstract only abstract conclusion abstract conclusion introduction abstract conclusion introduction methods
furthermore owing to the structure of the abstract itself we considered the rst second and nal paragraphs of the abstract also referred to as small abs as the source
our results ble show that using the complete abstract as input to bartl is the best performing setting for laysumm
since the papers in the dataset were published in various scientic journals the original abstracts contain highly domain specic technical
the bartl model captures the salient points from the abstract in a short word budget while transferring the text style from scientic to a layman style
after hyper parameter tuning on the generation end we achieved a score of
on the blind test corpus
our generated summaries are coherent in addition to being highly abstractive in nature
for comparison we also present results for a nave baseline which outputs the rst tokens of the abstract as the summary
as shown in table surprisingly this simple line leads to impressive results especially on recall metrics
running summarunner on the abstract leads to results which are worse than the baseline

long summary generation we used the summarunner nallapati et al
neural extractive summarization system as our base section summarizer
we pretrained this on the ing set of the publicly available pubmed dataset using glove pennington et al
word embeddings to generate the paper abstract as closely as possible from any given section
this grounds the network in a setting where it can easily capture salient points
we plan to explore ing with other datasets as part of future work
this was further netuned using the longsumm train set as follows
the given longsumm training dataset was divided into train and validation splits in a ratio
we used the same previous settings to netune on documents in the longsumm train split
now the pretrained summarunner model was conditioned to extract sentences which mize the overlap with the provided gold standard long summaries
finally based on our budget module we assign a weight to each available section and generate section summaries of computed lengths which are further concatenated to generate the nal summary
we experiment with various settings in the weight assignment based on specied overlap cutoffs in the budget module as shown in table
the best forming setting corresponds to selecting sections whose overlap with the long summary is greater than

intuitively this prunes out irrelevant sections such as abbreviations and knowledgements
the remaining sections were assigned weights based on the overlap with the provided long summary
the generated long summaries are extractive and capture the most salient pieces of information from the given search papers
the results improve slightly when we perform post processing using heuristics like removing paper citations within brackets moving non english unicode characters and ematical notation symbols

case studies in the following we present two cases of lay maries generated by our system
as we can see the generated summaries are highly abstractive and coherent
they also capture the important aspects of the paper
for the article at this the generated lay summary was as follows this paper proposes a novel approach to support the transformation of bioinformatics data into linked open data lod
it denes competency questions that drive not only the denition of transformation rules but also the data transformation and exploration afterwards
the paper also presents a support toolset and scribes the successful application of the proposed approach in the functional genomics domain
cording to this approach a set of competency teria drive the transformation process
this paper presents a framework for the development of an open data management system that can be easily adapted to different data types
for the article at this the generated lay

j
engappai


j
engappai


summary was as follows to foster interaction autonomous robots need to understand the ronment in which they operate
one of the main challenges is semantic segmentation together with the recognition of important objects which can aid robots during exploration as well as when planning new actions and interacting with the environment
in this study we extend a multi view semantic mentations system based on entangled forests by integrating and rening two object tectors mask r cnn and you only look once yolo with bayesian fusion and iterated graph cuts
the new system takes the best of its ponents successfully exploiting both and data
finally the following lay summary was ated by our model for this very paper in this paper we develop a novel system for summarizing scientic research papers from multiple domains
we differentiate between two types of summaries namely laysumm a very short summary that captures the essence of the research paper in man terms restricting overtly specic technical gon and longsumm a much longer detailed summary aimed at providing specic insights into various ideas touched upon in the paper
while leveraging latest transformer based models our systems are simple intuitive and based on how cic paper sections contribute to human summaries of the two types described above
conclusions in this paper we studied two scientic document summarization tasks laysumm and longsumm
we experimented with popular text neural models in a section aware manner
our results indicate that modeling of the document structure with strong focus on which parts of a research paper to attend to while composing a summary gives a signicant boost to the quality of the resultant output
on blind test corpora our system ranks rst and third for the longsumm and laysumm tasks respectively
references mehdi allahyari seyedamin pouriyeh mehdi asse saeid safaei elizabeth d trippe juan b text rez and krys kochut

arxiv preprint tion techniques a brief survey


m
k
chandrasekaran g
feigenblat hovy
e
and a
ravichander m
shmueli scheuer a
de waard

overview and insights from scientic document summarization shared tasks cl scisumm laysumm and longsumm
in proceedings of the first workshop on scholarly document processing sdp page forthcoming
arman cohan franck dernoncourt doo soon kim trung bui seokhwan kim walter chang and zli goharian

a discourse aware attention model for abstractive summarization of long ments
arxiv preprint

trevor cohn and mirella lapata

sentence in proceedings pression beyond word deletion
of the international conference on tional linguistics coling pages
ed collins isabelle augenstein and sebastian riedel

a supervised approach to extractive arxiv preprint marisation of scientic papers


jacob devlin ming wei chang kenton lee and kristina toutanova

bert pre training of deep bidirectional transformers for language ing
arxiv preprint

shai erera michal shmueli scheuer guy feigenblat ora peled nakash odellia boni haggai roitman doron cohen bar weiner yosi mass or rivlin et al

a summarization system for scientic documents
arxiv preprint

gunes erkan and dragomir r radev

lexrank graph based lexical centrality as salience in text summarization
journal of articial intelligence search
guy lev michal shmueli scheuer jonathan herzig achiya jerbi and david konopnicki

summ a dataset and scalable annotation method for scientic paper summarization based on conference talks
arxiv preprint

mike lewis yinhan liu naman goyal jan ghazvininejad abdelrahman mohamed omer levy ves stoyanov and luke zettlemoyer

bart denoising sequence to sequence pre training for natural language generation translation and comprehension
arxiv preprint

chin yew lin

rouge a package for automatic evaluation of summaries
page
yang liu

fine tune bert for extractive rization
arxiv preprint

yang liu and mirella lapata

text in proceedings of tion with pretrained encoders
the conference on empirical methods in ural language processing and the international joint conference on natural language processing emnlp ijcnlp pages
rada mihalcea and paul tarau

textrank ing order into text
in proceedings of the ference on empirical methods in natural language processing pages
ramesh nallapati feifei zhai and bowen zhou

summarunner a recurrent neural network based quence model for extractive summarization of ments
arxiv preprint

ani nenkova and kathleen mckeown

a in mining vey of text summarization techniques
text data pages
springer
jeffrey pennington richard socher and christopher d manning

glove global vectors for word resentation
in proceedings of the conference on empirical methods in natural language ing emnlp pages
colin raffel noam shazeer adam roberts katherine lee sharan narang michael matena yanqi zhou wei li and peter j liu

exploring the limits of transfer learning with a unied text to text former
arxiv preprint

abhishek kumar singh manish gupta and vasudeva varma

hybrid memnet for extractive rization
in proceedings of the acm on ence on information and knowledge management pages
abhishek kumar singh manish gupta and vasudeva varma

unity in diversity learning tributed heterogeneous sentence representation for extractive summarization
in aaai
sandeep subramanian raymond li jonathan lault and christopher pal

on tive and abstractive neural document summarization with transformer language models
arxiv preprint

bakhtiyar syed gaurav verma balaji vasan vasan anandhavelu natarajan and vasudeva varma

adapting language models for non parallel in aaai pages author stylized rewriting

oguzhan tas and farzad kiyani

a survey matic text summarization
pressacademia procedia
raghuram vadapalli bakhtiyar syed nishant prabhu balaji vasan srinivasan and vasudeva varma

when science journalism meets articial in gence an interactive demonstration
ings of the conference on empirical methods in natural language processing system strations pages
ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan n gomez ukasz kaiser and illia polosukhin

attention is all in advances in neural information you need
cessing systems pages
kristian woodsend and mirella lapata

ing to simplify sentences with quasi synchronous grammar and integer programming
in proceedings of the conference on empirical methods in natural language processing pages
wen xiao and giuseppe carenini

tive summarization of long documents by arxiv preprint bining global and local context


xingxing zhang furu wei and ming zhou

hibert document level pre training of cal bidirectional transformers for document in proceedings of the annual rization
ing of the association for computational linguistics pages

