leafnats an open source toolkit and live demo system for neural abstractive text summarization tian shi virginia tech
edu ping wang virginia tech
edu chandan k
reddy virginia tech
vt
edu a m l c
s c v
v i x r a abstract neural abstractive text summarization nats has received a lot of attention in the past few years from both industry and academia
in this paper we introduce an open source toolkit namely leafnats for training and tion of different sequence to sequence based models for the nats task and for ing the pre trained models to real world cations
the toolkit is modularized and tensible in addition to maintaining tive performance in the nats task
a live news blogging system has also been mented to demonstrate how these models can aid blog news editors by providing them gestions of headlines and summaries of their articles
introduction being one of the prominent natural language generation tasks neural abstractive text rization nats has gained a lot of ity rush et al
see et al
paulus et al

different from extractive text marization gambhir and gupta nallapati et al
verma and lee nats lies on modern deep learning models particularly sequence to sequence models to erate words from a vocabulary based on the resentations features of source documents rush et al
nallapati et al
so that it has the ability to generate high quality summaries that are verbally innovative and can also easily corporate external knowledge see et al

many nats models have achieved better mance in terms of the commonly used tion measures such as rouge lin score compared to extractive text summarization proaches paulus et al
celikyilmaz et al
gehrmann et al

we recently provided a comprehensive survey of the models shi et al
ing their network structures parameter inference methods and decoding generation approaches for the task of abstractive text summarization
a riety of nats models share many common erties and some of the key techniques are widely used to produce well formed and human readable summaries that are inferred from source articles such as encoder decoder framework sutskever et al
word embeddings mikolov et al
attention mechanism bahdanau et al
pointing mechanism vinyals et al
and beam search algorithm rush et al

many of these features have also found tions in other language generation tasks such as machine translation bahdanau et al
and dialog systems serban et al

in addition other techniques that can also be shared across different tasks include training strategies fellow et al
keneshloo et al
zato et al
data pre processing results processing and model evaluation
therefore ing an open source toolbox that modularizes ferent network components and unies the ing framework for each training strategy can et researchers in language generation from ious aspects including efciently implementing new models and generalizing existing models to different tasks
in the past few years different toolkits have been developed to achieve this goal
some of them were designed specically for a single task such as parlai miller et al
for dialog search and some have been further extended to other tasks
for example opennmt klein et al
and xnmt neubig et al
are marily for neural machine translation nmt but have been applied to other areas
the bottom up attention model gehrmann et al
which has achieved state of the art performance for stractive text summarization is implemented in
there are also several other general purpose language generation packages such as texar hu et al

compared with these toolkits leafnats is specically designed for nats research but can also be adapted to other tasks
in this toolkit we implement an end end training framework that can minimize the fort in writing codes for training evaluation dures so that users can focus on building models and pipelines
this framework also makes it easier for the users to transfer pre trained parameters of user specied modules to newly built models
in addition to the learning framework we have also developed a web application which is driven by databases web services and nats models to show a demo of deploying a new nats idea to a real life application using leafnats
such an plication can help front end users e

blog news authors and editors by providing suggestions of headlines and summaries for their articles
the rest of this paper is organized as follows section introduces the structure and design of leafnats learning framework
in section we describe the architecture of the live system demo
based on the request of the system we propose and implement a new model using leafnats for headline and summary generation
we conclude this paper in section
leafnats in this section we introduce the structure and sign of leafnats toolkit which is built upon the lower level deep learning platform torch paszke et al

as shown in fig
it consists of four main components i
e
engines modules data and tools and playground
engines in leafnats an engine represents a training algorithm
for example end to end training see et al
and adversarial ing goodfellow et al
are two different training frameworks
therefore we need to velop two different engines for them
specically for leafnats we implement a task independent end to end training engine for nats but it can also be adapted to other nlp tasks such as nmt question answering ment classication
the engine uses stract data models pipelines and loss functions figure the framework of leafnats toolkit
to build procedures of training validation ing evaluation and application respectively so that they can be completely reused when menting a new model
for example these dures include saving loading check point les ing training selecting n best models during idation and using the best model for generation during testing
another feature of this engine is that it allows users to specify part of a neural network to train and reuse parameters from other models which is convenient for transfer learning
modules modules are the basic building in leafnats we blocks of different models
provide ready to use modules for constructing current neural network to sequence models for nats e

pointer generator network see et al

these modules include embedder rnn encoder attention luong et al
temporal tion nallapati et al
attention on decoder paulus et al
and others
we also use these basic modules to assemble a pointer generator coder module and the corresponding beam search the embedder can also be used algorithms
to realize the embedding weights sharing nism paulus et al

data and tools different models in nats are tested on three datasets see table namely cnn daily mail cnn dm hermann et al
newsroom grusky et al
and
the pre processed cnn dm data is available
here we provide tools to process the last two datasets
data modules are used to prepare the input data for mini batch mization
playground with the engine and modules we can develop different models by just assembling
com
com
com leafnats process data of cnn dailymail dataset cnn dm newsroom bytecup table basic statistics of the datasets used
validation train test these modules and building pipelines in ground
we re implement different models in the nats toolkit shi et al
to this framework
the performance rouge scores lin of the pointer generator model on different datasets has been reported in table where we nd that most of the results are better than our previous plementations shi et al
due to some minor changes to the neural network
model dataset pointer generator newsroom s newsroom h pointer generator pointer generator coverage pointer generator cnn dm bytecup









r l




table performance of our implemented generator network on different datasets
s and represent newsroom summary and headline datasets respectively
a live system in this section we present a real world web cation of the abstractive text summarization els which can help front end users to write lines and summaries for their articles posts
we will rst discuss the architecture of the system and then provide more details of the front end design and a new model built by leafnats that makes automatic summarization and headline generation possible

architecture this is a news blog website which allows people to read duplicate edit post delete and comment articles
it is driven by web services databases and our nats models
this web application is developed with php html css and jquery lowing the concept of model view controller see fig

in this framework when people interact with the front end views they send html requests to controllers that can manipulate models
then the views will be changed with the updated tion
for example in nats we rst write an ticle in a text area
then this article along with
cs
vt
edu leafnats figure the architecture of the live system
the summarization request will be sent to the troller via jquery ajax call
the controller municates with our nats models asynchronously via json format data
finally generated lines and summaries are shown in the view

design of frontend fig
presents the front end design of our web application for creating a new post where labels represent the sequence of actions
in this website an author can rst click on new post step to bring a new post view
then he she can write content of an article in the corresponding text area step without specifying it s headline and lights i
e
summary
by clicking nats ton step and waiting for a few seconds he she will see the generated headlines and highlights for the article in a new tab on the right hand side of the screen
here each of the buttons in gray color denotes the resource of the training data
for example bytecup means the model is trained with bytecup headline generation dataset
the tokenized article content is shown in the bottom
apart from plain text headlines and highlights our system also enables users to get a visual standing of how each word is generated via tention weights luong et al

when ing the mouse tracker step on any token in the headlines or highlights related content in the ticle will be labeled with red color
if the author would like to use one of the suggestions he she can click on the gray button step to add it to the text area on the left hand side and edit it
nally he she can click post step to post the article

the proposed model as shown in the fig
our system can suggest to the users two headlines based on newsroom headline and bytecup datasets and summaries based on newsroom summary and cnn dm datasets
they are treated as four tasks in this section
to achieve this goal we use the figure front end design of the live demonstration of our system
dataset newsroom s newsroom h cnn dm bytecup model multi task multi task transfer coverage transfer









r l




table performance of our model
corresponding testing sets are shown in table
from the table we observe that our model forms better in headline generation tasks
ever the rouge scores in summarization tasks are lower than the models without sharing ding encoder and output layers
it should be noted that by sharing the parameters this model requires less than million parameters to achieve such performance
conclusion in this paper we have introduced a leafnats toolkit for building training testing evaluating and deploying nats models as well as a live news blogging system to demonstrate how the nats models can make the work of writing lines and summaries for news articles more cient
an extensive set of experiments on ent benchmark datasets has demonstrated the fectiveness of our implementations
the newly proposed model for this system has achieved petitive results with fewer number of parameters
figure overview of the model used to generate lines and summaries
ules provided in leafnats toolkit to assemble a new model see fig
which has a shared ding layer a shared encoder layer a task specic encoder decoder bi lstm encoder and generator decoder layer and a shared output layer
to train this model we rst build a multi task learning pipeline for newsroom dataset to learn parameters for the modules that are colored in ange in fig
because articles in this dataset have both headlines and highlights the size of the dataset is large and the articles come from a variety of news agents
then we build a fer learning pipeline for cnn daily and cup dataset and learn the parameters for modules labeled with blue and green color respectively
with leafnats we can accomplish this work ciently
the performance of the proposed model on the acknowledgments this work was supported in part by the us tional science foundation grants and
references dzmitry bahdanau kyunghyun cho and yoshua gio

neural machine translation by jointly arxiv preprint learning to align and translate


asli celikyilmaz antoine bosselut xiaodong he and yejin choi

deep communicating agents for in proceedings of the abstractive summarization
conference of the north american chapter of the association for computational linguistics man language technologies volume long pers volume pages
mahak gambhir and vishal gupta

recent matic text summarization techniques a survey
ticial intelligence review
sebastian gehrmann yuntian deng and alexander rush

bottom up abstractive summarization
in proceedings of the conference on cal methods in natural language processing pages
ian goodfellow jean pouget abadie mehdi mirza bing xu david warde farley sherjil ozair aaron courville and yoshua bengio

generative in advances in neural information versarial nets
processing systems pages
max grusky mor naaman and yoav artzi

newsroom a dataset of
million summaries with diverse extractive strategies
in proceedings of the conference of the north american chapter of the association for computational linguistics man language technologies volume long pers volume pages
karl moritz hermann tomas kocisky edward grefenstette lasse espeholt will kay mustafa leyman and phil blunsom

teaching chines to read and comprehend
in advances in ral information processing systems pages
zhiting hu haoran shi zichao yang bowen tan tiancheng zhao junxian he wentao wang xingjiang yu lianhui qin di wang al

texar a modularized versatile and arxiv preprint ble toolkit for text generation


yaser keneshloo tian shi chandan k reddy and naren ramakrishnan

deep reinforcement learning for sequence to sequence models
arxiv preprint

guillaume klein yoon kim yuntian deng jean senellart and alexander rush

opennmt open source toolkit for neural machine translation
proceedings of acl system demonstrations pages
chin yew lin

rouge a package for matic evaluation of summaries
text summarization branches out
thang luong hieu pham and christopher d ning

effective approaches to attention based in proceedings of the neural machine translation
conference on empirical methods in natural language processing pages
tomas mikolov ilya sutskever kai chen greg s rado and jeff dean

distributed tions of words and phrases and their in advances in neural information processing ity
systems pages
alexander miller will feng dhruv batra antoine bordes adam fisch jiasen lu devi parikh and jason weston

parlai a dialog research in proceedings of the ware platform
ference on empirical methods in natural language processing system demonstrations pages
ramesh nallapati feifei zhai and bowen zhou

summarunner a recurrent neural network based quence model for extractive summarization of ments
in thirty first aaai conference on articial intelligence
ramesh nallapati bowen zhou cicero dos santos c a glar and bing xiang

tive text summarization using sequence to sequence rnns and beyond
conll page
graham neubig matthias sperber xinyi wang matthieu felix austin matthews sarguna manabhan ye qi devendra singh sachan philip arthur pierre godard al

xnmt the tensible neural machine translation toolkit
vol
mt researchers track page
adam paszke sam gross soumith chintala gory chanan edward yang zachary devito ing lin alban desmaison luca antiga and adam lerer

automatic differentiation in pytorch
romain paulus caiming xiong and richard socher

a deep reinforced model for abstractive marization
arxiv preprint

marcaurelio ranzato sumit chopra michael auli and wojciech zaremba

sequence level ing with recurrent neural networks
arxiv preprint

alexander m rush sumit chopra and jason weston

a neural attention model for abstractive in proceedings of the tence summarization
conference on empirical methods in natural guage processing pages
abigail see peter j
liu and christopher d
manning

get to the point summarization with generator networks
in proceedings of the nual meeting of the association for computational linguistics volume long papers pages
association for computational linguistics
iulian vlad serban alessandro sordoni yoshua gio aaron c courville and joelle pineau

building end to end dialogue systems using ative hierarchical neural network models
tian shi yaser keneshloo naren ramakrishnan and chandan k reddy

neural abstractive text summarization with sequence to sequence models
arxiv preprint

ilya sutskever oriol vinyals and quoc v le

sequence to sequence learning with neural works
in advances in neural information ing systems pages
rakesh m
verma and daniel lee

extractive summarization limits compression generalized model and heuristics
computacion y sistemas
oriol vinyals meire fortunato and navdeep jaitly

pointer networks
in advances in neural formation processing systems pages

