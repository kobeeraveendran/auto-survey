literature retrieval for precision medicine with neural matching and faceted summarization jiho noh department of computer science university of kentucky kentucky usa jiho

edu ramakanth kavuluru division of biomedical informatics university of kentucky kentucky usa ramakanth

edu c e d l c
s c v
v i x r a abstract ir information retrieval for precision medicine pm often involves looking for multiple pieces of evidence that characterize a patient case
this typically includes at least the name of a condition and a genetic variation that applies to the patient
other factors such as demographic attributes comorbidities and social determinants may also be pertinent
as such the retrieval problem is often formulated as search but with multiple facets e

disease mutation that may need to be incorporated
in this paper we present a document reranking approach that combines neural query document matching and text summarization toward such retrieval scenarios
our architecture builds on the basic bert model with three specic components for reranking
document query matching
keyword extraction and c
facet conditioned abstractive summarization
the outcomes of b and c are used to essentially transform a candidate document into a concise summary that can be compared with the query at hand to compute a relevance score
component directly generates a matching score of a candidate document for a query
the full architecture benets from the complementary potential of document query matching and the novel document transformation approach based on summarization along pm facets
evaluations using nist s trec pm track datasets show that our model achieves state of the art performance
to foster reproducibility our code is made available here
com bionlproc t ext summ for doc retrieval
introduction the u
s
nih s precision medicine pm tive collins and varmus calls for designing treatment and preventative interventions ing genetic clinical social behavioral and vironmental exposure variability among patients
the initiative rests on the widely understood ing that considering individual variability is critical in tailoring healthcare interventions to achieve stantial progress in reducing disease burden wide
cancer was chosen as its near term focus with the eventual aim of expanding to other tions
as the biomedical research enterprise strives to fulll the initiative s goals computing needs are also on the rise in drug discovery predictive modeling for disease onset and progression and in building nlp tools to curate information from the evidence base being generated

trec precision medicine series facet input disease genetic variation braf k demographics melanoma year old female disease genetic variation amplication demographics year old male gastric cancer table example cases from trec pm dataset in a dovetailing move the u
s
nist s trec text retrieval conference has been running a pm track since with a focus on cancer roberts et al

the goal of the trec pm task is to identify the most relevant biomedical articles and clinical trials for an input patient case
each case is composed of a disease name a gene name and genetic variation type and demographic information sex and age
table shows two ample cases from the track
so the search is in the sense that we have a free text input in each facet but the facets themselves highlight the pm related attributes that ought to ize the retrieved documents
we believe this style of faceted retrieval is going to be more common across medical ir tasks for many conditions as the pm initiative continues its mission
retriever from a solr index grainger and potter of the corpora

vocabulary mismatch and neural ir the vocabulary mismatch problem is a prominent issue in medical ir given the large variation in the expression of medical concepts and events
for example in the query what is a potential side effect for tymlos the drug is referred by its brand name
relevant scientic literature may contain the generic name abaloparatide more frequently
traditional document search engines have clear limitations on resolving mismatch issues
the ir community has extensively explored methods to address the vocabulary mismatch problem ing query expansion based on relevance feedback query term re weighting or query reconstruction by optimizing the query syntax
several recent studies highlight exploiting ral network models for query renement in ment retrieval dr settings
nogueira and cho address this issue by generating a formed query from the initial query using a neural model
they use reinforcement learning rl to train it where an agent i
e
reformulator learns to reformulate the initial query to maximize the pected return i
e
retrieval performance through actions i
e
generating a new query from the put probability distribution
in a different proach narayan et al
use rl for sentence ranking for extractive summarization

our contributions in this paper building on the bert ture devlin et al
we focus on a different brid document scoring and reranking setup ing three components
a document relevance classication model which predicts and ently scores whether a document is relevant to the given query using a bert multi sentence setup b
a keyword extraction model which spots tokens in a document that are likely to be seen in pm lated queries and c
an abstractive document marization model that generates a pseudo query given the document context and a facet type e

genetic variation via the bert encoder decoder setup
the keywords from b and the query from c are together compared with the original query to generate a score
the scores from all the components are combined to rerank top set to documents returned with a basic okapi our main innovation is in pivoting from the cus on queries by previous methods to emphasis on transforming candidate documents into queries via summarization
additionally while generating the pseudo query we also let the coder output concept codes from biomedical nologies that capture disease and gene names
we do this by embedding both words and concepts in a common semantic space before letting the decoder generate summaries that include concepts
our overall architecture was evaluated using the pm datasets with the dataset used as the test set
the results show an absolute improvement in compared to prior best approaches while obtaining a small gain in r prec
qualitative analyses also highlight how the summarization is able to focus on document segments that are highly relevant to patient cases
background the basic reranking architecture we begin with is the bidirectional encoder representations from transformers bert devlin et al
model
bert is trained on a masked language eling objective on a large text corpus such as wikipedia and bookscorpus
as a sequence eling method it has achieved state of the art sults in a wide range of natural language standing nlu tasks including machine tion conneau and lample and text rization liu and lapata
with an additional layer on top of a pretrained bert model we can ne tune models for specic nlu tasks
in our study we utilize this framework in all three ponents identied in section
by starting with a bert base uncased pretrained huggingface model wolf et al


text summarization we plan to leverage both extractive and tive candidate document summarization in our framework
in terms of learning methodology we view extractive summarization as a sentence or token classication problem
previously posed models include the rnn based sequence model nallapati et al
the attention based neural encoder decoder model cheng and lapata and the sequence model with a global ing objective e

rouge for ranking sentences optimized via rl narayan et al
paulus et al

more recently graph convolutional neural networks gcns have also been adapted to allow the incorporation of global information in text summarization tasks sun et al
prasad and kan
abstractive summarization is cally cast as a sequence to sequence learning lem
the encoder of the framework reads a ument and yields a sequence of continuous resentations and the decoder generates the target summary token by token rush et al
lapati et al

both approaches have their own merits in generating comprehensive and novel summaries hence most systems leverage these two different models in one framework see et al
liu and lapata
we use the extractive ponent to identify tokens in a candidate document that may be relevant from a pm perspective and use the abstractive component to identify potential terms that may not necessarily be in the document but nevertheless characterize it for pm purposes

word and entity embeddings most of the neural text summarization models as described in the previous section adopt the encoder decoder framework that is popular in chine translation
as such the vocabulary on the decoding side does not have to be the same as that on the encoding side
we exploit this to design a summarization trick for pm where the decoder outputs both regular english tokens and also entity codes from a standardized biomedical terminology that captures semantic concepts discussed in the document
this can be trained easily by ing the textual queries in the training examples to their corresponding entity codes
this trick is to enhance our ability to handle vocabulary match in a different way besides the abstractive framing
we created biomedical entity tagged bmet for this purpose
bmet embeddings are trained on biomedical literature abstracts that were annotated with entity codes in the medical subject headings mesh codes are appended to the associated textual spans in the training examples
so regular tokens and the entity codes are thus embedded in the same semantic space via pretraining with the fasttext architecture bojanowski et al

besides
com romanegloo bmet e mbeddings html
nlm
nih
gov mesh meshhome
regular english tokens the vocabulary of bmet thus includes mesh codes and a subset of supplementary concepts
in the dictionary mesh codes are differentiated from the regular words by a unique prex for example for mesh code
with this our tion model can now translate a sequence of regular text tokens into a sequence of biomedical entity codes or vice versa
that is we use mesh as a new semantic facet besides those already provided by trec pm organizers
the expected output for the mesh facet is the set of codes that capture entities in the disease and gene variation facets
models and reranking in this effort toward document reranking we aim to measure the relevance match between a ment and a faceted pm query
each training stance is a tuple d q yd q where q is a query is a candidate document and yd q is a boolean man adjudicated outcome whether is relevant to
as mentioned in section
we ne tune bert for a query document relevance matching task modeled as a classication goal to predict yd q rel
next we ne tune bert for token level relevance classication different from rel where a token in d is deemed relevant during training if it occurs as part of q
we name this model ext for keyword extraction
lastly we train a bert model in the setting where the encoder is initialized with a pretrained ext model
the encoder reads in d and the decoder attends to the contextualized representations of d to generate a facet specic pseudo query sentence qd which is then compared with the original query q
we ceptualize this process as text summarization from a document to query and refer to it as abs
all three models are used together to rerank a candidate d at test time for a specic input query

document relevance matching rel neural text matching has been recently carried out through siamese style networks mueller and garajan which also have been adapted to biomedicine noh and kavuluru
our proach adapts the bert architecture for the ing task in the multi sentence setting as shown in figure
we use bert s tokenizer on its textual note queries here are not grammatically well formed sentences but are essentially sequences generated by the marization model
figure bert architecture for document relevance matching task rel inputs and the tokens are mapped to token dings
rel takes the concatenated sequence of a document and faceted query sentences
the tional symbols dened in the bert tokenizer e

cls are added to the input sequence
each input sequence starts with a cls token
each sentence of the document ends with the sep token with the last segment of the input sequence being the set of faceted query sentences which end with another sep token
in the encoding cess the rst cls token collects features for termining document relevance to the query
bert uses segment embeddings to distinguish two tences
we however use the them to distinguish multiple sentences within a document
for each sentence we assign a segment embedding either a or b alternatively
the positional embeddings encode the sequential nature of the inputs
the token embeddings along with the segment and sitional embeddings pass through the transformer layers
finally we use the output logit from the cls token as the matching score for the input document and query
we note that we do nt demarcate any boundaries within different facets of the query

keyword extraction ext ext model has an additional token classication layer on top of the pretrained bert
the output of a token is the logit that indicates the log of odds of the token s occurrence in the query
with pm datasets we expect to see the logits re for words related to different facets with an optimized ext at test time
unlike the rel model the input to ext is a sequence of words in a document out any sep delimiters
however the model still learns the boundaries of the sentence via ment inputs
this component essentially generates a brief extractive summary of a candidate ment
furthermore contextualized embeddings from ext are used in the decoder of abs to ate faceted abstractive document summaries

abstractive document summarization abs abs employs a standard attention model similar to that by nallapati et al
as shown in figure
we initialize the parameters of the encoder with a pretrained ext model
the decoder is a layer transformer in which the self attention layers attend to only the earlier positions in the output sequence as is typical in auto regressive in each training phase step the guage models
decoder takes each previous token from the ence query sentence in the generation process the decoder uses the token predicted one step earlier
facets disease name genetic variations demographic info
mesh terms document keywords unused unused unused unused unused table signals for different facets of the patient cases we differentiate facets by the special pairs of tokens assigned to each topic
in a typical eration process special tokens such as begin and end are used to indicate quence boundaries
in this model we use some special tokens in the bert vocabulary with x unused
specically unused i and unused i are used as bos and eos tokens respectively for different facets
these facet sentencesquery label figure architecture of the abstractive document summarization abs model
the encoder left component is initialized with a pretrained ext model
the class labels of the encoder are used for identifying keywords of the document and the output sequences generated from the decoder right component are used to build a pseudo query which is later used in computing similarity scores for the user provided query
signals are the latent variables for which abs is optimized
through them abs learns not only the thematic aspects of the queries but also the meta attributes such as length
the special tokens for facets are listed in table the last row indicates a new auxiliary facet we introduce in section

each faceted query is enclosed by its assigned bos eos pair and the decoder of abs learns i where is the facet signal
as in the encoder and the original transformer tecture vaswani et al
we add the soidal positional embedding pt and the segment vector a or b to the token embedding et
note that the dimension of the token embeddings used in the encoder bert embeddings is different from that of the decoder our custom bmet beddings which causes a discrepancy in ing context attentions of the target text across the source document
hence we add an additional linear layer to project the constructed decoder beddings en j a pi in the right hand portion of figure into the same space of embeddings of the encoder
these projected embeddings are fed to the decoder s transformer layers
each transformer layer applies multi head attention for computing the and context attentions
the attention tion reads the input masks to preclude attending to future tokens of the input and any padded tokens i
e
of the source text
both attention functions apply a residual connection he et al

lastly each transformer layer ends with a position wise feedforward network
final scores for each token are computed from the linear layer on top of the transformer layers
in training these scores are consumed by a cross entropy loss tion
in generation process the softmax function is applied over the vocabulary yielding a probability distribution for sampling the next token
finally to generate the pseudo query we use beam search to nd the most probable sentence among predicted candidates
the scores are nalized by two measures proposed by wu et al
equation
the length penalty where is the rent target length and is the length malization coefcient

the coverage penalty y pi j
where pi j is the attention score of the j get word yj on the i th source word xi is the source length and is the coverage malization coefcient
intuitively these functions avoid favoring shorter predictions and yielding plicate terms
we tune the parameters of the penalty functions
with grid search on the validation set for trec pm

reranking with rel ext and abs the main purpose of the models designed in the previous subsections is to come up with a bined measure for reranking
for a query q let


dr be the set of top r set to candidate transformer transformer layersattendspredictor linear softmaxtoken wise class labelsoutput documents returned by the solr edismax query
it is straightforward to impose an order on dj through rel via the output probability estimates of relevance
given q for each dj we generate the pseudo query summary qdj by concatenating all distinct words in the generated pseudo query sentences by abs along with the words selected by ext
repeating words and special tokens are removed
although faceted summaries are erated through abs in the end qdj is essentially the set of all unique terms from abs and ext
each dj is now scored by comparing q and qdj via two similarity metrics the recall score srou ge lin and a cosine similarity based score computed as qdj yq max xqdj where denote vector representations from bmet embeddings section

overall we compute four different scores and hence rankings of a document the retrieval score returned by solr the document relevance score by rel pseudo query based rouge score and pseudo query similarity score scos
in the end we merge the rankings with reciprocal rank fusion cormack et al
to obtain the nal ranked list of documents
the results are compared against the state of the art models from the trec pm task
experimental setup
data across trec pm tasks we have a tal of patient cases and qrels document relevance judgments as shown in table
year queries documents rel
irrel
table number of queries and pooled relevance ments in the trec pm tracks we create two new auxiliary facets mesh terms and keywords derived from any training query and document pair
we already covered the mesh facet in section

keywords are those assigned by authors to a biomedical article to capture its themes and are downloadable from nih s ncbi website
if no keywords were assigned to an article then we use the set of preferred names of mesh terms assigned to the articles by trained nih coders for that example
the following list shows associated facets for a sample training instance disease prostate cancer genetic variations atm deletion demographics year old male mesh terms keywords aged ataxia telangiectasia tated proteins prostate neoplasms genetics each model consumes data differently as shown in table
rel takes a document along with the given query as the source input and predicts document level relevance
we consider a document with the human judgment score either partially relevant or totally relevant as relevant for this study
note that we do not include mesh terms in the query sentences for rel
ext reads in a ment as the source input and predicts token level relevances
during training a relevant token is one that occurs in the given patient case
a query is the output for abs taking in a document and a facet type
model source target rel ext abs sentences doc relevance doc token relevances signal a pseudo query table data inputs and outputs for each model

implementation details for all three models we begin with the trained bert base uncased huggingface model wolf et al
to encode source texts
we use bert s wordpiece schuster and jima tokenizer for the source documents
rel and ext are trained for steps with batch size of
the maximum number of tokens for source texts is limited to
as the loss tion of these two models we use weighted binary cross entropy
that is given high imbalance with many more irrelevant instances than positive ones we put different weights on the classes in puting the loss according to the target distributions proportions of negative examples are for rel and for ext
the loss is y log where
for rel and
for ext
adam mizer with parameters
and
starting learning rate lr and xed weight decay of
was used
the learning rate is reduced when a metric has stopped improving by using the reducelronplateau scheduler in pytorch
for the decoder of abs multi head attention module from opennmt klein et al
was used
to tokenize target texts we use the nltk word tokenizer
nltk
org api n ltk
tokenize
html unlike the one used in the encoder this is because we use customized word embeddings the bmet embeddings section
trained with a domain specic corpus and ulary
the vocabulary size is which cludes the mesh codes
we use six former layers in the decoder
model dimension is and the feed forward layer size is
we use different initial learning rates for the encoder and decoder since the encoder is initialized with a pretrained ext model encoder and decoder
negative log likelihood is the loss tion for abs on the ground truth faceted query sentences
for beam search in abs beam size is set to
at test time we select top two best dictions and merge them into one query sentence
the max length of target sentence is limited to and a sequence is incrementally generated until abs outputs the corresponding eos token for each facet
all parameter choices were made based on best practices from prior efforts and experiments to optimize on validation subsets
evaluations and results we conducted both quantitative and qualitative uations with example outcomes
the nal tion was done on the trec pm dataset while all hyperparameter tuning was done using a training and validation dataset split of a shufed combined set of instances from and tracks validation and the rest for training

quantitative evaluations we rst discuss the performances of the constituent rel and ext models that were evaluated using train and validation splits from years
table shows their performance where rel can recover of the relevant documents and ext can identify of the tokens that occur in patient case information both at precisions over
we nd that learning a model for fying document token level relevance is relatively straightforward even with the imbalance
rel ext p r p r train





valid





table retrieval performance of rel and ext
next we discuss the main results comparing against the top two teams rows in the track in table
before we proceed we want to highlight one crucial evaluation consideration that applies to any trec track
trec evaluates tems in the craneld paradigm where pooled top documents from all participating teams are judged for relevance by human experts
because we did not participate in the original trec pm task our retrieved results are not part of the judged uments
hence we may be at a slight disadvantage when comparing our results with those of teams that participated in trec pm
nevertheless we believe that at least the top few most relevant documents are typically commonly retrieved by all models
hence we compare with both and r prec relevant doc count measures
model r prec julie mug faessler et al


bitem pm caucheteur et al


baseline solr edismax baseline solr mlt baseline rel baseline abs baseline









table our scores and top entries in trec pm
our baseline solr query results are shown in row with subsequent rows showing results from additional components
solr edismax is a document ranking function which is based on the jones et al
probabilistic model
we also evaluate edismax with solr mlt morelikethis in which a new query is document facet signal summary title association between braf mutation and the clinicopathological features of solitary papillary thyroid microcarcinoma
pmid papillary intrahepatic cholangiocarcinoma braf unused unused unused unused papillary thyroid braf clinicopathological title identication of differential and functionally active mirnas in both anaplastic lymphoma kinase and anaplastic large cell lymphoma
pmid lymphoma anaplastic lymphoma alk cell bradykinin unused unused unused unused lymphoma alk receptor tyrosine kinase table sample facet conditioned document summarizations by abs erated by adding a few interesting terms top tf idf terms from the retrieved documents of the initial edismax query
this traditional relevance feedback method row method has decreased the performance from the baseline and hence has not been used in our reranking methods
all our models rows present stable line scores in and the combined method tops the list with a improvement over the prior best model faessler et al

baseline with rel does the best in terms of prec
both prior top teams rely heavily on query expansion through external knowledge bases to add synonyms hypernyms and hyponyms of terms found in the original query

qualitative analysis table presents sample pseudo queries generated by abs
the summaries of the rst document show some novel words intrahepatic and cinoma that do not occur in the given document we only show title for conciseness but the abstract also does not contain those words
the model may have learned the close relationship between cholangiocarcinoma and braf the latter being part of the genetic facet of the actual query for which pmid turns out to be relevant
also embedding proximity between intrahepatic and cholangiocarcinoma may have introduced both into the pseudo query although they are not central to this document s theme
still this maybe tant in retrieving documents that have an indirect yet relevant link to the query through the query terms
this could be why although abs underperforms rel it still complements it when combined table
the table also shows that abs can generate concepts in a domain specic nology
for example the second document yields following mesh entity codes which are strongly related to the topics of the document cell transformation neoplastic phoma large cell anaplastic and anaplastic lymphoma kinase
for a qualitative exploration of what ext and different facets of abs capture we refer the reader to appendix a

machine conguration and runtime all training and testing was done on a single nvidia titan x gpu in a desktop with gb ram
the corpus to be indexed had biomedical tations titles and abstracts of biomedical
we trained the three models for ve epochs and the training time per epoch query doc pairs is mins for rel mins for ext and mins for abs
coming to test time per query the solr edismax query returns top results in ms
generating pseudo queries for candidates via ext and abs takes seconds and generating rel scores consumes seconds
so per query it takes nearly
mins at test time to return a ranked list of documents
although this does not facilitate real time retrieval as in commercial search engines given the complexity of the queries we believe this is at least near real time offering a convenient way to launch pm queries
furthermore this comes at an affordable conguration for many labs and clinics with a smaller carbon footprint
conclusion in this paper we proposed an ensemble document reranking approach for pm queries
it builds on trained bert models to combine strategies from document relevance matching and extractive stractive text summarization to arrive at document to copyright issues with full text trec pm is only conducted on abstracts titles of articles available on pubmed
rankings that are complementary in eventual uations
our experiments also demonstrate that entity embeddings trained on an annotated domain specic corpus can help in document retrieval tings
both quantitative and qualitative analyses throw light on the strengths of our approach
one scope for advances lies in improving the summarizer to generate better pseudo queries so that abs starts to perform better on its own
at a high level training data is very hard to generate in large amounts for ir tasks in biomedicine and this holds for the trec pm datasets too
to better train abs it may be better to adapt other biomedical ir datasets
for example the trec clinical decision support cds task that ran from to is related to the pm task roberts et al

a future goal is to see if we can apply our neural transfer learning rios and kavuluru and domain adaptation rios et al
efforts to repurpose the cds datasets for the pm task
another straightforward idea is to reuse ated pseudo query sentences in the edismax query by solr as a form of pseudo relevance feedback
the scos expression in section
focuses on an asymmetric formulation that starts with a query term and looks for the best match in the query
considering a more symmetric formulation where we also begin with the pseudo query terms and average both summands may provide a better estimate for reranking
additionally a thorough exploration of how external biomedical knowledge bases wagner et al
can be incorporated in the neural ir framework for pm is also tant nguyen et al

references piotr bojanowski edouard grave armand joulin and tomas mikolov

enriching word vectors with subword information
transactions of the tion for computational linguistics
deborah caucheteur emilie pasche julien gobeill anais mottaz luc mottin and patrick ruch

designing retrieval models to contrast driven search vs
recall driven treatment traction in precision medicine
jianpeng cheng and mirella lapata

neural marization by extracting sentences and words
in proceedings of the annual meeting of the sociation for computational linguistics volume long papers pages
francis s collins and harold varmus

a new tiative on precision medicine
new england journal of medicine
alexis conneau and guillaume lample

lingual language model pretraining
in advances in neural information processing systems pages
gordon v cormack charles la clarke and stefan buettcher

reciprocal rank fusion outperforms condorcet and individual rank learning methods
in proceedings of the international acm sigir conference on research and development in tion retrieval pages
jacob devlin ming wei chang kenton lee and kristina toutanova

bert pre training of deep bidirectional transformers for language standing
in naacl hlt pages
erik faessler michel oleynik and udo hahn

julie lab med uni graz trec precision medicine track
trey grainger and timothy potter

solr in action
manning publications co
kaiming he xiangyu zhang shaoqing ren and jian sun

deep residual learning for image nition
in proceedings of the ieee conference on computer vision and pattern recognition pages
k sparck jones steve walker and stephen e
son

a probabilistic model of information retrieval development and comparative experiments information processing management part

guillaume klein yoon kim yuntian deng jean lart and alexander m rush

opennmt source toolkit for neural machine translation
in ceedings of acl system demonstrations pages
chin yew lin

rouge a package for matic evaluation of summaries
in text tion branches out pages barcelona spain
association for computational linguistics
yang liu and mirella lapata

text in proceedings of tion with pretrained encoders
the conference on empirical methods in ral language processing and the international joint conference on natural language processing emnlp ijcnlp pages
jonas mueller and aditya thyagarajan

siamese recurrent architectures for learning sentence ity
in proceedings of the thirtieth aaai conference on articial intelligence pages
ramesh nallapati feifei zhai and bowen zhou

summarunner a recurrent neural network based quence model for extractive summarization of ments
in thirty first aaai conference on articial intelligence
ramesh nallapati bowen zhou cicero dos santos c aglar gulcehre and bing xiang

abstractive text summarization using sequence to sequence rnns and beyond
in proceedings of the signll conference on computational natural language learning pages
shashi narayan shay b cohen and mirella lapata

ranking sentences for extractive tion with reinforcement learning
in naacl hlt pages
gia hung nguyen laure soulier lynda tamine and nathalie bricon souf

dsrim a deep neural information retrieval model enhanced by a edge resource driven representation of documents
in proceedings of the acm sigir international ference on theory of information retrieval pages
rodrigo nogueira and kyunghyun cho

oriented query reformulation with reinforcement learning
in proceedings of the conference on empirical methods in natural language processing pages
jiho noh and ramakanth kavuluru

document trieval for biomedical question answering with neural sentence matching
in ieee international conference on machine learning and applications icmla pages
ieee
romain paulus caiming xiong and richard socher

a deep reinforced model for abstractive marization
international conference on learning representations
animesh prasad and min yen kan

glocal porating global information in local convolution for keyphrase extraction
in naacl hlt pages
anthony rios and ramakanth kavuluru

neural transfer learning for assigning diagnosis codes to emrs
articial intelligence in medicine
anthony rios ramakanth kavuluru and zhiyong lu

generalizing biomedical relation classication with neural adversarial domain adaptation
matics
kirk roberts dina demner fushman ellen m
voorhees william r
hersh steven bedrick der j
lazar shubham pant and funda bernstam

overview of the trec sion medicine track
kirk roberts matthew simpson dina fushman ellen voorhees and william hersh

state of the art in biomedical literature retrieval for clinical cases a survey of the trec cds track
information retrieval journal
alexander m rush sumit chopra and jason weston

a neural attention model for abstractive tence summarization
in emnlp pages
mike schuster and kaisuke nakajima

japanese and korean voice search
in ieee international conference on acoustics speech and signal ing icassp pages
ieee
abigail see peter j liu and christopher d manning

get to the point summarization with generator networks
in proceedings of the nual meeting of the association for computational linguistics pages
zhiqing sun jian tang pan du zhi hong deng and jian yun nie

divgraphpointer a graph pointer network for extracting diverse keyphrases
in proceedings of the international acm gir conference on research and development in information retrieval pages
ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan n gomez ukasz kaiser and illia polosukhin

attention is all you need
in advances in neural information cessing systems pages
alex h wagner brian walsh georgia mayeld david tamborero dmitriy sonkin kilannin krysiak jordi deu pons ryan p duren jianjiong gao julie mcmurry al

a harmonized knowledgebase of clinical interpretations of matic genomic variants in cancer
nature genetics
thomas wolf lysandre debut victor sanh julien chaumond clement delangue anthony moi ric cistac tim rault remi louf morgan towicz al

transformers state of arxiv preprint art natural language processing


yonghui wu mike schuster zhifeng chen quoc v le mohammad norouzi wolfgang macherey maxim krikun yuan cao qin gao klaus macherey et al

google s neural machine translation system bridging the gap between human and machine lation
arxiv preprint

a attention heatmaps by facet signals figure depicts words highlighted by ext
dently we see terms related to the regulations of gene expressions proteins or disease names turing more prominently
figure shows how abs reads the source document differently depending on which facet signal it starts with in the process of query generation compared to ease facet the attention heat map by genetic facet focuses more on the words related to gene regulations
figure heatmap of classication scores by ext
darker red indicates relatively higher probability of the token being relevant to the theme of the trec pm datasets
attention heatmap produced by signal topic of disease attention heatmap produced by signal topic of generic variants and gene regulations figure comparison between the attention heatmaps on a sample document conditioned by eld signals in abs model

