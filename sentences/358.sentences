metrics also disagree in the low scoring range revisiting summarization evaluation metrics manik bhandari pranav gour atabak ashfaq pengfei liu carnegie mellon university mbhandar pgour aashfaq
cmu
edu v o n l c
s c v
v i x r a abstract in text summarization evaluating the efcacy of automatic metrics without human judgments has become recently popular
one exemplar work peyrard concludes that automatic metrics strongly disagree when ranking high scoring summaries
in this paper we revisit their experiments and nd that their observations stem from the fact that metrics disagree in ranking summaries from any narrow scoring range
we hypothesize that this may be because maries are similar to each other in a narrow scoring range and are thus difcult to rank
apart from the width of the scoring range of summaries we analyze three other properties that impact inter metric agreement ease of summarization abstractiveness and coverage
to encourage reproducible research we make all our analysis code and data publicly available
introduction automatic metrics play a signicant role in summarization evaluation profoundly affecting the direction of system optimization
due to its importance evaluating the quality of evaluation metrics also known as meta evaluation has been a crucial step
generally there are two meta evaluation strategies i assessing how well each metric correlates with human judgments lin ng and abrecht louis and nenkova peyrard et al
bhandari et al
which requires procuring manual annotations that are expensive and time consuming and measuring the correlation between different metrics peyrard which is a human judgment free method
in this work we focus on the latter and ask two research questions how do automated metrics correlate when ranking summaries in different scoring ranges low average and high we revisit the experiments of peyrard which concludes that automated rics strongly disagree for ranking high scoring summaries
we nd that the scoring range has little effect on the correlation of metrics
it is rather the width of the scoring range which affects inter metric correlation
specically we observe that metrics agree in ranking summaries from the full scoring range but disagree in ranking summaries from low average and high scoring ranges when taken separately
which other factors affect the correlations of metrics in addition to the width of the scoring range we analyze three properties of a reference summary on inter metric correlation ease of rization abstractiveness and coverage
overall we nd that for highly extractive document reference summary pairs inter metric correlation is high whereas metrics disagree when ranking summaries of abstractive document reference summary pairs
we summarize our contributions as follows we extend the analysis of peyrard and nd that not only do metrics disagree in the high scoring range they also disagree in the low and medium scoring we perform our analysis on the popular cnn dailymail dataset using traditional lexical range

com manikbhandari revisitsummevalmetrics uses three experiments to reach their conclusion
due to limitations of space we focus on the rst one here
please see the appendix for a detailed analysis of the other two experiments
this work is licensed under a creative commons attribution
international license
license details creativecommons
org licenses

matching metrics like rouge as well as recently popular semantic matching metrics like bertscore and moverscore
apart from the width of the scoring range we analyze three linguistic properties of reference summaries which affect inter metric correlations
preliminaries
datasets dang and owczarzak dang and owczarzak are multi document multi reference summarization datasets used during the shared tasks
ing peyrard we combine the two and refer to the joined dataset as tac
cnn dailymail cnndm hermann et al
is a commonly used summarization dataset modied by nallapati et al
which contains news articles and associated highlights as summaries
we use the non anonymized version

evaluation metrics we examine six metrics that measure the semantic equivalence between two texts in our case between the system generated summary and the reference summary
bertscore bscore measures soft overlap between contextual bert embeddings of tokens between the two zhang et al

score ms applies a distance measure to contextualized bert and elmo word zhao et al

js divergence measures jensen shannon divergence between the two text s bigram lin et al

and measure the overlap of unigrams and bigrams lin
rouge l measures the overlap of the longest common quence between two texts lin
we use the recall variant of all metrics except moverscore which has no specic recall variant

correlation measure kendall s is a measure of the rank correlation between any two measured quantities in our case scores given by evaluation metrics and is popular in meta evaluating metrics at the summary level peyrard
we use the implementation given by virtanen et al

summary generation to simulate the full scoring range of summaries that are possible for a document we follow peyrard and use a genetic algorithm peyrard and eckle kohler to generate extractive summaries
we optimize for metrics rouge l bertscore and moverscore generating summaries per metric for each of the nearly k documents in the cnndm test set resulting in summaries per document
after de duplication we are left with nearly summaries per document on average
for the tac dataset we randomly sample summaries for each document from the nearly output summaries provided by peyrard
experiment and analysis
width of scoring range in this experiment we aim to re examine the results in peyrard and answer our rst research question how do different automated metrics correlate in ranking summaries in different scoring ranges we approach this as follows for each summary sij of document we rst calculate its mean score across all metrics after normalizing the metrics to be between and
we use this to partition the scoring range of each document into three parts low scoring l medium scoring m and top scoring t which are the bottom third the middle third and the top third of the scoring range respectively
we then analyze the summaries falling into these bins in two different ways github
com tiiiger bert score github
com aiphes moverscore the function dened in github
com ukplab genetic swarm mds and l the python wrapper github
com sebastiangehrmann rouge baselines ms























metric bin bscore ms


t t t t t ms rl rl


























bin l m t l m t l m t l m t l m t








rl


























table kendall for the cumulative and non cumulative settings on tac
higher values indicate greater inter metric correlation
l m and t correspond to the low medium and top scoring range spectively
please see sec

for more details

cumulative in this setting we aim to replicate peyrard s results which compared inter metric agreement on the whole set of summaries to that on the top scoring subset
to do this we compute the average inter metric correlation for summaries belonging to i l m t m t and t as shown in the left side in tab

note that here the width of the scoring range is different for each row

non cumulative in this setting we analyze the average inter metric correlation on summaries longing to each scoring bin separately as shown in the right side of tab

we advocate for the use of this setting as it controls for the width of the scoring range and it allows for a more ne grained analysis of the scoring range
note that for each bin the correlation is calculated for summaries generated for each document and then averaged over all documents
we only consider statistically signicant p
kendall s values
observations discussion our observations on the tac and cnndm datasets are shown in tab
and respectively
in the cumulative setting we observe the same trend reported by peyrard metric agreement decreases when the average score increases and is the lowest in the top scoring range t
however in the non cumulative setting where metrics rank summaries from a narrow scoring range we observe that i metrics have low correlations in all three scoring ranges low medium and top and ii there is no clear trend in correlations across the bins
comparing the cumulative and non cumulative settings one can see that decreasing the width of the scoring range reduces the inter metric correlations
this suggests that rather than the scoring range the width of the scoring range has a strong impact on the correlation between metrics
this may be because summaries from a narrow scoring range are similar to each other and thus difcult for different metrics to rank consistently

factors affecting inter metric correlation in this experiment we aim to answer the second research question apart from the width of the scoring range which factors affect inter metric correlations specically we identify three factors which affect the correlation of metrics ease of summarization abstractiveness and coverage

ease of summarization eos for each generated summary sij of document with reference summary ri we dene eos as maxj ri
here mk is a metric function n normalized to be between and
thus eos is the average over all metrics of the maximum score that any summary received
a higher eos score for a document implies that for that document we can generate higher scoring extractive summaries according to many metrics

abstractiveness we dene abstracriveness of a document with reference ri as where is the set of unique tokens of any text
abstractiveness measures the overlap in laries of the document and its reference summary

coverage we use the denition of coverage as provided by grusky et al
i
e
the percentage ms























metric bin bscore ms


t t t t t ms rl rl


























bin l m t l m t l m t l m t l m t








rl


























table kendall for the cumulative and non cumulative settings on cnndm
higher values cate greater inter metric correlation
l m and t correspond to the low medium and top scoring range respectively
please see sec

for more details
tac eos tac abs tac cov cnndm eos e cnndm abs cnndm cov figure effect of different properties of reference summaries
we only show correlation between bertscore and due to limited pages
the trend is similar for all other pairs as shown in the appendix
the plots for cnndm are more dense because of more documents in the cnndm test set as compared to tac
cov and abs stand for coverage and abstractiveness respectively
the trend lines in red are the point and point moving average for tac and cnndm respectively
of words in the summary that are part of an extractive fragment with the article
we refer the reader to grusky et al
for a detailed description of coverage
observations our observations are summarized in fig

each point in the graph represents a document reference summary pair with its corresponding property on the axis and inter metric relation of its summaries on the y axis
we nd that metrics agree with each other as documents become easier to summarize as documents become more abstractive the correlation between metrics decreases as the coverage of documents increases the correlation between metrics increases
these observations suggest that automatic evaluation metrics have higher correlations for easier to marize and more extractive lower abstractiveness higher coverage document reference summary pairs
implications and future directions in this work we revisit the conclusion of peyrard s work and show that instead of solely ing in high scoring range metrics disagree when ranking summaries from all three scoring ranges low medium and top
this highlights the need to collect human judgments to identify trustworthy metrics
moreover future meta evaluations should use uniform width bins when comparing correlations to ensure a more robust analysis
additionally we analyze three linguistic properties of reference summaries and their effect on inter metric correlations
our observation that metrics de correlate as references become more abstractive suggests that we need to exercise caution when using automatic metrics to compare summarization systems on abstractive datasets like xsum narayan et al

moreover future work proposing new evaluation metrics can analyze them using these properties to get more insights about their behavior
acknowledgments we would like to thank maxime peyrard for sharing the code and data used in peyrard and for his useful feedback about our experiments
we would also like to thank graham neubig for his feedback and for providing the computational resources needed for this work
references manik bhandari pranav narayan gour atabak ashfaq pengfei liu and graham neubig

re evaluating in proceedings of the conference on empirical methods in natural evaluation in text summarization
language processing emnlp
hoa dang and karolina owczarzak

overview of the tac update summarization task
in proceedings of the first text analysis conference tac pages
hoa dang and karolina owczarzak

overview of the tac summarization track
in proceedings of the first text analysis conference tac pages
max grusky mor naaman and yoav artzi

newsroom a dataset of
million summaries with diverse extractive strategies
in proceedings of the conference of the north american chapter of the association for computational linguistics human language technologies volume long papers volume pages
karl moritz hermann tomas kocisky edward grefenstette lasse espeholt will kay mustafa suleyman and phil blunsom

teaching machines to read and comprehend
in advances in neural information ing systems pages
chin yew lin guihong cao jianfeng gao and jian yun nie

an information theoretic approach to matic evaluation of summaries
in proceedings of the human language technology conference of the naacl main conference pages new york city usa june
association for computational linguistics
chin yew lin

rouge a package for automatic evaluation of summaries
text summarization branches out
annie louis and ani nenkova

automatically assessing machine summary content without a gold standard
computational linguistics
ramesh nallapati bowen zhou cicero dos santos c a glar and bing xiang

abstractive text summarization using sequence to sequence rnns and beyond
conll page
shashi narayan shay b cohen and mirella lapata

do nt give me the details just the summary aware convolutional neural networks for extreme summarization
in proceedings of the conference on empirical methods in natural language processing pages
jun ping ng and viktoria abrecht

better summarization evaluation with word embeddings for rouge
in proceedings of the conference on empirical methods in natural language processing pages lisbon portugal september
association for computational linguistics
maxime peyrard and judith eckle kohler

a general optimization framework for multi document rization using genetic algorithms and swarm intelligence
in proceedings of the international conference on computational linguistics coling pages dec
maxime peyrard teresa botschen and iryna gurevych

learning to score system summaries for better content selection evaluation
in proceedings of the emnlp workshop new frontiers in summarization page to appear september
maxime peyrard

studying summarization evaluation metrics in the appropriate scoring range
in ings of the annual meeting of the association for computational linguistics pages florence italy july
association for computational linguistics
pauli virtanen ralf gommers travis e
oliphant matt haberland tyler reddy david cournapeau evgeni burovski pearu peterson warren weckesser jonathan bright stefan j
van der walt matthew brett joshua wilson k
jarrod millman nikolay mayorov andrew r
j
nelson eric jones robert kern eric larson cj carey ilhan polat yu feng eric w
moore jake vand erplas denis laxalde josef perktold robert cimrman ian henriksen e
a
quintero charles r harris anne m
archibald antonio h
ribeiro fabian pedregosa paul van mulbregt and scipy
contributors

scipy
fundamental algorithms for scientic computing in python
nature methods
tianyi zhang varsha kishore felix wu kilian q
weinberger and yoav artzi

bertscore evaluating text generation with bert
in international conference on learning representations
wei zhao maxime peyrard fei liu yang gao christian m
meyer and steffen eger

moverscore text generation evaluating with contextualized embeddings and earth mover distance
in proceedings of the conference on empirical methods in natural language processing and the international joint conference on natural language processing emnlp ijcnlp pages hong kong china november
tion for computational linguistics
cumulative bins on tac non cumulative bins on tac cumulative bins on cnndm non cumulative bins on cnndm figure disagreement between metrics on tac and cnndm
a disagreement in addition to kendall s between metrics peyrard analyzes the disagreement between metrics and shows higher inter metric disagreement in the higher scoring range
to analyze disagreement they randomly sample pairs of summaries say sa and sb with corresponding references ra rb for each pair of metrics say and and bin them into cumulative bins according to the average score for any one metric i
e
according to ra rb
the disagreement for each bin is then calculated as the percentage of summary pairs for which ra rb but ra rb or vice versa i
e
ra rb but ra rb
the use of cumulative bins suffers from the same phenomena as described in section
i
e the width of the bin may play a role in the agreement of metrics
in fig
we replicate the cumulative disagreement plot for the tac and cnndm datasets and show the corresponding non cumulative versions
we observe that when we control for the width of scoring range inter metric disagreement is higher even in the low scoring range
b f n ratio peyrard s nal experiment measures if improvements according to one metric are consistent across other metrics
to this end they dene f n as follows let be the set of summaries for a document and m be the set of all metrics
let s sample a summary s randomly
then f n m m i
e
out of all the summaries ranked better than a summary by one metric how many are ranked better tac cnndm cnndm with random metrics figure f n ratio between metrics on tac and cnndm
tac cnndm cnndm with random metrics figure f ration between metrics on tac and cnndm
by all the metrics
as shown in fig
on the tac dataset as the average score of s averaged across all metrics increases f n decreases
this may suggest that as summary quality improves different metrics do not agree on which summaries are of better quality
however this quantity is misleading
as the average score of s increases the numerator f will naturally decrease because for a higher scoring s the number of summaries that are better than s are fewer while the denominator n may remain large even if one metric is misaligned with others
to prove this hypothesis we rst replicate the measure for tac and cnndm datasets in fig

next instead of the real metric scores we assign each summary a random number sampled from
in fig
we see the same trend for random scores as for real metric scores
this shows that this decreasing trend is indeed a property of the ratio f n rather than being a property specic to real evaluation metrics
moreover one can come up with a modied ratio f as follows f n m m which measures out of all the summaries that are ranked worse than a summary by one metric how many are ranked worse by all metrics
if metrics truly de correlated in only the higher scoring range one would expect the same decreasing trend for f
however as is clear from fig
the trend is reversed for real as well as random metric scores
f increases as average score of s increases
this is because similar to f n this measure is also misleading and sensitive to the numerator f which always increases as average score of s increases
c factors affecting inter metric correlation c
ease of summarization please see fig
for ease of summarization vs kendall s for all metric pairs
please see fig
for abstractiveness vs kendall s for all metric pairs
c
abstractiveness c
coverage please see fig
for coverage vs kendall s for all metric pairs
figure ease of summarization axis vs kendall s y axis for all metric pairs on the tac dataset
figure ease of summarization axis vs kendall s y axis for all metric pairs on the cnndm dataset
figure abstractiveness axis vs kendall s y axis for all metric pairs on the tac dataset
figure abstractiveness axis vs kendall s y axis for all metric pairs on the cnndm dataset
figure coverage axis vs kendall s y axis for all metric pairs on the tac dataset
figure coverage axis vs kendall s y axis for all metric pairs on the cnndm dataset

