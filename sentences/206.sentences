leveraging bert for extractive text summarization on lectures derek miller georgia institute of technology atlanta georgia
edu two decades automatic extractive abstract in text last the summarization on lectures has demonstrated to be a useful tool for collecting key phrases and sentences that best represent the content
however many current approaches utilize dated approaches producing sub par outputs or requiring several hours of manual tuning to produce meaningful results
recently new machine learning architectures have provided mechanisms for extractive summarization through the clustering of output embeddings from deep learning models
this paper reports on the project called lecture summarization service a python based restful service that utilizes the bert model for text embeddings and k means clustering to identify sentences closest to the centroid for summary selection
the purpose of the service was to provide student s a utility that could summarize lecture content based on their desired number of sentences
on top of summary work the service also includes lecture and summary management storing content on the cloud which can be used for collaboration
while the results of utilizing bert for extractive text summarization were promising there were still areas where the model struggled providing future research opportunities for further improvement
all code and results can be found here
com lecture summarizer
author keywords lecture summary bert deep learning extractive summarization acm classification keywords i


natural language processing introduction when approaching automatic text summarization there are two different types abstractive and extractive
in the case of abstractive text summarization it more closely emulates human summarization in that it uses a vocabulary beyond the specified text abstracts key points and is generally smaller in size genest lapalme
while this approach is highly desirable and has been the subject of many research papers since it emulates how humans summarize material it is difficult to automatically produce either requiring several gpus to train over many days for deep learning or complex algorithms and rules with limited generalizability for traditional nlp approaches
with this challenge in mind the lecture extractive summarization
in general extractive text summarization summarization service uses utilizes the raw structures sentences or phrases of the text and outputs a summarization leveraging only the content from the source material
for the initial implementation of the service only sentences are used for summarization
in education automatic extractive text summarization of lectures is a powerful tool extrapolating the key points without manual intervention or labor
in the context of many moocs transcripts from video lectures are available but the most valuable information from each lecture can be challenging to locate
currently there have been several attempts to solve this problem but nearly all solutions implemented outdated natural language processing algorithms requiring frequent maintenance due to poor generalization
due to these limitations many of the summary outputs from the mentioned tools can appear random in its construction of content
in the last year many new deep learning approaches have emerged proving state of the art results on many tasks such as automatic extractive text summarization
due to the need for more current tools in lecture summarization the lecture summarization service provides a restful api and command line interface cli tool that serves extractive summaries for any lecture transcripts with the goal of proving that the implementation can be expanded to other domains
the following sections will explore the background and related work around lecture summarization the methodologies used in building the service the results and metrics of the model and example summarizations showing how they compare to commonly used tools such as textrank
background and related work in order to provide necessary context to the proposed solution of automatic lecture summarization it is worth investigating previous research identifying the pros and cons of each approach
in the early days of lecture searching many multimedia created manual applications summarizations for each lecture
one example of this is from m
i
t s lecture processing project where they uploaded a large amount of lectures including transcripts for keyword searching and a summary of the content in the lecture glass hazen cyphers malioutov huynh barzilay
for a limited amount of content this approach can suffice but as the data scales the manual summary process can be inefficient
one motivation for manual summarization in the mid was due to the poor quality from extractive summary tools
in researchers created a tool that would automatically extract corporate meeting summaries using simple probabilistic models but quickly found that the output was far inferior to human constructed summarizations murray renals carletta
due to poor performance with this methodology it led to several research papers that aimed to improve the process
summarization improvements lacking the widespread use of deep learning algorithms in researchers attempted to include rhetorical information into their lecture summaries to help improve summarization performance zhang chan fung
while this led to a decent performance gain it still created sub par outputs concluding that the technology had potential but needed further research zhang chan fung
six years later engineers created an industry product called openessayist which would output the topics and key points in a student s essay aiding the student while they were completing their assignment van labeke whitelock field pulman richardson
in the product there were multiple types of summarization options that utilized algorithms such as textrank for key sentence and keyword extraction van labeke whitelock field pulman richardson
this demonstrated the usefulness of automatic summarization in the education field providing helpful topics sentences and more from an essay to the student which differentiated itself from prior research
while a great initial approach algorithms such as textrank contain a myopic view of spoken context
researchers balasubramanian doraisamy and kanakarajan built a similar application leveraging the naive bayes algorithm that would determine which phrases and elements of the lectures or slides would be the most descriptive in the summarization of a lecture balasubramanian doraisamy kanakarajan
this approach differentiated itself from the previous applications in that it used classification instead of unsupervised learning to create the summaries
although naive bayes has shown some success in the nlp domain its independent assumption of features can eliminate the broader context of a lecture potentially creating summaries that lack key components
while lacking the number of citations as the projects mentioned above in the last couple of years there have been a variety of new papers that have attempted to the summarization problem for lectures
in a small section of the book in recent developments in intelligent computing communication and devices the author implemented a video subtitle extraction program that would summarize the multimedia input utilizing tf idf garg
while such approaches may have a decent output for similar reasons as the naive bayes algorithm tf idf struggles in representing complex phrasing potentially missing key points in a lecture
in another lecture transcript summarization project was created specifically for moocs which had a similar objective as the lecture summarization service creating a basic probabilistic algorithm that achieved a precision of percent when comparing to manual summarizations che yang meinel
while not the best performance from the previously mentioned algorithms it was the first project that specifically focused on moocs supplying some prior history to the domain
text techniques have been extracting in more recent literature there have been several attempts at lecture summarization without lecture transcripts
two from popular whiteboards or slide decks then utilizing that information to create a summary
in one research project the authors created a tool that utilized deep learning to extract written content from the whiteboard and convert it to a text format for further summarization kota davila stone setlur govindaraju
while no deep learning was performed on the lecture transcripts themselves this was one of the first found research projects that utilized some sort of deep learning algorithm to extract information for lecture summarization
in a project focused around extracting information from slides the authors utilized both video and audio processing tools to retrieve content then implemented a tf idf to extract keywords and phrases for the final summarization shimada okubo yin ogata
as mentioned with kota et al
s research the authors used more state of the art approaches for the initial extraction but ended up selecting traditional nlp algorithms for final summarization
moving towards deep learning while highlighting all of the above research projects did not implement deep learning for the lecture summarization on transcripts even for the more modern projects there were plethora amount of reasons to not use it
until recently the recurrent neural network using long short term memory networks was the default approach for many natural requiring massive language processing applications amounts of data expensive compute resources and several hours of training to achieve acceptable results while suffering from poor performance with very long sequences and was prone to overfit vaswani al

with this fact in mind researcher vaswani presented a superior architecture called the transformer which completely moved away from rnns and convolutional neural networks cnn in favor using an architecture comprised of feed forward networks and attention mechanisms vaswani al

while the transformer architecture alleviated some of the problems with rnns and cnns it still had sub human performance on many nlp tasks
at the end of researchers from google built an unsupervised learning architecture on top of the transformer architecture called bert bidirectional encoder representations from transformers that exceeded nearly all existing models in the nlp space for a wide range of tasks devlin chang lee toutanova
on top of publishing the results of the model the research team also published several pre trained models which could be used for transfer learning on a multitude of different domains and tasks devlin chang lee toutanova
another component missing from previous research project was the feature of dynamic or configurable summary sizes
users of lecture summarization applications may want to configure the amount of sentences for each lecture summary providing more or less information based on their needs
since the bert model outputs sentence embeddings these sentences can be clustered with a size of k allowing dynamic summaries of the lecture celikyilmaz tr
with that in mind the lecture summarization service implemented the exact same approach creating dynamic summarizations from taking the centroid sentence in a cluster rather than static summaries with a fixed size
motivation using the background and related work a missing element to existing research and projects was a lecture summarization service that could be utilized by students with configurable lecture sizes leveraging the most up to date deep learning research
this fact provided the development of the lecture summarization service a based service that ran inference from a bert model to be used for dynamically sized lecture summarizations
the motivation for method the lecture summarization service comprises of two main components
one feature is the management of lecture transcripts and summarizations allowing users to create edit delete and retrieve stored items
the other component is the inference from the bert model to produce embeddings for clustering using a k means model creating a summary
below explores each component in detail outlining the associated features
the motivation and implementation of extractive text summarization with bert and k means when creating summaries from saved lectures the lecture summarization service engine leveraged a pipeline which tokenized the incoming paragraph text into clean sentences passed the tokenized sentences to the bert model for inference to output embeddings and then clustered the embeddings with k means selecting the embedded sentences that were closest to the centroid as the candidate summary sentences
textual tokenization due to the variability of the quality of text from lecture tokenization transcripts a combination of multiple techniques was utilized before passing the input to the models
for transcripts derived from udacity a custom parser was created to convert data from the
srt file format a special format that contains time stamps for associated phrases to a standard paragraph form
once converted the nltk library for python was used to extract sentences from the lecture breaking up the content to be passed into the subsequent models for inference
the final step of text tokenization consisted of removing or editing candidate sentences with the goal of only having sentences that did not need additional context in the final summary
one example that had of such behavior was removing sentences conjunctions at the beginning
on top of these types of sentences too small or large of sentences were also removed
another example was removing sentences that mentioned udacity quizzes
while the removed sentences were rarely selected for the extractive summarization when they were kept in the lecture they would change the cluster outputs affecting the centroids which lead to poorer summary candidates
once these tokenization steps were completed the content was ready for inference
bert for text embedding due to its superior performance to other nlp algorithms on sentence embedding the bert architecture was selected
bert builds on top of the transformer architecture but its objectives are specific for pre training
on one step it randomly masks out to of the words in the training data attempting to predict the masked words and the other step takes in an input sentence and a candidate sentence predicting whether the candidate sentence properly follows the input sentence devlin chang lee toutanova
this process can take several days to train even with a substantial amount of gpus
due to this fact google released two bert models for public consumption where one had million parameters and the other contained million parameters devlin chang lee toutanova
due to the superior performance in the larger trained bert model it was ultimately selected for the lecture summarization service
figure introduction to health informatics lecture with bert layer embeddings using the default pre trained bert model one can select multiple layers for embeddings
using the cls layer of bert produces the necessary n e matrix for clustering where n is the number of sentences and e is the embeddings dimension but the output of the cls layer does not necessarily produce the best embedding representation for sentences
due to the nature of the bert architecture outputs for other layers in the network produced n w e embeddings where w equaled the tokenized words
to get around this issue the embeddings can be averaged or maxed to produce an n e matrix
after experiments with udacity extractive summarizations on udacity lectures it was determined that the second to last averaged layer produced the best embeddings for representations of words
this was ultimately determined through visual examination of clusters of the initial embedding process
an example of the differences between the two different plots can be seen in both figure and figure
using a sample introduction to health informatics course lecture one initial hypothesis for the reason of a better sentence representation in the layer than the final cls layer of the bert network was that the figure introduction to health informatics lecture bert cls layer embeddings
final layer was biased by the classification tasks in the original training of the model
for the lecture summarization service the core bert implementation uses the pytorch pretrained bert library from the huggingface organization
at its core the library is a pytorch wrapper around google s pre trained implementations of the models
on top of the original bert model the pytorch pretrained bert library also contains the openai model which is a network that expands on the original bert architecture
when examining the sentence embeddings from both the and original bert model it was clear that the bert embeddings were more representative of the sentences creating larger figure ihi embeddings euclidean distances between clusters
below is an example of clustering with the embeddings
ensembling models while the openai and bert embeddings from the cls layer provided inferior results the ensembling of the multiple architectures produced the best results
however while the clusters had further euclidean distances from other clusters using this method its inference time was increased even when running in a multithreaded environment requiring a substantial amount of memory and compute as well
with this fact in mind ensembling was not used in the service as there needed to be a trade off between inference performance and speed
clustering embeddings finally once the layer embeddings were completed the n e matrix was ready for clustering
from the user s perspective they could supply a parameter k which would represent the number of clusters and requested sentences for the final summary output
during experimentation both means and gaussian mixture models were used for clustering library s implementation
due to models very similar performance k means was finally selected for clustering incoming embeddings from the bert model
from the clusters the sentences closest to the centroids were selected for the final summary
sci kit learn utilizing the lecture summarization service restful api to provide a sufficient interface to the bert clustered summarizations a restful api was put in place to serve the models for inference
since all of the necessary machine learning libraries required python the flask library was selected for the server
on top of summarization capabilities the service also contained lecture transcript management allowing users to add edit delete and update lectures
this also contained an endpoint which would convert
srt files to paragraph text form
once a lecture was saved into the system it could be used to run extractive summarizations
users could supply parameters such as the ratio of sentences to use and a name for a summary to properly organize their resources
once the summary was completed they would then be stored in the sqlite database requiring less compute resources when other users wanted to obtain a summarization of the given lecture
all of the server components were containerized using docker so that individuals could run the service locally or deploy it to a cloud provider
currently a free to use public service exists on aws and can be accessed with the following link



the primary motivation for the restful service was to make it extensible for other developers providing the opportunity for future web applications and command line interfaces to be built on the service
command line interface while users can directly use the restful api for querying the service a command line interface tool was included for easier interaction
this allows users the ability to upload lecture files from their machine and add it to the service with minimal parameters
users can also create summaries and list managed resources
the tool can be installed through pip using the base github repository
results in this section the focus is on the results of the bert model and comparing the output to other methodologies such as textrank
since there were no golden truth summaries for the lectures there were no other metrics used besides human comparison and quality of clusters which were discussed in detail in the above sections
some of the initial weaknesses found in the bert lecture summarization were the same that other methodologies had such as sufficiently summarizing large lectures difficulty in handling context words and language over written dealing with conversational transcripts which is more common in lectures
model weaknesses for larger lectures classified as those that have or more sentences the challenge was to have a small ratio of sentences be properly representative of the entire lecture
when the ratio of sentences to summarize was higher more of the context was sustained making it easier to understand the summary for the user
one hypothesis to get around the large lecture issue was to include multiple sentences in a cluster that were closest to the centroid
this would allow more context for the summary improving the quality of the output
it would also get around the requirement to add more clusters which could be less representative based on where the centroids converged
the problem with this approach is that it would go directly against the user s ratio parameter adding more sentences than requested and degrading the user experience with the tool
for this reason the methodology was not included in the service
another weakness with the current approach was that it would occasionally select sentences that contained words that needed further context such as this those these and also
while a brute force solution would be to remove sentences that contain these words quite frequently this change would dramatically the quality of summarizations
given more time one potential solution was to use nltk to find the parts of speech and attempt to replace pronouns and keywords with their proper values
this was initially attempted but some lectures contained context words that were referenced two to three sentences in the past making difficult to determine which item was actually the true context
reduce examples to get a sense of the performance it is worth looking at some of the summarized content then comparing the results to a traditional approach like textrank
below represents a few example summaries from the introduction to health informatics
udacity
com courses learning and
udacity
com courses lectures on udacity
reinforcement this lecture the subject health information exchange semantic interoperability is all around semantic in interoperability in health exchanges
both summaries below contain five out of the total thirty four sentences for a single sub section
the bert summary better captured the context about the creation of the technology around data governance
however the textrank model had the benefit of introducing ihie in the summary which was beneficial to the user for the background
at the same time textrank was inferior in selecting sentences that flowed together in its summaries selecting candidates that had missing context words and more
while the bert model also contained sentences needing context the model was able to collect sentences that supplied broader context
both outputs agreed on the final sentence which was introducing dr
jon duke for an interview
bert output the most sophisticated form of hie creates semantic interoperability thereby bridging the many ways the same concepts can be expressed by providers and represented in ehrs and other clinical systems
the regenstrief institute in indiana created the expensive and sophisticated technology used by ihie
this architecture is convenient for data governance analysis and reporting
in it all data is stored centrally but in so called data lockers that remain under the control of the entity that is the source of the data
next we will talk to my georgia tech colleague dr
jon duke who came here from the regenstrief institute that created the technology used the indiana health information exchange
in textrank output the premier example of this here in the u
s
is the indiana health information exchange or ihie pronounced i hi
support for that came from the regenstrief foundation a philanthropic organization that describes its mission as to bring to the practice of medicine the most modern scientific advances from engineering business and the social sciences and to foster the rapid dissemination into medical practice of the new knowledge created by research
absent such unfortunately rare funding source this type of hie is usually economically impossible to create
in the case of ihie as you see here all the curated data is aggregated and stored centrally
we built our georgia tech fhir server using the omop data model
next we will talk to my georgia tech colleague dr
jon duke who came here from the regenstrief institute that created the technology used in the indiana health information exchange
reinforcement learning in the reinforcement learning course the content is structured in a way that is conversational between two authors
this brings about another challenge which is summarizing content that is part of a conversation
below is an example of both bert and textrank summarizing this content for a lecture reducing the sentence size from to
in this example the strengths of the bert model can be seen as it addresses that the build up to the maximum likelihood is the equivalent to
it properly selects the definition of as well and strings together a summary which properly abstracts the data
while textrank contains the word maximum likelihood the sentences are rather random making it difficult to understand from the content given
bert output all right so here is a rule we going to call the td rule which gives it a different name from td
so the thing that random here at least the way we been talking about it is if we were in some state and we make a transition we do know what state we going to end up in
so really we taking an expectation over what we get as the next state of the reward plus the discounted estimated value of that next state
yeah so this is exactly what the maximum likelihood estimate is supposed to be
as long as these probabilities for what the next state is going to be match what the data has shown so far as the transition to state
textrank output that the way we going to compute our value estimate for the state that we just left when we make a transition at epoch t for trajectory t big t is what the previous value was
so what would we expect this outcome to look like on average right yeah so here the idea is that if we repeat this update rule on the finite data that we got over and over and over again then we actually taking an average with respect to how often we seen each of those transitions
kind of everything does the right thing in infinite data
and the issue is that if we run our update rule over that data over and over and over again then we going to get the effect of having a maximum likelihood model
future improvements for model future improvements one strategy could be to fine tune the model on udacity lectures since the current model is the default pre trained model from google
the other improvement would be to fill in the gaps for missing context from the summary and automatically determine the best number of sentences to represent the lecture
this could be potentially done through the sum of squares with clustering
for the service the database would eventually need to be converted to a more permanent solution over sqlite
also having logins where individuals could manage their own summaries would be another beneficial feature
conclusion having the capability to properly summarize lectures is a powerful study and memory refreshing tool for university students
automatic extractive summarization researchers have attempted to solve this problem for the last several years producing research with decent results
however most of the approaches leave room for improvement as they utilize dated natural language processing models
leveraging the most current deep learning nlp model called bert there is a steady improvement on dated approaches such as textrank in the quality of summaries combining context with the most important sentences
the lecture summarization service utilizes the bert model to produce summaries for users based on their specified configuration
while the service for automatic extractive summarization was not perfect it provided the next step in quality when compared to dated approaches
references
balasubramanian v
doraisamy s
g
kanakarajan n
k

a multimodal approach for extracting content descriptive metadata from lecture videos
journal of intelligent information systems

celikyilmaz a
hakkani tr d
june
discovery of topically coherent sentences for extractive summarization
in proceedings of the annual meeting of the association for computational linguistics human language technologies volume pp

association for computational linguistics

che x
yang h
meinel c

automatic online lecture highlighting based on multimedia analysis
ieee transactions on learning technologies

devlin j
chang m
w
lee k
toutanova k

bert pre training of deep bidirectional transformers for language understanding
arxiv preprint


garg s

automatic text summarization of video lectures using subtitles
in recent developments in intelligent computing communication and devices pp

springer singapore

genest p
e
lapalme g
june
framework for abstractive summarization using text to text generation
in proceedings of the workshop on monolingual text to text generation pp

association for computational linguistics

glass j
hazen t
j
cyphers s
malioutov i
huynh d
barzilay r

recent progress in the mit spoken lecture processing project
in eighth annual conference of the international speech communication association

kota b
u
davila k
stone a
setlur s
govindaraju v
august
automated detection of handwritten whiteboard content in lecture videos for summarization
in international conference on frontiers in handwriting recognition icfhr pp

ieee

logeswaran l
lee h

an efficient framework for learning sentence representations
arxiv preprint


murray g
renals s
carletta j

extractive summarization of meeting recordings

shimada a
okubo f
yin c
ogata h

automatic summarization of lecture slides for enhanced student preview technical report and user study
ieee transactions on learning technologies

vaswani a
shazeer n
parmar n
uszkoreit j
jones l
gomez a
n
polosukhin i

attention is all you need
in advances in neural information processing


van labeke n
whitelock d
field d
pulman s
richardson j
t
july
what is my essay really saying using extractive summarization to motivate reflection and redrafting
in aied workshops

wolf t
sanh v
rault t

pytorch pretrained bert the big extending repository of pretrained transformers

com huggingface pytorch bert
zhang j
j
chan h
y
fung p
december
improving lecture speech summarization using rhetorical information
in automatic speech recognition understanding
asru
ieee workshop on pp


