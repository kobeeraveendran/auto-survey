journal of articial intelligence research submitted published generating extractive summaries of scientic paradigms vahed qazvinian department of eecs university of michigan ann arbor mi dragomir r
radev department of eecs school of information university of michigan ann arbor mi saif m
mohammad national research council canada ottawa ontario canada
edu
edu saif
cnrc
gc
ca bonnie dorr david zajic michael whidby taesun moon institute for advanced computer studies computer science university of maryland college park md
umd
edu
umd
edu
edu
umd
edu abstract researchers and scientists increasingly nd themselves in the position of having to quickly understand large amounts of technical material
our goal is to eectively serve this need by using bibliometric text mining and summarization techniques to generate summaries of scientic literature
we show how we can use citations to produce matically generated readily consumable technical extractive summaries
we rst propose c lexrank a model for summarizing single scientic articles based on citations which employs community detection and extracts salient information rich sentences
next we further extend our experiments to summarize a set of papers which cover the same entic topic
we generate extractive summaries of a set of question answering qa and dependency parsing dp papers their abstracts and their citation sentences and show that citations have unique information amenable to creating a summary

introduction in today s rapidly expanding disciplines scientists and scholars are constantly faced with the daunting task of keeping up with knowledge in their eld
in addition the increasingly interconnected nature of real world tasks often requires experts in one discipline to rapidly learn about other areas in a short amount of time
cross disciplinary research requires scientists in areas such as linguistics biology and sociology to learn about computational approaches and applications such as computational linguistics biological modeling and social networks
authors of journal articles and books must write accurate summaries of previous work ranging from short summaries of related research to in depth historical notes
interdisciplinary review panels are often called upon to review proposals in a wide range of ai access foundation
all rights reserved
qazvinian et al
areas some of which may be unfamiliar to panelists
thus they must learn about a new discipline on the y in order to relate their own expertise to the proposal
our goal is to eectively serve these needs by combining two currently available nologies bibliometric lexical link mining that exploits the structure of citations and summarization techniques that exploit the content of the material in both the citing and cited papers
it is generally agreed upon that manually written abstracts are good summaries of vidual papers
more recently qazvinian and radev argued that citation sentences i
e
set of sentences that appear in other papers and cite a given article are useful in creating a summary of important contributions of a research paper
kaplan iida and tokunaga introduced citation site as a block of text that includes a citation and discusses the cited text
this work used a machine learning method for extracting citations from research papers and evaluates the result using an annotated corpus of papers citing articles
moreover qazvinian and radev showed the usefulness of using implicit citations i
e
context sentences sentences that occur before or after a citation sentence and do not explicitly cite the target paper but discuss its contributions in summary eration
teufel argued that citations could contain subjective content and that this content can be exploited for summary generation
additional work mohammad et al
demonstrated the usefulness of citations for producing multi document summaries of scientic articles
follow up work indicated that further improvements to citation handling enables the production of more uent summaries whidby
in our work we develop summarization systems that exploit citations
specically we compare and contrast the usefulness of abstracts and of citations in automatically generating a technical summary on a given topic from multiple research papers
our ndings suggest that abstracts and citations have some overlapping information but they also have a signicant amount of unique summary amenable information
ticularly we provide evidence that citation sentences contain crucial information that is not available or hard to extract from abstracts and papers alone
we propose c lexrank a graph based summarization system
this method models a set of citing sentences as a network in which vertices are sentences and edges represent their lexical similarity
c lexrank then identies vertex communities clusters in this network and selects sentences from dierent communities to increase diversity in the summary
using dierent sets of citation sentences extracted from dierent nlp topics in the anthology network we show that c lexrank is eective in producing a summary of a paper s contributions
we compare c lexrank with a wide range of state of the art summarization systems that leverage diversity mmr divrank mascs employ graph structure divrank lexrank or employ sentence compression mascs to produce a summary
we extend our experiments from summarizing the contributions of a single article to generating summaries of scientic topics
our evaluation experiments for extractive summary generation are applied to a set of papers in the research area of question answering qa and another set of papers on dependency parsing dp

association for computational linguistics generating extractive summaries of scientific paradigms we provide some background for this work including the primary features of a technical summary and also the types of input that are used in our study full papers abstracts and citation sentences

background automatically creating technical extractive summaries is signicantly distinct from tional multi document summarization
below we describe the primary characteristics of a technical extractive summary and we present dierent types of input texts that we used for the production of extractive summaries


technical extractive summaries in the case of multi document summarization the goal is to produce a readable tion of multiple documents whereas in the case of technical summary creation the goal is to convey the key features and basic underpinnings of a particular eld early and late developments important contributions and ndings contradicting positions that may verse trends or start new and basic denitions and examples that enable rapid understanding of a eld by non experts
a prototypical example of a technical summary is that of chapter notes i
e
short word descriptions of sub areas found at the end of chapters of textbooks such as jurafsky and martin s
one might imagine producing such descriptions cally then hand editing them and rening them for use in an actual textbook
previously mohammad et al
conducted a human analysis of these chapter notes and revealed a set of conventions an outline of which is provided here with example sentences in italics
introductory opening statement the earliest computational use of x was in y sidered by many to be the foundational work in this area

denitional follow up x is dened as y

elaboration of denition e

with an example most early algorithms were based on z
x

deeper elaboration e

pointing out issues with initial approaches unfortunately this model seems to be wrong

contrasting denition algorithms since then



introduction of additional specic instances historical background with citations two classic approaches are described in q

references to other summaries r provides a comprehensive guide to the details behind the notion of text level categories or zoning of technical papers related to the summary components enumerated above has been investigated previously in the work of teufel and moens and nanba kando and okumura
these earlier works focused on qazvinian et al
the analysis of scientic papers based on their rhetorical structure and on determining the portions of papers that contain new results comparisons to earlier work
the work described here focuses on the synthesis of technical summary based on knowledge gleaned from rhetorical structure not unlike that of the work of these earlier researchers but guided by structural patterns along the lines of the conventions listed above
although our current approach to summary creation does not yet incorporate a fully pattern based component our ultimate objective is to apply these patterns to guide the creation and renement of the nal output
as a rst step toward this goal we use citation sentences closest in structure to the patterns identied by convention above to pick out the most important content for summary creation


scholarly texts published research on a particular topic can be summarized from two dierent kinds of sources where an author describes her own work and where others describe an author s work usually in relation to their own work
the author s description of her own work can be found in her paper
how others perceive her work is spread across other papers that cite her work
traditionally technical summary generation has been tackled by summarizing a set of research papers pertaining to the topic
however individual research papers usually come with manually created summariestheir abstracts
the abstract of a paper may have sentences that set the context state the problem statement mention how the problem is approached and the bottom line results all in to words
thus using only the abstracts instead of full papers as input to a summarization system is worth exploring
whereas the abstract of a paper presents what the authors think to be the important aspects of a paper the citations to a paper capture what others in the eld perceive as the broader contributions of the paper
the two perspectives are expected to have some overlap in their content but the citations also contain additional information not found in abstracts elkiss shen fader erkan states radev nakov hearst
for example authors may describe how a particular methodology from one paper was combined with another from a dierent paper to overcome some of the drawbacks of each
citations are also indicators of what contributions described in a paper were inuential over time
another feature that distinguishes citations from abstracts is that citations tend to have a certain amount of redundant information
this is because multiple papers may describe the same contributions of a target paper
this redundancy can be exploited by automatic systems to determine the important contributions of the target paper
our goal is to test the hypothesis that an eective technical summary will reect mation on research not only from the perspective of its authors but also from the perspective of others who use commend discredit or add to it
before describing our experiments with technical papers abstracts and citations we rst summarize relevant prior work that used these sources of information as input
the rest of this paper is organized as follows
after reviewing the related work we present an analysis of citations and demonstrate that they contain summary amenable information
in the process we develop c lexrank a citation based summarization system
in section we show that state of the art automatic summarization systems create more generating extractive summaries of scientific paradigms contentful summaries of citations of individual documents than those created simply by random sampling
we also show that c lexrank performs better than other state of art summarization systems when producing both and word extracts
in section we extend our experiments to summarize a set of papers representing the same scientic topic using the source texts as well as citations to the topic papers
additionally we show the usefulness of citation sentences in automatically generating a technical summary on a given topic
we observe that as expected abstracts are useful in summary creation but notably we also conclude that citations contain crucial information not present in or at least not easily extractable from abstracts
we further discover that abstracts are biased and thus complementary to the broader perspective inherent in citation sentences these dierences enable the use of a range of dierent levels and types of information in the summary

related work in this section we review related prior work in two categories
first we review previous research on citation analysis and then we discuss prior work on capturing diversity in automatic text summarization

citation analysis previous work has analyzed citation and collaboration networks teufel siddharthan tidhar newman and scientic article summarization teufel moens
bradshaw beneted from citations to determine the content of articles and introduce reference directed indexing to improve the results of a search engine
nanba abekawa okumura and saito and nanba et al
analyzed citation sentences and automatically categorize citations into three groups using pre dened phrase based rules
this categorization was then used to build a tool to help researchers analyze citations and write scientic summaries
nanba and okumura also discussed the same citation categorization to support a system for writing a survey
nanba and okumura and nanba et al
reported that co citation implies similarity by showing that the textual similarity of co cited papers is proportional to the proximity of their citations in the citing article
previous work has shown the importance of the citation sentences in understanding scientic contributions
elkiss et al
performed a large scale study on citations and their importance
they conducted several experiments on a set of articles from the free pubmed central pmc and from acm digital library
results from this experiment conrmed that the average cosine between sentences in the set of citations to an article is consistently higher than that of its abstract
they also reported that this number is much greater than the average cosine between citation sentences and a randomly chosen document as well as between citation sentences and the abstract
finally they concluded that the content of citing sentences has much greater uniformity than the content of the corresponding abstract implying that citations are more focused and contain additional information that does not appear in abstracts


pubmedcentral
gov qazvinian et al
nakov and hearst performed a detailed manual study of citations in the area of molecular interactions and found that the set of citations to a given target paper cover most information found in the abstract of that article as well as more concepts mainly related to experimental procedures
kupiec pedersen and chen used the abstracts of scientic articles as a target summary
they used engineering information summaries that are mostly indicative in nature
kan klavans and mckeown used annotated bibliographies to cover certain aspects of summarization and suggest guidelines that summaries should also include metadata and critical document features as well as the prominent content based features
siddharthan and teufel described a new reference task and show high human agreement as well as an improvement in the performance of argumentative zoning teufel
in argumentative zoning a rhetorical classication task seven classes own other background textual aim basis and contrast are used to label sentences according to their role in the author s argument
the problem of automatic related work summarization is addressed by hoang and kan
in their work hoang and kan used a set of keywords representing a hierarchy of paper topics and assigned a score to each input sentence to construct an extractive summary
athar addressed the problem of identifying positive and negative sentiment ity in citations to scientic papers
similarly athar and teufel used context enriched citations to classify scientic sentiment towards a target paper

leveraging diversity in summarization in summarization a number of previous methods have focused on the diversity of spectives
mei guo and radev introduced divrank a diversity focused ranking methodology based on reinforced random walks in information networks
their random walk model which incorporates the rich gets richer mechanism to pagerank with forcements on transition probabilities between vertices showed promising results on the document understanding conference duc dataset
divrank is a state of the art graph based method and it leverages the diversity of perspectives in summarization
fore we chose this algorithm as an important baseline in our experiments and we will discuss it in more detail in section
a similar ranking algorithm described by zhu goldberg van gael and andrzejewski is the grasshopper ranking model which leverages an absorbing random walk
this model starts with a regular time homogeneous random walk and in each step the vertex with the highest weight is set as an absorbing state
paul zhai and girju addressed the problem of summarizing opinionated text using comparative lexrank a random walk model inspired by lexrank erkan radev
comparative lexrank rst assigns dierent sentences to clusters based on their contrastiveness with each other
it then modies the graph based on cluster information and performs lexrank on the modied cosine similarity graph
perhaps the most well known summarization method to address diversity in tion is maximal marginal relevance mmr carbonell goldstein
this method is based on a greedy algorithm that selects sentences in each step that are the least similar generating extractive summaries of scientific paradigms to the summary so far
we compare our summarization output with that of mmr and discuss this algorithm in more details in section
in prior work on evaluating independent contributions in content generation voorhees studied ir systems and showed that relevance judgments dier signicantly between humans but relative rankings show high degrees of stability across annotators
in other work van halteren and teufel asked dutch students and nlp researchers to summarize a bbc news report resulting in dierent summaries
they also used duc provided summaries and annotations from student participants and additional researchers to create summaries for another news article in the duc datasets
they calculated the kappa statistic carletta krippendor and observed high ment indicating that the task of atomic semantic unit factoid extraction can be robustly performed in naturally occurring text without any copy editing
the diversity of perspectives and the growth of the factoid inventory qazvinian radev also aects evaluation in text summarization
evaluation methods are ther extrinsic in which the summaries are evaluated based on their quality in performing a specic task sparck jones or intrinsic where the quality of the summary itself is evaluated regardless of any applied task van halteren teufel nenkova neau
these evaluation methods assess the information content in the summaries that are generated automatically

citation based summarization the acl anthology aan is a manually curated resource built on top of the acl bird dale dorr gibson joseph kan lee powley radev tan
aan includes all the papers published by acl and related organizations as well as the computational linguistics journal over a period of four decades
aan consists of more than papers from more than authors each distinguished with a unique acl id together with their full texts abstracts and citation information
it also includes other valuable metadata such as author aliations citation and collaboration networks and various centrality measures radev muthukrishnan qazvinian joseph radev
to study citations across dierent areas within computational linguistics we rst tracted six dierent sets of papers from aan corresponding to dierent nlp topics pendency parsing dp phrase based machine translation pbmt text summarization summ question answering qa textual entailment te and conditional random fields crf
to build each set we matched the topic phrase against the title and the tent of aan papers and extracted the highest cited papers
table shows the number of articles and the number of citation sentences in each
the number of citations in each set shows that number of sentences that are used as an input to various summarization systems in our experiments



umich
edu

aclweb
org anthology
the number of incoming citations are from aan s release
the alignment template approach to statistical machine translation improvements in phrase based statistical machine translation a hierarchical phrase based model for statistical machine translation sentence reduction for automatic text summarization cut and paste based text summarization the automated acquisition of topic signatures


title three new probabilistic models for dependency parsing


three generative lexicalized models for statistical parsing a statistical parser for czech pseudo projective dependency parsing on line large margin training of dependency parsers statistical phrase based translation acl id an evaluation exercise for word alignment centroid based summarization of multiple documents


the potential and limitations of automatic sentence extraction


a question answering system supported by information extraction a rule based question answering system for reading


measuring the semantic similarity of texts weekly supervised natural language learning


accurate information extraction from research papers


semantic role labelling with tree crfs discriminative word alignment with conditional random fields a hybrid markov semi markov crf for sentence segmentation learning surface text patterns for a question answering system towards answering opinion questions separating facts from opinions


oine strategies for online question answering


scaling web based acquisition of entailment relations a semantic approach to recognizing textual entailment recognising textual entailment with logical inference the distributional inclusion hypotheses and lexical entailment year citations p d t m b p m m u s a q e t f r c table papers were extracted from dierent nlp topics in aan dependency parsing dp phrase based machine translation pbmt text summarization summ question answering qa textual entailment te and conditional random fields crf
each set consists of the highest cited papers in aan s release whose title and content matched the corresponding topic phrase
qazvinian et al
generating extractive summaries of scientific paradigms below we describe our approach to citation analysis including our calculation of judge agreement
we then describe our c lexrank method for extracting citation tences

citation analysis to analyze citations we designed an annotation task that requires explicit denitions that distinguish between phrases that represent the same or dierent information units
nately there is little consensus in the literature on such denitions
therefore following van halteren and teufel qazvinian and radev we made the following tion
we dene a nugget to be a phrasal information unit i
e
any phrase that would contain some information about the contributions of the cited paper
dierent nuggets may all represent the same atomic semantic unit which we refer to as a factoid
in the context of citations a factoid refers to a unique contribution of a target paper mentioned in a citation sentence
for example the following set of citations to eisner s mous parsing paper illustrate the set of factoids about this paper and suggest that dierent authors who cite a particular paper may discuss dierent contributions factoids of that paper
in the context of dps this edge based factorization method was proposed by eisner
eisner gave a generative model with a cubic parsing algorithm based on an edge factorization of trees
eisner proposed an parsing algorithm for pdg
if the parse has to be projective eisner s bottom up span algorithm can be used for the search
this example also suggests that dierent authors use dierent wordings nuggets to represent the same factoids
for instance cubic parsing and parsing algorithm are two nuggets that represent the same factoid about eisner
a similar example which we will use throughout the paper is the paper by cohn and blunsom identied with the acl id in table
this paper is cited in dierent sentences within aan
all of these sentences are listed in table
in each sentence the nuggets extracted by the annotators are underlined
as this table suggests a citation sentence may not discuss any of the contributions of the cited paper
for instance the last sentence does not contain any factoids about cohn and blunsom s work
the nuggets that are identied using the citation to the paper cohn blunsom account for a total number of factoids contributions identied for this paper tree structures semantic role labeling and a pipelined approach
following these examples we asked two annotators with background in natural guage processing to review each citing sentence and extract a list of phrases that represent a contribution of the cited paper
moreover to ensure that the extracted nuggets are explicitly mentioned in the citations we asked the annotators to rely merely on the set of citations to do the task and not on their background on this topic or the source of the
one of the annotators is an author of this paper
qazvinian et al
cited paper
finally we reviewed each list and collapsed phrases that represent the same contribution factoid
finding agreement between annotated well dened nuggets is straightforward and can be calculated in terms of kappa
however when nuggets themselves are to be extracted by annotators the task becomes less obvious
to calculate the agreement we annotated randomly selected citation sets twice paper from each of the nlp areas in table and designed a simple evaluation scheme based on kappa
for each n gram w in a given citation sentence we determine w is part of any nugget in either human annotations
if w occurs in both or neither then the two annotators agree on it and otherwise they do not
based on this agreement setup we can formalize the statistic as where p is the relative observed agreement among annotators and p is the ability that annotators agree by chance if each annotator is randomly assigning categories
table shows the unigram bigram and trigram based between the two human tators in the ve datasets that were annotated twice
these results suggest that human annotators can reach substantial agreement when trigram nuggets are examined and have reasonable agreement for unigram and bigram nuggets

c lexrank in this section we describe c lexrank as a method to extract citing sentences that cover a diverse set of factoids
our method works by modeling the set of citations as a network of sentences and identifying communities of sentences that cover similar factoids
once a good division of sentences is made we extract salient sentences from dierent communities
figure illustrates a representative example that depicts c lexrank s process


citation summary network in the rst step as shown in figure a we model the set of sentences that cite a specic paper with a network in which vertices represent citing sentences and undirected weighted edges show the degree of semantic relatedness between vertex pairs normally quantied by a similarity measure
we refer to this network as the citation summary network of an article
the similarity function should ideally assign high scores to sentence pairs that have the same factoids and should assign low scores to sentences that talk about dierent contributions of the target paper
previously qazvinian and radev examined dierent similarity measures cluding tf idf with various idf databases longest common sub sequence generation probability erkan and the levenstein distance on a training set of citations
they showed that the cosine similarity measure that employs tf idf vectors assigns higher ilarities to pairs that contain the same factoids
following qazvinian and radev we use the cosine similarity between tf idf vector models that employ a general idf to construct the citation summary network of each article

we use the idf corpus in the mead summarization system radev et al
which is generated using the english portion of the hong kong news parallel ma
generating extractive summaries of scientific paradigms factoid citation sentence our parsing model is based on a conditional random eld model however unlike previous treecrf work e

cohn blunsom jousse et al
we do not assume a particular tree structure and instead nd the most likely structure and labeling
some researchers xue palmer koomen et al
cohn som punyakanok et al
toutanova et al
used a pipelined approach to attack the task
they have been used for tree labelling in xml tree labelling jousse et al
and semantic role labelling tasks cohn blunsom
finally probabilistic models have also been applied to produce the tured output for example generative models thompson levy manning sequence tagging with classiers et al
pradhan et al
and conditional random fields on tree structures cohn blunsom
as for srl on news most researchers used the pipelined approach i
e
viding the task into several phases such as argument identication argument classication global inference
and conquering them individually xue palmer koomen et al
cohn blunsom punyakanok et al
toutanova et al

although t crfs are relatively new models they have already been applied to several nlp tasks such as semantic role labeling semantic annotation word sense disambiguation image modeling cohn blunsom tang et al
jun et al
awasthi et al

the model can be used for tasks like syntactic parsing finkel et al
and semantic role labeling cohn blunsom
regarding novel learning paradigms not applied in previous shared tasks we nd relevant vector machine rvm which is a kernel based linear nant inside the framework of sparse bayesian learning johansson nugues and tree conditional random fields t crf cohn blunsom that extend the sequential crf model to tree structures
n a we use crfs as our models for both tasks cohn blunsom
table the aan paper on crf by cohn blunsom is cited in nine dierent aan sentences
in each citation sentence the nuggets extracted by the annotators are underlined
qazvinian et al
average unigram bigram trigram vs
average

















table agreement between dierent annotators in terms of kappa in citation sets
citation summary network community structure c c lexrank output figure the c lexrank method extracts citing sentences that cover a diverse set of toids
the citation summary network in a models the set of sentences that cite a specic paper where vertices represent citing sentences and weighted edges show the degree of semantic relatedness between vertex pairs
the community structure in b corresponds to clustered sets of representative sentences extracted from citation sentences
the c lexrank output in c corresponds to the date sentences from dierent clusters that are used for building a summary


community structure in the second step as shown in figure we extract vertex communities from the citation summary network to generate summaries
we generate summaries by extracting representative sentences from the citation summary network
intuitively a good summary should include sentences that represent dierent contributions of a paper
therefore a good sentence selection from the citation summary network will include vertices that are similar to many other vertices and which are not very similar to each other
on the other hand a bad selection would include sentences that are only representing a small set of vertices in the graph
this is very similar to the concept of maximizing social inuence in social networks kempe kleinberg eva tardos
figure shows an example in which the selected two vertices in the citation summary networks represent a small subset of vertices in our work we try to select vertices that left and a larger subset of vertices right
generating extractive summaries of scientific paradigms maximize the size of the set of vertices that they represent
we achieve this by detecting dierent vertex communities in the citation summary network
bad sentence selection good sentence selection figure summaries are produced by using vertex coverage to select a set of representative vertices corresponding to sentences
selecting two similar vertices will cause the summary to cover fewer contributions of the target paper in a while selecting less similar vertices as the summary will increase the coverage of the summary b
in order to nd vertex communities and thus a good sentence selection we exploit the small world property of citation summary networks
a network is called small world if most of its vertices are not neighbors of each other but can be reached from one another by a small number of steps watts strogatz
recent research has shown that a wide range of natural graphs such as biological networks ravasz somera mongru oltvai barabasi food webs montoya sole brain neurons bassett bullmore and human languages ferrer i cancho sole exhibit the small world property
this common characteristic can be detected using two basic statistical properties the clustering coecient c and the average shortest path length
the clustering coecient of a graph measures the number of closed triangles in the graph
it describes how likely it is that two neighbors of a vertex are connected newman
watts and strogatz dene the clustering coecient as the average of the local clustering values for each vertex
c ci n the local clustering coecient ci for the ith vertex is the number of triangles connected to vertex i divided by the total possible number of triangles connected to vertex i
watts and strogatz show that small world networks are highly clustered and obtain tively short paths i
e
is small
previous work qazvinian radev shows that citation summary networks are highly clustered
these networks have small shortest paths and obtain clustering coecient values that are signicantly larger than random networks
moreover qazvinian and radev suggest that this is because of a community structure qazvinian et al
citation summary network latent community structure clustering output c lexrank ranking figure the c lexrank algorithm operates as follows on cohn and blunsom s citation summary network in the network a the vertices are citation sentences annotated with their nuggets from table and each edge is the cosine larity between the corresponding node pairs
shows that the network has an underlying structure which is captured by c lexrank in c
finally d shows the c lexrank output where vertex size is proportional to its lexrank value within the cluster
where each community is composed of a set of highly connected vertices with a small number of edges that fall between communities
figure a illustrates a real citation summary network built using the citation sentences in table in which each vertex is labeled with its corresponding nugget
with some semantic role tree t semantic role pipelined pipelined semantic role labeling t semantic role tree t semantic role pipelined pipelined semantic role labeling t semantic role tree t semantic role pipelined pipelined semantic role labeling t semantic role tree t semantic role pipelined pipelined semantic role labeling t crfpajek generating extractive summaries of scientific paradigms arrangement of the vertices in figure it becomes clear that the citation summary network of this paper has an underlying community structure in which sentences that cover similar factoids are closer to each other and form communities
for instance in this network there are at least observable communities one that is about tree structure one about semantic role labeling and the last one about the pipelined approach as proposed by cohn and blunsom
in order to detect these communities automatically we use modularity
modularity newman is a measure to evaluate the divisions that a community detection rithm generates
for a division with g groups they dene matrix egg whose component eij is the fraction of edges in the original network that connect vertices in components i j
then the modularity q can be dened as q i eii eijeki ijk intuitively q is the fraction of all the edges that are embedded within communities minus the expected value of the same quantity in a network with the same degrees but in which edges are placed at random regardless of the community structure
newman and girvan and newman showed across a wide range of simulated and real world works that larger q values are correlated with better graph clusterings
it is also shown by newman that if no edges exist that connect vertices across dierent clusters then q and conversely if the number of inter cluster edges is no better than random then q
other work smyth white showed empirically that modularity works well in practice in terms of both a nding good clusterings of vertices in networks where community structure is evident and indicating what the appropriate number of clusters k is for such a graph
c lexrank uses the clustering algorithm of clauset newman and moore which exploits modularity to detect vertex communities in a network
this network clustering method as discussed by clauset et al
is a hierarchical agglomeration algorithm which works by greedily optimizing the modularity in a linear running time for sparse graphs
more particularly their method continuously merges vertex or cluster pairs with the highest similarity and stops when modularity reaches the maximum value
this clustering algorithm is ecient n in the number of nodes n and does not require a determined number of clusters
these two characteristics makes this community detection algorithm particularly useful
figure c shows how the clustering algorithm detects factoid communities in cohn and blunsom s citation summary network
in this gure we have color coded vertices based on their community
the clustering algorithm assigns sentences and which are all about the tree structures to one cluster sentences and which are all about semantic role labeling to another cluster and nally assigns sentences and sentences and are both about pipelined approach to the last cluster
this gure also shows that sentence which discusses two factoids semantic role labeling and t crf connects the two vertex communities corresponding to factoids as a bridge
to evaluate how well the clustering method works in all of our datasets we calculated both the purity and the normalized mutual information nmi for the divisions in each citation set extracted using the community detection algorithm
purity zhao karypis qazvinian et al
is a method in which each cluster is assigned to the class with the majority vote in the cluster and the accuracy of this assignment is then measured by dividing the number of correctly assigned documents by n
more formally c n max j where


k is the set of clusters and c


cj is the set of classes
k is interpreted as the set of documents in the cluster k and cj as the set of documents in the class cj
we also calculate the normalized mutual information nmi
manning raghavan and schutze describe nmi as follows
let us assume


k is the set of clusters and c


cj is the set of classes
then c c where c is the mutual information in which p k p cj and p k cj are the probabilities of a document being in cluster k class cj and in the intersection of k and cj respectively and h is entropy c p k cj log p k cj p cj k j j n log n p k log p k n log n c in equation measures the amount of information that we would lose about the classes without the cluster assignments
the normalization factor in equation enables us to trade o the quality of the clustering against the number of clusters since entropy tends to increase with the number of clusters
for example reaches its maximum when each document is assigned to a separate cluster
because nmi is normalized we can use it to compare cluster assignments with dierent numbers of clusters
moreover is a tight upper bound for c making nmi obtain values between and
table lists the average purity and nmi across the papers in our collected dataset along with the analogous numbers for a division of the same size where vertices are randomly assigned to clusters


ranking the third step of the c lexrank process as shown in figure c is applied after the graph is clustered and the communities are formed
to produce the c lexrank output generating extractive summaries of scientific paradigms c c c c average



condence interval







table the average purity in boldface and normalized mutual information nmi values are shown for the papers in our collected dataset along with analogous values for a division of the same size where vertices are randomly assigned to clusters
we extract sentences from dierent clusters to build a summary
we start with the largest cluster and extract sentences using lexrank erkan radev within each cluster
in other words for each cluster i we made a lexical network of the sentences in that cluster ni
using lexrank we can nd the most central sentences in ni as salient sentences of i to include in the main summary
we choose for each cluster i the most salient sentence of i and if we have not reached the summary length limit we do that for the second most salient sentences of each cluster and so on
the cluster selection is in order of decreasing size
figure shows cohn and blunsom s citation summary network in which each vertex is plotted with a size proportional to its lexrank value within its cluster
this gure shows how c lexrank emphasizes on selecting a diverse set of sentences covering a diverse set of factoids
previously we mentioned that factoids with higher weights appear in a greater number of sentences and clustering aims to cluster such fact sharing sentences in the same nities
thus starting with the largest community is important to ensure that the system summary rst covers the factoids that are more frequently mentioned in other citation sentences and thus are more important
the last sentence in the example in table is as follows
we use crfs as our models for both tasks cohn blunsom
this sentence shows that a citation may not cover any contributions of the target paper
such sentences are assigned by the community detection algorithm in c lexrank to clusters to which they are semantically most similar
the intuition behind employing lexrank within each cluster is to try to avoid extracting such sentences for the summary since lexrank within a cluster enforces extracting the most central sentence in that cluster
in order to verify this we also try a variant of c lexrank in which we do not select sentences from clusters based on their salience in the cluster but rather in a round robin fashion in which all the sentences within a cluster are equally likely to be selected
we call this variant c rr
table shows the word summary constructed using c lexrank for our exemplar paper in which dierent nuggets are illustrated in bold
this summary is a perfect summary in terms of covering the dierent factoids about the paper
it includes citing sentences that talk about tree crf pipelined approach and semantic role labeling which are indeed cohn and blunsom s three main contributions
qazvinian et al
our parsing model is based on a conditional random eld model however unlike previous treecrf work e

cohn blunsom jousse et al
we do not assume a particular tree structure and instead nd the most likely structure and labeling
some researchers xue palmer koomen et al
cohn blunsom punyakanok et al
toutanova et al
used a pipelined approach to attack the task
the model can be used for tasks like syntactic parsing finkel et al
and semantic role labeling cohn blunsom
table this word summary was constructed using c lexrank for cohn and som s citation summary network
factoids are shown in bold face

other methods in our experiments in section we compare c lexrank to a number of other summarization systems
we compare c lexrank with random summarization to nd a lower bound on the pyramid scores in our experiments
we use lexrank and c rr as two variants of lexrank to investigate the usefulness of community detection and salient vertex selection in c lexrank
we evaluate divrank as a state of the art graph based summarization system that leverages diversity as well as mmr as a widely used diversity based summarization system
finally we use multiple alternate sentence compression summarizer mascs as a system that does not rely merely on extraction but rather produces a list of candidates by applying pre dened sentence compression rules
this method simply chooses citations in random order without replacement
since a citing sentence may cover no information about the cited paper as in the last sentence in table randomization has the drawback of selecting citations that have no valuable information in them
moreover the random selection procedure is more prone to produce redundant summaries as citing sentences that discuss the same factoid may be selected

random
lexrank one of the systems that we compare c lexrank with is lexrank erkan radev
it works by rst building a graph of all the documents in a cluster
the edges between corresponding vertices represent the cosine similarity between them if the cosine value is above a threshold
following erkan radev
once the network is built the system nds the most central sentences by performing a random walk on the graph
dj di generating extractive summaries of scientific paradigms equation shows that the probability that the random walker would visit dj depends on a random jump element as well as the probability that the random walk visits each of its neighbors di times the transition probability between and dj p dj
comparing c lexrank summaries with the ones from lexrank gives insight into how we can benet from detecting communities in citation sets
essentially c lexrank is the same as lexrank if all the vertices are assigned to the same cluster
by construction c lexrank should produce more diverse summaries covering dierent perspectives by turing communities of sentences that discuss the same factoids

mmr maximal marginal relevance mmr is proposed by carbonell and goldstein and is a widely used algorithm in generating summaries that reect the diversity of perspectives in the source documents das martins
mmr uses the pairwise cosine similarity matrix and greedily chooses sentences that are the least similar to those already in the summary
in particular m m r argmindida max dj a dj where a is the set of documents in the summary initialized to a
in equation a sentence di that is not in the summary a is chosen such that its highest similarity to summary sentences maxdj a dj is minimum among all unselected sentences

divrank we also compare c lexrank with a state of the art graph based summarization system that leverages diversity divrank
divrank is based on calculating stationary distribution of vertices using a modied random walk model
unlike other time homogeneous random walks e

pagerank divrank does not assume that the transition probabilities remain constant over time
divrank uses a vertex reinforced random walk model to rank graph vertices based on a diversity based centrality
the basic assumption in divrank is that the transition ability from a vertex to other is reinforced by the number of previous visits to the target vertex mei et al

particularly let us assume pt u v is the transition probability from any vertex u to vertex v at time t
then pt dj

dj
nt dj dt di where nt dj is the number of times the walk has visited dj up to time t and dt di dj dj v here is the prior distribution that determines the preference of visiting vertex and v is the transition probability from u to v prior to any reinforcement
mei et al
argue that the random walk could stay at the current state at each time and therefore qazvinian et al
assumes a hidden link from each vertex to itself thus dening v as v
if u v if u v here we try two variants of this algorithm divrank in which is uniform and divrank with priors in which where is the number of the words in the document dj and is a parameter
in our experiments
this prior distribution assigns larger probabilities to shorter sentences which will increase their likelihood of being salient
this will cause more sentences to be included in the summary and might increase the factoid coverage
in our experiments we follow mei et al
and set
and


mascs the last summarization system that we use as a baseline is multiple alternate sentence compression summarizer mascs zajic dorr lin schwartz
similar to vious previous baseline systems mascs s goal is to leverage diversity in summarization
mascs performs preprocessing on sentences that transforms them into new sentences thus expanding the pool of candidates available for inclusion in a summary beyond the set of sentences that occur in source documents
this is what makes mascs somewhat non extractive
in addition the preprocessing used in mascs for these experiments was specically adapted to the genre of citation sentences from scientic papers whidby
more particularly mascs is a summarization system that utilizes trimmer s zajic et al
sentence compression candidates to create summaries for a single or set of documents
summarization with mascs is performed in three stages
in the rst stage mascs generates several compressed sentence candidates for every sentence in a document from the cluster
the second stage involves calculating various ranking features for each of the compressed sentence candidates
in the nal stage sentence candidates are chosen for inclusion in the summary and are chosen based on a linear combination of features
trimmer can leverage the output of any constituency parser that uses the penn treebank conventions
at present the stanford parser klein manning is used
the set of compressions is ranked according to a set of features that may include metadata about the source sentences details of the compression process that generated the compression and externally calculated features of the compression
summaries are constructed by iteratively adding compressed sentences from the date pool until a length threshold is met
candidates are chosen by an implementation of mmr that uses features directly calculated from a candidate metadata about a candidate s source and its compression history relevance of the candidate to the topic and redundancy of the candidate to already selected candidates
the redundancy score in mascs uses an index of the words in the document set
p
p w where is a weighting factor set to
in our experiments
generating extractive summaries of scientific paradigms
experiments we used the sets of citations listed in table and employ c lexrank to produce two extractive summaries with dierent summary lengths and words for each set
in addition to c lexrank and c rr we also performed the same experiments with the baseline methods described in section most of which are aimed at leveraging diversity in summarization

evaluation to evaluate our system we use the pyramid evaluation method nenkova passonneau
each factoid in the citations to a paper corresponds to a summary content unit scu in nenkova passonneau
the score given by the pyramid method for a summary is the ratio of the sum of the weights of its factoids to the sum of the weights of an optimal summary
this score ranges from to and high scores show the summary content contains more heavily weighted factoids
if a factoid appears in more citation sentences than another factoid it is more important and thus should be assigned a higher weight
to weight the factoids we build a pyramid and each factoid falls in a tier
each tier shows the number of sentences a factoid appears in
thus the number of tiers in the pyramid is equal to the citation summary size
if a factoid appears in more sentences it falls in a higher tier
so if the factoid fi appears times in the citation summary it is assigned to the tier
the pyramid score formula that we use is computed as follows
suppose the pyramid has n tiers ti where tier tn on top and on the bottom
the weight of the factoids in tier ti will be i i
e
they appeared in i sentences
if denotes the number of factoids in tier ti and di is the number of factoids in the summary that appear in ti then the total factoid weight for the summary is as follows
additionally the optimal pyramid score for a summary with x factoids is m ax i j x n i n i where j lated as follows
t i x
subsequently the pyramid score for a summary is
results and discussion table shows the average pyramid score of the summaries generated using dierent methods with dierent lengths
longer summaries result in higher pyramid scores since the amount of information they cover is greater than shorter summaries
for the random sentence extraction baseline we repeat the experiment with dierent randomly generated seed values and report the average pyramid score of these summaries in table
this table shows d i di n p d m ax qazvinian et al
method random mmr lexrank divrank divrank with priors mascs c rr c lexrank c
i
condence interval length words pyramid







c
i
















length words c
i
















pyramid







table average pyramid scores are shown for two dierent summary lengths words and words for eight dierent methods including a summary generator based on random citation sentence selection
c lexrank outperforms all other methods that leverage diversity as well as random summaries and lexrank
highest scores for each input source are shown in bold
that c lexrank outperforms all other methods that leverage diversity as well as random summaries and lexrank
the results in this table also suggest that employing lexrank within each cluster is essential for the selection of salient citing sentences as the average pyramid scores from c rr where sentences are picked in a round robin fashion are lower


effect of community detection the community detection that c lexrank employs assigns highly similar citing sentences to the same cluster
this enables c lexrank to produce a diverse summary by selecting sentences from dierent clusters
this selection is done by assigning a score to each vertex using lexrank within the cluster
the modularity based clustering method described in section

which works by maximizing modularity in a clustering will always produce at least clusters
intuitively in a network in which all the vertices are assigned to the same community the fraction of edges that are embedded within that community is equal to the expected value of the same quantity in a network in which edges are placed at random
this will make q obtain its lower bound q
however in the hypothetical case where all the vertices belong to the same cluster lexrank will be the same as lexrank i
e
it will perform lexrank on the entire network
therefore comparing c lexrank and lexrank helps us understand the eect of clustering on summary quality
table shows that c lexrank produces summaries that obtain higher pyramid scores at both words and words
table shows the word summary that was constructed using lexrank for cohn and blunsom s citations
this summary unlike the one produced by c lexrank table does not cover all of the factoids of the target paper e

pipelined approach
moreover this summary has redundant information e

treecrf vs
t crf and contains a citation sentence that does not cover any of the contributions of cohn and blunsom
generating extractive summaries of scientific paradigms the model can be used for tasks like syntactic parsing finkel et al
and semantic role labeling cohn blunsom
we use crfs as our models for both tasks cohn blunsom
our parsing model is based on a conditional random eld model however unlike previous treecrf work e

cohn blunsom jousse et al
we do not assume a particular tree structure and instead nd the most likely structure and labeling
although t crfs are relatively new models they have already been applied to several nlp tasks such as semantic role labeling semantic annotation word sense disambiguation image modeling cohn blunsom tang et al
jun et al
awasthi et al

table the summary constructed using lexrank and cohn and blunsom s citation sentences
compared to the c lexrank summary in table lexrank does not produce a summary of all cohn and blunsom s contributions the summary is not truncated for clarity


salient vertex extraction selecting representative sentences vertices from dierent clusters is done using lexrank in the c lexrank algorithm
more particularly for each cluster c lexrank rst extracts a subgraph of the network representing vertices and edges in that cluster and then employs lexrank to assign a salience score to each vertex
an alternative idea could be selecting sentences from clusters at random c rr
in c rr we traverse between clusters in a round robin fashion and randomly select a previously unselected sentence from the cluster to include a summary
comparing c lexrank with c rr enables us to understand the eect of salience tion within communities
selecting vertices that are not a good representative of the cluster may result in picking sentences that do not cover contributions of the target paper e

sentence from table vertex in figure d
in fact table shows that c lexrank produces summaries with relatively and higher pyramid scores than c rr when extracting and word summaries respectively
moreover c rr performs better when longer summaries are produced since it extracts a greater number of sentences from each cluster increasing the likelihood of covering dierent factoids captured by dierent clusters

summaries of scientic topics in previous sections we described c lexrank as a method to identify communities of citations that discuss the same scientic contributions
we showed that c lexrank is eective in summarizing contributions of single scientic papers
however the ultimate goal of our work is to investigate whether citations have summary amenable information and also to build an end to end system that receives a query representing a scientic topic such as dependency parsing and produces a citation based automatic summary of the given topic
qazvinian et al
in this section we extend our experiments on using the tools explained in previous sections for automatic summarization of scientic topics
our evaluation experiments for extractive summary generation are on a set of papers in the research area of question answering qa and another set of papers on dependency parsing dp
the two sets of papers were compiled by selecting all the papers in aan that had the words question answering and dependency parsing respectively in the title and the content
there were papers in the qa set and papers in the dp set
we also compiled the citation sentences for the qa papers and the citation sentences for the dp papers

data preparation our goal is to determine if citations do indeed have useful information that one will want to put in a summary and if so how much of this information is not available in the inal papers and their abstracts
for this we evaluate each of the automatically generated summaries using two separate approaches nugget based pyramid evaluation and rouge
recall oriented understudy for gisting evaluation rouge is a metric that evaluates automatic summaries by comparing them against a set of human written references lin
two sets of gold standard data were manually created from the qa and dp citation sentences and abstracts we asked three with background in natural language processing to identify important nuggets of information worth including in a summary
we asked four nlp to write word summaries of the qa and dp datasets
then we determined how well the dierent automatically generated summaries perform against these gold standards
if the citations contain only redundant formation with respect to the abstracts and original papers then the summaries of citations will not perform better than others


nugget annotations for our rst evaluation approach we used a nugget based evaluation methodology voorhees nenkova passonneau hildebrandt katz lin lin fushman
we asked three annotators with background in natural language ing to review the citation sentences abstract sets for each of the papers in the qa and dp sets and manually extract prioritized lists of factoids or main contributions supplied by each paper
each factoid was assigned a weight based on the frequency with which it was listed by annotators as well as the priority it was assigned in each case
our automatically generated summaries were then scored based on the number and weight of the nuggets that they covered
more particularly the annotators had two distinct tasks for the qa set and one for the dp set extract nuggets for each of the qa papers based only on the citations to those papers extract nuggets for each of the qa papers based only on the abstracts of those papers and extract nuggets for each of the dp papers based only on the citations to those papers

creating gold standard data from complete papers is fairly arduous and was not pursued

two of the annotators are authors of this paper

all of the annotators are authors of this paper
generating extractive summaries of scientific paradigms human performance pyramid score average input qa citations qa ct nuggets qa ab nuggets input qa abstracts qa ct nuggets qa ab nuggets input dp citations dp ct nuggets
























table pyramid scores were computed for human created summaries of qa and dp data
the summaries were evaluated using nuggets drawn from qa citation sentences qa ct qa abstracts qa ab and dp citation sentences dp ct
one annotator completed the three tasks in full and the remaining two annotators jointly completed tasks and providing us with two complete annotations of the qa and dp citation sets and one annotation of the qa abstract set
for each task annotators constructed lists of prioritized nuggets per paper
this gave us distinct nuggets from the qa citation set nuggets from the qa abstract set and nuggets from the dp citation set
by collapsing similar nuggets we were able to identify factoids for the qa citation set factoids for the qa abstract set and factoids for the dp citation set
we obtained a weight for each factoid by reversing its priority out of e

a factoid listed with priority was assigned a weight of a nugget listed with priority was assigned a weight of
and summing the weights over each listing of that factoid


expert summaries in addition to nugget annotations we asked four nlp researchers to write word maries of the qa citation set qa abstract set and dp citation set
table gives the pyramid scores of the word summaries manually produced by experts
the summaries were evaluated using the nuggets drawn from the qa citations qa abstracts and dp citations
the average of their scores listed in the rightmost column may be considered a good score to aim for by the automatic summarization methods
additionally table presents rouge scores lin of each of expert written word summaries against each other e

versus all others and so forth
the average last column could be considered a ceiling in the performance of the automatic summarization systems

results obtained with other weighting schemes that ignored priority ratings and multiple mentions of a nugget by a single annotator showed the same trends as the ones shown by the selected weighting scheme
qazvinian et al
human performance average input qa citations qa ct refs
qa ab refs


input qa abstracts

qa ct refs
qa ab refs
input dp citations















dp ct refs





table scores were obtained for each of the manually created summaries by using the other three as reference
and rouge l followed similar patterns

automatic extractive summaries we used four summarization systems for our summary creation approach c lexrank rr lexrank and mascs
we automatically generated summaries for both qa and dp from three dierent types of documents full papers from the qa and dp sets qa and dp full papers pa only the abstracts of the qa and dp papers qa and dp abstracts ab and the citation sentences corresponding to the qa and dp papers qa and dp citations ct
we generated summaries each of length words by applying mascs lexrank and c lexrank on the three data types citation sentences abstracts and full papers for both qa and dp
table shows a fragment of one of the automatically generated summaries from qa citation sentences
we created six additional word summaries by randomly choosing sentences from citations abstracts and full papers of qa and dp
we will refer to them as random summaries
most of work in qa and paraphrasing focused on folding paraphrasing knowledge into question analyzer or answer locater rinaldi al
tomuro
in addition number of researchers have built systems to take reading comprehension examinations designed to evaluate children s reading levels charniak et al
hirschman et al
ng et al
rilo thelen wang et al

so called denition or other questions at recent trec evaluations voorhees serve as good examples
to better facilitate user information needs recent trends in qa research have shifted towards complex context based and interactive question answering voorhees small et al
harabagiu et al

table a fragment of one of the mascs generated summaries is illustrated here using the qa citation sentences as input
generating extractive summaries of scientific paradigms system performance pyramid score random c lexrank c rr lexrank mascs























input qa citations qa ct nuggets qa ab nuggets input qa abstracts qa ct nuggets qa ab nuggets input qa full papers qa ct nuggets qa ab nuggets input dp citations dp ct nuggets input dp abstracts





input dp full papers




dp ct nuggets




dp ct nuggets



table pyramid scores were computed for automatic summaries of qa and dp data
the summaries were evaluated using nuggets drawn from qa citation sentences qa ct qa abstracts qa ab and dp citation sentences dp ct
lexrank is computationally intensive and so was not run on the dp pa dataset as indicated by about sentences
highest scores for each input source are shown in bold
table gives the pyramid score values of the summaries generated by the four tomatic summarizers evaluated using nuggets drawn from the qa citation sentences qa abstracts and dp citation sentences
the table also includes results for the baseline random summaries
when we used the nuggets from the abstracts set for evaluation the summaries created from abstracts scored higher than the corresponding summaries created from citations and papers
further the best summaries generated from citations outscored the best summaries generated from papers
when we used the nuggets from citation sets for evaluation the best automatic summaries generated from citations outperform those generated from abstracts and full papers
all these pyramid results demonstrate that citations can contain useful information that is not available in the abstracts or the original papers and that abstracts can contain useful information that is not available in the citations or full papers
among the various automatic summarizers mascs performed best at this task in two cases exceeding the average human performance
note also that the random rizer outscored the automatic summarizers in cases where the nuggets were taken from a source dierent from that used to generate the summary
however one or two summarizers still tended to do well
this indicates a diculty in extracting the overlapping amenable information across the two sources
system performance random c lexrank c rr lexrank mascs input qa citations qa ct refs
qa ab refs


input qa abstracts qa ct refs
qa ab refs


input qa full papers

qa ct refs
qa ab refs
input dp citations input dp abstracts dp ct refs

input dp full papers
dp ct refs































dp ct refs





table scores of automatic summaries of qa and dp data
the summaries were evaluated by using human references created from qa citation sentences qa ct qa abstracts qa ab and dp citation sentences dp ct
these results are obtained after jack kning the human references so that the values can be compared to those in table
lexrank is computationally intensive and so was not run on the dp full papers set as indicated by about sentences
highest scores for each input source are shown in bold
qazvinian et al
generating extractive summaries of scientific paradigms we then evaluated each of the random summaries and those generated by the four summarization systems against the references
table lists rouge scores of summaries when the manually created word summary of the qa citation sentences summary of the qa abstracts and the summary of the dp citation sentences were used as gold standard
when we use manually created citation summaries as reference then the summaries generated from citations obtained signicantly better rouge scores than the summaries generated from abstracts and full papers p
result
this conrms that cial information amenable to creating a summary and present in citation sentences is not available or hard to extract from abstracts and papers alone
further the summaries generated from abstracts performed signicantly better than those generated from the full papers p
result
this suggests that abstracts and citations are generally denser in summary amenable information than full papers
when we use manually created abstract summaries as reference then the summaries generated from abstracts obtained signicantly better rouge scores than the summaries generated from citations and full papers p
result
further and more tantly the summaries generated from citations performed signicantly better than those generated from the full papers p
result
again this suggests that abstracts and citations are richer in summary amenable information
these results also show that abstracts of papers and citations have some overlapping information result and sult but they also have a signicant amount of unique summary amenable information result and result
among the automatic summarizers c lexrank and lexrank perform best
this is unlike the results found through the nugget evaluation method where mascs performed best
this suggests that mascs is better at identifying more useful nuggets of information but c lexrank and lexrank are better at producing unigrams and bigrams expected in a summary
to some extent this may be due to mascs s compression preprocessing which breaks large complex sentences into smaller ner grained units of content that correspond better to the amount of content in a nugget

conclusion in this paper we investigated the usefulness of directly summarizing citation sentences set of sentences that cite a paper in the automatic creation of technical summaries
we proposed c lexrank a graph based summarization model and generated summaries of single scientic articles selected from dierent topics in the acl anthology network aan
we also generated summaries of a set of question answering qa and dependency parsing dp papers their abstracts and their citation sentences using four state of art summarization systems c lexrank c rr lexrank and mascs
we then used two dierent approaches nugget based pyramid and rouge to evaluate the summaries
the results from both approaches and all four summarization systems show that both citation sentences and abstracts have unique summary amenable information
these results also demonstrate that multidocument summarization especially technical summary creation benets considerably from citations
we next plan to generate summaries using both citation sentences and abstracts together as input
given the overlapping content of abstracts and citation sentences discovered in qazvinian et al
the current study it is clear that redundancy detection will be an integral component of this future work
creating readily consumable technical summaries is a hard task especially when using only raw text and simple summarization techniques
therefore we intend to combine these summarization and bibliometric techniques with suitable visualization ods towards the creation of iterative technical survey tools systems that present summaries and bibliometric links in a visually convenient manner and which incorporate user feedback to produce even better summaries
current work on generating topic summaries is focused almost exclusively on extracting diverse factoid rich summaries
meanwhile the uency of the produced summaries has been mostly ignored
in future work we plan to employ some post processing techniques such as reference scope extraction and sentence simplication as described by abu jbara and radev to generate more readable and cohesive summaries

acknowledgments we would like to thank ahmed hassan rahul jha pradeep muthukrishan and arzucan ozgur for annotations and melissa egan for preliminary developments
we are also grateful to ben shneiderman judith klavans and jimmy lin for fruitful discussions and the mous reviewers for insightful readings and constructive guidance
the following authors vahed qazvinian dragomir r
radev saif m
mohammad bonnie dorr david zajic and michael whidby were supported in part by the national science foundation under grant no
iopener information organization for penning expositions on research awarded to the university of michigan and the university of maryland
any opinions ndings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reect the views of the national science foundation
the following authors michael whidby and taesun moon were supported in part by the intelligence advanced research projects activity iarpa via department of interior tional business center doi nbc contract number
the u
s
government is authorized to reproduce and distribute reprints for governmental purposes not ing any copyright annotation thereon
disclaimer the views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the ocial policies or endorsements either expressed or implied of iarpa doi nbc or the u
s
government
references abu jbara a
radev d

coherent citation based summarization of scientic papers
in proceedings of the annual conference of the association for tational linguistics pp

athar a

sentiment analysis of citations using sentence structure based features
in proceedings of the acl student session hlt ss pp

athar a
teufel s

context enhanced citation sentiment detection
in ings of the conference of the north american chapter of the association for computational linguistics human language technologies pp
montreal canada
association for computational linguistics
generating extractive summaries of scientific paradigms awasthi p
gagrani a
ravindran b

image modeling using tree structured conditional random elds
in proceedings of the international joint conference on artical intelligence pp

bassett d
s
bullmore e

small world brain networks
the neuroscientist
bird s
dale r
dorr b
j
gibson b
joseph m
kan m

lee d
powley b
radev d
r
tan y
f

the acl anthology reference corpus a reference dataset for bibliographic research in computational linguistics
in proceedings of the international conference on language resources and evaluation lrec may june marrakech morocco
bradshaw s

reference directed indexing indexing scientic literature in the context of its use
ph
d
thesis northwestern university
bradshaw s

reference directed indexing redeeming relevance for subject search in citation indexes
in proceedings of the european conference on research and advanced technology for digital libraries
carbonell j
g
goldstein j

the use of mmr diversity based reranking for reordering documents and producing summaries
in proceedings of the annual international acm sigir conference on research and development in information retrieval pp

carletta j

assessing agreement on classication tasks the kappa statistic
putational linguistics
charniak e
altun y
braz r
d
s
garrett b
kosmala m
moscovich t
pang l
pyo c
sun y
wy w
yang z
zeller s
zorn l

reading comprehension programs in a statistical language processing class
in proceedings of the anlp naacl workshop on reading comprehension tests as tion for computer based language understanding sytems volume anlp readingcomp pp

clauset a
newman m
e
j
moore c

finding community structure in very large networks
phys
rev
e
cohn t
blunsom p

semantic role labelling with tree conditional random elds
in proceedings of the ninth conference on computational natural language learning pp

association for computational linguistics
das d
martins a

a survey on automatic text summarization
literature survey for the language and statistics ii course at cmu
eisner j

three new probabilistic models for dependency parsing an exploration
in proceedings of the annual conference of the association for computational linguistics pp

association for computational linguistics
elkiss a
shen s
fader a
erkan g
states d
radev d
r

blind men and elephants what do citation summaries tell us about a research article
journal of the american society for information science and technology
qazvinian et al
erkan g

language model based document clustering using random walks
in proceedings of the hlt naacl conference pp
new york city usa
sociation for computational linguistics
erkan g
radev d
r

lexrank graph based centrality as salience in text summarization
journal of articial intelligence research jair
ferrer i cancho r
sole r
v

the small world of human language
proceedings of the royal society of london b
finkel j
r
kleeman a
manning c
d

ecient feature based conditional random eld parsing
in proceedings of hlt pp
columbus ohio
association for computational linguistics
gildea d
jurafsky d

automatic labeling of semantic roles
comput
linguist

harabagiu s
hickl a
lehmann j
moldovan d

experiments with interactive in proceedings of the annual meeting on association for question answering
computational linguistics acl pp

hildebrandt w
katz b
lin j

overview of the trec question answering track
in proceedings of the north american chapter of the association for tational linguistics human language technologies hlt naacl
hirschman l
light m
breck e
burger j
d

deep read a reading prehension system
in proceedings of the annual meeting of the association for computational linguistics on computational linguistics acl pp

hoang c
d
v
kan m


towards automated related work summarization
in proceedings of the international conference on computational linguistics pp
beijing china
coling organizing committee
johansson r
nugues p

sparse bayesian classication of predicate arguments
in proceedings of the ninth conference on computational natural language learning pp
ann arbor michigan
association for computational linguistics
joseph m
t
radev d
r

citation analysis centrality and the acl thology
tech
rep
cse university of michigan
department of electrical engineering and computer science
jousse f
gilleron r
tellier i
tommasi m

conditional random elds for xml trees
in in workshop on mining and learning in graphs
jun hatori y
m
tsujii j
on contribution of sense dependencies to word sense disambiguation
journal of natural language processing
jurafsky d
martin j
h

speech and language processing an introduction to natural language processing speech recognition and computational linguistics edition
prentice hall
kan m

klavans j
l
mckeown k
r

using the annotated bibliography in the international conference on as a resource for indicative summarization
language resources and evaluation lrec las palmas spain
generating extractive summaries of scientific paradigms kaplan d
iida r
tokunaga t

automatic extraction of citation contexts for research paper summarization a coreference chain based approach
in proceedings of the workshop on text and citation analysis for scholarly digital libraries pp
suntec city singapore
kempe d
kleinberg j
eva tardos
maximizing the spread of inuence through a social network
in proceedings of the acm sigkdd international conference on knowledge discovery and data mining pp

acm
klein d
manning c
d

accurate unlexicalized parsing
in proceedings of the annual conference of the association for computational linguistics pp

koomen p
punyakanok v
roth d
yih w


generalized inference with multiple semantic role labeling systems
in proceedings of the ninth conference on computational natural language learning pp
ann arbor michigan
association for computational linguistics
krippendor k

content analysis an introduction to its methodology
beverly hills sage publications
kupiec j
pedersen j
chen f

a trainable document summarizer
in ceedings of the annual international acm sigir conference on research and development in information retrieval pp

lin c


rouge a package for automatic evaluation of summaries
in ings of the acl workshop on text summarization branches out
lin j
j
demner fushman d

methods for automatically evaluating answers to complex questions
information retrieval
ma x

hong kong news parallel text
linguistic data consortium philadelphia
manning c
d
raghavan p
schutze h

introduction to information retrieval
cambridge university press
l
comas p
gimenez j
n

semantic role labeling as sequential tagging
in proceedings of the ninth conference on computational natural language learning conll pp

mei q
guo j
radev d

divrank the interplay of prestige and diversity in proceedings of the acm sigkdd international in information networks
conference on knowledge discovery and data mining pp

mohammad s
dorr b
egan m
hassan a
muthukrishan p
qazvinian v
radev d
zajic d

using citations to generate surveys of scientic paradigms
in proceedings of the north american chapter of the association for computational guistics human language technologies hlt naacl pp
boulder colorado
montoya j
m
sole r
v

small world patterns in food webs
journal of theoretical biology
qazvinian et al
nakov p
d
a
hearst m

do peers see more in a paper than its authors
advances in bioinformatics special issue on literature mining solutions for life science research
nanba h
abekawa t
okumura m
saito s

bilingual presri integration in proceedings of riao pp
of multiple research paper databases
avignon france
nanba h
kando n
okumura m

classication of research papers using citation links and citation types towards automatic review article generation
in proceedings of the sig classication research workshop pp
chicago usa
nanba h
okumura m

towards multi paper summarization using reference information
in proceedings of the international joint conference on articial intelligence pp

nenkova a
passonneau r

evaluating content selection in summarization the pyramid method
in proceedings of the north american chapter of the association for computational linguistics human language technologies hlt naacl
newman m
e
j

the structure of scientic collaboration networks
pnas

newman m
e
j

the structure and function of complex networks
siam review newman m
e
j

analysis of weighted networks
physical review e
newman m
e
j

fast algorithm for detecting community structure in networks
phys
rev
e
newman m
e
j
girvan m

finding and evaluating community structure in networks
phys
rev
e
ng h
t
teo l
h
kwan j
l
p

a machine learning approach to swering questions for reading comprehension tests
in proceedings of the joint sigdat conference on empirical methods in natural language processing and very large corpora held in conjunction with the annual meeting of the association for computational linguistics volume emnlp pp

paul m
zhai c
girju r

summarizing contrastive viewpoints in opinionated in proceedings of the conference on empirical methods in natural language text
processing pp

pradhan s
hacioglu k
ward w
martin j
h
jurafsky d

semantic role chunking combining complementary syntactic views
in proceedings of the ninth conference on computational natural language learning conll pp

punyakanok v
roth d
yih w

the importance of syntactic parsing and inference in semantic role labeling
computational linguistics
qazvinian v
radev d
r

scientic paper summarization using citation mary networks
in proceedings of the international conference on computational linguistics manchester uk
generating extractive summaries of scientific paradigms qazvinian v
radev d
r

identifying non explicit citing sentences for based summarization

in proceedings of the annual conference of the tion for computational linguistics pp
uppsala sweden
qazvinian v
radev d
r

exploiting phase transition in latent networks in proceedings of the association for the advancement of articial for clustering
intelligence
qazvinian v
radev d
r

learning from collective human behavior to duce diversity in lexical choice
in proceedings of the annual conference of the association for computational linguistics pp

radev d
allison t
blair goldensohn s
blitzer j
c elebi a
dimitrov s
drabek e
hakim a
lam w
liu d
otterbacher j
qi h
saggion h
teufel s
topper m
winkel a
zhang z

mead a platform for multidocument multilingual text summarization
in lrec lisbon portugal
radev d
r
muthukrishnan p
qazvinian v

the acl anthology network corpus
in acl workshop on natural language processing and information retrieval for digital libraries singapore
ravasz e
somera a
mongru d
oltvai z
barabasi a

hierarchical nization of modularity in metabolic networks
science
rilo e
thelen m

a rule based question answering system for reading prehension tests
in proceedings of the anlp naacl workshop on reading comprehension tests as evaluation for computer based language understanding sytems volume anlp naacl readingcomp pp

rinaldi f
dowdall j
kaljurand k
hess m
molla d

exploiting phrases in a question answering system
in proceedings of the second international workshop on paraphrasing volume paraphrase pp

siddharthan a
teufel s

whose idea was this and why does it matter attributing scientic work to citations
in proceedings of the north american chapter of the association for computational linguistics human language technologies hlt naacl
small s
liu t
shimizu n
strzalkowski t

hitiqa an interactive question answering system a preliminary report
in proceedings of the acl workshop on multilingual summarization and question answering
smyth s
white s

a spectral clustering approach to nding communities in graphs
in proceedings of the siam international conference on data mining pp

sparck jones k

automatic summarizing factors and directions
in mani i
maybury m
t
eds
advances in automatic text summarization chap
pp

the mit press
tang j
hong m
li j
liang b

tree structured conditional random elds for semantic annotation
in proceedings of the international semantic web ference pp

qazvinian et al
teufel s

argumentative zoning for improved citation indexing
computing tude and aect in text theory and applications
teufel s
moens m

summarizing scientic articles experiments with relevance and rhetorical status
computational linguistics
teufel s
siddharthan a
tidhar d

automatic classication of citation in proceedings of the conference on empirical methods in natural language tion
processing pp
sydney australia
thompson c
levy r
manning c

a generative model for framenet semantic in proceedings of the fourteenth european conference on machine role labeling
learning croatia
tomuro n

interrogative reformulation patterns and acquisition of question phrases
in proceedings of the second international workshop on paraphrasing ume paraphrase pp

toutanova k
haghighi a
manning c

joint learning improves semantic role labeling
in proceedings of the annual meeting of the association for tational linguistics pp
ann arbor michigan
association for computational linguistics
toutanova k
haghighi a
manning c
d

a global joint model for semantic role labeling
comput
linguist

van halteren h
teufel s

examining the consensus between human summaries initial experiments with factoid analysis
in proceedings of the hlt naacl on text summarization workshop pp
morristown nj usa
van halteren h
teufel s

evaluating information content by factoid sis human annotation and stability
in proceedings of the conference on empirical methods in natural language processing barcelona
voorhees e
m

variations in relevance judgments and the measurement of retrieval eectiveness
in proceedings of the annual international acm sigir conference on research and development in information retrieval pp

voorhees e
m

overview of the trec question answering track
in text retrieval conference
voorhees e
m

overview of the trec question answering track
in ings of the twelfth text retrieval conference
voorhees e
m

using question series to evaluate question answering system tiveness
in in hlt emnlp
wang w
j
a
parasuraman r
zubarev i
brandyberry d
harper m

a question answering system developed as a project in a natural language cessing course
in in anlp naacl workshop on reading comprehension tests as evaluation for computerbased language understanding systems
watts d
j
strogatz s

collective dynamics of small world networks
nature
generating extractive summaries of scientific paradigms whidby m
a

citation handling processing citation texts in scientic documents
master s thesis university of maryland department of computer science college park md
xue n
palmer m

calibrating features for semantic role labeling
in lin d
wu d
eds
proceedings of emnlp pp
barcelona spain
association for computational linguistics
zajic d
m
dorr b
j
lin j
schwartz r

multi candidate reduction sentence compression as a tool for document summarization tasks
information processing and management special issue on summarization
zhao y
karypis g

criterion functions for document clustering experiments and analysis
technical report tr department of computer science versity of minnesota minneapolis mn
zhu x
goldberg a
van gael j
andrzejewski d

improving diversity in ranking using absorbing random walks
in proceedings of the north american chapter of the association for computational linguistics human language technologies hlt naacl pp


