pragmatically informative text generation sheng shen daniel fried jacob andreas dan klein computer science division uc berkeley computer science and articial intelligence laboratory mit sheng
dfried
edu
edu r a l c
s c v
v i x r a abstract we improve the informativeness of models for conditional text generation using techniques from computational pragmatics
these niques formulate language production as a game between speakers and listeners in which a speaker should generate output text that a tener can use to correctly identify the original input that the text describes
while such proaches are widely used in cognitive science and grounded language learning they have ceived less attention for more standard guage generation tasks
we consider two matic modeling methods for text generation one where pragmatics is imposed by tion preservation and another where ics is imposed by explicit modeling of tors
we nd that these methods improve the performance of strong existing systems for stractive summarization and generation from structured meaning representations
introduction computational approaches to pragmatics cast guage generation and interpretation as theoretic or bayesian inference procedures land et al
frank and goodman
while such approaches are capable of modeling a variety of pragmatic phenomena their main plication in natural language processing has been to improve the informativeness of generated text in grounded language learning problems monroe et al

in this paper we show that matic reasoning can be similarly used to improve performance in more traditional language tion tasks like generation from structured meaning representations figure and summarization
our work builds on a line of learned rational speech acts rsa models monroe and potts andreas and klein in which erated strings are selected to optimize the input meaning representation i shop of out human written a cheap coffee shop in riverside with a out of customer rating is fitzbillies
fitzbillies is family friendly and serves english food
base sequence to sequence model fitzbillies is a family friendly coffee shop located near the river
distractor based pragmatic system sd it is fitzbillies is a family friendly coffee shop that serves english food
located in riverside area
it has a customer rating of out of and is cheap
reconstructor based pragmatic system sr fitzbillies is a family friendly coffee shop that serves cheap english food in the riverside area
it has a customer rating of out of
figure example outputs of our systems on the generation task
while a base sequence to sequence model sec
fails to describe all attributes in the input meaning representation both of our matic systems sr sec

and sd sec

and the human written reference do
ior of an embedded listener model
the cal presentation of the rsa framework frank and goodman is grounded in reference tion models of speakers attempt to describe erents in the presence of distractors and models of listeners attempt to resolve descriptors to erents
recent work has extended these models to more complex groundings including images mao et al
and trajectories fried et al

the techniques used in these settings are similar and the primary intuition of the rsa framework is preserved from the speaker s perspective a good description is one that picks out as tively as possible the content the speaker intends for the listener to identify
outside of grounding cognitive modeling frank et al
and targeted analysis of guistic phenomena orita et al
rational speech acts models have seen limited application in the natural language processing literature
in this work we show that they can be extended to a distinct class of language generation lems that use as referents structured descriptions of lingustic content or other natural language texts
in accordance with the maxim of tity grice or the q principle horn pragmatic approaches naturally correct formativeness problems observed in state of art language generation systems in figure
we present experiments on two language ation tasks generation from meaning tions novikova et al
and summarization
for each task we evaluate two models of ics the reconstructor based model of fried et al
and the distractor based model of gordon et al

both models improve formance on both tasks increasing rouge scores by

points on the cnn daily mail stractive summarization dataset and bleu scores by points on the end to end generation dataset obtaining new state of the art results
tasks we formulate a conditional generation task as ing an input i from a space of possible inputs i e

input sentences for abstractive tion meaning representations for structured eration and producing an output o as a sequence of tokens


ot
we build our pragmatic approaches on top of learned base speaker els which produce a probability distribution i over output text for a given input
we cus on two conditional generation tasks where the information in the input context should largely be preserved in the output text and apply the matic procedures outlined in sec
to each task
for these models we use systems from past work that are strong but may still be mative relative to human reference outputs e

figure
meaning representations our rst task is eration from structured meaning representations mrs containing attribute value pairs novikova et al

an example is shown in figure where systems must generate a description of the restaurant with the specied attributes
we ply pragmatics to encourage output strings from which the input mr can be identied
for our model we use a publicly released neural ation system puzikov and gurevych that achieves comparable performance to the best lished results in dusek et al

abstractive summarization our second task is multi sentence document summarization
there is a vast amount of past work on tion nenkova and mckeown recent ral models have used large datasets e

hermann et al
to train models in both the extractive cheng and lapata nallapati et al
and abstractive rush et al
see et al
settings
among these works we build on the cent abstractive neural summarization system of chen and bansal
first this system uses a sentence level extractive model rnn ext to tify a sequence of salient sentences


in each source document
second the system uses an abstractive model abs to rewrite each into an output which are then concatenated to duce the nal summary
we rely on the xed ext model to extract sentences as inputs in our pragmatic procedure using abs as our model and applying pragmatics to the stractive step
pragmatic models to produce informative outputs we consider matic methods that extend the base speaker els using listener models l which produce a distribution o over possible inputs given an output
listener models are used to derive matic speakers i which produce output that has a high probability of making a listener model l identify the correct input
there are a large space of possible choices for designing l and deriving we follow two lines of past work which we categorize as reconstructor based and distractor based
we tailor each of these matic methods to both our two tasks by ing reconstructor models and methods of choosing distractors

reconstructor based pragmatics pragmatic approaches in this category dusek and jurccek fried et al
rely on a structor listener model lr dened independently of the speaker
this listener model produces a tribution o over all possible input contexts i i given an output description
we use sequence to sequence or structured classication models for lr described below and train these models on the same data used to supervise the models
the listener model and the base speaker model together dene a pragmatic speaker with output score given by o i sr where is a rationality parameter that controls how much the model optimizes for discriminative outputs see monroe et al
and fried et al
for a discussion
we select an output text sequence o for a given input i by choosing the highest scoring output under eq
from a set of candidates obtained by beam search in i
meaning representations we construct lr for the meaning representation generation task as a multi task multi class classier dening a tribution over possible values for each attribute
each mr attribute has its own prediction layer and attention based aggregation layer which ditions on a basic encoding of o shared across all attributes
see appendix a
for architecture tails
we then dene o as the joint ability of predicting all input mr attributes in i from o
summarization to construct lr for rization we train an abs model of the type we use for chen and bansal but in reverse i
e
taking as input a sentence in the summary and producing a sentence in the source document
we train lr on the same heuristically extracted and aligned source document sentences used to train chen and bansal

distractor based pragmatics pragmatic approaches in this category frank and goodman andreas and klein tam et al
cohn gordon et al
derive pragmatic behavior by producing outputs that tinguish the input i from an alternate distractor put or inputs
we construct a distractor for a given input i in a task dependent way
we follow the approach of cohn gordon et al
outlined briey here
the base speakers we build on produce outputs incrementally where the probability of ot the word output at time t is conditioned on the input and the previously erated words i t
since the output is generated incrementally and there is no separate tasks such as contrastive captioning or referring pression generation these distractors are given for the tional generation task we will show that pragmatic behavior can be obtained by constructing or selecting a single tor that contrasts with the input i
listener model that needs to condition on entire output decisions the distractor based approach is able to make pragmatic decisions at each word rather than choosing between entire output dates as in the reconstructor approaches
the listener ld and pragmatic speaker sd are derived from the base speaker and a belief tribution pt maintained at each timestep t over the possible inputs i d t t i ot i t o i t sd i t t where is again a rationality parameter and the initial belief distribution is uniform i
e


eqs
and are normalized over the true input i and distractor eq
is malized over the output vocabulary
we construct an output text sequence for the pragmatic speaker sd incrementally using beam search to mately maximize eq

meaning representations a distractor mr is automatically constructed for each input to be the most distinctive possible against the input
we construct this distractor by masking each present input attribute and replacing the value of each present attribute with the value that is most quent for that attribute in the training data
for ample for the input mr in figure the distractor is king
summarization for each extracted input tence we use the previous extracted sentence from the same document as the distractor put for the rst sentence we do not use a tor
this is intended to encourage outputs to contain distinctive information against other maries produced within the same document
experiments for each of our two conditional generation tasks we evaluate on a standard benchmark dataset following past work by using automatic tion against human produced reference text
we choose hyperparameters for our models beam size and parameters and to maximize task metrics on each dataset s development set see pendix a
for the settings used
code is publicly available at
com sincerass
system bleu nist meteor r l cider system r l meteor t gen best prev




sr sd

























table test results for the generation task in comparison to the t gen baseline dusek and jurccek and the best results from the challenge ported by dusek et al
juraska et al
and gurevych et al
and gong
we bold our highest performing model on each metric as well as previous work if it outperforms all of our models

meaning representations we evaluate on the task of generation from meaning representations containing restaurant tributes novikova et al

we report the task s ve automatic metrics bleu papineni et al
nist doddington meteor lavie and agarwal rouge l lin and cider vedantam et al

table compares the performance of our base and pragmatic models to the baseline t gen system dusek and jurccek and the best previous result from the primary systems uated in the challenge dusek et al

the systems obtaining these results encompass a range of approaches a template system puzikov and gurevych a neural model zhang et al
models trained with reinforcement learning gong and systems using ensembling and reranking juraska et al

to ensure that the benet of the reconstructor based pragmatic proach which uses two models is not due solely to a model combination effect we also compare to an ensemble of two base models
this ensemble uses a weighted combination of scores of two independently trained models ing eq
with weights tuned on the development data
both of our pragmatic systems improve over the strong baseline system on all ve metrics with the largest improvements
bleu
nist
meteor
rouge l and
cider from the sr model
this sr model outperforms the ous best results obtained by any system in the challenge on bleu nist and cider with rable performance on meteor and rouge l
extractive inputs abstractive sr sd







best previous



















for test results table the non anonymized cnn daily mail summarization task
we compare to extractive baselines and the best previous abstractive results of celikyilmaz et al
paulus et al
and and bansal
we bold our highest performing model on each metric as well as previous work if it outperforms all of our models

abstractive summarization we evaluate on the cnn daily mail tion dataset hermann et al
nallapati et al
using see et al
s non anonymized preprocessing
as in previous work chen and bansal we evaluate using rouge and teor
table compares our pragmatic systems to the base model with scores taken from chen and bansal we obtained comparable mance in our an ensemble of two of these base models and the best previous stractive summarization result for each metric on this dataset celikyilmaz et al
paulus et al
chen and bansal
we also report two extractive baselines which uses the rst three sentences of the document as the summary see et al
and inputs the concatenation of the extracted sentences used as inputs to our els i
e




the pragmatic methods obtain improvements of

in rouge scores and

meteor over the base model with the distractor based approach sd outperforming the based approach sr is strong across all rics obtaining results competitive to the best vious abstractive systems

sd use retrained versions of chen and bansal s sentence extractor and abstractive models in all our ments as well as their n gram reranking based inference cedure replacing scores from the base model with scores from sr in the respective pragmatic procedures
or sd coverage ratio for attribute area et food pr ff cr




sd sd sd sd sd sd
r t t a r o t c a r t s i d




































a coverage ratios by attribute type for the base model and pragmatic models sr
the pragmatic models typically improve coverage ratios across attribute types when compared to the base model
and sd coverage ratios by attribute type columns for the base model and for the pragmatic system sd when constructing the distractor by masking the specied attribute rows
cell colors are the degree the coverage ratio increases green or decreases red relative to
figure coverage ratios for the task by attribute type estimating how frequently the values for each attribute from the input meaning representations are mentioned in the output text
analysis the base speaker model is often mative e

for the task failing to mention certain attributes of a mr even though almost all the training examples incorporate all of them
to better understand the performance improvements from the pragmatic models for we compute a coverage ratio as a proxy measure of how well content in the input is preserved in the generated outputs
the coverage ratio for each attribute is the fraction of times there is an exact match between the text in the generated output and the attribute s value in the source mr for instances where the attribute is specied
figure shows coverage ratio by attribute category for all models
the sr model increases the coverage ratio when compared to across all attributes showing that using the reconstruction model score to select outputs does lead to an crease in mentions for each attribute
coverage ratios increase for sd as well in four out of six categories but the increase is typically less than that produced by sr
while sd optimizes less explicitly for attribute mentions than sr it still provides a potential method to control generated outputs by choosing alternate distractors
figure shows age ratios for sd when masking only a single tribute in the distractor
the highest coverage ratio for each attribute is usually obtained when ing that attribute in the distractor mr entries on the main diagonal underlined in particular for familyfriendly ff food pricerange that this measure roughly provides a lower bound on the model s actual informativeness for each attribute since the measure does not assign credit for paraphrases
pr and area
however masking a single tribute sometimes results in decreasing the erage ratio and we also observe substantial creases from masking other attributes

ing either familyfriendly or ing cr produces an equal increase in coverage ratio for the customerrating attribute
this may reect underlying correlations in the training data as these two attributes have a small number of possible values and respectively
conclusion our results show that models from previous work while strong still imperfectly capture the behavior that people exhibit when generating text and an explicit pragmatic modeling procedure can improve results
both pragmatic methods uated in this paper encourage prediction of puts that can be used to identify their inputs either by reconstructing inputs in their entirety or guishing true inputs from distractors so it is haps unsurprising that both methods produce ilar improvements in performance
future work might allow ner grained modeling of the tradeoff between and over informativity within the sequence generation pipeline e

with a learned communication cost model or explore tions of pragmatics for content selection earlier in the generation pipeline
acknowledgments thanks to reuben cohn gordon for many helpful discussions and suggestions
this work was ported by darpa through the xai program
df is supported by a tencent ai lab fellowship
ffetfoodprareacrmeaning representation






references jacob andreas and dan klein

reasoning about pragmatics with neural listeners and speakers
in proceedings of the conference on empirical ods in natural language processing
dzmitry bahdanau kyunghyun cho and yoshua gio

neural machine translation by jointly arxiv preprint learning to align and translate


asli celikyilmaz antoine bosselut xiaodong he and yejin choi

deep communicating agents in proceedings of for abstractive summarization
the annual meeting of the north american ter of the association for computational linguistics pages new orleans louisiana
ation for computational linguistics
yen chun chen and mohit bansal

fast tive summarization with reinforce selected sentence rewriting
in proceedings of the annual meeting of the association for computational linguistics
jianpeng cheng and mirella lapata

neural summarization by extracting sentences and words
in proceedings of the annual meeting of the ciation for computational linguistics pages berlin germany
association for tional linguistics
kyunghyun cho bart van merrienboer caglar cehre dzmitry bahdanau fethi bougares holger schwenk and yoshua bengio

learning phrase representations using rnn encoder decoder in proceedings for statistical machine translation
of the conference on empirical methods in ral language processing pages doha qatar
association for computational linguistics
reuben cohn gordon noah goodman and chris potts

pragmatically informative image tioning with character level reference
in ings of the annual meeting of the north american chapter of the association for computational guistics
george doddington

automatic evaluation of machine translation quality using n gram occurrence statistics
in proceedings of the second international conference on human language nology research pages
morgan mann publishers inc
ondrej dusek and filip jurccek

sequence sequence generation for spoken dialogue via deep syntax trees and strings
in acl
ondrej dusek jekaterina novikova and verena rieser

findings of the nlg challenge
in ceedings of the international conference on natural language generation
michael c frank and noah d goodman

dicting pragmatic reasoning in language games
ence
michael c frank noah d goodman peter lai and informative joshua b tenenbaum

nication in word production and word learning
in proceedings of the annual conference of the cognitive science society pages
daniel fried jacob andreas and dan klein

unied pragmatic models for generating and ing instructions
in proceedings of the annual ing of the north american chapter of the tion for computational linguistics
dave golland percy liang and dan klein

a game theoretic approach to generating spatial scriptions
in proceedings of the conference on empirical methods in natural language ing pages
association for computational linguistics
heng gong

technical report for nlg lenge
in nlg challenge system descriptions
herbert p grice

logic and conversation
karl moritz hermann tomas kocisky edward grefenstette lasse espeholt will kay mustafa leyman and phil blunsom

teaching chines to read and comprehend
in advances in ral information processing systems
laurence horn

toward a new taxonomy for pragmatic inference q based and based ture
meaning form and use in context linguistic applications
juraj juraska panagiotis karagiannis kevin bowden and marilyn walker

a deep ensemble model with slot alignment for sequence to sequence ral language generation
in proceedings of the nual meeting of the north american chapter of the association for computational linguistics pages new orleans louisiana
association for computational linguistics
alon lavie and abhaya agarwal

meteor an automatic metric for mt evaluation with high levels of correlation with human judgments
in ings of the second workshop on statistical machine translation pages prague czech lic
association for computational linguistics
chin yew lin

rouge a package for automatic in text summarization evaluation of summaries
branches out proceedings of the shop
junhua mao jonathan huang alexander toshev oana camburu alan yuille and kevin murphy

generation and comprehension of unambiguous ject descriptions
arxiv preprint

will monroe robert xd hawkins noah d goodman and christopher potts

colors in context a pragmatic neural model for grounded language derstanding
transactions of the association for computational linguistics
abigail see peter j
liu and christopher d
manning

get to the point summarization with in proceedings of the annual generator networks
meeting of the association for computational guistics pages vancouver canada
sociation for computational linguistics
ramakrishna vedantam samy bengio kevin murphy devi parikh and gal chechik

context aware captions from context agnostic supervision
ramakrishna vedantam c
lawrence zitnick and devi parikh

cider consensus based in proceedings of the age description evaluation
conference on computer vision and pattern nition
biao zhang jing yang qian lin and jinsong su

attention regularized sequence to sequence learning for nlg challenge
in nlg lenge system descriptions
will monroe jennifer hu andrew jong and pher potts

generating bilingual pragmatic in proceedings of the annual color references
meeting of the north american chapter of the sociation for computational linguistics
will monroe and christopher potts

learning in the rational speech acts model
in proceedings of amsterdam colloquium amsterdam
illc
ramesh nallapati feifei zhai and bowen zhou

summarunner a recurrent neural network based quence model for extractive summarization of ments
in proceedings of the meeting of the ation for the advancement of articial intelligence
ramesh nallapati bowen zhou cicero dos santos caglar gulcehre and bing xiang

tive text summarization using sequence to sequence rnns and beyond
in signll conference on putational natural language learning pages berlin germany
association for tional linguistics
ani nenkova and kathleen mckeown

matic summarization
foundations and trends in information retrieval
jekaterina novikova ondrej dusek and verena rieser

the dataset new challenges for end end generation
in proceedings of the annual dial meeting on discourse and dialogue
naho orita eliana vornov naomi feldman and hal daume iii

why discourse affects ers choice of referring expressions
in proceedings of the annual meeting of the association for computational linguistics and the international joint conference on natural language processing volume long papers volume pages
kishore papineni salim roukos todd ward and jing zhu

bleu a method for automatic in proceedings of uation of machine translation
the annual meeting of the association for tational linguistics pages
association for computational linguistics
romain paulus caiming xiong and richard socher

a deep reinforced model for abstractive in proceedings of the international marization
conference on learning representations volume

yevgeniy puzikov and iryna gurevych

nlg challenge neural models vs
templates
in proceedings of the international conference on ural language generation
alexander m
rush sumit chopra and jason weston

a neural attention model for abstractive in proceedings of the tence summarization
conference on empirical methods in natural guage processing pages lisbon portugal
association for computational linguistics
a supplemental material a
reconstructor model details for the reconstructor based speaker in the task we rst follow the same data preprocessing steps as puzikov and gurevych which cludes a delexicalization module that deals with sparsely occurring mr attributes name near by mapping such values to placeholder tokens
mrs have only a few possible values for most attributes six out of eight attributes have fewer than seven unique values and the remaining two attributes name near are handled by our and sd using delexicalized placeholders ing puzikov and gurevych
in this way the reconstructor only needs to predict the presence of these two attributes with a boolean variable and other attributes with the corresponding categorical variable
we use a one layer bi directional gru cho et al
for the shared sentence encoder
we concatenate the latent vectors from both tions to construct a bi directional encoded vector hi for every single word vector as hi gru hi gru hi hi hi i l since not all words contribute equally to ing each mr attribute we thus use an attention mechanism bahdanau et al
to determine the importance of every single word
the gated sentence vector for task k is calculated by i a hi k a hj i hi l the task specic sentence representation is then used as input to k layers with softmax outputs turning a probability vector y k for each of the k mr attributes
a
hyperparameters for structured generation we use beam size
and
tuned to maximize the normalized average of all ve metrics on the velopment set
for abstractive summarization we use beam size
and
tuned to maximize rouge l on the development set

