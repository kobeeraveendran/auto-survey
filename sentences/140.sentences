computer assisted text analysis for social science topic models and beyond ryan wesslen college of computing and informatics university of north carolina at charlotte charlotte north carolina usa email
edu r a l c
s c v
v i x r a abstract topic models are a family of statistical based gorithms to summarize explore and index large collections of text documents
after a decade of research led by computer scientists topic models have spread to social science as a new generation of data driven social scientists have searched for tools to explore large collections of unstructured text
recently social scientists have contributed to topic model literature with developments in causal inference and tools for handling the problem of multi modality
in this paper i provide a literature review on the evolution of topic modeling including extensions for document covariates methods for evaluation and interpretation and advances in interactive visualizations along with each aspect s relevance and application for social science research
keywords computational social science computer assisted text analysis visual analytics structural topic model i
introduction topic models are a framework of statistical based rithms used to identify and measure latent hidden topics within a corpus of text documents
despite their wide use in computer science research topic models have remained largely absent from the average social scientist s analytical toolkit
historically most social science text analysis has instead focused on either human coding or dictionary based methods that are semi automated but require high pre analysis costs before implementation
moreover this problem is magnied when considering the tremendous increase in the volume and variety of unstructured text documents for social scientists to study
to overcome this problem social scientists have started to adopt computer assisted text analysis techniques like supervised learning and topic models for their research with the goal of amplifying and augmenting social science analysis not replacing it
unlike computer scientists who typically use machine learning techniques for prediction social scientists have found use in machine learning for the analysis of latent variables that could previously only be measured under untestable and sequential assumptions
ultimately the rise of assisted text analysis tools in social science research is one of the major drivers of the emerging eld of computational social science
the purpose of this paper is to survey the literature of one such computer assisted text technique topic models and provide background on its importance to social science research
this overview leads itself to the newly created structural topic model stm that extends the general topic model framework to estimate causal effects within text documents
in section i review the evolution of topic models by introducing latent dirichlet allocation lda and related seminal models
i also consider computational methods for topic models and discuss the extensions to the lda based framework to include document covariates within the model
in section i explore tools for the application of topic models including methods for evaluation measures of interpretation and visualization interfaces
in section i discuss two major contributions by social scientists to the topic model literature structural topic model stm and techniques to handle modality
i provide examples of social science applications that use these techniques for texts like open ended survey responses political rhetoric and social media
last i consider ongoing limitations in the methodology along with future opportunities for topic models within social science research in relation to computational social science and explainable articial intelligence xai
ii
evolution of topic models topic models identify and measure latent topics within a corpus of text documents
topic models are called generative models because they assume that observable data is generated by joint probability of variables that are interpreted to be topics
lda is the workhorse topic model and is a bayesian two tiered mixture model that identies word co occurrence patterns which are interpreted as topics
in this section i summarize key properties of this work by reviewing the evolution of topic models starting with its seminal models
next i review model extensions along with advances in computational methods in the model framework
a
lda and seminal papers as a generalization there are two approaches to assisted text analysis natural language processing nlp and statistical based algorithms like topic models
unlike nlp methods that tags parts of speech and grammatical structure statistical based models like topic models are largely based on the bag of words bow assumption
in bow models collection of text documents are quantied into a term matrix dtm that counts the occurrence of each word columns for each document rows
in the case of most topic models like lda the dtm is one of two model inputs along with the number of topics
the bow approach provides two key advantages plicity statistical properties at the expense of ignoring word order
for example the bow approach reduces the information contained in a collection of text documents into the word and document counts
an implication of word counts is that it ignores word order which is opposite of nlp methods that parse language structure
without accounting for word order bow methods perform poorly on micro level problems like question and answer and others that require exact semantic meaning
however for large collections of documents sample size the bow assumption provides the theoretical foundation for a richer set of statistical methods mixture models by the assumption of exchangeability
ultimately this vantage underpins statistical based methods success in macro document level summarization problems for a large enough collection of inter related documents
an early motivation for topic models was the goal of dimensionality reduction of the document term matrix for large collections of documents or corpora
for example deerwester et al
presented one of the rst predecessor models latent semantic indexing or lsi by applying singular value decomposition svd a linear algebra dimensionality tion technique to reduce the document term matrix to latent factors
their goal was to identify broad semantic correlation structure within the documents by removing noise from formative factors
later landauser and dumais extended the lsi model to create the latent semantic analysis lsa model
further these methods could be improved by substituting the term frequency inverse document cies tf idf weightings in place of the raw term counts
however hofmann identied two major drawbacks to lsa
first the approach lacked theoretical foundation as the method svd relied on a gaussian noise assumption that could not be justied for word counts document term matrix
second lsa could not account for polysemy the multiple uses of words in different contexts
to address these problems hoffman introduced probabilistic latent semantic index plsi model through the addition of a probabilistic mixture component to the lsa model by assuming each word is generated by a word probability distribution interpreted to be a topic
these seminal models set the stage for blei et al
to extend these predecessor models to build the workhorse model lda
the key contribution of lda was to tend hofmann s plsi model to include a second probability mixture component for the document level thus assuming documents are a mixture of topics
this addition yielded a two tiered model a core component of the typical topic model framework in which observed words are assumed to be generated by the joint probability of two mixtures
at the top documents are a mixture of topics
at the bottom topics are also a mixture of words
each topic is dened as a unique distribution of words and yields the algorithms second output the word topic matrix
the word topic matrix provides a conditional probability for every word row given each hidden topic column
using these probability distributions a researcher can rank order any word by each topic to determine is the most common word the use when what ding et al
show a different but related methodology non negative matrix factorization nmf is theoretically identical to plsi as both approaches are maximizing the same objective function
the only difference between the two methods is that each approach differs in its inference method
referring to each topic
similar to the idea of singular value decomposition svd used in earlier topic models like latent semantic analysis lsa the probabilistic mixture nature of lda acts similarly as dimensionality reduction process by reducing the information about each document from the large number of columns words to a much smaller number of columns topics
one consequence of the introduction of the mixture ponents within the lda based framework was the problem of intractability
in fact like many other bayesian methods the introduction of the mixture components allowed in theory the measurement of the latent variable of topics but at the expense of the ability to measure precisely the optimal model because the exponentially large potential solutions of topic values
this problem lead to the question what is the best approach to compute topic models b
computational methods following the introduction of lda a major theme in topic modeling literature was on computational methods for topic models given the problem of intractability of computing the evidence or the marginal probability of observations
without an analytical solution the goal of computational methods for topic model inference is to nd the most tational efcient method that also best approximates the terior
in general there are two common approaches for topic model inference sampling based methods e

mcmc gibbs sampling and variational inference
sampling based rithms simulate samples of the posterior to approximate the true posterior
gibbs sampling the most common sampling method was introduced for topic model inference by grifths and steyvers and uses a markov chain to estimate a sequence of dependent random variables that asymptotically serves as the posterior distribution
ultimately based methods like gibbs sampling have the advantages that they are theoretically backed unbiased and tationally convenient to implement
the downside of this approach is that it can be very slow for large inputs number of documents words or topics
the goal as an alternative to approximating the posterior through sampling variational inference methods transform the problem is into an optimization problem
in this context to nd families of distributions over the hidden variables topics that most closely estimates the actual posterior
in other words variational inference attempts to most tightly estimate the posterior with a simpler distribution that includes free variational parameters used as the optimization arguments
blei et al
introduce variational methods for topic model inference by using an expectation maximization em algorithm
figure provides the two graphical models the the left simplied model used in the variational inference algorithm
to use variational inference for lda the model is simplied by removing the edges between w and z variables and introducing the free variational parameters and
the optimization problem uses kullback leibler kl divergence to best minimize the estimated posterior to the true posterior
the theoretical model for lda and the right will be discussed in section normally the highest probability words conditioned on each topic serve to aid the researcher in the interpretation of each topic
left graphical model for latent dirichlet allocation lda and fig

right a graphical model of the variational distribution used for variational inference for lda from blei et al
fig

mccallum example of an down stream model extension from mimno and the em algorithm is used as a two step process in which the variational distribution is estimated with estimated parameters and then the new variational parameters are chosen through the variational inference optimization problem
the process is repeated until a convergence threshold is met
hoffman et al
extended the variational inference to introduce a faster online batch algorithm that can be used to massively scale lda for very large corpora or streaming data
ultimately given that sampling based or variational ence methods are estimates and never ever exact solutions neither method is perfect and the decision of each depends on the trades off of speed complexity accuracy and simplicity required for the problem at hand
in the next section i ll introduce how the basic lda framework has been generalized to include document metadata variables within the algorithm which thus require modications to the computational methods as well
later in section i ll discuss how such metadata able extensions has an impact on the computational methods possible when introducing the structural topic model stm
c
model extensions for document metadata a second theme in topic modeling literature deals with the inclusion of metadata variables into the model
one of the rst examples of such a model is the author topic model proposed by rosen zvi et al

the original motivation was the explicit point that who the author is will have a direct impact on what topics are discussed in a publication e

a biologist will more likely write about topics in biology than sociology or politics
in contrast lda does not take into account any document variables like author and thus fails to incorporate author into the model
using only lda the only way to analyze the impact of author post hoc was by comparing how the model outputs topics compare relative to author
a noted downside of this approach is that the model would likely be less effective as its omitting a known variable that affects the topic proportions
to address this problem rosen zvi et al
introduced the author topic model to incorporate the author attribute by modifying lda s assumption that author not documents are a multinomial distribution over the topics
soon after many metadata topic model extensions were created for a variety of metadata attributes like time dynamic topic model geography geographical topic model and emotion emotion topic model
given the large collection of metadata topic model tensions mimno and mccallum categorized metadata extension models into two groups down stream and up stream models
the key difference between each approach is on the role the metadata variables take in the process to generate the text
for instance in the down stream approach the metadata fig

example of an up stream model extension from mimno and mccallum like the text itself is assumed to be generated by the hidden topics
in this approach topics are word distributions as well as distributions over the metadata variables
figure provides an example plate notation for a down stream model
in this gure the metadata variables m and the words in the text w are conditionally generated by the hidden topics
the most common example of this approach is the supervised latent dirichlet allocation slda model
whereas in the up stream approach the algorithm is tioned on the metadata covariates such that the document topic distributions are mixtures of the covariate specic distribution
the classic example of this approach is the topic model or dynamic topic model
essentially the up stream models learn an assignment of the words in each document to one of a set of entities
in addition to incorporating metadata into the model other information like topic model extensions include additional word context graphical network relationships and cal topic structure
for example the hidden markov model hmm extended the normal bow assumption to facilitate a consideration of word context into the topic model framework
in the case of network data the link topic model and the relational topic model rtm are two additional topic models that incorporate relational based information into the model for further analysis
finally teh et al
introduce a generalized hierarchical structure to the topics that alleviates the problem of the number of topics and allows analysis from different topic levels
nevertheless one of the main advantages of topic models is its exibility to incorporate multiple different computational methods and alternative specications
in the next section i ll move on to more practical considerations of how to use and apply topic models before introducing social science applications in section
iii
tools for model application interpretation model selection and visualization in this section i review research that has focused on pre processing interpretation evaluation and analysis of topic models
while not necessarily changing the underlying model framework like discussed in section this research is critical for the implementation of topic models and their application for social science research
a
pre processing a critical step to analyze text is the process of quantifying text
this process called pre processing is a series of decisions a researcher makes to clean and normalize text with the goal of removing potential noise to maximize her analysis on underlying signal
while many researchers overlook these steps relying instead of default rules without questioning their merits recent research has found that text analysis methods like topic modeling are susceptible to potential forking paths that leave results subject to initial coding decisions
moreover there is no set rules for what pre processing steps are necessary and the need is dened by the quality quantity and style of the underlying text
b
model selection prediction interpretability trade off the question of how to select and validate topic model specications e

number of topics model framework
depends on the researcher s objective
in predictive modeling a researcher s goal is to build a model that can best dict out of sample or future documents using log likelihood perplexity measures
whereas for researchers whose goal is exploration and knowledge discovery human judgment e

interpretable topics may take precedence over holdout prediction
initially in the topic model literature prediction accuracy was the key goal of model evaluation
wallach et al
outlined different evaluation methods on lda using gibbs sampling
they consider two approaches to evaluating topic models maximizing the held out documents likelihood plexity and document completion in which long documents are trained on part of the document and evaluated on the model s ability to correctly complete the document
they nd that methods like harmonic mean importance sampling and document completion methods are inaccurate and may tort the relative advantage of one model versus another model
instead they recommend either the chib style estimator or the left to right algorithm as more accurate evaluation methods
however chang el al
explored the trade off tween prediction and interpretability
through the word sion tasks they found the counter intuitive result that highly predictive topics tend to be negatively correlated with pretability
semantic coherence was introduced by mimno et al
as a measure for how internally consistent words are within topics
on the other hand exclusivity is a measure to identify words that have high probabilities for only a few topics rather than many topics
roberts et al
argue that a topic that is both cohesive and exclusive is more likely to be semantically useful
finally other researchers have studied the effectiveness of topic modeling while controlling for other key factors e

limitations to wallach et al
is that these results were only tested using gibbs sampling based inference and on vanilla lda
hyperparameters number of topics document sample size document length
wallach et al
explored the effect of relaxing lda s prior distribution assumptions including using non symmetric dirichlet parameters for the topic or word topic matrices
they nd that asymmetric priors on the document topic distribution can provide substantial advantages while asymmetric priors do not provide much benet to the word topic distribution
taddy explored estimation methods for choosing the optimal number of topics via block diagonal approximations to the information matrix and goodness analysis for likelihood based model tion
in a more systematic review of topic model performance tang et al
analyzed lda performance controlling for four limiting factors document length number of documents and the two prior distribution hyperparameters
considering two simulated and three real datasets wikipedia new york times and twitter they make ve recommendations
first they argue that a sufcient number of documents is the most important factor to ensure accurate inference
for example they nd lda is more difcult when running on a small sample e

less than a thousand documents
second they nd the length of the document matters as well
this matters in social media data like twitter messages in which all messages are less than characters
as an alternative they cite natives like aggregating messages to transform the documents to a user level to expand the size of documents
third they nd that collections with too many topics lend statistical inference methods to be inefcient
fourth they nd that lda performance is affected by how well separated the underlying topics are relative to a euclidean measure
last they nd that the variability of hyperparameters is important depending on the number of topics within documents
for example they recommend using a lower alpha dirichlet parameter when documents have few topics whereas using a high alpha for documents with many topics
c
visualizations in this section i review semi automation methods of alyzing topic models through visualizations
hu et al
argue that topic models suffer from an interpretation problem that requires the need for interactive system for end users
topic results are never perfect and often include bad topics that do not perfectly align with an end user s judgment and intuition similar to the argument by chang et al

to address this problem they argue for systems that allow the end user to annotate the model results and incorporate feedback into the model s output
moreover they identify social science in addition to digital humanities and information studies as a discipline that would greatly benet from interactive systems when implementing topic models
they emphasize social science as such a eld because of its take it or leave it problem because many social scientists have extensive domain knowledge but lack the machine learning expertise to modify topic model algorithms
a shortcoming of their argument is they omit the role that visualizations can play within such interactive systems
further they fail to recognize the body of research by the visualization community that has extended and proposed many different applications for visualizations to t into such interactive systems
dou and liu argue that visual interfaces allow decision makers to explore and analyze the model results
this point is more important when considering the application of topic models for non computer scientists like most social entists who may not have the programming or computational training to run the algorithms on their own
a major motivation for the use of visualizations for analyzing topic models is that the output is too large for a researcher to absorb manually
for example lda s output is two large datasets the word topic and the document topic matrices the size of both are proportional to the number of documents terms and topics
therefore the larger the corpus the larger the output and the more difcult it is for a researcher to analyze the results
in general there are two common approaches to alizing topic models topic oriented and time oriented
each approach differs based on which is the most important element of interest
in topic oriented visualizations the focus is on the relationship between either the words and topics word topic matrix or the document and topics topic matrix
such approaches focus on the task of document summarization information retrieval and relationships between documents
common examples of these approaches include matrix representations like termite and serendip see figure as well as parallel coordinates visualizations as in paralleltopics
chuang et al
provide a general design framework for topic oriented interactive visual systems based on how an analyst makes inference on the topics interpretation and the actual and perceived accuracy of the analyst s inference trust
other interfaces like hierarchicaltopics have generalized the model and facilitated interfaces that focus on a hierarchical structure within the ics that can aid in drill down on multiple levels for document summarization
moreover new research has used network graphs to represent the correlations between topics especially when coupled with models with a more exible correlation structure like ctm
fig

topic oriented visualizations including termite model from chuang et al
and right the serendip model from alexander et al
on the other hand documents that are time oriented e

twitter messages or news articles can be aided with dependent visualizations that can aid in exploring the trend evolution lead lag effect and event detection relative to the topics
tiara is an interface created to visualize topical trends by using an enhanced stacked graph
similarly textflow was introduced with the goal of exploring the evolution of topics including identifying how topics merge and split over time
further textpioneer is a visual interface that introduces the problem of lead lag relationships in exploring topic results
lead lag is the problem in which a researcher needs to understand the relationship tween two corpora especially in the case when one corpus e

social media may lead the information that may then appear in another corpus e

news articles that recount information spread through social media
last another oriented component that has been explored in visual interfaces is event detection and analysis
for example leadline is a visual analysis system used to identify and explore events by detecting the most common words topics used in short discrete bursts
last another major consideration in the use of visual terfaces for topic models includes the type of data used within the model
the simplest approach is when homogeneous data is used and thus the focus is on a single corpus and the topics that stem from the corpus
however much deeper insight can be found with the inclusion of heterogeneous data sources that append document metadata to the corpus
one example of a recent approach to combine such sources include the topicpanorama interface that combines text from multiple data sources e

news articles and twitter messages and provides a network graph to link across these sources see figure
further another avenue of topic analysis includes the impact of analyzing topics within streaming data sources like twitter or other social media platforms
fig

the integration of multiple corpora within topicpanorama from liu et al
d
lda based topic model applications in social sciences one of the rst applications of topic models within social science literature comes from quinn et al

this paper laid the foundation for social science mainly political science application of topic models in three ways
first the paper directly compares topic model relative to other text analysis methods reading human coding dictionary based and pervised learning comparing and contrasting the costs and benets of each method
second quinn et al
set out a list of ve criterion based concepts goals based on more traditional social science content analysis to be used for model evaluation that they argue should be considered when applying topic models
last the authors modied lda for their analysis by using a continuous time model that limited the number mentioned in the introduction of this paper this cost benet comparison justied the benet of computer assisted text analysis tools like topic models relative traditional text analysis research like human coding and based methods
ve criterion are semantic validity convergent construct validity discriminant construct validity predictive validity and hypothesis validity
semantic validity is the coherent meaning from interpreting the word topic probabilities
convergent construct validity is the extent that the results align to existing measures or benchmarks of known truths
discriminant construct validity measures how the results depart from the same existing benchmarks
predictive validity is the ability that the results can correctly predict external events
last hypothesis validity is the extent to which the results can be effectively used to test hypotheses
fig

comparison of lda dynamic multitopic model and expressed agenda model from grimmer and stewart of topics for each speech to only one as opposed to the mixture assumed in lda
the model is named the dynamic multitopic model and is represented in figure
unlike lda the model is a single membership mixture model in which each speech was assigned to a unique topic and each day was assumed to be a mixture of speeches
as shown in figure this can be analogous to how lda assigns each word to a topic then assumes each document is a mixture of those topics
this is important as it was one of the rst social science based topic model modications created to test a theoretically driven hypothesis for social science research
similarly grimmer created a modied topic model to analyze political rhetoric the expressed agenda model by individual senators through their press releases
also shown in figure the expressed agenda model modied the lda framework into a single membership mixture model and assumed each press release was assigned to a single topic while each senator s press releases corpus were assumed to be a mixture of those topics
like the dynamic multitopic model the units of measurement e

topic assignment and level were modied from standard lda to allow the author to test a political theory on how a senator divides his or her attention across the multiple political issues as represented by their explicit press releases
other social scientists have considered using lda to analyze text from social media in particular twitter
one novel example comes from health informatics by paul and dredze
using twitter messages the authors modify lda to create the ailment topic aspect model atam with the goal of identifying health related keywords automatically
the authors discover interpretable topics e

seasonal u allergies exercise and obesity that correlate signicantly with u
s
geographic survey data
figure provides an except of their ndings by comparing the normalized frequency score trends of external survey results on allergies and four approaches to identify allergy related tweets by words allergy and allergies lda and atam
the key methodological contribution of this model was the inclusion of external background words that represent health aspects that the model will identify as topics that describe the aspect
further topics relative to document level metadata regarding the time and geography of the tweet
a limitation of their approach is that while the results correlate with external gallup benchmarks their work does not demonstrate with statistical signicance the relationship between twitter topics and its effect on public awareness via the gallup results
at its core this approach lacks a causal inference mechanism to explain the signicance of the discussions on twitter
the authors explore their output fig

differences in the estimated responsiveness of different constituent follower groups using f tests from barbera et al
one application of lda on twitter that attempts to provide a causal inference mechanism comes from political science by barbera et al

in their paper the authors analyzed the responsiveness to members of the united states congress to constituent conversations on twitter
in order to categorize the topics they used lda on the tweets of both constituents and the members of congress
next they measured the variance between the topics over time to estimate whether members of fig

monthly allergy prevalence for tweets using four methods and gallup survey results from paul and dredze congress lead or follow their constituents on political issues by employing granger causality testing after running lda
the key contribution of this paper was the use of a temporal causal framework granger causality along with lda to provide a higher level statistical signicance for the effect of covariates author and time
to analyze this result they divided the members of congress and their constituents into six groups three groups per political party democrat and publican members of congress hardcore constituents and uninterested constituents
also they categorized the topics from lda into four categories democrat owned owned non political and political topics
then they ran post hoc regressions four topics six groups using ve day lags of topic proportions for each of the six groups as the independent variables
figure provides standard f test results by topic rows columns group and the relationship

they nd that members of congress are responsive to their constituents especially prominent issues by their hardcore constituents
however they nd little evidence of that members of congress have inuence on what topics their constituents discuss publicly
while being a novel contribution to estimating causal effects with lda their approach has two limiting factors
first to analyze author and time they did not directly model
instead employing the aggregation approach suggested by they combined tweets by author and day to modify the denition of a document from a tweet to the collection of all tweets in a day by each author
by doing aggregation this enables them to control for the document covariates of time and author but still employing lda that does not have a mechanism to directly control for these covariates
second they use regression post hoc and not within the generative topic model itself
as noted in the appendix of roberts et al
the problem with this approach is the measurement of uncertainty that can lead to spurious results
a general theme of these applications represent the ity of how lda based framework can be modied to address a unique theoretical question for a specic document level variate e

time author geography
however grimmer and stewart recognized that requiring all social scientists to tune each model to their task is a daunting task
further each provides the motivation for deriving a causal inference component within the lda framework by asking questions involving document covariates that facilitate hypothesis testing
in response to these problems roberts et al
introduced structural topic model stm as a general causal inference framework for hypothesis testing for document covariates
iv
structural topic model multi modality in this section i review literature by social scientists to reconcile two major problems with the standard topic model framework the lack of causal inference and multi modality
f mc column represents f test for measuring the followers impact on the member of congress topic proportions while the mc f column is the opposite i
e
a member of congress impact on his her followers topic proportions et al
call this approach two stage in which in lda is run stage followed by running a regression for topic proportions conditioned on a covariate of interest stage
the rst issue i address is that standard down stream metadata topic models e

author topic dynamic
make point but not standard error estimates to facilitate cal hypothesis testing
the next problem is that computational methods for topic model inference as it is an np hard problem can provide local optima but can not guarantee global optima which is termed multi modality
this problem threatens the stability of a topic model output and can lead researchers to question whether they did not stumble across the result completely by chance
a
stm model the rst major critique of topic models is that its output provides point estimate of word or topic probabilities without condence intervals that facilitate statistical hypothesis testing
this problem is especially important in down stream sions that include metadata that affect the topic proportions
for example researchers could estimate that a topic proportion is different for various covariate levels but without statistical condence provided by standard error estimates
this problem is directly contrary to the tradition of causal inference that employs statistical condence within the social sciences to termine causation for example
in an effort to reconcile this problem roberts et al
and roberts et al
introduced the structural topic model stm by incorporating a generalized linear model glm framework for document metadata by extending elements of three previous topic model extensions ctm dmr and sage
let s rst start with ctm
blei and lafferty troduce the correlated topic model ctm to provide more realism to the original lda model which incorporates a more exible correlation structure than the independence assumption assumed in lda
the model replaces the dirichlet assumption for topic proportions as used in lda with a logistic normal distribution
the main advantage of the ctm versus the lda is improved predictive power a dirichlet based model will predict items based on the latent topics that the observations items associated with suggest but additional topics that are correlated with the conditionally probable topics
the ctm will predict however like many other topic model extensions ation of model assumptions comes at the expense of model complexity and even intractability for existing methods
in this case simulation techniques like gibbs sampling are no longer possible as markov chain monte carlo mcmc process metropolis hastings become untenable given their size and scale
as an alternative blei and lafferty introduce a fast variational inference that approximates posterior butions for ctm
this approach underpins the computational framework for stm and stm can be thought of as identical to ctm when no metadata covariates are included
the other two models dirichlet multinomial model dmr and sparse additive generative model sage provide the framework for introducing document metadata covariates
first the dmr model applies to the introduction of metadata covariates that can affect the topic proportions
the model noted earlier simulation approaches are ideal for topic models because they are theoretically backed unbiased computationally convenient
fig

roberts et al
covariate inference for lda and stm on simulated datasets by replaces lda s assumption of a dirichlet prior for the topic distribution with a dirichlet multinomial regression for the given covariates
on the other hand researchers found that the same approach dirichlet multinomial regression was not feasible for the word distributions
eisenstein et al
identify three main problems when applying the multinomial framework to the word distribution the increase in parameters the computational complexity and the lack of sparsity
this is especially a problem when considering the word topic relationship for a large corpus of documents can have an extensive vocabulary
to address this problem they introduce the sparse additive generative model sage
the sage consists of an alternative framework that uses ations in log frequency from a benchmark distribution
there are two main advantages of this model
by cutting down on the number of parameters this approach reduces overtting issues and can combine generative facets through simple addition in log space avoiding the need for latent switching variables
given the inclusion of these predecessor models its tant to review the terminology to distinguish between the two types of covariates used in the model prevalence and content
prevalence covariates are document level attributes that affect which topics are communicated in a document
in other words prevalence covariates impact the topic proportions through the dmr model
on the other hand a content covariate is a document level attribute that affects how the topics are conveyed in a document
in this case a content covariate modies which words are used to communicate a topic and thus the word proportion that dene each topic
similarly content covariate is used in the sage component of the stm model
for stm inference like its predecessor models the act posterior is intractable which restricts the computational methods possible
to address intractability roberts et al
introduce a partially collapsed variational maximization algorithm for inference estimation
another problem with the inference estimation with this model is the non conjugacy of the logistic normal distribution which replaces the dirichlet prior distribution assumed in lda to the posterior distribution multinomial
to account for the non conjugacy they also introduce a laplace approximation for the non conjugate elements of the model
current stm computation only allows one content covariate
however this covariate can have multiple levels and thus a researcher can approximate this with multiple variables by using the interaction between all of the levels of the variables
for example instead of having two binary content covariates for gender male or female and treatment yes or no this can be approximated with four levels of one covariate male yes male no female yes female no
fig

lda stm sage and dmr out of sample performance by roberts et al
roberts et al
analyze the estimation benets of stm relative to lda and stms sub component models like ctm sage and dmr
they nd that for metadata generated topic processes stm outperforms lda in covariate inference and out of sample prediction
figure provides the formance of lda and stm on samples of a simulated dataset including a random component that is generated along with a continuous non linear covariate
lda was on average able to identify the non linear pattern of the covariate with the topic however due to the noise component lda was not robust to consistently identify the true linear pattern
for example there were cases in which the model s estimates inferred a reverse pattern than the true shape value
on the other hand stm was able to correctly identify the pattern consistently across all datasets
the conclusion is by incorporating the covariates directly into the algorithm rather than through a post hoc analysis we can be condent that stm will consistently detect the pattern while on average lda can but not always
ultimately the improved performance of measuring covariate relationships yield the model to consistently out perform post hoc lda in out of sample prediction
figure is an analysis by on the same dataset but using lda sage dmr and stm model separately
they nd that stm largely driven by the inuence of dmr had the best performance while sage and lda performed worse across all candidate sets for a different number of topics
there are three major advantages to the stm model
first the model facilitates a statistical based framework for facilitating hypothesis testing for the causal impact of ment metadata that affects which and how the topics vary by document
in fact the model allows for general relationship framework outside of typical linear relationships in which searchers can test for non linear patterns through log spline or interaction terms
second the model introduces enhancements to the computational methods in order to make the model feasible for modeling as well as methods for model evaluation interpretation and handling multi modality
last the authors introduce an open source r package to accompany the paper
this package facilitates the implementation of the model to a much wider audience e

social scientists by providing the model in a high level r rather than the traditional level languages most topic models methods have previously been available e

java in mallet python for gensim
is able to model non linear patterns through its extension to allow spline transformations and covariate interactions
fig

posterior predictive checks ppc using instantaneous mutual information from roberts et al
despite these advancements stm has multiple limitations
first the model remains intractable for a large number of covariates or more than one content covariate
second the model can produce statistical testing for prevalent covariates but not content covariates
third given topic proportions zero sum property i
e
must sum to one interpretation of covariates marginal effects in stm are difcult as an increase in one topic proportion must be tied to a decrease in similar magnitude to the other topic proportions
last the model uses are approximation methods that are more complex than simulation based e

mcmc or gibbs sampling methods
last as the model is more complex than lda so too is its output as it now must account for differences in the input covariates
with this last point in mind i ll consider the role that explainable interactive visualizations can play in future research for stm in the conclusion of this paper
b
multi modality the second problem with topic models is its lack of stability due to its inherent computational complexity
topic model inference is a np hard problem
this hardness leads to the problem of multi modality i
e
an optimization problem like maximum likelihood can be solved locally but can not with certainty be solved globally
multi modality is important when an algorithm output can change with initialization parameters
in other words different starting parameters can alter results and threatened the legitimacy of the approach and results
while this implication can not be directly solved as its a product of the algorithms hardness they argue the solution for researchers is to use improved initialization spectral methods and posterior predictive bility checks to achieve the best of all possible local optima
further chuang et al
provide an example of how a visualization interface can help researchers understand the effect of multi modality on the stability of their results
a good initialization approach needs to balance the off between the computational cost of implementation and the et al
write in a footnote that connecting np hard plexity and multi modality local modes is difcult to state easily
they argue that hardness is sufcient to prove an algorithm is not easily solved with global convergence
roberts et al
note that such sensitivity to starting positions is well known by computer scientists yet infrequently discussed
relative model improvement for optimizing the initial state
in the stm model a researcher has the option to a spectral initialization that provides a quick starting point that minimizes the chance of nding sub optimal local minima
one approach for stm is to run standard lda on the dataset and use the lda results to help determine the initial state for stm
roberts et al
nd that using lda s results for initialization not only improves the model s results but also using gibbs sampling leads to a faster convergence
however they nd that using lda for initialization does not help increase the average quality of the results with more simulations
instead they recommend a spectral learning approach that provides a more robust initialization with the computational complexity of using lda s results
the spectral approach utilizes the connection of lda with non negative matrix factorization see that provides theoretical antees that the optimal parameters will be recovered
essentially this approach makes stronger model assumptions matrix decomposition elements must be non negative in order to avoid the problems of multi modality
however roberts et al
identify two practical limitations with spectral initialization
first it requires large amounts of data to perform adequately
second the modication of the model s tions have the potential of leading to less interpretable models
posterior predictive checks ppc provide insight on how well the model s assumptions hold
figure vides one such ppc instantaneous mutual information from for three topics each plot representing the top ten most likely words for each of the three topics
in these examples the model s assumptions does not hold if there is a signicant difference between what is observed the black circle and the simulated reference distribution the open circles
for instance in the sars avian flu topic the words sar and bird are observed to be drastically outside of its reference distribution
this indicates that these words indicate would most likely be better of split between two separate topics
while one deviation like this may not jeopardize the entire topic model results the goal of analyzing ppc is to identify systematic errors e

caused by a poor local optimal solution or initialization like this across multiple topics that can threaten the legitimacy of the model as a whole
last chuang et al
provide an interactive solution to the problem of multi modality topiccheck a visualization rather than assume them
second it provides a formal way of quantifying the content and prevalance effects on the topics especially with a treatment variable through a cheaper more consistent semi automated process
for their application of stm they use the anes can national election studies survey dataset of dents on questions related to the election
in addition to open ended survey responses related to the election the dataset includes individual covariates about the respondent including their top voting issue party identication education and age
further the survey randomly assigned respondents into test and control groups in which either group was subjected to ferent treatment to simulate either intuitive test or reective control thinking
the treatment variable was collected to test political science theory on the role either intuition or reection shapes decision making as demonstrated through their ended responses
to analyze their results the authors compare their nding with the stm model relative to that of human coders who had previously analyzed the same dataset
they make three sions
first they nd that in aggregate stm categorized most of the same responses into similar topics as human coders
second they nd that stm recovers covariate relationships closely identical to the anes human coders
the rst two ndings suggest that stm can do the job very similar to the man coders but at a much lower cost through automation rather than hiring multiple human coders
last they do nd stm has the additional advantage in that it required less assumptions about the topics themselves as stms unsupervised approach naturally found the occurring topics without requiring to know in advance what topics were likely to be discussed
however they do nd one disadvantage to stm is that low incidence pre determined categories are unlikely to be identied in stm as the unsupervised approach will likely identify these topics
instead stm may have writing style topics using frequent words like go and get that are not relevant but occur in the unsupervised approach because they have enough volume and were not used as stop words
nevertheless they nd that the benets of stm largely outstrip any such costs relative to human coding in this example
another signicant application of stm has been by litical scientists to analyze political rhetoric
one of the rst examples come from milner and tingley in which they analyzed the text within lobbying reports to estimate the impact of whether the white house was lobbied on that specic issue
they nd that economic interest groups tend to have higher inuence in topics that have high distributional impacts and low information asymmetries like military spending
moreover they nd in topics like military spending such economic interest groups are less likely to directly solicit the white house instead targeting the white house with direct lobbying efforts for efforts that are more policy focused
a further stm application by political scientists includes the measurement of media bias in news articles related to china by ve different international media outlets
using a dataset of over news articles roberts et al
analyzed the difference in what prevalence and how content ve media outlets are xinhua china bbc great britain jen japan ap united states and afp france fig

topiccheck with iterations of a topic stm to assess topic stability from chuang et al
interface to assess the stability of topics across multiple runs of stm
figure shows topiccheck for iterations of a topic stm for a dataset of political blogs
each rectangle is a topic with each column being one run of stm
topics are aligned across each stm iteration into horizontal groups or rows
topics that are not aligned to other topics for a different stm run are included below the baseline axis
one of the key goals of this visualization is to determine how robust or stable a topic is for different stm iterations
for example the top row including words like barack obama has topics for all models and thus we can feel condent this aligned topic is very robust
on the other hand the last topic row nancial crisis is a topic in only of the stm iterations
we can infer that this topic is not as stable as the topics that have aligned topics in more stm iterations
chuang et al
ndings suggest the need to consider more than one topic model as one single topic may not capture all perspectives on a dataset
further another contribution of this paper is analyzing the impact of including or excluding rare words on the stability of the topics
this is a novel approach to provide users an interactive understanding of the impact of a pre processing step like sparse word removal that they nd can potentially have a signicant impact on the nal topics
c
stm applications in social sciences already stm has been applied to multiple areas of social science including open ended surveys political rhetoric social media and massive online open courses moocs
roberts et al
provide the rst extensive application of stm with open ended survey responses
traditionally human coding is the most common methodology for social scientists to analyze open ended survey responses
the authors argue stm has two key advantages
first being an unsupervised algorithm stm allows the researcher to discover topics from the data et al
was the rst paper that introduced stm and provided two simple applications open ended surveys and media bias in news articles
however both of these analyses only covered a few paragraphs and did not provide the full details of the model including model setup selection and validation
et al
acknowledge that many survey analyses simply ignore open ended survey responses in favor of closed ended surveys given the lack of tools to analyze such results
fig

the most likely words for a topic falungong conditioned on the news source left and the estimated topic proportion for each source right from roberts et al
topics related to the rise of china were communicated by ferent media sources
they found that the chinese based media outlet xin had in general a more positive view of related topics than other international media sources while covering less and using different language for more sial topics like chinese dissidents and protesters
for example figure provides two graphics relating the topic relating to the falungong movement a chinese religious group that the chinese government outlawed in leading to many protests and a government crackdown on participants
the left side represents the different words used to characterize this topic conditioned on each media source content covariate
xinhua xin used words like illegal smuggler and criminal to describe the movement as an illegal operation
on the other western media outlets like the u
s
associated press ap and the french agence france presse afp characterized the movement with language characterizing the movement as a protest rather than a criminal operation
for example ap and afp s conditioned topic words include protest dissident movement and crackdown
moreover stm could also be used to measure the difference in topic proportion each outlet used each topic
in the example of the falungong topic the right side of figure shows the estimated topic proportions along with condence intervals for each of the ve media outlets
xinhua covered the falungong topic much less frequently than did western media outlets like ap and afp with statistical signicance
in addition to political and news rhetoric other applications of stm by social scientists include social media messages in particular twitter and online course feedback
lucas et al
analyzed bilingual social media messages through automated machine translation to estimate the effect language arabic or chinese has on twitter users topics regarding edward snowden
sachdeva et al
used stm to analyze smoke related tweets and the potential temporal effects of wildres have on users tweets relative to those individuals who reside or work close to affected areas
reich et al
analyzed the use of massive online mooc course feedback to scale open ended course responses
reich et al
extended this work in moocs and connected it with political rhetoric to identify political discussions within these courses
ultimately given stm s recent introduction these examples represent just a handful of the early applications
as the technique begins to mature and more researchers learn about the methodology there is no doubt the number and the range of applications will begin to increase rapidly in the near future
v
conclusion and future research while used rarely by most social scientists topic els offer social scientists an innovative and objective way to measure latent qualities on large unstructured datasets like social media open ended surveys and news or research publications
topic models are one of many machine ing frameworks that when combined with causal inference tools represent a signicant opportunity for social scientists to answer many of society s large scale problems
in particular stm represents an example of integrating machine learning topic models with causal inference mechanisms in a generalized framework that can be applied to many social science problems
nevertheless a major impediment to the expansion of stm and machine learning algorithms in general for most social scientists is the high knowledge barriers to use these models
given the quick development of such models many social scientists may lack the training and experience in machine learning and computational ming to implement and to analyze topic models
to address this concern a potential research opportunity with the stm model is through an explainable user interface visualization that can aid social scientists in hypothesis testing large collections of text documents
the importance of this opportunity is exemplied through darpa s recent explainable articial intelligence xai program
an inherent problem with most machine learning niques and tasks for decision makers is the lack of explanation fig

explainable articial intelligence xai from darpa of why a specic prediction or choice was made by the algorithm
instead the goal of the xai program is to provide explainable features to traditional machine learning techniques along with an explainable user interface e

visualization to combine human insight with the predictions of the model to answer the question of why
structural topic modeling would directly t under the program s approach of interpretable models that that learn more structured interpretable or causal models
figure provides a graphic representing the role of explainable user interfaces for machine learning tasks like stm inference
therefore a key research opportunity for stm is the velopment of an explainable intelligent interactive system to analyze for interpretation model evaluation multi modality pre processing and validation
such an interface could be built integrating high level visualization tools like shiny and vega lite that can provide a robust set of tools with minimal amount of coding
further if written in r such an interface could easily combine with r packages like stm for widespread use see appendix
appendix a appendix stm r package the r package stm was introduced by roberts et al
to facilitate the widespread use of stm for r users
a key advantage to the stm r package is that includes multiple methods of posterior predictive checks interpretation data preprocessing model selection and static visualizations
figure provides an outline of the functions within the stm package categorized by their different functionalities
appendix b appendix word embedding models and glove models more generally topic models are part of a larger work of language models called vector space models
in this approach language is encoded by vector representations based addition to interpretable models the program cites two other proaches deep explanation and model induction
interfaces could be used to measure the impact of processing stemming stop words
to help researchers determine its impact on the model s results
there are two stm based visualization packages stmviz and stmbrowser but their functions are limited
fig

functions within stm package from roberts et al
on the distributional hypothesis of language i
e
words are used in similar context to words with similar meanings
under this interpretation bag of words models like lsa and lda are considered as count based models as they encode language as a series of vector counts
as mentioned earlier one downside of this approach is that it ignores the context in which words are used an obvious deciency when considering semantic meaning
alternatively context based models are a new generation of vector space models that encode word text within a vector framework
these models are sometimes called word embedding neural language models or simply predictive models
the rst example of context based models was introduced by mikolov et al
as the models continuous bag of words cbow and skip grams
unlike the traditional bag of words approaches that treated each word like an atomic unit this approach considered a rolling window context e

ve words to build a vector space model that provided deeper semantic meaning while facilitating scalability to millions of words and documents
the difference between the models is a reversal of the model inputs and outputs
the cbow model uses a list of words e

ve words to best predict a word that will most likely be used in a similar context
alternatively the skip gram model uses a given word to predict what are the most likely words that will be used in a similar context
building off of this framework pennington socher and manning unied vector space models by combining features of the count based models like lsa and lda and context based models like to a more robust model named glove or global vectors of word representations
acknowledging the deciencies of the count based models that motivated the model pennington socher and manning argue that models have the opposite lem that by only analyzing the local context of words this approach fails to utilize the statistical properties provided through a count based approach
ultimately their approach resulted in not only a better predictive model that produced deeper semantic meanings the neural language structure connected directly with related breakthroughs in the tion of deep learning to text analysis
references d
m
blei
probabilistic topic models
communications of the acm
k
m
quinn b
l
monroe m
colaresi m
h
crespin and d
r
radev
how to analyze political attention with minimal assumptions and costs
american journal of political science
j
grimmer and b
stewart
text as data the promise and pitfalls of automatic content analysis methods for political texts
political analysis
j
grimmer
we are all social scientists now how big data machine learning and causal inference work together
ps political science and politics

d
lazer et al
life in the network the coming age of computational social science
science new york ny
h
wallach
computational social science toward a collaborative future
computational social science discovery and prediction
m
e
roberts b
m
stewart d
tingley and e
m
airoldi
the structural topic model and applied social science
advances in neural information processing systems workshop on topic models tion application and evaluation
m
e
roberts et al
structural topic models for open ended survey responses
american journal of political science
d
blei and j
lafferty
correlated topic models
neural information d
blei and j
mcauliffe
supervised topic models
neural information processing systems
processing systems
y
w
teh m
i
jordan m
j
beal and d
m
blei
hierarchical dirichlet processes
journal of the american statistical association
a
gelman and e
loken
the statistical crisis in science
american scientist
m
j
denny and a
spirling
text preprocessing for unsupervised learning why it matters when it misleads and what to do about it
unpublished manuscript dep
polit
sci stanford univ and inst
quant
soc
sci
harvard univ

com abstract
a
schoeld l
thompson and d
mimno
quantifying the effects of text duplication on semantic models
emnlp
a
schoeld m
magnusson l
thompson and d
mimno
derstanding text pre processing for latent dirichlet allocation
acl workshop for women in nlp winlp
a
schoeld m
magnusson and d
mimno
pulling out the stops rethinking stopword removal for topic models
eacl
a
schoeld and d
mimno
comparing apples to apple the effects of stemmers on topic models
tacl vol

d
mimno and a
mccallum
topic models conditioned on arbitrary features with dirichlet multinomial regression
in uncertainty in cial intelligence uai
j
eisenstein et al
sparse additive generative models of text
in international conference on machine learning icml
a
gruber m
rosen zvi and y
weiss
hidden topic markov models
in proc
of the conference on articial intelligence and statistics
y
liu a
niculescu mizil and w
gryc
topic link lda joint models of topic and author community
in proc
of the international conference on machine learning acm
d
m
blei a
y
ng and m
i
jordan
latent dirichlet allocation
j
chang and d
blei
hierarchical relational models for document journal of machine learning research
networks
annals of applied statistics
m
e
roberts b
m
stewart and e
m
airoldi
a model of text for experimentation in the social sciences
journal of the american statistical association
j
chang j
boyd graber c
wang s
gerrish and d
m
blei
reading topic models
neural information tea leaves how humans interpret processing systems
s
deerwester s
t
dumais g
w
furnas t
k
landauer and r
harshman
indexing by latent semantic analysis
journal of the american society of information science
t
landauer and s
dumais
a solution to platos problem the latent semantic analysis of acquisition induction and representation of knowledge
psychological review
t
hofmann unsupervised learning by probabilistic latent semantic analysis machine learning
c
ding t
li and w
peng
on the equivalence between non negative matrix factorization and probabilistic latent semantic indexing
comput
stat
data analysis
s
p
crain k
zhou s

yang and h
zha
dimensionality reduction and topic modeling from latent semantic indexing to latent dirichlet allocation and beyond
in mining text data c
aggarwal and c
zhai eds
springer
d
blei and j
lafferty
topic models
text mining theory and applications
t
grifths and m
steyvers
finding scientic topics
proc
of the national academy of science
m
d
hoffman d
m
blei and f
bach
online learning for latent dirichlet allocation
neural information processing systems
m
rosen zvi t
grifths m
steyvers and p
smyth
the author topic model for authors and documents
uai proc
of the conference on uncertainty in articial intelligence
d
blei and j
lafferty
dynamic topic models
in proc
of the international conference on machine learning acm
j
eisenstein b
oconnor n
a
smith and e
p
xing
a latent variable model for geographic lexical variation
in emnlp
y
rao q
li x
mao and l
wenyin
sentiment topic models for social emotion mining information science
d
mimno h
m
wallach e
talley m
leenders and a
mccallum
optimizing semantic coherence in topic models
proc
of the conference on empirical methods in natural language processing
j
bischof and e
m
airoldi
summarizing topical content with word frequency and exclusivity
in icml h
wallach i
murray r
salakhutdinov and d
mimno
evaluation methods for topic models in proc
of the international conference on machine learning
h
wallach d
mimno and a
mccallum
rethinking lda why priors matter
in neural informational processing systems
m
a
taddy
on estimation and selection for topic models
in proc
of the international conference on articial intelligence and statistics aistats
j
tang and j
l
zhao
understanding the limiting factor of topic modeling via posterior contraction analysis
journal of machine learning research
l
hong and b
d
davison
empirical study of topic modeling in twitter
in proc
of the first workshop on social media analytics
acm y
hu j
boyd graber b
satinoff and a
smith
interactive topic modeling machine learning oct

w
dou and s
liu
and time oriented visual text analysis
ieee computer graphics and applications
j
chuang c
d
manning and j
heer
termite visualization niques for assessing textual topic models
proc
international working conference on advanced visual interfaces acm
e
alexander j
kohlmann r
valenza m
witmore and m
gleicher
serendip topic model driven visual exploration of text corpora
proc
of the ieee conference visual analytics science and technology vast
w
dou x
wang r
chang and w
ribarsky
paralleltopics a probabilistic approach to exploring document collections
in ieee visual analytics science and technology ieee
j
chuang et al
topiccheck interactive alignment for assessing topic model stability
in y
weiss b
schlkopf j
c
platt eds
proceedings of naacl hlt cambridge ma mit press
j
chuang d
ramage c
d
manning and j
heer
interpretation and trust designing model driven visualizations for text analysis
proc
of the acm annual conference on human factors in computing systems acm
w
dou l
yu x
wang z
ma and w
ribarsky
hierarchicaltopics visually exploring large text collections using topic hierarchies
ieee tvcg
w
cui s
liu z
wu and h
wai
how hierarchical topics evolve in large text corpora
ieee trans
vis
comput
graphics
x
wang s
liu j
liu j
chen j
zhu and b
guo
topicpanorama a full picture of relevant topics
ieee trans
vis
comput
graphics
f
wei et al
tiara a visual exploratory text analytic system
proc
of the acm sigkdd international conference knowledge discovery and data mining
s
liu m
x
zhou s
pan y
song w
qian w
cai and x
lian
tiara interactive topic based visual text summarization and analysis
acm trans
intelligent systems and technology
w
cui et al
textow towards better understanding of evolving topics in text
ieee trans
vis
comput
graphics
s
liu y
chen h
wei j
yang and k
zhou
exploring topical lag across corpora
ieee trans
knowl
data eng

w
dou x
wang d
skau w
ribarsky and m
x
zhou
leadline interactive visual analysis of text data through event identication and exploration
proc
of ieee conference on visual analytics science and technology vast
s
liu j
yin x
wang w
cui k
cao and j
pei
online visual analytics of text streams
ieee trans
vis
comput
graphics
h
v
milner and d
tingley
sailing the waters edge the domestic politics of american foreign policy
united states princeton university press c
lucas r
a
nielsen m
e
roberts b
m
stewart a
storer and d
tingley
computer assisted text analysis for comparative politics
political analysis feb
s
sachdeva s
mccaffrey and d
locke
social media approaches to modeling wildre smoke dispersion spatiotemporal and social scientic investigations
information communication society
j
reich d
h
tingley j
leder luis m
e
roberts and b
stewart
computer assisted reading and discovery for student generated text in massive open online courses journal of learning analytics
j
reich b
stewart k
mavon and d
tingley
the civic mission of moocs measuring engagement across political differences in forums association for computing machinery learning at scale
j
chuang et al
computer assisted content analysis topic models for exploring multiple subjective interpretations
conference on neural information processing systems nips
workshop on human propelled machine learning
montreal canada
darpa
explainable articial intelligence xai darpa
darpa
mil attachments darpa
pdf
t
kulesza m
burnett w

wong and s
stumpf
principles of explanatory debugging to personalize interactive machine learning
in proc
of the international conference on intelligent user interfaces
shiny by rstudio

rstudio
com june k
wongsuphasawat d
moritz a
anand j
mackinlay b
howe and j
heer
voyager exploratory analysis via facted browsing of visualization recommendations
ieee trans
visualization comp
graphics
j
grimmer a bayesian hierarchical topic model for political texts measuring expressed agenda in senate press releases political analysis
a
satyanarayan d
moritz k
wongsuphasawat and j
heer
lite a grammar of interactive graphics ieee transactions on ization and computer graphics jan

m
baroni g
dinu and g
kruszewski
do nt count predict a tematic comparison of context counting vs
context predicting semantic vectors
acl
t
mikolov k
chai g
s
corrado and j
dean
efcient estimation of word representations in vector space arxiv preprint

j
pennington r
socher and c
d
manning
glove global vectors for word representation in emnlp
y
bengio r
ducharme p
vincent and c
janvin
a neural bilistic language model in jmlr
y
lecun y
bengio and g
hinton
deep learning nature
m
j
paul and m
dredze
discovering health topics in social media using topic models plos one
p
barbera r
bonneau p
egan j
t
jost j
nagler and j
tucker
leaders or followers measuring political responsiveness in the u
s
congress using social media data
working paper
w
h
greene econometric analysis ed
harlow pearson tion
m
roberts b
stewart and d
tingley
stm r package for structural topic models
r package version


a
mccallum
mallet a machine learning for language toolkit

cs
umass
edu
r
rehurek and p
sojka
software framework for topic modelling with large corpora proceedings of the lrec workshop on new challenges for nlp frameworks
c
fong and j
grimmer
discovery of treatments from text corpora in proceedings of the annual meeting of the association for computational linguistics
d
sontag and d
m
roy
complexity of inference in topic models advances in neural information processing workshop on applications for topic models text and beyond
m
roberts b
stewart and d
tingley
navigating the local modes of big data the case of topic models
in data analytics in social science government and industry
new york cambridge university press
a
gelman et al
bayesian data analysis
boca raton fl chapman hall crc
s
arora et al
a practical algorithm for topic modeling with provable guarantees
proc
of the intl
conf
on machine learning acm
d
mimno and d
blei
bayesian checking for topic models
proc
of the conference on empirical methods in natural language processing

