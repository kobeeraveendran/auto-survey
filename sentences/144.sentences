r a l c
s c v
v i x r a data driven summarization of scientic articles nikola i
nikolov michael pfeiffer richard h
r
hahnloser institute of neuroinformatics university of zurich and eth zurich switzerland niniko pfeiffer
ethz
ch abstract data driven approaches to sequence to sequence modelling have been successfully applied to short text summarization of news articles
such models are typically trained on input summary pairs consisting of only a single or a few sentences partially due to limited ability of multi sentence training data
here we propose to use scientic articles as a new milestone for text summarization large scale training data come almost for free with two types of high quality summaries at different levels the title and the abstract
we generate two novel multi sentence summarization datasets from scientic articles and test the suitability of a wide range of existing extractive and abstractive neural network based summarization approaches
our analysis demonstrates that scientic papers are suitable for data driven text summarization
our results could serve as valuable benchmarks for scaling sequence to sequence models to very long sequences

introduction the goal of automatic text summarization is to produce a shorter informative version of an input text
while tractive summarization only consists of selecting important sentences from the input abstractive summarization erates content without explicitly re using whole sentences nenkova et al

text summarization is an area with much promise in today s age of information overow
in the domain of scientic literature the rate of publications grows exponentially hunter and cohen which calls for efcient automatic summarization tools
recent state of the art summarization methods learn to summarize in a data driven way relying on large tions of input summary training examples
the majority of previous work focused on short summarization of news articles such as to generate a title rush et al
lapati et al

one major challenge is to scale these methods to process long input output sequence pairs
rently availability of large scale high quality training data is scarce
in this paper we explore the suitability of scientic nal articles as a new benchmark for data driven text marization
the typical well structured format of scientic papers makes them an interesting challenge and provides plenty of freely available training data because every ticle comes with a summary in the form of its abstract and in even more compressed form its title
we make a st step towards summarization of whole scientic cles by composing two novel large datasets for scientic summarization title abstract pairs title gen composed of million papers in the biomedical domain and body pairs abstract gen composed of
the second dataset is particularly challenging because it is tended for summarizing the full body of the paper in terms of the abstract the lengths of input output sequences are substantially longer than what has been considered so far in previous research see table
we evaluate a range of existing state of the art approaches on these datasets extractive approaches based on word beddings as well as word subword and character level datasets are available at
including ninikolov data driven summarization versions with and without preprocessing
encoder decoder models that use recurrent as well as volutional modules
we perform a quantitative and tive analysis of the models outputs

background

extractive summarization given an input document consisting of ts sentences sss


the goal of extractive summarization is to select the k most salient sentences as the output mary
extractive summarization typically involves a tence representation module e that represents each put sentence si in a common space as ri e

as a vector of real numbers as well as a ranking module score that weights the salience wi of each sentence
a typical approach to unsupervised extractive summarization is to implement wi as the similarity between ri and a document representation or a document centroid rd radev et al

alternatively one can pute wi as the sentence centrality which is an based measure of sentence importance erkan and radev
in this work we propose two simple unsupervised lines for extractive summarization both of which rely on word embeddings mikolov et al

the rst emb represents each sentence in the input document as the weighted sum of its constituent word embeddings similar to rossiello et al
ri ti xsi xsi where is the embedding of word is an tional weighting function that weighs the importance of a word and ti is a normalization factor
as a weighing function we use the term frequency inverse ument frequency tf idf score similar to brokos et al

each sentence embedding ri can then be ranked by computing its cosine similarity ri to a document centroid rd computed similarly as ri
the summary sists of the top k sentences with embeddings most similar to the document embedding
the second baseline rwmd rank ranks the salience of a sentence in terms of its similarity to all the other sentences in the document
all similarities are stored in an sentence similarity matrix w
we use the relaxed word mover s distance rwmd to compute this matrix ner et al
wij sj sj si sj min xsi where si and sj are two sentences and is the euclidean distance between the embeddings of words and in the sentences
to rank the sentences we apply the graph based method from the lexrank system erkan and radev
lexrank represents the input as a highly connected graph in which vertices represent sentences and edges between sentences are assigned weights equal to their similarity from w
the centrality of a sentence is then computed using the pagerank algorithm page et al



abstractive summarization given an input sequence of tx words xxx


coming from a xed length input vocabulary vx of size kx the goal of abstractive summarization is to produce a densed sequence of ty summary words yyy


yty from a summarization vocabulary vy of size ky where tx ty
abstractive summarization is a structured diction problem that can be solved by learning a tic mapping for the summary yyy given the input sequence xxx dietterich et al



xxx
ty i the encoder decoder architecture is a recently proposed general framework for structured prediction cho et al
in which the distribution arg maxyyy is learned using two neural networks an encoder network e which produces intermediate representations of the put and a decoder language modelling network d which generates the target summary
the decoder is conditioned on a context vector c which is recomputed from the coded representation at each decoding step
the decoder was rst implemented using recurrent neural works rnns sutskever et al
cho et al
that process the input sequentially
recent studies have shown that convolutional neural networks cnns lecun et al
can outperform rnns in sequence transduction tasks kalchbrenner et al
gehring et al

unlike rnns cnns can be efciently implemented on parallel gpu hardware
this advantage is particularly important when working with very long input and output sequences such as whole paragraphs or documents
cnns create archical representations over the input in which lower ers operate on nearby elements and higher layers implement increasing levels of abstraction
in this work we investigate the performance of three isting systems that operate on different levels of sequence lstm is a recurrent long short granularity
the rst term memory lstm encoder decoder model sutskever et al
with an attention mechanism bahdanau et al
that operates on the word level processing the put sequentially
the second system fconv is a tional encoder decoder model from gehring et al

fconv works on the subword level and segments words into smaller units using the byte pair encoding scheme
using subword units improves the generation quality when ing with rare or unknown words sennrich et al

the third system is a character level encoder decoder model from lee et al
that models xxx and yyy as dividual characters with no explicit segmentation between tokens
rst builds representations of groups of acters in the input using a series of convolutional layers
it then applies a recurrent encoder decoder similar to the lstm system


scientic articles previous research on summarization of scientic articles has focused almost exclusively on extractive methods nenkova et al

in lloret et al
the authors develop an unsupervised system for abstract generation of biomedical papers that rst selects relevant content from the body following which it performs an abstractive formation fusion step
more recently kim et al
consider the problem of supervised generation of level summaries for each paragraph of the introduction of a paper
they construct a training dataset of computer ence papers from arxiv selecting the most informative tence as the summary of each paragraph using the jaccard similarity
thus their target summary is fully contained in the input
in collins et al
they develop a pervised extractive summarization framework which they apply to a dataset of computer science papers
to the best of our knowledge our work is the rst on abstractive title generation of scientic articles and is the rst to sider supervised generation of the absctract directly from the full body of the paper
the datasets we utilize here are also substantially larger than in previous work on scientic summarization
scientic articles are potentially more challenging to marize than news articles because of their compact plicit discourse style biber and gray
while the events described by news headlines frequently recur in lated articles a scientic title focuses on the unique bution that sets a paper apart from previous research teufel and moens
furthermore while the rst two tences of a news article are typically sufciently tive to generate its headline nallapati et al
teufel and moens the rst sentences of the abstract or troduction of a paper typically contain background mation on the research topic
constructing a good scientic title thus requires understanding and integrating concepts from multiple sentences of the abstract

datasets to investigate the performance of encoder decoder neural networks as generative models of scientic text we structed two novel datasets for scientic summarization
table statistics mean and standard deviation of the two scientic summarization datasets ken sentence counts are computed with nltk
title gen and abstract gen
title gen token count sentence count sent
token count overlap repeat size tr val test abstract title abstract gen token count sentence count sent
token count overlap repeat size tr val test body abstract for title gen we used whereas for gen we used the pubmed open access
medline contains scientic metadata in xml format of lion papers in the biomedical domain whereas the pubmed open access subset contains metadata and full text of
million papers
we processed the xml to pair the abstract of a per to its title title gen dataset or the full body gen skipping any gures tables or section headings in the body
we then apply several preprocessing steps from the moses statistical machine translation including tokenization and conversion to lowercase
any urls were removed all numbers replaced with and any pairs with abstract lengths not in the range of tokens title lengths not within tokens and body lengths not within tokens were excluded
the overlap y is the fraction of unique output summary tokens y that overlap with an input token excluding punctuation and stop words
as can be seen in table the overlaps are large in our datasets indicating frequent reuse of words
the repeat is the average overlap of each sentence si in a text with the remainder of the text where si denotes the complement of sentence si
repeat measures the redundancy of content within a text a high value indicates frequent repetition of content
whereas in abstracts there are only moderate els of repetition in the bodies the repetition rates are much higher possibly because concepts and ideas are reiterated in multiple sections of the paper
i si
evaluation set up we evaluated the performance of several state of the art proaches on our scientic summarization datasets
the tractive systems we consider are lead lexrank tdf emb and rwmd rank
the lead baseline returns the rst sentence of the abstract for title gen or the rst sentences of the body for abstract gen
lexrank erkan and radev is a graph based centrality approach frequently used as a baseline in the literature
emb tdf uses sentence to select the most salient sentences from the input
nih
gov databases
html
nlm
nih
gov pmc openftlist parser
use
github
io pubmed
com moses smt mosesdecoder use the best performing model from chiu et al
which is trained on pubmed and medline
while rwmd rank uses the relaxed word mover s distance as described in section


oracle estimates an upper bound for the extractive summarization task by nding the most similar sentence in the input document for each tence in the original summary
we use the relaxed word mover s distance to compute the output of the oracle
the abstractive systems we consider are lstm fconv and described in section


for lstm we set the put output vocabularies to use two lstm layers of hidden units each and word embedding of dimension we found no improvement from additionally ing the size of this model
for and fconv we use the default hyper parameters that come with the public mentations provided by the authors of the systems
the title gen lstm and fconv were trained for and epochs respectively until convergence
we were unable to train lstm and on abstract gen cause of the very high memory and time requirements ciated with the recurrent layers in these models
we found fconv to be much more efcient to train and we succeeded in training a default model for epochs
for title gen we used beam search with beam size while for abstract gen we found a beam size of to perform better


quantitative evaluation in tables and we evaluate our approaches using the rouge metric lin which is a recall based metric frequently used for summarization and meteor denkowski and lavie which is a precision based metric for machine translation
overlap can be interpreted as the tendency of the model to directly copy input tent instead of generating novel correct or incorrect words whereas repeat measures a model s tendency to repeat self which is a frequent issue with encoder decoder models suzuki and nagata
on title generation rwmd rank achieved the best mance in terms of selecting a sentence as the title
in all the abstractive systems signicantly outperformed the extractive systems as well as the extractive oracle
and fconv performed much better than lstm with a very high rate of overlap
the rouge performance of and fconv is similar despite the difference of a few points in favour of fconv that model is evaluated on a level ground truth le where we observe a slight increase of rouge points on average due to the conversion
the baseline remained on abstract generation tough to beat in terms of rouge and only the extractive systems managed to surpass it by a small margin
all tractive systems achieved similar results with rwmd rank table metric results for the title gen dataset
r l represent the l metrics
r l meteor







token count overlap model oracle lexrank emb tdf rwmd rank lstm fconv























table metric results for the abstract gen dataset
r l represent the l metrics
model oracle lexrank emb tdf rwmd rank fconv repeat r l meteor





token count overlap

















having a minor edge while the abstractive fconv performed poorly even though it performed best in terms of teor
we observed a much higher repeat rate in the put summaries than the observed average in the inal abstracts table
as revealed by the large repeat standard deviation for fconv some examples are affected by very frequent repetitions


qualitative evaluation in tables and we present two shortened inputs from our title gen and abstract gen test sets along with original and system generated summaries
in figure we show a histogram of the locations of input sentences that estimates which locations were most preferred on average when ducing a summary
we observe a large variation in the sentence locations lected by the extractive systems on title gen figure with the rst sentence having high importance
based on our inspection it is rare that a sentence from the abstract will match the title exactly the title is also typically shorter than an average sentence from the abstract table
a good title seems to require the selection combination and paraphrasing of suitable parts from multiple sentences as also shown by the original titles in our examples
many of the titles generated by the abstractive systems sound ful and at rst glance can pass for a title of a scientic per
the abstractive models are good at discerning tant from unimportant content in the abstract at extracting long phrases or sometimes whole sentences and at tively combining the information to generate a title
lstm is more prone to generate novel words whereas and fconv mostly rely on direct copying of content from the abstract as also indicated by their overlap scores
closer inspection of the titles reveals occasional subtle takes for example in the rst example in table the fconv model incorrectly selected and duced which was investigated in the previous work of the authors and is not the main focus of the article
the model also copied the incorrect genus mouse instead of rat
sometimes the generated titles sound too general and fail to communicate the specics of the paper in the second example all models produced a model of basal ganglia missing to include the keyword reinforcement learning a model of reinforcement learning in the basal ganglia
these mistakes highlight the complexity of the task and show that there is still much room for further improvement
as shown in figure the introductory and concluding sections are often highly relevant for abstract generation however relevant content is spread across the entire paper
interestingly in the example in table there is a wide range of content that was selected by the extractive systems with little overlap across systems
for instance rwmd rank overlaps with oracle by sentences and only by sentence with emb tdf
the outputs of the abstractive fconv tem on abstract generation are poor in quality and many of the generated abstracts lack coherent structure and content ow
there is also frequent repetition of entire sentences as shown by the last sentences produced by fconv in table
fconv also appears to only use the rst sentences of the paper to construct the abstract figure

conclusion we evaluated a range of extractive and abstractive ral network based summarization approaches on two novel datasets constructed from scientic journal articles
while the results for title generation are promising the models struggled with generating the abstract
this difculty lights the necessity for developing novel models capable of efciently dealing with long input and output sequences while at the same time preserving the quality of generated sentences
we hope that our datasets will promote more work in this area
a direction to explore in future work is hybrid extractive abstractive end to end approaches that jointly select content and then paraphrase it to produce a summary
c figure sentence selection normalized histograms computed on the test set showing the input locations that were most preferred on average by the systems on title gen a and abstract gen b c
for we normalize the sentence locations by the length of each paper to get a better uniform view there is a large variation in the length of a paper as shown in table
for the abstractive systems we search for the closest sentences in the input using relaxed word mover s distance see section


table examples from the test set of title gen
the outputs of the extractive systems are highlighted as oracle tdf emb rwmd rank
for the abstractive systems we manually highlighted the text of the concepts that are relevant for the task errors are highlighted in red
example giridharan et al
abstract amyloid neurotoxicity is a major pathological mechanism of alzheimers disease ad
our previous studies have demonstrated that schisandrin b sch b an antioxidant lignan from schisandra chinensis could protect mouse brain against and cisplatin induced neuronal dysfunction
in the present study we examined the protective effect of sch b against intracerebroventricular a induced neuronal dysfunction in rat cortex and explored the potential mechanism of its action
our results showed that days co administration of sch b significantly improved the ioral performance of a rats in step through test
at the same time sch b attenuated a induced increases in oxidative and nitrosative stresses


the aforementioned effects of sch b suggest its protective role against a induced neurotoxicity through intervention in the negative cycle of rage mediated a accumulation during ad patho physiology
original title schisandrin b ameliorates icv infused amyloid induced oxidative stress and neuronal dysfunction through inhibiting rage nf b mapk and up regulating hsp beclin expression lstm schisandrin b an antioxidant lignan from schisandra chinensis protects against amyloid neurotoxicity schisandra chinensis b protects against intracerebroventricular infused amyloid induced neuronal dysfunction in rat cortex fconf schisandrin b protects mouse brain against and induced neurotoxicity in rats example fee abstract in its simplest formulation reinforcement learning is based on the idea that if an action taken in a particular context is followed by a favorable outcome then in the same context the tendency to produce that action should be strengthened or reinforced



recent experiments in the songbird suggest that vocal related bg circuitry receives two functionally distinct excitatory inputs



the other is an efference copy of motor commands from a separate cortical brain region that generates vocal variability during learning
based on these ndings i propose here a general model of vertebrate bg function that combines context information with a distinct motor efference copy signal



the model makes testable predictions about the anatomical and functional properties of hypothesized context and efference copy inputs to the striatum from both thalamic and cortical sources
original title oculomotor learning revisited a model of reinforcement learning in the basal ganglia incorporating an efference copy of motor actions
lstm a model of basal ganglia function
a general model of vertebrate basal ganglia function
fconf a model of basal ganglia function in the songbird

acknowledgments we thank the reviewers for their useful comments and nvidia for the donation of a titan x graphics card

references bahdanau d
cho k
and bengio y

neural chine translation by jointly learning to align and late
arxiv preprint

biber d
and gray b

challenging stereotypes about academic writing complexity elaboration plicitness
journal of english for academic purposes
brokos g

malakasiotis p
and androutsopoulos i

using centroids of word embeddings and word mover s distance for biomedical document retrieval in question answering
arxiv preprint

chiu b
crichton g
korhonen a
and pyysalo s

how to train good word embeddings for ical nlp
proceedings of page
cho k
van merrienboer b
gulcehre c
danau d
bougares f
schwenk h
and gio y

learning phrase representations using rnn encoder decoder for statistical machine translation
arxiv preprint

cho k
courville a
and bengio y

ing multimedia content using attention based encoder decoder networks
arxiv preprint

collins e
augenstein i
and riedel s

a vised approach to extractive summarisation of scientic table two examples from the test set of abstract gen
the outputs of the extractive systems are highlighted as tdf emb and rank whereas gray denotes overlap between the two
in bold we mark the content that was selected by the fconv system next page in full and in underline we mark the selection of the oracle
example pyysalo et al
body in recent years there has been a significant shift in focus in biomedical information extraction from simple pairwise relations representing associations such as protein protein interactions ppi toward representations that capture typed structured associations of arbitrary numbers of entities in specic roles frequently termed event extraction
much of this work draws on the genia event corpus


this resource served also as the source for the annotations in the rst collaborative evaluation of biomedical event extraction methods the bionlp shared task on event extraction bionlp st as well as for the genia subtask of the second task in the series
another recent trend in the domain is a move toward the application of extraction methods to the full scale of the existing literature with results for various targets covering the entire pubmed literature database of nearly million citations being made available
as event extraction methods initially developed to target the set of events dened in the genia bionlp st corpora are now being applied at pubmed scale it makes sense to ask how much of the full spectrum of gene protein associations found there they can maximally cover



by contrast we will assume that associations not appearing in this data can not be extracted as the overwhelming majority of current event extraction methods are based on supervised machine learning or hand crafted rules written with reference to the annotated data it reasonable to assume as a rst approximation that their coverage of associations not appearing in that data is zero
in this study we seek to characterize the full range of associations of specic genes proteins described in the literature and estimate what coverage of these associations event extraction systems relying on currently available resources can it is necessary not only to have an inventory of concepts that largely maximally achieve
to address these questions covers the ways in which genes proteins can be associated but also to be able to estimate the relative frequency with which these concepts are used to express gene protein associations in the literature



here as we are interested in particular in texts describing associations between two or more gene protein related entities we apply a focused selection picking only those individual sentences in which two or more mentions co occur
while this excludes associations in which the entities occur in different sentences their relative frequency is expected to be low for example in the bionlp st data all event participants occurred within a single sentence in of the targeted biomolecular event statements



here we follow the assumption that when two entities are stated to be associated in some way the most important words expressing their association will typically be found on the shortest dependency path connecting the two entities
the shortest path hypothesis of bunescu and mooney
the specic dependency representation


table shows the words most frequently occurring on these paths
this list again suggests an increased focus on words relating to gene protein associations expression is the most frequent word on the paths and binding appears in the top ranked words



finally to make this pair data consistent with the tps event spans tokenization and other features we aligned the entity annotations of the two corpora



this processing was applied to the bionlp st training set creating a corpus of entity pairs of which were marked as expressing an association positive



evaluation
we rst evaluated each of the word rankings discussed in the section on identication of gene protein associations by comparing the ranked lists of words against the set of single words marked as trigger expressions in the bionlp st development data



to evaluate the capability of the presented approach to identify new expressions of gene protein associations we next performed a manual study of candidate words for stating gene protein associations using the e w ranking



we then selected the words ranked highest by e w that were not known grouped by normalized and lemmatized form and added for reference examples of frequent shortest dependency paths on which any of these words appear see example in table



if static relations and experimental observations and manipulations are excluded as arguably not in scope for event extraction this estimate suggests that currently available resources for event extraction cover over of all events involving gene protein entities in pubmed
sion
we found that out of all gene protein associations in pubmed currently existing resources for event extraction are lacking in coverage of a number of event types such as dissociation many relatively rare though biologically important protein post translational modications as well as some high level process types involving genes proteins such as apoptosis
this suggests that for practical applications it may be important to consider also this class of associations



while these results are highly encouraging it must be noted that the approach to identifying gene protein associations considered here is limited in a number of ways it excludes associations stated across sentence boundaries and ones for which the shortest path hypothesis does not hold does not treat multi word expressions as wholes ignores ambiguity in implicitly assuming a single sense for each word and only rectly includes associations stated between exactly two entities
the approach is also fundamentally limited to associations expressed through specic words and thus blind to e

part of relations implied by statements such as binding site



sions
we have presented an approach to discovering expressions of gene protein associations from pubmed based on named entity co occurrences shortest dependency paths and an unlexicalized classier to identify likely statements of gene protein associations
drawing on the automatically created full pubmed annotations of the turku pubmed scale tps corpus and using the shared task data to dene positive and negative examples of association statements we distilled an initial set of over million protein mentions into a set of unique unlexicalized paths estimated likely to express gene protein associations
these paths were then used to rank all words in pubmed by the expected number of times they are predicted to express such associations and candidate association expressing words not appearing in the shared task data evaluated manually
study of these candidates suggested new event classes for the genia ontology and indicated that the majority of statements of gene protein associations not covered by currently available resources are not statements of biomolecular events but rather statements of static relations or experimental manipulation



it could thus be assumed that the event types and the specic statements annotated in genia would have only modest coverage of all gene protein association types and statements in pubmed






et al
body the same site is exceedingly rare with less both alopecia areata aa and vitiligo are autoimmune diseases and their than ve cases being reported in the literature
example original abstract background event extraction following the genia event corpus and bionlp shared task models has been a considerable focus of recent work in biomedical information extraction
this work includes efforts applying event extraction methods to the entire pubmed literature database far beyond the narrow subdomains of biomedicine for which annotated resources for extraction method development are available
results in the present study our aim is to estimate the coverage of all statements of gene protein associations in pubmed that existing resources for event extraction can provide
we base our analysis on a recently released automatically annotated for gene protein entities and syntactic analyses covering the entire pubmed and use named entity co occurrence shortest dependency paths and an unlexicalized classier to identify likely statements of gene protein ciations
a set of high frequency high likelihood association statements are then manually analyzed with reference to the genia ontology
conclusions we present a rst estimate of the overall coverage of gene protein associations provided by existing resources for event extraction
our results suggest that for event type associations this coverage may be over
we also identify several biologically signicant associations of genes and proteins that are not addressed by these resources suggesting directions for further extension of extraction coverage
example fconv background in recent years there has been a signicant shift in focus in biomedical information extraction from simple pairwise relations representing associations such as protein protein interactions ppi toward representations that capture typed structured associations of arbitrary numbers of entities in specic roles frequently termed event extraction
as event extraction methods are based on supervised machine learning or hand crafted rules written with reference to the annotated data it is necessary not only to have an inventory of concepts that largely covers the ways in which genes proteins can be associated but also to be able to estimate the relative frequency with which these concepts are used to express gene protein associations
results we apply a focused selection picking king only those individual sentences in which at least one named entity has been tagged
we apply a focused selection picking only those individual sentences in which at least one named entity has been tagged
we apply a focused selection picking only those individual sentences in which at least one named entity has been tagged
we apply a probabilistic approach example kumar coexistence in the same patient is not uncommon as vitiligo has been reported to occur in
of patients of aa and is about times more common in patients with aa than in the general population
their ization over we present a case of a year old male child who had vitiligo and later developed aa over the existing lesions of vitiligo over face and scalp and have attempted to elucidate the current understanding of mechanisms of coexistence of these two diseases
a year old boy presented to the skin outpatient department with history of depigmented areas on the scalp face neck arms and legs for years
he also gave a history of development of patchy loss of hair over some of these lesions for years
there was no previous history of any trauma or medications
family history was not relevant
on examination there were depigmented macules over the scalp forehead eyebrows eyebrows perioral preauricular regions neck elbows hands feet shins nose chin hands knees and feet
patches of hair loss were seen limited to some of these depigmented areas over the vertex and occipital region of the scalp and eyebrows figure
other body areas were not affected by patchy hair loss
clinically the diagnosis of vitiligo with aa was made


additionally the basal layer of the epidermis was almost devoid of pigment figure conrming the diagnosis of vitiligo over the same site


both aa and vitiligo are clubbed under the spectrum of autoimmune ders


our case lends support to the hypothesis that aa and vitiligo share a common pathogenic pathway including autoimmune antigens response against some common antigens like those derived from the bulb melanocytes
melanocytes derived released during vitiligo pathogenesis could act as auto antigens not only for vitiligo but also for aa and autoimmune th cells against them could also trigger a response against the hair follicle melanocytes thus pre disposing to aa


text omitted

example original abstract both alopecia areata aa and vitiligo are autoimmune diseases and their coexistence in the same patient is not uncommon as vitiligo has been reported to occur in
of patients of aa
we present a case of a year old male child who had vitiligo and later developed aa over the existing lesions of vitiligo over face and scalp and have attempted to elucidate the current understanding of mechanisms of coexistence of these two diseases
our case lends support to the hypothesis that aa and vitiligo share a common pathogenic pathway including autoimmune response against some common antigens like those derived from the bulb melanocytes
stimulation of proinammatory t cell mediated immunological response or inactivation of a suppressor t cell mediated response could be the common underlying phenomenon
however the striking rarity of colocalization of these two diseases has led to the recent debate over current understanding of their pathogenesis and whether this association is merely a coincidence
as both aa and vitiligo are frequent and chronic dermatological disorders it is of utmost importance to gain more understanding into their pathogenic mechanisms so that more denitive treatment modalities may be devised and the quality of life of these patients can be improved
example fconv alopecia areata aa and vitiligo are autoimmune diseases and their coexistence in the same patient is not uncommon as vitiligo has been reported to occur in
of patients of aa and is about times more common in patients with aa than in the general population
we present a case of a year old male child who had vitiligo and later developed aa over the scalp forehead eyebrows eyebrows perioral preauricular regions nose and chin depigmented macules over the scalp forehead eyebrows eyebrows perioral preauricular regions nose and chin depigmented macules over the scalp forehead eyebrows periorbital perioral preauricular regions nose and chin depigmented macules over the scalp forehead eyebrows periorbital perioral preauricular regions nose and chin depigmented macules over the scalp forehead eyebrows periorbital however peptide
abstractive text summarization using b
sequence to sequence rnns and beyond
arxiv preprint

nenkova a
maskey s
and liu y

automatic summarization
in proceedings of the annual ing of the association for computational linguistics tutorial abstracts of acl page
association for computational linguistics
page l
brin s
motwani r
and winograd t

the pagerank citation ranking bringing order to the web
technical report stanford infolab
pyysalo s
ohta t
and tsujii j

an analysis of gene protein associations at pubmed scale
journal of biomedical semantics
rossiello g
basile p
and semeraro g
radev d
r
jing h
stys m
and tam d

centroid based summarization of multiple documents
information processing management

centroid based text summarization through tionality of word embeddings
multiling page
rush a
m
chopra s
and weston j

a neural attention model for abstractive sentence summarization
arxiv preprint

sennrich r
haddow b
and birch a

neural machine translation of rare words with subword units
arxiv preprint

sutskever i
vinyals o
and le q
v

sequence to sequence learning with neural networks
in advances in neural information processing systems pages
suzuki j
and nagata m

cutting off redundant repeating generations for neural abstractive tion
in proceedings of the conference of the pean chapter of the association for computational guistics volume short papers volume pages
teufel s
and moens m

summarizing scientic articles experiments with relevance and rhetorical tus
computational linguistics
papers
arxiv preprint

denkowski m
and lavie a

meteor universal language specic translation evaluation for any target language
in proceedings of the eacl workshop on statistical machine translation
dietterich t
g
domingos p
getoor l
muggleton s
and tadepalli p

structured machine learning the next ten years
machine learning
erkan g
and radev d
r

lexrank graph based lexical centrality as salience in text summarization
nal of articial intelligence research
fee m
s

oculomotor learning revisited a model of reinforcement learning in the basal ganglia rating an efference copy of motor actions
frontiers in neural circuits
gehring j
auli m
grangier d
yarats d
and dauphin y
n

convolutional sequence to quence learning
arxiv preprint

giridharan v
v
thandavarayan r
a
arumugam s
mizuno m
nawa h
suzuki k
ko k
m
namurthy p
watanabe k
and konishi t

schisandrin b ameliorates icv infused amyloid induced oxidative stress and neuronal dysfunction through hibiting rage nf b mapk and up regulating hsp beclin expression
plos one
hunter l
and cohen k
b

biomedical language processing what s beyond pubmed molecular cell
kalchbrenner n
espeholt l
simonyan k
van den
time
corr oord a
graves a
and kavukcuoglu k
neural machine translation in linear

kim m
singh m
d
and lee m

wards abstraction from extraction multiple timescale gated recurrent unit for summarization
arxiv preprint

kumar s
mittal j
and mahajan b

tion of vitiligo and alopecia areata coincidence or sequence international journal of trichology
kusner m
j
sun y
kolkin n
i
weinberger k
q
et al

from word embeddings to document tances
in icml volume pages
lecun y
bottou l
bengio y
and haffner p

gradient based learning applied to document tion
proceedings of the ieee nov
lee j
cho k
and hofmann t

fully level neural machine translation without explicit tation
arxiv preprint

lin c


rouge a package for automatic tion of summaries
in text summarization branches out proceedings of the workshop volume
lloret e
roma ferri m
t
and palomar m

compendium a text summarization system for ating abstracts of research papers
data knowledge engineering
mikolov t
chen k
corrado g
and dean j

efcient estimation of word representations in vector space
arxiv preprint

nallapati r
zhou b
c
and xiang
