l u j v c
s c v
v i x r a query focused video summarization dataset evaluation and a memory network based approach aidean sharghi jacob s
laurel and boqing gong center for research in computer vision university of central florida orlando fl department of computer science university of alabama at birmingham al aidean

ucf
edu
edu
ucf
edu abstract recent years have witnessed a resurgence of interest in video summarization
however one of the main obstacles to the research on video summarization is the user subjectivity users have various preferences over the summaries
the subjectiveness causes at least two problems
first no single video summarizer ts all users unless it interacts with and adapts to the individual users
second it is very challenging to evaluate the performance of a video summarizer
to tackle the rst problem we explore the recently posed query focused video summarization which introduces user preferences in the form of text queries about the video into the summarization process
we propose a memory work parameterized sequential determinantal point process in order to attend the user query onto different video frames and shots
to address the second challenge we contend that a good evaluation metric for video summarization should focus on the semantic information that humans can perceive rather than the visual features or temporal overlaps
to this end we collect dense per video shot concept tions compile a new dataset and suggest an efcient uation method dened upon the concept annotations
we conduct extensive experiments contrasting our video marizer to existing ones and present detailed analyses about the dataset and the new evaluation method

introduction recent years have witnessed a resurgence of interest in video summarization probably due to the overwhelming video volumes showing up in our daily life
indeed both consumers and professionals have the access to ubiquitous video acquisition devices nowadays
while the video data is a great asset for information extraction and knowledge discovery due to its size and variability it is extremely hard for users to monitor or nd the occurrences in it
jacob s
laurel contributed to this work while he was an nsf reu student at ucf thanks to the support of nsf cns
intelligent video summarization algorithms allow us to quickly browse a lengthy video by capturing the essence and removing redundant information
early video rization methods were built mainly upon basic visual ities e

low level appearance and motion features while recently more abstract and higher level cues are leveraged in the summarization works
however one of the main obstacles to the research on video summarization is the user subjectivity users have various preferences over the summaries they would like to watch
the subjectiveness causes at least two problems
first no single video summarizer ts all users unless it teracts with and adapts to the users
second it is very lenging to evaluate the performance of a video summarizer
in an attempt to solve the rst problem we have ied a new video summarization mechanism query focused video summarization that introduces user preferences in the form of text queries about the video into the rization process
while this may be a promising direction to personalize video summarizers the experimental study in was conducted on the datasets originally collected for the conventional generic video summarization
it remains unclear whether the real users would generate tinct summaries for different queries and if yes how much the query focused summaries differ from each other
in this paper we explore more thoroughly the focused video summarization and build a new dataset ticularly designed for it
while we collect the user tions we meet the challenge how to dene a good tion metric to contrast system generated summaries to user labeled ones the second problem above mentioned due to the user subjectivity about the video summaries
we contend that the pursuit of new algorithms for video summarization has actually left one of the basic problems underexplored i
e
how to benchmark different video marizers
user study is too time consuming to compare different approaches and their variations at large in the prior arts of automating the evaluation scale
figure comparing the semantic information captured by captions in and by the concept tags we collected
cedure on one end a system generated summary has to consist of exactly the same key units frame or shot as in the user summaries in order to be counted as a good one
on the other end pixels and low level features are used to compare the system and user maries whereas it is unclear what features and distance metrics match users criteria
some works strive to nd a balance between the two extremes e

using the temporal overlap between two summaries to dene the evaluation metrics
however all such metrics are derived from either the temporal or visual representations of the videos without explicitly encoding how humans perceive the information after all the tem generated summaries are meant to deliver similar mation to the users as those directly labeled by the users
in terms of dening a better measure that closely tracks what humans can perceive from the video summaries we share the same opinion as yeung et al
it is key to evaluate how well a system summary is able to retain the semantic information as opposed to the visual quantities of the user supplied video summaries
arguably the semantic information is best expressed by the concepts that represent the fundamental characteristics of what we see in the video at multiple grains with the focus on different areas and from a variety of perspectives e

objects places people actions and their ner grained entities

therefore as our rst contribution we collect dense video shot concept annotations for our dataset
in other words we represent the semantic information in each video shot by a binary semantic vector in which the indicate the presence of corresponding concepts in the shot
we suggest a new evaluation metric for the query focused and generic video summarization based on these semantic tor representations of the video
in addition we propose a memory network terized sequential determinantal point process for ling the query focused video summarization
unlike the hierarchical model in our approach does not rely on the costly user supervision about which queried concept pears in which video shot or any pre trained concept tectors
instead we use the memory network to implicitly attend the user query about the video onto different frames within each shot
extensive experiments verify the tiveness of our approach
the rest of the paper is organized as follows
we discuss some related works in section
section elaborates the process of compiling the dataset acquiring annotations as well as a new evaluation metric for video summarization
in section we describe our novel query focused rization model followed by detailed experimental setup and quantitative results in sections

related work we discuss some related works in this section
this work extends our previous efforts on alizing video summarizers
both works explore the the dataset and the code of the new evaluation metric are publicly available at
aidean sharghi
com
caption i walked around my bedroomdense tags chaircomputerroomdeskofficecaption i drove the car in trafficdense tags skystreetbuildinghandscartreewindowcaption i waited in line with my frienddense tags ladyfoodmendrinkhandshatcomputermarketbuildingdeskcaption i looked at my phonedense tags facecomputermenphonehandschairroomdeskhall figure the frequencies of concepts showing up in the video shots counted for each video separately
focused video summarization but we study this problem more thoroughly in this paper through a new dataset with dense per video shot tagging of concepts
our memory work based video summarizer requires less supervision for training than the hierarchical model in
unlike our user annotated semantic vectors for the video shots yeung et al
asked annotators to caption each video shot using a sentence
a single sentence targets only limited information in a video shot and misses many details
figure contrasts the concept annotations in our dataset with the captions for a few video shots
the concept notations clearly provide a more comprehensive coverage about the semantic information in the shots
memory networks are versatile in modeling the attention scheme in neural networks
they are widely used to address question answering and visual question answering
the query focusing in our marization task is analogous to attending questions to the facts in the previous works but the facts in our context are temporal video sequences
moreover we lay a tial determinantal point process on top of the memory network in order to promote diversity in the summaries
a determinantal point process dpp denes a tribution over the power sets of a ground set that ages diversity among items of the subsets
there have been growing interest in dpp in machine learning and computer vision
our model in this paper extends dpps modeling capabilities through the memory neural network

dataset in this section we provide the details on compiling a comprehensive dataset for video summarization
we opt to build upon the currently existing ut egocentric ute dataset mainly for two reasons the videos are sumer grade captured in uncontrolled everyday scenarios and each video is hours long and contains a diverse set of events making video summarization a naturally sirable yet challenging task
in what follows we rst plain how we dene a dictionary of concepts and determine the best queries over all possibilities for the query focused video summarization
then we describe the procedure of gathering user summaries for the queries
we also show informative statistics about the collected dataset


concept dictionary and queries we plan to have annotators to transform the semantic formation in each video shot to a binary semantic vector
figures and with s indicating the presence of the responding concepts and the absence
such annotations serve as the foundation for an efcient and automatic uation method for video summarization described in tion


the key is thus to have a dictionary that covers a wide range and multiple levels of concepts in order to have the right basis to encode the semantic information
in we have constructed a lexicon of concepts by overlapping nouns in the video shot captions with those in the sentibank
those nouns serve as a great starting point for us since they are mostly entry level words
we prune out the concepts that are weakly related to visual content e

area which could be interpreted in ous ways and applicable to most situations
additionally we merge the redundant concepts such as children and kids
we also add some new concepts in order to struct an expressive and comprehensive dictionary
two strategies are employed to nd the new concept candidates
first after watching the videos we manually add the cepts that appear for a signicant frequency e

puter
second we use the publicly available statistics about youtube and vine search terms to add the terms that are frequently searched by users e

pet animal
the nal lexicon is a concise and diverse set of concepts
figure that are deemed to be comprehensive for the ute videos of daily lives
figure all annotators agree with each other on the prominent concepts in the video shot while they miss different subtle concepts
figure two summaries generated by the same user for the queries hat phone and food drink respectively
the shots in the two summaries beside the green bars exactly match each others while the orange bars show the query specic shots
we also construct queries to acquire query focused user summaries using two or three concepts as opposed to gletons
imagine a use case of video search engines
the queries entered by users are often more than one word
for each video we formalize queries
they cover the ing four distinct scenarios i all the concepts in the query appear in the same video shots together such queries all concepts appear in the video but never jointly in a single shot queries only one of the concepts stituting the query appears in some shots of the video queries and none of the concepts in the query are present in the video such query
we describe in the suppl
materials how we obtain the queries to cover the four scenarios
such queries and their user annotated maries challenge an intelligent video summarizer from ferent aspects and extents


collecting user annotations we plan to build a video summarization dataset that fers efcient and automatic evaluation metrics and user summaries in response to different queries about the videos
for the former we collect user annotations about the presence absence of concepts in each video shot
this is a quite daunting task conditioning on the lengths of the videos and the size of our concept dictionary
we use zon mechanical turk mturk
mturk
for economy and efciency considerations
for the latter we hire three student volunteers to have better quality control over the labeled video summaries
we formly partition the videos to second long shots


shot tagging visual content to semantic vector we ask mturkers to tag each video shot with all the cepts that are present in it
to save the workers time from watching the shots we uniformly extract ve frames from each shot
a concept is assumed relevant to the shot as long as it is found in any of the ve frames
figure trates the tagging results for the same shot by three ent workers
while all the workers captured the prominent concepts like sky lady street tree and car they missed different subtle ones
the union of all their tations however provides a more comprehensive tic description about the video shot than that of any vidual annotator
hence we ask three workers to annotate each shot and take their union to obtain the nal semantic vector for the shot
on average we have acquired


and
concepts per shot for the four ute in sharp contrast the automatically videos respectively
derived concepts from the shot captions are far from enough on average there are only


and
concepts respectively associated with each shot of the four videos
evaluating video summaries
thanks to the dense cept annotations per video shot we can conveniently trast a system generated video summary to user summaries according to the semantic information they entail
we rst dene a similarity function between any two video shots by intersection over union iou of their ing concepts
for instance if one shot is tagged by car street and another by street tree sign then the iou similarity between them is

to nd the match between two summaries it is nient to execute it by the maximum weight matching of a bipartite graph where the summaries are on opposite sides of the graph
the number of matched pairs thus enables us to compute precision recall and score
although this procedure has been used in the previous work there the edge weights are calculated by low level visual features which by no means match the semantic information humans obtain from the videos
in sharp contrast we use the iou user sky lady street market building hands tree car windowuser sky lady street hands tree car hat auser sky lady street tree car hat windowfooddrinkphonehat table inter user agreement evaluated by score and the three student volunteers o the oracle summary
table the average lengths and standard deviations of the maries for different queries
o o o user user user oracle





similarities dened directly over the user annotated tic vectors as the edge weights


































acquiring user summaries in addition to the dense per video shot concept tagging we also ask annotators to label query focused video summaries for the queries described in section

to ensure consistency in the summaries and better ity control over the summarization process we switch from mturk to three student volunteers in our university
we meet and train the volunteers in person
they each marize all four videos by taking queries into account an annotator receives videos queries summarization tasks in total
we thus obtain three user summaries for each query video pair
however we acknowledge that it is infeasible to have the annotators to summarize all the query video pairs from scratch the ute videos are each hours long
to overcome this issue we expand each temporal video to a set of static key frames
first we uniformly extract ve key frames to represent each shot in the same way as in tion


second we pool all the shots corresponding to the three textual summaries as the initial candidate set
third for each query we further include all the shots that are relevant to it into the set
a shot is relevant to the query if the intersection of the concepts associated with it and the query is nonempty
as a result we have a set of candidate shots for each query that covers the main story in the video as well as those of relevance to the query
the annotators summarize the video by removing redundant shots from the set
there are to shots in the candidate sets and the summaries labeled by the participants contain only shots on average
oracle summaries
supervised video summarization methods often learn from one summary per video or per query video pair in query focused rization while we have three user generated summaries per query
we aggregate them into one called the oracle mary per query video pair by a greedy algorithm
the gorithm starts from the common shots in the three user maries
it then greedily chooses one shot every time such that this shot gives rise to the largest marginal gain over the evaluated score
we leave the details to the suppl
rials
the oracle summaries achieve better agreements with the users than the inter user consensus
table
summaries of the same video differ due to queries
figure shows two summaries labeled by the same user for two distinct queries hat phone and food drink
note that the summaries both track the main events pening in the video while they differ in the query specic parts
besides table reports the means and standard tions of the lengths of the summaries per video per user
we can see that the queries highly inuence the resulting maries the large standard deviations attribute to the queries
budgeted summary
for all the summaries thus far we do not impose any constraints over the total number of shots to be included into the summaries
after we receive the notations however we let the same participants further duce the lengths of their summaries to respectively shots and shots
we call them budgeted summaries and leave them for future research

approach we elaborate our approach to the query focused video summarization in this section
denote by v a video that is partitioned to t segments and by q the query about the video
in our experiments every segment vt sists of video shots each of which is second long and is used in section
to collect the concept annotations


query conditioned sequential dpp the sequential determinantal point process dpp is among the state of the art models for generic video rization
we condition it on the query q as our overarching video summarization model p yt yt q t p q p yt q where the t th dpp variable yt selects subsets from the segment vt i
e
vt and the distribution p yt q is specied by a conditional dpp p yt q
det it the nominator on the right hand side is the principle minor of the l ensemble kernel matrix indexed by the sets yt
the denominator calculates the determinant figure our query focused video summarizer memory network right parameterized sequential determinantal point process left
of the sum of the kernel matrix and a corrupted identity trix whose elements indexed by are
readers are referred to the great tutorial on dpp for more details
note that the dpp kernel is parameterized by the query q
we have to carefully devise the way of terizing it in order to take account of the following ties
in query focused video summarization a user selects a shot to the summary for two possible reasons
one is that the shot is quite related to the query and thus becomes pealing to the user
the other may attribute to the tual importance of the shot

the user would probably choose a shot to represent a prominent event in the video even if the event is not quite relevant to the query
to this end we use a memory network to model the two types of importance query related and contextual of a video shot simultaneously


memory network to parameterize dpp kernels the memory network offers a neural network tecture to naturally attend a question to facts
the most panel of figure
in our work we shall measure the relevance between the query q and a video shot and rate such information into the dpp kernel
therefore it is straightforward to substitute the question in memory network by our query but the facts are less obvious
as discussed in section
there could be various narios for a query and a shot
all the query concepts may appear in the shot but possibly in different frames one or two concepts of the query may not be present in the shot it is also possible that none of the concepts are relevant to any frame in the shot
in other words the memory network is supposed to screen all the video frames in order to mine the shot s relevance to the query
hence we uniformly sample frames from each shot as the facts
the video frames are represented using the same feature as
fk on the rightmost panel of figure
the memory network takes as input the video frames fk of a shot and a query q
the frames are transformed to memory vectors mk through an embedding matrix a
similarly the query q represented by a binary indication vector is mapped to the internal state u using an embedding matrix c
the attention scheme is implemented simply by a dot product followed by a softmax function pk mk where pk carries how much attention the query q incurred over the frame fk
equipped with the attention scores pk we assemble another embedding ck of the frames obtained by the mapping matrix b in gure into the video shot sentation o o pick which is conditioned on the query q and entails the vance strength of the shot to the query
as a result we pect the dpp kernel parameterized by the following ot i dt doj is also exible in modeling the importance of the shots to be selected into the video summary
here i and j index two shots and d is another embedding matrix
note that the contextual importance of a shot can be inferred from a shot s similarities to the others by the kernel matrix while the query related importance is mainly by the attention scheme in the memory network


learning and inference we learn the overall video summarizer including the quential dpp and the memory network by maximizing the log likelihood of the user summaries in the training set
we use stochastic gradient descent with mini batching to mize the embedding matrices a b c d
the learning rates and numbers of epochs are chosen using the tion set
at the test stage we sequentially visit the video segments vt and select shots from them using the learned summarization model
it is notable that our approach requires less user tions than the sh dpp
it learns directly from the user summaries and implicitly attend the queries to the video shots
however sh dpp requires very costly annotations about the relevances between video shots and queries
our new dataset does supply such supervisions so we shall clude sh dpp as the baseline method in our experiments

experimental results we report experimental setup and results in this section
features
we extract the same type of features as used in the existing sh dpp method in order to have fair features of shot networkfeatures of shot nmemory networkfeatures of shot imemory networkfeatures of shot q q conditional
bembedding aqueryembedding cinner table comparison results for query focused video summarization
seqdpp sh dpp ours precision recall precision recall precision recall avg













































comparisons
first we employ concept detectors from sentibank and use the detection scores for the features of each key frame key frames per second long shot
however it is worth mentioning that our approach is not limited to using concept detection scores and more tantly unlike sh dpp does not rely on the per shot tions about the relevance to the query the shot user labeled semantic vectors serve for evaluation purpose only
additionally we extract a six dimensional contextual ture vector per shot as the mean correlations of low level features including color histogram gist lbp bag of words as well as an attribute feature in a poral window whose size varies from to shots
the six dimensional contextual features are appended to the key frame features in our experiments
data split
we run four rounds of experiments each ing one video out for testing and one for validation while keeping the remaining two for training
since our video summarizer and the baselines are sequential models the small number i
e
two of training videos is not an issue as the videos are extremely long providing many variations and supervisions at the training stage


comparison results query focused video summarization
we contrast our video summarizer the memory network based sequential determinantal point process to several closely related ods
we rst include sh dpp the most recent proach to the query focused video summarization
our model improves upon seqdpp by taking the query into account and parameterizing the dpp kernel by the memory network
seqdpp is thus directly comparable to ours
we concatenate the query features binary indication vectors with the shot features and input them to seqdpp and dpp
we set the same dimensionality for all the embedding spaces in our and the two baseline methods
it turns out the embeddings are chosen due to their performances on the validation videos
table compares the performances of the three video summarizers
each video is taken in turn as the test video and the corresponding results are shown in each row
the average results are included as the last row
precision figure the effectiveness of various individual components in our proposed video summarizer
call and score are reported for all the video rizers
our approach outperforms the other two by a large margin more than score on average
it seems like video is especially challenging for all the methods
for video our summarizer generates a little longer summaries than the others do
in the future work we will explore how to control the summary length in the sequential dpp model
component wise analyses
to investigate how each component in our framework contributes to the nal results we conduct more experiments by either removing or fying them
figure shows the corresponding results
the main benet from the memory network is the tion mechanism
equation
if we instead use a form distribution for the attention scores pi and append the query information u directly after the memory network output o the results become worse on all the four videos cf
noattention in figure
the noembd results are tained after we remove the last embedding matrix d when we compute the dpp kernels
finally embsize are the results when we change the embeddings in our approach to
the performance drops from our plete model verify that all the corresponding components are complementary jointly contributing to the nal results
generic video summarization
recall that our queries incur four different scenarios
section

when there are no video shots relevant to the query it reduces to the generic video summarization in some sort
we single out such queries and contrast our summarizer to some ing and recent methods for generic video summarization scoresnoattentionnoembdembsize complete model table comparison results for generic video summarization i
e
when no video shots are relevant to the query
submod quasi ours precision recall precision recall precision recall avg













































figure a nice behavior of our evaluation metric
when we randomly remove video shots from the user summaries the recall between the original user summaries and the corrupted ones decreases almost linearly
the evaluation by rouge is included for reference
submod which employs submodular functions to courage diversity and quasi which is an unsupervised method based on group sparse coding
unlike the dpp type of summarizers the baseline methods here are not able to automatically determine the lengths of the summaries
we tune the threshold parameter in quasi such that the output lengths are no more or less than the oracle summary by shots
for submod we set the budget parameter such that it generates summaries that are exactly as long as the oracle summaries
as shown in table our approach still gives the best overall performance even though we reveal the acle sumamries lengths to the baseline methods probably due to its higher neural network based modeling capacity


a nice behavior of our evaluation metric our evaluation method for video summarization is mainly motivated by yeung et al

particularly we share the same opinion that the evaluation should focus on the semantic information which humans can perceive rather than the low level visual features or temporal laps
however the captions used in are diverse making the rouge evaluation unstable and poorly correlated with human judgments and often missing subtle details cf
figure for some examples
we rectify those caveats by instead collecting dense cept annotations
figure exhibits a few video shots where the concepts we collected provide a better coverage than the captions about the semantics in the shots
moreover we conveniently dene an evaluation metric based on the iou similarity function between any two shots
section

thanks to the concept annotations
our evaluation metric has some nice behaviors
if we randomly remove some video shots from the user maries and compare the corrupted summaries with the nal ones an accuracy like metric should give rise to linearly decreasing values
this is indeed what happens to our recall as shown in figure
in contrast the rouge recall taking as input the shot captions exhibits some ality
more results on randomly replacing some shots in the user summaries are included in the suppl
materials

conclusion in this work our central theme is to study the tiveness in video summarization
we have analyzed the key challenges caused the subjectiveness and proposed some lutions
in particular we compiled a dataset that is densely annotated with a comprehensive set of concepts and signed a novel evaluation metric that benets from the lected annotations
we also devised a new approach to erating personalized summaries by taking user queries into account
we employed memory networks and tal point processes in our summarizer so that our model leverages their attention schemes and diversity modeling pabilities respectively
extensive experiments verify the fectiveness of our approach and reveals some nice behaviors of our evaluation metric
acknowledgements
this work is supported by nsf iis a gift from adobe systems and a gpu from nvidia
we thank fei sha the anonymous reviewers and area chairs especially for their insightful suggestions




over unionrouge



over unionrouge

















scoreintersection over unionrouge













references r
h
affandi e
b
fox r
p
adams and b
taskar
ing the parameters of determinantal point process kernels
in icml pages
a
agarwal a
choromanska and k
choromanski
notes on using determinantal point processes for arxiv preprint ing with applications to text clustering


s
antol a
agrawal j
lu m
mitchell d
batra c
lawrence zitnick and d
parikh
vqa visual question answering
in proceedings of the ieee international ference on computer vision pages
d
bahdanau k
cho and y
bengio
neural machine translation by jointly learning to align and translate
arxiv preprint

n
k
batmanghelich g
quon a
kulesza m
kellis p
golland and l
bornn
diversifying sparsity using variational determinantal point processes
arxiv preprint

d
borth t
chen r
ji and s

chang
sentibank large scale ontology and classiers for detecting sentiment and emotions in visual content
in proceedings of the acm international conference on multimedia pages
acm
w

chao b
gong k
grauman and f
sha
margin determinantal point processes
uai
x
chen h
fang t

lin r
vedantam s
gupta p
dollar and c
l
zitnick
microsoft coco captions arxiv preprint data collection and evaluation server


w

chu y
song and a
video jaimes
video summarization by visual summarization occurrence
in proceedings of the ieee conference on puter vision and pattern recognition pages
s
e
f
de avila a
p
b
lopes a
da luz and a
querque araujo
vsumm a mechanism designed to produce static video summaries and a novel evaluation method
tern recognition letters
m
gartrell u
paquet and n
koenigstein
low rank torization of determinantal point processes for tion
arxiv preprint

j
a
gillenwater a
kulesza e
fox and b
taskar
expectation maximization for learning determinantal point in advances in neural information processing processes
systems pages
d
b
goldman b
curless d
salesin and s
m
seitz
schematic storyboarding for video visualization and editing
in acm transactions on graphics tog volume pages
acm
b
gong w

chao k
grauman and f
sha
diverse sequential subset selection for supervised video tion
in advances in neural information processing systems pages
m
gygli h
grabner h
riemenschneider and l
van gool
creating summaries from user videos
in european ence on computer vision pages
springer
m
gygli h
grabner and l
van gool
video tion by learning submodular mixtures of objectives
in ceedings of the ieee conference on computer vision and pattern recognition pages
a
khosla r
hamid c

lin and n
sundaresan
scale video summarization using web image priors
in ceedings of the ieee conference on computer vision and pattern recognition pages
g
kim l
sigal and e
p
xing
joint summarization of large scale collections of web images and videos for line reconstruction
in proceedings of the ieee conference on computer vision and pattern recognition pages
a
kulesza and b
taskar
k dpps fixed size in proceedings of the nantal point processes
national conference on machine learning icml pages
a
kulesza and b
taskar
learning determinantal point cesses

a
kulesza and b
taskar
determinantal point processes for machine learning
arxiv preprint

j
t
kwok and r
p
adams
priors for diversity in generative latent variable models
in advances in neural information processing systems pages
j
kwon and k
m
lee
a unied framework for event marization and rare event detection
in cvpr pages
r
r
bacco a
hocevar p
lambert g
pas and b
e
ionescu
video summarization from spatio temporal in proceedings of the acm trecvid video features
summarization workshop pages
acm
y
j
lee j
ghosh and k
grauman
discovering important people and objects for egocentric video summarization
in cvpr volume page
y
j
lee and k
grauman
predicting important objects for international journal of egocentric video summarization
computer vision
c
li s
jegelka and s
sra
fast dpp sampling for arxiv preprint om with application to kernel methods


t
liu and j
r
kender
optimization algorithms for the selection of key frame sequences of variable length
in european conference on computer vision pages
springer
z
lu and k
grauman
story driven summarization for in proceedings of the ieee conference egocentric video
on computer vision and pattern recognition pages
z
mariet and s
sra
fixed point algorithms for learning terminantal point processes
advances in neural information systems nips
z
mariet and s
sra
kronecker determinantal point cesses
arxiv preprint

t
ojala m
pietikainen and t
maenpaa
multiresolution gray scale and rotation invariant texture classication with local binary patterns
ieee transactions on pattern analysis and machine intelligence
recognition
in proceedings of the ieee conference on puter vision and pattern recognition pages
k
zhang w

chao f
sha and k
grauman
summary transfer exemplar based subset selection for video rizatio
arxiv preprint

k
zhang w

chao f
sha and k
grauman
video summarization with long short term memory
arxiv preprint

b
zhao and e
p
xing
quasi real time summarization for in proceedings of the ieee conference consumer videos
on computer vision and pattern recognition pages
a
oliva and a
torralba
modeling the shape of the scene a holistic representation of the spatial envelope
international journal of computer vision
v
ordonez j
deng y
choi a
c
berg and t
l
berg
from large scale image categorization to entry level gories
in proceedings of the ieee international conference on computer vision pages
d
potapov m
douze z
harchaoui and c
schmid
category specic video summarization
in eccv ropean conference on computer vision
a
rav acha y
pritch and s
peleg
making a long video in ieee computer short dynamic video synopsis
society conference on computer vision and pattern nition volume pages
ieee
a
sharghi b
gong and m
shah
query focused extractive video summarization
in european conference on computer vision pages
springer
j
snoek r
zemel and r
p
adams
a determinantal point process latent variable model for inhibition in neural spiking data
in advances in neural information processing systems pages
y
song j
vallmitjana a
stent and a
jaimes
tvsum summarizing web videos using titles
in proceedings of the ieee conference on computer vision and pattern tion pages
s
sukhbaatar j
weston r
fergus et al
end to end ory networks
in advances in neural information processing systems pages
j
weston a
bordes s
chopra a
m
rush b
van merrienboer a
joulin and t
mikolov
towards complete question answering a set of prerequisite toy tasks
arxiv preprint

j
weston s
chopra and a
bordes
memory networks
arxiv preprint

w
wolf
key frame selection by motion analysis
in tics speech and signal processing

ference proceedings
ieee international conference on volume pages
ieee
b
xiong and k
grauman
detecting snap points in tric video with a web photo prior
in european conference on computer vision pages
springer
c
xiong s
merity and r
socher
dynamic memory networks for visual and textual question answering
arxiv preprint

j
xu l
mukherjee y
li j
warner j
m
rehg and v
singh
gaze enabled egocentric video summarization via constrained submodular maximization
in proceedings of the ieee conference on computer vision and pattern tion pages
t
yao t
mei and y
rui
highlight detection with pairwise deep ranking for rst person video summarization
s
yeung a
fathi and l
fei fei
videoset video summary evaluation through text
arxiv preprint

f
x
yu l
cao r
s
feris j
r
smith and s

chang
designing category level attributes for discriminative visual
