n a j g l
s c v
v i x r a deep transfer reinforcement learning for text summarization yaser keneshloo discovery analytics center virginia tech
edu naren ramakrishnan discovery analytics center virginia tech
edu chandan k
reddy discovery analytics center virginia tech
vt
edu abstract deep neural networks are data hungry models and thus face diculties when attempting to train on small text datasets
transfer learning is a potential solution but their eectiveness in the text domain is not as explored as in areas such as image analysis
in this paper we study the problem of transfer learning for text summarization and discuss why existing state of the art models fail to generalize well on other unseen datasets
we propose a reinforcement learning framework based on a self critic policy gradient approach which achieves good generalization and state the art results on a variety of datasets
through an extensive set of experiments we also show the ability of our proposed framework to ne tune the text summarization model using only a few training samples
to the best of our knowledge this is the rst work that studies transfer learning in text summarization and provides a generic solution that works well on unseen data
keywords transfer learning text summarization self critic reinforcement learning
introduction text summarization is the process of summarizing a long document into few sentences that capture the in recent years essence of the whole document
searchers have used news article datasets

from cnn dm and newsroom as a main resource for building and evaluating text summarization models
however all these models suer from a critical problem a model trained on a specic dataset works well only on that dataset
for instance if a model is trained on the cnn dm dataset and tested on the newsroom dataset the result is much poorer than when it is trained directly on the newsroom dataset
this lack of generalization ability for current state of the art models is the main motivation for our work
this problem arises in situations where there is a need to perform summarization on a specic dataset but either no ground truth summaries exist for this dataset or where collecting ground truth summaries could be expensive and time consuming
thus the only recourse in such a situation would be to simply apply a pre trained summarization model to generate summaries for this data
however as discussed in this paper this approach will fail to satisfy the basic requirements of this task and thus fails to generate high quality summaries
throughout our analysis we work with two of the well known news related datasets for text summarization and one could expect that a model trained on either one of the datasets should perform well on the other or any news related dataset
on the contrary as shown in table the fast rl model trained on cnn dm which holds the state of the art result for text summarization task on the cnn dm test dataset with
a f score according to the measure will reach only a
on this metric on the newsroom test data a performance fall of almost
this observation shows that these models suer from poor generalization capability
in this paper we rst study the extent to which the current state of the art models are vulnerable in alizing to other datasets and discuss how transfer ing could help in alleviating some of these problems
in addition we propose a solution based on ment learning which achieves good generalization formance on a variety of summarization datasets
ditional transfer learning usually works by pre training a model using a large dataset ne tuning it on a get dataset and then testing the result on that get dataset
however our proposed method as shown in fig is able to achieve good results on a variety of datasets by only ne tuning the model on a single dataset
therefore it removes the requirement of ing separate transfer models for each dataset
to the best of our knowledge this is the rst work that ies transfer learning for the problem of text rization and provides a solution for rectifying the eralization issue that arises in current state of the art summarization models
in addition we conduct various experiments to demonstrate the ability of our proposed method to obtain state of the art results on datasets with small amounts of ground truth data
the rest of the paper is organized as follows copyright by siam unauthorized reproduction of this article is prohibited table the pointer generator and fast rl models are trained using the cnn dm dataset and tested on the cnn dm and newsroom datasets
pointer generator rouge cnn dm
newsroom


l

fast rl

l



figure in standard transfer learning settings a model is pre trained on ds all network layers are transferred the model is ne tuned using dg and nally tested only on dg
on the contrary our proposed method transferrl uses dg to create a model that works well on a variety of test datasets
tion describes transfer learning methods and recent research related to this problem
section presents our proposed model for transfer learning in text tion
section shows our experimental results and pares them with various benchmark datasets section concludes our discussion
related work recently there has been a surge in the development of deep learning based methods for building models that have the ability to transfer and generalize to other similar problems
transfer learning tl has been studied in the domain of image processing however its utility in nlp problems is yet to be thoroughly investigated
in this section we will review some of these works

transferring trained models
in these works the underlying model is rst trained on a specic dataset and then used as a pre trained model for another problem or dataset
in this method depending on the underlying model one can transform dierent types of neural network layers from the pre trained model to the transfer model
examples of these transferable layers are the word embedding layer the convolutional layers in a cnn model fully connected fc hidden layers and nally the output layer
yosinski et al
studied the eect of transferring dierent layers of a deep neural network and found that lower level layers learn general features while higher level layers capture mostly the specic characteristic of the problem at hand
researchers have also demonstrated how one can transfer both low level and high level neural layers from a cnn for tl
recently semwal et al
used this idea of ferring various network layers for text classication
aside from transferring the network layers they also experimented with freezing or ne tuning these layers after the transfer and concluded that ne tuning the transfer layers will always provide a better result
over tl has also been studied in the context of the named entity recognition problem
our posed method falls into this category
we not only study the eect of transferring network layers but also pose a new co training model for training text rization models using reinforcement learning techniques

knowledge distillation
knowledge tion refers to a class of techniques that trains a small network by transferring knowledge from a larger work
these techniques are typically used when we quire building models for devices with limited tional resources
usually in these models there is a teacher larger model a student smaller model and the goal is to transfer knowledge from teacher to dent
recently researchers have also used this idea to create models using meta learning few shot ing one shot learning and domain tation mostly for image classication problems
however the eect of these types of models on nlp tasks is yet to be studied or not well studied

building generalized models
recently cann et al
released a challenge called decathlon nlp which aims at solving ten dierent nlp problems with a single unied model
the main intuition hind this model is to comprehend the impact of ferring knowledge from dierent nlp tasks on building a generalized model that works well on every task
though this model outperforms some of the state of art models in specic tasks it fails to even reach line results in tasks like text summarization
we also observe such poor results from other generalized works such as google s framework

text summarization
there is a vast amount of research work on the topic of text summarization ing deep neural networks
these works range from fully extractive methods to completely stractive ones
as one of the earliest works on using neural networks for extractive summarization nallapati et al
proposed a framework that used a ranking technique to extract the most salient copyright by siam unauthorized reproduction of this article is prohibited tence in the input
on the other hand for tive summarization it was rush et al
that for the rst time used attention over a sequence to sequence model for the problem of headline generation
to further improve the performance of these models the pointer generator model was proposed for fully handling out of vocabulary oov words
this model was later improved by using the coverage nism
however all these models suer from a mon problem known as exposure bias which refers to the fact that during training the model is trained by feeding ground truth input at each decoder step while during the test it should rely on its own output to erate the next token
also the training is typically done using cross entropy loss while during test metrics such as rouge or bleu are used to evaluate the model
to tackle this problem researchers suggested various models using scheduled sampling and forcement learning based approaches
recently several authors have investigated methods which try to rst perform extractive summarization by selecting the most salient sentences within a document using a classier and then apply a language model or a paraphrasing model on these selected sentences to obtain the nal abstractive summarization
however none of these models as discussed in this paper and shown in table have the capability to generalize to other datasets and thus only perform well for the specic dataset used as target data during the pre training process
proposed model in this paper we propose various transfer learning methods for the problem of text summarization
for all experiments we consider two dierent datasets ds a source dataset used to train the pre trained model while dg the target dataset is the dataset used to ne tune our pre trained model
following the idea of transferring layers of a pre trained model our rst proposed model transfers dierent layers of a trained model trained using ds and ne tunes them using dg
we then propose another method which uses a novel reinforcement learning rl framework to train the transfer model using training signals received from both ds and dg

transferring network layers
there are ious network layers used in a deep neural network
for instance if the model has a cnn encoder and a lstm decoder the cnn layers and the hidden decoder ers trained on ds could be used to ne tune using dg
moreover the word embedding representation is a key layer in such a model
either we use pre trained word embeddings such as glove during the training of ds or let ds drive the inference of its own word beddings
we can still let the model to ne tune a trained word embedding during the training of such a model
in summary we can transfer the embedding layer convolutional layer if using cnn hidden ers if using lstm and the output layer in a text summarization transfer learning problem
one way to understand the eect of each of these layers is to tune or freeze these layers during model transfer and report the best performing model
however as gested by the best performance is realized when all layers of a pre trained model on ds are transferred and the model is led to ne tune itself using dg
fore we follow the same practice and let the transferred model ne tune all trainable variables in our model
as shown later in the experimental result section this way of transferring network layers provides a strong baseline in text summarization and the performance of our posed reinforced model is close to this baseline
ever one of the main problems with this approach is that not only should source dataset ds should contain a large number of training samples but dg must also have a lot of training samples to be able to ne tune the pre trained model and generalize the distribution of the pre trained model parameters
therefore a ful transfer learning using this method requires a large number of samples both for ds and dg
this could be problematic specically for cases where the target dataset is small and ne tuning a model will cause tting
for these reasons we will propose a model which uses reinforcement learning to ne tune the model only based on the reward that is obtained over the target dataset

transfer reinforcement learning ferrl
in this section we explain our proposed inforcement learning based framework for knowledge transfer in text summarization
the basic ing summarization mechanism used in our work is the pointer generator model


why pointer generator the reason we choose a pointer generator model as the basis of our framework is its ability to handle out of vocabulary oov words which is necessary for transfer learning
note that once a specic vocabulary generated from ds is used to train the pre trained model we can not use a dierent vocabulary set during the ne tuning stage on dg since the indexing of words could change for words in the second
according to our experiments that we use dg instead of dt for the target dataset to avoid confusing this t subscript with time
instance the word is could have index in the rst dataset while it could have index in the second dataset
copyright by siam unauthorized reproduction of this article is prohibited amongst the top k words in the cnn dm and room datasets only k words are common between the two datasets and thus a model trained on each of these datasets will have more than k oovs during the ne tuning step
therefore a framework that is not able to handle these oov words elegantly will strate signicantly poor results after the transfer
one nave approach in resolving this problem could be to use a shared set of vocabulary words between ds and dg
however such a model will still suer from inability to generalize to other datasets with a dierent vocabulary set


pointer generator
as shown in fig a pointer generator model comprises of a series of lstm encoders blue boxes and lstm decoders green boxes
let us consider dataset d dn as a dataset that contains n documents along with their summaries
each document is represented by a series of te words i
e
xte where xt v
each encoder takes the ding of word xt as the input and generates the output state ht
the decoder on the other hand takes the last state from the encoder i
e
hte and starts generating an output of size t te y based on the current state of the decoder st and the truth summary word yt
at each step of decoding j the attention vector j context vector cj and output distribution pvocab can be calculated as follows
wssj vt sof fij j cj pvocab sof cj ijhi where wh and ws are trainable model rameters and is the concatenation operator
in a ple sequence to sequence model with attention we use pvocab to calculate the cross entropy loss
however since pvocab only captures the distribution of words within the vocabulary this will generate a lot of oov words ing the decoding step
a pointer generator model igates this issue by using a switching mechanism which either chooses a word from the vocabulary with a tain probability or from the original document using the attention distribution with a probability of as follows
j wccj wssj wxxj j jpvocab j p ij figure pointer generator w
self critic policy gradient the cross entropy ce loss is calculated as follows
lce log p st x t where shows the training parameters and e
returns the word embedding of a specic token
however as mentioned in section one of the main problems with cross entropy loss is the exposure bias which curs due to the inconsistency between the decoder input during training and test
a model that is trained using only ce loss does not have the generalization power quired for transfer learning since such a model is not trained to generate samples from its own distribution and heavily relies on the ground truth input
thus if the distribution of input data changes which can likely happen during transfer learning on another dataset the trained model will have to essentially re calibrate every transferred layer to achieve a good result on the target dataset
to avoid these problems we propose a reinforcement learning framework which slowly removes the dependency of model training on the ce loss and increases the reliance of the model on its own output


reinforcement learning objective
in rl training the focus is on minimizing the negative pected reward rather than on directly minimizing the ce loss
this allows the framework to not only use the model s output for training itself but also helps in ing the model based on the metric that is used during decoding such as rouge
to achieve this during rl training the following objective is minimized
minimize lrl t t t p where wc wx and are trainable model parameters and if a word xj is oov then pvocab and the model will rely on the attention values to select the right token
once the nal probability is calculated using eq

where t are sample tokens drawn from the output of the policy p i
e
p p t
in practice we usually sample only one sequence of tokens to calculate this expectation
hence the derivative of the copyright by siam unauthorized reproduction of this article is prohibited log p t t lt rl log p t figure the proposed transferrl framework
the encoder and decoder units are shared between the source ds and target datasets dg
above loss function is given as follows
lrl e t p this minimization can be further improved by adding a baseline reward
in text summarization the baseline reward could either come from a separate network called a critic network or it could be the reward from a sequence coming from greedy selection over p in this work we consider the greedy t in summary the objective sequence as the baseline
that we minimize during rl training is as follows

lrl t log p st t where yt represents the greedy selection at time t
this model is also known as a self critic policy gradient approach since the model uses its own greedy output to create the baseline
moreover the model uses the sampled sentence as the target for training rather than the ground truth sentence
therefore given this objective the model focuses on samples that do better than greedy selection during training while penalizing those which do worse than greedy selection


transfer reinforcement learning
though a model trained using eq

does not suer from exposure bias it can still perform poorly in a transfer learning setting
this is mostly due to the fact that the model is still being trained using the tion from the source dataset and once transferred to the target dataset aims to generate samples according to the distribution of the source dataset
therefore we need a model that not only remembers the distribution of the source dataset but also tries to learn and adapt to the distribution of the target dataset
the overall rl based framework that is being proposed in this paper is shown in fig

at each step we select a random mini batch from ds and dg and feed them to the shared encoder units and the decoder starts decoding for each mini batch
once the decoding is completed the model generates a sentence based on greedy selection and another by sampling from the output distribution
finally we calculate the ferrl loss according to eq

and back propagate the error according to the trade o parameter
the thick and thin dashed lines in this plot shows the eect of on the extent to which the model needs to rely on either ds or dg for back propagating the error
let us consider sequences drawn from greedy tion and sampling from the source dataset ds and the target dataset dg as ys g tively
we will dene the transfer loss function using these variables as follows
s yg and ys s log p t yg g where us s st xs ug g st xg and controls the trade o between self critic loss of the samples drawn from the source dataset and from the target dataset
therefore a means that we train the model only using the samples from the source dataset while means that model is trained only using samples from the target dataset
as seen in eq

the decoder state st and the context vector are shared between the source and target samples
moreover we use a shared embedding trained on the source dataset es
for both datasets while the input data given to the encoder i
e
xs and xg come from source and target datasets
in practice the rl objective loss only activates after a good pre trained model is obtained
we follow the same practice and rst pre train the model using the source dataset and then activate the transfer rl loss in eq

by combining this loss with the ce loss from eq

using the parameter as follows
lm ixed lt rl experimental results we performed a range of experiments to understand the dynamics of transfer learning and to investigate best practices for obtaining a generalized model for text summarization
in this section we discuss some of the insights we gained through our experiments
all evaluations are done using rouge and l scores on the test data
all our rouge scores have a condence interval of
as reported by the ocial rouge
similar to multi task learning frameworks such as decanlp we use a measure for comparing the result of transfer learning on various datasets by taking the average score of each measure
org project copyright by siam unauthorized reproduction of this article is prohibited table basic statistics for the datasets used in our experiments
newsroom cnn dm train val test avg
summary sentences avg
words in summary




over these datasets
in addition we also introduce a weighted average score which takes into account the size of each dataset as the weight for averaging the

datasets
we use four widely used datasets in text summarization for our experiments
the rst two datasets are newsroom and cnn daily mail which are used for training our models while the duc and duc datasets are only used to test the generalization capability of each model
table shows some of the basic statistics of these datasets
in all these datasets a news article is accompanied by to written summaries and therefore will cover a wide range of challenges for transfer learning
for instance a model that is trained on the newsroom dataset will most likely generate only one long summary sentence while for the cnn dm dataset the model is required to generate up to four smaller summary sentences
for all experiments we either use newsroom as ds and cnn dm as dg or vice versa

training setup
for each experiment we run our model for epochs during pre training and epochs during the transfer process and an extra epochs for the coverage mechanism
we use a batch size of during training the encoder reads the rst words and the decoder generates a summary with words
both encoder and decoder units have a hidden size of while the embedding dimension is set to and we learn the word embedding during training
for all models we used the top k words in each dataset as the vocabulary and during test we use beam search of size
we use adagrad to optimize all models with an initial learning rate of
during pre training and
during rl and coverage and linearly decrease this learning rate based on the epoch numbers as t epoch
moreover is set to zero at the start of rl training and is increased linearly so that it gets to by the end of training
during rl training we use scheduled sampling with sampling probability equal to the value
we use the framework to build our model

eect of dataset size
we will now discuss some of the insights we gained starting with ing the eect of data size for pre training
according to our experiments as shown in table on average a model trained using the newsroom dataset as the source dataset ds has much better performance than models that use cnn dm as the ds in almost all
this is not a surprising result since deep ral networks are data hungry models and typically work the best when provided with a large number of samples
the rst experiment in table and table uses only the newsroom dataset for training the model and not surprisingly it performs good on this dataset however as discussed earlier its performance on other datasets is poor

common vocabulary
as mentioned in tion one way to avoid excessive oov words in transfer learning between two datasets is to use a common cabulary between ds and dg and train a model using this common vocabulary set
although a model trained using such a vocabulary could perform well on these two datasets it still suers from poor generalization to other unseen datasets
to demonstrate this we combine all articles in cnn dm and newsroom training datasets to create a single unied dataset in table and table and train a model using ce loss in eq

and the common set of vocabulary
the result of this experiment is shown as experiment in table and in table
as shown here by comparing these results to experiment we see that combining these two datasets will decrease the performance on newsroom and test datasets but will increase the mance for cnn dm test data
moreover by ing the generalization ability of this method on and datasets we see that it performs up to worse than the proposed method
this is also witnessed by comparing the average scores and weighted average scores of our proposed model against this model
on erage our method improves up to compared to this method according to the weighted average score

transferring layers
in this experiment we discuss the eect of transferring dierent layers of a pre trained model for transfer learning
in the generator model described in section the embedding matrix the encoder and decoder model parameters are the choices for layers we can use for transfer learning
that due to page limitations some of the results are presented in the arxiv version of the paper


to space constraints we only report the results from this setup and refer the readers to the arxiv version of the paper
copyright by siam unauthorized reproduction of this article is prohibited table results on newsroom cnn dm and test data
ds shows the dataset that is used during pre training and dg is our target dataset
n stands for the newsroom dataset and c denotes the cnn dm dataset
the method column shows whether we use ce loss transferring layers tl or transferrl trl loss during training
we use a coverage mechanism for all experiments
the result from the proposed method is shown with
ds dg method n n n c c ce loss ce loss tl newsroom



rl







cnn dm







rl











rl











rl



table normalized and weighted normalized rouge f scores for table
ds dg method avg
score n n n c c ce loss ce loss tl







rl



weighted avg
score







rl



for this experiment we pre train our model using ds and during transfer learning we replace ds with dg and continue training of the model with ce loss
as shown in tables and this way of transferring network layers provides a strong baseline for comparing the formance of our proposed method
these results show that even a simple transferring of layers could provide enough signals for the model to adapt itself to the new data distribution
however as discussed earlier in tion this way of transfer learning tends to completely forget the pre trained model distribution and entirely changes the nal model distribution toward the dataset used for ne tuning
this eect can be observed in ble by comparing the result of experiments and
as shown in this table after transfer learning the formance drops on the newsroom test dataset from
to
based on while it increases on the cnn dm dataset from
to
based on
however since our proposed method tries to remember the distribution of the pre trained model through the parameter and slowly changes the distribution of the model according to the distribution coming from the target dataset it performs better than simple transfer learning on these two datasets
this is shown by paring the result in experiments and in table which shows that our proposed model performs better than nave transfer learning in all test datasets

eect of zeta
as mentioned in section the trade o between emphasizing the training to samples drawn from ds or dg is controlled by the parameter
to see the eect of on transfer learning we clip the value at
and train a separate model using this objective
basically a
means that we treat the samples coming from source and target datasets equally during training
therefore for these experiments we start at zero and increase it linearly till the end of training but clip the nal value at

table shows the result of this experiment
for simplicity sake we provide the result of our proposed model achieved from not clipping along with these results
by comparing the results from these two setups we can see that on average increasing the value of to
will yield better results than clipping this value at

for instance according to the average and weighted average score there is an increase of
in and rouge l scores when we do not clip the at

by comparing the cnn dm score in this table we see that clipping the value will denitely hurt the performance on dg since the model shows equal attention to the distribution coming from both datasets
on the other hand the surprising component here is that by avoiding clipping the performance on the source dataset also increases

transfer learning on small datasets
as discussed in section transfer learning is good for uations where the goal is to do summarization on a dataset with little or no ground truth summaries
for this purpose we conducted another set of experiments to test our proposed model on transfer learning using and datasets as our target datasets
we have only seen small improvement in the results and hence we omitted them from this section
copyright by siam unauthorized reproduction of this article is prohibited table result of transferrl after clipping at
and
on newsroom cnn dm and test datasets along with the average and weighted average scores
table result of transfer learning using newsroom for pre training and for ne tuning
dg ds n n method tl



rl

newsroom
cnn dm



avg
score weighted avg
score






rl















rl









table result of transfer learning methods using the newsroom dataset for pre training and for ne tuning
the underlined result shows that the improvement from tl is not statistically signicant compared to the proposed model
dg ds n n method tl



rl

i
e
dg
for these experiments we randomly pick of each dataset as our training set as a validation dataset and the rest of the dataset as our test data
this will generate and articles as our training dataset for and respectively
similar to other experiments in this paper we use cnn dm and use and as and newsroom as ds dg during transfer learning
due to the size of these datasets the models are trained only for tions during ne tuning and the best model is selected according to the validation set
tables and depict the results of this experiment
as shown in these tables for when we simply transfer network layers it performs slightly better not statistically higher cording to a condence interval than our proposed model however our proposed model will achieve a far better result on
as shown in these tables the results achieved from ne tuning a pre trained model using these datasets is very close to the ones achieved in table and in the case of the dataset our proposed method in table achieves even better sults than the ones shown in table
this shows the ability of our proposed framework in generalizing to seen datasets
note that unlike these experiments the proposed model in table has no information about the data distribution of and and still performs better on these datasets
table comparing our best performing model with state of the art multi task learning frameworks on the cnn dm dataset and according to the average of rouge and l f scores
the result with is the same as reported in the original paper
decanlp average rouge

proposed model

other generalized models
we also pare the performance of our proposed model against some of the recent methods from multi task learning
in text summarization the decanlp and are two of the most recent frameworks that use multi task learning
following the setup in these works we focus on models that are trained using cnn dm datasets and report the average rouge and l f scores for our best performing model
table compares the result of our proposed approach against these methods
conclusion in this paper we tackled the problem of transfer ing in text summarization
we studied this problem from dierent perspectives through transfer of network layers from a pre trained model to proposing a ment learning framework which borrows insights from a self critic policy gradient strategy and oers a tematic mechanism that creates a trade o between the amount of reliance on the source or target dataset ing training
through an extensive set of experiments we demonstrated the generalization power of the posed model on unseen test datasets and how it can reach state of the art results on such datasets
to the best of our knowledge this is the rst work that studies transfer learning in text summarization and oers a lution that beats state of the art models and generalizes well to unseen datasets
acknowledgments this work was supported in part by the us national science foundation grants and
references cnn dm results are excluded due to space constraints j
ba and r
caruana
do deep nets really need to be the reader can refer to the arxiv version of our paper
deep in nips pages
copyright by siam unauthorized reproduction of this article is prohibited s
bengio o
vinyals n
jaitly and n
shazeer
scheduled sampling for sequence prediction with current neural networks
in nips pages
pages
s
narayan s
b
cohen and m
lapata
ranking tences for extractive summarization with reinforcement learning
in naacl hlt
l
bertinetto j
f
henriques j
valmadre p
torr and a
vedaldi
learning feed forward one shot ers
in nips pages
k
papineni s
roukos t
ward and w

zhu
bleu a method for automatic evaluation of machine translation
in acl pages
y

chen and m
bansal
fast abstractive rization with reinforce selected sentence rewriting
in acl volume pages
r
paulus c
xiong and r
socher
a deep reinforced model for abstractive summarization
arxiv preprint

j
donahue y
jia o
vinyals j
homan n
zhang e
tzeng and t
darrell
decaf a deep convolutional activation feature for generic visual recognition
in icml pages
y
duan m
andrychowicz b
stadie o
j
ho j
schneider i
sutskever p
abbeel and w
zaremba
one shot imitation learning
in nips pages
y
ganin e
ustinova h
ajakan p
germain h
larochelle f
laviolette m
marchand and v
lempitsky
domain adversarial training of neural networks
the journal of machine learning research
s
gehrmann y
deng and a
m
rush
arxiv preprint summarization
up abstractive

m
grusky m
naaman and y
artzi
newsroom a dataset of
million summaries with diverse extractive strategies
in naacl hlt
k
m
hermann t
kocisky e
grefenstette l
holt w
kay m
suleyman and p
blunsom
ing machines to read and comprehend
in nips pages
y
keneshloo t
shi n
ramakrishnan and c
k
reddy
deep reinforcement learning for sequence to sequence models


w
kryscinski r
paulus c
xiong and r
socher
improving abstraction in text summarization
arxiv preprint

z
li x
jiang l
shang and h
li
paraphrase generation with deep reinforcement learning
arxiv preprint

b
y
lin and w
lu
neural adaptation layers for in emnlp cross domain named entity recognition
pages
c

lin
rouge a package for automatic evaluation of summaries
in proc
of workshop on text rization branches out
b
mccann n
s
keskar c
xiong and r
socher
the natural language decathlon multitask learning as question answering


r
nallapati f
zhai and b
zhou
summarunner a recurrent neural network based sequence model for extractive summarization of documents
in aaai pages
r
nallapati b
zhou c
dos santos c
gulcehre and b
xiang
abstractive text summarization using in signll sequence to sequence rnns and beyond
j
pennington r
socher and c
d
manning
glove in emnlp global vectors for word representation
pages
e
m
ponti i
vulic g
glavas n
mrksic and a
korhonen
adversarial propagation and zero shot cross lingual transfer of word vector specialization
in emnlp pages
acl
s
ravi and h
larochelle
optimization as a model for few shot learning
in iclr
a
m
rush s
chopra and j
weston
a neural tention model for abstractive sentence summarization
in emnlp pages
d
s
sachan p
xie and e
p
xing
eective use of bidirectional language modeling for medical named entity recognition
arxiv

a
santoro s
bartunov m
botvinick d
stra and t
lillicrap
meta learning with in icml pages augmented neural networks

a
see p
j
liu and c
d
manning
get to the point summarization with pointer generator networks
in acl volume pages
t
semwal p
yenigalla g
mathur and s
b
nair
a practitioners guide to transfer learning for text classication using convolutional neural networks
in sdm pages
siam
t
shi y
keneshloo n
ramakrishnan and c
k
summarization arxiv preprint reddy
with sequence to sequence models


neural abstractive text j
snell k
swersky and r
zemel
prototypical networks for few shot learning
in nips pages
e
tzeng j
homan k
saenko and t
darrell
versarial discriminative domain adaptation
in cvpr volume page
a
vaswani s
bengio e
brevdo f
chollet a
n
gomez s
gouws l
jones
kaiser n
ner n
parmar al
for neural chine translation


j
yosinski j
clune y
bengio and h
lipson
how transferable are features in deep neural networks in nips pages
q
zhou n
yang f
wei s
huang m
zhou and t
zhao
neural document summarization by jointly learning to score and select sentences
in acl pages
acl
copyright by siam unauthorized reproduction of this article is prohibited supplemental material in this section we provide the full details of our experimental analysis
each table in this document represents a specic table in the main paper
copyright by siam unauthorized reproduction of this article is prohibited dm dg method cov table table in the main paper results on newsroom cnn dm and test data
ds shows the dataset that is used during pre training and dg is our target dataset
n stands for newsroom and c stands for cnn dm dataset
the method column shows whether we use ce loss transferring layers tl or transferrl trl loss during training
for each experiment we run two dierent setups with coverage mechanism and without it
this is represented as we use coverage mechanism for all experiments
the result from the proposed method is shown with
newsroom













cnn dm









































ce loss ce loss ce loss ce loss tl tl ce loss ce loss tl tl rl



























rl









































rl



























no yes no yes no yes no yes no yes no yes no yes rl













n n n n n n n n n n c c c c c c c c c c table table in the main paper normalized and weighted normalized f scores for table
ds dg method cov avg
score n n n c n c n c n c c c c n c n c n c n ce loss ce loss ce loss ce loss tl tl ce loss ce loss tl tl no yes no yes no yes no yes no yes no yes no yes



























rl













weighted avg
score



























rl













table table in the main paper result of transferrl after clipping at
and
on newsroom cnn dm and test data along with the average and weighted average scores
dm dg method cov n n c c n n c c c c n n c c n n







no yes no yes no yes no yes newsroom







rl















cnn dm







rl























rl































rl







copyright by siam unauthorized reproduction of this article is prohibited table table in the main paper normalized and weighted normalized f scores for table
ds dg method cov avg
score n c
n c
c n
c n
n c
n c
c n
c n
















rl







no yes no yes no yes no yes weighted avg
score















rl







table table in the main paper result of transfer learning methods using newsroom for pre training and for ne tuning
the underlined result shows that the improvement from tl is not statistically signicant compared to the proposed model
dg ds n n n n c c c c method cov no yes no yes no yes no yes tl tl tl tl















rl







table table in the main paper result of transfer learning methods using newsroom for pre training and for ne tuning
dg ds n n n n c c c c method cov no yes no yes no yes no yes tl tl tl tl















rl







copyright by siam unauthorized reproduction of this article is prohibited
