diverse beam search for increased novelty in abstractive summarization cibils andre musat claudiu hossmann andreea baeriswyl michael ecole polytechnique federale de lausanne epfl email rstname

ch articial intelligence group swisscom ag email rstname

com e f l c
s c v
v i x r a abstract text summarization condenses a text to a shorter version while retaining the important informations
abstractive summarization is a recent development that generates new phrases rather than simply copying or rephrasing sentences within the original text
recently neural sequence to sequence models have achieved good results in the eld of abstractive summarization which opens new possibilities and applications for industrial purposes
however most practitioners observe that these models still use large parts of the original text in the output summaries making them often similar to extractive frameworks
to address this drawback we rst introduce a new metric to measure how much of a summary is tracted from the input text
secondly we present a novel method that relies on a diversity factor in computing the neural network loss to improve the diversity of the summaries generated by any neural abstractive model implementing beam search
nally we show that this method not only makes the system less extractive but also improves the overall rouge score of state of the art methods by at least points
introduction summarization is a process of generating a condensed version of a text that contains the key information from the original
for automatic summarization two approaches can be used extractive and abstractive sometimes also called generative
extractive summarization is focused on nding the most relevant text spans or phrases in the original document and to copying them to construct a summary from it
the nal text consists exclusively of passages from the input
this process can thus be viewed as a ranking mechanism jin et al

a second summary generation paradigm is to generate novel text to produce the summary
this generative cess sometimes uses words coming from a vocabulary seen in the source document as a human usually does
stractive summarization is considered more difcult because it requires high level competences such as generalization or reformulation compared to extractive summarization which ensures baseline levels of grammaticality and accuracy
until the advent of powerful deep architectures such as generative adversarial networks for text generation jeswar et al
zhang et al
and pointer works schmidhuber the generative summarization was largely considered infeasible
these novel methods are now opening the way to a new generation of summarization systems that carry the promise of near human text generation quality
practitioners however quickly discarded the earlier generative models due to their inability to solve problems that make the result look robotic and incomprehensible chiey among them repetition
a recent model the pointer generator network see et al
or pgnet achieves state of the art in term of tomatic summarization on a widely used dataset the n dailymail hermann et al
nallapati et al
see et al

this model is an abstractive sequence sequence neural model that uses both extractive and tive summarization techniques
to overcome problems that plagued previous sequence to sequence neural models like repetition and the inability to handle out of vocabulary oov words the pgnet features several improvements
these include attention and a hybrid mechanism that enables the model to copy words from the original text a pointer work schmidhuber
the inclusion of a pointer generation step has a clear tive impact on the performance of the system
this progress is evident both quantitatively e

rouge scores and itatively see et al

there is a caveat though
due to its extractive pointer generator component the model has the option of copying large swaths of the original text
by ing this option the method becomes self defeating and leads to an overly extractive behavior in an abstractive method
we argue that this drawback is inherently common across marizers that use pointers to elements in the original text
in this work we propose a way to measure the tiveness of abstractive summarization methods
we show that common methods used in plagiarism detection can not identify cases where the summarizer simply pieces together several large text spans from the original text
we thus vise a novel method to penalize this behaviour and use it as a complement to traditional evaluations like rouge n and rouge l scores
the goal of this orthogonal evaluation is to discourage abstractive models from going for the low hanging fruit copying
a second contribution is a method to reduce the tiveness of abstractive summaries based on a diverse beam search dbs vijayakumar et al

a previous attempt li et al
to improve abstractive summaries with dbs had lacklustre results
while each verse summary is by itself not superior to the one obtained by a baseline model we propose a method of combining the novelty within various dbs based summaries
we show that the dbs based results are both qualitatively and tively better the the previous state of the art
a major sult is that we can thus decrease the extractive nature of the summarizer while at the same time increasing the and rouge l scores on the cnn daily mail dataset hermann et al

the method is compatible with any abstractive summarizer that uses beam search which includes the leading methods at the time of the writing lapati et al
see et al
paulus et al
chopra et al
hasselqvist et al

related work until recently text summarization has been in a vast majority extractive wong et al
chuang and yang
we are however on the cusp of a major paradigm shift towards abstractive summarization largely due to the good results tained by recent models
the rst work that succeed in doing abstractive rization using a sequence to sequence model was ati et al
who introduced a major dataset cnn daily mail that we also use in the current work
they addresses multiple issues such as capturing the hierarchy of to word structure and emitting words that are rare at training time
independently chopra et al
created an tive recurrent model that yielded good results on the shared task
more recently paulus et al
created a reinforcement model for abstractive summarization that came a new state of the art on the cnn daily mail dataset
this surge in interest in abstractive summarization from the research community thus lead to increasingly promising sults
in practice however there are remaining issues that clude their use in industrial settings inaccurately ing factual details an inability to deal with out of vocabulary oov words and repetition
the pgnet see et al
was designed as an architecture to tackle all of these issues simultaneously by pointing to and retrieving elements from the original text
while it successfully alleviates the impact of the tioned ills the pgnet tends to abuse its copying mechanism and the generated summaries are largely extractive
to address this novel problem we rst need to quantify it
we dene the extractiveness of an abstractive rizer as how much of the extracted summary is copied from the original text
the extractiveness is thus akin to rism detection which measures how much a text is extracted from another
plagiarism detection is a well studied problem and two frequently used measures are grams frequencies and longest common sequence lcs zhang et al
anzelmi et al

these techniques are not adequate measures of the extractiveness of a summary as we can pect from a summary to share a lot of words with the ment
the solution we present to reduce the extractiveness is based on diverse beam search dbs vijayakumar et al

dbs was shown to be effective for creating diverse image captions machine translation and visual question eration
it is a variation of the classic beam search designed for neural sequence models which addresses the lack of versity of the original algorithm gimpel et al

dbs has been used in multiple topics such as logue response generation asghar et al
machine et al
but also abstractive tion li et al

however dbs on its own contributes only marginally
score to the performance of stractive summarization
this is why we combine it with a candidate selection algorithm used in multiple elds imal marginal relevance mmr guo and sanner carbonell and goldstein
mmr is an algorithm that balances relevance and diversity in multiple set based mation retrieval tasks
in order to use mmr one needs to compute the ilarity between two sentences
many options exist but recently good results have been obtained using sentence embeddings for instance ones based on n gram features pagliardini et al

produces cic word and n gram embedding vectors which are tively combined into a sentence embedding
similarly to word embeddings mikolov et al
allow us to represent semantic relatedness between phrases
summary generation the rst part of this section is devoted to briey describing the baseline model pgnet see et al

in the second half we describe one of our main contributions how to hance pgnet in order to generate less extractive summaries

baseline model the core idea behind current methods for text tion consists in leveraging a corpus containing both the source documents and their summaries
pgnet in particular learns how to map an input sequence of words the source ument to another sequence of words the summary of the document by implementing the well known sequence sequence neural network architecture extended with an tention mechanism bahdanau et al

additionally pgnet addresses some of the shortcomings of previous els such as the inability to handle out of vocabulary words and repetition sankaran et al
tu et al

is the main novelty of pgnet the pointer work schmidhuber that aggregates the context vector produced by the attention mechanism and the decoder state making the model able to copy words from the original ument and to combine them with the fragments of the output summary generated in an abstractive fashion
it is thus ble for the model to make use of words that are not contained quences from the original document and a summary that is half composed of new words outside its lcs with the article
to address this issue we dene a metric which takes more parameters than just lcs into account and more generally nalizes any large spans of text copied from the original text
merging sentences or splitting them should not be penalized as heavily as just copying them wholly
moreover we want the metric to be consistent deterministic and to output a malized score
finally novel word combinations should not be penalized at all
we thus dene an extraction score as extraction s sp acss where acss is the set of all long non overlapping mon sequences between the summary s and the document and p acss is the set of the proportion of these common sequences i
e
each element of this set is the length of a mon sequence divided by the length of the summmary
we rst start by nding the long common sequences tween the summary and the original article
for this we use a similar algorithm to lcs which will return all long non lapping common sequences
this extraction score ensures that the sum of these scores is between and
moreover having two distinct common sequences of text of proportion and will be less ized than having one common sequence of text of proportion
a summary which consists of only one long common quence with the document will have a score of while a summary with only new words not encountered in the article will have a score of
on the reference summaries the average plagiarism score and extraction score over the test set are respectively
and

this means that the reference summaries are mainly paraphrasing and not copying text from the article as expected
it also outlines the value of the ness measure as the goal of the automated systems is to be close to zero as is the case for the human standard
the extraction score allows us to compare different system and rank them with respect to their generative capacity
improved decoding mechanism our solution to the extractivness problem is complementary to the baseline architecture and could be used on any tive model that employs a beam search

diverse beam search beam search is an iterative algorithm widely used to decode rnns nallapati et al
see et al
paulus et al
chopra et al
hasselqvist et al
that approximates the optimal solutions
at each time step the model computes yt


yb t the set of b solutions held at the start of the t time step yt arg max


yb tvt t s
t
yi t yj t figure a summary generated by the baseline model the pgnet
although it is overall extractive we see that some parts have been cut of
in its vocabulary i
e
they were never seen during the ing phase but that appear in the input text
concretely when computing the next word composing the summary a bility pt is generated based on the context vector the decoder state and the decoder input
such probability is then used as a soft switch to choose between generating a word from the known vocabulary or extracting a word from the original text
finally as it is common practice in many tasks such as machine translation and text summarization pgnet uses the beam search algorithm to generate its summaries
when generating each word composing the summary instead of greedily taking the word with the highest score the algorithm selects the top b best scoring candidates thus exploring tiple sequences in parallel
nevertheless what often happens in practice is that the output generated by such heuristic tend to stem from a gle highly valued beam resulting on minor perturbations of a single sequence
pgnet in practice in practice pgnet acts very much as an extractive model as see et al
point out in their work of the time the model copies whole article tences meaning the model behave as fully extractive
in the others the model encompasses a range of abstractive techniques such as truncating sentences to correct shorter versions which still make the model feel tractive see et al

this can also be seen in figure in which we highlighted the portion of the input text that is reused in the summary

extractiveness these observations motivated the current work
to tackle tractiveness a rst step is to measure it
some methods ready exist for simple plagiarism detection based on n gram frequencies analysis or longest common sequence zhang et al
anzelmi et al

from lcs we can easily dene a plagiarism score by normalizing the length of the lcs between a given summary and a document by the length of the summary
however this metric has a major aw it completely discards the rest of the summary and thus make no difference between a summary constructed from two where b is the beam width t the log probability of a partial solution v the vocabulary and vt yt v the set of all possible token extensions of the beams yt from which bs will pick b elements from
mmr diverse beam search on another hand decodes diverse lists by dividing equally the beam size between groups ing to each of them the same beam budget i
e
the number of nodes expanded at each time step for a given group and enforcing diversity between groups of beams y g b t arg max


yg t t


y v g t t b s
t
t


y t yj t where i is the diversity term ing the dissimilarity of group g against prior groups if token y is chosen to extend any of the beams in the group g and the diversity strength
t by applying a variable diversity strength and a diversity measure we observed that forcing the model to generate tiple and diverse summaries logically pushed the model to provise more and use less of its extractive capabilities
many different diversity terms can be used and we present two amples in the following section

merging diverse summaries the summaries based on dbs are not intrinsically better than the summaries generated by the classic beam search but they contain more novelty
from this we inferred that generating multiple summaries and then picking and merging the best sentences of these summaries could lead to not only a less extractive model but also one that is better at capturing the relevant aspects
we thus split all the diverse summaries into sentences which become candidates to form part of a good and diverse summary
to pick the best sentences we rst rank them
we use a framework which gave good results in keyphrase tion with sentence embeddings bennani smires et al

the method is based on pagliardini et al
which allows the embedding of arbitrary length sequences of words
similarly to word embeddings it can be used to sent semantic relatedness between phrases by using standard similarity measures like cosine or euclidean
we generate a document embedding using
this is done by simply concatenating all the sentences from the document and subsequently treating the document as a single phrase
the document embedding is then used to rank the usefulness of candidate sentences
the intuition is that the most useful candidate phrases are at the same time close to the document and far away from each other
we thus use maximal marginal relevance mmr to pick the candidates
mmr is used in information retrieval guo and sanner carbonell and goldstein and balances relevance in our case similarity to the original document to summarize and diversity
we compute the cosine similarity between the candidates embedding and the document embedding to obtain a score which measures how much relevant information the candidate phrase contains
more precisely we pick n candidates iteratively using m m r arg max max cj k ci where c is the set of candidates i
e
the sentences k is the set of already picked candidates ci and d are the beddings of candidate i and of the document respectively and cossimb are a similarity measure here a ized cosine similarity as described below and is a trade off parameter between diversity and classic ranking
the choice of n and the diversity factor is detailed in the following section
we dene as below ci ci max ckc ck ci
ci c c where d is the document embedding c and c represent the average similarity and the standard deviation between d and the set of candidates c
we apply the same kind of transformation for the ity among the candidate phrases themselves as shown by the following equations for ci ci max ck ci
ci c ci c ci by selecting the best sentences we can construct the nal summary which contains the best elements of the generated diverse summaries
experiments we evaluate the baseline pgnet and our diverse generative model using the same pyrouge as nallapati et al
see et al

for all experiments we use a trained pgnet as described in the original paper see et al
with coverage using hidden states for the encoder and the decoder lstms and dimensional words embeddings
the words embedding are not pretrained but are learned ing training
we train using adagrad duchi et al
with learning rate
and an initial accumulator value of

gradient clipping is used with a maximum gradient norm of and there is no regularization
during training we also truncate the article to tokens and for generating new summaries the limit length of the summary is at rst set to similarly to the original model

python
pyrouge


figure two summaries generated by pgnet with beam search and by pgnet with
although the baseline model behave as a fully extractive one our post processing module output a more diverse and abstractive summary
pointer generator network with beam search decoder baseline pointer generator network with decoder n
pointer generator network with decoder n
reference summaries plagiarism score



extraction score



table plagiarism score and extraction score on the test set
except for n and the hyper parameters stay the same for the generator network
we also display the score from the reference summaries for comparison

hyper parameter description and selection best set of hyper parameters testing exhaustively each of these hyper parameter is sive thus we performed the experiments on a small portions on the dataset randomly picked examples on the ing set on multiple sets of hyper parameters
using and rouge l we then ranked the sets of hyper parameters and picked the best set
this set is then tested on the nal test dataset to conrm that no bias is hidden in this small dataset
hyper parameter description for the diverse beam search we enforce a maximum and minimum number of tokens per summary and
periments show that these are not the most important factors as the second and third best set of parameters were just ations of that
it is important to note that the baseline pgnet uses a maximum number of tokens per summaries of ing testing
the beam width b is the parameter used to dene the ber of nodes expanded at each time step during the search
a high value for b means that the search space is larger which is more computationally expensive
we set b to which is signicantly higher compared to the pgnet where it was set to
a high beam size means more resources to explore each of the groups which leads in turn to better candidates
there is a trade off between b and the number of groups g
each group requires resources and if g is set to it duces dbs to bs
on the other hand setting it to the beam size b allows for the maximum exploration of the search space in opposition to having a high budget for each group
we set the group number g to with a beam width of per group
this allow the model to perform evenly to the baseline on the rst group
this also generates different summaries
the diversity strength is a scalar between and which penalizes summaries that look alike
more precisely this species the trade off between the joint probability and the diversity terms
a high value produces more diverse mary but excessively high values of the diversity strength can result in grammatically incorrect outputs as it can overpower model probability
finally for beam search is dened as a function that outputs a vector of similarity scores for potential beam pletions
many options exist for a diversity function
hamming diversity the current group is penalized for producing the same words at the same time
more cisely we compute the hamming distance between two strings and normalized it over the length of the string and over the group
n grams diversity the current group is penalized for producing the same n grams as previous group less of alignment in time
we use the hamming diversity with a diversity strength of
meaning that we penalizes the selection of tokens used in previous groups proportional to the number of times it was selected before
it ensures that different words are used at ferent times which forces the model to rephrases it sentences pointer generator network with a beam search decoder baseline model pointer generator network with decoder n
pointer generator network with decoder n
rouge l








table rouge scores on the test set
all of our rouge scores have a condence interval of at most
as reported by the ofcial rouge script
except for n and the hyper parameters stay the same
and add new words
then we dene the hyper parameters for embedrank which are the number of candidates to select n and the versity factor for the mmr
more precisely we dene n as the number of iteration that we run on the mmr algorithm and thus the number of sentences in our nal summary
the factor is a trade off between a standard relevance ranked list and a maximal diversity ranking of the candidates
finally the parameters for embedrank are set to n and
for and rouge l optimization meaning that each summary will be three sentences long

dbs improvements extractiveness we rst evaluate the impact of the diverse generative marization through the prism of the extractiveness of the sulting models
in figure we portray the original pgnet and diverse summaries side by side
it is immediately ous to a human that the latter is less extractive and contains shorter excerpts from the original text
to quantify this ing we compute the previously discussed plagiarism score and extraction score for each method
the results are shown in table
we portray two parameter combinations for the diverse summary generator n
and n

firstly we see that both models perform considerably better with regards of both metrics
a plagiarism score of
means than in average our summaries are less than half tracted from the text
moreover having a value
instead of
value on the extraction score means that the model is in its core less extractive even if it keeps some common sequence with the original text
secondly we notice that we can reduce the extractiveness respectively getting
and
on plagiarism score and extraction score with our model by having slightly longer summaries
this is a counterintuitive result that is in fact ily explainable
when picking a lower number of sentences the impact of the diversity is in fact reduced whereas for higher selected candidate counts the relevant information is spread across multiple sentences
rouge scores we compare the rouge results for the diverse summaries with two different parameter combinations in table
all of the summaries are evaluated using the standard rouge metric more precisely the scores for and rouge l which measures respectively the word overlap bigram overlap and the longest common sequence between the reference summary and the generated summary
both the summaries containing n and n sentences perform signicantly above the baseline with score gains in addition we observe that there is a exceeding points
slight tradeoff with the extractiveness measure
a much lower extractiveness can be obtained for a marginal cost in term of rouge score
a notable result is that the and rouge l score improvements are obtained ously with the extractiveness decrease making the diverse symmary generation a win win proposition
conclusions and future work in this paper we presented ways of measuring and reducing the extractiveness of abstractive generative summaries while increasing their overall quality
we showed that while one of the best generative summary architectures the pgnet alleviates other ills of summaries including repetition and out of vocabulary words it suffers from a highly extractive nature
we outlined why previous methods found in plagiarism detection are not well suited for evaluating generative summaries and proposed an alternative extractiveness measure
we then showed that this ness measure is correlated with human judgment with the ground truth being close to the minimum extractiveness ues
we then presented an alternative decoding mechanism which can be applied to any abstractive framework which uses beam search
the method leverages multiple recent velopments complementing the diverse beam search with a diverse summary combination mechanism
the latter is based on similarity measures computed using sentence embeddings
we showed the advantages of this diverse summary eration method
it not only reduce the extractiveness of the architecture measured in two separate ways but that it also improves the overall rouge score by a signicant
finally we believe that this method opens the door to eral directions of research for reducing the extractiveness of abstractive framework alongside improving their overall formance
one one hand selecting the candidates is a crucial step as doing it optimally can further improve the rouge score
one the other hand this method could be mented on other abstractive summarization frameworks in order to reduce their extractiveness while improving their overall rouge score
references anzelmi et al
daniele anzelmi domenico carlone fabio rizzello robert thomsen and d m akbar sain
plagiarism detection based on scam algorithm
proceedings of the international multiconference of neers and computer scientists
asghar et al
nabiha asghar pascal poupart xin jiang and hang li
deep active learning for dialogue generation
pages
bahdanau et al
dzmitry bahdanau kyunghyun cho and yoshua bengio
neural machine translation by jointly learning to align and translate
pages
bennani smires et al
kamil bennani smires claudiu musat martin jaggi andreea hossmann and michael baeriswyl
embedrank unsupervised keyphrase extraction using sentence embeddings

carbonell and goldstein jaime carbonell and jade goldstein
the use of mmr diversity based reranking for reordering documents and producing summaries
ceedings of the annual international acm sigir ference on research and development in information trieval sigir pages
chopra et al
sumit chopra michael auli and alexander m
rush
abstractive sentence summarization with attentive recurrent neural networks
pages
chuang and yang wesley t
chuang and jihoon yang
extracting sentence segments for text tion
proceedings of the annual international acm sigir conference on research and development in mation retrieval sigir pages
duchi et al
john duchi elad hazan and yoram singer
adaptive subgradient methods for online journal of machine ing and stochastic optimization
learning research
gimpel et al
kevin gimpel dhruv batra chris dyer and gregory shakhnarovich
a systematic ploration of diversity in machine translation
emnlp
guo and sanner shengbo guo and scott sanner
probabilistic latent maximal marginal relevance
ceedings of the international acm sigir conference on research and development in information retrieval pages
hasselqvist et al
johan helmertz and mikael kageback
stractive summarization using neural networks

hasselqvist niklas query based hermann et al
karl moritz hermann tomas kocisky edward grefenstette lasse espeholt will kay mustafa suleyman and phil blunsom
teaching machines to read and comprehend
pages
jin et al
feng jin minlie huang and xiaoyan zhu
a comparative study on ranking and selection strategies for multi document summarization
coling
et al
jiwei li will monroe and dan jurafsky
a simple fast diverse decoding algorithm for neural eration

mikolov et al
tomas mikolov kai chen greg rado and jeffrey dean
efcient estimation of word resentations in vector space
pages
nallapati et al
ramesh nallapati bowen zhou cero nogueira dos santos caglar gulcehre and bing ang
abstractive text summarization using sequence sequence rnns and beyond

pagliardini et al
matteo pagliardini prakhar gupta and martin jaggi
unsupervised learning of sentence beddings using compositional n gram features

et al
romain paulus caiming xiong and richard socher
a deep reinforced model for tive summarization

rajeswar et al
sai rajeswar sandeep subramanian francis dutil christopher pal and aaron courville
versarial generation of natural language

sankaran et al
baskaran sankaran haitao mi yaser al onaizan and abe ittycheriah
temporal tention model for neural machine translation

schmidhuber jurgen schmidhuber
pointer works
neural networks
see et al
abigail see peter j
liu and pher d
manning
get to the point summarization with pointer generator networks

tu et al
zhaopeng tu zhengdong lu yang liu xiaohua liu and hang li
modeling coverage for neural machine translation

vijayakumar et al
ashwin k vijayakumar michael cogswell ramprasath r
selvaraju qing sun stefan lee david crandall and dhruv batra
diverse beam search decoding diverse solutions from neural sequence els
pages
wong et al
kam fai wong mingli wu and jie li
extractive summarization using supervised and the semi supervised learning
international conference on computational volume
association for computational linguistics

proceedings of zhang et al
fangfang zhang yoon chan jhi hao wu peng liu and sencun zhu
a rst step towards algorithm plagiarism detection
proceedings of the international symposium on software testing and sis issta page
zhang et al
yizhe zhang zhe gan kai fan zhi chen ricardo henao dinghan shen and lawrence carin
adversarial feature matching for text generation


