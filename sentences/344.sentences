re evaluating evaluation in text summarization manik bhandari pranav gour atabak ashfaq pengfei liu graham neubig carnegie mellon university mbhandar pgour aashfaq
cmu
edu t c o l c
s c v
v i x r a abstract automated evaluation metrics as a stand in for manual evaluation are an essential part of the development of text generation tasks such as text summarization
however while the eld has progressed our standard metrics have not for nearly years rouge has been the standard evaluation in most in this paper we make an rization papers
attempt to re evaluate the evaluation method for text summarization assessing the ity of automatic metrics using top scoring tem outputs both abstractive and extractive on recently popular datasets for both level and summary level evaluation settings
we nd that conclusions about evaluation rics on older datasets do not necessarily hold on modern datasets and systems
we release a dataset of human judgments that are lected from top scoring neural tion systems abstractive and extractive
com neulab realsumm introduction in text summarization manual evaluation as plied by the pyramid method nenkova and sonneau is the gold standard in evaluation
however due to time required and relatively high cost of annotation the great majority of research papers on summarization use exclusively automatic evaluation metrics such as rouge lin louis and nenkova peyrard et al
bertscore zhang et al
score zhao et al

among these metrics rouge is by far the most popular and there is relatively little discussion of how rouge may deviate from human judgment and the potential for this deviation to change conclusions drawn garding relative merit of baseline and proposed methods
to characterize the relative goodness of evaluation metrics it is necessary to perform evaluation graham lin and och where a dataset annotated with human judgments e

dang and owczarzak is used to test the degree to which automatic metrics correlate therewith
however the classic tac meta evaluation datasets are now years and it is not clear whether conclusions found there will hold with modern systems and summarization tasks
two lier works exemplify this disconnect peyrard observed that the human annotated maries in the tac dataset are mostly of lower ity than those produced by modern systems and that various automated evaluation metrics strongly disagree in the higher scoring range in which rent systems now operate
rankel et al
observed that the correlation between rouge and human judgments in the tac dataset decreases when looking at the best systems only even for systems from eight years ago which are far from today s state of the art
constrained by few existing human judgment datasets it remains unknown how existing metrics behave on current top scoring summarization tems
in this paper we ask the question does the rapid progress of model development in rization models require us to re evaluate the ation process used for text summarization to this end we create and release a large benchmark for meta evaluating summarization metrics including outputs from top scoring extractive and abstractive summarization systems on the cnn dailymail dataset
automatic evaluations from several ation metrics including traditional metrics e

rouge and modern semantic matching metrics e

bertscore moverscore

nist
tac summarization was in
in the task was biomedical summarization
ability of metrics to observations on existing human judgments tac observations on new human judgments cnndm exp i evaluate all systems sec

moverscore and outperform all other metrics
exp ii evaluate top k systems sec

as k becomes smaller de correlates with humans
exp iii compare systems sec

moverscore and outperform all other metrics
exp iv evaluate summaries sec

moverscore and outperform all other metrics
metrics have much lower correlations when evaluating maries than systems
outperforms all other metrics
moverscore and performs worse both in extractive only achieved nearly
pearson correlation and abstractive summaries
for extractive and abstractive systems highly relates with humans
for evaluating a mix of extractive and abstractive systems all metrics de correlate
is the most reliable for abstractive systems while is most reliable for extractive systems
rouge metrics outperform all other metrics
for extractive summaries most metrics are better at evaluating summaries than systems
for abstractive summaries some metrics are better at summary level others are better at system level
table summary of our experiments observations on existing human judgments on the tac and contrasting observations on newly obtained human judgments on the cnndm dataset
please refer to sec
for more details
manual evaluations using the lightweight pyramids method shapira et al
which we use as a gold standard to evaluate rization systems as well as automated metrics
using this benchmark we perform an extensive analysis which indicates the need to re examine our assumptions about the evaluation of automatic summarization systems
specically we conduct four experiments analyzing the correspondence between various metrics and human evaluation
somewhat surprisingly we nd that many of the previously attested properties of metrics found on the tac dataset demonstrate different trends on our newly collected cnndm dataset as shown in tab

for example moverscore is the best performing metric for evaluating summaries on dataset tac but it is signicantly worse than on our collected cnndm set
additionally many ous works novikova et al
peyrard et al
chaganty et al
show that metrics have much lower correlations at comparing summaries than systems
for extractive summaries on cnndm however most metrics are better at comparing maries than systems
calls for future research these observations demonstrate the limitations of our current performing metrics highlighting the need for future meta evaluation to i be across multiple datasets and evaluate metrics on different plication scenarios e

summary level vs
tem level the need for more systematic evaluation of summarization metrics that updates with our ever evolving systems and datasets and the potential benet to the summarization munity of a shared task similar to the rics task in machine translation where systems and metrics co evolve

statmt
org preliminaries in this section we describe the datasets systems metrics and meta evaluation methods used below

datasets dang and owczarzak are multi document multi reference rization datasets
human judgments are available on for the system summaries submitted during the shared tasks
cnndm cnn dailymail hermann et al
nallapati et al
is a commonly used summarization dataset that contains news articles and associated highlights as summaries
we use the version without entities anonymized

representative systems we use the following representative top scoring systems that either achieve state of the art sota results or competitive performance for which we could gather the outputs on the cnndm dataset
extractive summarization systems
we use cnn lstm biclassier clstm sl kedzie et al
latent zhang et al
ditsum dong et al
refresh narayan et al
neusum zhou et al
hibert zhang et al
bert ext liu and lapata cnn biclassier ctrans sl zhong et al
cnn transformer pointer ctrans pn zhong et al
hetergraph wang et al
and matchsum zhong et al
as representatives of extractive systems totaling extractive system outputs for each document in the cnndm test set
abstractive summarization systems
we use pointer see et al
fastabsrl chen and bansal rank chen and bansal bottom up gehrmann et al
raffel et al
unilm dong et al
unilm dong et al
twostagerl zhang et al
summabs liu and lapata ext liu and lapata bart lewis et al
and semsim yoon et al
as tive systems
in total we use abstractive system outputs for each document in the cnndm test set

evaluation metrics we examine eight metrics that measure the ment between two texts in our case between the system summary and reference summary
bertscore bscore measures soft overlap tween contextual bert embeddings of tokens tween the two zhang et al

moverscore mscore applies a distance measure to contextualized bert and elmo word zhao et al

sentence mover similarity sms applies imum distance matching between text based on sentence embeddings clark et al

word mover similarity wms measures larity using minimum distance matching between texts which are represented as a bag of word kusner et al

js divergence measures jensen shannon divergence between the two text s bigram lin et al

and measure overlap of igrams and bigrams lin
rouge l measures overlap of the longest mon subsequence between two texts lin
we use the recall variant of all metrics since the pyramid method of human evaluations is inherently recall based except mscore which has no specic recall variant

correlation measures pearson correlation is a measure of linear lation between two variables and is popular in evaluating metrics at the system level lee rodgers
we use the implementation given by nen et al

william s signicance test is a means of ing the statistical signicance of differences in relations for dependent variables williams code at github
com tiiiger bert score code at github
com aiphes moverscore wms and sms github
com sms is calculated using the function dened in github
com ukplab genetic swarm mds and l we used the python wrapper
com sebastiangehrmann rouge baselines graham and baldwin
this is useful for us since metrics evaluated on the same dataset are not independent of each other

meta evaluation strategies there are two broad meta evaluation strategies summary level and system level
setup for each document i


n in a dataset d we have j system outputs where the outputs can come from extractive systems ext abstractive systems abs or a union of both mix
let sij j


j be the jth summary of the ith document mi be a specic metric and k be a correlation measure


summary level summary level correlation is calculated as follows ksum n n





here correlation is calculated for each document among the different system outputs of that ment and the mean value is reported


system level system level correlation is calculated as follows k sys k


n n n n n n n n


additionally the quality of a system sysj is dened as the mean human score received by it i
e
hscoresysj mean
n n collection of human judgments we follow a step process to collect human judgments we collect system generated maries on the most commonly used tion dataset cnndm we select representative test samples from cnndm and we manually evaluate system generated summaries of the selected test samples



system generated summary collection we collect the system generated summaries from top scoring covering extractive and abstractive systems sec

on the cnndm dataset
we organize our collected generated maries into three groups based on system type cnndm abs denotes collected output maries from abstractive systems
cnndm ext denotes collected output maries from extractive systems
cnndm mix is the union of the two

representative sample selection since collecting human annotations is costly we sample documents from cnndm test set samples and evaluate system generated summaries of these documents
we aim to include documents of varying difculties in the resentative sample
as a proxy to the difculty of summarizing a document we use the mean score received by the system generated summaries for the document
based on this we partition the cnndm test set into equal sized bins and sample uments from each bin
we repeat this process for metrics bertscore moverscore l obtaining a sample of documents
this methodology is detailed in alg
in sec
a


human evaluation in text summarization a good summary should represent as much relevant content from the input document as possible within the acceptable length limits
many human evaluation methods have been proposed to capture this desideratum nenkova and passonneau chaganty et al
fan et al
shapira et al

among these pyramid nenkova and passonneau is a reliable and widely used method that evaluates content tion by exhaustively obtaining semantic tent units scus from reference summaries weighting them based on the number of times they are mentioned and scoring a system summary based on which scus can be inferred
recently shapira et al
extended mid to a lightweight crowdsourcable method litepyramids which uses amazon mechanical amt for gathering human annotations
litepyramids simplies pyramid by allowing contacted the authors of these systems to gather the corresponding outputs including variants of the systems

mturk
crowd workers to extract a subset of all possible scus and eliminating the difcult task of ing duplicate scus from different reference maries instead using scu sampling to simulate frequency based weighting
both pyramid and litepyramid rely on the ence of multiple references per document to sign importance weights to scus
however in the cnndm dataset there is only one reference summary per document
we therefore adapt the litepyramid method for the single reference ting as follows
scu extraction the litepyramids annotation structions dene a semantic content unit scu as a sentence containing a single fact written as briey and clearly as possible
instead we focus on shorter more ne grained scus that contain at most entities
this allows for partial content overlap between a generated and reference mary and also makes the task easy for workers
tab
gives an example
we exhaustively extract up to from each reference summary
requiring the set of scus to be exhaustive creases the complexity of the scu generation task and hence instead of relying on crowd workers we create scus from reference summaries ourselves
in the end we obtained nearly
scus on age from each reference summary
system evaluation during system evaluation the full set of scus is presented to crowd workers
workers are paid similar to shapira et al
scaling the rates for fewer scus and shorter mary texts
for abstractive systems we pay
per summary and for extractive systems we pay
per summary since extractive summaries are more readable and might precisely overlap with scus
we post process system output summaries before presenting them to annotators by true casing the text using stanford corenlp manning et al
and replacing unknown tokens with a cial symbol chaganty et al

tab
depicts an example reference summary system summary scus extracted from the ence summary and annotations obtained in ating the system summary
annotation scoring for robustness shapira et al
each system summary is evaluated by crowd workers
each worker annotates up to scus by marking an scu present if it can be our representative sample we found no document ing more than scus
reference summary bayern munich beat porto in the champions league on tuesday
pep guardiola s side progressed on aggregate to reach
thomas muller scored champions league goal to pass mario gomez
muller is now the leading german scorer in the competition
after game muller led the celebrations with supporters using a megaphone
system summary bart lewis et al
bayern munich beat porto at the allianz arena on tuesday night
thomas muller scored his champions league goal
the year old became the highest scoring german since the tournament took its current shape in
bayern players remained on the pitch for some time as they celebrated with supporters
c scus with corresponding evaluations bayern munich beat porto
bayern munich won
bayern munich won in champions league
bayern munich won on tuesday
bayern munich is managed by pep guardiola
bayern munich progressed in the competition
bayern munich reached
bayern munich progressed on aggregate
thomas muller scored champions league goal
thomas muller passed mario gomez in goals
thomas muller is now the leading german scorer in the competition
after the game thomas muller led the celebrations
thomas muller led the celebrations using a phone
table example of a summary and corresponding annotation
a shows a reference summary from the tative sample of the cnndm test set
shows the corresponding system summary generated by bart one of the abstractive systems used in the study
c shows the scus semantic content units extracted from a and the marked by crowd workers when evaluating b
inferred from the system summary or not present otherwise
we obtain a total of human tations documents systems workers
for each document we identify a noisy worker as one who disagrees with the majority i
e
marks an scu as present when majority thinks not present or vice versa on the largest number of scus
we remove the annotations of noisy workers and retain annotations of the
after this ltering we obtain an average inter annotator agreement krippendorff s alpha krippendorff of

finally we use the majority vote to mark the presence of an scu in a system mary breaking ties by the class not present
experiments motivated by the central research question does the rapid progress of model development in rization models require us to re evaluate the tion process used for text summarization we use the collected human judgments to meta evaluate current metrics from four diverse viewpoints suring the ability of metrics to evaluate all systems evaluate top k strongest systems compare two systems evaluate individual maries
we nd that many previously attested erties of metrics observed on tac exhibit different trends on the new cnndm dataset
agreement was
and
for extractive and stractive systems respectively

exp i evaluating all systems automatic metrics are widely used to determine where a new system may rank against existing state of the art systems
thus in meta evaluation studies calculating correlation of automatic rics with human judgments at the system level is a commonly used setting novikova et al
bojar et al
graham
we follow this setting and specically ask two questions can metrics reliably compare different systems to answer this we observe the pearson correlation tween different metrics and human judgments in fig
nding that moverscore and which were the best forming metrics on tac have poor correlations with humans in comparing cnndm ext systems
most metrics have high correlations on the dataset but many suffer on especially rouge based metrics
however rouge metrics consistently perform well on the collected cnndm datasets
are some metrics signicantly better than ers in comparing systems since automated metrics calculated on the same data are not dent we must perform the william s test williams to establish if the difference in correlations between metrics is statistically signicant graham and baldwin
in fig
we report the values of william s test
we nd that cells with p value
have been rounded up
cnndm mix cnndm abs e cnndm ext figure value of william s signicance test for the hypothesis is the system on left y axis signicantly better than system on top axis
bscore refers to bertscore and mscore refers to moverscore
a dark green value in cell i j denotes metric mi has a signicantly higher pearson correlation with human scores compared to metric mj value

in cell i j refers to the case when pearson correlation of mi with human scores is less that of mj sec


ments in comparing the top k systems where top k are chosen based on a system s mean human score eqn

our observations are presented in fig

we nd that as k becomes smaller metrics de correlate with humans on the and cnndm mix datasets even getting negative correlations for small values of k fig

interestingly sms and r l improve in performance as k becomes smaller on cnndm ext
had negative correlations with human ments on for k however it mains highly correlated with human judgments on cnndm abs for all values of k
takeaway metrics can not reliably quantify the improvements made by one system over others pecially for the top few systems across all datasets
some metrics however are well suited for specic datasets e

and are reliable indicators of improvements on and cnndm abs respectively

exp iii comparing t wo systems instead of comparing many systems sec


ranking two systems aims to test the discriminative power of a metric i
e
the degree to which the ric can capture statistically signicant differences between two summarization systems
we analyze the reliability of metrics along a useful dimension can metrics reliably say if one system is signicantly better than another since we only have annotated summaries to compare any two systems and we use paired bootstrap resampling to test with statistical a caveat we do not perform signicance testing for this experiment due to the small number of data points
figure system level pearson correlation between metrics and human scores sec


moverscore and are signicantly better than other metrics in correlating with human ments on the tac datasets
however on cnndm abs and cnndm mix signicantly outperforms all others whereas on cnndm ext none of the metrics show signicant improvements over others
takeaway these results suggest that metrics run the risk of overtting to some datasets ing the need to meta evaluate metrics for modern datasets and systems
additionally there is no one all metric that can outperform others on all datasets
this suggests the utility of using different metrics for different datasets to evaluate systems e

moverscore on on and on cnndm datasets

exp ii evaluating top k systems most papers that propose a new state of the art tem often use automatic metrics as a proxy to man judgments to compare their proposed method against other top scoring systems
however can metrics reliably quantify the improvements that one high quality system makes over other competitive systems to answer this instead of focusing on all of the collected systems we evaluate the tion between automatic metrics and human
a cnndm mix cnndm abs e cnndm ext figure system level pearson correlation with humans on top k systems sec


figure scores with bootstrapping sec


summary level pearson correlation with human scores
nicance if is better than according to metric m koehn dror et al

we take pairs of systems and compare their mean all human score eqn
using paired bootstrap pling
we assign a label ytrue if is better than with condence ytrue for versa and ytrue if the condence is below
we treat this as the ground truth label of the pair
this process is then repeated for all metrics to get a prediction ym pred from each ric m for the same pairs
if m is a good proxy for human judgments the score goutte and gaussier between ym pred and ytrue should be high
we calculate the weighted macro score for all metrics and view them in fig

we nd that rouge based metrics perform moderately well in this task
performs the best on cnndm datasets
while on the tac dataset achieves the highest score its formance is low on cnndm ext
takeaway different metrics are better suited for different datasets
for example on the cnndm datasets we recommend using while on the tac datasets we recommend using

exp iv evaluating summaries in addition to comparing systems real world plication scenarios also require metrics to reliably compare multiple summaries of a document
for example top scoring reinforcement learning based summarization systems bohm et al
and the current state of the art extractive system zhong et al
heavily rely on summary level reward difference between system level and summary level son correlation
figure pearson correlation between metrics and man judgments across different datasets sec


scores to guide the optimization process
in this experiment we ask the question how well do different metrics perform at the summary in comparing system summaries level i
e
erated from the same document we use eq
to calculate pearson correlation between different metrics and human judgments for different datasets and collected system outputs
our observations are summarized in fig

we nd that as compared to semantic matching metrics and r l have lower correlations on the tac datasets but are strong indicators of good maries especially for extractive summaries on the cnndm dataset
notably bertscore wms and r l have negative correlations on but form moderately well on other datasets including cnndm
previous meta evaluation studies novikova et al
peyrard et al
chaganty et al
conclude that automatic metrics tend to relate well with humans at the system level but have poor correlations at the instance here mary level
we nd this observation only holds on









some metrics summary level lations can outperform system level on the cnndm dataset as shown in fig
bins below y
notably moverscore has a correlation of only
on cnndm ext at the system level but
at the summary level
takeaway meta evaluations of metrics on the old tac datasets show signicantly different trends than meta evaluation on modern systems and datasets
even though some metrics might be good at comparing summaries they may point in the wrong direction when comparing systems
moreover some metrics show poor generalization ability to different datasets e

bertscore on vs other datasets
this highlights the need for empirically testing the efcacy of ent automatic metrics in evaluating summaries on multiple datasets
related work this work is connected to the following threads of topics in text summarization
human judgment collection despite many approaches to the acquisition of human judgment chaganty et al
nenkova and passonneau shapira et al
fan et al
pyramid nenkova and passonneau has been a mainstream method to meta evaluate various automatic metrics
specically pyramid provides a robust technique for evaluating content selection by exhaustively obtaining a set of semantic content units scus from a set of references and then scoring system summaries on how many scus can be inferred from them
recently shapira et al
proposed a lightweight and crowdsourceable version of the original pyramid and demonstrated it on the duc dang and dang multi document summarization datasets
in this paper our human evaluation methodology is based on the pyramid nenkova and passonneau and litepyramids shapira et al
techniques
chaganty et al
also obtain human evaluations on system summaries on the cnndm dataset but with a focus on language quality of summaries
in comparison our work is focused on evaluating content selection
our work also covers more systems than their study extractive abstractive vs
abstractive
meta evaluation with human judgment the effectiveness of different automatic metrics lin rouge l lin rouge we ng and abrecht louis and nenkova and peyrard et al
is commonly evaluated based on their tion with human judgments e

on the dang and owczarzak and dang and owczarzak datasets
as an important supplementary technique to evaluation graham advocate for the use of a signicance test william s test williams to measure the improved correlations of a metric with human scores and show that the lar variant of rouge mean score is sub optimal
unlike these works instead of ing a new metric in this paper we upgrade the meta evaluation environment by introducing a able human judgment dataset evaluating current top scoring systems and mainstream datasets
and then we re evaluate diverse metrics at both level and summary level settings
novikova et al
also analyzes existing metrics but they only focus on dialog generation
implications and future directions our work not only diagnoses the limitations of current metrics but also highlights the importance of upgrading the existing meta evaluation testbed keeping it up to date with the rapid development of systems and datasets
in closing we highlight some potential future directions the choice of metrics depends not only on different tasks e
g summarization translation but also on different datasets e

tac cnndm and application narios e
g system level summary level
future works on meta evaluation should investigate the fect of these settings on the performance of metrics
metrics easily overt on limited datasets
dataset meta evaluation can help us better stand each metric s peculiarity therefore achieving a better choice of metrics under diverse ios
our collected human judgments can be used as supervision to instantiate the most proposed pretrain framework nally for machine translation sellam et al
learning a robust metric for text summarization
acknowledgements we sincerely thank all authors of the systems that we used in this work for sharing their systems outputs
references ondrej bojar yvette graham amir kamran and milos stanojevic

results of the in proceedings of the first rics shared task
ference on machine translation volume shared task papers pages berlin germany
sociation for computational linguistics
florian bohm yang gao christian m
meyer ori shapira ido dagan and iryna gurevych

ter rewards yield better summaries learning to marise without references
arun tejasvi chaganty stephen mussman and percy liang

the price of debiasing automatic rics in natural language evaluation
yen chun chen and mohit bansal

fast tive summarization with reinforce selected sentence rewriting
in proceedings of the annual ing of the association for computational tics volume long papers pages bourne australia
association for computational linguistics
elizabeth clark asli celikyilmaz and noah a smith

sentence mover s similarity automatic in proceedings of uation for multi sentence texts
the annual meeting of the association for putational linguistics pages
hoa dang and karolina owczarzak

overview of the tac update summarization task
in ceedings of the first text analysis conference tac pages
hoa dang and karolina owczarzak

overview of the tac summarization track
in proceedings of the first text analysis conference tac pages
hoa trang dang

overview of duc
in in proceedings of the document understanding conf
wksp
duc at the human language technology conf

on empirical methods in natural language processing hlt emnlp
hoa trang dang

overview of duc
in in proceedings of hlt naacl
li dong nan yang wenhui wang furu wei aodong liu yu wang jianfeng gao ming zhou and hsiao wuen hon

unied language model pre training for natural language ing and generation
in advances in neural tion processing systems pages
rotem dror gili baumer segev shlomov and roi ichart

the hitchhiker s guide to testing tical signicance in natural language processing
in proceedings of the annual meeting of the sociation for computational linguistics volume long papers pages melbourne tralia
association for computational linguistics
angela fan david grangier and michael auli

controllable abstractive summarization
in ings of the workshop on neural machine lation and generation pages melbourne australia
association for computational tics
sebastian gehrmann yuntian deng and alexander rush

bottom up abstractive summarization
in proceedings of the conference on cal methods in natural language processing pages
cyril goutte and eric gaussier

a probabilistic interpretation of precision recall and score with implication for evaluation
in european conference on information retrieval pages
springer
yvette graham

re evaluating automatic marization with bleu and shades of rouge
in proceedings of the conference on cal methods in natural language processing pages lisbon portugal
association for tational linguistics
yvette graham and timothy baldwin

testing for signicance of increased correlation with human in proceedings of the conference judgment
on empirical methods in natural language ing emnlp pages doha qatar
ation for computational linguistics
karl moritz hermann tomas kocisky edward stette lasse espeholt will kay mustafa suleyman and phil blunsom

teaching machines to read in advances in neural and comprehend
tion processing systems pages
chris kedzie kathleen mckeown and hal daume iii

content selection in deep learning models of in proceedings of the summarization
ference on empirical methods in natural language processing pages
philipp koehn

statistical signicance tests in for machine translation evaluation
ings of the conference on empirical ods in natural language processing pages barcelona spain
association for tional linguistics
yue dong yikang shen eric crawford herke van hoof and jackie chi kit cheung

sum extractive summarization as a contextual dit
in proceedings of the conference on pirical methods in natural language processing pages brussels belgium
association for computational linguistics
klaus krippendorff

computing krippendorff alpha reliability
matt kusner yu sun nicholas kolkin and kilian weinberger

from word embeddings to ument distances
in international conference on chine learning pages
w alan lee rodgers

thirteen ways to look at the correlation coefcient
the american tician
mike lewis yinhan liu naman goyal jan ghazvininejad abdelrahman mohamed omer levy ves stoyanov and luke zettlemoyer

bart denoising sequence to sequence pre training for natural language generation translation and comprehension
arxiv preprint

chin yew lin

rouge a package for matic evaluation of summaries
text summarization branches out
chin yew lin guihong cao jianfeng gao and yun nie

an information theoretic approach to automatic evaluation of summaries
in ings of the human language technology ence of the naacl main conference pages new york city usa
association for tational linguistics
chin yew lin and franz josef och

orange a method for evaluating automatic evaluation rics for machine translation
in coling ceedings of the international conference on computational linguistics pages geneva switzerland
coling
yang liu and mirella lapata

text in proceedings of tion with pretrained encoders
the conference on empirical methods in ural language processing and the international joint conference on natural language processing emnlp ijcnlp pages hong kong china
association for computational linguistics
yang liu and mirella lapata

text tion with pretrained encoders
annie louis and ani nenkova

automatically assessing machine summary content without a gold standard
computational linguistics
christopher d
manning mihai surdeanu john bauer jenny finkel steven j
bethard and david closky

the stanford corenlp natural guage processing toolkit
in association for tational linguistics acl system demonstrations pages
ramesh nallapati bowen zhou cicero dos santos c a glar and bing xiang

tive text summarization using sequence to sequence rnns and beyond
conll page
shashi narayan shay b
cohen and mirella lapata

ranking sentences for extractive tion with reinforcement learning
in proceedings of the conference of the north american ter of the association for computational linguistics human language technologies volume long pers pages new orleans louisiana
association for computational linguistics
ani nenkova and rebecca passonneau

ing content selection in summarization the in proceedings of the human mid method
guage technology conference of the north can chapter of the association for computational linguistics hlt naacl pages boston massachusetts usa
association for putational linguistics
jun ping ng and viktoria abrecht

better marization evaluation with word embeddings for rouge
in proceedings of the conference on empirical methods in natural language processing pages lisbon portugal
association for computational linguistics
jekaterina novikova ondrej dusek amanda cas curry and verena rieser

why we need in proceedings new evaluation metrics for nlg
of the conference on empirical methods in natural language processing pages copenhagen denmark
association for tional linguistics
maxime peyrard

studying summarization uation metrics in the appropriate scoring range
in proceedings of the association for computational linguistics pages florence italy
association for tational linguistics
the annual meeting of maxime peyrard teresa botschen and iryna gurevych

learning to score system summaries for better content selection evaluation
in proceedings of the workshop on new frontiers in summarization pages copenhagen mark
association for computational linguistics
colin raffel noam shazeer adam roberts katherine lee sharan narang michael matena yanqi zhou wei li and peter j
liu

exploring the limits of transfer learning with a unied text to text former
peter a
rankel john m
conroy hoa trang dang and ani nenkova

a decade of automatic tent evaluation of news summaries reassessing the state of the art
in proceedings of the annual meeting of the association for computational guistics volume short papers pages soa bulgaria
association for computational guistics
abigail see peter j
liu and christopher d
manning

get to the point summarization with generator networks
in proceedings of the nual meeting of the association for computational linguistics volume long papers pages vancouver canada
association for tional linguistics
thibault sellam dipanjan das and ankur p parikh

bleurt learning robust metrics for text eration
arxiv preprint

ori shapira david gabay yang gao hadar nen ramakanth pasunuru mohit bansal yael sterdamer and ido dagan

crowdsourcing lightweight pyramids for manual summary tion
in proceedings of the conference of the north american chapter of the association for putational linguistics human language gies volume long and short papers pages minneapolis minnesota
association for putational linguistics
wei zhao maxime peyrard fei liu yang gao tian m
meyer and steffen eger

moverscore text generation evaluating with contextualized beddings and earth mover distance
in proceedings of the conference on empirical methods in natural language processing and the tional joint conference on natural language cessing emnlp ijcnlp pages hong kong china
association for computational guistics
ming zhong pengfei liu yiran chen danqing wang xipeng qiu and xuanjing huang

tive summarization as text matching
arxiv preprint

ming zhong pengfei liu danqing wang xipeng qiu and xuan jing huang

searching for tive neural extractive summarization what works and what s next
in proceedings of the ence of the association for computational tics pages
qingyu zhou nan yang furu wei shaohan huang ming zhou and tiejun zhao

neural ument summarization by jointly learning to score and select sentences
in proceedings of the nual meeting of the association for computational linguistics volume long papers pages melbourne australia
association for tational linguistics
pauli virtanen ralf gommers travis e
oliphant matt haberland tyler reddy david peau evgeni burovski pearu peterson warren weckesser jonathan bright stefan j
van der walt matthew brett joshua wilson k
jarrod millman nikolay mayorov andrew r
j
nelson eric jones robert kern eric larson cj carey ilhan polat yu feng eric w
moore jake vand erplas denis laxalde josef perktold robert cimrman ian riksen e
a
quintero charles r harris anne m
archibald antonio h
ribeiro fabian pedregosa paul van mulbregt and scipy
contributors

scipy
fundamental algorithms for scientic computing in python
nature methods
danqing wang pengfei liu yining zheng xipeng qiu and xuanjing huang

heterogeneous graph neural networks for extractive document marization
arxiv preprint

evan j
williams

regression analysis
wiley new york
wonjin yoon yoon sun yeo minbyul jeong jun yi and jaewoo kang

learning by mantic similarity makes abstractive summarization better
arxiv preprint

haoyu zhang yeyun gong yu yan nan duan jun xu ji wang ming gong and ming zhou
language
arxiv preprint eration for text summarization


pretraining based natural tianyi zhang varsha kishore felix wu kilian q
weinberger and yoav artzi

bertscore in international uating text generation with bert
conference on learning representations
xingxing zhang mirella lapata furu wei and ming zhou

neural latent extractive document marization
in proceedings of the conference on empirical methods in natural language ing pages brussels belgium
association for computational linguistics
xingxing zhang furu wei and ming zhou

hibert document level pre training of cal bidirectional transformers for document in proceedings of the annual rization
ing of the association for computational tics pages florence italy
association for computational linguistics
summary level kendall correlation with human scores
difference between system level and summary level kendall correlation
figure kendall correlation between metrics and man judgements across different datasets
a
exp ii using kendall s tau correlation please see figure for the system level kendall s tau correlation on top k systems between different metrics and human judgements
a
exp iv using kendall s tau correlation please see figure for the summary level kendall s tau correlation between different metrics and man judgements
a appendices a
sampling methodology please see algorithm
algorithm sampling methodology data ri si d where d is cnndm test set is source document ri is reference summary and si is a set of individual system summaries sij si
m rouge l bertscore output dout sampled set of documents m i si m i std
si m m m m dout for m m do d sorted by m i for k do k sorted by m i i for l do l randomly sample di from dout dout end end end a
exp i using kendall s tau correlation please see figure for the system level kendall s tau correlation between different metrics and man judgements
figure system level kendall correlation between metrics and human scores






a cnndm mix cnndm abs e cnndm ext figure system level kendall correlation with humans on top k systems



