point less more abstractive summarization with pointer generator networks freek boutkan university of amsterdam david rau university of amsterdam jorn ranzijn university of amsterdam eelco van der wel university of amsterdam r a l c
s c v
v i x r a abstract the pointer generator architecture has shown to be a big ment for abstractive summarization models
however the summaries produced by this model are largely extractive as over of the generated sentences are copied from the source text
this work proposes a multihead attention mechanism pointer dropout and two new loss functions to promote more abstractive summaries while maintaining similar rouge scores
both the multihead tention and dropout do not improve n gram novelty however the dropout acts as a regularizer which improves the rouge score
the new loss function achieves significantly higher novel n grams and sentences at the cost of a slightly lower rouge score
introduction more data is becoming available on the web every day for instance in the form of news articles and scientific publications and tracting the most relevant information is becoming increasingly difficult
a well written summary should be able to provide the gist of a text and can help to reduce the effort of obtaining all the relevant information
automated text summarization has fore received a lot of interest since the advent of deep learning techniques
the process of summarization is often divided into extractive summarization and abstractive summarization
in tractive summarization a summary is obtained by copying the relevant parts of the text
abstractive summarization aims to distil the relevant information from the source text into a summary by paraphrasing and is therefore not limited to the use of the exact phrases of the source text
see et al
propose a new approach for abstractive rization by combining a sequence to sequence model with a pointer network
additionally see et al
incorporate a erage mechanism that aims to tackle the problem of over generation by penalizing attention to words in the source document that have already received attention in past timesteps
one issue with pointer generator networks is that during test time the model focuses mainly on the source text during summary generation and does not introduce many novel words
the resulting summaries therefore tend to be more extractive than abstractive
see et al
state that this focus on the source text is likely caused by the word by word supervision during training time which is not possible during test time
another issue with the pointer generator architecture is that the generator is undertrained as the network learns to use the pointer mechanism early in training and arrives at a local minimum
we hypothesise that these two factors are the main contributors to the over reliance on the pointer mechanism during test time
another limitation is the used dataset and evaluation metric rouge
this is discussed in detail in section
in abstractive summarization there are many candidate solutions
however the provided dataset rarely contains all of these and perfectly viable summaries are sometimes penalized
this is closely related to the problem with the rouge metric as this can also produce low score for viable summaries
these problems are not specific to the pointer generator model and addressing them is less obvious
the goal of this research to increase the number of novel grams while obtaining similar rouge scores therefore improving abstraction in an end to end trainable text summarization model
our contributions comprise of multihead attention over the source text dropout mechanism over the pointer naive pointer regularization pointer regularization based on word priors related work abstractive and extractive summarization
in extractive tion fragments of the source text are concatenated to generate a summary
an advantage of this task is that it is relatively easy to obtain a summary with good fluency and factual correctness
in contrast abstractive methods allow for the use of synonyms alization and rephrasing of the source text
while in theory this can lead to results that are closer to human generated summaries jing it is a much more difficult task than extractive summarization
common problems include factual and grammatical mistakes but also over under generation of words
in more recent work hybrid models are proposed to combine the strengths of both methods
these models can create abstractive summaries with extractive elements to promote factual correctness and out of vocabulary oov word generation
pointer networks
the incorporation of a copying mechanism to the sequence to sequence has proved to be a powerful addition for summarization tasks
both the copynet and pointer generator propose adding such a mechanism to bypass the generator network in order to generate words directly from the input ument
while this is useful in many cases both papers observe balancing the strength of the pointer mechanism and the generator is a difficult task
the pointer generator seems easier to train and as a result most of the generated summary is generated by directly copying from the source
weber et al
confirmed the over reliance on the pointer mechanism and introduced a penalty during beam decoding in order to increase the probability of generating a word from the generator distribution
however no changes are made to the training process and the clear downside of this approach is that the over reliance is not solved during training time but only afterwards
song et al
add structural elements to the copy mechanism
they say a possible problem of the copy mechanism is that it only looks at semantic information while structural information such as grammatical structure might be more important for generating good summaries
attention
a main difference between the copynet architecture and pointer generator is that copynet uses a separate tion distribution for pointing and generating while the generator only uses one
see et al
pose that similar information is needed for both pointing and generating and that decoupling the two distributions might lead to a loss in performance
however a popular recent architecture proposed for machine translation takes an opposite approach
vaswani et al
propose a multi head attention mechanism which is able to learn multiple attention tribution over an input sequence
these attention mechanisms are merged and projected with a linear layer and can theoretically code a more varied representation of the input sequence compared to the regular attention mechanism
fan et al
are the first to use multi head attention with a parable model architecture for abstractive summarization
they show that multi head mechanisms are useful for summarization tasks and that different useful features are learnt by the different attention heads
this could be particularly useful for the generator since the distribution used by the pointer and the bution for the generator are identical in the original architecture
model evaluation
generated summaries will be compared against provided target summaries
the rouge score lin indicates the recall of overlapping n grams between the generated and target summary
using rouge as the evaluation metric is problematic as has been noted by dohare et al

not only because rouge scores do not correlate with human judgement but more tally because rouge can not evaluate restructured sentences in a proper way
rouge matches overlap in complete words and in reconstructed sentences different word forms can be used which might lead to low rouge scores
see et al
shows an example of a valid summary that has a rouge score of
krantz and kalita propose a new metric vert that pares similarity scores of sentences
since this method does not match exact word forms and it is to some extend robust to matical changes word reordering and sentence reconstructions
versatile evaluation of reduced texts vert is made up out of a similarity and dissimilarity sub score
a sentence vector is created out of the reference and created summaries and the cosine larity between these two vectors is measure of semantic similarity between the summaries
the dissimilarity sub score is calculated using the word mover distance algorithm that indicates how much a created summary has to change in order to match a reference mary
this new metric correlates stronger with human judgement compared to the commonly used rouge metric
both rouge and vert only measure the accuracy of the ated sentences with respect to the target summary but they provide no insight in the abstractiveness
to measure abstractiveness we use the proportion of new n grams in the generated summary
a low proportion of higher order n grams indicates that the model is copying long phrases from the input sequence and is thus ing in a more extractive way
improving both rouge and novel n grams seems like a contradiction since improving rouge will decrease the number of new n grams if the generated summary is not rephrased in the same way as the reference summary
directly improving novel n grams using policy learning
kryciski et al
optimize the rouge score directly
since the rouge ric is not differentiable this can only be done by using reinforcement techniques such as policy improvement
the loss function bines the maximum likelihood and rouge objective
in addition an abstractive reward is added to the loss
this reward is defined as the proportion of novel n grams in the generated summary
this metric has a bias towards very short summaries and needs to be normalised using the length ratio of the generated and ground truth summaries
they achieve similar rouge scores as but show that the number of new n grams increases significantly and thus is less extractive
methods in this section we describe our dataset
and
the baseline pointer generator network
then we introduce our extensions over the baseline network which comprises our multi head attention
pointing penalty losses
and pointer dropout mechanism


dataset we use the cnn dailymail dataset hermann et al

we use the same preprocessing and training splits as see et al
which in turn uses the method from nallapati et al

the training set consists of approximately training pairs with a validation set of thousand pairs and test set of thousand examples
the average article length is tokens and the summary length is on average tokens
sentences

pointer generator network the baseline model is the pointer generator network described by see et al

this model allows for copying of words from the source document using a pointing mechanism and also generation of novel words by selecting words from a fixed vocabulary
the main advantage of this approach over previous methods is that it allows the model to produce out of vocabulary words during summary generation
the basic architecture is a sequence to sequence attention model
words from the source document are fed sequentially into a single bidirectional lstm resulting in a sequence of encoder hidden states
the decoder is a single layer lstm that is initialised with the final hidden states of the encoder
more specifically a linear layer maps the final bidirectional hidden states to a fixed size output that represent the initial values of the decoder at the first time step
during the decoding at time step t an attention distribution is calculated over the source words et i vt wcct i bat t at here vt wh ws wc bat t are learnable parameters
st refers to the output of the decoder at time step t and hi is the representation of the word at position i produced by the encoder
ct i is the coverage vector
see et al
include a coverage mechanism in their model to reduce the amount of repetition
by taking into account the amount of attention that has been given to words from the source text in previous time steps they manage to significantly decrease repetition in the produced summaries
the coverage vector at time step t is just the sum of attention of the previous time steps ct t the attention distribution indicates which words from the source text are relevant to produce the next word of the summary
this information is stored in a fixed size representation called the context vector h t that is a weighted combination of the encoder hidden states a h t t i hi i based on the context vector and decoder hidden state a ity distribution is calculated over the fixed size vocabulary
the context vector and decoder hidden state are concatenated and sequently fed through two linear layers and a softmax to obtain a valid probability distribution over the vocabulary words called pvocab
pvocab h t here v v and b are the learnable parameters
a copy tribution over the source words is also required in order to select words from the source text during summary generation
see et al
decided to recycle the corresponding attention distribution and also made it serve as the pointing distribution
the probabilities of the words that occurred multiple times in the source text were summed
a trade off has to be made between copying a word with the help of the attention distribution and generating a word by the pvocab distribution
therefore a generation probability pen was introduced that acts as a soft switch
a pen of would mean that only words from the pvocab distribution can be used and none from the pointing distribution while a pen of has the opposite effect
t en p wt hh t wt s wt bpt r s wt h wt here wt bpt r are learnable parameters
xt refers to the input of the decoder at time step t and is the sigmoid function
for every document there is an extended vocabulary that is the union of the words in that document and all the words in fixed vocabulary
now the probability of a word in this extended vocabulary is defined as where ppoint pen pen pvocab w ppoint a t i i wi w the loss function at time step t is defined as t i losst t c t i i where the first term is the negative log likelihood of target word w and the second term is the coverage loss
this coverage loss is introduced to penalize repeated attention to the same words and is reweighted by a hyperparameter
the final loss function is defined as the average loss over all time steps loss t t t
dropout mechanism we propose a dropout mechanism on the pointer network to make the model less dependent on the pointer mechanism
generally dropout is a simple method to prevent overfitting in neural works by dropping parts of the network during training
with a predefined probability weights are set to zero during training
this ensures that the model can not rely on hidden co dependencies and generalises better
during evaluation the pointer generator model tends to rely to much on the pointer mechanism
the contribution of the generator network to the final output probability is on erage only
our pointer dropout method can be implemented by randomly setting pen with probability
to during ing where a value of makes the output distribution of the model the same as the output of the generator
we expect the model to rely less on the pointing mechanism and use the copy mechanism only when necessary
hopefully this would result in a model that generates more abstractive summaries

multihead attention in the original paper the pointer and the generator make use of the exact same attention distribution
in our opinion this is problematic because pointer and generator carry out different functions that require different underlying features
for example the generator might use syntactical features to create a correct sentence structure or point to multiple words to create a more abstract summary
in contrast the pointer only attends to words that it wants to copy to the summary
in order to both differentiate between pointer and generator attention distributions but still supply all information of the pointer mechanism to the generator we use a modification of the head attention mechanism
figure shows a schematic of our new pointer generator multi head attention mechanism where the first attention head is shared between the pointer and the generator whereas the generator receives all attention heads
this way by naively implemented the model could also minimize the cross tropy when ppoint is high
this would cause the model to attend to uncommon words while pointing which is not what the loss is supposed to achieve
to prevent this the gradient of this loss term is only back propagated to ppoint during training
experiments this section describes the experimental setup
that is shared between all of the models and also the different model variations
that are tested

experimental setup all the experiments follow the same setup as described by see et al

a fixed size vocabulary of
words is used for both the source and target words
all models use dimensional word embeddings that are learned during training time and hidden states for the pointer and generator are kept at a fixed size of
the input summaries are truncated to tokens during training and test time
the reason for this is that the most important words for the summary appear in the beginning of the articles and keeping longer source document even decreases performance
mary lengths are limited to tokens during training time and during test time in order to speed up training
summaries are generated using beam search and use a beam size of at test time
all the model parameters are optimised with adagrad using a learning rate of
and accumulator value of
as this proved to work best for see et al

gradients are clipped with a maximum norm value of and no further regularisation methods are used

model variations the baseline model is the pointer generator network described by see et al

the baseline is trained with and without coverage where the coverage mechanism is trained separately after epochs for iterations as including this from the beginning turns out to decrease performance
the multi head attention mechanism is tested with four heads
every head produces a context vector that is th of the size of the context vector of the baseline
next these context vector are concatenated into a single vector resulting in a vector of the same size that is independent on the number of heads
this is similar to the approach taken by vaswani et al

the probability for dropping out the pointer mechanism is set to

the decision to drop out the pointer holds for all the words during summary generation
this means that for some summaries the model can not rely on the pointing mechanism at all
the pointing losses are added at the end of training and for the same amount of iterations as the coverage loss
for both losses experiments were conducted with different scalars



as best performing scalars
for nloss and
for wploss were found
prior probabilities of the words are calculated on the rence in the entire training set
also only the prior probabilities for words that occur in the generation vocab are calculated
for the other words this probability is set to zero
every adaptation to the baseline model that is proposed in this work is tested as a separate addition to the baseline
this gives a clear estimation of the influence of each adaptation although it leaves out the influence figure a schematic of the pointer generator multi head attention with four attention heads where the first head is used as shared attention
introducing regularizations to the pointer mechanism we only affect the shared attention head while dedicating the rest of the attention heads to generator specific features

pointing losses see et al
show that the model exploits the pointing mechanism during evaluation
we hypothesise this is because pointing is easier than generating sentences
the model takes a shortcut by copying a lot of phrases and sentences in cases where this might not be essary
to discourage the network to count on the pointer network we add a term to the loss called the naive pointing loss
we add the sum of all pointing probabilities and weigh it with a ter
this way the model can still use the pointer network but it will directly contribute to a higher loss
for readability we define the pointer mechanism weight ppoint pen
lnaive t t p t point a disadvantage of this relatively simple penalty term could be that every word gets the same penalty even for words where ing is desired
we propose a second pointer penalty term the word prior pointing loss that only penalizes the pointer when a word is common in the vocabulary
w p p t point p t a w w x t a where p is the attention distribution used for the pointer mechanism and pw is a pre calculated word prior
as with the previous loss p is a hyper parameter to weigh the influence of the loss during training
t intuitively the cross entropy between the prior pw and p a expresses the surprisal of not pointing to a word given the word prior
if the prior is high and the word has a high weight in the attention distribution this loss term will be high
in any other case the loss term will be small
the desired behaviour of this loss term is that ppoint gets mized when attending to words with a high prior
however when of possible interactions that might occur
due to the time intensive training we use the same hyperparameters for all models
in order to test our results for statistical significance we form wilcoxon signed rank tests wilcoxon
all models without coverage are compared to the baseline whereas models with age are tested against the baseline coverage model
statistical tests for rouge rouge and rouge l are conducted
a p value of
is used to determine statistical significance
since the vert score correlates strongly with the rouge scores separate tests are not needed
the differences in novel n grams are generally much bigger and have lower variance than rouge scores so statistical tests are not needed for a large test set examples
results table mean rouge and vert scores of the tested els examples in testset
all models were trained from epoch on with coverage for steps
here nloss sponds to naive pointing loss and wploss to the word prior pointing loss
scores with a star are not significantly ent from the baseline
best results are marked in bold
attention heads model extensions rouge rouge rouge l vert dropout dropout baseline baseline see et al



































nloss nloss wploss wploss in table the average rouge scores are reported on all models and table shows the amount of novel n grams and sentences
these models are all trained with coverage results without coverage are included in appendix a
the obtained baseline rouge scores are on average points lower than reported in see et al
paper
however we use a pytorch re implementation and did not perform any eter tuning to optimize this score
as a reference to compare our models to we therefore use the baseline that we trained ourselves

com lipiji neural summ cnndm pytorch table percentage of novel n grams and sentences that are produced for each of the tested models
best results are marked in bold
attention heads model extensions grams grams grams grams sentences dropout dropout baseline target summaries












































nloss nloss wploss wploss on average multi head obtains slightly worse rouge scores
this can be a simple case of undertraining and sub optimal choice of hyperparameters
however we notice that in case of the word prior model the multi head architecture performs better
a possible explanation is that the decrease of weights in the pointer head might hurt the rouge score if the model mostly relies on the pointer and when the generator gets a more important task the multi head might be beneficial
this hypothesis needs more extensive training and tuning to prove and is not in the scope of this research
dropout on the single head model achieves slightly higher rouge scores and does increase n gram novelty
the multi head dropout model is not significantly different from the baseline
both proposed losses greatly improve the number of novel grams and sentences
this is especially noticeable in case of the word prior loss the number of novel n grams is more than double
however in both cases this increase in novel n grams decreases the rouge score
in example we can clearly observe that the model favours generating over pointing when predicting simple words like articles or prepositions
on less common words like names and uncommon nouns the pointer is still used

is the model pointing less table shows the average pen during train and test time which show how much the model uses the pointer mechanism on average
the baseline has a pen of
on average which is in line with the findings of see et al

the multihead does not change this behaviour
while the dropout model uses the generator cantly more during training during test time it falls back to the same value as the baseline
both loss functions greatly increase the amount the generator is used
this is to be expected when actively penalizing the pointer mechanism the pointer mechanism is used less at test time
table average value of pen during the end of training and test time
model baseline heads dropout nloss wploss train pen




test pen




the examples in appendix b show the pen value on each ated word for the baseline and two new losses
the model trained word prior loss shows that it achieves a much higher average pen on common words such as articles and verbs
in the first example only three fragments have a low pen ellie meredith down syndrome and let
the first two fragments are cases where we want the model to point whereas the third fragment is less clear
on inspection of the source article this fragment starts a direct quote from the article let s party like it s
table most frequent novel words for the best model heads coverage and word prior loss
says scored been he beat has diagnosed say found unk premier year said taken boss since by
novel words to investigate the novel words that are used further the most quent new words for the most abstractive model multihead erage and wploss is calculated tab

it can be noted that the majority of the words are verbs
when a sentence is rephrased it can happen that the same root of a verb is used but with another suffix
similarly when the tense of a verb changes this can introduce new words
this suggests that the newly introduced words are valid rephrases and not random words
note that the tokens and are the result of incorrect parsing of unk and s tokens
the average reference summary length is tokens
the length of the generated summaries is approximately for models with a single attention head and word prior loss and around for all other models including the baseline
for none of the models the generated summary was on average longer than the target maries
the maximum allowed length for generating summaries is
the new n grams are thus not a result of simply generating more text because the average length does not change
instead the novel n grams replace non novel n grams
this observation and the type of newly generated words as can be seen in table gests that the novel n grams are valid rephrasings and not random words or model artefacts
discussion
multi head evaluation the multi head attention mechanism improves the results of the new loss function in our measurements
the rouge l score creases by
nloss and
wploss but the novel n grams drop slightly for both losses
this shows that penalizing the pointer mechanism when the attention between pointer and generator is shared can reduce the overall quality of summaries which indicates the multi head mechanism is working as intended by splitting the pointer and generator attentions penalizing the pointer affects the generator less
figure shows the kl divergence between each head of the head attention where cell i j corresponds to dk
in the last column the kl divergence between the attention tributions of the multi head and the attention distribution of the same model with just one attention head is shown
we can read from the plot that the head used for the pointer head is on average very different from the other heads in the multi head
this means that on average they attend to different words in the source text and perform a different function when generating words
on the other hand the pointer head is most similar to the tion distribution in the single head model
additionally the other heads in the multi head are more similar to the single head than to eachother
this result indicates that the single head model tempts to incorporate information needed for both pointing and generating but that it can be desirable to split this information into multiple heads
figure the average kl divergence between each pair of heads of the multi head model and between each head of the multi head and the single head model last column

dropout models that used the dropout mechanism show an increased rouge score while having lowered amounts of novel n grams
this is the exact opposite of what was expected
a possible explanation is that dropout does not produce gradients that indicate that the ing mechanism is wrong but that it only slows its training
still it does force the generator to be more reliable and this leads to better scores in general
this is in line with the idea of see et al
that the generator might not be optimally trained because of over reliance on the pointer
using dropout is therefore very similar to the baseline model expect that dropout does give the generator the opportunity to be better trained

coverage ablation studies to understand the relationship between the coverage loss and our proposed losses we trained all models without the coverage nism
tables and in the appendix show that the rouge l score decreases about points for all models without using the coverage loss
further it can be seen that the number of novel n grams creases significantly
this matches our expectations for n grams which reflect the order of words
however we can also observe a decrease in novel grams which suggests that the coverage nism favours extractive summarization which stands in contrast to our goal towards a more abstractive model
this claim is supported by table which shows that introducing the coverage mechanism reduces the amount of repetition whereas both wploss and nloss result in more duplicate n grams
the losses thus interfere with coverage
coverage reduces repetition at the expense of more tractive summaries
penalizing the pointing mechanism introduces new n grams but also increases the problem of overgeneration
table duplicated n grams within summaries for head models where cov stands for models that have been trained with coverage
model extensions grams grams grams grams sentences baseline nloss wploss baseline nloss wploss





cov cov cov
























pointer and generator distributions when examining the examples produced by the model we notice that even with a high pen the model is copying full sentences
this effect can be seen in figure appendix b the last sentence is mostly made with the generator but is a copy of line in the source article
an issue of the pointing distribution is that it has a much lower cardinality compared to the generator distribution in our case it is two orders of magnitude smaller for the pointer for the generator
this means that there is generally a bias towards words from the source text even with high values of pen
we can conclude that the pointer mechanism by definition decreases the novel gram score and makes the model less abstractive
instead of learning the soft switch pen which introduces this bias a hard pointing mechanism could be learnt by reinforcement learning or with a gumbel softmax approximation to diminish this bias

rouge metric and dataset the aforementioned problems of rouge are a serious limitation to the evaluation of abstractive summarization models
the idea of abstractive summarization is that valid summaries can be created in many ways
however rouge measures the exact overlap between a set of references summaries and summaries created by our models
this means that either the set of reference summaries should be representative of all valid ways in which abstractive summaries can be produced or that there is a need for a new metric that does not rely on exact overlap but rather on semantic similarity
conclusion in this work we investigated several additions to the generator framework in order to improve abstraction while taining similar summary quality
while the multi head mechanism learns different features for pointing and generating it does not improve the rouge score
when dropout is used on the pointer mechanism the multi head attention does promote novel n grams in the produced summary but in other models the results are very similar
the dropout mechanism does the opposite of what was expected as the amount of novel n grams decreased while the rouge scores increased
it seems likely that dropout partly removes the reliance on the pointer and therefore gives improved performance compared to the baseline
the two introduced loss functions prove the generation of novel n grams significantly
for the word prior loss we observe an improvement of
more novel tences compared to the baseline model
however in both cases we did not manage to maintain the same rouge scores
this might be a problem with the loss functions but also with the training process
the vert metric looked like a promising metric to measure the semantic similarity between generated and target summaries
however the resulting vert scores are completely correlated with the rouge metric and do not produce any new insights
as our goal was to train a model that is more abstractive we used the number of novel n grams with respect to the reference summary as a measure
the novel n grams score can easily be increased by adding random words to the summary
this does not lead to more abstractive or higher quality summaries
since there is no metric to evaluate abstractive summarization tasks effectively it is not possible to claim that the summaries are more abstractive based on just the increase of new n grams
however we have shown that the summary length does not increase which suggests that new words replace other words instead of adding more words
we have also shown that the novel words are plausible words for rephrasing summarization tasks

future work it appears that the newly introduced losses interfere with the age mechanism and increase the over generation problem
it could be the case that by introducing the coverage and new loss at the same point in training produces gradients that are too different from training without these losses which would interfere with the vergence of the network
in future work the interaction between these new loss function and the coverage can be investigated
the difference in pointing behaviour between training and ference could be reduced by using scheduled teacher forcing which gradually decreases the frequency the model receives the ground truth as input to the generator
this reduces the difference between training and inference which could result in higher values of pen
references mehdi allahyari seyedamin pouriyeh mehdi assefi saeid safaei elizabeth d trippe juan b gutierrez and krys kochut

text summarization techniques a brief survey
arxiv preprint

tal baumel matan eyal and michael elhadad

query focused tive summarization incorporating query relevance multi document age and summary length constraints into models
arxiv preprint

samy bengio oriol vinyals navdeep jaitly and noam shazeer

scheduled sampling for sequence prediction with recurrent neural networks
in advances in neural information processing systems

shibhansh dohare harish karnick and vivek gupta

text summarization using abstract meaning representation
arxiv preprint

john duchi elad hazan and yoram singer

adaptive subgradient methods for online learning and stochastic optimization
j
mach
learn
res
july

acm
org citation

lisa fan dong yu and lu wang

robust neural abstractive tion systems and evaluation against adversarial information
arxiv preprint

jiatao gu zhengdong lu hang li and victor ok li

incorporating copying mechanism in sequence to sequence learning
arxiv preprint

karl moritz hermann tomas kocisky edward grefenstette lasse espeholt will kay mustafa suleyman and phil blunsom

teaching machines to read and comprehend
in advances in neural information processing systems

eric jang shixiang gu and ben poole

categorical reparameterization with gumbel softmax
arxiv preprint

hongyan jing

using hidden markov modeling to decompose written summaries
computational linguistics
diederik p
kingma and jimmy ba

adam a method for stochastic tion
corr



org
jacob krantz and jugal kalita

abstractive summarization using attentive neural techniques
arxiv preprint

wojciech kryciski romain paulus caiming xiong and richard socher

improving abstraction in text summarization
arxiv preprint

alex m lamb anirudh goyal alias parth goyal ying zhang saizheng zhang aaron c courville and yoshua bengio

professor forcing a new algorithm for training recurrent networks
in advances in neural information processing systems

piji li wai lam lidong bing and zihao wang

deep recurrent generative decoder for abstractive text summarization
arxiv preprint

chin yew lin

of summaries
rouge a package for automatic evaluation of rouge a package for automatic evaluation
microsoft
com en us research ramesh nallapati bowen zhou caglar gulcehre bing xiang al

stractive text summarization using sequence to sequence rnns and beyond
arxiv preprint

abigail see peter j liu and christopher d manning

get to the point summarization with pointer generator networks
arxiv preprint

kaiqiang song lin zhao and fei liu

structure infused copy mechanisms for abstractive summarization
arxiv preprint

nitish srivastava geoffrey hinton alex krizhevsky ilya sutskever and ruslan salakhutdinov

dropout a simple way to prevent neural networks from overfitting
the journal of machine learning research
ilya sutskever oriol vinyals and quoc v le

sequence to sequence learning with neural networks
in advances in neural information processing systems

ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan n gomez ukasz kaiser and illia polosukhin

attention is all you need
in advances in neural information processing systems

oriol vinyals meire fortunato and navdeep jaitly

pointer networks
in advances in neural information processing systems

noah weber leena shekhar niranjan balasubramanian and kyunghyun cho

controlling decoding for more abstractive summaries with copy based networks
arxiv preprint

frank wilcoxon

individual comparisons by ranking methods
biometrics bulletin
appendices a scores without coverage table mean rouge and vert scores of the tested models examples in testset
all models were trained without coverage
here nloss corresponds to naive pointing loss and wploss to the word prior pointing loss
attention heads model extensions rouge rouge rouge l vert baseline see et al
dropout dropout nloss nloss wploss wploss


































table percentage of novel n grams and sentences that are produced for each of the tested models
attention heads model extensions grams grams grams grams sentences dropout dropout target summaries












































nloss nloss wploss wploss b examples the following pages contain randomly chosen examples from the multihead model with coverage combined with the new losses
the words highlighted in red reflect the overall attention the model paid to a word during the constructing the summary
italic words denote out of vocabulary words
the green shading intensity represents the value of the generation probability pen
figure in this example the average pen is much higher in the word prior loss model except on words with a low prior ellie meredith down syndrome
figure this example shows clearly that the new losses re introduce the over generation problems that the coverage loss aimed to solve
figure another example
while the average pen of each word is significantly higher in the wploss model the generated summaries are the same
figure the last sentence in the wploss example on average mostly uses the generator but is an exact copy of line in the source article

