few shot learning for opinion summarization arthur mirella ivan university of edinburgh university of amsterdam
ac
uk mlap
ed
ac
t c o g l
s c v
v i x r a abstract opinion summarization is the automatic ation of text reecting subjective information expressed in multiple documents such as user reviews of a product
the task is practically important and has attracted a lot of attention
however due to the high cost of summary duction datasets large enough for training pervised models are lacking
instead the task has been traditionally approached with tive methods that learn to select text fragments in an unsupervised or weakly supervised way
recently it has been shown that abstractive summaries potentially more uent and better at reecting conicting information can also be produced in an unsupervised fashion
ever these models not being exposed to actual summaries fail to capture their essential erties
in this work we show that even a ful of summaries is sufcient to bootstrap eration of the summary text with all expected properties such as writing style ness uency and sentiment preservation
we start by training a conditional transformer guage model to generate a new product review given other available reviews of the product
the model is also conditioned on review erties that are directly related to summaries the properties are derived from reviews with in the second stage we no manual effort
ne tune a plug in module that learns to dict property values on a handful of summaries
this lets us switch the generator to the rization mode
we show on amazon and yelp datasets that our approach substantially forms previous extractive and abstractive ods in automatic and human evaluation
introduction summarization of user opinions expressed in line resources such as blogs reviews social media or internet forums has drawn much attention due to its potential for various information access cations such as creating digests search and report gold ours reviews these shoes run true to size do a good job supporting the arch of the foot and are well suited for exercise
they re good looking comfortable and the sole feels soft and cushioned
overall they are a nice light weight pair of shoes and come in a variety of stylish colors
these running shoes are great they t true to size and are very comfortable to run around in
they are light weight and have great support
they run a little on the narrow side so make sure to order a half size larger than normal
perfect t for me


supply the support that i need


are exible and


it is very comfortable


able





i enjoy wearing them running


running shoes


felt great right out of the box


they run true to size





my feet and feel like a dream


tally light weight





shoes run small


more true to size


t is great


supports my arch very well





they are lightweight


usually wear a size women s


ordered a
and the t is great table example summaries produced by our system and an annotator colors encode its alignment to the input reviews
the reviews are truncated and delimited with the symbol
generation hu and liu medhat et al
angelidis and lapata
although signicant progress has been observed in supervised summarization in non subjective single document context such as news articles rush et al
nallapati et al
paulus et al
see et al
liu et al
ern deep learning methods rely on large amounts of annotated data that are not readily available in the opinion summarization domain and expensive to produce
a key obstacle making data annotation expensive is that annotators need to consider ple input texts when writing a summary which is time consuming
moreover annotation would have to be undertaken for multiple domains as online views are inherently multi domain blitzer al
and summarization systems can be sensitive isonuma et al

this suggests that it is unlikely that human annotated corpora large enough for training deep models will be available
recently a number of unsupervised tive multi document models were introduced e

copycat brazinskas et al
and meansum chu and liu that are trained on large tions of unannotated product reviews
however unsurprisingly perhaps since the models are not exposed to the actual summaries they are unable to learn their key characteristics
for instance sum chu and liu is prone to producing summaries that contain a signicant amount of formation that is unsupported by reviews cat generates summaries that are better aligned with reviews yet they are limited in detail
over both systems are trained mostly on tively written reviews and as a result tend to erate summaries in the same writing style
the main challenge in the absence of large tated corpora lies in successful utilization of scarce annotated resources
unlike recent approaches to language model adaptation for abstractive document summarization hoang et al
fel et al
that utilize hundreds of thousands of summaries our two annotated datasets consist of only and annotated data points
it was also observed that a naive ne tuning of multi million parameter models on small corpora leads to rapid and poor generalization vinyals et al
finn et al

in this light we propose a few shot learning framework and demonstrate that even a tiny number of annotated instances is sufcient to bootstrap generation of the formal mary text that is both informative and uent see table
to the best of our knowledge this work is the rst few shot learning approach applied to summarization
in our work we observe that reviews in a large unannotated collection vary a lot for example they differ in style the level of detail or how much they diverge from other reviews of the product in terms of content and overall sentiment
we refer to vidual review characteristics and their relations to other reviews as properties ficler and goldberg
while reviews span a large range of property values only a subset of them is appropriate for maries
for example summaries should be close to the product s reviews in content avoid using the rst person pronouns and agree with the reviews in sentiment
our approach starts with ing a property aware model on a large collection of reviews and then adapts the model using a few annotator created summaries effectively switching the generator to the summarization regime
as we demonstrate in our experiments the summaries do not even have to come from the same domain
more formally we estimate a text model on a dataset of reviews the generator is a former conditional language model clm that is trained with a leave one out objective besag brazinskas et al
by attending to other reviews of the product
we dene properties of unannotated data that are directly related to the end task of summarization
those properties are easy to derive from reviews and no extra annotation fort is required
the clm is conditioned on these properties in training
the properties encode partial information about the target review that is being predicted
we capitalize on that by ne tuning parts of the model jointly with a tiny plug in work on a handful of human written summaries
the plug in network is trained to output property values that make the summaries likely under the trained clm
the plug in has less than half a cent of the original model s parameters and thus is less prone to on small datasets
theless it can successfully learn to control ics of a large clm by providing property values that force generation of summaries
we shall refer to the model produced using the procedure as few shot summarizer fewsum
we evaluate our model against both extractive and abstractive methods on amazon and yelp human created summaries
summaries generated by our model are substantially better than those produced by competing methods as measured by automatic and human evaluation metrics on both datasets
finally we show that it allows for cessful cross domain adaption
our contributions can be summarized as follows we introduce the rst few shot learning work for abstractive opinion summarization simplicity we use the term product to refer to both amazon products and yelp businesses
we demonstrate that the approach tially outperforms extractive and abstractive models both when measured with automatic metrics and in human evaluation we release datasets with abstractive maries for amazon products and yelp nesses
unsupervised training user reviews about an entity e

a product are naturally inter dependent
for example knowing that most reviews are negative about a product s battery life it becomes more likely that the next review will also be negative about it
to model inter dependencies yet to avoid intractabilities sociated with undirected graphical models koller and friedman we use the leave one out ting besag brazinskas et al

specically we assume access to a large pus of user text reviews which are arranged as m groups n m where n are reviews about a particular product that are arranged as a get review ri and n source reviews ri





rn
our goal is to estimate the conditional distribution by optimizing the parameters as shown in eq

arg max log i i m n m n m n m n arg max log i i our model has an encoder generator former architecture vaswani et al
where the encoder e produces contextual representations of ri that are attended by the generator g which in turn is a conditional language model ing the target review ri estimated using forcing williams and zipser
an tion is presented in fig

the objective lets the model exploit common formation across reviews such as rare brand names or aspect mentions
for example in fig
the generator can directly attend to the word vacuum in the source reviews to increase its prediction bility
additionally we condition on partial tion about the target review ri using an oracle the code and datasets are available at github
com abrazinskas fewsum ri as shown in eq

n m m n log i i i rj i we refer to this partial information as properties ficler and goldberg which correspond to text characteristics of ri or relations between ri and ri
for example one such property can be the rouge score lin between ri and ri which indicates the degree of overlap between ri and ri
in fig
a high rouge value can signal to the generator to attend the word vacuum in the source reviews instead of predicting it based on language statistics
intuitively while the model serves a wide distribution of rouge scores during training on reviews during summarization in test time we can achieve a high degree of input output text overlap by setting the property to a high value
we considered properties that are listed below
content coverage and rouge l scores between ri and ri signal to g how much to rely on syntactic information in ri during prediction of ri
writing style as a proxy for formal and informal writing styles we compute pronoun counts and create a distribution over three points of view
we also added an ditional class for cases with no pronouns see pendix
for details and examples rating tion we compute the difference between the ri s rating and the average ri rating length deviation we similarly compute the difference between the ri s length and the average length of ri in terms of tokens

novelty reduction while summary and review generation are cally similar there is an important difference that needs to be addressed
reviews are often very diverse so when a review is predicted the erator often needs to predict content that is not present in source reviews
on the other hand when a summary is predicted its semantic content ways matches the content of the source reviews
to address this discrepancy in addition to using the rouge scores as was explained previously we introduce a novelty reduction technique which is similar to label smoothing pereyra et al

specically we add a regularization term l scaled by that is applied to word distributions figure illustration of the fewsum model that uses the leave one out objective
here predictions of the target review ri is performed by conditioning on the encoded source reviews i
the generator attends the last encoder layer s output to extract common information in red
additionally the generator has partial information about ri passed by the oracle r
produced by the generator g as shown in eq

m n m n log i i i rj i i i i rj i it penalizes assigning the probability mass to words not appearing in ri as shown in eq
and thus steers towards generation of text that is more grounded in content of ri
ri t ri i ri ri here t is the length of ri and the inner sum is over all words that do not appear in the word cabulary of ri
intuitively in fig
the penalty could reduce the probability of the word hoover to be predicted as it does not appear in the source reviews
summary adaptation k once the unsupervised model is trained on reviews our task is to adapt it to generation of summaries
here we assume access to a small number of annotator written summaries sk rk where s is a summary for n input reviews
as we will show in sec

naive ne tuning of the vised model on a handful of annotated data points leads to poor generalization
instead we ize on the fact that the generator g has observed a wide range of property values associated with ri during the unsupervised training phase
itively some combinations of property values drive it into generation of text that has qualities of a mary while others of a review
however we might not know values in advance that are necessary for generation of summaries
furthermore ri can not be applied at test time as it requires access to target texts
in the following section we scribe a solution that switches the generator to the summarization mode relying only on input reviews

plug in network we start by introducing a parametrized plug in work that yields the same types of erties as ri
from a practical perspective the plug in should be input permutation invariant and allow for an arbitrary number of input reviews zaheer et al

importantly the trainable plug in can have a marginal fraction of the main model s parameters which makes it less prone to when trained on small datasets
we initialize the parameters of by matching its output to ri on the unannotated reviews
specically we used a weighted combination of distances as shown for one group of reviews in eq

n l here is a distance for the property l and l is an associated weight
specically we used norm for content age rating and length deviations and leibler divergence for writing style
for the plug in network we employed a very sturdy vacuum ri latexit ri latexit latexit ou latexit ou great vacuum rn latexit wczcevt rn latexit wczcevt encoder statesoracle this vacuumhooverproductgenerator attention layer feed forward network with multi head tion modules over the encoded states of the source reviews at each layer followed by a linear mation predicting property values
note that the encoder is shared with the main model

fine tuning unsurprisingly perhaps the network p being tialized on unannotated reviews inherits a strong bias towards outputting property values resulting in generation of reviews which should not be propriate for generating summaries
fortunately due to the simplicity of the chosen properties it is possible to ne tune p to match the output of q on the annotated data sk rk n k an alternative is to optimize the plug in to rectly increase the likelihood of summaries under g while keeping all other parameters xed
using eq

as the generator is trained on unannotated views it might not encounter a sufcient amount of text that is written as a summary and that highly overlaps in content with the input reviews
we dress that by unfreezing the attention module of g over input reviews and the plug in p and by maximizing the likelihood of summaries k k log n n this allows the system to learn an interaction tween g and p
for example what property values are better associated with summaries and how g should better respond to them
experimental setup
dataset for training we used customer reviews from zon he and mcauley and yelp
from the amazon reviews we selected categories tronics clothing shoes and jewelry home and kitchen health and personal care
we used a ilar pre processing schema as in brazinskas et al
details are presented in appendix

for training we partitioned business product reviews to the groups of reviews by sampling without replacement
thus for unsupervised training in sec
we conditioned on reviews for each target review
the data statistics are shown in table
explored that option and observed that it works larly yet leads to a slightly worse result

yelp
com challenge dataset yelp amazon training validation table data statistics after pre processing
the format in the cells is businesses reviews and ucts reviews for yelp and amazon respectively
we obtained human written summaries for amazon and for yelp for reviews each using amazon mechanical turk amt
each uct business received summaries and averaged rouge scores are reported in the following tions
also we reserved approximately for ing and the rest for training and validation
the details are in appendix


experimental details for the main model we used the transformer tecture vaswani et al
with trainable length embeddings and shared parameters between the coder and generator raffel et al

subwords were obtained with bpe sennrich et al
using merges
subword embeddings were shared across the model as a form of regularization press and wolf
for a fair comparison we approximately matched the number of parameters to copycat brazinskas et al

we domly initialized all parameters with glorot rot and bengio
for the plug in network we employed a multi layer feed forward network with multi head attention modules over encoded states of the source review
after the last layer we performed a linear projection to compute property values
further parameter optimization was formed using adam kingma and ba and beam search with n gram blocking paulus et al
was applied to our model and copycat for summary generation
all experiments were conducted on geforce rtx ti

hyperparameters our parameter shared encoder generator model used a head and layer transformer stack
dropout in sub layers and subword embeddings dropout was both set to
and we used dimensional position wise feed forward neural works
we set subword and length embeddings to and respectively and both were nated to be used as input
for the plug in network we set the output dimension to and internal forward network hidden dimensions to
we used a stack of layers and the attention modules with heads at each layer
we applied
internal dropout and
attention dropout
property values duced by the plug in or oracle were concatenated with subword and length embeddings and linearly projected before being passed to the generator
in total our model had approximately m ters while the plug in network only k i
e
less than
of the main model s parameters
in all experiments the hyperparameter tuning was performed based on the rouge l score on yelp and amazon validation sets

baselines lexrank erkan and radev is an vised extractive graph based algorithm selecting sentences based on graph centrality
sentences resent nodes in a graph whose edges have weights denoting similarity computed with tf idf
meansum is an unsupervised abstractive marization model chu and liu that treats a summary as a discrete latent state of an coder
the model is trained in a multi task fashion with two objectives one for prediction of reviews and the other one for summary reviews alignment in the semantic space using the cosine similarity
copycat is the state of the art unsupervised abstractive summarizer brazinskas et al
that uses continuous latent representations to model review groups and individual review semantics
it has an implicit mechanism for novelty reduction and uses a copy mechanism
as is common in the summarization literature we also employed a number of simple tion baselines
first the clustroid review was computed for each group of reviews as follows
we took each review from a group and computed rouge l with respect to all other reviews
the review with the highest rouge score was selected as the clustroid review
second we sampled a random review from each group to be used as the summary
third we constructed the summary by selecting the leading sentences lead from each review of a group
evaluation results automatic evaluation we report rouge score lin based evaluation results on the amazon and yelp test sets in tables and tively
the results indicate that our model forms abstractive and extractive methods on both table rouge scores on the amazon test set

fewsum copycat
meansum

lexrank
clustroid
lead
random
fewsum copycat
meansum

lexrank
clustroid
lead
random













rl






rl






table rouge scores on the yelp test set
datasets
also the results are supported by tive improvements over other models see examples in the appendix
best worst scaling we performed human uation with the best worst scaling louviere and woodworth louviere et al
itchenko and mohammad on the amazon and yelp test sets using the amt platform
we assigned multiple workers to each tuple containing summaries from copycat our model lexrank and human annotators
the judgment criteria were the following fluency coherence redundancy informativeness sentiment
details are provided in appendix

for every criterion a system s score is computed as the percentage of times it was selected as best minus the percentage of times it was selected as worst orme
the scores range from unanimously worst to unanimously best
the results are presented in tables and for amazon and yelp respectively
on the amazon data they indicate that our model is preferred across the board over the baselines
copycat is preferred over lexrank in terms of uency and non redundancy yet it shows worse results in terms of informativeness and overall sentiment preservation
in the same vein on yelp in table our model outperforms the other models
all pairwise differences between our model and other models are statistically signicant at fluency coherence non redundancy informativeness sentiment fewsum

copycat
lexrank
gold



fewsum

copycat
lexrank
gold



























table human evaluation results in terms of the best worst scaling on the amazon test set
fluency coherence non redundancy informativeness sentiment table human evaluation results in terms of the best worst scaling on the yelp test set
fewsum copycat full partial no





table content support on the amazon test set

using post hoc hd tukey tests
the only exception is non redundency on yelp when comparing our model and copycat where our model shows a slightly lower score
content support as was observed by falke et al
tay et al
brazinskas et al
the rouge metric can be insensitive to lucinating facts and entities
we also investigated how well generated text is supported by input views
we split summaries generated by our model and copycat into sentences
then for each mary sentence we hired amt workers to judge how well content of the sentence is supported by the reviews
three following options were able
full support all the content is reected in the reviews partial support only some content is reected in the reviews no support content is not reected in the reviews
the results are presented in table
despite not using the copy mechanism that is benecial for fact preservation falke et al
and tion of more diverse and detailed summaries see appendix we score on par with copycat
analysis
alternative adaptation strategies we further explored alternative utilization proaches of annotated data points based on the same split of the amazon summaries as explained in sec


first we trained a model in an pervised learning setting usl on the amazon views with the leave one out objective in eq

in this setting the model has neither exposure to maries nor the properties as the oracle ri is not used
further we considered two alternative settings how the pre trained unsupervised model can be adapted on the gold summaries
in the rst setting the model is ne tuned by predicting maries conditioned on input reviews
in the second one similar to hoang et al
we performed adaptation in a multi tasking learning mtl fashion
here usl is further trained on a mixture of unannotated review and gold summary batches with a trainable embedding dicating the task
the results are presented in table
first we observed that usl generates maries that get the worst rouge scores
tionally the generated text tends to be informal and substantially shorter than an average summary we shall discuss that in sec


second when the model is ne tuned on the gold summaries it noticeably improves the results yet they are substantially worse than of our proposed few shot approach
it can be explained by strong inuence of the unannotated data stored in lions of parameters that requires more annotated data points to overrule
finally we observed that mtl fails to decouple the tasks indicated by only a slight improvement over usl
observed that the review summary proportion works the best
fewsum

mtl

usl random





rl




table rouge scores on the amazon test set for alternative summary adaptation strategies
fewsum gold
fewsum

usl

reviews




nopr









len




gold these shoes run true to size do a good job supporting the arch of the foot and are well suited for exercise
they re good looking comfortable and the sole feels soft and cushioned
overall they are a nice light weight pair of shoes and come in a variety of stylish colors
these running shoes are great they t true to size and are very comfortable to run around in
they are light weight and have great support
they run a little on the narrow side so make sure to order a half size larger than normal
this is my second pair of reebok ning shoes and they are the best ning shoes i have ever owned
they are lightweight comfortable and provide great support for my feet
this is my second pair of reebok ning shoes and i love them
they are the most comfortable shoes i have ever worn
table text characteristics of generated summaries by different models on the amazon test set
usl
inuence of unannotated data we further analyzed how plain ne tuning on maries differs from our approach in terms of turing summary characteristics
for comparison we used usl and which are presented in sec


additionally we analyzed unannotated reviews from the amazon training set
specically we focused on text formality and the average word count difference len from the gold summaries in the amazon test set
as a proxy for the former we computed the marginal distribution over points of view pov based on pronoun counts an ditional class nopr was allocated to cases of no pronouns
the results are presented in table
first we observed that the training reviews are largely informal
and
for and pov respectively
unsurprisingly the model trained only on the reviews usl transfers a lar trait to the summaries that it generates
on the contrary the gold summaries are largely formal indicated by a complete absence of the and a marginal amount of pov pronouns
also an average review is substantially shorter than an age gold summary and consequently the generated summaries by usl are also shorter
example maries are presented in table
further we investigated how well adapts to the summary characteristics by being beam search attempting to nd the most likely didate sequence was utilized opposed to a random sequence sampling we observed that generated sequences had no cases of the pov pronouns and complete absence of pronouns nopr
table example summaries produced by models with different adaptation approaches
in domain cross domain domain cloth electronics health home avg









table in and cross domain experiments on the zon dataset rouge l scores are reported
tually ne tuned on them
indeed we observed that starts to shift in the direction of the maries by reducing the pronouns of the pov and increasing the average summary length
theless the gap is still wide which would probably require more data to be bridged
finally we served that our approach adapts much better to the desired characteristics by producing well formed summary text that is also very close in length to the gold summaries

cross domain we hypothesized that on a small dataset the model primarily learns course grained features such as common writing phrases and their correlations tween input reviews and summaries
also that they in principle could be learned from remotely lated domains
we investigated that by ne tuning the model on summaries that are not in the get domain of the amazon dataset
specically pipeline framework for abstractive summary eration
our conditioning on text properties proach is similar to ficler and goldberg yet we rely on automatically derived properties that associate a target to source and learn a separate module to generate their combinations
moreover their method has not been studied in the context of summarization
conclusions in this work we introduce the rst to our edge few shot framework for abstractive opinion summarization
we show that it can efciently lize even a handful of annotated reviews summary pairs to train models that generate uent tive and overall sentiment reecting summaries
we propose to exploit summary related properties in unannotated reviews that are used for vised training of a generator
then we train a tiny plug in network that learns to switch the generator to the summarization regime
we demonstrate that our approach substantially outperforms competitive ones both abstractive and extractive in human and automatic evaluation
finally we show that it also allows for successful cross domain adaptation
acknowledgments we would like to thank members of edinburgh nlp group for discussion and the anonymous reviewers for their valuable feedback
we gratefully edge the support of the european research council titov erc stg broadsem lapata erc cog transmodal and the dutch national science foundation nwo vidi


we matched data point count for domains of training and validation sets to the in domain zon data experiment presented in sec the test set remained the same for each domain as in the in domain experiment
then we ne tuned the same model times with different seeds per target domain
for comparison we used the in domain model which was used in amazon experiments in sec

we computed the average rouge l score per target domain where overall was

the results are reported in table
the results indicate that the models perform par on most of the domains supporting the sis
on the other hand the in domain model shows substantially better results on the health domain which is expected as intuitively this domain is the most different from the rest
related work extractive weakly supervised opinion rization has been an active area of research
lexrank erkan and radev is an pervised extractive model
opinosis ganesan et al
does not use any supervision and relies on pos tags and redundancies to generate short opinions
however this approach is not well suited for the generation of coherent long summaries and although it can recombine fragments of input text it can not generate novel words and phrases
other earlier approaches gerani et al
di brizio et al
relied on text planners and plates which restrict the output text
a more cent extractive method of angelidis and lapata frames the problem as a pipeline of steps with different models for each step
isonuma et al
introduce an unsupervised approach for gle product review summarization where they rely on latent discourse trees
the most related pervised approach to this work is our own work copycat brazinskas et al

unlike that work we rely on a powerful generator to learn ditional spaces of text without hierarchical latent variables
finally in contract to meansum chu and liu our model relies on inductive ases without explicitly modeling of summaries
a concurrent model denoisesum amplayo and pata uses a syntactically generated dataset of source reviews to train a generator to denoise and distill common information
another lel work opiniondigest suhara et al
considers controllable opinion aggregation and is a references reinald kim amplayo and mirella lapata

supervised opinion summarization with noising and denoising
proceedings of association for tional linguistics acl
stefanos angelidis and mirella lapata

marizing opinions aspect extraction meets ment prediction and they are both weakly supervised
in proceedings of the conference on cal methods in natural language processing pages
julian besag

statistical analysis of non lattice data
journal of the royal statistical society series d the statistician
deep networks
in proceedings of the tional conference on machine learning volume pages
jmlr
org
kavita ganesan chengxiang zhai and jiawei han

opinosis a graph based approach to tive summarization of highly redundant opinions
in proceedings of the international conference on computational linguistics coling pages
shima gerani yashar mehdad giuseppe carenini raymond t ng and bita nejat

abstractive summarization of product reviews using discourse structure
in proceedings of the conference on empirical methods in natural language processing emnlp pages
john blitzer mark dredze and fernando pereira

biographies bollywood boom boxes and blenders in domain adaptation for sentiment classication
proceedings of the annual meeting of the ciation of computational linguistics pages
xavier glorot and yoshua bengio

ing the difculty of training deep feedforward neural networks
in proceedings of the thirteenth tional conference on articial intelligence and tics pages
arthur brazinskas mirella lapata and ivan titov

unsupervised opinion summarization as copycat review generation
in proceedings of ciation for computational linguistics acl
eric chu and peter liu

meansum a ral model for unsupervised multi document tive summarization
in proceedings of international conference on machine learning icml pages
hoa trang dang

overview of duc
in ceedings of the document understanding conference volume pages
giuseppe di fabbrizio amanda stent and robert gaizauskas

a hybrid approach to document summarization of opinions in reviews
pages
gunes erkan and dragomir r radev

lexrank graph based lexical centrality as salience in text summarization
journal of articial intelligence search
tobias falke leonardo fr ribeiro prasetya ajie utama ido dagan and iryna gurevych

ranking generated summaries by correctness an teresting but challenging application for natural guage inference
in proceedings of the annual meeting of the association for computational guistics pages
jessica ficler and yoav goldberg

controlling linguistic style aspects in neural language in proceedings of the workshop on tion
tic variation pages copenhagen denmark
association for computational linguistics
chelsea finn pieter abbeel and sergey levine

model agnostic meta learning for fast adaptation of ruining he and julian mcauley

ups and downs modeling the visual evolution of fashion trends with one class collaborative ltering
in proceedings of the international conference on world wide web pages
andrew hoang antoine bosselut asli celikyilmaz and yejin choi

efcient adaptation of trained transformers for abstractive summarization
arxiv preprint

minqing hu and bing liu

mining and rizing customer reviews
in proceedings of the tenth acm sigkdd international conference on edge discovery and data mining pages
acm
masaru isonuma toru fujino junichiro mori yutaka matsuo and ichiro sakata

extractive marization using multi task learning with document in proceedings of the classication
ence on empirical methods in natural language processing pages
masaru isonuma junichiro mori and ichiro sakata

unsupervised neural single document marization of reviews via learning latent discourse structure and its ranking
in proceedings of tion for computational linguistics acl
diederik p kingma and jimmy ba

adam a method for stochastic optimization
arxiv preprint

svetlana kiritchenko and saif m mohammad

capturing reliable ne grained sentiment tions by crowdsourcing and best worst scaling
in proceedings of the conference of the north american chapter of the association for tional linguistics human language technologies pages
abigail see peter j liu and christopher d manning

get to the point summarization with generator networks
in proceedings of association for computational linguistics acl
rico sennrich barry haddow and alexandra birch

neural machine translation of rare words with subword units
proceedings of association for putational linguistics acl
yoshihiko suhara xiaolan wang stefanos angelidis and wang chiew tan

opiniondigest a ple framework for opinion summarization
ings of association for computational linguistics acl
wenyi tay aditya joshi xiuzhen jenny zhang naz karimi and stephen wan

red faced rouge examining the suitability of rouge for ion summary evaluation
in proceedings of the the annual workshop of the australasian language technology association pages
ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan n gomez ukasz kaiser and illia polosukhin

attention is all in advances in neural information you need
cessing systems pages
oriol vinyals charles blundell timothy lillicrap daan wierstra al

matching networks for in advances in neural one shot learning
tion processing systems pages
ronald j williams and david zipser

a ing algorithm for continually running fully recurrent neural networks
neural computation
manzil zaheer satwik kottur siamak ravanbakhsh barnabas poczos russ r salakhutdinov and alexander j smola

deep sets
in advances in neural information processing systems pages
daphne koller and nir friedman

probabilistic graphical models principles and techniques
mit press
chin yew lin

rouge a package for matic evaluation of summaries acl
in proceedings of workshop on text summarization branches out post conference workshop of acl pages
peter j liu mohammad saleh etienne pot ben goodrich ryan sepassi lukasz kaiser and noam shazeer

generating wikipedia by ing long sequences
in proceedings of international conference on learning representations iclr
jordan j louviere terry n flynn and anthony fred john marley

best worst scaling ory methods and applications
cambridge sity press
jordan j louviere and george g woodworth

best worst scaling a model for the largest ence judgments
university of alberta working per
walaa medhat ahmed hassan and hoda korashy

sentiment analysis algorithms and tions a survey
ain shams engineering journal
ramesh nallapati bowen zhou cicero dos santos caglar gulcehre and bing xiang

tive text summarization using sequence to sequence in proceedings of the rnns and beyond
signll conference on computational natural guage learning pages
bryan orme

maxdiff analysis simple counting individual level logit and hb
sequim wa tooth software
romain paulus caiming xiong and richard socher

a deep reinforced model for abstractive marization
arxiv preprint

gabriel pereyra george tucker jan chorowski ukasz kaiser and geoffrey hinton

izing neural networks by penalizing condent output distributions
arxiv preprint

or press and lior wolf

using the output in bedding to improve language models
ings of the conference of the european ter of the association for computational linguistics pages
colin raffel noam shazeer adam roberts katherine lee sharan narang michael matena yanqi zhou wei li and peter j liu

exploring the limits of transfer learning with a unied text to text former
arxiv preprint

alexander m rush sumit chopra and jason weston

a neural attention model for abstractive in proceedings of the tence summarization
conference on empirical methods in natural guage processing pages
appendices
summary annotation
dataset pre processing we selected only amazon products and yelp nesses with minimum of reviews and the imum and maximum lengths of and words respectively
also popular products businesses above the percentile were removed
from each business product we sampled reviews out replacement to form groups of reviews

evaluation data split from the amazon annotated dataset we used products for training validation and testing respectively
on yelp we used for ing validation and testing respectively
both the automatic and human evaluation experiments were performed on the test sets

training procedure first to speed up the training phase we trained an unconditional language model for epoch on the amazon reviews with the learning rate lr set to
on yelp we trained it for epochs with lr set to
the language model was used to initialize both the encoder and generator of the main model
subsequently we trained the model using eq
for epochs on the amazon reviews with lr and for epochs with lr set to
additionally we reduced novelty using eq
by training the model further for epoch with lr and on amazon on yelp we trained for epochs with lr and

for the plugin network s initialization as plained in sec

we performed optimization by output matching with the oracle for epochs on the unannotated amazon reviews with lr
on yelp we trained for epochs with lastly we ne tuned the plugin network on the human written summaries by output matching with the
on the amazon data for epochs with and for epochs with on yelp
we set weights to



for length ation rating deviation pov and rouge scores respectively
then ne tuned the attention part of the model and the plug in network jointly for epochs with on the amazon data
and epochs with lr on yelp
set rating deviation to as summaries do not have associated human annotated ratings
for summary annotation we reused amazon products from brazinskas et al
and sampled businesses from yelp
we assigned ical turk workers to each product business and instructed them to read the reviews and produce a summary text
we used the following instructions the summary should reect user common opinions expressed in the reviews
try to serve the common sentiment of the opinions and their details e

what exactly the users like or dislike
for example if most reviews are negative about the sound quality then also write negatively about it
please make the summary coherent and uent in terms of sentence and information ture
iterate over the written summary tiple times to improve it and re read the views whenever necessary
the summary should not look like a review please write formally
keep the length of the summary reasonably close to the average length of the reviews
please try to write the summary using your own words instead of copying text directly from the reviews
using the exact words from the reviews is allowed but do not copy more than consecutive words from a review

human evaluation setup to perform the human evaluation experiments scribed in sec we hired workers with proval rate hits location usa uk canada and the maximum score on a tion test that we had designed
the test was asking if the workers were native english speakers and was verifying that they correctly understood the instructions of both the best worst scaling and tent support tasks

best worst scaling details we performed human evaluation based on the zon and yelp test sets using the amt platform
we assigned workers to each tuple containing maries from copycat our model lexrank and human annotators
due to dataset size ferences we assigned and workers to each written informally populated by pronouns such as i and you
in contrast summaries are desirable to be written formally
in this work we observed that a surprisingly simple way to achieve that is to condition the generator on the distribution over pronoun classes of the target review
we computed pronoun counts and produced the class tions person pov and other in case if no pronouns are present
consider the example sentences in table
here one can observe that the sentences of ferent pronoun classes differ in the style of writing and often the intention of the message pov sentences tend to provide clues about the personal experience of the user pov sentences on the other hand commonly convey recommendations to a reader pov and other sentences often describe aspects and their associated opinions
i bought this as a gift for my husband
i ve been using drakkar noir balm for over twenty years
i purchased these for my son as a kind of a joke
this is the best product you can buy you get what you pay for
please do yourself a favor and avoid this product
this is his every work day scent
it s very hard to buy the balm separately
it smells like drakkar but it is hard to nd
very nice not too overpowering
no pronouns this product has no smell what ever
nice to use for hardwood oors
table examples of review sentences that contain only pronouns belonging to a specic class
tuple in the amazon and yelp test sets tively
we presented the associated reviews in a random order and asked the workers to judge summaries using the best worst scaling bws louviere and woodworth louviere et al
that is known to produce more reliable sults than ranking scales kiritchenko and mad
the judgment criteria are presented below where non redundancy and coherence were taken from dang
fluency the summary sentences should be grammatically correct easy to read and understand coherence the summary should be well structured and well organized redundancy there should be no unnecessary tition in the summary informativeness how much useful information about the product does the mary provide sentiment how well the sentiment of the summary agrees with the overall sentiment of the original reviews
points of view summaries differ from reviews in terms of the ing style
specically reviews are predominantly gold ours bennett medical has poor customer service
phone calls can take a long time to get answered and leaving voice mails tend to be fruitless
the products are overpriced and take a long time to be relled
using this medical supply company can be a hassle
this medical supply is the worst medical supply company in the valley
the customer service is horrible the staff is rude the wait times are long and the service reps are not helpful at all
do not recommend this company to anyone
copycat if i could give them stars i would
the customer service is terrible the staff is extremely rude and helpful
if you re looking for a new provider this is the place to be
i will never use them again
service is horrible especially the manager
i have a lot of kids but not this place
two months later i was able to go in and get a new one to go in the next day
they would tell me that they would have to do a lot of our water to be there to get a new one
but this is the rst time i have dealt with him and we will never go back again
thanks for your hard work and i will never go anywhere else
bennett medical for cpap supplies are horrible
never enough staff to answer phone so you ll need to leave messages
dont use this medical supply
if i could give bennett medical zero stars i would will be moving to another medical supply as soon as i can
bennett medical for cpap supplies are horrible
we have waited for three weeks to rell supplies and we are still waiting
this company does not have good customer service you can only leave messages and they never call back
if i could give bennett medical zero stars i would teachers health trust please look into the practice of the billing and lling of durable services
the mask cushions go for to days because of the lack of communication
the people in charge of billing are very argumentative and lack customer service
i will drop them after annual because of my insurance obligations
review fantastic service from jocelyn at the front desk we had a really hard time getting the right paperwork together from drs but she stuck with us and helped us every step of the way even calling to keep us updated and to update info we might have for her
thanks jocelyn
i hardly ever write reviews but i d like to spare someone else from what i experienced
so a warning to the wise


if you like rude incompetent employees almost an hour long wait for just picking up a phone order and basically being treated like a second class citizen then look no further than bennett medical
dont use this medical supply
never enough staff to answer phone so you ll need to leave messages
no return phone calls
i am unable to get my cpap supplies every quarter without hours of calling waiting calling
poor customer service
will be moving to another medical supply as soon as i can
terrible experience
they have ridiculous price also bad customer services
you can get nebulizer machine around at amazon bennet medical charge you almost twice more expensive price
and breathing kit price was unbelievable too
because of deduction i had to pay them all out of my pocket whatever they charged
i do nt recommand this medical company to anyone
good luck getting a phone call back or someone to answer the phone without hanging up immediately
i have called over times left voicemails over the last days just to rell a mask perscription
this is an ongoing issue that is beyond frustrating
not trying to destroy this businesses name just want the owners to implement some basic customer service skills
meansum lexrank review review review review review review review always receive friendly customer service whenever we call or go into the location
my questions are always answered and i am very happy with the supplies we get from them
great people providing a great service thank you for all you do table example summaries produced by different systems on yelp data
gold ours it is very clean and nice inside
everyone is so kind and friendly
they do an amazing job on both nails and pedis
they get it done with speed and precision with a price that is very much affordable
they have the best customer service
this nail salon is very clean and the staff is very friendly
they have a wide variety of gel colors to choose from
the prices are very reasonable and they do a great job
the nail techs are very nice and do great work
copycat this is the best nail salon i have ever been to
everyone is very friendly and professional
my nails look great and i m glad i did i will denitely be coming back to this place
meansum the owner is so nice and accommodating
i went to get my nails done by a friend and i was extremely happy with this salon
everyone was very friendly and i was able to use them for nails
they did a great job on my nails and the best part about it was that it was a busy day but it was a treat highly recommend them
lexrank review review review review review review review review i really enjoy coming here to get my nails done
b did an amazing job on my nails
amazing service and nails
however b did an amazing job on my cofn chrome nails and nancy was extremely helpful guring out how i wanted my nails done too
everyone is so friendly there too
tim and tami always always always have the best customer service and do the best nails
i will never go anywhere else
even after weeks my nails look and feel as good as they did when i rst got them done i m so dedicated i recommend and bring in all my friends denitely my new nail salon everyone is so friendly and kind i felt so welcomed b did an amazing job on my nails
he made sure everything was perfect and happily changed something to make me happy
i would highly recommend this place to anyone who wants a work at a totally affordable price
love it amazing service and nails
this is the second time i have been here they did a perfect job again
they get it done fast yet with precision
everyone is so friendly there too
best nail salon i have ever been too
i m glad i found it
i really enjoy coming here to get my nails done
they do a wonderful job on both pedis and nails
it is nice and clean inside
they are very friendly and welcoming
it is worth it to stop in and try it out
my rst set of acrylics ever


i decided years was a lot enough time to wait and i m so happy with them
i m not a huge nail person and was glad to stumble upon this salon
my nail tech was quiet clean and very detail oriented
very pleased with my experience here and i recommend this place
i called to make an appointment for later today for adults and kids and the man who answered the phone said we only have techs today we ca nt do that
poor customer service and i never even went in
golden nails has been my nail place for almost a year so it was surprising to see new management
however b did an amazing job on my cofn chrome nails and nancy was extremely helpful guring out how i wanted my nails done too
denitely excited to keep coming back seriously the best service i have ever gotten at a tempe nail salon i walked in and they helped me right away
nancy helped me pick the perfect color and was very honest and up front about everything i wanted something very natural and using the dip method i love my nails table example summaries produced by different systems on yelp data
gold ours these are a very comfortable and cute sandal
this thong sandal goes with a variety of outts and the cushy sole allows for all day comfort
however they do run a little small sizing up provides a better t
overall a reasonably priced shoe that will last for years to come
these sandals are very cute and comfortable
they t true to size and are very comfortable to wear
they look great with a variety of outts and can be dressed up or down depending on the occasion
copycat i love these sandals
they are very comfortable and i can wear them all day without any discomfort
i wear them to work and they are comfortable to wear
meansum i love these shoes
they are so comfortable and i love the style
they are very comfortable and the perfect price i would denitely recommend this product to anyone
they are comfortable and stylish
lexrank review review review review review review review review i have been wearing white mountain beaded sandals for a couple of years now and they are wonderful
i will never buy from white mountain again
i love white mountain sandels
lots of compliments every time i wear them
i get constant compliments on these sandals
i order them every summer in a variety of colors
i had heel spurs and back problems so the cushy softness of these is the only thing i can wear comfortably and the small wedge heel is perfect for my back
these thongs are fun festive exible and surprisingly comfortable
i have very sensitive feet and i can wear these cuties all day
the arch support is great and there is a nice give in the sole
i love these so much i want to put a few pairs away in case they discontinue them
they go with everything
i have been wearing white mountain beaded sandals for a couple of years now and they are wonderful
they are lightweight and cushion the feet when worn for long hours
they are also beautiful and usually hold up for two or more seasons
this was great price for this cute sandal
unfortunately the toe piece was very hard and they were a little narrow


unusual since i normally wear a b width
for the right person they would probably be ne
they just did nt work for me
i love white mountain sandels
this is my pair of these shoes
i wore out the last pair after
they are very very blingy and i like that
would i order another pair you bet i would will
item was too small purchased for a friend their size is smaller than the size in the store
sent it to the wrong address and i can not seem to nd anyone that will tell me where my bill is
i will never buy from white mountain again
i lived in sandals that looked exactly like this but i thought they were by bjorn
i could nt nd them anywhere but found these go gure
while they are nt quite as comfy as my other ones i think with a bit of breaking in they ll be just ne
lots of compliments every time i wear them
not only are these super comfortable yes even between your toes they look great with just about anything i wear i have been complimented on these daily i typically wear a i ordered a and they t perfect i need more of these highly recommended table example summaries produced by different systems on amazon data
gold ours this is a perfect compact table that ts well in many places
the chairs are surprisingly very comfortable as well
it is cute and perfect for smaller living quarters and the best part is assembly is simple and straightforward
this is a very nice table set for the price
it was easy to assemble and looks great in the kitchen
the only problem is that it is not sturdy enough to hold a lot of weight
it would be nice if it had a little more weight to it so that it would not tip over
copycat this is a great table set for the price
it was easy to put together and looks great
the only thing is that the chairs are a little imsy but they are easy to assemble
meansum the table was very easy to assemble and was easy to assemble
the only thing i would say is that the box is very small and not very sturdy
the table is very sturdy
i would recommend it to anyone looking for a sturdy table and to put on the wall
lexrank review review review review review review review review the table and chairs are very nice but not quite the color i expected but i am getting used to it
the table and chairs are solid and sturdy i received this table and chairs completely damaged
table and chairs delivered by the carrier right on time and with no damage
it was easy to put together and looks great
however when the item was shipped to me one of the backs of the chairs was broken
i just xed it myself with wood glue
its not even visible now
the rest of it was in perfect condition
the table and chairs are very nice but not quite the color i expected but i am getting used to it
table and chairs delivered by the carrier right on time and with no damage
very easy to assemble but very difcult to get out of the box it was so well protected
this table was super easy to put together
the table and chairs are solid and sturdy the seats are very comfortable
the table is the perfect size for our not so big kitchen
we are very pleased with this purchase
moved to smaller living quarters and this just ts the bill
color is perfect and it was easy to assemble
one fault to nd is that the top scratches easily
it even came with a scratch
other than that it is ne
i love my new dining room set
the set is very sturdy the walnut nish is a nice color
this set is great for a small area kitchen nook
would not recommend for a large eating area
table is small and so are the chairs
yet strong enough to hold big boys and girls thumps up great price packed well arrived in a timely matter
it ts perfectly in the kitchen at the ofce
my staff assembled it without any delay
everyone loves the dining set and they ca nt believe i ordered it on line
i made the measurements and made sure of the dimensions of the room and the dining set and it s a perfect t
i received this table and chairs completely damaged
the customer service experience with this company was terrible
in my opinion this set is cheap and overpriced
it s not durable and not worth the money
do nt waste your time
the box looked like it had been opened and then re taped for resell
one of the chairs was broken and the broken piece was nowhere close to the originating piece
possibly other pieces damaged too though did nt bother looking instead just re taped it back up to be sent back
i hope they do nt just resell it to someone else
table example summaries produced by different systems on amazon data

