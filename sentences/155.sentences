l u j l c
s c v
v i x r a modeling comprehending and summarizing textual content by vinicius guilherme medeiros leandro krug and jose palazzo moreira de ppgc instituto de informatica ufrgs porto alegre rs brazil vwoloszyn guimmachado wives
ufrgs
br abstract
automatic text summarization strategies have been cessfully employed to digest text collections and extract its essential content
usually summaries are generated using textual corpora that belongs to the same domain area where the summary will be used
nonetheless there are special cases where it is not found enough tual sources and one possible alternative is to generate a summary from a dierent domain
one manner to summarize texts consists in using a graph model
this model allows giving more importance to words responding to the main concepts from the target domain found in the summarized text
this gives the reader an overview of the main text concepts as well as their relationships
however this kind of tion presents a signicant number of repeated terms when compared to human generated summaries
in this paper we present an approach to produce graph model extractive summaries of texts meeting the target domain exigences and treating the terms repetition problem
to ate the proposition we performed a series of experiments showing that the proposed approach statistically improves the performance of a model based on graph centrality achieving better coverage accuracy and call
keywords graph model summarization text modeling graph centrality biased summarization
introduction automatic text summarization ats systems play a signicant role by tracting essential content from textual documents
this is important given the exponential growth of textual information
despite not being one of the newest areas of research there are still open ended summarization issues that pose many challenges to the scientic community
one of the examples is when one summary must be generated prioritizing sentences that present terms of another specic domain cross domain summarization
another example is the dancy problem that occurs when a wrong text modeling leads to a repetition of content
supported by capes cnpq and fapergs
wolozsyn et al
the cross domain summarization is a strategy to generate biased summaries which generally favors a subject
the need for such bias happens in situations when a summary containing specic aspects must be extracted from general purpose documents
for instance if a teacher wants to know better what are the educational aspects of a movie she is hoping to use in her class and to do so she is looking to other peoples comments about that movie
another example imagine a person who is shopping a new novel hoping to nd one that also presents for instance historical facts of a city during the story
most works on summarization rely on supervised algorithms such as cation and regression
however the quality of results produced by supervised algorithms is dependent on the existence of a large domain dependent training dataset
one drawback of such strategy is that those datasets are not always available and their construction is labor intense and error prone since documents must be manually annotated to train the algorithms correctly
unsupervised models conversely are an interesting strategy for a situation where there are not enough textual sources since it does not need a large ing set for the learning process
however a common problem of these models is redundancy
it happens when a wrong text modeling can benet from the generation of summaries that repeat the most central sentences or select a set of very similar ones in the documents
this causes a gain in accuracy but generates redundant summaries with poor coverage of text aspects
to meet the cross domain summarization needs and mitigate the redundancy problem we propose an unsupervised graph based model to generate domain summaries
the generated graphs are able to uncover the main topics concepts of a document or a set of documents
to do so the summarization algorithm focus on the most relevant i
e
central nodes using pre determined domain corpora and nodes relationships
in our experiments this combination of cross domain generation avoiding redundancy improves graph based ats system s achieving better coverage precision and recall
the contributions of this work are the following it is an unsupervised cross domain summarization i
e
it does not depend on specic annotated ing set it address the redundancy problem performing a re ranking of the initial centrality index to improve coverage and decrease redundancy and considering two distinct datasets the results obtained in our experiments were signicantly superior to the baselines analyzed
the rest of this paper is organized as follows
section presents the ground of the area and related work
section details the proposed model
section provides a case study and section describes the design of our periments
section discusses the results
section summarizes our conclusions and presents future research directions
background automatic text summarization ats techniques have been successfully ployed on user content to highlight the most relevant information among ments
regarding techniques usually employed several works have explored supervised learning strategies to predict text relevance
tionally the use of regression algorithms consistently improves the prediction of modeling comprehending and summarizing textual content by graphs fig

illustration of graph centrality steps where symbols represent text words
helpfulness
however these supervised techniques need annotated corpus for the training process which for the most of the cross domains cases is unavailable
graph centrality is also widely employed on unsupervised extractive marization systems where a graph representation of documents is used to weight sentences relevance on a set of documents
based on that central nodes indicate that the sentence they represent is relevant in the group of uments
let s be a set of all sentences extracted from the input a graph representation g v e is created where v s and e is a set of edges that connect pairs v
then the score of each node is usually given by an algorithm like pagerank or hits
figure depicts the general steps of a summarization system based on graph centrality it builds a similarity graph between pairs of sentences it prunes the graph by removing all edges that do not meet a minimum threshold of similarity it uses pagerank to calculate the centrality scores of each node
then a greedy strategy is employed where the centrality index produces a ranking of vertices importance which is used to indicate the ranking of the most relevant sentences to compose the nal summary
this a well known strategy used as the basis for many novel unsupervised approaches
pagerank computes the centrality of nodes where each edge is ered as a vote to determine the overall centrality score of each node in a graph
however as in many types of social networks not all of the relationships are considered of equal importance
the premise underlying pagerank is that the importance of a node is measured in terms of both the number and the portance of vertices it is related to
the pagerank centrality function is given by p vbu p nv where bu is the set containing all neighborhood of u and nv the number of neighborhoods of v
however this strategy is normally employed with no restrictions that ensure an empty or a minimal intersection between sentences
this lack of restrictions would increase the overall redundancy on these approaches
wolozsyn et al
lexrank which is a popular general purpose extractive summarizer relies on a graph representation of the building a complete graph where each sentence from the document set becomes a node and each edge weight is dened by the value of the cosine similarity between the sentences
then the centrality index of each node is computed producing a ranking of vertices based on their importance which indicates the ranking of the most relevant sentences to compose the summary
this well known strategy is used as the basis for many recent unsupervised approaches
nevertheless these approaches do not take into sideration the repetition problem that causes redundancy of words
neither they present a conceptual model to meet the cross domain summarization mands
thus next section describes an unsupervised cross domain tion model and a post processing algorithm to reduce repetition and improve the coverage of summaries
developed model the developed model structures a given text set in a graph model and uses another specialized text set from another domain to put a bias in the extracted summary
as already described sometimes it is necessary to extract a specialized summary from a more general purpose text set
the example given before is related to extracting educational aspects from user comments in movies
besides the domain bias the model also structures a post processing that treats the problem of sentences repetition
in figure it is shown how the cross domain redundancy free summary is extracted by using the graph model
since the rst steps are the same of a general graph based summary shown in figure this process starts with the output of the general process i
e
a graph where each node have a page rank score that represents how central a determined sentence is figure
the initial page rank scores are then recomputed by taking in consideration keywords found on an external corpus
such keywords are used as a bias to compute the importance of each sentence
the nal specialized summary is based on the centrality score of the sentences weighted by the presence of keywords from the external corpora
let s be a set of all sentences extracted from the r user s reviews about a single movie the rst step is to build a graph representation g v e where v s and e is a set of edges that connect pairs v
the score of each node that represent a sentence is given by the harmonic mean between its centrality score on the graph given by pagerank and the sum of the cies of its specialized keywords stated in equation
the pseudo code of the cross domain re scoring is given in algorithm where g is represented as the adjacency matrix w
modeling comprehending and summarizing textual content by graphs fig

illustration of the cross domain graph centrality building and the processing to avoid redundancy
algorithm cross domain re scoring algorithm s b o input a set of sentences extracted from a general purpose corpora e

amazon s movies reviews r and a corpora b used as a bias output an extractive biased summary o based on the general purpose corpora r
w u v idf modied if w u v then w v else for each u s do end for for each u s do end for p p for each u s do end for return o w v end if k sim keyword u b the main steps of the cross domain re scoring algorithm are it builds a similarity graph w between pairs of texts of the same product or subject lines the graph is pruned w by removing all edges that do not meet a minimum similarity threshold given by the parameter lines the best parameter obtained in our experiments is
wolozsyn et al
c using pagerank the centrality scores of each node is calculated line using the educational corpora each sentence is scored according the presence of educational keywords line e the nal importance score of each node is given by the harmonic mean between its centrality score on the graph and the sum of its education keywords frequencies line
to get the similarity between the two nodes we dene an adapted metric that is the cosine dierence between two corresponding sentence vectors idf modied y wx y tfw xtfw xidfxi where tfw s is the number of occurrences of the word w in the sentence s
we employed the approach described by to extract the keywords from the nal corpora
the similarity between the sentences and the keywords extracted from the external corpora are given by the following equation sim b wx to reduce the textual redundancy problem it was developed an algorithm that employs a clustering technique to nd groups of sentences from the graph of sentences that are both homogeneous and well separated where entities within the same group should be similar and entities in dierent groups dissimilar
then it takes the most central sentence from each group to compose the nal summary
while graph centrality chooses the sentences based on their ity our algorithm divides the graph into k groups of sentences and chose the most central sentence from each group figure
in the literature we nd work employing clustering paradigms to provide a non redundant multi view of textual data
the agglomerative hierarchical clustering method is one of them and it creates a hierarchy tree or dendrogram which can be used for sentence coverage searching purposes
conceptually the process of agglomerating documents creates a cluster hierarchy for which the leaf nodes correspond to individual sentences and the internal nodes correspond to merged groups of clusters
when two groups are merged a new node is created in this hierarchy tree corresponding to this bigger merged group
in our work we employed the complete link hierarchical clustering rithm since it achieves better results on the experiments carried out when compared with other clustering techniques such as k means k medoids and em
by default we remove stop words and the remaining terms of the sentence are represented as uni grams weighted by the known term inverse document frequency tf idf
the pseudo code for decreasing redundancy is displayed in algorithm where g represents a complete graph obtained from the ats approach based on graph centrality and cross domain re scoring
l represents the cluster labels extracted using the function and s the nal solution containing k sentences
modeling comprehending and summarizing textual content by graphs algorithm post processing redundancy algorithm g p k s input a complete graph g v e where v are the sentences and e is a set of edges that represents the similarity between sentences p the centrality score of each node k number the sentences to extract
output ordered list s of sentences
s l k for each k do c c s sort nodes by p s end for return s case study as explained before the goal of the presented approach was to build a graph representation of the main concepts of a document or set of documents
this representation works as a cross domain summary avoiding redundancy
to do so we selected two application domains one to validate the cross domain summary generation and other to validate the redundancy
the cross domain generation was tested in the educational domain and the redundancy control was tested in the news domain
three datasets were employed to perform the experiments
the rst served as a word thesaurus to implement the educational bias in the cross domain generation and it was collected from an educational website teachwithmovies twm where a set of movies are described by teachers with the goal to use them as learning objects inside a classroom
the second dataset is amazon movie reviews amr which provides user comments about a large set of movies
since we were interested in movies that appeared in both datasets a lter was applied and we ended up with movies to perform our evaluation
the third was used to evaluate the post processing that which treats the redundancy problem
is a novel corpus which comprises news texts in brazilian portuguese divided into groups which has been fully employed as gold standard for many recent works on content selection and automatic production of summaries
next we describe each dataset with more details

teaching with movies the teachwithmovies dataset was collected through a crawler developed by us
dierent teachers described the movies on the website but each movie has only one description this was a challenge while collecting the data because the information was not standardized or had associated metadata

teachwithmovies
org index
html public available on
icmc
usp
br pessoas taspardo cstnews
html wolozsyn et al
however we have noticed that some movies presented common information such as movie description rationale for using the movie movie benets for teaching a subject movie problems and warnings for young watchers and v objectives of using this movie in class
the developed crawler extracted such information and we have used the movie description since it contains the greatest amount of educational aspects
in the end unique movies and video clips were extracted but after matching with the amazon dataset we could use movies
this dataset was used as a gold standard to cross domain summary generation

amazon movie reviews the amazon movie reviews was collected with a timespan of more than ten years and consists of proximately millions of reviews that include product and user information ratings and a plain text review
in table is shown some statistics about the data
table
amazon movie reviews statistics dataset statistics number of reviews number of users expert users with reviews number of movies mean number of words per review timespan aug oct
cstnews in this dataset each group of news has from to texts on the same topic having in average sentences and words
it comprises clusters of news texts ally annotated in dierent ways to discursive organization rhetorical structure theory and cross document structure theory annotations
the corpus includes manual multi document summaries one for each cluster of news with pression rate in relation to the longest text
the texts are manually annotated with high level of agreement more than of human judges using cohen s kappa coecient which is a statistic to measure inter rater agreement for sication tasks
that means the annotation agreement is reliable and similar to that in presented in other works for other languages than portuguese
for such reason these human generated summaries were used as a gold standard
since this post processing is not language dependent and the redundancy is also a problem observed in dierent languages this corpus was used to evaluate this post processing strategy
modeling comprehending and summarizing textual content by graphs experiment design this section presents the experimental setting used to evaluate the cross domain summary generation and the post processing that reduces redundancy
it scribes the methods employed as the baselines for comparison the educational plans adopted as gold standard and the metrics applied for evaluation as well as details of the experiment performed to assess the approach

first baseline the results obtained from our cross domain summary are compared with trank algorithm
textrank was chosen because it is also a graph based ing algorithm and has been widely employed in natural language tools
textrank essentially decides the importance of a sentence based on the idea of voting or recommending
considering that in this approach each edge represents a vote the higher the number of votes that are cast for a node the higher the portance of the node or sentence in the graph
the most important sentences compose the nal summary

second baseline centrality based ranking has been successfully on recent works to content tion and automatic production of textual summaries
lexrank is a well know ats system based on graph centrality that has been used many times in the literature for comparisons purposes due to its good performance
since the post processing strategy aims to reduce redundancy in based approaches we employ lexrank as baseline due it uses only the tence centrality index to the ranking task
we used mead s implementation of lexrank which is a publicly available for researching purposes framework for text summarization that provides a set of perl components for the rization of texts written in english as well as in other languages such as chinese

evaluation metrics rouge n the evaluation was performed by applying rouge oriented understudy for gisting evaluation which is a metric inspired on the bilingual evaluation understudy bleu
specically we used rouge n in the evaluation this version of rouge makes a comparison of n grams between the summary to be evaluated and the gold standard in our case cross domain summaries and twm lesson plans respectively
we ated the rst words of the summaries obtained by our approach and the baseline since it corresponds to the median size of the gold standard
rouge was chosen because it is one of most used measures in the elds of machine translation and automatic text summarization
wolozsyn et al
redundancy an important aspect related to redundancy is lexical cohesion
therefore cohesive links between sentences is a positive component of the mary and it has long been considered a key component in assessing content relevance in text summarization
however in some cases it could improve mean redundancy in summaries
to show how redundancy would aect a document summarizer we perform a comparison between the baseline and man gold standard summary
coverage it is the extent to which all words of the automatic summaries are found in the source documents
in other words it is a global score assessing to what extent the candidate summary covers the text given as input
results the next subsections present the evaluation results

cross domain summaries in this section we present cross domain summaries evaluation regarding the adopted baselines concerning precision recall and score obtained by using rouge n
the gold standard utilized in the experiments as already stated is the cational description extracted from the twm website
table shows the mean precision recall and f score considering both our cross domain strategy and textrank the gold standard used as the baseline
the results presented in table show that our strategy outperformed the baseline in all measurements carried out
regarding precision the dierences range from
to
percentage points pp on all rouge n analyzed where n is the size of the n gram used by rouge
using wilcoxon statistical test with a signicance level of
we veried that our strategy is statistically superior when compared to the baseline
regarding recall the dierences are also in favor of our strategy ranging from
to
pp when compared to the baseline
regarding the distribution of rouge s results in fig it is shown a boxplot indicating that our strategy results are not only better in mean but also in terms of lower and upper quartiles minimum and maximal values

post processing in this section we will discuss the results obtained in our experiments regarding the adopted baselines in terms of coverage redundancy precision and recall using cstnews
redundancy and coverage figures and show that our post processing strategy outperformed the unsupervised baseline generation summaries with less redundancy and more coverage being closer to the human gold standard maries
the mean redundancy dierences range from
to
percentage modeling comprehending and summarizing textual content by graphs table
mean of rouge results achieved by the baseline column a and our cross domain strategy column b rouge n column a column b p values

f


f



f

















fig

distribution of rouge results
points pp when compared to lexrank
in terms of coverage the mean ence is up to
pp
using a wilcoxon statistical test with a signicance level of
we veried that our strategy results are statistically superior both in redundancy and coverage
precision and recall figure and show that our strategy also formed the unsupervised baseline in terms of recall and precision obtained using
for recall the mean dierences ranging from
to
pp when compared to lexrank
for precision the mean dierences ranging from
to
pp in all cases
using the wilcoxon statistical test with a signicance level of
we veried that our strategy results are statistically superior in all cases
wolozsyn et al
fig

mean redundancy fig

mean coverage fig

mean recall fig

mean precision conclusion in this paper we presented an approach to generate cross domain summaries based on graphs that are able to represent the main concepts of a document or set of documents
the proposed approach also reduces text redundancy in the generated summaries
we showed that our approach achieved statistically rior results than textrank a general summary algorithm and lexrank another general summary algorithm
the proposed algorithms require no training data which avoids costly and error prone manual training annotations
compared to the baselines our proach outperforms the unsupervised techniques in terms of precision and recall statistically reduces redundancy and improves coverage and is easy to plug into any standard graph centrality approach in any domain
our periments were performed in two domains the educational and the news one attesting the approach versatility
finally it is also important to state that we found out a considerable number of highly helpful sentences with low centrality indexes which lead us to consider the investigation of other techniques to select the most relevant sentences to compose the movies educational description
it is also important to rearm the approach source language independence for that reason we consider in the future to extend the evaluation using dierent languages and summaries length
modeling comprehending and summarizing textual content by graphs references
aggarwal c
c
zhai c
mining text data
springer science business media
al dhelaan m
al suhaim a
sentiment diversication for short review rization
in proceedings of the international conference on web intelligence
pp

acm
cardoso p
c
jorge m
l
d
r
c
pardo t
a
s
al
exploring the rhetorical structure theory for multi document summarization
in congreso de la sociedad espanola para el procesamiento del lenguaje natural xxxi
sociedad espanola para el procesamiento del lenguaje natural sepln
cardoso p
c
maziero e
g
jorge m
l
seno e
m
di felippo a
rino l
h
nunes m
g
pardo t
a
cstnews a discourse annotated corpus for single and multi document summarization of news texts in brazilian portuguese
in ings of the rst brazilian meeting
pp

carletta j
assessing agreement on classication tasks the kappa statistic
putational linguistics
cheng g
tran t
qu y
relin relatedness and informativeness based ity for entity summarization
the semantic web iswc pp

condori r
e
l
pardo t
a
s
opinion summarization methods comparing and extending extractive and abstractive approaches
expert systems with tions
da cunha i
torres moreno j
m
sierra g
on the development of the rst spanish treebank
in proceedings of the linguistic annotation workshop
pp

association for computational linguistics
dias m

s
pardo t
a
s
al
a discursive grid approach to model local coherence in multi document summaries
in annual meeting of the special terest group on discourse and dialogue
association for computational linguistics acl
erkan g
radev d
r
lexrank graph based lexical centrality as salience in text summarization
journal of articial intelligence research
ganesan k
zhai c
han j
opinosis a graph based approach to abstractive summarization of highly redundant opinions
in proceedings of the national conference on computational linguistics
pp

association for computational linguistics
kleinberg j
m
authoritative sources in a hyperlinked environment
journal of the acm jacm
lin c
y
rouge a package for automatic evaluation of summaries
in text marization branches out proceedings of the workshop
pp

maziero e
g
hirst g
pardo t
a
s
al
semi supervised never ending ing in rhetorical relation identication
in international conference on recent advances in natural language processing
bulgarian academy of sciences
mcauley j
j
leskovec j
from amateurs to connoisseurs modeling the evolution of user expertise through online reviews
in proceedings of the international conference on world wide web
pp

www international world wide web conferences steering committee republic and canton of geneva switzerland
acm
org citation


mihalcea r
tarau p
textrank bringing order into texts
association for putational linguistics
nobrega f
a
a
pardo t
a
improving content selection for update tion with subtopic enriched sentence ranking functions
int
j
comput
linguistics appl

page l
brin s
motwani r
winograd t
the pagerank citation ranking bringing order to the web
wolozsyn et al

poibeau t
saggion h
piskorski j
yangarber r
multi source multilingual information extraction and summarization
springer science business media
radev d
allison t
blair goldensohn s
blitzer j
celebi a
dimitrov s
drabek e
hakim a
lam w
liu d
al
mead a platform for ment multilingual text summarization
saggion h
poibeau t
automatic text summarization past present and future
in multi source multilingual information extraction and summarization pp

springer
dos santos h
d
ulbrich a
h
d
woloszyn v
vieira r
ddc outlier ing medication errors using unsupervised learning
ieee journal of biomedical and health informatics
thomas s
beutenmuller c
la puente x
remus r
bordag s
exb text summarizer
in sigdial conference
pp

wan x
co regression for cross language review rating prediction
in ings of the annual meeting of the association for computational linguistics volume short papers
pp

association for computational linguistics soa bulgaria august
aclweb
org anthology
woloszyn v
machado g
m
palazzo moreira de oliveira j
krug wives l
saggion h
beatnik an algorithm to automatic generation of educational scription of movies
in proceedings of the sbie
pp

recife brazil


cbie
sbie


woloszyn v
nejdl w
distrustrank spotting false news domains
in ings of the acm conference on web science
pp

acm
woloszyn v
dos santos h
d
wives l
k
becker k
mrr an unsupervised algorithm to rank reviews by relevance
in proceedings of the international ference on web intelligence
pp

acm
wu j
xu b
li s
an unsupervised approach to rank product reviews
in fuzzy systems and knowledge discovery fskd eighth international conference on
vol
pp

ieee
xiong w
litman d
automatically predicting peer review helpfulness
in ceedings of the annual meeting of the association for computational guistics human language technologies
pp

association for tional linguistics portland oregon usa
aclweb
anthology
yang y
yan y
qiu m
bao f
semantic analysis and helpfulness tion of text for online product reviews
in proceedings of the annual ing of the association for computational linguistics
pp

association for computational linguistics beijing china july
aclweb
anthology
yang z
duan l
j
lai y

online public opinion hotspot detection and analysis based on short text clustering using string distance
journal of beijing university of technology
zeng y
c
wu s
h
modeling the helpful opinion mining of online consumer reviews as a classication problem
in proceedings of the ijcnlp workshop on nlp for social media socialnlp
pp

asian federation of natural guage processing nagoya japan
aclweb
org
zhai z
liu b
xu h
jia p
clustering product features for opinion mining
in proceedings of the fourth acm international conference on web search and data mining
pp

acm
