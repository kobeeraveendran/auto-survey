deep learning for text style transfer a survey di jin mit csail
edu zhijing jin max planck institute
mpg
zhiting hu uc san diego
edu olga vechtomova university of waterloo
ca rada mihalcea university of michigan
edu abstract text style transfer tst is an important task in natural language generation nlg which aims to control certain attributes in the erated text such as politeness emotion mor and many others
it has a long history in the eld of natural language processing nlp but recently it has gained signicant attention thanks to the promising performance brought by deep learning models
in this paper we present a systematic survey of the research done on neural text style transfer
we have lected summarized and discussed nearly representative articles since the rst neural text style transfer work in
overall we have covered the task formulation existing datasets and subtasks evaluation metrics and methods on parallel and non parallel data
we also vide discussions a variety of important topics regarding tst which can shed light on new development in this eld
introduction language is situational
every utterance ts in a specic time place and scenario conveys specic characteristics of the user and has a specic intent
let us denote the utterance as and the discourse attribute value as a
the attribute value a can be an extent of formality politeness simplicity sonality emotion partner effect genre of writing e

ction or and other attributes
there are three different settings to model the interactions between and a as shown in table
the rst task discriminative modeling of has existed for more than half a century through equal contributions
curated paper list is at
zhijing jin
that we interchangeably use attribute and style in this survey
attribute is a broader terminology because sides style many subtasks also concern transferring content preferences e

sentiment topic and so on
we also use style because it is more widely known to the community
formulation task author attribute detection style conditioned language modeling text style transfer similar tasks conditional text tion counterfactual text generation trastive text generation table formulation of three tasks that center around text and attributes
tasks such as authorship identication e

speare vs
non shakespeare works mendenhall yule for a review see stamatatos and author speaker attribute detection cher trudgill mulac et al
era and yarowsky rao et al
welch et al

the second and third tasks fall into the general category of controllable text tion e

hovy reiter and dale hu et al
aiming to generate text with control over various textual properties
specically the second task style conditioned language modeling e

wen et al
ficler and goldberg keskar et al
ziegler et al
which els aims to generate text given the style as the condition
our survey focuses on the third task which is the specic generation task often called text style fer tst by learning
specically tst aims to produce text of a desired attribute value a given the existing text carrying a different tribute
for example given the existing informal sentence got ta go asap tst should be able to modify the formality and generate for example the formal expression we have to leave as soon as we can
different from the style conditioned language modeling for tst the given text constrains the content of the sentence which we aim to generate
crucial to the denition of style transfer is the c e d l c
s c v
v i x r a distinction of style and content for which there are two common practices
first is by linguistic denition where non functional linguistic features are classied into the style e

formality and the semantics into content
in contrast the second way is data driven given two corpora e

a positive review set and a negative review set the invariance between the two corpora is the content whereas the variance is the style e

sentiment topic mou and vechtomova
hence the commonly used criteria for successful style transferred text include i maintaining the attribute independent content as the source text conforming to the target attribute and iii language uency
tst is motivated by a wide range of applications as preluded in the discussions of mcdonald and pustejovsky and hovy on the matics of language i
e
attributes
hovy points out that attributes of language are crucial cause they make natural language processing nlp centered around the human end users
for example regarding intelligent bots customers tend to prefer bots with distinct and consistent persona e

pathetic instead of those with emotionless tones or inconsistent persona
tst can be used as a tool to post process the generated text of other tasks and equip the text with a desired attribute
another signicant application is the intelligent writing sistant
for example non expert writers often need to polish their writings to better t their purpose e

more formal more polite more humorous and other advanced writing requirements
tst can be a handy help for these needs
more applications include automatic text simplication where a simple debiasing online text where a tive detoxicalization where a non toxic or polite and so on
driven by the strong needs for tst many ods have emerged
traditional approaches rely on term replacement and templates
for ple early works in nlg of weather forecasts build domain specic templates to express different els of certainty of the future weather
research that more distinctively focuses on tst starts from the frame language based systems mcdonald and pustejovsky and schema based nlg tems hovy which generate text with pragmatic constraints such as formality der small scale well dened schema
most works require domain specic templates hand featured phrase sets that expresses a certain attribute e

friendly and sometimes also a look up table of expressions with the same meaning but multiple different attributes bateman and paris et al
power et al
reiter et al
sheikha and inkpen mairesse and walker
with the success of deep learning a variety of neural methods have been proposed for tst
if parallel data are provided standard to sequence models are often directly applied for the task rao and tetreault see section
however most use cases do not have parallel data so tst on non parallel corpora becomes a lic research area see section
the rst line of approaches disentangle text into its content and attribute in the latent space and apply tive modeling hu et al
shen et al

this trend was then joined by another distinctive line of approach prototype based text editing li et al
which extracts a sentence template and attribute markers to generate the text
another paradigm soon followed i
e
back translation to generate pseudo parallel data zhang et al
jin et al
inspired by unsupervised chine translation umt
these three directions disentanglement prototype based text ing and back translation are further advanced with the emergence of transformer based models sudhakar et al
malmi et al

given the advance of its methodology tst now starts to radiate its impact on downstream tions such as stylized dialog generation niu and bansal huang et al
stylistic marization jin et al
stylized language modeling to imitate specic authors syed et al
online text debiasing pryzant et al
ma et al
simile generation chakrabarty et al
and many others
we compile this survey motivated by the ing research interests on tst
to our knowledge this is the rst survey to comprehensively marize the past and future of this exciting eld of research analyze the trends and provide guidelines for standard practice in the eld
to summarize the key contributions of this survey are as follows
we conduct the rst comprehensive review that covers most existing works on tst
we overview the task setting datasets and tion metrics as well as categorize the existing approaches

we discuss the arguable nature of style and content the connection of traditional nlg with deep learning models of tst and ethical considerations of tst

we cover the broad applications of tst and how it can be tailored for downstream tasks

we include a discussion of some open sues that are long standing and awaiting to be solved and identify some possible future research directions
paper selection
the neural tst papers viewed in this survey are mainly from top ences in nlp and articial intelligence ai ing acl emnlp naacl coling neurips icml iclr aaai and ijcai
other than conference papers we also include some peer reviewed preprint papers that can offer some insightful information about the eld
the major factors for selecting non peer reviewed preprint pers include novelty completeness and so on
survey organization
the organization of this survey is as follows
we rst introduce the inition formulation and existing subtasks with datasets in section
we then overview the uation metrics in section
section and gorize the existing methods for text style transfer and elaborate each type of methods in details and depth
in section we discuss some open issues in tst that present challenges to its development
we also highlight the association of tst with other nlp tasks in section
finally we discuss portant future directions in section and draw the conclusion in section
task overview
what is style denition an intuitive notion of style refers to the manner in which some semantics is expressed mcdonald and pustejovsky
just as one has their own signatures style is inherent to every person s utterance
for pragmatic uses style becomes a protocol to regularize the manner of communication
for example for academic ing the protocol requires formality and alism
hovy denes style by its pragmatic aspects including both personal e

personality gender
and interpersonal e

humor mance aspects
most existing literature also takes these well dened categories of styles
style in the scope of this survey this survey aims to provide an overview on neural text style transfer approaches
to be concise we will limit the scope to the setting in most literature including data driven denition of style and ity of style specic corpora
the data driven denition of style is opposed to the linguistic or rule based denition of style which theoretically constrains what constitutes a style and what not such as a style guide e

sociation et al

for example formal text should not include any contraction e

is nt
in contrast the data driven denition of style sumes that the style simply refers to the istics of the given corpus distinct from a general text corpus or another given style corpus which is widely adopted in neural tst approaches since shen et al

consequently most neural approaches also sumes that there are available style specic corpora for each style of interest parallel or non parallel
note that there can be works that do not take these two assumptions which will be discussed as future directions in section

task formulation we dene the main notations used in this survey in table
not
meaning a a ai xi xi e g fc e g fc an attribute value e

the formal style an attribute value different from a a predened set of attribute values the i th attribute value in a a sentence with attribute value a a sentence with attribute value a corpus of sentences with attribute value ai a sentence from the corpus xi encoder of a tst model generator of a tst model attribute classier parameters of the encoder parameters of the generator parameters of the attribute classier attribute transferred sentence of learned by the model latent representation of text i
e
latent representation of the attribute value in text a table notation not
of each variable and its sponding meaning
as mentioned previously in section
most neural approaches assume a given set of attribute
sandbox
yahoo
catalog
attribute values datasets size para
task style features formality politeness gender biasedness informal formal impolite polite masculine feminine factual humorous mantic biased neutral toxicity offensive non offensive authorship shakespearean modern different bible translators simplicity complicated simple gyafc rao and tetreault politeness madaan et al
yelp gender prabhumoye et al
flickrstyle gan et al
wiki neutrality pryzant et al
twitter dos santos et al
reddit dos santos et al
reddit politics tran et al
shakespeare xu et al
bible carlson et al
pwkp zhu et al
expert den bercken et al
msd cao et al
content preferences sentiment positive negative topic political slant entertainment politics democratic republican yelp shen et al
amazon he and mcauley yahoo political voigt et al
k m
m k k k k k k m k
k k k k k k table list of common subtasks of tst and their corresponding attribute values and datasets
for datasets with multiple attribute specic corpora we report their sizes by the number of sentences of the smallest of all corpora
we also report whether is dataset is parallel para

values a and each attribute value has its own pus
for example if the task is about formality transfer then for the attribute of text formality there are two attribute values a formal and informal corresponding to a of formal sentences and another of mal sentences
the style corpora can be parallel or non parallel
parallel data means that each tence with the attribute a is paired with a part sentence with another attribute
in contrast non parallel data only assumes mono style corpora

existing subtasks with datasets we list the common subtasks for neural text style transfer in table
the attributes of interest vary from style features e

formality and politeness to content preferences e

sentiment and topics each task of which will be elaborated below
formality
adjusting the extent of formality in text was rst proposed by hovy
it is one of the most distinctive stylistic aspects that can be observed through many linguistic ena such as more full names e

television instead of abbreviations e

tv and more nouns e

solicitation instead of verbs e

request
the formality dataset grammarly s yahoo answers formality corpus gyafc rao and tetreault contains k formal informal pairs retrieved by rst getting k informal tences from the yahoo answers corpus and then recruiting crowdsource workers to rewrite them in a formal way
politeness
politeness transfer madaan et al
aims to control the politeness in text
for example could you please send me the data is a more polite expression than send me the data
madaan et al
compiled a dataset of
million automatically labeled instances from the raw enron shetty and adibi
as politeness is culture dependent this dataset mainly focuses on politeness in north american english
gender
linguistic phenomena related to gender is a heated research area trudgill lakoff tannen argamon et al
boulis and ostendorf
the gender related tst dataset is proposed by prabhumoye et al
who compiled
m reviews from yelp dataset that are labeled with the gender of the user

humor and romance are some artistic attributes that can provide readers with joy
li et al
rst propose to borrow the flickrstyle stylized caption dataset gan et al
from the computer vision domain
in the
yelp
com challenge flickrstyle image caption dataset each image has three captions with a factual a humorous and a romantic style respectively
by keeping only the captions of the three styles li et al
created a subset of the flickrstyle dataset of k parallel factual humorous romantic triplets
biasedness
wiki neutrality corpus pryzant et al
is the rst corpus of biased and ized sentence pairs
it is collected from wikipedia revisions that adjusted the tone of existing tences to a more neutral voice
the types of bias in the biased corpus include framing bias logical bias and demographic bias
toxicity
another important use of tst is to ght against offensive language
tran et al
collect k offensive sentences and m offensive sentences by crawling sentences from reddits using a list of restricted words
authorship
changing the tone of the author is an artistic use of text style transfer
xu et al
created an aligned corpus of k pairs of spearean english and its modern english tion
carlson et al
collected m parallel data from english versions of the bible by different translators
simplicity
another important use of tst is to lower the language barrier for readers such as lating legalese or medical jargon into nontechnical english to avoid discrepancies between expert vice and laymen s understanding tan and awardene
a common task is to translate the standard english wikipedia into simple wikipedia which has parallel datasets in the general domain with k samples zhu et al
and cal domain with
k samples den bercken et al

cao et al
compose a more relevant medical dataset from the health reference merck manuals msd where discussions on each cal topic has one version for professionals and the other for consumers
this msd dataset is k in size
sentiment
sentiment modication is the most popular task in previous works of tst
it aims to change the sentiment polarity in reviews for example from a negative review to positive review or vice versa
there are also works transferring sentiments on ne grained review ratings e

scores
commonly used datasets include yelp reviews shen et al
and amazon product reviews he and mcauley
topic
there are a few works that cover topic transfer
for example huang et al
form the two topic corpora by compiling yahoo swers under two topics entertainment and politics respectively
there is also a recent dataset with text styles such as sciences sport politics and others zeng et al

political slant
political slant transfer proposed by prabhumoye et al
aims to transfer the political view in text
for example a republican s comment can be defund all illegal immigrants while democrats are more likely to support manistic actions towards immigrants
the political slant dataset voigt et al
is collected from comments on facebook posts of the united states senate and house members
the dataset uses level comments directly responding to the posts of a democratic or republican congressperson
there are k training k development and k test instances in the dataset
combined attributes
lample et al
pose a more challenging setting of text attribute transfer multi attribute transfer
for example the source sentence can be a positive review on an asian restaurant written by a male reviewer and the target sentence is a negative review on an american restaurant written by a female
each of their datasets has independent categories of attributes
their rst dataset is fyelp which is compiled from the yelp dataset challenge labeled with sentiment positive or negative gender male or female and eatery category american asian bar dessert or mexican
their second dataset amazon which is based on the amazon product review dataset li et al
contains the lowing attributes sentiment positive or negative and product category book clothing electronics movies or music
their third dataset social dia content dataset collected from internal book data which is private contains gender male or female age group or and annotated feeling relaxed or annoyed
criterion human evaluation method human rating ranking overall transferred style strength accuracy by a separately trained style classier human rating ranking human rating ranking bleu rouge
with modied inputs semantic preservation human rating ranking perplexity by a separately trained lm fluency automatic evaluation method bleu with gold references table overview of evaluation methods for each criterion
evaluation metrics
some datasets do not have human written a successful style transferred output not only needs to demonstrate the correct target style but also due to the uncontrollability of neural networks we need to verify that it preserves the original semantics and maintains natural language uency
therefore the commonly used practice of evaluation considers the following three criteria transferred style strength semantic preservation and uency
we will rst introduce the practice of automatic evaluation on the three criteria discuss the ts and caveats of automatic evaluation and then introduce human evaluation as a remedy for some of the intrinsic weaknesses of automatic evaluation
the overview of evaluation methods regarding each criterion is listed in table

automatic evaluation automatic evaluation provides an economic ducible and scalable way to assess the quality of generation results
however due to the edness of natural language each metric introduced below can address certain aspects but also has trinsic blind spots
bleu with gold references
similar to many text generation tasks text style transfer also has human written references on several datasets e

yelp captions
so it is common to use the bleu score papineni et al
between the gold references and model outputs
using bleu to evaluate tst models has been seen across deep learning works xu et al
jhamtani et al
and deep learning approaches rao and tetreault li et al
jin et al

there are three problems with using bleu tween the gold references and model outputs
simply copying the input can result in high bleu scores
bleu is shown to correlate not too well with human evaluation erences problem different from machine translation where using bleu only is sufcient tst has to consider the caveat that simply copying the put sentence can achieve high bleu scores e

on some datasets with the gold references
it is because most text rewrites have a large tent of n gram overlap with the source sentence
a possible x to consider is to combine bleu with pinc chen and dolan as in paraphrasing xu et al
jhamtani et al

by using pinc and bleu as a dimensional metric we can minimize the n gram overlap with the source sentence but maximize the n gram overlap with the reference sentences
problem other problems include cient correlation of bleu with human tions e


w

t
human rated ity shown in li et al
and
w

t
human evaluations shown in mir et al
and the availability of human written references for some datasets e

gender and political datasets humoye et al
and the politeness dataset madaan et al

a commonly used x is to make the evaluation more ne grained using three different independent aspects namely transferred style strength semantic preservation and uency which will be detailed below
transferred style strength
to automatically evaluate the transferred style strength most works separately train a style classier to distinguish the attributes hu et al
shen et al
fu et al
li et al
prabhumoye et al

this classier is used to judge whether each sample generated by the model conforms to the target attribute
the transferred style strength is calculated as test samples correctly classied
li et al
shows that the attribute classier correlates all test samples that this style classier usually report or accuracy and we will discuss the the problem of false positives and false negatives in last paragraph of automatic evaluation
well with human evaluation on some datasets e

yelp and captions but has almost no correlation with others e

amazon
the reason is that some product genres has a dominant number of positive or negative reviews
an alternative metric is to measure the earth mover s distance rubner et al
between the attribute distributions of the source style corpus and the transferred outputs where the attribute tribution of a corpus is calculated by running the attribute classier on that corpus mir et al

semantic preservation
many metrics can be applied to measure the similarity between the put and output sentence pairs such as bleu pineni et al
rouge lin and och meteor banerjee and lavie chrf popovic cosine similarity based on sentence embeddings fu et al
distance tian et al
wmd kusner et al
bertscore zhang et al
and so on
another newly proposed metric is to rst delete all attribute related expressions in the text and then apply the above similarity evaluations mir et al

among all the metrics mir et al
yamshchikov et al
showed that meteor and wmd have better correlation with human uation than bleu although in practice bleu is the most widely used metric to evaluate the mantic similarity between the source sentence and style transferred output yang et al
madaan et al

fluency
fluency is a basic requirement for ural language outputs
to automate this tion perplexity is calculated via a language model lm pretrained on the training data of all tributes yang et al

however the tiveness of perplexity remains debateable since pang and gimpel showed its high tion with human ratings of uency whereas mir et al
suggested no signicant correlation of perplexity with human scores
we provide a note that perplexity by lm can suffer from the following undesired properties
biased towards shorter sentences than longer sentences

for the same meaning less frequent words will have worse perplexity e

agreeable than frequent words e

good

a sentence s own perplexity will change if the sentence prior to it changes

lm is not good enough yet

lm do not necessarily handle well the domain shift between its training corpus and the transferred text
such properties will bias against certain models which is not desired for an evaluation metric
as a potential remedy future researchers can try grammaticality checker to score the generated text
task specic criteria
as tst can serve as a component for other downstream applications some task specic criteria are also proposed to evaluate the quality of generated text
for ple reiter et al
evaluated the effect of their tailored text on reducing smokers intent to smoke through clinical trials
jin et al
applied tst to generate eye catchy headlines so they have an attractive score and future works in this tion can also test the click through rates
hu et al
evaluated how the generated text as mented data can improve the downstream attribute classication accuracy
tips for automatic metrics
for the evaluation metrics that rely on the pretrained models namely the style classier and lm we need to beware of the following
the pretrained models for automatic ation should be separate from the proposed tst model
machine learning models can be imperfect so we should be aware of the potential false positives and false negatives
the pretrained models are imperfect in the sense that they will favor towards a certain type of methods for the rst point it is important to not use the same style classier or lm in the proposed tst approach otherwise it can overt or hack the rics
for the second point we need to understand what can be the false positives and false negatives of the generated outputs
an illustrative ple is that if the style classier only reports performance e

on the gender dataset moye et al
and amazon dataset li et al
even perfect style rewrites can only score but maybe an imperfect model can score because it can resemble the imperfect style classication model more and makes advantage of the false positives
other reasons for false tives can be adversarial attacks
jin et al
showed that merely paraphrasing by synonyms can drop the performance of high accuracy tion models from textcnn kim to bert devlin et al
by
therefore higher scores by the style classier does not necessarily indicate more successful transfer
moreover the style classier can produce false negatives if there is a distribution shift between the training data and style transferred outputs
for example in the ing corpus the product a may appear often with the positive attribute and in the style transferred outputs the product a co occurs with the site negative attribute
such false negatives are observed on the amazon product review dataset li et al

on the other hand the biases of the lm correlate with sentence length synonym replacement and prior context
the third point is a direct result implied by the second point so in practice we need to keep in mind and check whether the proposed model takes advantage of the evaluation metrics or makes provements that are generalizable

human evaluation compared to the pros and cons of the automatic evaluation metrics mentioned above human ation stands out for its exibility and siveness
for example when asking humans to evaluate the uency we do not need to worry the bias towards shorter sentences as in the lm
we can also design criteria that are not computationally easy such as comparing and ranking the outputs of multiple models
there are several ways to duct human evaluation
in terms of evaluation type there are pointwise scoring namely asking humans to provide absolute scores of the model outputs and pairwise comparison namely asking humans to judge which of the two outputs is better or viding a ranking for multiple outputs
in terms of the criteria humans can provide overall evaluation or separate scores for transferred style strength semantic preservation and uency
however the well known limitation of human evaluation is irreproducibility
the human tion results in two studies can not be directly pared because human evaluation results are ble and subjective
as a remedy we encourage future researchers to report inter rater agreement scores such as the cohen s kappa cohen and krippendorff s alpha krippendorff
for human evaluation as common tips practice most works use outputs for each style transfer direction e

puts for formalinformal and outputs for informalformal and two human annotators for each task shen et al
fu et al
li et al


suggested standard practices currently the experiments of various tst works do not adopt the same setting making it difcult to do head to head comparison among the empirical results of multiple works
although it is reasonable to customize the experimental settings according to the needs of a certain work it is suggested to at least use the standard setting in at least one of the many reported experiments to make it easy to compare with previous and future works
for example at least experiment on one commonly used dataset list up to date best performing previous models as baselines and report on a superset of the most commonly used metrics
for we suggest that future works use at least one of the most commonly used benchmark datasets such as the yelp data prepreocessed by shen et al
and its ve human references provided by jin et al
amazon data processed by li et al
and formality data provided by rao and tetreault
at for we suggest that future works tively check the latest style transfer papers curated
com style transfer in text and our repository
com zhijing jin text and compare with the state of the art performances instead of older ones
we also call for more reproducibility in the community including source codes and evaluation codes because for example there are several different scripts to evaluate the bleu scores
for since no single evaluation metric is perfect and comprehensive enough for tst it is strongly suggested to use both human and matic evaluation on three criteria
in evaluation apart from customized use of metrics we suggest that most future works to include at least the lowing evaluation practices human evaluation rate at least two state the art models according to the curated paper lists automatic evaluation at least report the bleu score with all available references if there exist human written references e

the ve references for the yelp dataset provided by jin et al
and bleu with the input only when there are no human written ences
it could also be an option to establish an online leaderboard and let existing works upload their output les
it is also good if the online website can automatically evaluate the model outputs since there can be different scripts to evaluate the bleu scores as well as different style classiers and lm
methods on parallel data over the last several years various methods have been proposed for text style transfer
in general they can be categorized based on whether the dataset has parallel text with different styles or several non parallel monostyle corpora
the last column in table shows the dataset parallelism for each tst subtask
in this section we will cover tst methods on parallel datasets and in tion we will detail the approaches on non parallel datasets
to ease the understanding for the readers we will in most cases explain tst on one attribute between two values such as transferring the mality between informal and formal tones which can potentially be extended to multiple attributes
encoder decoder architecture most methods adopt the standard neural sequence to sequence model with the encoder decoder tecture which was initially developed for neural machine translation nmt sutskever et al
bahdanau et al
cho et al
and sively seen on text generation tasks such as marization rush et al
and many others song et al

the encoder decoder model can be implemented by either lstm as in rao and tetreault shang et al
or transformer vaswani et al
as in xu et al

copy mechanism gulcehre al
see et al
is also added to better handle stretches of text which should not be changed e

some proper nouns and rare words gu et al
merity et al

based on this architecture cent works have developed multiple directions of improvement multi tasking inference techniques and data augmentation
multi tasking in addition to the ing on paired attributed text xu et al
propose adding three other loss functions classier guided loss which is calculated using a well trained attribute classier and encourages the model to generate sentences conforming to the target attribute reconstruction loss which courages the model to reconstruct the put itself and one sided cycle loss which rst transfers the input sentence to the target attribute and then transfers the output back to its original attribute
each of the three losses can gain mance improvement of bleu points with the human references xu et al

another type of multi tasking is to jointly learn tst and machine translation from french to english which improves the performance by bleu score with written references niu et al

specic for formality transfer zhang et al
multi task tst and grammar error correction gec so that knowledge from gec data can be transferred to the informal to formal style transfer task
apart from the additional loss designs using the pretrained language model radford et al
can lead to improvement by at least bleu scores with human references wang et al

inference techniques to avoid the model ing too many parts of the input sentence and not performing sufcient edits to ip the attribute jiwara rst identify words in the source sentence requiring replacement and then change the words by negative lexically constrained ing post and vilar that avoids naive copying
since this method only changes the beam search process for model inference it can be applied to any text style transfer model without model re training
data augmentation since style transfer data is expensive to annotate there are not as many allel data as machine translation
hence various methods have been proposed for data augmentation to enrich the data
for example rao and tetreault rst train a phrase based machine tion pbmt model on the given parallel dataset and then using back translation sennrich et al
to construct a pseudo parallel dataset as additional training data which leads to an ment of around
bleu scores with human ten references
most recently zhang et al
use a data augmentation technique by making use of largely available online text
they scrape informal text from online forums and generate back translations i
e
informal english a pivot language such as french formal english where the formality of the back translated english text is ensured by using a formality classier to only keep text that are classied as formal text
methods on non parallel data since parallel data for tst is difcult to obtain and for some styles impossible to annotate e

mark twain novels rewritten in hemmingway s style the majority of deep learning methods for text style transfer investigate how to learn from non parallel monostyle corpora
we will introduce three main ideas that guide many style transfer works disentanglement prototype editing and back translation
note that they can be used either independently or in a combined way

disentanglement disentanglement based methods usually adopt the following process
encode the text with attribute a into a latent representation i
e
z
manipulate the latent to remove the source attribute i
e

decode into text with the target attribute i
e
in the following we will rst introduce the decoder training objectives that is used for step and namely z and section

and then introduce three main approaches to ulate the latent representation for step namely z section


finally we will cover a plethora of training objectives for successful manipulations of the latent representation in tion


table summarizes how existing ment based methods have adopted these choices and we also provide their performance on the yelp dataset for comparison


encoder decoder training method there are three model choices to obtain the tent representation z from the discrete text and then decode it into the new text via struction training auto encoder ae variational auto encoder vae and generative adversarial works gans
auto encoder ae
auto encoding ae is a commonly used method to learn the latent sentation z which rst encodes the input sentence into a latent vector z and then reconstructs a tence as similar to the input sentence as possible
ae is used in many tst works e

shen et al
hu et al
fu et al
zhao et al
prabhumoye et al
yang et al

to avoid auto encoding from blindly copying all elements from the input hill et al
adopt noising auto encoding dae vincent et al
to replace ae in nlp tasks
specically dae rst passes the input sentence through a noise model to randomly drop shufe or mask some words and then reconstruct the original sentence from this corrupted sentence which is used in later tat works such as lample et al
jin et al

variational auto encoder vae
instead of reconstructing data based on the deterministic latent representations by ae variational encoder vae kingma and welling rezende et al
reconstructs data based on the sampled latent vector from its posterior and use the regularization by kullback leibler gence
vae is also commonly used in tst works mueller et al
hu et al
liu et al
liao et al
yi et al

the vae loss is formulated as g log where is the hyper parameter to balance the construction loss and the kl term is the prior drawn from the standard normal distribution of n i and is the posterior in the form of n where and are predicted by the encoder
acc advr advo mueller et al
hu et al
shen et al
fu et al
prabhumoye et al
zhao et al
yang et al
logeswaran et al
tian et al
liao et al
romanov et al
john et al
bao et al
dai et al
wang et al
li et al
liu et al
yi et al
jin et al
enc dec disen
lre acc vae vae ae ae gan ae ae ae vae ae vae ae ae gan vae vae ae acc acc acc acc acc acc lre lrs lrs lrs acc lre acc lre acc lre settings aim aco advr aco advr lmo advo advo aco aco aim cycle noun cycle cycle performance on yelp bl ref acc
bl inp






































table summary of the settings adopted by existing disentanglement based methods and their performance on the yelp dataset
we list the types of encoder decoder training method enc dec the disentanglement method disen
and the loss types used to achieve aim and aim
we also list the automatic evaluations w

t
accuracy acc
bleu with the input bl inp and bleu with the one human reference bl ref provided by li et al

marks numbers reported by liu et al

latent representation editing attribute code control latent representation splitting figure three methods to manipulate the latent space based on disentanglement for text style transfer
generative adversarial networks gans
generative adversarial networks gans fellow et al
can also be applied to tst shen et al
zhao et al
li et al

the way gans work is to rst mate the samples drawn from a true distribution by employing a noise sample s and a generator function g to produce
then a critic discriminator is used to distinguish the critic real data and generated samples
is trained to distinguish the real samples from generated samples and the generator is trained to fool the critic
formally the training process is expressed a min max game played among the encoder e generator g and the critic fc max c min e g lgan log ep


latent representation manipulation based on the general encoder and decoder training method the core element of disentanglement is the manipulation of latent representation z
figure lustrates three main methods latent representation editing attribute code control and latent tation splitting
in addition the disen
column of table shows the type of latent representation manipulation for each work in disentanglement
the rst approach latent representation ing lre shown in figure is achieved by suring two properties of the latent representation z
the rst property is that z should be able to serve as the latent representation for auto encoding namely aligning with the input where z
the second property is that should be learned to incorporate the new attribute value of interest
to achieve this the common practice is to rst learn an attribute classier fc e

a multilayer zzlatent spaceencoder egenerator gzencoder egenerator gattribute specificgenerator i
e
egenerator gza mlp that takes the latent representation z as input and then iteratively update z within the strained space by the rst property and in the same time maximize the prediction condence score garding by this attribute classier mueller et al
liao et al
wang et al
liu et al

an alternative way to achieve the second property is to multi task by another encoding task on the corpus with the attribute and share most layers of the transformer except the query transformation and layer normalization layers jin et al

the second approach attribute code control acc as shown in figure rst enforces the latent representation z of the sentence to contain all information except its attribute value a via versarial learning and then the transferred output is decoded based on the combination of z and a structured attribute code a corresponding to the attribute value a
during the decoding process the attribute code vector a controls the attribute of erated text by acting as either the initial state shen et al
yi et al
or the embedding fu et al
dai et al

the third approach latent representation ting lrs as illustrated in figure rst entangles the input text into two parts the latent attribute representation a and semantic tion z that captures attribute indepent information
then we replace the source attribute a with the target attribute and the nal transferred text is generated using the combination of z and john et al
romanov et al



training objectives when disentangling the attribute information a and the attribute independent semantic information z we need to achieve two aims aim the target attribute is fully and exclusively controlled by a and not aim the attribute independent information is fully and exclusively captured by z and not we will detail the various training objectives for aim in section


and the training tives for aim in section






style oriented losses for aim to achieve aim many different learning tives have been proposed
the following learning objectives work at least partially towards nudging the model to learn a more clearly disentangled a and exclude the attribute information from z
attribute classier on outputs aco
aco aims to make sentences generated by the tor g carry the target attribute according to a pre trained attribute classier fc hu et al
prabhumoye et al
yamshchikov et al

the generator g take as input the learned attribute vector which can be either an attribute code tor trained from scratch as in the acc approach or the attribute representation disentangled from text by the lrs approach
we denote the eration process to obtain the transferred sentence as
correspondingly aco minimizes the following learning objective log
in training aco can be trained in two ways ther a normal loss function trained by softmax distribution to approximate the discrete training jang et al
or a negative reward for reinforcement learning by policy gradient training williams as in luo et al

attribute classier on representations acr
different from the previous aco objective whose training signal is from the the output sentence acr directly enforces the disentangled attribute representation a to be correctly classied by the attribute classier by the following objective john et al
romanov et al
fc log
adversarial learning on representations advr
as the previous acr explicitly requires the latent a to be classied by fc advr trains from another perspective enforcing that no attribute related information is contained in z fu et al
zhao et al
romanov et al
john et al
li et al

note that by combining acr and advr we can make attribute information captured fully and exclusively in a
to achieve advr the encoder e is trained to generate the latent representation z so that can not be discriminated by the attribute classier fc which is expressed by the following learning objective maxe minfc fc log
discriminative classication can be alternatively achieved by generative language modeling namely lmi for each monostyle corpus with the attribute ai yang et al

specically the training objective for each attribute is since advr can be imbalanced if the number of samples of each attribute value differs largely an extension of advr is to treat different attribute values with equal weight shen et al
max e min fc fc log
note that is the distribution of sentences of one attribute and is the distribution of tences of the other attribute
adversarial learning on outputs advo
apart from advr that adversarially learn the latent representations we can also use advo to perform adversarial training on the outputs to make them undistinguishable from the real data shen et al
logeswaran et al
tian et al

specically for each attribute ai we train a sier i to distinguish between true xi from the c monostyle corpus of attribute ai and the generated ai where k i which sentence xi aims to have the attribute ai
the loss function is max e g min i c g i c log i i ai
in the training process usually we rst optimize all attribute classiers i and then train the encoder c generator and the attribute classiers together by optimizing the sum the all advo training losses i min i c max e g g
c note that in order to propagate the gradients it is feasible to use the sequence of hidden states in the generator instead of discrete text for ai shen et al

g lmi log plmi xi log plmi ai where is a hyperparameter to weight the two terms
the total training objective sums over the losses of all attributes i max e g min g lmi



content oriented losses for aim the style oriented losses introduced above ensures the attribute information to be contained in a but not necessarily putting constraints on the independent semantics z
to learn the independent information fully and exclusively in z i
e
aim the following learning objectives are proposed cycle reconstruction cycle
the cycle struction loss dos santos et al
logeswaran et al
luo et al
dai et al
yi et al
huang et al
rst encodes a sentence to its latent representation z and then feed z to the generator g to obtain the generated sentence
since the alignment of the input and the generated sentence is to serve attribute independent semantic information the generator can be conditioned on any attribute namely a or
the cycle loss constrains the put to align with the input so that the content information can be preserved g log
one way to train the above cycle loss is by inforcement learning as done by luo et al
who use the loss function as a negative for content preservation
language modeling on outputs lmo
the above advo learns classiers to distinguish tween true samples and generated samples
such bag of words overlap bow
to mately measure content preservation bag of words bow features are used in john et al
bao et al

to focus on content information only john et al
exclude stopwords and specic words
denote the vocabulary set as v
we rst predict the distribution of bow features of the latent representation using softmax on the bow features
we then calculate the cross entropy loss of this bow distribution against the ground truth bow distribution in the put sentence
the bow loss is formulated as follows qbow log
adversarial bow overlap advbow
bow ensures the content to be fully captured in z
as a further step we want to ensure that the content information is exclusively captured in z namely not contained in a at all via the following advbow loss on a john et al
bao et al

when disentangling z and a in the lrs work we train an adversarial classier to predict the bow features given a by aligning it with the ground truth bow distribution namely minimizing
for the manipulation method chosen above select multiple appropriate loss functions for example shen et al
adopt ae and gan as the backbone the acc method to manipulate the latent representations and corresponding losses advr or

prototype editing despite a plethora of models that use end to end training of neural networks the prototype based text editing approach still attracts lots of tion since the emergence of deleteandretrieve li et al

prototype editing is reminiscent of traditional nlg which has higher ity and interpretability
different from black box end to end models we build an explicit pipeline for text style transfer from with attribute a to its counterpart with attribute
detect attribute markers of a in the input sentence and delete them resulting in a content only sentence
retrieve candidate attribute markers carrying the desired attribute qbow log
the nal min max objective is
inll the sentence by adding new attribute markers and make sure the generated sentence is uent
max e min qbow qbow
other losses rewards
there are also other losses rewards in recent works such as the noun overlap loss noun tian et al
as well as rewards for semantics and uency xu et al
gong et al
sancheti et al

we do not discuss them in much detail because they do not directly operate on the disentanglement of latent representations


summary inspired by the categorization of disentanglement works discussed above we summarize the ing workow of building a disentanglement based model
select a model as the backbone for the encoder decoder learning e

ae vae gan
select a manipulation method of the latent representation e

lre acc lrs an early approach for transferring formality is introduced by sheikha and inkpen who rst build a dictionary of formal and its ing informal expressions
then given an input sentence they rst detect the attribute markers by string matching with the dictionary items and then replace them by looking up the ing counter expression in the dictionary
their approach is based on the simplenlg framework gatt et al

with the advances of deep ing techniques the three steps are replaced by more statistically powerful models in later works
step attribute marker detection
ing attribute markers is a non trivial nlp task
traditional ways to do it involve rst using ging parsing and morphological analysis to select features and then ltering by mutual information and chi square testing
in recent deep learning pipelines there are three major types of approaches to identify attribute markers frequency ratio ods attention based methods and fusion methods
frequency ratio methods calculate some statistics for each n gram in the corpora
for ple li et al
detect the attribute markers by calculating its relative frequency of co occurrence with attribute a versus and those with cies higher than a threshold are considered the markers of a
using a similar approach madaan et al
rst calculate the ratio of mean idf between the two attribute corpora for each n gram then normalize this ratio across all ble n grams and nally mark those n grams with a normalized ratio p higher than a pre set threshold as attribute markers
attention based methods train an attribute classier using the attention mechanism bahdanau et al
and consider words with attention weights higher than average as markers xu et al

for the architecture of the classier zhang et al
use lstm and sudhakar et al
use a bert classier
fusion methods combine the advantages of the above two methods
for example wu et al
prioritize the attribute markers predicted by frequency ratio methods and use attention based methods as an auxiliary back up
one use case is when frequency ratio methods fail to identify any attribute markers in a given sentence they will use the attention based methods as a secondary choice to generate attribute markers
another case is to reduce false positives
to reduce the number of attribute markers that are wrongly recognized wu et al
set a threshold to lter out low quality attribute markers by frequency ratio methods and in cases where all attribute markers are deleted they use the markers predicted by attention based methods
there are still remaining limitations of the ous methods such as the pre set vocabulary lem of the frequency ratio methods imperfect racy of the attribute classier and unclear relation between attribute and attention scores
hence lee propose word importance scoring similar to what is used by jin et al
for adversarial paraphrasing to measure how important a token is to the attribute by the difference in the attribute probability of the original sentence and that after deleting a token
step target attribute retriever
after ing the attribute markers of the tence with attribute a we need to nd a the template of the sentence part attribute marker from another sentence carrying a different attribute
note the sentence template with all attribute ers deleted as
is similarly
a common approach is to nd the counterpart attribute marker by its context because the templates of the original attribute and its counter attribute marker should be similar
specically we rst match a plate with the most similar template in the opposite attribute corpus and then identify the attribute markers and as counterparts of each other
to match templates with their counterparts most vious works nd the nearest neighbors by the sine similarity of sentence embeddings
commonly used sentence embeddings include tf idf as used in li et al
sudhakar et al
averaged glove embedding distance used in li et al
sudhakar et al
and universal sentence coder cer et al
used in sudhakar et al

apart from sentence embeddings tran et al
use part of speech templates to match eral candidates in the opposite corpus and conduct an exhaustive search to ll parts of the candidate sentences into the masked positions of the original attribute markers
step generate from prototypes
li et al
and sudhakar et al
feed the only sentence template and new attribute markers into a pretrained language model that rearranges them into a natural sentence
this inlling cess can naturally be achieved by a masked guage model mlm malmi et al

for example wu et al
use mlm of the plate conditioned on the target attribute and this mlm is trained on an additional attribute cation loss using the model output and a xed pre trained attribute classier
since these eration practices are complicated madaan et al
propose a simpler way
they skip step that explicitly retrieves attribute candidates and instead directly learn a generation model that only takes attribute masked sentences as inputs
this generation model is trained on data where the attribute carrying sentences are paired with their templates
training on the pairs of constructed in the above way can make the model learn how to ll the masked sentence template with the target attribute a

back translation to convey more signal for training another method is to use back translation for the training dure
back translation is commonly used in both computer vision isola et al
lample et al
liu and tuzel taigman et al
zhu et al
and nlp tasks such as machine translation sennrich et al
artetxe et al
lample et al
a
there are two ways that back translation is usually applied tive back translation ibt which is the procedure of iteratively constructing pseudo parallel data and training on them and online back translation which is a loss function that can be combined with other training objectives
iterative back translation ibt
typically erative back translation ibt starts from ing two style transfer models which fers from the attribute a to the other attribute and which transfers from to a
the iterative training process of ibt is as follows
use the models to generate pseudo parallel corpora
specically generates pseudo pairs for all x and generates pairs of for all x
further train these two style transfer els on the corresponding datasets i
e
train on pairs and on pairs
for step in order to generate the initial parallel corpora a simple baseline is to randomly initialize the two models and and use them to translate the attribute of each sentence in x and x
however this simple initialization is subject to randomness and may not bootstrap well
another way adopted by zhang et al
borrows the idea from unsupervised machine translation lample et al
that rst learns an unsupervised word to word lation table between attribute a and and uses it to generate an initial pseudo parallel corpora
based on such initial corpora they train initial style transfer models and bootstrap the ibt cess
another model iterative matching and lation imat jin et al
does not learn the word translation table and instead uses a retrieval method
specically jin et al
empirically observe that semantically similar sentences in the two attribute corpora are likely to be the transferred sentence of each other
hence imat constructs the initial pseudo corpora by ing sentence pairs in the two attributed corpora according to the cosine similarity of pretrained tence embeddings
formally for each sentence its pseudo counterpart is the most similar sentence with it in the other attribute corpora x namely
such a retrieval based pseudo parallel data construction can also be seen in later approaches for vised machine translation ren et al

for step during the iterative process it is possible to encounter divergence as there is no constraint to ensure that each iteration will produce better pseudo parallel corpora than the previous iteration
one way to enhance the convergence of ibt is to add additional loss
for example attribute loss zhang et al
checks whether the erated sentence by back translation ts the desired attribute
specically zhang et al
pass the generated sentence through a pre trained style classier and use the cross entropy classication loss to back propagate the model
alternatively jin et al
use a checking mechanism instead of designing additional losses
at the end of each iteration their imat looks at all candidate pairs of an original sentence and uses word mover distance kusner et al
to select the sentence that has the desired attribute and is the closest to the original sentence
in this way ibt is less likely to diverge
online back translation
unlike ibt that erates a pseudo parallel corpus at each iteration and often requires specic bootstrapping and straints in iterations online back translation is to use back translation to formulate a loss as part of the end to end training prabhumoye et al
jin et al

online bt s generator takes as input the latent representation z of an input sentence and an assigned attribute
the attribute can be either the original attribute of the input sentence or a different attribute that is assigned
as we need to force the decoder to only look at the to trol its generation style one way is to make sure only contains the content but not any attribute by enforcing disentanglement losses such as the ae loss used in logeswaran et al
dae loss used in fu et al
and machine tion loss prabhumoye et al

these losses are used to learn an encoder which encodes the attribute carrying sentence into a latent variable that carries the semantics of the input
in contrast another way that does not require disentanglement is to enforce the decoder pay attention only to the attribute of and ignores the attribute inside z by denoising the input sentence because this ing involves deleting random words including the attribute words lample et al

note that the online bt method has been extended to multiple attributes but not limited to two attributes lample et al
logeswaran et al

discussions
can style be separated from semantics the semantics of text refers the subject matter or the argument that the author wants to convey whereas the attribute can include the unique voice of the author expressed through the use of tain stylistic devices such as metaphors as well as choice of words syntactic structures and so on
attributes can also go beyond the sentence level to the discourse level such as the stylistic ture of the entire piece of the work e

stream of consciousness or ashbacks
in some cases it can be difcult or impossible to separate attributes from meaning
one reason is that the subject that the author is going to write about can interfere the choice of writing style
for example a science ction can use the rst person voice and fancy owery tone when describing a place
another reason is that many stylistic devices such as allusion depend on content words
nonetheless to proceed forward in the tst eld it is still a good idea for the current stage to plify the problem and address scenarios where the attribute and semantics can be approximately arated leaving the more challenging interwoven nature of attributes and semantics to future work
in practice a generally acceptable way is to leave the judgment to the human annotators which is why human evaluation is crucial in the context of tst tasks
is sentiment a style a majority of existing methods developed for text style transfer adopted the sentiment manipulation task for performance evaluation shen et al
fu et al
yang al

exemplary datasets for this task are the yelp review dataset shen et al
and amazon review dataset he and mcauley
however it is arguable whether the sentiment of a text can be deemed its style
from the linguistic perspective sentiments such as the polarity of views change the semantics of a sentence
from the perspective of the deep learning methods for tst most of the current deep learning methods mentioned in this survey can be applied to and validated on any datasets labeled with different attributes which naturally include sentiment ication datasets political slant datasets and so on

from traditional nlg to deep learning despite the exciting methodological revolution led by deep learning recently we are also interested in the merging point of traditional computational linguistics and the deep learning techniques derson
specic to the context of tst we will introduce the traditional nlg framework and its impact on the current tst approaches cially the prototype editing method
traditional nlg framework
the traditional nlg framework stages sentence generation into the following steps reiter and dale
content determination not applicable
discourse planning not applicable
sentence aggregation
lexicalization
referring expression generation
linguistic realisation the rst two steps content determination and course planning are not applicable to most datasets because the current focus of tst is sentence level and not discourse level
among the step to sentence aggregation groups necessary information into a single tence lexicalization chooses the right word to press the concepts generated by sentence gation referring expression generation produces surface linguistic forms for domain entities and linguistic realization edits the text so that it forms to grammar including syntax morphology and orthography
this framework is widely applied to nlg tasks e

zue and glass mani mctear gatt and reiter droutsopoulos and malakasiotis
re viewing prototype based tst
among the three main approaches for tst namely glement prototype editing and back translation the most relevant with the traditional nlg is the prototype based text editing which has been duced in section

using the language of the traditional nlg framework the prototype based techniques can be viewed as a combination of sentence aggregation lexicalization and linguistic realization
cally prototype based techniques rst prepare an attribute free sentence template and supply it with candidate attribute markers that carries the desired attribute both of which are sentence aggregation
then using language models to inll the prototype with the correct expressions corresponds to calization and linguistic realization
note that the existing tst systems do not explicitly deal with referring expression generation e

generating co references leaving it to be handled by language models
other than tst prototype based nlg can also be seen on other deep learning models
for ample guu et al
improves lm by rst sampling a lexically similar sentence prototype and then editing it using variational encoder and coders
this prototype and then edit approach can also be seen in summarization cao and xiong wang et al
machine translation wu et al
gu et al
zhang et al
and tezcan conversation eration weston et al
cai et al
code generation hashimoto et al
and question answering lewis et al

as an extension to the retrieve and edit steps hossain et al
use an ensembled approach to retrieve a set of relevant prototypes edit and nally rerank to pick the best output for machine translation
such extension can also be potentially applied to text style transfer
there are several advantages of addressing text style transfer by aligning with traditional nlg steps
first sentence planning like steps make the generated contents more controllable
for ple the template of the original sentence is saved and the counterpart attributes can also be explicitly retrieved as a preparation for the nal rewriting
such a controllable white box approach can be easy to tune debug and improve
the accuracy of attribute marker extraction for example is stantly improving across literature sudhakar et al
and different ways to extract attribute ers can be easily fused wu et al

second sentence planning like steps ensure the ness of information
as most content words are kept and no additional information is hallucinated by the black box neural networks we can better sure that the information of the attribute transferred output is consistent with the original input

ethical considerations recently there are more and more attentions paid to ethical problems of ai research
we discuss in the following two ethical considerations ity of applications and data privacy problem of text style transfer
in some elds that involve human subjects or direct application to humans such as biomedics people have compiled a set of principles of lines beauchamp et al

before initiating a research project responsible research bodies use these principles as a ruler to judge whether the search is ethically correct to start
different from the biomedical domain which is regulated under a central regulatory board institutional review board irb the nlp research community has not yet formulated such a board to scrutinize ethically sensitive projects so we want to provide as many guidelines as possible to avoid ethical misconduct in future publications on text style transfer


social impact of applications technologies can have unintended negative quences hovy and spruit
for example tst can facilitate the automation of intelligent sistants with designed attributes but can also be used to create fake text or fraud
thus inventors of a technology should beware how other people very probably adopt this nology for their own incentives
for tst since it has a wide range of subtasks and applications we examine each of them with the following two questions who will benet from such a technology who will be harmed by such a technology although many ethical issues are debatable we try to categorize the text attribute tasks into three ethical levels benecial neutral and tasks that can be obvious double sided swords
benecial an important direction of nlp for cial good is to ght against abusive online text
text style transfer can serve as a very helpful tool as it can be used to transfer malicious text to mal language
shades of abusive language include hate speech offensive language sexist and racist language aggression profanity cyberbullying rassment trolling and toxic language waseem et al

there are also other negative text such as propaganda bernays carey and others
it is widely known that malicious text is harmful to people
for example research shows that cyberbullying victims tend to have more stress and suicidal ideation kowalski et al
and also detachment from family and ofine tion oksanen et al

there are more and more efforts put into combating toxic language such as k content moderators that facebook and instagram employ harrison
therefore the automatic malicious to normal language transfer can be a helpful intelligent assistant to address such needs
apart from purifying malicious text on cial media it can also be used on social chatbots to make sure there are no bad contents in language they generate roller et al

neutral most text style transfer tasks are neutral
for example informal to formal transfer can be used as a writing assistant to help make writings more professional and formal to informal transfer can tune the tone of bots to be more casual
most applications to customized the persona of bots are also neutral with regard to their societal impact
double sided sword besides positive and there are unfortunately tral applications eral text style transfer tasks that are double sided swords
for example of the most popular task sentiment modication although it can be used to change intelligent assistants or robots from a negative to positive mood which is unlikely to harm any parties the vast majority of papers plies this technology to manipulate the polarity of reviews such as yelp shen et al
and zon reviews he and mcauley
actually this technique of changing a negative restaurant view to a positive comment or vice versa is highly skeptical
it can be used as a cheating method for the commercial body to polish its reviews or harm the reputation of their competitors
once this nology is used it will automatically manipulate the online text to contain polarity that the model owner desires
hence we suggest the research community raise serious concern against the review sentiment modication task
another task political slant transfer may induce concerns within some specic context
for ple social bots i
e
autonomous bots on social media such as twitter bots and facebook bots are a big problem in the u
s
even playing a signicant role in the united states presidential election bessi and ferrara shao et al

it is reported that at least bots were ble for about of the total tweets
social bots usually target to advocate certain ideas supporting campaigns or aggregating other sources either by acting as a follower gathering followers itself
so the political slant transfer task which transfers the tone and content between republican comments and democratic ones are highly tive and may face the risk of being used on social bots to manipulate political views of the mass
some more arguable ones are male to female tone transfer which can be potentially used for fraud
the cheater can create an online account and pretend to be an attractive young lady
note that it is arguable because the other direction to male tone transfer can be used for positive plications such as authorship obfuscation shetty et al
which anonymizes the author attributes by hiding the gender of a female author by synthesizing the text in the style of the male class


data privacy another ethical concern is the use of data in the research practice
researchers should not mine user data such as the demographic identities
such data privacy widely exist in the data science community as a whole and there have been many ethical discussions tse et al
russell et al

although the text style transfer task needs data containing some attributes along with the text tent
it is totally ne to have ratings of reviews that are classied into positive and negative but the user attributes are sensitive including the gender of the user s account prabhumoye et al
and age lample et al

these sensitive user attributes should be collected with each user ing to a form of consent
where is tst in nlp in this section we will connect tst with many other nlp tasks including its closely related tasks in section
and applications to other nlp tasks


closely related tasks conditional text generation can be used for conditioned language modeling and persona based story generation prabhumoye et al

style conditioned language modeling
ferent from language modeling that learns how to generate general natural language text tional language modeling learns how to generate text given a condition such as some context or a control code pfaff poplack
recent advances of conditional language models keskar et al
dathathri et al
also includes text generation conditioned on a style token such as positive or negative
however they are currently limited to a small set of pre dened condition kens and can only generate from scratch a sentence but not yet able to be conditioned on an original sentence for style rewriting
counterfactual story rewriting
tual story rewriting aims to learn a new event quence in case of perturbation of one previous event i
e
counterfactual condition goodman starr
qin et al
propose the rst dataset of the task timetravel each sample of which takes an originally sentence story and changes the event in the second sentence to a new counterfactual event
the task is to generate the last three sentences of the story based on the newly altered second sentence that initiates the story
the criteria of the counterfactual story rewriting include relevance with the rst two sentences and minimal edit from the original story ending
the motivation and dataset nature is different from the general text style transfer and more importantly this task is not conditioned on a predened categorized style token but the free form textual story beginning
contrastive text generation
as neural network based nlp models are easy to learn spurious statistical correlations in the data rather than robust understanding jia and liang there emerges recent works to construct auxillary datasets composed of near misses of the original data
for example gardner et al
ask crowdsource workers to rewrite the input of the task by minimal changes but will lead to a different output
to alleviate from expensive human labor xing et al
develop an automatic text editing approach to generate contrast set for aspect based sentiment analysis
the difference between contrastive text generation and text style transfer is that the former does not require content preservation but mainly aims to construct a slightly textually different input that can result in a change to test the model of the ground truth output robustness
neural machine translation
the problem tings of neural machine translation and text style transfer have much in common the source and target language in neural machine translation is analogous to the original and desired attribute a and respectively
the major difference is that in nmt the source and target corpora are completely different languages which have almost disjoint cabulary whereas in text style transfer the input and output are in the same language and the model is usually encouraged to copy most content words from input such as the bow loss introduced in section



image style transfer
neural style transfer rst originates in computer vision gatys et al

however for images it is feasible to disentangle the explicit reresentation of the image texture as the gram matrix of image neural feature vectors
text attributes do not ahe such an explicit sentation but on a more abstract level related to human understanding of language

applications to other nlp tasks text style transfer has many applications to other important nlp tasks which will be elaborated low
paraphrasing and data augmentation
one of the most direct chain of tst s applications is rst paraphrasing and then data augmentation
for example paraphrasing can be achieved by ring text from one style to another e

informal to formal text as long as the styles do not terfere with the downstream task
several works have already noticed the connection of text style transfer and paraphrasing krishna et al
yamshchikov et al

and such style differing paraphrasing can be used for a further downstream task data augmentation which generates similar training data to enlarge the dataset size
this plication already takes place in computer vision zheng et al
jackson et al
although not yet in nlp
adversarial robustness probing
another use of style transferred text is adversarial robustness probing
for example styles that are task agnostic can be used for general adversarial attack e

liteness transfer to probe sentiment classication robustness while the styles that can change the task output can be used to construct contrast sets e

sentiment transfer to probe sentiment cation robustness xing et al

persona consistent dialog generation
other useful downstream application is consistent dialog generation li et al
zhang et al
shuster et al

since tional agents directly interact with users there is a strong demand for human like dialog generation
previously this is done by encoding speaker traits into a vector and the conversation is then tioned on this vector li et al

as future work text style transfer can also be used as part of the pipeline of persona based dialog generation where the persona can be categorized into tive style types and then the generated text can be post processed by a style transfer model
deanonymization
text style transfer can also be used for deanonymization which is an tant application to protect user privacy especially since there are heated discussions of ethics in the recent articial intelligence community
the criminative task of author proling by stylometry can mine the demographic identities of the author of a writing even including privacy invading erties such as gender and age schler et al
so one possible remedy is to use tst to alter the text and obfuscate the real identity of the users reddy and knight grondahl and asokan
summarization
tst can be applied to other tasks such as summarization
for example jin et al
li et al
use tst to erate eye catchy headlines
moreover jin et al
manage to generate headlines with three different styles including humorous romantic and clickbaity styles
style specic machine translation
moreover tst can also be integrated into machine tion and empower machine translation with the control of style
commonly used styles are ity niu et al
and politeness sennrich et al

for example wu et al
translates from informal chinese to formal english
future directions based on this survey we identify some important future directions of tst
loosening the assumptions a common sumption for most deep learning based tst works as mentioned in section
is the availability of style specic corpora for each style of interest ther parallel or non parallel
this assumption can potentially be loosened
for example xu et al
use unsupervised representation learning to separate the style and contents from a mixed corpus of unspecied styles
guo et al
use cycle training with a conditional variational auto encoder to unsupervisedly learn to express the same semantics through different styles
oretically although disentanglement is impossible without inductive biases or other forms of vision locatello et al
disentanglement is achievable with some weak signals such as only knowing how many factors have changed but not which ones locatello et al

more difcult nature in terms of the difculty in methodology the current sentence level tst can potentially be extended with the following ities aspect based style transfer e

transferring the sentiment on one aspect but not the other aspects on aspect based sentiment analysis data authorship transfer which has highly coupled style and content document level style transfer which includes discourse planning domain adaptive style transfer which is ceded by li et al
better evaluation metrics there has been lots of attention on the evaluation metrcs of tst pang and gimpel mir et al
pang yamshchikov et al

apart from the overall scoring of methods we can also add specic guistic categories like a checklist to evaluate what capabilities the tst model has achieved
for ple there can be a checklist for formality transfer according to existing style guidelines such as the apa style guide association et al

such a checklist can make the black box deep learning models more interpretable and also it can allow for more insightful error analysis
more subtasks apart from the subtasks and datasets listed in table we also suggest research into non native to native transfer i
e
ing grammatical error correction with tst factual to empathetic transfer applications to more nlp tasks tst can also be applied to more nlp tasks such as style transfer for data augmentation e

resource machine translation style transfer for adversarial text generation conclusion this paper presents a comprehensive review of text style transfer with deep learning methods
we have surveyed recent research efforts in tst and oped schemes to categorize and distill the existing literature
this survey has covered the task lation evaluation metrics and methods on parallel and non parallel data
we also discussed several important topics on tst its connection to other nlp tasks and important future directions
this survey provide a reference for future researchers working on tst
acknowledgement we thank qipeng guo for his insightful discussions and constructive suggestions
references ion androutsopoulos and prodromos malakasiotis

a survey of paraphrasing and textual ment methods
journal of articial intelligence search
shlomo argamon moshe koppel jonathan fine and anat rachel shimoni

gender genre and writing style in formal written texts
text talk
mikel artetxe gorka labaka eneko agirre and kyunghyun cho

unsupervised neural in international conference chine translation
on learning representations iclr ver bc canada april may ence track proceedings
openreview
net
american psychological association et al

lication manual
american psychological tion washington dc
dzmitry bahdanau kyunghyun cho and yoshua gio

neural machine translation by jointly in learning to align and translate
national conference on learning representations iclr san diego ca usa may conference track proceedings
satanjeev banerjee and alon lavie

meteor an automatic metric for mt evaluation with proved correlation with human judgments
in ceedings of the acl workshop on intrinsic and trinsic evaluation measures for machine tion summarization pages ann bor michigan
association for computational guistics
yu bao hao zhou shujian huang lei li lili mou olga vechtomova xin yu dai and jiajun chen

generating sentences from disentangled in proceedings of the tactic and semantic spaces
annual meeting of the association for putational linguistics pages florence italy
association for computational linguistics
john a bateman and cecile paris

phrasing a in ijcai text in terms the user can understand
pages
tom l beauchamp james f childress al

principles of biomedical ethics
oxford university press usa
laurens van den bercken robert jan sips and christoph lo

evaluating neural text cation in the medical domain
in the world wide web conference www san francisco ca usa may pages
acm
edward l bernays

propaganda
ig publishing
alessandro bessi and emilio ferrara

social bots distort the us presidential election online cussion
first monday
constantinos boulis and mari ostendorf

a quantitative analysis of lexical differences between in acl genders in telephone conversations
annual meeting of the association for tational linguistics proceedings of the conference june university of michigan usa pages
the association for computer guistics
bram bulte and arda tezcan

neural fuzzy repair integrating fuzzy matches into neural chine translation
in proceedings of the ence of the association for computational tics acl florence italy july august volume long papers pages
sociation for computational linguistics
deng cai yan wang wei bi zhaopeng tu xiaojiang liu wai lam and shuming shi

skeleton response dialogue generation guided by retrieval memory
arxiv preprint

qian cao and deyi xiong

encoding gated lation memory into neural machine translation
in proceedings of the conference on empirical methods in natural language processing brussels belgium october november pages
association for computational tics
yixin cao ruihao shui liangming pan min yen kan zhiyuan liu and tat seng chua

expertise style transfer a new task towards better cation between experts and laymen
in proceedings of the annual meeting of the association for computational linguistics pages line
association for computational linguistics
alex carey

taking the risk out of democracy corporate propaganda versus freedom and liberty
university of illinois press
kyunghyun cho bart van merrienboer dzmitry danau and yoshua bengio

on the properties of neural machine translation encoder decoder proaches
arxiv preprint

jacob cohen

a coefcient of agreement for nominal scales
educational and psychological surement
ning dai jianze liang xipeng qiu and xuanjing huang

style transformer unpaired text style transfer without disentangled latent in proceedings of the annual tation
ing of the association for computational tics pages florence italy
association for computational linguistics
sumanth dathathri andrea madotto janice lan jane hung eric frank piero molino jason yosinski and rosanne liu

plug and play language models a simple approach to controlled text generation
in international conference on learning tations iclr addis ababa ethiopia april
openreview
net
jacob devlin ming wei chang kenton lee and kristina toutanova

bert pre training of deep bidirectional transformers for language in proceedings of the conference standing
of the north american chapter of the association for computational linguistics human language technologies naacl hlt minneapolis mn usa june volume long and short pers pages
association for tional linguistics
keith carlson allen riddell and daniel rockmore

evaluating prose style transfer with the bible
royal society open science
jessica ficler and yoav goldberg

controlling linguistic style aspects in neural language tion
corr

daniel cer yinfei yang sheng yi kong nan hua nicole limtiaco rhomni st
john noah constant mario guajardo cespedes steve yuan chris tar brian strope and ray kurzweil

in proceedings sal sentence encoder for english
of the conference on empirical methods in natural language processing emnlp tem demonstrations brussels belgium october november pages
association for computational linguistics
tuhin chakrabarty smaranda muresan and nanyun peng

generating similes effortlessly like a pro a style transfer approach for simile generation
in proceedings of the conference on empirical methods in natural language processing emnlp online november pages
association for computational linguistics
david l
chen and william b
dolan

ing highly parallel data for paraphrase evaluation
in the annual meeting of the association for computational linguistics human language nologies proceedings of the conference june portland oregon usa pages
the association for computer linguistics
john l fischer

social inuences on the choice of a linguistic variant
word
zhenxin fu xiaoye tan nanyun peng dongyan zhao and rui yan

style transfer in text ration and evaluation
in proceedings of the second aaai conference on articial intelligence the innovative applications of cial intelligence and the aaai posium on educational advances in articial ligence new orleans louisiana usa february pages
aaai press
chuang gan zhe gan xiaodong he jianfeng gao and li deng

stylenet generating attractive in ieee visual captions with styles
ence on computer vision and pattern recognition cvpr honolulu hi usa july pages
ieee computer society
matt gardner yoav artzi victoria basmova jonathan berant ben bogin sihao chen pradeep dasigi dheeru dua yanai elazar ananth gottumukkala nitish gupta hannaneh hajishirzi gabriel ilharco daniel khashabi kevin lin jiangming liu son f
liu phoebe mulcaire qiang ning sameer singh noah a
smith sanjay subramanian reut tsarfaty eric wallace ally zhang and ben zhou

evaluating models local decision boundaries via contrast sets
in proceedings of the ference on empirical methods in natural language processing findings emnlp online event november pages
tion for computational linguistics
nikesh garera and david yarowsky

modeling latent biographic attributes in conversational genres
in acl proceedings of the annual ing of the association for computational linguistics and the international joint conference on ral language processing of the afnlp august singapore pages
the association for computer linguistics
albert gatt francois portet ehud reiter jim hunter saad mahamood wendy moncur and somayajulu sripada

from data to text in the neonatal tensive care unit using nlg technology for sion support and information management
ai mun

albert gatt and ehud reiter

simplenlg a in alisation engine for practical applications
ceedings of the european workshop on natural language generation enlg pages
leon a
gatys alexander s
ecker and matthias image style transfer using bethge

in ieee conference tional neural networks
on computer vision and pattern recognition cvpr las vegas nv usa june pages
ieee computer society
hongyu gong suma bhat lingfei wu jinjun xiong and wen mei hwu

reinforcement ing based text style transfer without parallel ing corpus
in proceedings of the conference of the north american chapter of the association for computational linguistics human language technologies volume long and short papers pages minneapolis minnesota
ation for computational linguistics
ian j
goodfellow jean pouget abadie m
mirza bing xu david warde farley sherjil ozair aaron c
courville and yoshua bengio

erative adversarial nets
in nips
nelson goodman

the problem of factual conditionals
the journal of philosophy
tommi grondahl and n
asokan

effective writing style transfer via combinatorial paraphrasing
proc
priv
enhancing technol

jiatao gu zhengdong lu hang li and victor o
k
incorporating copying mechanism in li

in proceedings of sequence to sequence learning
the annual meeting of the association for putational linguistics volume long papers pages berlin germany
association for computational linguistics
jiatao gu yong wang kyunghyun cho and victor o
k
li

search engine guided neural in proceedings of the chine translation
second aaai conference on articial intelligence the innovative applications of cial intelligence and the aaai posium on educational advances in articial ligence new orleans louisiana usa february pages
aaai press
c aglar gulcehre sungjin ahn ramesh nallapati bowen zhou and yoshua bengio

pointing the unknown words
in proceedings of the nual meeting of the association for computational linguistics acl august berlin germany volume long papers
the association for computer linguistics
qipeng guo zhijing jin ziyu wang xipeng qiu weinan zhang jun zhu zheng zhang and david wipf

fork or fail cycle consistent arxiv preprint ing with many to one mappings


kelvin guu tatsunori b
hashimoto yonatan oren and percy liang

generating sentences by editing prototypes
trans
assoc
comput
tics
sara harrison

twitter and instagram unveil new ways to combat hate again
tatsunori b
hashimoto kelvin guu yonatan oren and percy liang

a retrieve and edit work for predicting structured outputs
in advances in neural information processing systems nual conference on neural information processing systems neurips december montreal canada pages
ruining he and julian mcauley

ups and downs modeling the visual evolution of fashion trends with one class collaborative ltering
in proceedings of the international conference on world wide web pages
james henderson

the unstoppable rise of putational linguistics in deep learning
in ings of the annual meeting of the association for computational linguistics acl online july pages
association for computational linguistics
felix hill kyunghyun cho and anna korhonen

learning distributed representations of sentences in naacl hlt the from unlabelled data
conference of the north american chapter of the association for computational linguistics man language technologies san diego california usa june pages
the sociation for computational linguistics
nabil hossain marjan ghazvininejad and luke simple and effective retrieve moyer

in proceedings of the rerank text generation
annual meeting of the association for tional linguistics acl online july pages
association for tional linguistics
robin jia and percy liang

adversarial amples for evaluating reading comprehension in proceedings of the conference on tems
empirical methods in natural language processing emnlp copenhagen denmark september pages
association for tational linguistics
dirk hovy and shannon l
spruit

the social impact of natural language processing
in ings of the annual meeting of the association for computational linguistics acl august berlin germany volume short papers
the association for computer linguistics
eduard hovy

generating natural language der pragmatic constraints
journal of pragmatics
eduard h
hovy

pragmatics and natural guage generation
artif
intell

zhiting hu zichao yang xiaodan liang r
dinov and e
xing

toward controlled ation of text
in icml
chenyang huang osmar zaane amine trabelsi and nouha dziri

automatic dialogue generation in proceedings of the with expressed emotions
conference of the north american chapter of the association for computational linguistics man language technologies volume short pers pages new orleans louisiana
ciation for computational linguistics
yufang huang wentao zhu deyi xiong yiye zhang changjian hu and feiyu xu

cycle consistent adversarial autoencoders for arxiv preprint supervised text style transfer


phillip isola jun yan zhu tinghui zhou and alexei a
efros

image to image tion with conditional adversarial networks
in ieee conference on computer vision and pattern recognition cvpr honolulu hi usa july pages
ieee computer ciety
philip t
g
jackson amir atapour abarghouei stephen bonner toby p
breckon and boguslaw obara

style augmentation data in ieee conference tion via style randomization
on computer vision and pattern recognition shops cvpr workshops long beach ca usa june pages
computer sion foundation ieee
eric jang shixiang gu and ben poole

ical reparameterization with gumbel softmax
arxiv preprint

harsh jhamtani varun gangal eduard h
hovy and eric nyberg

shakespearizing modern guage using copy enriched sequence to sequence models
corr

di jin zhijing jin joey tianyi zhou lisa orii and ter szolovits

hooks in the headline ing to generate headlines with controlled styles
in proceedings of the annual meeting of the ciation for computational linguistics pages online
association for computational guistics
di jin zhijing jin joey tianyi zhou and peter szolovits

is bert really robust a strong baseline for natural language attack on text in the thirty fourth sication and entailment
aaai conference on articial intelligence aaai the thirty second innovative applications of articial intelligence conference iaai the tenth aaai symposium on educational advances in articial intelligence eaai new york ny usa february pages
aaai press
di jin zhijing jin joey tianyi zhou and peter szolovits

unsupervised domain adaptation for neural machine translation with iterative back translation
arxiv preprint

zhijing jin di jin jonas mueller nicholas matthews and enrico santus

imat unsupervised text attribute transfer via iterative matching and in proceedings of the conference on tion
empirical methods in natural language processing and the international joint conference on ural language processing emnlp ijcnlp hong kong china november pages
association for computational tics
vineet john lili mou hareesh bahuleyan and olga vechtomova

disentangled representation learning for non parallel text style transfer
in ceedings of the annual meeting of the tion for computational linguistics pages florence italy
association for computational guistics
tomoyuki kajiwara

negative lexically strained decoding for paraphrase generation
in ceedings of the annual meeting of the ciation for computational linguistics pages florence italy
association for tional linguistics
nitish shirish keskar bryan mccann lav r varshney caiming xiong and richard socher

ctrl a conditional transformer language model for lable generation
arxiv preprint

yoon kim

convolutional neural networks for sentence classication
in proceedings of the conference on empirical methods in natural guage processing emnlp october doha qatar a meeting of sigdat a special interest group of the acl pages
acl
diederik p
kingma and m
welling

encoding variational bayes
corr

robin m kowalski gary w giumetti amber n schroeder and micah r lattanner

ing in the digital age a critical review and analysis of cyberbullying research among youth
psychological bulletin
klaus krippendorff

content analysis an duction to its methodology
sage publications
kalpesh krishna john wieting and mohit iyyer

reformulating unsupervised style transfer as phrase generation
corr

matt kusner yu sun nicholas kolkin and kilian weinberger

from word embeddings to ment distances
in international conference on chine learning pages
robin lakoff

language and woman s place
language in society
guillaume lample alexis conneau ludovic denoyer and marcaurelio ranzato

unsupervised machine translation using monolingual corpora only
in international conference on learning resentations iclr vancouver bc canada april may conference track ings
openreview
net
guillaume lample myle ott alexis conneau dovic denoyer and marcaurelio ranzato

phrase based neural unsupervised machine in proceedings of the conference on lation
empirical methods in natural language ing brussels belgium october november pages
association for tional linguistics
guillaume lample eric michael marcaurelio ranzato
multiple attribute text rewriting
in iclr
ludovic subramanian denoyer and y lan boureau
sandeep smith guillaume lample neil zeghidour nicolas usunier antoine bordes ludovic denoyer and marcaurelio ranzato

fader networks manipulating images by sliding attributes
in advances in neural information processing systems annual conference on neural information processing systems december long beach ca usa pages
patrick s
h
lewis ethan perez aleksandra tus fabio petroni vladimir karpukhin naman goyal heinrich kuttler mike lewis wen tau yih tim rocktaschel sebastian riedel and douwe kiela

retrieval augmented ation for knowledge intensive nlp tasks
corr

dianqi li yizhe zhang zhe gan yu cheng chris brockett bill dolan and ming ting sun

main adaptive text style transfer
in proceedings of the conference on empirical methods in ural language processing and the international joint conference on natural language processing emnlp ijcnlp hong kong china ber pages
association for computational linguistics
jiwei li michel galley chris brockett georgios p
spithourakis jianfeng gao and william b
dolan

a persona based neural conversation model
in proceedings of the annual meeting of the sociation for computational linguistics acl august berlin germany volume long papers
the association for computer guistics
juncen li robin jia he he and percy liang

delete retrieve generate a simple approach to timent and style transfer
in proceedings of the conference of the north american chapter of the association for computational linguistics human language technologies volume long papers pages new orleans louisiana
ation for computational linguistics
mingzhe li xiuying chen min yang shen gao dongyan zhao and rui yan

the content duality of attractiveness learning to write eye catching headlines via disentanglement
arxiv preprint

yuan li chunyuan li yizhe zhang xiujun li qing zheng lawrence carin and jianfeng gao

complementary auxiliary classiers for in the label conditional text generation
fourth aaai conference on articial intelligence aaai the thirty second innovative cations of articial intelligence conference iaai the tenth aaai symposium on educational advances in articial intelligence eaai new york ny usa february pages
aaai press
yi liao lidong bing piji li shuming shi wai lam and tong zhang

quase sequence editing under quantiable guidance
in proceedings of the conference on empirical methods in ral language processing pages sels belgium
association for computational guistics
joosung lee

stable style transformer delete and generate approach with encoder decoder for text style transfer
corr

chin yew lin and franz josef och

matic evaluation of machine translation quality ing longest common subsequence and skip bigram statistics
in proceedings of the annual ing of the association for computational linguistics pages barcelona spain
dayiheng liu jie fu yidan zhang chris pal and jiancheng lv

revision in continuous space unsupervised text style transfer without adversarial in the thirty fourth aaai conference learning
on articial intelligence aaai the second innovative applications of articial gence conference iaai the tenth aaai posium on educational advances in articial ligence eaai new york ny usa february pages
aaai press
ming yu liu and oncel tuzel

coupled in advances in erative adversarial networks
ral information processing systems annual conference on neural information processing tems december barcelona spain pages
francesco locatello stefan bauer mario lucic ratsch sylvain gelly bernhard scholkopf and olivier bachem

challenging common sumptions in the unsupervised learning of gled representations
in proceedings of the ternational conference on machine learning icml june long beach california usa volume of proceedings of machine learning search pages
pmlr
francesco locatello ben poole gunnar ratsch hard scholkopf olivier bachem and michael tschannen

weakly supervised ment without compromises
in proceedings of the international conference on machine learning icml july virtual event volume of proceedings of machine learning research pages
pmlr
lajanugen logeswaran honglak lee and samy gio

content preserving text generation with in advances in neural attribute controls
mation processing systems annual conference on neural information processing systems neurips december montreal canada pages
fuli luo peng li jie zhou pengcheng yang baobao chang zhifang sui and xu sun

a dual forcement learning framework for unsupervised text in proceedings of the style transfer
tional joint conference on articial intelligence cai
xinyao ma maarten sap hannah rashkin and yejin choi

powertransformer unsupervised trollable revision for biased language correction
in proceedings of the conference on empirical methods in natural language processing emnlp online november pages
association for computational linguistics
aman madaan amrith setlur tanmay parekh barnabas poczos graham neubig yiming yang ruslan salakhutdinov alan w
black and shrimai prabhumoye

politeness transfer a tag and generate approach
in proceedings of the nual meeting of the association for computational linguistics acl online july pages
association for computational linguistics
mairesse and marilyn a
walker

trolling user perceptions of linguistic style able generation of personality traits
comput
guistics
eric malmi aliaksei severyn and sascha rothe

unsupervised text style transfer with padded in proceedings of the masked language models
conference on empirical methods in ral language processing emnlp online november pages
ation for computational linguistics
eric malmi aliaksei severyn and sascha rothe

unsupervised text style transfer with padded in proceedings of the masked language models
conference on empirical methods in natural language processing emnlp pages online
association for computational linguistics
inderjeet mani

automatic summarization ume
john benjamins publishing
david d
mcdonald and james pustejovsky

a computational theory of prose style for natural guage generation
in eacl conference of the european chapter of the association for putational linguistics march sity of geneva geneva switzerland pages
the association for computer linguistics
michael f mctear

spoken dialogue technology enabling the conversational user interface
acm computing surveys csur
thomas corwin mendenhall

the characteristic curves of composition
science
stephen merity caiming xiong james bradbury and r
socher

pointer sentinel mixture models
arxiv

remi mir bjarke felbo nick obradovich and iyad rahwan

evaluating style transfer for text
in proceedings of the conference of the north american chapter of the association for tional linguistics human language technologies volume long and short papers pages minneapolis minnesota
association for tional linguistics
lili mou and olga vechtomova

stylized text in generation approaches and applications
ceedings of the annual meeting of the ciation for computational linguistics tutorial stracts pages online
association for putational linguistics
jonas mueller david gifford and tommi jaakkola

sequence to better sequence continuous vision of combinatorial structures
in international conference on machine learning pages
maja popovic

chrf character n gram score for automatic mt evaluation
in proceedings of the tenth workshop on statistical machine translation pages lisbon portugal
association for computational linguistics
anthony mulac lisa b studley and sheridan blau

the gender linked language effect in primary sex and secondary students impromptu essays
roles
tong niu and mohit bansal

polite dialogue eration without parallel data
transactions of the sociation for computational linguistics
xing niu marianna j
martindale and marine carpuat

a study of style in machine translation trolling the formality of machine translation in proceedings of the conference on put
empirical methods in natural language processing emnlp copenhagen denmark september pages
association for tational linguistics
xing niu sudha rao and marine carpuat

multi task neural models for translating between styles within and across languages
in proceedings of the international conference on tional linguistics pages santa fe new mexico usa
association for computational guistics
atte oksanen james hawdon emma holkeri matti nasi and pekka rasanen

exposure to line hate among young social media users
in soul of society a focus on the lives of children youth
emerald group publishing limited
richard yuanzhe pang

the daunting task of real world textual style transfer auto evaluation
corr

richard yuanzhe pang and kevin gimpel

supervised evaluation metrics and learning criteria arxiv preprint for non parallel textual transfer


kishore papineni salim roukos todd ward and jing zhu

bleu a method for automatic in proceedings of uation of machine translation
the annual meeting of the association for putational linguistics pages philadelphia pennsylvania usa
association for computational linguistics
carol w pfaff

constraints on language ing intrasentential code switching and borrowing in spanish english
language pages
matt post and david vilar

fast lexically strained decoding with dynamic beam allocation for in proceedings of the neural machine translation
conference of the north american chapter of the association for computational linguistics man language technologies volume long pers pages new orleans louisiana
association for computational linguistics
richard power donia scott and nadjet in agha

generating texts with style
ternational conference on intelligent text ing and computational linguistics pages
springer
shrimai prabhumoye khyathi raghavi chandu lan salakhutdinov and alan w
black

my way of telling a story persona based grounded story generation
corr

shrimai prabhumoye yulia tsvetkov ruslan style transfer dinov and alan w black

in proceedings of the through back translation
annual meeting of the association for putational linguistics volume long papers pages melbourne australia
association for computational linguistics
reid pryzant richard diehl martinez nathan dass sadao kurohashi dan jurafsky and diyi yang

automatically neutralizing subjective bias in text
in the thirty fourth aaai conference on cial intelligence aaai the thirty second novative applications of articial intelligence ference iaai the tenth aaai symposium on educational advances in articial intelligence eaai new york ny usa february pages
aaai press
lianhui qin antoine bosselut ari holtzman dra bhagavatula elizabeth clark and yejin choi

counterfactual story reasoning and in proceedings of the conference on tion
empirical methods in natural language processing and the international joint conference on ural language processing emnlp ijcnlp hong kong china november pages
association for computational tics
alec radford jeffrey wu rewon child david luan dario amodei and ilya sutskever

language models are unsupervised multitask learners
openai blog
shana poplack

sometimes i ll start a sentence in spanish y termino en espanol toward a ogy of code switching
the bilingualism reader
delip rao david yarowsky abhishek shreevats and manaswi gupta

classifying latent user in proceedings of the tributes in twitter
ternational workshop on search and mining generated contents toronto on canada october pages
acm
sudha rao and joel tetreault

dear sir or madam may i introduce the gyafc dataset pus benchmarks and metrics for formality style transfer
in proceedings of the conference of the north american chapter of the association for computational linguistics human language nologies volume long papers pages new orleans louisiana
association for tional linguistics
sravana reddy and kevin knight

obfuscating in proceedings of gender in social media writing
the first workshop on nlp and computational cial science austin tx usa november pages
association for computational linguistics
ehud reiter and robert dale

building applied natural language generation systems
nat
lang
eng

ehud reiter and robert dale

building natural language generation systems
cambridge university press
ehud reiter roma robertson and liesl m osman

lessons from a failure generating tailored smoking cessation letters
articial intelligence
yossi rubner carlo tomasi and leonidas j guibas

a metric for distributions with applications to image databases
in sixth international conference on computer vision ieee cat
no
pages
ieee
alexander m
rush sumit chopra and jason weston

a neural attention model for abstractive in proceedings of the tence summarization
conference on empirical methods in natural guage processing emnlp lisbon portugal september pages
the ciation for computational linguistics
stuart j
russell daniel dewey and max tegmark

research priorities for robust and benecial articial intelligence
ai mag

abhilasha sancheti kundan krishna balaji vasan srinivasan and anandhavelu natarajan

inforced rewards framework for text style transfer
in european conference on information retrieval pages
springer
ccero nogueira dos santos igor melnyk and inkit padhi

fighting offensive language on cial media with unsupervised text style transfer
in proceedings of the annual meeting of the sociation for computational linguistics acl melbourne australia july volume short papers pages
association for putational linguistics
shuo ren yu wu shujie liu ming zhou and shuai ma

a retrieve and rewrite tion method for unsupervised machine translation
in proceedings of the annual meeting of the sociation for computational linguistics acl online july pages
tion for computational linguistics
jonathan schler moshe koppel shlomo argamon and james w
pennebaker

effects of age in computational and gender on blogging
proaches to analyzing weblogs papers from the aaai spring symposium technical report stanford california usa march pages
aaai
danilo jimenez rezende shakir mohamed and daan stochastic backpropagation and wierstra

approximate inference in deep generative models
in proceedings of the international ence on machine learning icml beijing china june volume of jmlr shop and conference proceedings pages
jmlr
org
stephen roller emily dinan naman goyal da ju mary williamson yinhan liu jing xu myle ott kurt shuster eric m smith al

recipes for building an open domain chatbot
arxiv preprint

alexey romanov anna rumshisky anna rogers and david donahue

adversarial decomposition in proceedings of the of text representation
conference of the north american chapter of the association for computational linguistics human language technologies volume long and short papers pages minneapolis minnesota
association for computational linguistics
abigail see peter j
liu and christopher d
manning

get to the point summarization with generator networks
in proceedings of the nual meeting of the association for computational linguistics acl vancouver canada july august volume long papers pages
association for computational linguistics
rico sennrich barry haddow and alexandra birch

controlling politeness in neural machine in naacl hlt translation via side constraints
the conference of the north american chapter of the association for computational guistics human language technologies san diego california usa june pages
the association for computational linguistics
rico sennrich barry haddow and alexandra birch

edinburgh neural machine translation tems for wmt
in proceedings of the first ference on machine translation volume shared task papers pages berlin germany
sociation for computational linguistics
rico sennrich barry haddow and alexandra birch

improving neural machine translation in proceedings of the els with monolingual data
annual meeting of the association for tational linguistics acl august berlin germany volume long papers
the ciation for computer linguistics
mingyue shang piji li zhenxin fu lidong bing dongyan zhao shuming shi and rui yan

semi supervised text style transfer cross projection in proceedings of the in latent space
ference on empirical methods in natural language processing and the international joint ence on natural language processing ijcnlp pages hong kong china
sociation for computational linguistics
chengcheng shao giovanni luca ciampaglia onur varol kai cheng yang alessandro flammini and filippo menczer

credibility content by social bots
nature nications
the spread of fadi abu sheikha and diana inkpen

generation of formal and informal sentences
in enlg proceedings of the european workshop on ural language generation september nancy france pages
the association for computer linguistics
tianxiao shen tao lei regina barzilay and tommi jaakkola

style transfer from non parallel text by cross alignment
in advances in neural tion processing systems pages
jitesh shetty and jafar adibi

the enron email dataset database schema and brief statistical report
information sciences institute technical report versity of southern california
rakshith shetty bernt schiele and mario fritz

author attribute anonymity by adversarial in training of neural machine translation
usenix security symposium usenix security baltimore md usa august pages
usenix association
kurt shuster samuel humeau antoine bordes and son weston

image chat engaging grounded in proceedings of the annual conversations
meeting of the association for computational guistics acl online july pages
association for computational tics
kaitao song xu tan tao qin jianfeng lu and yan liu

mass masked sequence to quence pre training for language generation
in ceedings of the international conference on machine learning icml june long beach california usa volume of ceedings of machine learning research pages
pmlr
efstathios stamatatos

a survey of modern thorship attribution methods
j
assoc
inf
sci
nol

efstathios stamatatos s michos nikos fakotakis and george kokkinakis

a user assisted business letter generator dealing with text s stylistic tions
in proceedings ninth ieee international ference on tools with articial intelligence pages
ieee
william starr

counterfactuals
the stanford encyclopedia of philosophy
akhilesh sudhakar bhargav upadhyay and arjun heswaran

transforming delete retrieve erate approach for controlled text style transfer
in emnlp ijcnlp
ilya sutskever oriol vinyals and quoc v le

sequence to sequence learning with neural networks
in advances in neural information processing tems pages
bakhtiyar syed gaurav verma balaji vasan vasan anandhavelu natarajan and vasudeva varma

adapting language models for non parallel author stylized rewriting
in aaai
yaniv taigman adam polyak and lior wolf

unsupervised cross domain image generation
in international conference on learning sentations iclr toulon france april conference track proceedings
view
net
sharon swee lin tan and nadee goonawardene

internet health information seeking and the physician relationship a systematic review
journal of medical internet research
deborah tannen

gender differences in cal coherence creating involvement in best friends talk
discourse processes
youzhi tian zhiting hu and zhou yu

tured content preservation for unsupervised text style transfer
corr

minh tran yipeng zhang and mohammad soleymani

towards a friendly online community an unsupervised style transfer framework for profanity redaction
corr

peter trudgill

sex covert prestige and tic change in the urban british english of norwich
language in society
jonathan tse dawn e
schrader dipayan p
ghosh tony c
liao and david lundie

a ric analysis of privacy and ethics in ieee security and privacy
ethics inf
technol

ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan n
gomez lukasz kaiser and illia polosukhin

attention is all you need
in advances in neural information cessing systems annual conference on neural information processing systems december long beach ca usa pages
pascal vincent hugo larochelle isabelle lajoie yoshua bengio and pierre antoine manzagol

stacked denoising autoencoders learning useful representations in a deep network with a local noising criterion
j
mach
learn
res

rob voigt david jurgens vinodkumar prabhakaran dan jurafsky and yulia tsvetkov

rtgender a corpus for studying differential responses to in proceedings of the eleventh international der
conference on language resources and evaluation lrec miyazaki japan may
ropean language resources association elra
kai wang xiaojun quan and rui wang

biset bi directional selective encoding with template for in proceedings of the abstractive summarization
conference of the association for tional linguistics acl florence italy july august volume long papers pages
association for computational tics
ke wang hang hua and xiaojun wan

trollable unsupervised text attribute transfer via ing entangled latent representation
in advances in neural information processing systems annual conference on neural information processing tems neurips december vancouver bc canada pages
yunli wang yu wu lili mou zhoujun li and han chao

harnessing pre trained neural in works with rules for formality style transfer
proceedings of the conference on empirical methods in natural language processing and the international joint conference on natural guage processing emnlp ijcnlp pages hong kong china
association for tional linguistics
zeerak waseem thomas davidson dana warmsley and ingmar weber

understanding abuse a typology of abusive language detection subtasks
in proceedings of the first workshop on abusive language online vancouver bc canada august pages
association for computational linguistics
charles welch veronica perez rosas jonathan k
kummerfeld and rada mihalcea

look who s talking inferring speaker attributes from sonal longitudinal dialog
corr

tsung hsien wen milica gasic nikola mrksic hao su david vandyke and steve young

mantically conditioned lstm based natural language generation for spoken dialogue systems
in ings of the conference on empirical methods in natural language processing pages
jason weston emily dinan and alexander h
miller

retrieve and rene improved sequence in proceedings of eration models for dialogue
the international workshop on search oriented conversational ai brussels belgium october pages
tion for computational linguistics
ronald j williams

simple statistical following algorithms for connectionist ment learning
machine learning
jiawei wu xin wang and william yang wang

extract and edit an alternative to back translation in for unsupervised neural machine translation
proceedings of the conference of the north american chapter of the association for tional linguistics human language technologies naacl hlt minneapolis mn usa june volume long and short papers pages
association for computational tics
xing wu tao zhang liangjun zang jizhong han and songlin hu

mask and inll plying masked language model to sentiment transfer
corr

yu wu yunli wang and shujie liu

a dataset for low resource stylized sequence to sequence in the thirty fourth aaai conference eration
on articial intelligence aaai the second innovative applications of articial gence conference iaai the tenth aaai posium on educational advances in articial ligence eaai new york ny usa february pages
aaai press
xiaoyu xing zhijing jin di jin bingning wang qi zhang and xuanjing huang

tasty ers soggy fries probing aspect robustness in in proceedings of the based sentiment analysis
conference on empirical methods in ral language processing emnlp online november pages
ation for computational linguistics
jingjing xu xu sun qi zeng xiaodong zhang ancheng ren houfeng wang and wenjie li

unpaired sentiment to sentiment translation a in cled reinforcement learning approach
ings of the annual meeting of the association for computational linguistics volume long pers pages melbourne australia
ciation for computational linguistics
peng xu jackie chi kit cheung and yanshuai cao

on variational learning of controllable sentations for text without supervision
in ings of the international conference on chine learning icml july tual event volume of proceedings of machine learning research pages
pmlr
ruochen xu tao ge and furu wei

formality style transfer with hybrid textual annotations
arxiv preprint

wei xu alan ritter bill dolan ralph grishman and colin cherry

paraphrasing for style
in ing international conference on tational linguistics proceedings of the conference technical papers december mumbai dia pages
indian institute of ogy bombay
ivan yamshchikov viacheslav shibaev nikolay khlebnikov and alexey tikhonov

transfer and paraphrase looking for a arxiv preprint ble semantic similarity metric


ivan p
yamshchikov viacheslav shibaev aleksander nagaev jurgen jost and alexey tikhonov

decomposing textual information for style fer
in proceedings of the workshop on neural generation and translation pages hong kong
association for computational linguistics
zichao yang zhiting hu chris dyer eric p
xing and taylor berg kirkpatrick

unsupervised text style transfer using language models as in advances in neural information tors
ing systems annual conference on neural mation processing systems neurips december montreal canada pages
xiaoyuan yi zhenghao liu wenhao li and maosong sun

text style transfer via learning style stance supported latent space
in proceedings of the twenty ninth international joint conference on ticial intelligence ijcai pages
ijcai
org
g udny yule

on sentence length as a tical characteristic of style in prose with tion to two cases of disputed authorship
biometrika
kuo hao zeng mohammad shoeybi and ming yu liu

style example guided text generation using generative adversarial transformers
corr

jingyi zhang masao utiyama eiichiro sumita ham neubig and satoshi nakamura

ing neural machine translation with retrieved in proceedings of the lation pieces
ence of the north american chapter of the ation for computational linguistics human guage technologies naacl hlt new leans louisiana usa june volume long papers pages
association for computational linguistics
saizheng zhang emily dinan jack urbanek arthur szlam douwe kiela and jason weston

sonalizing dialogue agents i have a dog do you have pets too in proceedings of the annual meeting of the association for computational guistics acl melbourne australia july volume long papers pages
association for computational linguistics
tianyi zhang varsha kishore felix wu kilian q
weinberger and yoav artzi

bertscore arxiv evaluating text generation with bert


yi zhang tao ge and xu sun

parallel data in augmentation for formality style transfer
ceedings of the annual meeting of the ciation for computational linguistics pages online
association for computational guistics
yi zhang jingjing xu pengcheng yang and xu sun

learning sentiment memories for sentiment modication without parallel data
in proceedings of the conference on empirical methods in natural language processing brussels belgium october november pages
association for computational linguistics
zhirui zhang shuo ren shujie liu jianyong wang peng chen mu li ming zhou and enhong chen

style transfer as unsupervised machine lation
arxiv preprint

junbo jake zhao yoon kim kelly zhang der m
rush and yann lecun

ally regularized autoencoders
in proceedings of the international conference on machine learning icml stockholmsmassan stockholm sweden july volume of proceedings of chine learning research pages
pmlr
xu zheng tejo chalasani koustav ghosal tian lutz and aljosa smolic

stada style in proceedings of transfer as data augmentation
the international joint conference on puter vision imaging and computer graphics ory and applications visigrapp volume visapp prague czech republic february pages
scitepress
jun yan zhu taesung park phillip isola and alexei a
efros

unpaired image to image translation using cycle consistent adversarial networks
in ieee international conference on computer vision iccv venice italy october pages
ieee computer society
zhemin zhu delphine bernhard and iryna gurevych

a monolingual tree based translation model for sentence simplication
in coling international conference on computational tics proceedings of the conference august beijing china pages
tsinghua university press
daniel m
ziegler nisan stiennon jeffrey wu tom b
brown alec radford dario amodei paul tiano and geoffrey irving

fine tuning guage models from human preferences
corr

victor w zue and james r glass

tional interfaces advances and challenges
ceedings of the ieee

