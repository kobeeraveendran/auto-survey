r a g l
s c v
v i x r a summarizing event sequences with serial episodes a statistical model and an application soumyajit mitra and p s sastry abstract in this paper we address the problem of discovering a small set of frequent serial episodes from sequential data so as to adequately characterize or summarize the data
we discuss an algorithm based on the minimum description length mdl principle and the algorithm is a slight modication of an earlier method called
we present a novel generative model for sequence data containing prominent pairs of serial episodes and using this provide some statistical justication for the algorithm
we believe this is the rst instance of such a statistical justication for an mdl based algorithm for summarizing event sequence data
we then present a novel application of this data mining algorithm in text classication
by considering text documents as temporal sequences of words the data mining algorithm can nd a set of characteristic episodes for all the training data as a whole
the words that are part of these characteristic episodes could then be considered the only relevant words for the dictionary thus resulting in a considerably reduced feature vector dimension
we show through simulation experiments using benchmark data sets that the discovered frequent episodes can be used to achieve more than four fold reduction in dictionary size without losing any classication accuracy
index terms frequent episodes mdl principle compressing frequent patterns hmm models for episodes dictionary learning text classication
introduction f requent pattern mining is an important problem in data mining with applications in diverse domains
frequently occurring local patterns can capture useful pects of the semantics of the data
however in practice the mined frequent patterns are often large in number and quite redundant in nature which makes it difcult to effectively use them
isolating a small set of non redundant informative frequent patterns that best describes the data is an interesting current research problem
in this paper we are concerned with mining of sequential data in the framework of frequent episodes
we address the problem of isolating a small set of redundant serial episodes that best characterize the data
there have been many recent efforts for extracting a small subset of non redundant characteristic patterns
there are mainly two families of methods
one family of methods retain only those patterns which are in some sense tistically signicant
the statistical signicance is assessed using either a suitable null model in a hypothesis testing framework or by tting a generative model for the data source
while this can reduce the number of frequent patterns to some extent this approach can not tackle redundancy in the discovered patterns
another prominent family of methods for deciding which subset of patterns best explains the data is based on an information theoretic approach called minimum scription length mdl principle
in the context of the problem of isolating a best subset of frequent patterns soumyajit mitra was at the department of electrical engineering indian institute of science bangalore india
he is currently with samsung bangalore india
e mail
com p
s
sastry is with the department of electrical engineering indian institute of science bangalore india
e mail
ac
in the use of mdl principle can be explained as follows
we formulate a mechanism so that given any subset of frequent patterns we can use them as a model to encode the data
then the subset that results in the overall best level of data compression is considered to be the subset that best characterizes the data
such a view motivated by mdl principle has been found effective for many frequent pattern mining algorithms
mdl principle views learning as data compression
if we are able to discover all the important regularities in data then we should be able to use these to compress the data well
in this view the coding mechanism used should be lossless that is the original data should be exactly recoverable given the encoded compressed representation
the krimp algorithm is one of the rst methods that used mdl principle to identify a small set of vant patterns in the context of frequent itemset mining
as mentioned earlier in this paper we are concerned with sequential data
for sequential data unlike in the case of transaction data the temporal ordering of data tuples is important and our encoding mechanism should be such that we should be able to recover the original data sequence in correct order along with all time stamps
this presents additional complications while encoding sequential data using frequent patterns
see for more discussion on this
there are many mdl motivated algorithms proposed for characterizing sequence data through a subset of frequent patterns
different algorithms use different strategies for coding data using frequent patterns
while the methods are motivated by mdl principle the coding gies and hence the computation of compression achieved by a given subset of frequent patterns are essentially arbitrary
in this paper we consider a recently proposed algorithm called which is an efcient method to discover a subset of serial episodes that best characterizes data of event sequences
it uses a novel pattern class consisting of injective serial episodes with xed inter event times
a similar pattern class was also used recently for ing association rules from temporal data
the algorithm uses the number of distinct occurrences of an episode as its frequency
here we extend it to the case of non overlapped occurrences as episode frequency and then provide some statistical justication for the algorithm based on a generative model
the main contribution of this paper is a hmm based generative model which provides some statistical tion for the algorithm
in all mdl based approaches a subset of patterns is selected based on the data pression it can achieve
this depends on the arbitrary coding scheme used by the algorithm which is selected heuristically
in this paper we provide a justication for the coding scheme and the algorithm used in based on our proposed statistical generative model
this is the rst time to our knowledge that such a formal connection is established between mining of episodes using the mdl principle and a generative model for data source
since this generative model is markovian and hence can handle only non overlapped occurrences based episode frequency we extended to use non overlapped occurrences as episode frequency
another major contribution of this paper is a novel plication of this method of discovering a set of characteristic episodes from sequential data
the application is in text sication
most text classication methods represent each document as a vector over a dictionary of words which is often called the bag of words representation
the dictionary for this is taken to be all the words in the corpus after propriate stemming and dropping of stop words
often the dictionary sizes are large resulting in high dimensionality of the feature vectors representing individual documents
a text document can be viewed as a sequence of events with event types being words
hence using our method we can discover a subset of characteristic episodes that best represent the full corpus of document data
we can then use the words event types in the subset of discovered episodes to form our dictionary
since our method does not even need a frequency threshold this constitutes a parameter less unsupervised method of feature selection for this problem
we show through empirical experiments that this method results in a very signicant reduction of dictionary size without any loss of classication accuracy
the rest of the paper is organized as follows
section ii describes the episode mining algorithm
section iii presents our proposed generative model
section iv explains our method of nding a smaller sized dictionary in text sication problems and reports results obtained on different text datasets
conclusions are presented in section v
discovering best subset of serial episodes
episodes in event sequences we begin with a brief informal description of the episodes framework
see for more details
here the data is abstractly viewed as an event sequence denoted as d


en tn where in each tuple or event ei ti ei is the event type and ti is the time of occurrence of that event
we have ei e a nite alphabet set and ti i
an example event sequence is d d a c e a b c d b c e a c b c the patterns of interest here are called episodes
in this paper we are concerned with only serial episodes
we represent an n serial episode as where is the event type of the ith event of the episode
an episode is said to be injective if all event types in the episode are distinct
for example a b c is a three node injective serial episode
an currence of the serial episode is constituted by events in the data sequence that have appropriate event types and their times of occurrence are in the correct order
in a b c constitutes an occurrence of a b c while a b c does not because b does not occur after a
note that the events constituting an occurrence need not be contiguous in the data
the data mining problem is to discover all frequently occurring episodes
in the frequent episodes framework many different frequency measures are dened based on counting different subsets of occurrences
we mention two such frequencies below which are relevant for this paper
there are efcient algorithms for discovering serial episodes under many frequency measures
see for more details on different frequencies and algorithms for discovering rial episodes
two occurrences of a serial episode are said to be non overlapped if no event of one occurrence is in tween events of the other
in a b c and a b c are non overlapped occurrences of a b c while a b c is another occurrence of this episode which overlaps with both the earlier ones
the non overlapped frequency of an episode is dened as the maximum number of non overlapped occurrences of the episode in the event sequence
two occurrences are said to be distinct if they do not share any event
all three occurrences above are distinct
the mum number of distinct occurrences is another frequency of interest
in our method here we use a special class of serial episodes called xed interval serial episodes
a xed val serial episode can be denoted as n where i is the prescribed gap between the times of ith and i events of any occurrence of
for example a b c is a xed interval injective serial episode
in a b c is an occurrence of this episode while a b c is not
as is easy to see all events constituting an occurrence of a xed interval serial episode are completely specied by giving only the time of occurrence of the rst event
also two occurrences starting at different times would be distinct if the episode is injective that is all event types in the episode are different

mining algorithm for the best subset here our interest is in discovering a small set of interval serial episodes that best explains the data
we use the minimum description length mdl principle for this
hence we rank different subsets of episodes by the total encoding length that results when we use them as models to encode data
under mdl the encoding should be such that we should be able to recover the original data completely
since we are considering sequential data this means we should be able to recover the data in the original sequence with all time stamps
we rst explain the strategy of coding the data sequence using our episodes
the basic idea is that we can encode all events constituting the occurrence of a xed interval serial episode by just giving the start times of the occurrence
the encoding strategy is same as that used in
for obtaining the best subset of episodes we essentially use the algorithm from with the main difference being we use the non overlapped frequency while that algorithm uses distinct occurrences as frequency
below we rst explain the encoding scheme through an example and then briey explain the algorithm
for more details on the coding scheme and the algorithm please see
table illustrates the coding scheme by encoding the event sequence in using essentially three arbitrarily lected episodes
each row species the size and description of an episode the number of occurrences of the episode and the start times of these occurrences
thus the rst row of table species a three node episode namely a b c which has two occurrences starting at time instants and
thus this row codes for six events in the data constituting the two occurrences of this node episode
similarly the second row codes for six events by specifying two occurrences of a node episode and the third row codes for two events by specifying one occurrence of a node episode
suppose we are interested in asking how good is this subset of three episodes
these three episodes together as specied through table account for all but two events in the data
but coding under mdl should be lossless
hence in the last row of table we have used two occurrences of a node episode to make sure that all events in the data sequence are covered
it is easy to see that given this table we can recreate the entire data sequence exactly
in this table we can think of the rst two columns as coding the model that is the subset of episodes and the last two columns as coding the data using this model
thus the length or size of this table can be the total encoded length for the subset of episodes
given any subset of episodes such as the three episodes in the rst three rows of the table we can nd an encoding like this for the whole data by adding occurrences of a few node episodes as needed which is what is done in the fourth row of the table
in this table one can see that the event c is coded for by both the rst and the second episode in the table
intuitively we get better data compression if such overlaps among the parts of data encoded by different episodes in the selected set are minimized
thus we should get better compression of data if we can choose episodes with high frequency so that they can cover for large number of events which are non redundant so that the overlaps as mentioned above are reduced
this is the intuitive reason for using this coding scheme and looking for a subset of episodes that achieves best compression of data
table encoding of event sequence size of episode no
of occurrences episode name c a c d b e a b c list of occurrences our objective is to nd a subset of episodes to encode data like this so as to get best data compression
for poses of counting length memory we assume that event types as well as times of occurrence are integers and that each integer takes one unit of memory
let be an n node episode of frequency f used for encoding
its row in the table would need f units unit to represent the size of the episode n units to represent the event types of the episode n units for representing the inter event gaps unit for frequency and f units to represent the start times of the occurrences
since non overlapped or distinct occurrences do not share events this episode encodes for fn events in the data and hence we need at least fn units of memory if we want to encode these events in the data using node episodes
dene d fn f if d then we can conclude that is a useful candidate since selecting it can improve encoding length in comparison to the trivial encoding using only node episodes
however the true utility of is to be assessed with respect to what it would add to compression given the other selected episodes
let fs be a set of episodes of size greater than one
given any such fs let d denote the total encoded length of d when we encode all the events which are part of the occurrences of episodes in fs by using episodes in fs and encode the remaining events in d if any by episodes of size one
given any two episodes let om denote the number of events in the data that are covered by occurrences of both and in the data sequence d
dene overlap d fs d om fs p overlap score gives an estimate of how much extra ing efciency can be achieved by selecting given the set fs
it can be proved prop
that if overlap d fs then d d this means that given a current set of episodes fs adding to fs an episode with positive overlap score would only reduce the total encoded length
the algorithm in is essentially a greedy algorithm that keeps adding episodes with highest overlap score
this greedy selection of best episode based on overlap score is done from a set of candidate episodes generated through a search of the lattice of all serial episodes
each candidate episode is the best episode in one of the paths of the search tree
for the sake of completeness we give the pseudocode of this algorithm as algorithm for more details see
algorithm tg k algorithm find no input d event sequence tg maximum inter event gap k maximum number of selected episodes
output the set of selected frequent episodes f
initialize the nal set of selected episodes f as while data compression achievable and k do fs c generate candidate episodes calculate events shared by occurrences for every pair of candidate episodes repeat argmaxcoverlap d fs if overlap d fs then fs fs c end if until overlap d fs or k delete the events from d corresponding to the currences of selected episodes fs f f fs end while a episodes to encode remaining events in d f f a return f we can run this algorithm to nd top k best episodes
if we give a very large value of k the algorithm exits when it can not nd any more episodes of size greater than which improves coding efciency
the algorithm needs no frequency threshold given by users
our score naturally prefer episodes with higher frequency and we need no threshold because we pick episodes based on what they add to coding efciency
thus the algorithm does not really have any hyperparameters except for tg the maximum allowable inter event gap which is not a critical one
while calculating overlap score we need to decide what type of occurrences we would count toward frequency
as mentioned earlier uses distinct occurrences
in this paper we use non overlapped occurrences for frequency of episodes
the reason for this is that the generative model we present in the next section is for non overlapped currences
also in our application to text classication non overlapped occurrences is a more natural choice for frequency
we obtain the sequence of non overlapped occurrences from the distinct occurrences returned by using a simple algorithm
we take the rst occurrence from the sequence of distinct occurrence as the rst one in the quence of non overlapped occurrences
then onwards we take the rst distinct occurrence starting after the last overlapped occurrence we have as the next one in our sequence of non overlapped occurrences
the pseudocode for this algorithm is listed as algorithm
below we prove the correctness of this algorithm
that is we show that the sequence of non overlapped occurrences we get is a maximal one and hence we get the correct non overlapped frequency
let h


hl denote the set of input occstarttime list of start times of distinct occurrences of in increasing order of start times i


n inter event gaps of episode
output no occurrences list of start times of a mal set of non overlapping occurrences of
no itr pointer to rst element of occstarttime if occstarttime list is empty then return no end if ts itr
starttime no no ts itr itr
next while itr n u of list do t s itr
starttime n if s ts i then no no t ts t s p s end while return no end if itr itr
next overlapped occurrences returned by algorithm
each currence hi can be thought of as a tuple of indices in the data sequence which give the position of events in data that constitute this occurrence
for example in data sequence the occurrence of the episode a b c constituted by the events a b c would be represented by the tuple
hence as a notation we use to denote the time of the kth event of the episode in the occurrence hi
on the set of occurrences h there is a natural order occurrence hi is earlier than hj if the thj
because of the way the occurrences in h are selected by our algorithm the following property is easily seen to hold property is the earliest distinct occurrence of the episode
for any i hi is the rst distinct occurrence starting after and there is no distinct occurrence which starts after
proposition h is a maximal set of non overlapped occurrences of proof note that for xed interval injective serial episodes occurrences having different start times are distinct
sider any other set of non overlapped occurrences of the episode h m
let p l
we rst show that


h h h th i


p we use induction on i to prove this
let us show this rst for i
suppose
since the inter event gaps are xed we have
this means we have found a distinct occurrence of the episode which starts before
this contradicts the rst statement of property that is the earliest distinct occurrence
hence th
suppose that implies overlapped occurrences we have we have
suppose
again since h is true for some i
we show
this is a set of
hence th but this contradicts the fact of property that is the earliest distinct occurrence after
hence
now we prove the maximality of the set h
suppose we assume that i
m l
from inequality is an occurrence beyond
but this contradicts h the last statement of property that there is no distinct for every set of occurrence beyond
hence non overlapped occurrences h
this proves the maximality of the set h
we can now sum up our method of nding a subset of serial episodes that best characterizes the data sequence
we use the coding scheme as described here and use a greedy heuristic to nd the subset that achieves the best sion
this is essentially the same as the algorithm of
however we use algorithm to get non overlapped occurrences of episodes from distinct occurrences and then use that frequency in selecting episodes with best score
in the next section we present an interesting tive model that provides some statistical justication for our algorithm based on selecting an episode with best score
a generative model episodes for pairs of in this section we present a class of generative models which is a specialized class of hmms
this model is motivated by a hmm based model for single episodes proposed in
an hmm contains a markov chain over some state space
but the states are unobservable
in each state a symbol is emitted from a nite symbol set according to a symbol ability distribution associated with that state
the stream of symbols is the observable output sequence of the model
in our case the symbol set would the set of event types and thus the observed output sequence would be a sequence of event types
we think of this as an event sequence where the event times are not explicitly specied
for occurrences and hence for frequencies of general serial episodes without any inter event times specied only the time ordering of the event types in the data sequence is important actual event times play no role
hence in this section we consider serial episodes without any xed inter event times
in our generative model the state transition probability matrix of the markov chain is parameterized by a single parameter which is called the noise parameter
for every pair of serial episodes we have one such generative model
for small enough value of the noise parameter the model is such that the output from the model would be an event sequence containing many non overlapped occurrences of the two corresponding episodes
while occurrences of any one episode would be non overlapped in the output event sequence an occurrence of one episode may be arbitrarily interleaved with occurrences of the other episode
thus this is a good class of generative model for a data source where a pair of episodes form the most prominent frequent patterns under the frequency based on non overlapped occurrences
this is the rst instance of such a statistical generative model for multiple episodes
consider the family of such models containing a model for every possible pair of episodes
let denote the model for the pair of episodes and
given an event sequence we can now ask which is the maximum likelihood estimate of a model from this class of models
this would essentially tell us which pair of episodes best explains the data sequence in the sense of maximizing the likelihood
we show that such a pair of episodes are not necessarily the two most frequent episodes
the data likelihood depends both on the frequencies of the episodes as well as on the number of events in the data that the occurrences of the two episodes share
thus we show for example that may have better likelihood than even when has lower frequency than if overlap between and is much less than that between and
the results we present here provide some statistical justication for the coding scheme and the algorithm that we presented in the previous section

the hmm model a hmm is specied as p where p pij is the state transition probability matrix of the markov chain with state space say s is the initial state probabilities and bq q s where denotes the symbol probability distribution in state q
let o ot be an observed symbol or output sequence
the joint probability of the output sequence o and a state sequence q qt given an hmm is p o bqt ot t y to determine the model with maximum likelihood we need to nd p
this data likelihood is often assessed by evaluating the above joint probability of along a most likely state sequence q where q argmax p o q we also follow this simplication often employed by ods using hmm models
thus we assume p p if p o p o
this would be referred to as assumption
let denote the model corresponding to the pair of episodes and
we give full description of this model below
for the sake of simplicity we consider that both are n episodes
the model depends on whether or not the two episodes share any event types and hence we consider two separate cases wherever necessary case i and have no common event types i
e i j


n
case ii and have some common event types
the state space the number of states in the hmm is
the state space can be partitioned into two parts episode states se comprising of states and noise states sn comprising of states
episode states are denoted by sk i j k i j

n
the noise states are given by n k i j i j

n and the state n
emission structure the symbol probability distribution for the episode states is a delta function
the episode state i j emits the symbol with probability whereas i j emits the symbol with probability
for each noise state the symbol probability distribution is uniform over the alphabet set e
we denote m
transition structure under case i where and do not share any event types the state transition probabilities out of episode states are given by fig

under case ii also where and may share some event types the transition probabilities out of episode states are as given by the state transition structure of fig except for the states i mod n where i j are such that mod n mod n
for such i j the transition probabilities are as given in fig

mod n and s i mod n s mod n s i j s i mod n s i j s mod n n i j n i j figure episode state transition structure i mod n mod n mod n mod n mod n i mod n n mod n n i mod n figure transition structure when mod n mod n for all the noise states n k i j k the transition can probability to each of the episode states structure is as shown in fig

the noise state n transit with and with probability
or remain in n s i mod n s mod n it may be noted that all transition probabilities are termined by a single parameter which is called the noise parameter
the values of individual transition probabilities are xed in an intuitively simple manner
from any state transitions into a noise state has probability
the ing probability is equally divided between all reachable episode states
or one can intuitively see the logic of the state transition structure also
recall that in state i j we emit symbol
so after this we can either go to to emit the next event type of or go to to now emit an event type from
this allows for arbitrary overlap of occurrences of and
similarly from i j after emitting we can either go to
since event types constituting occurrence of an episode need not be contiguous from the episode states we can go to the noise states and cycle there zero or more times before coming back to episode states
after emitting the last event types of say the next event type of that can be emitted is
hence from n j we should go to either or or a noise state
that is why in the transition structure as given whenever an index is incremented it is always with respect to modulo n
all the above is ne when and do not share event types
suppose they share an event type
when that event type appears in the data it could be part of an occurrence of only or that of only or neither
these possibilities are all accounted for by the above transition structure
however there is one more possibility namely it is part of an occurrence of both as well as that is the two occurrences share an event
the transition structure given in fig
ensures that our generative model includes this possibility too
initial states if the initial state is n with probability and the initial state is n with probability
with probability with probability
if with probability an example consider a model where a b c and d b e
let the alphabet set e a b c d e f g
we show a few example state quences and output sequences of length that can be emitted by in fig

as can be seen from the gure the output sequence contains occurrences of and that may be arbitrarily interleaved
here we have
hence transitions out of episode states are as given in fig
and for all other episode states they are as given in fig

the special transition structure for allows some occurrences of and to share an event of event type b as can be seen in the transition from of fig

to n i j s i mod n n i j s mod n
analysis n i j n i j figure noise state transition structure in this section we derive expressions for the likelihood of a joint state and output sequence of our hmm model and use this to compare likelihoods of models corresponding to different pairs of episodes
the expressions depend on n n n n b d n f a b d n g n a c n n f a d b e b n g n b n b b b c e c n n f n g a e c c n f n d g e a b b b e c g figure first events of four sample output sequences whether or not the pairs of episodes share event types and hence the two cases are dealt with separately
in all our analysis we assume m where m


case i here i j


n
hence all episode states have only transition into them
decomposing any state sequence into two sub sequences qe and qn corresponding to the episode and noise states we have the following observation in equation whenever the transition probability is the state qt has to be an episode state and hence the bqt ot is either or
similarly whenever is the corresponding is m
thus for any state sequence with non zero probability we can write the joint probability as p o m here and denote lengths of the respective sequences
since t the length of output or event sequence can be written as under our assumption we have m and hence
then p o is monotonically increasing with
thus the most likely state sequence is the one that spends the longest time in episode states
due to constraints imposed on the state transition structure in any state quence of having non zero probability the episode states corresponding to a particular episode have to occur in sequence
moreover when a particular episode state i j or i j is revisited it implies one cycle of all the types corresponding to that episode have been emitted
suppose are the maximum possible number of and f non overlapping occurrences of and respectively in
since and do not share any event type and at each episode state we emit one symbol the most likely state sequence has at least n number of episode states n f n f in it i

for the sake of simplicity we make the assumption referred to as that there is no state sequence with zero probability that includes any incomplete occurrence of either of the episodes
under the assumption we have n f n f
consider two models and
p o p o n n t t m m n f p o p o f if f hence under assumption we have p p
this essentially implies that given an already selected episode the episode here if we want to select the next one from the set of episodes that do not share any event type with the already selected one we should choose the most frequent one from that set


case ii in this case we have some episode states with a transition into them while some have transition into them
it should be noted that because of the transition structure a symbol emitted from an episode state with transition into it is part of an occurrence of both the episodes
it means that the event is shared across rences of the two episodes
on the other hand a symbol emitted from an episode state with transition into it is part of an occurrence of only one episode and hence is not shared
now we further decompose the episode states part of any state sequence qe into two parts
the episode states corresponding to event types that are not shared form while those corresponding to shared ones form
since every state emits one symbol we have t
now the joint probability of an output and state sequence is given by m t m let us consider a state sequence q having non zero probability that contains f and f number of occurrences of the episodes and respectively
let the number of events shared between these occurrences be o
then the
we can ensure that the assumption always holds by modifying our model by adding an extra symbol end of sequence marker at the end of the output sequence o and modifying the symbol probability and n distribution of the noise states n by following the trick used with hmms for single episodes as in
p o t m p o no of events covered by the occurrences of the episodes in the output sequence is n f n f o out of which n f n f number of events are not shared and o number of events are shared
under assumption we have o and n f n f
so for this state sequence p o m n f t t m o n f o for m
so we see that the joint ability is an increasing function of the no of occurrences of the episodes and for xed f and f a decreasing function of the number of events shared between the occurrences
let f and f be the maximum possible number of overlapped occurrences of and respectively in
so the most likely state sequence q is the one which emits all the number of occurrences from the episode states and among all such state sequences it is the one which shares minimum number of events between these occurrences
let be the number of shared events corresponding to q
o then from p o t m n o episode then has higher data likelihood compared to under our assumption on and under and
case a o o from under assumption is easily seen it that p p
also easy to it o f check that ply overlap overlap and overlap overlap and o is case b o o in this scenario depending on the values of overlaps the two metrics for may be greater or smaller than those of
hence we consider these two sub cases
case overlap overlap and overlap overlap
since overlap overlap n f n f n f we have from n f o o o o p o p o n f o o f p p n f o under assumption case overlap overlap and overlap overlap since overlap overlap n f o o we have o
let n f where
since we assume m o o be
then we can write o n f n f n f n f
now from we will have a similar expression for the model and hence p o p o p o p o n f o o thus under assumption we see that if f f lihood is higher for the pair of episodes that share lesser number of events
in general the relative likelihood of and depends both on the frequencies of and as well as on the difference in their overlaps with
to better understand this let us dene two metrics to rate any other episode with respect to episode
overlap n f overlap n f o o we will show that given an episode if the values of both metric for an episode are higher than those for an p p under the results presented here provide statistical justication for our algorithm presented in the previous section where we select episodes based on their overlap score as given by
suppose we have selected only and want to choose either or as our second episode
based on this choice depends on the sign of n which is a gure of merit motivated by considerations of coding efciency
this is essentially same as the difference of overlap between and which is a gure of o o merit that determines which pair of episodes maximize the data likelihood
application to text classification in this section we present a novel application of our method of nding a good subset of frequent episodes to acterize data
the application is in the domain of text classication
most text classication techniques use a of words approach where each document or data sample is represented as a collection of words that belong to a dictionary
the dictionary is usually considered as the set of all unique words present in the training corpus after preliminary preprocessing
this makes the size of the tionary large leading to high dimensionality of the feature vector representation of each document
other vector space representation of documents e

word averaging in also depends largely on the dictionary of words used
one can think of a text document as a sequence of events with event types being the words
then using all training data in an unsupervised fashion we can use our method to nd the best subset of serial episodes that represent the data well
these episodes are likely to contain all specic words that are important for this document collection
thus a dictionary built using only the words event types found in the subset of discovered episodes is likely to be useful
this is what we explore here
let the dictionary obtained by using all unique words after usual preprocessing from the training data collection be termed dictionary i
we run our algorithm for ering the best subset of serial episodes that achieve best data compression on the entire training corpus
we form a new dictionary as the set of all the unique words types that are present in the non singleton episodes i
e
episodes of size or more discovered by our algorithm
we call this smaller sized dictionary as dictionary ii
in each case we would represent documents as vectors over one of these dictionaries and investigate standard classiers such as naive bayes and svm
using simulations on some standard benchmark datasets we show that we get large dimensionality reduction without any loss of accuracy by the classier
typically in training data for text classication we have many documents but each document is short
mining for episodes that can achieve signicant compression ually for each document does not give any interesting episodes mainly because each sequence is short
we string together all training data of all classes to make one long document and we mine for a set of frequent serial episodes that achieve best compression using the algorithm cussed in this paper
we employ special symbols to denote end of each training document and modify our mining algorithm so that occurrence of no episode would span two different documents
for single label text categorization
we used the cessed stemmed version of these datasets
for we use the class stemmed version of the dataset is a class dataset while is a class dataset
for these the dictionary i is the set of all unique words present in the stemmed training data
apart from these we also used the movie review dataset prepared by pang and lee
we used the polarity dataset

this sentiment analysis dataset consist of movie reviews
as preprocessing steps we converted all letters to lower case and removed all words less than characters long
no stop words except and the were removed
dictionary i was created from this preprocessed training data


feature vectors we compared the text classication accuracies using two different models bag of each data sample is converted into a feature vector of the dimension of the size of the corresponding dictionary used
each feature represents the frequency of that word in that data sample except for the movie review dataset where as in we used binary features denoting presence or absence of the word in the corresponding ment
further tf idf along with cosine normalization were done on these feature vectors as explained in the next subsection
average embedding vecavg is used to produce the word embeddings and each text is then represented as the average of all the embeddings of the words present in that text
in case of dictionary ii averaging of word embeddings were done only for words which were part of the tionary and the rest were ignored
in case of movie reviews and newsgroup the pretrained model of googlenews vectors were used whereas in case of the other two datasets since these were stemmed the model was trained with gensim library with parameters vector and


tf idf term frequency inverse document frequency tf idf is a merical statistic which is good at quantifying the importance of a word to a document in a collection
let wf w d the frequency that is the number of occurrences of a word w in a document d
instead of using this raw frequency as the feature value we use a modied word frequency dened by m odif ied wf w d wf w d idf where the inverse document frequency idf w is given as idf log nd df w
experimental results

datasets here nd is the total number of documents and df w is the number of documents that contain the word w
we use this we compared the classication accuracies on three dard benchmarks newsgroup and webkb downloaded from a publicly available repository of datasets

cachopo
org datasets for single label text categorization

cs
cornell
edu people pabo movie review

com mmihaltz googlenews vectors modied frequency of each word m odif ied wf w as the feature value
the feature vectors were further cosine normalized
dataset webkb newsgroup dictionary i





dictionary ii







results we compare the classication accuracies obtained ing our proposed dictionary ii with those obtained with dictionary i
for bow and vecavg representation we present results using linear svm
for bow naive results are also presented for comparison with accuracies reported in literature
for the movie review dataset we present the mean value corresponding to the ten fold cross validation on the original folds introduced in
dataset webkb newsgroup movie review number of discovered episodes size of dict i size of dict ii table dictionary sizes for different datasets table shows sizes of the two dictionaries for different datasets
the number of episodes reported in table is the number of non singleton episodes
as can be seen from the table the size of dictionary ii is almost a fourth of that of dictionary i in case of webkb for the other datasets it is about one eighth to one tenth
thus this method results in a very signicant reduction in dictionary size and hence in feature vector dimension
the classication accuracies obtained with different tionaries are shown in tables
table shows accuracies and f measure with linear svm classier under vec avg representation while table shows these for naive bayes and linear svm classiers under bow representation
we did not try any nonlinear svm because all other studies on these benchmark data sets reported only accuracies with linear svm
as is easy to see the accuracies and f measure scores dataset accuracy f measure webkb newsgroup movie review dict i dict ii dict i dict ii















table linear svm accuracy and f for vecavg sentation dataset classier accuracy f measure scores webkb newsgroup movie review nb svm nb svm nb svm nb svm dict i dict ii dict i dict ii































table naive bayes linear svm classication accuracy and f bow representation table mean and standard deviation of classication accuracy with naive bayes using different dictionaries bow representation dataset webkb newsgroup dictionary i





dictionary ii





table mean and standard deviation of classication accuracy with linear svm using different dictionaries bow representation scores under both bow as well as vecavg representation achieved by either classier with different dictionaries are mostly very close
thus we can conclude that our frequent episodes based method allows us to get a very large tion in dictionary size without any signicant change in the classication accuracy
we also note that the accuracy of our dictionary i in table is consistent with the bag of words accuracy reported in and
the above are with the train test split as given in the original datasets
for bow representation we also generated random splits for the datasets webkb newsgroup having the same train test distribution of each class as in the original split
the results showing averages and standard deviations are presented in tables
once again the results clearly show that there are no signicant differences between accuracies achieved with the two tionaries
for the bow representation for this document cation application our method of learning a dictionary results in a signicant decrease in feature vector sion
but this method is quite different from generic mensionality reduction techniques such as pca
with pca we may get dimensionality reduction by choosing certain linear combinations of earlier features
with the original feature vector dimension being in tens of thousands the new features obtained as such linear combinations would not be semantically interpretable
however our data mining method essentially decides on which words of i to be retained and which are to be rejected
thus this method is essentially a feature selection method rather than a dimensionality reduction method
hence the ality reduction achieved here is semantically interpretable
to get such a feel for what the data mining does we present in some sample of words that are retained and rejected by our method in case of movie review and webkb datasets
the words shown are hand picked but only from a set of randomly selected words
it is easy to see that this makes good semantic sense
for example in movie review we reject many movie related words like stunts theater performances
which while they may appear in the reviews may not carry any information regarding sentiment of the review
on the other hand we retain words like hilarious boring surprised
that can carry sentiment information
similar comments apply to webkb dataset e

selected words like prerequisite introductory project can be commonly found on a project or course web page and hence they may carry dataset movie review sentiment negative sentiment stunts theatre cinematographer moviestar directorship producers storyteller scripts spotlight audition auditorium backstage torrent reviewer performances entertainment
enjoyable funny hilarious entertaining superb boring sleepy disappointed twists clever impressed surprised liked interested awful pleasing miserably dumber interesting impressive intelligent fantastic
webkb chemistry cryptography probabilistic lagrangian arithmetic scholarship bibtex manuscript newsletter computer interdisciplinary mathematician biotechnology accuracy baseline neurocomputing gaussian
syllabus internet introductory prerequisite research bibliography professor student quiz exercise credit query tutor project phd fellowship conference curriculum scientist magazine instructor theorem homework examination semester journal homepage
rejected words selected words table sample words from the set of rejected and selected words for dictionary ii tive information
thus the data mining method based on nding episodes for compressing data seems to be effective in picking a dictionary that is relevant to the text corpus
conclusions in this paper we considered the problem of discovering a small set of serial episodes to characterize sequential data
we extended the existing algorithm of to work with non overlapped frequency
our main contribution is a novel hmm based ative model for pairs of episodes
the model generates very general output sequences where the two episodes are the most prominent frequent episodes under overlapped frequency
the model is very intuitive
the symbols emitted from episode states constitute the based occurrences of episodes
the noise states can emit any symbol and hence symbols emitted from the noise states can be thought of as the distracting signal that may mask real episodes and contribute spurious frequent episodes
the transition structure is also intuitively motivated
from any state transitions into a noise state has probability
the remaining probability is equally divided between all reachable episode states
for this model class we showed that the episode pair model that has best likelihood for the data sequence is determined both by the frequencies of the episodes as well as overlaps between their occurrences
the analytical expressions we derived for the data likelihoods provide statistical justication for our algorithm of selecting a subset of episodes
the algorithm is motivated based on the mdl principle
using an intuitively appealing coding scheme to encode data using episodes the algorithm nds a subset of episodes to maximize data compression achieved
it is essentially incrementally picking episodes based on the so called overlap score which depends both on the frequency of the episode as well as on the extent of overlap in its occurrences with those of already selected episodes
our hmm based model provides some statistical justication for this strategy used by the algorithm
a generative model for sequential data to capture teractions of two episodes as well as using it to justify an mdl based algorithm for frequent episodes are both novel contributions of this paper
as mentioned in section there have been many algorithms motivated by the mdl philosophy for succinctly characterizing data using a small set of frequent patterns
however all such algorithms for sequential data are heuristic in nature
we believe that the hmm model we presented here is a good rst step in developing a statistical theory for mdl based algorithms that nd a good subset of frequent episodes
another important contribution of this paper is a novel application of frequent episodes mining to text tion
we view the text document as a sequence of events with event types being the words
then we nd the subset of episodes that best characterizes the entire text corpus in terms of data compression
the words appearing in this subset of frequent episodes is likely to gives us the most formative words for the corpus and hence we use only these words to form the dictionary using which the documents are represented as vectors
thus the method amounts to learning a context sensitive dictionary using the idea of quent pattern mining
also since our data mining method does not need any user specied hyperparameters same is true for this method of dimensionality reduction
to the best of our knowledge this is a rst instance of application of frequent pattern methods to dictionary learning
as we showed through extensive simulations the method results in many fold decrease in the size of dictionary without compromising the classication accuracy
also as can be seen from the examples of retained and rejected words the method seems to be quite effective in learning a good subset of words
the hmm model we presented is for pairs of episodes
while it is in principle extendable to any number of episodes notationally it would be very complex
a good extension of the work presented here is in the direction of extending these analytical techniques to arbitrary number of episodes
generative models can in general be used for sessing statistical signicance of the frequency of an episode e


since the model introduced here also accounts for interactions among episodes it should be usable for questions such as whether or not the observed frequencies of two episodes would make both of them signicant given the extent of overlap between their occurrences
this is also a useful direction in which the work presented here can be extended
references c
c
aggarwal and j
han frequent pattern mining
springer
j
vreeken m
van leeuwen and a
siebes krimp mining itemsets that compress data mining and knowledge discovery vol
no
pp

n
tatti and j
vreeken the long and the short of it summarising event sequences with serial episodes in proceedings of the acm sigkdd international conference on knowledge discovery and data mining
acm pp

m
mampaey n
tatti and j
vreeken tell me what i need to know succinctly summarizing data with itemsets in proceedings of the acm sigkdd international conference on knowledge discovery and data mining
acm pp

h
t
lam f
m orchen d
fradkin and t
calders mining pressing sequential patterns statistical analysis and data mining vol
no
pp

a
ibrahim s
sastry and p
s
sastry discovering compressing serial episodes from event sequences knowledge and information systems vol
no
pp

a
bhattacharyya and j
vreeken efciently summarizing event sequences with rich interleaving patterns in proceedings of the siam international conference on data mining
siam
q
fan y
li d
zhang and k

tan discovering thy themes from sequenced data a step towards computational journalism ieee transactions on knowledge and data engineering vol
no
pp

h
mannila h
toivonen and a
inkeri verkamo discovery of frequent episodes in event sequences data mining and knowledge discovery vol
no
pp

r
gwadera m
j
atallah and w
szpankowski reliable tion of episodes in event sequences knowledge and information systems vol
no
pp

n
tatti signicance of episodes based on minimal windows in data mining

ninth ieee international conference on
ieee pp

r
gwadera and f
crestani ranking sequential patterns with respect to signicance advances in knowledge discovery and data mining pp

c
low kam c
rassi m
kaytoue and j
pei mining cally signicant sequential patterns in data mining icdm ieee international conference on
ieee pp

s
laxman p
s
sastry and k
p
unnikrishnan discovering frequent episodes and learning hidden markov models a formal connection ieee transactions on knowledge and data engineering vol
no
pp

p
d
grunwald the minimum description length principle vol

cambridge ma mit press
j
v
matthijs van leeuwen mining and using sets of patterns through compression in frequent pattern mining c
c
aggarwal and j
han eds
springer ch

x
ao p
luo j
wang f
zhuang and q
he mining positioning episode rules from event sequences ieee transactions on knowledge and data engineering vol
no
pp

a
achar s
laxman and p
s
sastry a unied view of the apriori based algorithms for frequent episode discovery edge and information systems vol
no
pp

s
laxman v
tankasali and r
w
white stream prediction using a generative model based on frequent episodes in event sequences in proceedings of the acm sigkdd international conference on knowledge discovery and data mining
acm pp

r
socher a
perelygin j
wu j
chuang c
d
manning a
ng and c
potts recursive deep models for semantic ity over a sentiment treebank in proceedings of the conference on empirical methods in natural language processing pp

b
pang and l
lee a sentimental education sentiment analysis using subjectivity in proceedings of acl pp

a
cardoso cachopo improving methods for single label text categorization pdd thesis instituto superior tecnico dade tecnica de lisboa
s
wang and c
d
manning baselines and bigrams simple good sentiment and topic classication in proceedings of the annual meeting of the association for computational linguistics short volume
association for computational linguistics pp


