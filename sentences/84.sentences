improving multi document summarization via text classication ziqiang wenjie sujian furu of computing the hong kong polytechnic university hong kong kong polytechnic university shenzhen research institute china laboratory of computational linguistics peking university moe china research beijing china cszqcao
polyu
edu
hk
edu
cn
com v o n l c
s c v
v i x r a abstract developed so far multi document summarization has reached its bottleneck due to the lack of sufcient ing data and diverse categories of documents
text sication just makes up for these deciencies
in this paper we propose a novel summarization system called tcsum which leverages plentiful text classication data to improve the performance of multi document summarization
tcsum projects documents onto tributed representations which act as a bridge between text classication and summarization
it also utilizes the classication results to produce summaries of different styles
extensive experiments on duc generic document summarization datasets show that tcsum can achieve the state of the art performance without ing any hand crafted features and has the capability to catch the variations of summary styles with respect to different text categories
introduction the increasing online information has necessitated the velopment of effective automatic multi document rization systems
through long term research the based summarization approaches have grown to become dominant in the literature
by far a prominent issue that hinders the further improvement of supervised approaches is the lack of sufcient human summaries used for ing cao et al

for instance the widely used generic multi document summarization benchmark datasets contain less than human reference summaries in total
writing summaries is an extremely labor intensive and consuming process
because of the limitation of training data a learning based summarization system is often forced to heavily rely on well designed features
simple models like support vector regression can achieve the state of art performance with extensive linguistic and statistical tures hong and nenkova
to break through the tleneck of insufcient summarization training data taking advantage of other rich data sources might be a good idea worth considering
copyright association for the advancement of articial intelligence www
aaai
org
all rights reserved

nist
meanwhile existing summarization approaches basically apply a uniform model to generate summaries for the uments in different text categories
however according to what we observe summary styles in different categories can vary to a large degree
take the two common gories in duc datasets i
e
natural disaster and phy as an example
to summarize a natural disaster like a hurricane people tend to present its moving path and the loss it brings
by contrast a biography summary is pected to include the personal prole and the main butions of the person
apparently summaries should focus on different aspects of the topics which belong to the sponding categories
when the document category is given kedzie mckeown and diaz nds that the tion of category specic language models largely promotes the summarization performance
the experiments of wan et al
also show that a summarization model with good overall performance still produces low quality summaries in certain document sets
the summary style issue previously mentioned may partly explain these phenomena and suggest a possible way to improve the summarization performance
compared with summarization the text classication datasets are much richer
note that both summarization and text classication require models to understand the tics of documents
better text representations learned by classication data can help to train more effective rization models
moreover if we know the category of a document we will have a chance to explore more proper summary styles
to this end we propose a novel rization system called tcsum which leverages text cation data to improve the performance of summarization
since distributed representations of documents have strated advantages in both summarization e

kobayashi noguchi and yatsuka and text classication e

lai et al
tcsum projects all documents onto the distributed representations that are shared by the two tasks
then for text classication the document embeddings are followed by a classier to learn their association with the categories
for summarization the document embeddings are transformed to match the meaning of the reference summaries
to make the transformed embeddings also hold the information of summary styles we utilize the cation result and develop a category specic transformation process
our model adopts the recent hot topic of neural work based transfer learning e

from syntactic parsing to discourse parsing li li and hovy
it is also noted that our model is totally data driven i
e
all the abstract tures are learned automatically
we verify the effectiveness of tcsum on duc generic summarization benchmark datasets
tcsum is able to pete with state of the art summarization systems which ally heavily depends on hand crafted features
we also serve that tcsum indeed catches the variations of the mary styles among different text categories
the contributions of this paper are listed as follows we leverage text classication datasets to learn better ument representations for summarization
we explore the variations of summary styles with respect to different text categories
we develop a competitive summarization system which does not need any hand crafted features
method let d denote a document which is composed of a set of sentences n
for text classication we use c to stand for the entire set of categories
we assume d belongs to one of c i
e
cd where cd sents the actual category for of the document d
the text classication model is trained to predict a category for d
for supervised sentence ranking required by learning based summarization each sentence holds a saliency score ally measured with respect to the human summaries after the reference summaries
the summarization model is expected to learn how to rank sentences in accord with the actual sentence saliency
in this section we describe how our summarization tem called tcsum ranks the sentences with the help of text classication
the overall framework of tcsum is trated in fig

at rst a text classication model is trained using a convolutional neural network
this model projects a document onto the distributed representation and adds a softmax classier to predict the category of the document
the summarization model shares the same projection cess to generate document embeddings given that the mantic analysis and understanding of documents are tial for both classication and summarization
afterwards it transforms the document embedding to the summary bedding and tries to maximize the match to the meaning of the reference summaries
to make the transformed mary embedding sensitive to the different summary styles tcsum learns category specic transformation matrices cording to the predicted categories
finally the sentences are ranked according to their saliency scores calculated based on the similarity between the sentence embedding and the summary embedding
the rest of this section describes the details of our model
text classication model convolutional neural networks cnns can learn the stract representations of n grams effectively and tackle the sentences with variable lengths naturally
models ing cnns have achieved excellent performance both in text figure overview of tcsum
classication lai et al
and summarization yin and pei
in this paper we develop a simple cnn based classication model
specically we use a cnn to project a sentence s onto its distributed representation rm i
e
s cnn a basic cnn contains a convolution operation on the top of word embeddings which is followed by a pooling eration
let rk refer to the k dimensional word embedding corresponding to the ith word in the sentence
let be the concatenation of word embeddings
a convolution operation involves a lter w rmhk which is applied to a window of h words to produce the abstract features gh i rm gh i w where is a non linear function and the use of tanh is the common practice
to make it simple the bias term is left out
this lter is applied to each possible window of words in the sentence to produce a feature map
subsequently a pooling operation is applied over the feature map to obtain the nal features gh rm of the lter
here we use the max over time pooling collobert et al

gh gh the primary purpose of this pooling is to capture the most important features in a feature map
gh is the output of the cnn i
e
the embedding of a sentence
then a document is represented by the average pooling of its sentence embeddings just like lai et al
sd to learn the association between the document embedding and the categories the document embedding is followed by a softmax classier where w is the weight matrix and is the predicted probability distribution over the gories
summarization model as previously mentioned the summarization model in sum shares the same convolution and pooling operations with the classication model when generating the document embedding
then tcsum transforms to match the meaning of the reference summary i
e
where rm is the transformed embedding called summary embedding and w rmm is the tion matrix
note that we dene the same dimension for both document and summary embeddings
this setting simplies the sentence ranking process which is explained later
we would also like the summary embedding to hold the information of summary styles
inspired by the work of dong et al
we develop the category specic formation matrix w according to the predicted category
we introduce sub matrices with each directly corresponding to one text category
based on the predicted category derived from eq
the transformation matrix w is computed as the weighted sum of these matrices
in this way w is automatically biased to the sub matrix of the predicted text category
vi the summary embedding is expected to match the meaning of the reference summaries
it should have the ability to properly judge the sentence saliency which is sistent with the reference summaries
following kobayashi noguchi and yatsuka we use the cosine similarity between the summary embedding and a sentence embedding to predict the sentence saliency rs
rs vt s d that is why both document and summary embeddings are of the same dimensionality
training we use the pre trained word embeddings and do not date them to avoid
thus there are three types of weight matrices in our models i
e
w w and the formation sub matrices
since the text classication dataset is much larger than the summarization dataset w and w are learned from the classication data only
yet the transformation matrices have to be trained with the summarization data
for text classication we adopt the cross entropy as the cost function i
e
i vi where i equals iff the actual category is i
der this cost function the gradient of softmax is similar to a linear function which fastens the training process
for summarization we apply the pairwise ranking egy collobert et al
to tune the weights
specically each time we randomly select a sentence with a high actual saliency score and the other one with a low actual saliency score
they are denoted as and s respectively
by eq
we obtain their predicted saliency scores
with the pairwise ranking criterion tcsum should give a higher score in comparison with s
therefore the cost function is dened as follows rs where is a margin threshold
with the above two cost functions we apply the diagonal variant of adagrad with mini batches duchi hazan and singer to update model parameters
adagrad adapts the learning rate for different parameters at different steps
thus it is less sensitive to initial parameters than the tic gradient descent
experiments datasets summarization the most commonly used evaluation pora for summarization are the ones published by the ment understanding conferences duc and text analytics conferences
in this work we focus on the generic multi document summarization task which was carried out in duc and
the documents are all from the news domain and a collection of documents related to the same topic are grouped together into a cluster
each ter is accompanied by to reference summaries written by human experts
our summarization model compiles the uments in a cluster into a single document
table shows the size of the three datasets and the summary length itation for each task
the duc datasets come from a wide range of categories and we manually categorize the duc documents into categories i
e
biography culture ness health politics law society natural disaster ence sports and international
the category distribution of duc is illustrated in fig

among these categories natural disaster politics and biography account for of the documents
dataset duc duc duc cluster doc
ref
limitation words words bytes table statistics of the summarization datasets
text classication in order to benet from text tion we need to have a classication dataset large enough to cover all the categories discovered in the duc datasets
we build such a dataset from the new york times nyt annotated corpus
the nyt corpus contains over
million articles published and annotated by the new york times
notably the new york times is also an important data provider for duc
the nyt documents have rich data
we utilize three types of metadata types of material
nist
gov from now
ldc
upenn
edu ing it employs a simple greedy algorithm similar to our vious work cao et al
to select summary sentences
baseline methods we compare tcsum with the best peer systems participating duc evaluations which are named as peer plus their ids
in addition we include cao et al
a of the art supervised summarization model based on neural networks
it applies the recursive neural network to learn the combination of hand crafted features
notably still heavily depends on hand crafted features
by contrast sum is fully data driven i
e
features are all learned matically
we implement a widely used learning based tion method support vector regression svr li et al

it extracts a number of manually compiled features from a sentence such as tf the frequency of a word in the cluster number of documents containing this word in the cluster and number whether the sentence contains a number
we also design three neural network based baselines named as notc singlet and emsim
the rst two are used to verify the value of text classication notc does not use any classication data and just applies the marization model of tcsum
it is designed to check whether the summarization model can work alone
singlet ignores the predicted text category and uses a single transformation matrix
it explores the effect of summary styles
the last one emsim aims to test whether or not we need to learn the summary embedding
it just uses the cosine similarity between a sentence embedding and the document bedding to rank sentences
emsim is an unsupervised summarization model and similar to kobayashi noguchi and yatsuka
all these baselines employ the same tence selection process as our model
summarization performance we conduct three fold validation
the model is trained on two years data and tested on the remaining year
the rouge scores of the models being compared are presented in table
we draw lines in this table to distinguish the els with and without hand crafted features
as can be seen among the models completely dependent on automatically learned features tcsum achieves est performance on all the three datasets
the poor mance of emsim denotes that we could not directly use the document embeddings learned from text classication to measure the sentence saliency for summarization
note that even notc achieves competitive performance with svr
thus summarization models without hand crafted features are doable
meanwhile singlet greatly outperforms notc
it veries that text classication can indeed help a rization model to learn better document representations
though tcsum does not always greatly surpass singlet in terms of rouges we will show in the next section that it usually captures different summary styles
figure category distribution on duc
taxonomic classiers and online descriptors to pick out the documents within those categories
we notice that the numbers of documents in different categories are extremely imbalanced
for example the category of business contains more than documents while there are only documents in the category of natural disaster
therefore we conduct a sampling process to ensure that each category contains documents
this classication dataset is about times larger than the summarization dataset
the cross validation shows that the learned tion model of tcsum achieves over accuracy on this dataset
since classication is not the focus of this paper here we ignore the detailed performance evaluation of our classication model
evaluation metric for summarization for evaluation we use lin which has been regarded as a standard automatic evaluation metric since
rouge measures summary quality by ing overlapping units such as n grams word sequences and word pairs between the candidate summary and the ence summary
following the common practice we take and recall scores as the main metrics for comparison
and measure the gram and bi gram similarities respectively
during training the actual saliency of a sentence eq
is also evaluated by
model settings for cnn we introduce a word embedding set trained on a large english news corpus tokens using mikolov et al

the dimension of word embeddings is set to as in many previous papers e

collobert et al

we also set the dimension of tence and document embeddings equivalent the dimension of word embeddings and the window size to to be sistent with evaluation
we empirically set the margin threshold of pairwise ranking

the initial learning rate is
and batch size is
a summary is obliged to offer both informative and redundant content
while tcsum focuses on sentence

with options a

the parameter of length constraint is for duc and for duc
can use integer linear programming to select better sentences here we just consider the result of greedy selection for a fair comparison
year model
peer t
svr
notc
emsim
singlet
tcsum

peer
svr
notc
emsim
singlet
tcsum

peer
svr
notc
emsim
singlet
tcsum





















table rouge scores of different methods
compared with other models tcsum largely forms svr and peer systems most of the time and it is always superior to the state of the art method
sidering tcsum is not supplemented with any hand crafted features its performance is very promising
after taking a closer look at the feature weights learned by svr we nd the most important feature to measure sentence saliency is cf
since we treat the documents in a topic cluster as a gle document this feature is lost in our current tion model
it may be an important aspect that impedes the more excellent performance of tcsum
discussion on summary style learning we examine the ability of tcsum to learn summary styles in two ways
at rst we speculate that similar tion matrices tend to generate summaries with similar styles
therefore we calculate the similarity among the tion matrices
here we atten each matrix into a vector and use the cosine similarity to measure the similarity
the scores of different transformation matrices are presented in fig

for ease of reference we only show the results of three common categories on ducs i
e
raphy politics and natural disaster
as can be seen the ilarity relations of these three categories vary greatly which matches the intuition that the large difference of the mary styles exists among these categories
for biography we nd its transformation matrix is similar to categories
they are business culture politics and international lation
one possible reason is that summaries in biography necessarily tell the career related information of a person
since duc prefers choosing biographies about artists nessmen and politicians it is reasonable the summary style for biography to be associated with these categories
by contrast natural disaster does not present obvious ity to any other category
we observe that summaries in ural disaster often contain a series of times sites and bers while other categories seldom need so many details
for politics we nd it is similar to international ship and law
the former is understandable since we may use a number of terms of politics when describing tional relationships
the latter may be caused by the news content
many documents in this category are concerned with political scandals which often lead to lawsuits
ingly there is an obvious negative similarity between tics and culture
the wordings in politics are often thought to be serious while the documents in culture are usually lated to entertainment
we also inspect the style change of the summaries ated according to different categories
to this end we ually assign a category to a document cluster and then culate the sentence saliency based on our summarization model
the salient sentences with respect to different egories are shown in table
due to the limit of space we only display the top ranked summary sentences with the styles of three common text categories
is about a hurricane natural disaster
introduces the founder of wall mart biography
describes the resignation of a prime minister itics
as can be seen the salient sentences calculated by the rect categories can properly represent the main idea of the document cluster
although and are not lated to politics sentences selected by the corresponding transformation matrix still contain many terms of politics
it is also shown that the three biography sentences contain ther the words describing the careers killer mayor founder or the evaluative words better boldly
the career is a part of personal prole and the description of main contributions of a person usually involves the evaluative words
therefore the corresponding transformation matrix seems to well catch the two types of needs for biography summaries
we read the documents in and carefully and nd there is no sentence exactly matching natural disaster
thus it is not surprising that the sentences selected by natural aster in these two clusters are somewhat strange
however we can see both sentences contain the date and site mation
this is absolutely consistent with the style that a summary of natural disaster is expected to have
moreover both the money value and the word bombing can be used to describe the loss of a disaster
it appears that the mation matrix for natural disaster still works well even on a topic other than natural disaster with due diligence to complete its own task
related work work on extractive summarization spans a large range of proaches
starting from unsupervised methods one of the widely known approaches is maximum marginal relevance mmr carbonell and goldstein
it used a greedy approach to select sentences and considered the trade off between saliency and redundancy
good results could be achieved by reformulating it as an integer linear cluster category natural disaster biography politics biography natural disaster politics politics biography sentence the storm packing winds of up to mph raged into charleston thursday night
this is a dangerous killer hurricane the likes of which few people who have lived all their lives in charleston have experienced warned mayor joseph p
riley jr
gov
joe frank harris declared a state of emergency in six counties
sam walton founder of the wal mart chain of discount supermarkets who died of cancer in april negotiated these pitfalls much better than most
by the chain s sales had risen to nearly dollars making it the world s largest retailer in terms of revenues and the walton family probably america s richest
bud is a senior vice president and board member of wal mart
flamboyant former defense minister hazeltine s challenge to prime minister garet thatcher for leadership of the conservative party has caused a political tion in britain
in the persian gulf crisis she boldly joined with george bush in sending troops to the middle east
natural disaster among western allies she was alone at ronald reagan s side in in supporting the u
s
bombing of libya
table salient sentences selected by different categories
sentences in the correct categories are displayed rst
trained a language model based on convolutional ral networks to project sentences onto distributed tations
cheng and lapata treated single document summarization as a sequence labeling task and modeled it by recurrent neural networks
others like kobayashi noguchi and yatsuka simply used the sum of trained word beddings to represent sentences or documents
in addition to extractive summarization deep learning technologies have also been applied to compressive and abstractive rization filippova et al
rush chopra and weston
conclusion and future work in this paper we propose a novel summarization system called tcsum which leverages text classication to prove the performance of summarization
extensive iments on duc generic summarization benchmark datasets show that tcsum achieves the state of the art performance even without using any hand crafted features
we also serve that tcsum indeed catches the variations of summary styles among different text categories
we believe our model can be used to other summarization tasks including focused summarization and guided summarization
in tion we plan to let the model distinguish documents in a topic cluster which is better adapted to the multi document summarization
acknowledgments the work described in this paper was supported by search grants council of hong kong polyu national natural science foundation of china and the hong kong polytechnic university b
the correspondence authors of this paper are wenjie li and sujian li
references cao et al
cao z
wei f
dong l
li s
and zhou m

ranking with recursive neural networks figure similarity among the transformation matrices we set the self similarity scores to
ming ilp problem which was able to nd the global timal solution mcdonald gillick and favre
graph based models such as manifold wan and xiao played an important role in extractive summarization cause of its ability to reect various sentence relationships
in contrast to these unsupervised methods there are also many successful learning based summarization approaches
different classiers have been explored including tional random field galley support vector sion li et al
and logistic regression li qian and liu
recently the application of deep neural network niques has attracted more and more interest in the rization research
genest gotti and bengio used unsupervised auto encoders to represent both manual and system summaries for summary evaluation
their method however did not surpass rouge
cao et al
cao et al
tried to use neural networks to complement sentence ranking features
although the models achieved the state of the art performance they still relied on hand crafted features
a few researches explored to directly measure ilarity based on distributed representations
yin and pei lai et al
lai s
xu l
liu k
and zhao j

recurrent convolutional neural networks for text tion
in proceedings of aaai
li et al
li s
ouyang y
wang w
and sun b

multi document summarization using support vector regression
in proceedings of duc
li li and hovy li j
li r
and hovy e
h

recursive deep models for discourse parsing
in ings of emnlp
li qian and liu li c
qian x
and liu y

using supervised bigram based ilp for extractive rization
in proceedings of acl
lin lin c


rouge a package for automatic evaluation of summaries
in proceedings of the acl shop
mcdonald mcdonald r

a study of global inference algorithms in multi document summarization
springer
mikolov et al
mikolov t
chen k
corrado g
and dean j

efcient estimation of word tions in vector space
arxiv preprint

rush chopra and weston rush a
m
chopra s
and weston j

a neural attention model for in proceedings of emnlp tive sentence summarization

wan and xiao wan x
and xiao j

based multi modality learning for topic focused document summarization
in proceedings of ijcai
wan et al
wan x
cao z
wei f
li s
and zhou m

multi document summarization via discriminative summary reranking
arxiv preprint

yin and pei yin w
and pei y

optimizing sentence modeling and selection for document tion
in proceedings of ijcai
in and its application to multi document summarization
proceedings of aaai
cao et al
cao z
wei f
li s
li w
zhou m
and wang h

learning summary prior tation for extractive summarization
proceedings of acl short papers
cao et al
cao z
chen c
li w
li s
wei f
and zhou m

tgsum build tweet guided document summarization dataset
in proceedings of aaai
cao et al
cao z
li w
li s
and wei f

attsum joint learning of focusing and summarization with neural attention
in proceedings of coling
carbonell and goldstein carbonell j
and goldstein j

the use of mmr diversity based reranking for ordering documents and producing summaries
in ings of sigir
cheng and lapata cheng j
and lapata m

neural summarization by extracting sentences and words
arxiv preprint

collobert et al
collobert r
weston j
bottou l
karlen m
kavukcuoglu k
and kuksa p

natural language processing almost from scratch
the journal of machine learning research
dong et al
dong l
wei f
zhou m
and xu k

adaptive multi compositionality for recursive neural models with applications to sentiment analysis
in ings of aaai
duchi hazan and singer duchi j
hazan e
and singer y

adaptive subgradient methods for online learning and stochastic optimization
the journal of chine learning research
filippova et al
filippova k
alfonseca e
menares c
a
kaiser l
and vinyals o

tence compression by deletion with lstms
in proceedings of emnlp
galley galley m

a skip chain conditional random eld for ranking meeting utterances by importance
in proceedings of emnlp
genest gotti and bengio genest p

gotti f
and bengio y

deep learning for automatic mary scoring
in proceedings of the workshop on automatic text summarization
gillick and favre gillick d
and favre b

a scalable global model for summarization
in proceedings of the workshop on ilp for nlp
hong and nenkova hong k
and nenkova a

improving the estimation of word importance for news multi document summarization
in proceedings of eacl
kedzie mckeown and diaz kedzie c
mckeown k
and diaz f

predicting salient updates for disaster summarization
in proceedings of acl
kobayashi noguchi and yatsuka kobayashi h
noguchi m
and yatsuka t

summarization based in proceedings of emnlp on embedding distributions


