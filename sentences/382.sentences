how to evaluate a summarizer study design and statistical analysis for manual linguistic quality evaluation julius steen and katja markert department of computational linguistics heidelberg university heidelberg germany
uni heidelberg
abstract manual evaluation is essential to judge progress on automatic text summarization
however we conduct a survey on recent marization system papers that reveals little agreement on how to perform such evaluation studies
we conduct two evaluation ments on two aspects of summaries linguistic quality coherence and repetitiveness to pare likert type and ranking annotations and show that best choice of evaluation method can vary from one aspect to another
in our vey we also nd that study parameters such as the overall number of annotators and tion of annotators to annotation items are ten not fully reported and that subsequent tistical analysis ignores grouping factors ing from one annotator judging multiple maries
using our evaluation experiments we show that the total number of annotators can have a strong impact on study power and that current statistical analysis methods can inate type i error rates up to eight fold
in tion we highlight that for the purpose of tem comparison the current practice of ing multiple judgements per summary leads to less powerful and reliable annotations given a xed study budget
introduction current automatic metrics for summary evaluation have low correlation with human judgements on summary quality especially for linguistic quality evaluation fabbri et al

as a consequence manual evaluation is still vital to properly compare the linguistic quality of summarization systems
while the document understanding conferences duc established a standard manual evaluation procedure dang we conduct a sive survey of recent works in text summarization that reveals a wide array of different evaluation questions and methods in current use
furthermore duc procedures were designed for a small set of expert judges while current evaluation campaigns are often conducted by untrained crowd workers
the design of the manual annotation specically the overall number of annotators as well as the distribution of annotators to annotation items has substantial impact on power reliability and type i errors of subsequent statistical analysis
ever most current papers see section do not consider the interaction of annotation design and statistical analysis
we investigate the optimal notation methods design and statistical analysis of summary evaluation studies making the following contributions
we conduct a comprehensive survey on the current practices in manual summary ation in section
often important study parameters such as the total number of notators are not reported
in addition tistical signicance is either not assessed at all or with tests t test or one way anova that lead to inated type i error in the ence of grouping factors barr et al

in summarization evaluation grouping factors arise whenever one annotator rates multiple summaries

we carry out annotation experiments for herence and repetition
we use both and ranking style questions on the output of four recent summarizers and reference maries
we show that ranking style uations are more reliable and cost efcient for coherence similar to prior ndings by novikova et al
and sakaguchi and van durme
however on repetition where many documents do not exhibit any problems likert outperforms ranking

based on our annotation data we perform n a j l c
s c v
v i x r a monte carlo simulations to show the risk posed by ignoring grouping factors in cal analysis and nd up to eight fold increases in type i errors when using standard icance tests
as an alternative we propose to either use mixed effect models barr et al
for analysis or to design studies in such a manner that results can be aggregated into dependent samples amenable to simpler ysis tools

finally we show that the common practice of eliciting repeated judgements for the same summary leads to less reliable and ful studies for system level comparison when compared to studies with the same budget but only one judgement per summary
code and data for our experiments is able at
com julmaxi summary
literature survey we survey all summarization papers in acl eacl naacl conll emnlp tacl and the computational linguistics journal in the years
we choose this timeframe as we are terested in current practices in summarization uation marks the publication of the pointer generator network see et al
which has been highly inuential for neural summarization
we focus our analysis on papers that present a novel system for or multi document tion and take a single or multiple full texts as input and also output text sds mds
this allows us to concentrate on recommendations for human ation of newly developed summarization systems
out of the resulting sds mds system pers we identify all papers that conduct at least one new comparative system evaluation with man annotators for further analysis leading to papers in the survey
the fact that this is only about half of all papers is troubling given that it has been recently demonstrated that current automatic uation measures such as rouge lin are from the analysis are sentence summarization or headline generation papers although most of the points we make hold for their evaluation campaigns as well
rization evaluation papers that do not present a new system but concentrate on sometimes large scale system comparisons are discussed in the related work section instead
lists of all included and excluded papers are given in supplementary material which also contains exact evaluation parameters per paper in a spreadsheet
category overall content fluency coherence repetition faithfulness referential clarity other likert pairwise rank bws qa binary other not given reference reference not given not given crowd other t test anova ci other unspecied none pap
std
evaluation questions evaluation method number of documents in evaluation number of systems considered number of annotations per summary overall number of annotators annotator recruitment statistical evaluation table our survey for system papers with ual evaluation studies
we show numbers both for individual studies and per paper
as a paper may contain several studies with different parameters counts in the paper column do not always add up
not good at predicting summary scores for modern systems schluter kryscinski et al
peyrard
we assess both what studies ask annotators to judge as well as how they elicit and analyse ments
the survey was conducted by one of the authors for most papers the categories they fell into were obvious
for difcult cases unclear ications papers that do not t the normal mould the two authors discussed the categorisations
vey results are given in table
further details about the choices made in the survey including egory groupings denitions and what is included under other can be found in appendix b
as many papers conduct more than one human evaluation for example on different corpora we also list dividual annotation studies a total of
of the systems that do have human evaluation many focus on content including informativeness coverage focus and relevance
where linguistic quality is evaluated most focus on general tions about uency readability with a smaller ber of papers evaluating coherence and repetition
in the rest of this section we focus on the three aspects of evaluation we cover in this paper how to elicit judgements how these judgements are lysed statistically and how studies are designed

methods the majority of evaluations is conducted using likert type judgements with the second most quent method being rank based annotations cluding pairwise comparison
best worst scaling bws is a specic type of ranking oriented uation that requires annotators to specify only the rst and last rank kiritchenko and mohammad
qa narayan et al
is used for tent evaluation only
this motivates us to compare both likert and ranking annotations in section


statistical analysis if a signicance test is conducted most papers yse their data either using anova or a sequence of paired t tests
both tests are based on the sumption that judgements or pairs of judgements in case of paired t test are sampled independently from each other
however in almost all studies annotators give judgements on more than one mary from the same system
thus the resulting judgements are only independent if we assume that all annotators behave identically
given that prior work gillick and liu amidei et al
as well as our own reliability analysis in section
show that especially crowd workers tend to disagree about judgements this assumption does not seem warranted
as a consequence traditional signicance tests are at high risk of inated type i error rates
this is well known in the broader eld of linguistics barr et al
but is disregarded in summarization evaluation
we show in section that this is a substantial problem for current rization evaluations and suggest alternative analysis methods

design most papers only report the number of documents in the evaluation and the number of judgements per summary
this however is not sufcient to scribe the design of a study lacking any indication about the overall number of annotators that made these judgements
a study with summaries and annotations per summary can mean annotators did all judgements in one extreme or a study with distinct annotators in the other
only of the studies describe their annotation design in full almost all of which use designs in which a small number of annotators judge all summaries
only of crowdsourced studies report the full design
we show in section that a low total number of annotators aggravates type i error rates with proper statistical analysis
in section we further show that with proper analysis a low total number of annotators leads to less powerful experiments
almost all analysed papers choose designs with multiple judgements per summary
however we show in section
that this for the purpose of system ranking leads to loss of reliability as well as power when compared to a study with the same budget and only one annotation per summary
coherence and repetition annotation to elicit summary judgements for analysis we in duct studies on two linguistic quality tasks
the rst we ask annotators to judge the ence of the summaries while in the second we ask for the repetitiveness of the summary
we lect these two tasks over the more frequent fluency task as we found in preliminary investigations that many recent summarization systems already duce highly uent text making them hard to entiate
we do not evaluate overall and content as both require access to the input document which differentiates these questions from the linguistic quality evaluation of the summaries
for both tasks we conduct one study using a seven point likert scale likert and another ing a ranking based annotation method rank where annotators rank summaries for the same ument from best to worst
screenshots of the terfaces for both approaches and full annotator structions are given in appendix a
corpus and systems
mirroring a common setup see section we select four abstractive summarization systems and the reference maries ref for analysis
the pointer generator summarizer pg see et al
which is still often used as a baseline for abstractive summarization the abstractive sentence rewriter asr of gehrmann et al
which is a strong summarization system that does not rely on external pretraining for its generation step seneca sharma et al
a system that combines explicit modelling of coreference information with an external coherence model bart lewis et al
a transformer work that achieves sota on cnn dm
we randomly sample documents from the popular cnn dm hermann et al
with corresponding summaries from all systems to form the item set for all our studies
study design
we ensure a sufcient total ber of annotators by using a block design
we separated our corpus into blocks of documents and included all summaries for each document in the same block which results in summaries per block
all items in a block were judged by the same set of three annotators
no annotator was allowed to judge more than one block
this results in a total of annotators and judgements per task
figure shows a schematic overview of our design which balances the need for a large enough annotator pool with a sufcient task size to be worthwhile to annotators
we recruited native english speakers from the crowdsourcing platform and carefully justed the reward to be no lower than
per hour based on pilot studies
summaries or sets
com figure schematic representation of our study sign
rows represent annotators columns documents
each blue square corresponds to a judgement of the summaries of all ve systems for a document
every rectangular group of blue squares forms one block
of summaries for rank within a block were sented in random order
ranking vs
likert table shows the average likert scores and the average rank for all systems tasks and annotation methods
we use mixed effect ordinal regression to identify signicant score differences see tion for details
both annotation methods vide compatible system rankings for the two tasks though for the repetition task both methods gle to differentiate between systems
if we were interested in the true ranking we could conduct a power analysis given some effect size of and elicit additional judgements to improve the ranking
however as we are concerned with the process of system evaluation and not the evaluation itself we do not conduct any further analysis
in the remainder of this section we focus on the reliability of the two methods as well as their cost effectiveness

reliability traditionally reliability is computed by adjusted agreement on individual instances
ever for nlg evaluation amidei et al
gue that a low agreement often reects variability in language perception
additionally we are not interested in individual document scores but in whether independent runs of the same study would result in consistent system scores
in table we thus report split half reliability shr in addition to krippendorffs krippendorff
to pute shr we randomly divide judgements into two groups that share neither annotators nor ments i
e
two independent runs of the study
we likert coh rank coh likert rep rank rep system bart ref asr pg seneca



















table results of our annotation experiment
numbers in brackets indicate rank for a system for a given notation method
multiple ranks in the brackets indicate systems at these ranks are not statistically signicantly different p
mixed effects ordinal regression
system coh likert
coh rank
rep likert
rep rank
shr



table krippendorffs with ordinal level of surement and split half reliability for both annotation methods on the two tasks
then compute the between the system scores in both halves
the nal score is the average correlation after trials
though agreement on individual summaries is relatively low for all annotation methods studies still arrive at consistent system scores when we average over many annotators as demonstrated by the shr
this reects similar observations made by gillick and liu
we nd that on coherence rank is more able than likert though not on repetition
an investigation of the likert score distributions for both tasks in figure shows that coherence scores are relatively well differentiated whereas a majority of repetition judgements give the highest score of indicating no repetition at all in most summaries
we speculate overall agreement fers because ranking summaries with similarly low level of repetition and not allowing ties is tially arbitrary

cost efciency while more reliable annotation methods allow for fewer annotations the cost of a study is ultimately determined by the work time that needs to be vested to achieve a reliable result
to investigate use the pearson correlation implementation of scipy virtanen et al

is supported by feedback we received from tors that the summaries were difcult to rank as they mostly avoided repetition well
figure score distribution of likert for both tasks
each data point shows the number of times a particular score was assigned to each system
figure time spent on annotation in minutes vs
relation with the full sized score
we gather annotation times in buckets with a width of ten minutes and show the condence interval for each bucket
this we randomly sample between and blocks from our annotations and compute the total time annotators spent to complete each sample
we also compute the pearson correlation of the system scores in each sample with the scores on the full annotation set
we relate time spent to similarity between sample and full score in figure
for coherence rank is more efcient than likert
on repetition the lower reliability of rank also results in lower efciency
however with additional annotation effort reliability comes on par with likert
this is a consequence of the overall lower annotator workload for rank
statistical analysis and type i errors the two most common signicance tests in rization studies anova and t test see table both assume judgements or pairs of judgements in the case of t test are independent
this is however not true for most study setups as a single annotator typically judges multiple summaries and multiple summaries are generated from the same input ument
both documents and annotators are thus grouping factors in a study that must be taken into account by the statistical analysis
generalized mixed effect models barr et al
offer a lution but have to the best of our knowledge not been used in summarization evaluation at all
we choose a mixed effect ordered logit model to yse our likert data for both tasks
we will show that traditional analysis methods have a tially elevated risk of type i errors i
e
differences between systems found in manual analysis might be overstated
method
the ordered logit model we employ can be described as follows y c x zaua zdud where p y c is the probability that the score of a summary is at most c
c is the threshold cient for level c is the vector of xed effects and ud are the vectors of and level random effects respectively where ua ud are both drawn from normal distributions with mean
finally x za zd are design matrices for xed and random effects
as the only xed effect we use a dummy coded variable indicating the system that has produced the summary with ref as the ence level
we estimate both random intercepts and slopes for both documents and annotators ing advice of barr et al
to always specify the maximal random effect structure
in practical terms this means that we allow annotators to both differ in how harsh or generous they are in their assessment as well as in which system they prefer
similarly we allow system performance to vary per document leading to both generally higher or lower scores as well as different system rankings per document
do not include rank data as the ordinal regression model does not generate ranks
figure relation of type i error rates at p
to the total number of annotators for different designs all with documents and judgements per summary
we conduct the experiment with both the t test and proximate randomization test art
we show results both with averaging results per document and without any aggregation
we run trials per design
the red line marks the nominal error rate of

we t all models using the ordinal r package christensen and compute pairwise contrasts between the parameters estimated for each system using the emmeans package lenth et al
with tukey adjustment
to demonstrate the problem of ignoring the grouping factors we can now sample articial data from the model distribution and try to analyse it with inappropriate tests
this monte carlo tion is similar to the more general analysis of barr et al

we set to so all systems perform equally well on the population level and only keep the zero mean document and annotator effects in the model
the false positive rate of statistical tests on this articial data should thus be no higher than the signicance level
we then repeatedly apply both the t test and the approximate randomization test art noreen a non parametric test to samples drawn from the model and determine the type i error rate at p

we set the number of documents to and demand judgements per summary to mirror a common setup in manual evaluation
we then vary the total number of tators between and by changing how many summaries a single annotator judges
results
we report results given the model mated for likert in figure
ignoring the pendencies between samples leads to inated type i error rates whether using the t test or the art
this is especially severe when only few annotators judge the whole corpus
in the extreme case with only three annotators in total the null hypothesis is rejected in about of trials at a signicance level of
in both tasks
even our original design with annotators still sees an increase of the type i error rate by about
only if every annotator judges a single document and annotations are aged per document samples are independent and thus the real error is at the nominal
level
this design however is unrealistic given that annotators must be recruited and instructed
we suggest two solutions to this problem either use mixed effect models or aggregate the ments so samples become independent
this lows the assumptions of simpler tools such as art to be met
in our study we could average ments in every block to receive independent ples
this is only possible however if the design of the study considers this problem in advance a crowd sourcing study that allows annotators to judge as many samples as they like is unlikely to result in such a design
study design and study power when conducting studies for system comparison we are interested in maximizing their power to tect differences between systems
for traditional analysis the power is ultimately determined by the number of documents or judgements when no gregation takes place in the study
however when analysis takes into account individual annotators power becomes additionally dependent on the total number of annotators and how evenly they pated in the study
this gives additional importance to the design of evaluation studies
in this section we thus focus on how to optimize studies for power and reliability
we rst show that for well powered experiments we need to ensure that a sufcient total number of annotators participates in a study
in the second part of this section we will then demonstrate studies can improve their power by not eliciting multiple judgements per summary

overall number of annotators to demonstrate the difference in power caused by varying the total number of annotators in a study we determine the power for a design with the same total number of documents and judgements per document but different total numbers of annotators
we run the experiment both with regression and art with proper aggregation of dependent samples as described in section
we refer to the latter as artagg to differentiate it from normal art
figure power for documents and judgements per summary with different number of total annotators
for each design we repeatedly sample articial data from the likert model and apply both tests to the data
the process is the same as in tion except we do not set to zero and count acceptances of the null hypothesis
we again set the number of documents to and the number of repeat judgements to and vary the total number of annotators between and by varying the number of blocks between and
we test for power at a signicance level of

figure shows how power drops sharply when only few annotators take part in the study
this is in line with the theoretical analysis of judd et al
that shows that the number of pants is crucial for power when analysing studies with mixed effect models
the drop is worse for artagg as fewer annotators mean fewer dent blocks and thus a lack of datapoints for the analysis

annotator distribution most studies elicit multiple judgements per mary following best practices in nlp for design carletta
while this leads to better judgements per document the goal of many marization evaluations is a per system judgement
for this kind of study judd et al
show that for mixed models that include both annotator and target in our case input document effects a design where targets are nested within annotators i
e
every annotator has its own set of documents is always more powerful than one where they are partially crossed with annotators i
e
a study with multiple annotations per summary given the same total number of judgements
in fact power could be maximized by having each annotator judge the this is an observed power analysis it probably estimates the power of our analysis for the true effect
the analysis is thus only useful to compare designs under our best estimate of actual effect sizes
figure power for p
of nested and crossed designs for artagg and regression
x axis shows the number of judgements elicited y axis the power level
related work human evaluation has a long history in tion research
this includes work on the correlation of automatic metrics with human judgements lin liu and liu graham peyrard and eckle kohler gao et al
sun and nenkova xenouleas et al
zhao et al
fabbri et al
gao et al
and improving the efciency of the annotation cess nenkova and passonneau hardy et al
shapira et al

the impact of annotator inconsistency on system ranking has been studied both by owczarzak et al
and gillick and liu
to the best of our knowledge we are the rst to investigate the implications of annotator variance on the statistical analysis and the design in summarization system comparison studies
for general nlg evaluation van der lee et al
establish best practices for evaluation ies
we extend on their advice by conducting perimental studies specically for summary tion
in addition we show the importance of study design and consideration of annotator effects in analysis on real world data
the advice of mathur et al
regarding annotation sequence effects should be taken into account in addition to our suggestions
method comparison
ranking has been shown to be effective in multiple nlp tasks kiritchenko and mohammad zopf including nlg quality evaluation novikova et al

in this work we conrm this for coherence evaluation although we nd evidence that ranking is less cient on repetition where many documents do not exhibit any problems
we also add the dimension of annotator workload as a primary determinant of cost to the analysis of the comparison
figure reliabilities of nested vs
crossed designs for rank and likert for both tasks
maries for only a single unique document
ever this is usually not realistic due to the xed costs of annotator recruitment and instruction
we demonstrate on our dataset how both reliability and power are affected by nested vs
crossed design
to compare reliability we randomly sample both nested and crossed designs from our full study and then compute the pearson correlation of the system scores given by this smaller annotation set with the system scores given by the full study
as shown in figure nested samples are always at least as good and mostly better at approximating the results of the full annotation compared to a crossed sample with the same annotation effort
we also conduct a power analysis for sion and artagg comparing nested and crossed designs
we again turn to monte carlo tion on the likert models and sample nested and crossed designs with the same total number of judgements i
e
the same cost
we keep the block size constant at and vary the number of annotators between and
for nested designs we drop the document level random effects from the ordinal regression as document is no longer a grouping factor in nested designs
figure shows that nested designs always have a power advantage over crossed designs especially when few judgements are elicited
we also nd that art can be used to analyse data without loss of power when there are enough independent blocks
this might be attractive as art is less tionally expensive than ordinal regression
multiple methods have been suggested to duce study cost by sample selection sakaguchi et al
novikova et al
sakaguchi and van durme liang et al
or tion with automatic metrics chaganty et al

these efforts complement ours as care still needs to be taken in analysis and study design
recently rank based magnitude estimation has been shown to be a promising method for eliciting judgements in nlg tasks and offers a combination of ranking and rating approaches novikova et al
santhanam and shaikh
however it has not yet found widespread use in the rization community
while magnitude estimation has been shown to reduce annotator variance our advice regarding experimental design and grouping factors in statistical analysis applies to this method as well as annotators can still systematically differ in which systems they prefer
statistical analysis
with regard to statistical analysis of experimental results dror et al
give advice for hypothesis testing in nlp
however they do not touch on the problem of dependent ples
rankel et al
analyse tac data and show the importance of accounting for input ments in statistical analysis of summarizer mance and suggest the use of the wilcoxon signed rank test for analysis
sadeqi azer et al
argue that values are often not well understood and advocate bayesian methods as an alternative
while the analysis in our paper is frequentist the mixed effect model approach can also be integrated into a bayesian framework
kulikov et al
model annotator bias in such a framework but do not account for differences in annotator preferences
in work conducted in parallel to ours card et al
show that many human experiments in nlp underreport their experimental parameters and are underpowered including likert type judgements
their simulation approach to power analysis is very in addition to their similar to our experiments
analysis we show that ignoring grouping factors in statistical analysis of human annotations leads to inated type i error rates
we also show that power can be increased by choosing nested over crossed designs with the same budget
the problem of derpowered studies has also been tackled outside of nlp by brysbaert
for psycholinguistics barr et al
strate how generalizability of results is negatively impacted by ignoring grouping factors in the ysis
mixed effect models have found use in nlp before green et al
cagan et al
rimova et al
kreutzer et al
but to the best of our knowledge they have not been used in summary evaluation
conclusion we surveyed the current state of the art in manual summary quality evaluation and investigated ods statistical analysis and design of these studies
we distill our ndings into the following guidelines for manual summary quality evaluation method
both ranking and likert type tions are valid choices for quality judgements
however we present preliminary evidence that the optimal choice of method is dependent on task acteristics if many summaries are similar for a given aspect likert may be the better option
analysis
analysis of elicited data should take into account variance in annotator preferences to avoid inated type i error rates
we suggest the use of mixed effect models for analysis that can explicitly take into account grouping factors in ies
alternatively traditional tests can be used with proper study design and aggregation
study design
study designers should control the number of annotators and how many summaries each individual annotator judges to ensure cient study power
additionally to ensure ity of results studies should report the design and the total number of annotators in addition to the number of documents and repeat judgements
ies with repeat judgements on the same summary do not provide any advantage for system son and are less reliable and powerful than nested studies of the same size
we hope that these ndings will help researchers plan their own evaluation studies by allowing them to allocate their budget better
we also hope that our ndings will encourage researchers to take more care in the statistical analysis of results
this prevents misleading conclusions due to ignoring the effect of differences in annotator behaviour
acknowledgements we would like to thank stefan riezler for many fruitful discussions about the applications of mixed effect models
references jacopo amidei paul piwek and alistair willis

rethinking the agreement in human evaluation tasks
in proceedings of the international conference on computational linguistics pages santa fe new mexico usa
association for putational linguistics
dale j
barr roger levy christoph scheepers and harry j
tily

random effects structure for conrmatory hypothesis testing keep it maximal
journal of memory and language
marc brysbaert

how many participants do we have to include in properly powered experiments a tutorial of power analysis with reference tables
journal of cognition
tomer cagan stefan l
frank and reut tsarfaty

data driven broad coverage grammars for ated natural language generation onlg
in ceedings of the annual meeting of the tion for computational linguistics volume long papers pages vancouver canada
sociation for computational linguistics
dallas card peter henderson urvashi khandelwal robin jia kyle mahowald and dan jurafsky

in with little power comes great responsibility
proceedings of the conference on empirical methods in natural language processing emnlp pages online
association for tional linguistics
jean carletta

assessing agreement on tion tasks the kappa statistic
computational guistics
arun chaganty stephen mussmann and percy liang

the price of debiasing automatic metrics in natural language evalaution
in proceedings of the annual meeting of the association for putational linguistics volume long papers pages melbourne australia
association for computational linguistics
r
h
b
christensen

ordinal regression els for ordinal data
r package version


r project
org package ordinal
hoa trang dang

overview of duc
in in proceedings of the document understanding conf
wksp
duc at the human language technology conf

on empirical methods in natural language processing hlt emnlp
alexander r
fabbri wojciech kryscinski bryan mccann caiming xiong richard socher and dragomir radev

summeval re evaluating summarization evaluation
yang gao wei zhao and steffen eger

pert towards new frontiers in unsupervised ation metrics for multi document summarization
in proceedings of the annual meeting of the ciation for computational linguistics pages online
association for computational guistics
yanjun gao chen sun and rebecca j
passonneau

automated pyramid summarization tion
in proceedings of the conference on putational natural language learning conll pages hong kong china
association for computational linguistics
sebastian gehrmann yuntian deng and alexander rush

bottom up abstractive summarization
in proceedings of the conference on pirical methods in natural language processing pages brussels belgium
association for computational linguistics
dan gillick and yang liu

non expert tion of summarization systems is risky
in ings of the naacl hlt workshop on creating speech and language data with amazon s ical turk pages los angeles
association for computational linguistics
yvette graham

re evaluating automatic marization with bleu and shades of rouge
in proceedings of the conference on cal methods in natural language processing pages lisbon portugal
association for tational linguistics
spence green sida i
wang jason chuang jeffrey heer sebastian schuster and christopher d
ning

human effort and machine ity in computer aided translation
in proceedings of the conference on empirical methods in ural language processing emnlp pages doha qatar
association for computational linguistics
hardy hardy shashi narayan and andreas vlachos

highres highlight based reference less evaluation of summarization
in proceedings of the annual meeting of the association for putational linguistics pages florence italy
association for computational linguistics
rotem dror gili baumer segev shlomov and roi ichart

the hitchhiker s guide to testing tical signicance in natural language processing
in proceedings of the annual meeting of the sociation for computational linguistics volume long papers pages melbourne tralia
association for computational linguistics
karl moritz hermann tomas kocisky edward stette lasse espeholt will kay mustafa suleyman and phil blunsom

teaching machines to read and comprehend
in proceedings of the national conference on neural information ing systems volume page cambridge ma usa
mit press
charles m judd jacob westfall and david a kenny

experiments with more than one random tor designs analytic models and statistical power
annual review of psychology
sariya karimova patrick simianer and stefan riezler

a user study on online adaptation of neural machine translation to human post edits
machine translation
svetlana kiritchenko and saif mohammad

worst scaling more reliable than rating scales a case study on sentiment intensity annotation
in ceedings of the annual meeting of the tion for computational linguistics volume short papers pages vancouver canada
ciation for computational linguistics
julia kreutzer nathaniel berger and stefan riezler

correct me if you can learning from error corrections and markings
proceedings of the annual conference of the european association for machine translation
klaus krippendorff

content analysis an troduction to its methodology
sage beverly hills ca
wojciech kryscinski nitish shirish keskar bryan cann caiming xiong and richard socher

neural text summarization a critical evaluation
in proceedings of the conference on empirical methods in natural language processing and the international joint conference on natural guage processing emnlp ijcnlp pages hong kong china
association for tional linguistics
ilia kulikov alexander miller kyunghyun cho and jason weston

importance of search and uation strategies in neural dialogue modeling
in proceedings of the international conference on natural language generation pages tokyo japan
association for computational linguistics
chris van der lee albert gatt emiel van miltenburg sander wubben and emiel krahmer

best practices for the human evaluation of automatically generated text
in proceedings of the tional conference on natural language generation pages tokyo japan
association for putational linguistics
russell lenth henrik singmann jonathon love paul buerkner and maxime herve

emmeans timated marginal means aka least squares means
r package version
linguistics pages online
association for computational linguistics
weixin liang james zou and zhou yu

yond user self reported likert scale ratings a parison model for automatic dialog evaluation
in proceedings of the annual meeting of the ciation for computational linguistics pages online
association for computational guistics
chin yew lin

rouge a package for matic evaluation of summaries
in text tion branches out pages barcelona spain
association for computational linguistics
feifan liu and yang liu

correlation between rouge and human evaluation of extractive meeting summaries
in proceedings of hlt short papers pages columbus ohio
tion for computational linguistics
nitika mathur timothy baldwin and trevor cohn

sequence effects in crowdsourced in proceedings of the conference on tions
empirical methods in natural language processing pages copenhagen denmark
tion for computational linguistics
shashi narayan shay b
cohen and mirella lapata

ranking sentences for extractive tion with reinforcement learning
in proceedings of the conference of the north american ter of the association for computational linguistics human language technologies volume long pers pages new orleans louisiana
association for computational linguistics
ani nenkova and rebecca passonneau

ing content selection in summarization the in proceedings of the human mid method
guage technology conference of the north can chapter of the association for computational linguistics hlt naacl pages boston massachusetts usa
association for putational linguistics
eric w noreen

computer intensive methods for testing hypotheses
wiley new york
jekaterina novikova ondrej dusek and verena rieser

rankme reliable human ratings for natural in proceedings of the language generation
conference of the north american chapter of the association for computational linguistics human language technologies volume short papers pages new orleans louisiana
association for computational linguistics
mike lewis yinhan liu naman goyal jan ghazvininejad abdelrahman mohamed omer levy veselin stoyanov and luke zettlemoyer

bart denoising sequence to sequence training for natural language generation translation and comprehension
in proceedings of the nual meeting of the association for computational karolina owczarzak john m
conroy hoa trang dang and ani nenkova

an assessment of the accuracy of automatic evaluation in in proceedings of workshop on evaluation tion
metrics and system comparison for automatic marization pages montreal canada
tion for computational linguistics
maxime peyrard

studying summarization uation metrics in the appropriate scoring range
in proceedings of the association for computational linguistics pages florence italy
association for tational linguistics
the annual meeting of maxime peyrard and judith eckle kohler

a principled framework for evaluating summarizers comparing models of summary quality against in proceedings of the man judgments
nual meeting of the association for computational linguistics volume short papers pages vancouver canada
association for computational linguistics
peter rankel john conroy eric slud and dianne oleary

ranking human and machine in proceedings of the marization systems
conference on empirical methods in natural guage processing pages edinburgh land uk
association for computational tics
erfan sadeqi azer daniel khashabi ashish wal and dan roth

not all claims are created equal choosing the right statistical approach to sess hypotheses
in proceedings of the annual meeting of the association for computational guistics pages online
association for computational linguistics
keisuke sakaguchi matt post and benjamin van durme

efcient elicitation of tions for human evaluation of machine translation
in proceedings of the ninth workshop on cal machine translation pages baltimore maryland usa
association for computational linguistics
keisuke sakaguchi and benjamin van durme

efcient online scalar annotation with bounded port
in proceedings of the annual meeting of the association for computational linguistics ume long papers pages melbourne australia
association for computational tics
sashank santhanam and samira shaikh

wards best experiment design for evaluating in proceedings of the logue system output
international conference on natural language eration pages tokyo japan
association for computational linguistics
natalie schluter

the limits of automatic marisation according to rouge
in proceedings of the conference of the european chapter of the association for computational linguistics volume short papers pages valencia spain
sociation for computational linguistics
abigail see peter j
liu and christopher d
manning

get to the point summarization with generator networks
in proceedings of the nual meeting of the association for computational linguistics volume long papers pages vancouver canada
association for tional linguistics
ori shapira david gabay yang gao hadar nen ramakanth pasunuru mohit bansal yael sterdamer and ido dagan

crowdsourcing lightweight pyramids for manual summary tion
in proceedings of the conference of the north american chapter of the association for putational linguistics human language gies volume long and short papers pages minneapolis minnesota
association for putational linguistics
eva sharma luyang huang zhe hu and lu wang

an entity driven framework for abstractive in proceedings of the summarization
ference on empirical methods in natural language processing and the international joint ence on natural language processing ijcnlp pages hong kong china
sociation for computational linguistics
simeng sun and ani nenkova

the feasibility of embedding based automatic evaluation for in proceedings of gle document summarization
the conference on empirical methods in ural language processing and the international joint conference on natural language processing emnlp ijcnlp pages hong kong china
association for computational linguistics
pauli virtanen ralf gommers travis e
oliphant matt haberland tyler reddy david peau evgeni burovski pearu peterson warren weckesser jonathan bright stefan j
van der walt matthew brett joshua wilson k
jarrod millman nikolay mayorov andrew r
j
nelson eric jones robert kern eric larson cj carey ilhan polat yu feng eric w
moore jake vand erplas denis laxalde josef perktold robert cimrman ian riksen e
a
quintero charles r harris anne m
archibald antonio h
ribeiro fabian pedregosa paul van mulbregt and scipy
contributors

scipy
fundamental algorithms for scientic computing in python
nature methods
stratos xenouleas prodromos malakasiotis marianna apidianaki and ion androutsopoulos

qe a bert based summary quality estimation model
in proceedings of the conference on empirical methods in natural language processing and the international joint conference on ral language processing emnlp ijcnlp pages hong kong china
association for computational linguistics
wei zhao maxime peyrard fei liu yang gao tian m
meyer and steffen eger

moverscore text generation evaluating with contextualized beddings and earth mover distance
in proceedings of the conference on empirical methods in natural language processing and the tional joint conference on natural language cessing emnlp ijcnlp pages hong kong china
association for computational guistics
evaluation method
binary includes any task with a yes no style decision while pairwise cludes any method in which two systems are ranked against each other
other includes markus zopf

estimating summary quality with in proceedings of the pairwise preferences
conference of the north american chapter of the association for computational linguistics human language technologies volume long papers pages new orleans louisiana
ation for computational linguistics
a interface screenshots we show screenshots of the instructions for both annotation methods and tasks in figure and faces in figure
b survey b
categories while most categories are self explanatory we elaborate on some of the decisions we made during the survey in this section
evaluation questions
we allow a single study to include multiple evaluation questions as long as all questions are answered by the same annotators and use the same method
we make no tion between informativeness coverage focus and relevance and summarize them under content
ilarly we summarize uency grammaticality and readability under fluency
other includes one study with a specialized set of tion questions evaluating the usefulness of a generated related work summary one study of polarity in a sentiment rization context one study where annotators were asked to identify the aspect a summary covers in the context of review summarization the aspect identication task mentioned above one study in which participants selected a single best summary out of a set of summaries annotator recruitment
other includes any cruitment strategy that does not rely on ing
this includes cases in which the recruitment was not specied students experts the authors themselves and various kinds of volunteers
statistical evaluation
other unspecied cludes four studies which reported statistical cance without reporting the test used two studies using the approximate ization test one study using the chi square test one study using a tukey test without prior anova
b
survey files all papers we considered for the survey are listed in the supplementary material in the le all papers
yaml by their i d in the acl thology
the sds mds system papers that contain new human evaluation studies and are thus included in the survey are listed in the category with human eval
for the sake of completeness we further list marization papers we did not include in our survey
we separate them into the following categories two studies evaluating formality and meaning similarity of reference and system summary no human eval sds mds system papers without human evaluation one study evaluating diversity one study conducting a turing test sentsum sentence summarization and headline generation papers one study asking paper authors whether they would consider a sentence part of a summary of their own paper
non system summarization papers that do not introduce new systems like surveys opinion pieces and evaluation studies one study evaluating structure and topic versity other papers that conduct summarization with either non textual input or non textual output likert coherence rank coherence likert repetition rank repetition figure screenshots of the annotator instructions
likert coherence rank coherence likert repetition rank repetition figure screenshots of the annotation interfaces
summary instructionsplease read the following summaries and sort them in descendingorder of coherence in the list to the right
fa announced this week that england are pulling out of the eventwith immediate effect in order to achieve a more varied fixture list including more foreign opposition
england have pulled out of thehome nations international under tournament
sky say theirrecommendations including shortening the time between games would have raised the profile of an historic competition that first tookplace in
sky sports drastic cost cutting across the board after paying a match to retain premier league rights is being blamed forthe demise of the victory shield
england have pulled out of thehome nations international under tournament
bt sport are tobroadcast the inaugural european games in baku in june havingfinally agreed terms
sky allegedly withdrew their title sponsorship of under bt sport are to broadcast inaugural european games inbaku in june brazilian legend pele is due in london on thursday for anart exhibitionsky sports drastic cost cutting across the board after paying a match to retain premier league rights is being blamed forthe demise of the victory shield the home nations under
england are pulling out of the event with immediateeffect in order to achieve a more varied fixture list
england havepulled out of a home nations international under
the and
sky sports drastic cost cutting across the board after paying a match
england are pulling out of the event withimmediate effect in order
sky price hikes involving all theirprogramming since almost breaking the bank by committing

england have pulled out of home nations under
most coherentleast coherentsummary instructionsplease rank the following summaries into the list to the right so thatthe summary with the least amount of unnecessary repetition is firstand the one with the most unnecessary repetition is last
bundchen was the highest paid model in according to forbesmagazine with a total million in contracts
she is the face ofchanel and carolina herrera has her own line of lingerie
the and
tom brady to gisele bundchen you inspire me every day bundchen had last runway show wednesday she be focusing moreon family special projects tom brady love for his wife will never go out of fashion
bundchenwas the highest paid model in
bundchen announced herretirement from the catwalk last weekend
bundchen walked therunway for the last time wednesday and the new england patriotsquarterback was there to support her in person
gisele bundchen announced her retirement from the catwalklast weekend
she was the highest paid model in according toforbes magazine
she is the face of chanel and carolina herrera andhas her own line of lingerie
tom brady love for his wife model gisele bundchen will never goout of fashion
bundchen announced her retirement from thecatwalk last weekend
she is the face of chanel and carolina herreraand has her own line of lingerie
least unnecessary repetitionmost unnecessary repetition we give a full list of the survey results for all papers with human evaluation studies in the le survey details
csv
the le has the ing columns paper i d of the paper in the acl anthology eval i d i d of the evaluation study to differentiate them in papers with multiple studies task summarization task of the paper sds vs
mds genre genre of the summarized documents docs number of documents in the evaluation systems number of systems in the evaluation includes reference whether the reference mary is included in the human evaluation ann total total number of annotators in the study ann item number of annotators per summary content uency repetition coherence tial clarity other overall binary columns indicating evaluation questions in the paper measure annotation method used in the study anntype annotator recruitment strategy stattest statistical test used design specied indicates whether it is possible to determine the full design from the tion given about the study in the paper comments comment column
this column scribes the use of other where present

