get to the point summarization with pointer generator networks abigail see stanford university
edu peter j
liu google brain
com christopher d
manning stanford university
edu r a l c
s c v
v i x r a abstract neural sequence to sequence models have provided a viable new approach for stractive text summarization meaning they are not restricted to simply selecting and rearranging passages from the nal text
however these models have two shortcomings they are liable to reproduce factual details inaccurately and they tend to repeat themselves
in this work we pose a novel architecture that augments the standard sequence to sequence attentional model in two orthogonal ways
first we use a hybrid pointer generator network that can copy words from the source text via pointing which aids accurate duction of information while retaining the ability to produce novel words through the generator
second we use coverage to keep track of what has been summarized which discourages repetition
we apply our model to the cnn daily mail marization task outperforming the current abstractive state of the art by at least rouge points
original text truncated lagos nigeria cnn a day after winning ria s presidency muhammadu buhari told cnn s christiane amanpour that he plans to aggressively ght corruption that has long plagued nigeria and go after the root of the nation s unrest
buhari said he ll rapidly give attention to curbing violence in the northeast part of nigeria where the rorist group boko haram operates
by cooperating with neighboring nations chad cameroon and niger he said his administration is condent it will be able to thwart criminals and others contributing to nigeria s instability
for the rst time in nigeria s history the opposition defeated the ruling party in democratic elections
buhari defeated incumbent goodluck jonathan by about million votes according to nigeria s independent national electoral commission
the win comes after a long history of military rule coups and botched attempts at democracy in africa s most populous nation
baseline attention unk unk says his administration is dent it will be able to destabilize nigeria s economy
unk says his istration is condent it will be able to thwart criminals and other nigerians
he says the country has long nigeria and nigeria s economy
pointer gen muhammadu buhari says he plans to aggressively ght ruption in the northeast part of nigeria
he says he ll rapidly give tention to curbing violence in the northeast part of nigeria
he says his administration is condent it will be able to thwart criminals
pointer gen coverage muhammadu buhari says he plans to aggressively ght corruption that has long plagued nigeria
he says his administration is condent it will be able to thwart criminals
the win comes after a long tory of military rule coups and botched attempts at democracy in africa most populous nation
figure comparison of output of tive summarization models on a news article
the baseline model makes factual errors a sical sentence and struggles with oov words muhammadu buhari
the pointer generator model is accurate but repeats itself
coverage eliminates repetition
the nal summary is composed from several fragments
introduction summarization is the task of condensing a piece of text to a shorter version that contains the main formation from the original
there are two broad approaches to summarization extractive and stractive
extractive methods assemble summaries exclusively from passages usually whole tences taken directly from the source text while abstractive methods may generate novel words and phrases not featured in the source text as a human written abstract usually does
the tractive approach is easier because copying large chunks of text from the source document ensures baseline levels of grammaticality and accuracy
on the other hand sophisticated abilities that are crucial to high quality summarization such as paraphrasing generalization or the incorporation of real world knowledge are possible only in an abstractive framework see figure
due to the difculty of abstractive tion the great majority of past work has been tractive kupiec et al
paice gion and poibeau
however the recent cess of sequence to sequence models sutskever figure baseline sequence to sequence model with attention
the model may attend to relevant words in the source text to generate novel words e

to produce the novel word beat in the abstractive summary germany beat argentina the model may attend to the words victorious and win in the source text
et al
in which recurrent neural networks rnns both read and freely generate text has made abstractive summarization viable chopra et al
nallapati et al
rush et al
zeng et al

though these systems are promising they exhibit undesirable behavior such as inaccurately reproducing factual details an inability to deal with out of vocabulary oov words and repeating themselves see figure
in this paper we present an architecture that addresses these three issues in the context of multi sentence summaries
while most recent stractive work has focused on headline tion tasks reducing one or two sentences to a single headline we believe that longer text marization is both more challenging requiring higher levels of abstraction while avoiding tition and ultimately more useful
therefore we apply our model to the recently introduced daily mail dataset hermann et al
ati et al
which contains news articles sentences on average paired with multi sentence summaries and show that we outperform the of the art abstractive system by at least rouge points
our hybrid pointer generator network tates copying words from the source text via ing vinyals et al
which improves racy and handling of oov words while retaining the ability to generate new words
the network which can be viewed as a balance between tive and abstractive approaches is similar to gu et al
s copynet and miao and blunsom s forced attention sentence compression that were applied to short text summarization
we propose a novel variant of the coverage vector tu et al
from neural machine translation which we use to track and control coverage of the source document
we show that coverage is markably effective for eliminating repetition
our models in this section we describe our baseline our sequence to sequence model generator model and our coverage mechanism that can be added to either of the rst two models
the code for our models is available online

sequence to sequence attentional model our baseline model is similar to that of nallapati et al
and is depicted in figure
the kens of the article wi are fed one by one into the encoder a single layer bidirectional lstm ducing a sequence of encoder hidden states hi
on each step t the decoder a single layer tional lstm receives the word embedding of the previous word while training this is the previous word of the reference summary at test time it is the previous word emitted by the decoder and has decoder state st
the attention distribution at is calculated as in bahdanau et al
i vt wsst battn at where v wh ws and battn are learnable ters
the attention distribution can be viewed as
github
com abisee pointer generator


attention distribution start vocabulary distributioncontext vectorgermanyazoopartial emerge victorious in win against argentina on saturday


encoder hiddenstates decoderhidden statessource text figure pointer generator model
for each decoder timestep a generation probability pgen is calculated which weights the probability of generating words from the vocabulary versus copying words from the source text
the vocabulary distribution and the attention distribution are weighted and summed to obtain the nal distribution from which we make our prediction
note that out of vocabulary article words such as are included in the nal distribution
best viewed in color
a probability distribution over the source words that tells the decoder where to look to produce the next word
next the attention distribution is used to produce a weighted sum of the encoder hidden states known as the context vector h t i at h the context vector which can be seen as a size representation of what has been read from the source for this step is concatenated with the coder state st and fed through two linear layers to produce the vocabulary distribution pvocab pvocab st h ihi where v v b and are learnable parameters
pvocab is a probability distribution over all words in the vocabulary and provides us with our nal distribution from which to predict words w during training the loss for timestep t is the ative log likelihood of the target word w t for that timestep losst log t and the overall loss for the whole sequence is loss t t losst
pointer generator network our pointer generator network is a hybrid between our baseline and a pointer network vinyals et al
as it allows both copying words via ing and generating words from a xed vocabulary
in the pointer generator model depicted in figure the attention distribution at and context vector h t are calculated as in section

in addition the generation probability pgen for timestep t is calculated from the context vector h t the decoder state st and the decoder input xt hh s st wt xt bptr pgen wt t where vectors wh ws wx and scalar bptr are able parameters and is the sigmoid function
next pgen is used as a soft switch to choose tween generating a word from the vocabulary by sampling from pvocab or copying a word from the input sequence by sampling from the attention tribution at
for each document let the extended vocabulary denote the union of the vocabulary and all words appearing in the source document
we obtain the following probability distribution over the extended vocabulary wi w at note that if w is an out of vocabulary oov word then is zero similarly if w does i source textgermany emerge victorious in win against argentina on saturday





start vocabulary distributioncontext vectorgermanyazoobeatazoopartial summaryfinal distributionencoder hiddenstatesdecoder hidden states not appear in the source document then i wi w at i is zero
the ability to produce oov words is one of the primary advantages of pointer generator models by contrast models such as our baseline are restricted to their pre set vocabulary
the loss function is as described in equations and but with respect to our modied ability distribution given in equation
our loss function is more exible because marization should not require uniform coverage we only penalize the overlap between each tion distribution and the coverage so far ing repeated attention
finally the coverage loss reweighted by some hyperparameter is added to the primary loss function to yield a new composite loss function
coverage mechanism repetition is a common problem for to sequence models tu et al
mi et al
sankaran et al
suzuki and nagata and is especially pronounced when ating multi sentence text see figure
we adapt the coverage model of tu et al
to solve the problem
in our coverage model we maintain a coverage vector ct which is the sum of attention distributions over all previous decoder timesteps ct intuitively ct is a unnormalized distribution over the source document words that represents the gree of coverage that those words have received from the attention mechanism so far
note that is a zero vector because on the rst timestep none of the source document has been covered
the coverage vector is used as extra input to the attention mechanism changing equation to vt wsst wcct et i battn where wc is a learnable parameter vector of same length as v
this ensures that the attention nism s current decision choosing where to attend next is informed by a reminder of its previous decisions summarized in ct
this should make it easier for the attention mechanism to avoid peatedly attending to the same locations and thus avoid generating repetitive text
we nd it necessary see section to ally dene a coverage loss to penalize repeatedly attending to the same locations covlosst i i ct i note that the coverage loss is bounded in lar covlosst i at i
equation differs from the coverage loss used in machine translation
in mt we assume that there should be a roughly to one translation ratio accordingly the nal erage vector is penalized if it is more or less than
losst log t i i ct related work neural abstractive summarization
rush et al
were the rst to apply modern neural works to abstractive text summarization ing state of the art performance on and gigaword two sentence level summarization datasets
their approach which is centered on the attention mechanism has been augmented with current decoders chopra et al
abstract meaning representations takase et al
erarchical networks nallapati et al
ational autoencoders miao and blunsom and direct optimization of the performance metric ranzato et al
further improving mance on those datasets
however large scale datasets for tion of longer text are rare
nallapati et al
adapted the deepmind question answering dataset hermann et al
for summarization ing in the cnn daily mail dataset and provided the rst abstractive baselines
the same authors then published a neural extractive approach lapati et al
which uses hierarchical rnns to select sentences and found that it signicantly outperformed their abstractive result with respect to the rouge metric
to our knowledge these are the only two published results on the full set
prior to modern neural methods abstractive summarization received less attention than tive summarization but jing explored ting unimportant parts of sentences to create maries and cheung and penn explore tence fusion using dependency trees
pointer generator networks
the pointer work vinyals et al
is a sequence sequence model that uses the soft attention tribution of bahdanau et al
to produce an output sequence consisting of elements from the input sequence
the pointer network has been used to create hybrid approaches for nmt cehre et al
language modeling merity et al
and summarization gu et al
gulcehre et al
miao and blunsom nallapati et al
zeng et al

our approach is close to the forced attention sentence compression model of miao and som and the copynet model of gu et al
with some small differences we culate an explicit switch probability pgen whereas gu et al
induce competition through a shared max function
we recycle the attention bution to serve as the copy distribution but gu et al
use two separate distributions
when a word appears multiple times in the source text we sum probability mass from all corresponding parts of the attention distribution whereas miao and blunsom do not
our reasoning is that i lating an explicit pgen usefully enables us to raise or lower the probability of all generated words or all copy words at once rather than individually ii the two distributions serve such similar poses that we nd our simpler approach sufces and we observe that the pointer mechanism often copies a word while attending to multiple currences of it in the source text
our approach is considerably different from that of gulcehre et al
and nallapati et al

those works train their pointer nents to activate only for out of vocabulary words or named entities whereas we allow our model to freely learn when to use the pointer and they do not mix the probabilities from the copy tion and the vocabulary distribution
we believe the mixture approach described here is better for abstractive summarization in section we show that the copy mechanism is vital for accurately reproducing rare but in vocabulary words and in section
we observe that the mixture model ables the language model and copy mechanism to work together to perform abstractive copying
coverage
originating from statistical chine translation koehn coverage was adapted for nmt by tu et al
and mi et al
who both use a gru to update the erage vector each step
we nd that a simpler approach summing the attention distributions to obtain the coverage vector sufces
in this spect our approach is similar to xu et al
who apply a coverage like method to image tioning and chen et al
who also rate a coverage mechanism which they call traction as described in equation into neural summarization of longer text
temporal attention is a related technique that has been applied to nmt sankaran et al
and summarization nallapati et al

in this approach each attention distribution is vided by the sum of the previous which tively dampens repeated attention
we tried this method but found it too destructive distorting the signal from the attention mechanism and reducing performance
we hypothesize that an early vention method such as coverage is preferable to a method such as temporal attention it is better to inform the attention mechanism to help it make better decisions than to override its cisions altogether
this theory is supported by the large boost that coverage gives our rouge scores see table compared to the smaller boost given by temporal attention for the same task nallapati et al

dataset we use the cnn daily mail dataset hermann et al
nallapati et al
which tains online news articles tokens on average paired with multi sentence summaries
tences or tokens on average
we used scripts supplied by nallapati et al
to obtain the same version of the the data which has training pairs validation pairs and test pairs
both the dataset s published results nallapati et al
use the anonymized version of the data which has been pre processed to replace each named entity e

the united tions with its own unique identier for the ple pair e


by contrast we operate directly on the original text or non anonymized version of the which we believe is the vorable problem to solve because it requires no pre processing
experiments all experiments our model has for dimensional hidden states and dimensional word embeddings
for the pointer generator els we use a vocabulary of words for both source and target note that due to the pointer work s ability to handle oov words we can use www
github
com abisee pointer generator meteor exact match stem syn para abstractive model nallapati et al
seq to seq attn baseline vocab seq to seq attn baseline vocab pointer generator pointer generator coverage baseline ours baseline nallapati et al
extractive model nallapati et al
rouge







l

























table rouge and meteor scores on the test set
models and baselines in the top half are abstractive while those in the bottom half are extractive
those marked with were trained and evaluated on the anonymized dataset and so are not strictly comparable to our results on the original text
all our rouge scores have a condence interval of at most
as reported by the ofcial rouge script
the meteor improvement from the baseline to the pointer generator model and from the pointer generator to the pointer model were both found to be statistically signicant using an approximate randomization test with p

a smaller vocabulary size than nallapati et al
s source and target vocabularies
for the baseline model we also try a larger ulary size of
note that the pointer and the coverage nism introduce very few additional parameters to the network for the models with vocabulary size the baseline model has ters the pointer generator adds extra eters wh ws wx and bptr in equation and erage adds extra parameters wc in equation
unlike nallapati et al
we do not train the word embeddings they are learned from scratch during training
we train using grad duchi et al
with learning rate
and an initial accumulator value of

this was found to work best of stochastic gradient descent adadelta momentum adam and sprop
we use gradient clipping with a maximum gradient norm of but do not use any form of ularization
we use loss on the validation set to implement early stopping
during training and at test time we truncate the article to tokens and limit the length of the summary to tokens for training and kens at test time
this is done to expedite ing and testing but we also found that truncating the article can raise the performance of the model upper limit of is mostly invisible the beam search algorithm is self stopping and almost never reaches the step
see section
for more details
for training we found it efcient to start with highly truncated sequences then raise the maximum length once converged
we train on a single tesla m gpu with a batch size of
at test time our summaries are produced using beam search with beam size
we trained both our baseline models for about iterations epochs this is similar to the epochs required by nallapati et al
s best model
training took days and hours for the vocabulary model and days hours for the vocabulary model
we found the pointer generator model quicker to train quiring less than training iterations
in epochs a total of days and hours
ticular the pointer generator model makes much quicker progress in the early phases of training
to obtain our nal coverage model we added the coverage mechanism with coverage loss weighted to as described in equation and trained for a further iterations about hours
in this time the coverage loss converged to about
down from an initial value of about

we also tried a more aggressive value of this duced coverage loss but increased the primary loss function thus we did not use it
we tried training the coverage model without the loss function hoping that the attention anism may learn by itself not to attend repeatedly to the same locations but we found this to be fective with no discernible reduction in repetition
we also tried training with coverage from the rst iteration rather than as a separate training phase but found that in the early phase of training the coverage objective interfered with the main tive reducing overall performance
results
preliminaries our results are given in table
we ate our models with the standard rouge metric lin reporting the scores for and rouge l which respectively measure the word overlap bigram overlap and longest common sequence between the reference summary and the summary to be evaluated
we obtain our rouge scores using the pyrouge package
we also evaluate with the meteor metric denkowski and lavie both in act match mode rewarding only exact matches between words and full mode which ally rewards matching stems synonyms and phrases
in addition to our own models we also report the baseline which uses the rst three tences of the article as a summary and compare to the only existing abstractive nallapati et al
and extractive nallapati et al
els on the full dataset
the output of our models is available online
given that we generate plain text summaries but nallapati et al
generate anonymized summaries see section our rouge scores are not strictly comparable
there is evidence to suggest that the original text dataset may sult in higher rouge scores in general than the anonymized dataset the baseline is higher on the former than the latter
one possible nation is that multi word named entities lead to a higher rate of n gram overlap
unfortunately rouge is the only available means of ison with nallapati et al
s work
nevertheless given that the disparity in the scores is


l points respectively and our best model scores exceed nallapati et al
by


rouge l points we may estimate that we outperform the only previous abstractive system by at least rouge points round

python
org pypi


cs
cmu
meteor
github
com abisee pointer generator s e t a c i l p u e r a t a h t g r a m s m s g r a m s g r a g r a m s n t e s e e s c n pointer generator no coverage pointer generator coverage reference summaries figure coverage eliminates undesirable tition
summaries from our non coverage model contain many duplicated n grams while our age model produces a similar number as the erence summaries

observations we nd that both our baseline models perform poorly with respect to rouge and meteor and in fact the larger vocabulary size does not seem to help
even the better performing baseline with vocabulary produces summaries with several common problems
factual details are quently reproduced incorrectly often replacing an uncommon but in vocabulary word with a common alternative
for example in figure the baseline model appears to struggle with the rare word thwart producing destabilize instead which leads to the fabricated phrase destabilize nigeria s economy
even more catastrophically the summaries sometimes devolve into repetitive nonsense such as the third sentence produced by the baseline model in figure
in addition the baseline model ca nt reproduce out of vocabulary words such as muhammadu buhari in figure
further examples of all these problems are vided in the supplementary material
our pointer generator model achieves much better rouge and meteor scores than the baseline despite many fewer training epochs
the difference in the summaries is also marked of vocabulary words are handled easily factual details are almost always copied correctly and there are no fabrications see figure
however repetition is still very common
our pointer generator model with coverage proves the rouge and meteor scores further convincingly surpassing the best abstractive model article smugglers lure arab and african migrants by ing discounts to get onto overcrowded ships if people bring more potential passengers a cnn investigation has revealed



summary cnn investigation uncovers the business inside a human smuggling ring
article eyewitness video showing white north charleston police ofcer michael slager shooting to death an unarmed black man has exposed discrepancies in the reports of the rst ofcers on the scene



summary more questions than answers emerge in troversial s

police shooting
figure examples of highly abstractive reference summaries bold denotes novel words
of nallapati et al
by several rouge points
despite the brevity of the coverage ing phase about of the total training time the repetition problem is almost completely nated which can be seen both qualitatively figure and quantitatively figure
however our best model does not quite surpass the rouge scores of the baseline nor the current best tive model nallapati et al

we discuss this issue in section

discussion
comparison with extractive systems it is clear from table that extractive systems tend to achieve higher rouge scores than abstractive and that the extractive baseline is extremely strong even the best extractive system beats it by only a small margin
we offer two possible planations for these observations
firstly news articles tend to be structured with the most important information at the start this partially explains the strength of the line
indeed we found that using only the rst tokens about sentences of the article yielded signicantly higher rouge scores than using the rst tokens
secondly the nature of the task and the rouge metric make extractive approaches and the baseline difcult to beat
the choice of tent for the reference summaries is quite subjective sometimes the sentences form a self contained summary other times they simply showcase a few interesting details from the article
given that the articles contain sentences on average there are many equally valid ways to choose or lights in this style
abstraction introduces even more options choice of phrasing further ing the likelihood of matching the reference mary
for example smugglers prot from perate migrants is a valid alternative abstractive summary for the rst example in figure but it scores rouge with respect to the reference summary
this inexibility of rouge is erbated by only having one reference summary which has been shown to lower rouge s bility compared to multiple reference summaries lin
due to the subjectivity of the task and thus the diversity of valid summaries it seems that rouge rewards safe strategies such as ing the rst appearing content or preserving inal phrasing
while the reference summaries do sometimes deviate from these techniques those deviations are unpredictable enough that the safer strategy obtains higher rouge scores on average
this may explain why extractive systems tend to obtain higher rouge scores than abstractive and even extractive systems do not signicantly the baseline
to explore this issue further we evaluated our systems with the meteor metric which rewards not only exact word matches but also matching stems synonyms and paraphrases from a dened list
we observe that all our models ceive over meteor point boost by the sion of stem synonym and paraphrase matching indicating that they may be performing some straction
however we again observe that the baseline is not surpassed by our models
it may be that news article style makes the baseline very strong with respect to any metric
we believe that investigating this issue further is an important direction for future work

how abstractive is our model we have shown that our pointer mechanism makes our abstractive system more reliable copying tual details correctly more often
but does the ease of copying make our system any less abstractive figure shows that our nal model s maries contain a much lower rate of novel n grams i
e
those that do nt appear in the article than the reference summaries indicating a lower degree of abstraction
note that the baseline model produces novel n grams more frequently however this statistic includes all the incorrectly copied words unk tokens and fabrications alongside the good instances of abstraction
l e v o n e r a t a h t g r a m s m s g r a m s g r a g r a m s n t e s e e s c n pointer generator coverage sequence to sequence attention baseline reference summaries figure although our best model is abstractive it does not produce novel n grams i
e
n grams that do nt appear in the source text as often as the reference summaries
the baseline model produces more novel n grams but many of these are erroneous see section

article andy murray


is into the of the ami open but not before getting a scare from year old austrian dominic thiem who pushed him to in the ond set before going down in an hour and three quarters



summary andy murray defeated dominic thiem in an hour and three quarters
article


wayne rooney smashes home during ester united s win over aston villa on saturday



summary manchester united beat aston villa at old trafford on saturday
figure examples of abstractive summaries duced by our model bold denotes novel words
in particular figure shows that our nal model copies whole article sentences of the time by comparison the reference summaries do so only
of the time
this is a main area for improvement as we would like our model to move beyond simple sentence extraction
however we observe that the other encompasses a range of abstractive techniques
article sentences are cated to form grammatically correct shorter sions and new sentences are composed by ing together fragments
unnecessary interjections clauses and parenthesized phrases are sometimes omitted from copied passages
some of these ities are demonstrated in figure and the mentary material contains more examples
figure shows two examples of more sive abstraction both with similar structure
the dataset contains many sports stories whose maries follow the x beat y on plate which may explain why our model is most condently abstractive on these examples
in eral however our model does not routinely duce summaries like those in figure and is not close to producing summaries like in figure
the value of the generation probability pgen also gives a measure of the abstractiveness of our model
during training pgen starts with a value of about
then increases converging to about
by the end of training
this indicates that the model rst learns to mostly copy then learns to generate about half the time
however at test time pgen is heavily skewed towards copying with a mean value of

the disparity is likely due to the fact that during training the model ceives word by word supervision in the form of the reference summary but at test time it does not
nonetheless the generator module is ful even when the model is copying
we nd that pgen is highest at times of uncertainty such as the beginning of sentences the join between stitched together fragments and when producing periods that truncate a copied sentence
our ture model allows the network to copy while multaneously consulting the language model abling operations like stitching and truncation to in any case be performed with grammaticality
encouraging the pointer generator model to write more abstractively while retaining the accuracy advantages of the pointer module is an exciting direction for future work
conclusion in this work we presented a hybrid generator architecture with coverage and showed that it reduces inaccuracies and repetition
we plied our model to a new and challenging text dataset and signicantly outperformed the abstractive state of the art result
our model hibits many abstractive abilities but attaining higher levels of abstraction remains an open search question
acknowledgment we thank the acl reviewers for their helpful ments
this work was begun while the rst author was an intern at google brain and continued at stanford
stanford university gratefully edges the support of the darpa deft program afrl contract no

any ions in this material are those of the authors alone
references dzmitry bahdanau kyunghyun cho and yoshua gio

neural machine translation by jointly learning to align and translate
in international ference on learning representations
qian chen xiaodan zhu zhenhua ling si wei and hui jiang

distraction based neural networks in international joint for modeling documents
conference on articial intelligence
jackie chi kit cheung and gerald penn

pervised sentence enhancement for automatic marization
in empirical methods in natural guage processing
sumit chopra michael auli and alexander m rush

abstractive sentence summarization with in north tentive recurrent neural networks
ican chapter of the association for computational linguistics
michael denkowski and alon lavie

meteor universal language specic translation evaluation in eacl workshop for any target language
on statistical machine translation
john duchi elad hazan and yoram singer

adaptive subgradient methods for online learning journal of machine and stochastic optimization
learning research
jiatao gu zhengdong lu hang li and victor ok incorporating copying mechanism in in association for li

sequence to sequence learning
computational linguistics
chin yew lin

rouge a package for matic evaluation of summaries
in text tion branches out acl workshop
stephen merity caiming xiong james bradbury and pointer sentinel mixture richard socher

in nips workshop on multi class models
and multi label learning in extremely large label spaces
haitao mi baskaran sankaran zhiguo wang and abe ittycheriah

coverage embedding models for neural machine translation
in empirical methods in natural language processing
yishu miao and phil blunsom

language as a latent variable discrete generative models for tence compression
in empirical methods in ral language processing
ramesh nallapati feifei zhai and bowen zhou

summarunner a recurrent neural network based sequence model for extractive summarization of documents
in association for the advancement of articial intelligence
ramesh nallapati bowen zhou cicero dos santos c aglar gulcehre and bing xiang

tive text summarization using sequence to sequence rnns and beyond
in computational natural guage learning
chris d paice

constructing literature abstracts by computer techniques and prospects
information processing management
caglar gulcehre sungjin ahn ramesh nallapati bowen zhou and yoshua bengio

pointing in association for the unknown words
tional linguistics
marcaurelio ranzato sumit chopra michael auli and wojciech zaremba

sequence level ing with recurrent neural networks
in international conference on learning representations
karl moritz hermann tomas kocisky edward grefenstette lasse espeholt will kay mustafa leyman and phil blunsom

teaching chines to read and comprehend
in neural tion processing systems
hongyan jing

sentence reduction for automatic in applied natural language text summarization
processing
philipp koehn

statistical machine translation
cambridge university press
julian kupiec jan pedersen and francine chen

a trainable document summarizer
in international acm sigir conference on research and ment in information retrieval
chin yew lin

looking for a few good metrics automatic summarization evaluation how in nacsis nii test many samples are enough collection for information retrieval ntcir shop
alexander m rush sumit chopra and jason weston

a neural attention model for abstractive tence summarization
in empirical methods in ural language processing
horacio saggion and thierry poibeau

matic text summarization past present and future
in multi source multilingual information tion and summarization springer pages
baskaran sankaran haitao mi yaser al onaizan and abe ittycheriah

temporal attention model arxiv preprint for neural machine translation


ilya sutskever oriol vinyals and quoc v le

sequence to sequence learning with neural works
in neural information processing systems
jun suzuki and masaaki nagata

rnn based encoder decoder approach with word frequency timation
arxiv preprint

sho takase jun suzuki naoaki okazaki tsutomu rao and masaaki nagata

neural headline generation on abstract meaning representation
in empirical methods in natural language ing
zhaopeng tu zhengdong lu yang liu xiaohua liu and hang li

modeling coverage for neural in association for machine translation
tional linguistics
oriol vinyals meire fortunato and navdeep jaitly

pointer networks
in neural information cessing systems
kelvin xu jimmy ba ryan kiros kyunghyun cho aaron c courville ruslan salakhutdinov richard s zemel and yoshua bengio

show attend and tell neural image caption generation with visual attention
in international conference on machine learning
wenyuan zeng wenjie luo sanja fidler and raquel efcient summarization with arxiv preprint urtasun

read again and copy mechanism


supplementary material this appendix provides examples from the test set with side by side comparisons of the reference maries and the summaries produced by our models
in each example italics denote out of vocabulary words red denotes factual errors in the summaries green shading intensity represents the value of the generation probability pgen yellow shading intensity represents nal value of the coverage vector at the end of nal model s summarization process
article truncated andy murray came close to giving himself some extra preparation time for his wedding next week before ensuring that he still has unnished tennis business to attend to
the world no is into the of the miami open but not before getting a scare from year old austrian dominic thiem who pushed him to in the second set before going down in an hour and three quarters
murray was awaiting the winner from the last eight match between tomas berdych and argentina s juan monaco
prior to this tournament thiem lost in the second round of a challenger event to soon to be new brit aljaz bedene
andy murray pumps his rst after defeating dominic thiem to reach the miami open semi nals
muray throws his sweatband into the crowd after completing a victory in orida
murray shakes hands with thiem who he described as a strong guy after the game
and murray has a fairly simple message for any of his fellow british tennis players who might be agitated about his imminent arrival into the home ranks do nt complain
instead the british no believes his colleagues should use the assimilation of the world number originally from slovenia as motivation to better themselves
reference summary british no defeated dominic thiem in miami open quarter nals
andy murray celebrated his career win in the previous round
third seed will play the winner of tomas berdych and juan monaco in the semi nals of the atp masters event in key biscayne
baseline andy murray beat unk bedene in an hour and three quarters
british no believes his colleagues should use the maze of the world number originally from slovenia as motivation to better themselves
pointer generator no coverage andy murray came close to giving himself some extra preparation for his wedding next week before getting a scare from year old austrian dominic thiem
the world no is into the of the miami open but not before getting a scare from year old austrian dominic thiem
pointer generator with coverage andy murray defeated dominic thiem in an hour and three quarters
murray was awaiting the winner from the last eight match between tomas berdych and argentina s juan monaco
prior to this tournament thiem lost in the second round of a challenger event to soon to be new brit aljaz bedene
figure the baseline model reports the wrong score substitutes bedene for thiem and gles with the uncommon word assimilation
the pointer network models accurately reproduce the of vocabulary words thiem and aljaz
note that the nal model produces the novel word defeated to incorporate several fragments into a single sentence
article truncated munster have signed new zealand international francis saili on a two year deal
utility back saili who made his all blacks debut against argentina in will move to the province later this year after the completion of his contractual commitments
the year old currently plays for auckland based super rugby side the blues and was part of the new zealand side that won the junior world championship in italy in
saili signature is something of a coup for munster and head coach anthony foley believes he will be a great addition to their backline
francis saili has signed a two year deal to join munster and will link up with them later this year
we are really pleased that francis has committed his future to the province foley told munster s ofcial website
he is a talented centre with an impressive skill set and he possesses the physical attributes to excel in the northern hemisphere
i believe he will be a great addition to our backline and we look forward to welcoming him to munster
saili has been capped twice by new zealand and was part of the under side that won the junior championship in
reference summary utility back francis saili will join up with munster later this year
the new zealand international has signed a two year contract
saili made his debut for the all blacks against argentina in
baseline dutch international francis unk has signed a two year deal to join irish unk super rugby side the blues
unk s signature is something of a coup for munster and his head coach anthony foley believes he will be a great addition to their respective prospects
unk has been capped twice by new zealand
pointer generator no coverage new zealand international francis saili will move to the province later this year
utility back saili made his all blacks debut against argentina in
utility back saili will move to the province later this year
pointer generator with coverage francis saili has signed a two year deal to join munster later this year
the year old was part of the new zealand side that won the junior world championship in italy in
saili s signature is something of a coup for munster and head coach anthony foley
figure the baseline model incorrectly substitutes dutch for new zealand perhaps reecting the pean bias of the dataset fabricates irish and struggles with out of vocabulary words saili and based
though it is not clear why the phrase addition to our backline is changed to the nonsensical addition to their respective prospects
the pointer generator model xes these accuracy problems and the addition of coverage xes the repetition problem
note that the nal model skips over large passages of text to produce shorter sentences
article truncated right from the moment he breezed through the doors at old trafford louis van gaal was a man with a plan
the rst season he stated would see him deliver manchester united back into their rightful place in the champions league
he would restore them to the premier league top four but loftier aims of silverware would have to wait
his three year vision would allow for such thoughts but rst things rst united needed to be dining from european football s top table again
louis van gaal is close to delivering his rst season aim of returning man united into champions league
wayne rooney smashes home during manchester united win over aston villa on saturday
united win over aston villa took them third eight points ahead of fth placed liverpool in the table
april manchester city h
april chelsea a
april everton a
may west bromwich albion h
may crystal palace a
may arsenal h
may hull city a
one season out of the champions league was far from ideal but two seasons would be an absolute disaster and something he understood that would not be tolerated
in november even that was looking optimistic
a defeat to manchester city meant that united had taken just points from their opening matches it was their worst start to a league campaign since when ron atkinson only lasted until november
reference summary man united have an eight point cushion from fth place liverpool
van gaal looks likely to deliver on his promise of top four nish
but the dutchman has a three year vision mapped out
next season will have to see united mount sustained challenge for title
they must also reach the later stages of the champions league
baseline manchester united beat aston villa at old trafford on saturday
louis van gaal is close to delivering his unk aim of returning man united into the premier league top four
louis van gaal is close to delivering his unk aim of returning man united into champions league
pointer generator no coverage louis van gaal is close to delivering his rst season aim of returning man united into champions league
united win over aston villa took them third eight points ahead of fth placed liverpool in the table
louis van gaal is close to delivering his rst season aim of returning man united into champions league
pointer generator with coverage manchester united beat aston villa at old trafford on saturday
louis van gaal is close to delivering his rst season aim of returning man united into champions league
united needed to be dining from european football s top table again
figure in this example both our baseline model and nal model produce a completely abstractive rst sentence using a novel word beat
article truncated having been on the receiving end of a thumping a defeat like that could be justiably met with a backlash by angry supporters
watching a rst leg aggregate advantage turn into a decit come the end of the reverse encounter too could send many fans apoplectic at the capitulation of their side
however that does nt appear the case for those devoted to porto
porto supporters gave their team a hero s welcome following their defeat at bayern munich on tuesday
porto star striker jackson martinez was one of many players to look perplexed by their warm reception
porto boss julen lopetegui left was hugged by fans congratulating him on their champions league run
police escorts were needed to keep the delirious supporters at bay as the porto team bus drove past
the team bus was met with a cacophony of noise from porto supporters proudly chanting about their club
on their return from a humiliating champions league loss at the hands of bayern munich on tuesday night the squad were given a heroes reception as they arrived back in portugal
in the early hours of wednesday morning fans mobbed the squad congratulating them on their run in the tournament
star striker jackson martinez and ricardo quaresma were one of many porto players who looked perplexed as they were hugged by fans before they making their way on to the team bus set upon a cacophony of ercely proud chanting
it was the rst time that porto who had been unbeaten in this season s tournament up until tuesday night had reached the of the champions league since the season
reference summary bayern munich beat porto in their champions league tie on tuesday
result saw bayern win encounter on aggregate
it was the rst time porto had reached that stage since the season
baseline porto beat bayern munich in the champions league on tuesday night
porto star james unk was one of many players involved in the match
the squad were given a trophy as they arrived back in portugal
pointer generator no coverage porto star striker jackson martinez was one of many players to look perplexed by their warm reception
porto star striker jackson martinez was one of many players to look perplexed by their warm reception
porto star striker jackson martinez was one of many players to look perplexed by their warm reception
pointer generator with coverage porto star striker jackson martinez was one of many players to look perplexed by their warm porto boss julen lopetegui was hugged by fans congratulating him on their champions league reception
run
however that does nt appear the case for those devoted to porto
police escorts were needed to keep the delirious supporters at bay
figure the baseline model makes several factual inaccuracies it claims porto beat bayern munich not vice versa the score is changed from to jackson is changed to james and a heroes reception is replaced with a trophy
our nal model produces sentences that are individually accurate but they do not make sense as a whole
note that the nal model omits the parenthesized phrase left from its second sentence
article cnn it s showtime michael keaton paid homage ever so slightly to his roles in beetlejuice and batman in his third turn hosting saturday night live this weekend
keaton acknowledged in his opening monologue that a lot has changed since he rst hosted the comedy sketch show in
i had a baby he s
i also have a new girlfriend she s he said
fans who were hoping for a full blown revival of keaton s most memorable characters might have been a little disappointed
snl cast members taran killam and bobby moynihan begged the actor with a song to play batman and beetlejuice with them
all they got in response were a couple of one liners
overall keaton s performance drew high marks from viewers and critics for its deadpan manner and unpredictable weirdness in the words of a

club s dennis perkins
fans also delighted in a cameo from walking dead star norman reedus during weekend update
keaton scored some laughs from the audience as an ad executive who s not very good at his job a confused grandfather and a high school teacher who gets asked to the prom in a riff on the romantic comedy she s all that
other crowd pleasing spots included a scientology parody music video and a news conference spoong the ncaa student athlete debate
the show also poked fun at cnn with cast member cecily strong playing anchor brooke baldwin
baldwin said on twitter that she s taking it as a crazy compliment and shared a clip from the skit
reference summary michael keaton hosted saturday night live for the rst time in
in his nods to starring roles in beetlejuice and batman are brief
baseline snl wins saturday night live
keaton acknowledged in his opening saturday night live
keaton acknowledged in his opening night s unk
the show also pokes fun at cnn with cast member mariah carey
pointer generator no coverage beetlejuice michael keaton paid homage to his roles in beetlejuice and batman in his third turn hosting saturday night live this weekend
keaton acknowledged in his opening monologue that a lot has changed since he rst hosted the comedy sketch show in
pointer generator with coverage michael keaton paid homage to his roles in beetlejuice and batman in his third turn hosting saturday night live
comedy sketch show in
keaton acknowledged in his opening monologue that a lot has changed since he rst hosted the figure baseline model replaces cecily strong with mariah carey and produces generally nonsensical output
the baseline model may be struggling with the out of vocabulary word beetlejuice or perhaps the unusual non news format of the article
note that the nal model omits ever so slightly from its rst sentence
article truncated they are supposed to be the dream team who can solve the conundrum of how to win the election
but david cameron and boris johnson were left scratching their heads today as they struggled with a children s jigsaw teaching toddlers about the seasons
as the london mayor tried to hammer pieces together with his hands the prime minister tried out what could be a new election slogan telling him if in doubt wedge it in
after being put right by a four year old who spotted their errors the pair had more fun nger painting with tory blue paint
david cameron and boris johnson were left stumped by the puzzle at advantage children s day nursery in surbiton as three year old stephanie looked on
when they tried to put the puzzle back together they hit trouble after it proved to be more difcult than expected
the conservative duo made their rst appearance together on the campaign trail with a visit to advantage day nursery in surbiton south west london
they were supposed to be highlighting tory plans to double free childcare for children aged three and four
they sat at a table with joshua and stephanie both aged three and four year olds hamish and leo where they broke up a puzzle which depicted the four seasons
when they tried to put it back together they hit trouble after it proved to be more difcult than expected
reference summary david cameron and boris johnson visit nursery to promote childcare
duo left ummoxed by jigsaw aimed at teaching children about seasons
london mayor resorted to brute force to hammer wrong pieces together
prime minister gave his approval telling boris if in doubt wedge it in
baseline david cameron and boris johnson were left mystied by the puzzle
they were at the nursery in unk to promote a tory pledge to double free childcare for children aged three and four
the pair were at the nursery in unk to promote a tory pledge to double free childcare for children aged three and four
pointer generator no coverage london mayor tried to hammer pieces together with his hands
they were supposed to be highlighting tory plans to double free childcare
they were supposed to be highlighting tory plans to double free childcare
they were supposed to be highlighting tory plans to double free childcare
pointer generator with coverage london mayor tried to hammer pieces together with his hands
prime minister tried out what could be a new election slogan
but they hit trouble after it proved to be more difcult than expected
figure the baseline model appropriately replaces stumped with novel word mystied
however the reference summary chooses ummoxed also novel so the choice of mystied is not rewarded by the rouge metric
the baseline model also incorrectly substitutes for
in the nal model s output we observe that the generation probability is largest at the beginning of sentences especially the rst verb and on periods
article truncated muhammadu buhari lagos nigeria cnn a day after winning nigeria s presidency that he plans to aggressively ght corruption that has long plagued nigeria and go after the root of the nation s unrest
buhari told cnn s christiane amanpour said he ll rapidly give attention to curbing violence in the northeast part of nigeria where the terrorist group boko haram operates
by cooperating with neighboring nations chad cameroon and niger he said his administration is condent it will be able to thwart criminals and others contributing to nigeria s instability
for the rst time in nigeria s history the opposition defeated the ruling party in democratic elections
buhari defeated incumbent goodluck jonathan by about million votes according to nigeria s independent national electoral commission
the win comes after a long history of military rule coups and botched attempts at democracy in africa s most populous nation
reference summary muhammadu buhari tells cnn s christiane amanpour that he will ght corruption in nigeria
nigeria is the most populous country in africa and is grappling with violent boko haram extremists
nigeria is also africa s biggest economy but up to of nigerians live on less than a dollar a day
baseline unk unk says his administration is condent it will be able to destabilize nigeria s economy
unk says his administration is condent it will be able to thwart criminals and other nigerians
he says the country has long nigeria and nigeria s economy
pointer generator no coverage muhammadu buhari says he plans to aggressively ght corruption in the northeast part of nigeria
he says he ll rapidly give attention to curbing violence in the northeast part of nigeria
he says his administration is condent it will be able to thwart criminals
pointer generator with coverage muhammadu buhari says he plans to aggressively ght corruption that has long plagued nigeria
he says his administration is condent it will be able to thwart criminals
the win comes after a long history of military rule coups and botched attempts at democracy in africa s most populous nation
figure the baseline model incorrectly changes thwart criminals and others contributing to nigeria s instability to destabilize nigeria s economy which has a mostly opposite meaning
it also produces a nonsensical sentence
note that our nal model produces the novel word says to paraphrase told cnn s christiane amanpour
article cairo cnn at least people were killed sunday and more injured in separate attacks on a police station a checkpoint and along a highway in egypt s northern sinai authorities said
six people including one civilian were killed when a car bomb exploded near the police station in al arish capital of north sinai health ministry spokesman hossam abdel ghafar told ahram online
he said people were injured
ansar beit al maqdis an isis afliate claimed responsibility for the attack which came hours after another operation that the group also claimed
in that earlier attack a rst lieutenant a sergeant and four conscripts were killed when their armored vehicle was attacked on the highway from al arish to sheikh zuweid in northern sinai the military said
two other soldiers were injured and taken to a military hospital
ansar beit al maqdis has claimed many attacks against the army and police in sinai
a third attack sunday on a checkpoint in rafah left three security personnel injured after unknown assailants opened re at them according to state media
the attacks come as the military announced a reshufe of several senior military positions state media reported
among those being replaced are the generals in charge of military intelligence and egypt second eld army which is spearheading the battle against the insurgents in the northern sinai
egypt s army has been ghting a decade long militant islamist insurgency which has spiked since the ouster of muslim brotherhood president mohamed morsy in the summer of
hundreds of police and soldiers as well as civilians have been killed in militant attacks in the past months
ian lee reported from cairo
anas hamdan reported from atlanta
reference summary six people including one civilian are killed when a car bomb explodes near a police station
six others are killed when their armored vehicle is attacked on a highway in northern sinai
ansar beit al maqdis an isis afliate claims responsibility
baseline he says the attacks come after another operation that killed
the attacks come as military announced a u
n
peacekeeping force in northern sinai
pointer generator no coverage ansar beit al maqdis an isis afliate claimed responsibility for the attack
ansar beit al maqdis an isis afliate claimed responsibility for the attack
the attacks come as the military announced a reshufe of several senior military positions
pointer generator with coverage six people including one civilian were killed when a car bomb explodes near the police station
ansar beit al maqdis an isis afliate claimed responsibility for the attack
egypt s army has been ghting a decade long militant islamist insurgency
figure the baseline model fabricates a completely false detail about a u
n
peacekeeping force that is not mentioned in the article
this is most likely inspired by a connection between u
n
peacekeeping forces and northern sinai in the training data
the pointer generator model is more accurate correctly reporting the reshufe of several senior military positions

