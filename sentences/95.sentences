a semantic qa based approach for text summarization evaluation ping chen fei wu tong wang wei ding university of massachusetts boston ping

edu abstract many natural language processing and computational guistics applications involve the generation of new texts based on some existing texts such as summarization text simplification and machine translation
however there has been a serious problem haunting these applications for ades that is how to automatically and accurately assess ity of these applications
in this paper we will present some preliminary results on one especially useful and challenging problem in nlp system evaluation how to pinpoint content differences of two text passages especially for large passages such as articles and books
our idea is intuitive and very ferent from existing approaches
we treat one text passage as a small knowledge base and ask it a large number of tions to exhaustively identify all content points in it
by paring the correctly answered questions from two text sages we will be able to compare their content precisely
the experiment using duc summarization clearly shows promising results
introduction technologies spawned from natural language processing nlp and computational linguistics cl have tally changed how we process share and access mation e

search engines and questions answering tems
however there has been a serious problem haunting many nlp applications that is how to automatically and accurately assess the quality of these applications
in some case evaluation of a nlp task itself has become an active research area itself such as text summarization evaluation
the main difficulty for developing such evaluation comes from the diversity of the nlp domain and our insufficient understanding of natural languages and human intelligence in general
in this paper we focus on one especially useful and challenging area in nlp evaluation how to cally compare the content of two text passages e

graphs articles or even large corpora
pinpointing content differences among texts is critical to evaluation of many portant nlp applications such as summarization text gorization text simplification and machine translation
not surprisingly many evaluation methods have been proposed but the quality of existing methods themselves is hard to sess
in many cases human evaluation must be adopted which is often slow subjective and expensive
in this paper we present an intuitive and innovative idea completely ferent from existing methods if we treat one text passage as a small knowledge base can we ask it a large number of questions to exhaustively identify all content points in it by comparing the correctly answered questions from two text passages we can compare their content precisely
this idea may seem confusing as circling around the target stead of directly hitting the target
however our question answering content evaluation is intuitive and supported by the following insights when we assess someone s understanding on a subject we do not ask him to write down all he knows about the subject
instead a list of questions will be asked and rate and objective assessment can be achieved by counting the number of correct answers
during this question ing process we can also identify which areas he needs to improve
practical operability
when assessing the similarity of two texts direct comparison may look natural
however with current methods no matter supervised or rule based this direct approach becomes increasingly difficult as we move to larger text passages
for example comparing two articles needs to answer the following questions how to align sentences how to semantically represent a sentence how to generate similarity scores without annotated samples or as few as possible to minimize cost how to interpret and evaluate these scores how to find the content differences of two texts
easy to interpret
many existing methods only generate a single score which illustrates little detail as how an ment measure is generated and offers no help for system provement
on the other hand our qa based approach quires minimum manual efforts clearly shows how a ure is calculated and pinpoints exactly the content ences of two text passages
in next section we will discuss some existing work
tion will show the architecture for our qa based tion approach and experiment results will be presented in section
we will provide some insights and findings when we design our evaluation system and conduct experiments in one discussion section
we conclude in section
related work human evaluations of nlp applications are expensive and slow
a fast option is to use crowdsourcing such as amazon mechanical turk to quickly get a large amount of tion results from non expert annotators callison burch lasecki
however besides the expense there is little control over the annotation quality
automatic semantic evaluation has been studied for ades
evaluation systems like rouge lin in marization and bleu papineni in machine tion have been widely adopted as measures
yet these methods utilize only shallow features such as n grams and longest common subsequences which suffer from the inherent term specificity muhalcea
moreover they usually require gold standard simplifications generated by human annotators as reference and which are subjective expensive and not always available
to better represent mantics mikolov developed word embedding els e

to semantically encode words and phrases
more recent work moves to learn similarity of larger text pieces such as sentences le kiros
one fundamental difficulty in embedding models is their high requirement of a large number of text samples as geted text pieces get larger
secondly the quality of ity measures generated by these models is often vague and hard to assess
usually authors handpick only a few samples or extrinsic evaluation has to be adopted
for example mueller shows the following output the similarity of a boy is waving at some young ners from the ocean and a group of men is playing with a ball on the beach is
according to the lstm model and
according to a dependency tree based model
just by looking at this sentence pair it is not clear why
is a better similarity estimation than

moreover current work on text similarity measurement focuses only on generation of such a single similarity score and largely ignores a more interesting and important issue what are the exact content differences between two text passages although our evaluation method applies to any tion where semantic comparison of texts is needed our rent experiments focus on text summarization evaluation
in this section we will only discuss the existing work on marization evaluation
automatic text summarization is the process to find the most important content from a document and create a mary in natural language
how to automatically evaluate summaries remains a challenging problem jones jing steinberger
any such process must be able to comprehend the full document extract the most salient and novel facts check if all main topics are covered in the mary and evaluate the quality of the content wang
the problem of co selection measure is that it needs to count the common sentences between a machine summary and one of the human summaries which introduces a bias since they are based on a small number of assessors and a small change of sentences may affect the performance
donaway introduced content based measure comparing the term frequency tf vectors of machine summary with the tf vectors of the full text or human summary
the score is puted based on bag of words or tf idf model using sine similarity
however it is likely the summary vector is sparse compared with the document vector and a summary may use terms that are not frequently used in the full ment
an alternative is to use latent semantic indexing lsi to capture semantic topics based on singular value decomposition steinberger
unfortunately lsi is expensive to compute and suffers from the polysemy lem
louis proposed to use input summary similarity and pseudomodels to assess machine summary without a gold standard
other content based measures include common subsequence unit overlap radev amid nenkova basic elements hovy and compression dissimilarity wang
rouge recall oriented understudy for gisting ation is perhaps the most widely adopted automatic marization evaluation tool
it determines the quality of a summary by comparing it with human summaries using grams word sequences and word pairs lin
its put correlates very well with human judgements
but rouge is unsuitable to evaluate abstractive tion or summaries with a significant amount of ing
ng incorporates word embeddings learned from neural network to rouge
there have been other efforts to improve automatic summarization evaluation measures such as the automatically evaluating summaries of peers aesop task in tac
the major problem of these methods is the requirement of gold standard summaries and ally only a single score is generated which is hard to pret and does not provide any clue as how a summarization system can be improved
after examining existing nlp evaluation methods ing from shallow analysis e

n gram to deep semantics e

deep neural network word embeddings there still main many major challenges high requirement of manual efforts in general supervised machine learning methods e

deep learning or other classification methods need sufficient annotated samples for robust performance which become prohibitively expensive when evaluation moves from sentences to large text passages
even with methods using only shallow features e

rogue gold standard needs to be provided which is often created by highly trained personnel
lack of details in evaluation most existing methods produce only a single score as the evaluation result
in evaluation more information is certainly desirable and can significantly help researchers gain more insights and improve their work
for example when simplifying a text passage it will be very helpful to pinpoint information differences between simplified text and original text so we will know whether what information is missing from the simplified version
in this paper we will present a question answering based content evaluation method that can identify information ferences of different text passages without any manual forts
our method can process various text sizes ranging from a sentence a paragraph a document to even a large corpus
due to its fundamental nature our work can be plied anywhere comparison of two texts is required ing summarization evaluation text simplification tion and machine translation evaluation
a qa based method for semantic comparison of texts our automated evaluation method will leverage two nlp fields question generation qg and question answering qa
its architecture is illustrated in figure
the main idea is first to generate a large number of questions from an original text passage to exhaustively cover its content and it is reasonable to assume that the original text contains formation to answer these questions
to semantically assess the content of a newly generated text passage e

a mary a simplified version or a translation to another guage a qa system will use the new passage as the only knowledge source to answer the questions generated from the original text
if a question is correctly answered it means that this new text passage contains the same specific piece of information as in the original text although it may be pressed in a different way
by examining all correct swers we can have an accurate measure of information tained in the new passage
by comparing the questions that can be answered by original text passage but can not be swered by new passage we can pinpoint exactly the content differences between these two text passages
question generation qg has been widely used in many fields
in a document retrieval system a qg system can be used to construct well formed questions hasan
rent qg methods are designed to generate questions with some focus e

a query in an ir system main topic in a figure our qa based semantic evaluation system architecture figure customizing a typical qa system for our evaluation approach tutoring system le
usually questions are formed by exploiting named entity information and predicate ment structures of sentences
then a qg system ranks the questions in two aspects
one is the question s relevance to the topic and the subtopics of the original passage and the other is the syntactic similarity of each question with the original passage
as a result the system outputs questions with high relevance to the topic of original text passage
in the qg step we generated factual questions by ing the grammatical structure labeling the lexical items with name entities or other high level semantic roles e

son location time and performing syntactic mations such as subject auxiliary inversion and wh ment heilman
we used a different ranking nent in our qg system so that we could generate not only the questions that closely relate to the topic but also the questions covering even minor content points
such tions are highly related to the original text
on the other hand these questions might not exclusively cover all the literal information or always be well structured due to the difficulty in extracting simplified statements from complicated structures in the original text
hence using named entities and predefined templates to generate tions can be alternative way in this qg step
we can first apply a named entity recognition method to identify named entities e

person name time from a text passage
for each identified named entity we will generate a set of questions according to predefined question templates
for example there is one text passage born in hodgenville kentucky lincoln grew up on the western frontier in kentucky and indiana if lincoln is recognized as a person questions will be tomatically generated e

who is lincoln when was lincoln born where was lincoln born when did lincoln die where did lincoln die these questions are generated without considering cific text passages so it is possible some answers can not be found in the original text passage
in this case all questions are still asked to an original text passage and to a generated e

summarized simplified translated text passage
the difference of the two answer sets will show the content ference of two texts
the advantage of this approach is that we do not have to always rely on the quality of questions from a qg system
as long as predefined question template is carefully constructed we can obtain questions with good coverage over coverage does not matter and high quality
after a large set of questions is generated from original text we need a qa system to check how many questions can be correctly answered using the content from a single text
a typical qa system usually includes an figure the qa based summarization evaluation process using duc corpus retrieval component to return a large set of ranked ments that may contain the answer
figure shows the chitecture of a customized qa system that will satisfy the needs of this evaluation project
after the question cessing step our qa system component does nt pass lated queries to a passage retrieval component
instead it uses the queries to search for relevant sentences within a document from which the system will extract answers
the change in structure increases difficulties in the question cessing step and answer processing step of the qa system
experiment to test our idea we have built a proof of concept system using some existing qg and qa systems
for the question generation component we adapted heilman m
qg system
for the qa component we need our qa system component to be able to answer questions from a single ument instead of using an information retrieval system to return a large set of ranked documents that may contain the answer
we take advantage of the open source qa work openephyra by replacing the passage retrieval ponent with a text searching component which searches within one document
to test our prototype system we use the corpus from ument understanding conference duc
the corpus contains sets of text passages
the first set is the original documents divided into topics
each topic consists of original documents
the second set of texts is the summaries of each topic
the summaries were generated by baseline summarization systems participating summarization systems and human summarizers
thus in total there are summaries for each topic
all of these summaries have been evaluated by human assessors and have been given scores on their content responsiveness and linguistic quality
we hypothesized that the content quality of a summary can be measured by the number of questions answered by the qa system given this summary as the only knowledge
the whole process of our experiment is shown in figure
figure comparison of our evaluation scores and human evaluation scores in the question generation phase we use all the original documents of each topic as input to the qg component
as output the qg component generates a large set of questions for each topic
the number of generated questions ranges from a few hundred to a few thousand and varies from topic to topic depending on the document length
all questions were limited to wh factoid questions that are shorter than a certain threshold
in the qa phase we run the qa nent through each generated summarization from each topic
for summarizations in each topic we use the set of tions corresponding to this topic as input to the qa system
the goal of our automatic evaluation system is to determine the performance of different automatic summarization tems based on the content quality of the summarizations generated by them
in this experiment due to time straints we chose to compare the performance of marization systems with system id and
these systems are evenly distributed mance wise as evaluated by human assessors
to make sure the answers generated by the qa system are mostly correct we set the confidence score of the answers to a very high value
if the qa system is not highly positive about the swer to a question it will not answer that question
as stated above for each topic we have a set of questions generated by qg component and these questions are used to evaluate summarization systems performance over this topic
each summarization system s overall performance is measured by averaging its performance over the topics
more specifically to evaluate summarization systems performance over a certain topic we ran the qa system times on this topic
each time as input to the qa system we use the same set of questions generated from this topic s original documents but as knowledge source we use ent summaries generated by different summarization tems
within each run to measure the performance of a summarization system we calculate the percentage of swered questions among the total questions for that topic given the this summarization system s summary as knowledge source
the percentages are later normalized to the range of matching the scores given by the human assessors
finally we average the scores of each zation system over the number of topics
these average scores are the output of our automatic evaluation system which is the performance measure of each summarization system
in the last step we compare the content scores given by human assessors and our system s output score by puting pearson s correlation between automatic evaluation system scores and human assigned mean content scores
as shown in figure our performance scores and human score correlate very well
to further evaluate the robustness of our approach we ied the parameters used in our system to check how these changes affect the system performance
the confidence threshold was set to a very high value
or
as shown in table to ensure correctness of generated answers and to minimize possibility of generating false answers
as shown in table in general our scores and human scores not able to find relations between entities
however our current qg system may fail to generate deep and ligent inference questions and discover long distance dependencies
our method currently focuses more on how much mation a summary contains rather than the importance of certain information within the original text by using topic words as a separate filter
instead if integrating topic words into our qg system we can provide more personalized evaluation as certain information may be more important to a specific user
the core focus of our idea is on how to semantically compare two texts which can be a summary and a ument a text and its simplified revised version a text and its translation in another language
hence our approach can have broad applications in other nlp tasks such as text simplification and machine tion where evaluation is also very important and lenging
although end to end deep neural network methods in nlp become popular due to their good performance our white box style approach has its unique appealing advantages in the evaluation field since humans are ten closely involved in this process and need assess the soundness of evaluation and find clues to improve their nlp system
conclusion in this paper we present an innovative semantic evaluation method for various nlp applications by leveraging tion generation and question answering fields
our method requires no manual efforts is easy to interpret and trates details about nlp systems being evaluated
our periments on text summarization evaluation showed ising results
since our focus is on a fundamental issue in nlp how to semantically compare two texts besides marization evaluation we expect that our idea will have broader applications on various nlp tasks such as text plification and machine translation
acknowledgements this work is partially funded by nsf grant
correlate very well
the longer questions can generally more difficult for a qa system and sometimes results in lower performance which is why we limited the question length
since summarization tends to keep most important mation not all questions are of the same importance
tions covering important information should be given higher priority
by question filtering we applied lda to identify topic words of documents and filtered out the questions that do not contain these topic words
performance gets better since a summarization system will not be penalized by not covering unimportant information
confidence question correlation length question filtering







no no yes yes no no yes yes









rouge rouge table experiment results discussion in this section we will provide some insights and findings as we design our qa based semantic evaluation system and analyze the experiment results
we have manually examined some questions that we generated and found that they are highly related to the text both semantically and structurally
the over erating approach helps us to obtain questions that can cover almost all the text content and literal information
the average number of questions generated for a topic with documents is when setting the question length to be less than words
the large amount of generated questions ensures our system s robustness when a small number of false answers exist
in addition the generated questions are able to discover basic entity relations within sentences and between tences
for example there are questions like who were assaulted by aryan nations guards who does richard cohen argue to who was co founder of the southern poverty law center what was morris dees the co founder of this fact also suggests the proposed qa based evaluation approach is potentially superior to rouge based measures since rouge is jun ping ng and viktoria abrecht
better summarization tion with word rouge
arxiv preprint

embeddings for dragomir radev simone teufel horacio saggion wai lam john blitzer arda celebi hong qi elliott drabek and danyu liu
evaluation of text summarization in a cross lingual information trieval framework
center for language and speech processing johns hopkins university baltimore md tech
rep
kishore papineni salim roukos todd ward and wei jing zhu
bleu a method for automatic evaluation of machine translation
in proceedings of the annual meeting on association for putational linguistics pp

association for computational linguistics
josef steinberger karel jeek

evaluation measures for text summarization
in computing and informatics vol
pp tong wang ping chen and dan simovici
a new evaluation measure using compression dissimilarity on text summarization
applied intelligence
references chris callison burch
fast cheap and creative evaluating lation quality using amazon mechanical turk
in proceedings of the conference on empirical methods in natural language processing volume volume pp

association for computational linguistics
robert l
donaway kevin w
drummey and laura a
mather
a comparison of rankings produced by summarization evaluation measures
in proceedings of the naacl anlpworkshop on automatic summarization volume pp

association for computational linguistics
hasan yllias chali sadid a
towards automatic topical question generation
proceedings of coling heilman michael
automatic factual question generation from text
diss
carnegie mellon university
eduard hovy chin yew lin liang zhou and junichi fukumoto
automated summarization evaluation with basic elements
in ceedings of the fifth conference on language resources and uation lrec pp

genoa italy
hongyan jing regina barzilay kathleen mckeown and michael elhadad
summarization evaluation methods experiments and analysis
in aaai symposium on intelligent summarization pp


ks jones and julia r
galliers
evaluating natural language cessing systems an analysis and review
vol

springer ence business media
kiros r
zhu y
salakhutdinov r
zemel r
s
torralba a
urtasun r
and fidler s

skip thought vectors
nips
walter s
lasecki luz rello and jeffrey p
bigham
measuring text simplification with the crowd
in proceedings of the web for all conference p

acm
le q
and mikolov t
distributed representations of sentences and documents
icml
chin yew lin
rouge a package for automatic evaluation of maries
in text summarization branches out proceedings of the workshop vol


annie louis and ani nenkova
automatically assessing machine summary content without a gold standard
computational tics

mikolov t
sutskever i
chen k
corrado g
and dean j

distributed representations of words and phrases and their compositionality
nips
mihalcea r
corley c
and strapparava c

corpusbased and knowledge based measures of text semantic similarity
aaai conference on artificial intelligence
mueller j and thyagarajan a
siamese recurrent architectures for learning sentence similarity
proceedings of the aaai conference on artificial intelligence aaai ani nenkova and rebecca j
passonneau
evaluating content lection in summarization the pyramid method
in hlt naacl vol
pp



