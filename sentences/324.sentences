noisy self knowledge distillation for text summarization yang liu sheng shen and mirella lapata university of edinburgh yang

ac
uk
ed
ac
uk university of california berkeley sheng

edu p e s l c
s c v
v i x r a abstract in this paper we apply self knowledge tion to text summarization which we argue can alleviate problems with maximum likelihood training on single reference and noisy datasets
instead of relying on one hot annotation labels our student summarization model is trained with guidance from a teacher which generates smoothed labels to help regularize training
furthermore to better model uncertainty ing training we introduce multiple noise nals for both teacher and student models
we demonstrate experimentally on three marks that our framework boosts the mance of both pretrained and non pretrained summarizers achieving state of the art results
introduction automatic summarization has enjoyed renewed thanks to the interest in recent years ity of neural network models and their ability to learn continuous representations without recourse to preprocessing tools or linguistic annotations
the availability of large scale datasets sandhaus hermann et al
grusky et al
narayan et al
containing hundreds of sands of document summary pairs has driven the development of neural architectures for rization
several approaches have been proposed in the vast majority sequence to sequence models which are trained in an end to end fashion with a maximum likelihood estimation loss see et al
celikyilmaz et al
paulus et al
gehrmann et al

despite promising results the are specic acteristics of the summarization task which der it ill suited to standard sequence to sequence training
for instance maximum likelihood ing on single reference datasets might not be timal for summarization which is subject to a great deal of human variation harman and over in the context of nenkova
tive summarization different people select ent sentences to include in a summary rath et al
and when writing abstracts disagreement exists both in terms of writing style and the specic content deemed important for the mary harman and over
although marization models would naturally benet from multiple target references it is unrealistic to pect that multi reference summarization datasets can be created at scale for neural network ing
in fact most popular benchmarks are collated opportunistically based on summaries which only loosely correspond to the source input
for example narayan et al
create a dataset by pairing the rst sentence of a news ticle with the rest of the document under the sumption that the introductory sentence expresses the gist of the article
grusky et al
pair ticles with metadata available in html pages der the assumption that html tags e

tion summary like content
in other work liu et al
perez beltrachini et al
multidocument summarization datasets are ated by viewing lead sections in wikipedia articles as summaries of documents cited therein
the herent noise in the data collection process further hampers training with models often being prone to hallucination song et al
maynez et al
and struggling to identify which content units are salient tan et al

in this paper we propose to alleviate these problems by turning to knowledge tion bucilu et al
ba and caruana hinton et al
kim and rush
edge distillation transfers knowledge from a larger teacher network to a smaller student model by training the student to imitate the teacher s outputs in addition to learning from the training data set
in born again networks furlanello et al
the teacher and student have the same neural chitecture and model size and yet surprisingly the student is able to surpass the teacher s accuracy
intuitively self knowledge distillation is effective because the teacher s output distribution provides a richer training signal capturing additional mation about training examples
in the context of summarization the teacher can benet student training in two ways
it provides a softened tribution over reference summaries thereby ing the single reference setting
moreover the teacher s distribution is to a certain extent noised enabling the student to circumvent curacies in the training data
we further talize on the idea that both the teacher and the student should be robust to noise and introduce several noise injection techniques which together with knowledge distillation improve model alization and performance
covering we present experiments on several narayan et al
marization benchmarks hermann et al
perez beltrachini et al
and multi document summarization settings as well as different types of summaries e

verbose or more telegraphic
across datasets we the proposed framework boosts the performance of pretrained and pretrained abstractive summarizers achieving new state of the art results
background
neural abstractive summarization neural approaches to abstractive summarization conceptualize the task as a sequence to sequence problem where the encoder maps the sequence of tokens in the source document


xn to a sequence of continuous representations


and the decoder autoregressively erates the target summary my


ym token by token hence modeling the conditional probability





xn
rush et al
and nallapati et al
were among the rst to apply the neural encoder decoder architecture to text tion
see et al
enhance this model with a pointer generator network which allows to copy words from the source text and a coverage anism which keeps track of words that have been summarized
other work develops tive models trained end to end with reinforcement learning based on multiple encoders and cal attention celikyilmaz et al
or a erage mechanism where the decoder attends over previously generated words paulus et al

gehrmann et al
follow a bottom up proach where a content selector rst determines which phrases in a source document should be part of the summary and a copy mechanism is applied only to preselected phrases during decoding
though the majority of summarization systems are composed of lstm units narayan et al
and perez beltrachini et al
propose stractive models based on convolutional neural works
in gains abstractive pretrained language models have recently emerged as a key technology for achieving impressive tion liu and lapata lewis et al
song et al

these models rst pretrain a language model with self supervised tives on large corpora and then ne tune it liu and lapata on summarization datasets
combine a pretrained encoder based on bert devlin et al
with a randomly demonstrating substantial initialized decoder gains on summarization performance
song et al
pretrain an encoder decoder framework fragments within a to reconstruct sentence and then ne tune it on summarization datasets
in the same vein lewis et al
present bart an encoder decoder transformer vaswani et al
pretrained by ing a text corrupted with several arbitrary noising functions
bao et al
design a transformer based neural network pretrained as a pseudo masked language model
masked
knowledge distillation knowledge distillation refers to a class of ods for training a new smaller student network by learning from a teacher network in addition to learning from the training data
it is generally sumed that the teacher has been previously trained and the parameters for the student are estimated by matching the student s predictions to the teacher
let t and s denote teacher and student models respectively
also let ft and fs be functions of the teacher and student
the models are typically neural networks and function can be in ple dened using the output of any network layer e

a hidden or softmax layer
knowledge tillation methods are commonly expressed as imizing an objective function over training set x lkd x xix xi where l is a loss function that penalizes the ference between the teacher and the student
logits based hidden ba and caruana output on representations specic instantiations of this general work include minimizing the teacher student difference mediate attention maps and derivatives of the loss to the romero et al
put zagoruyko and komodakis czarnecki et al

other work integrates an ensemble of teachers in order to improve the student urban et al
trains a succession of students furlanello et al
introduces a teacher assistant for better knowledge transfer mirzadeh et al
and regularizes parisotto et al
teh et al
task agents in reinforcement learning
compared to direct training knowledge distillation provides a more stable training process which leads to better performing student models hinton et al
phuong and lampert
recent work furlanello et al
hahn and choi also sheds light on leveraging knowledge distillation for training a high performing student model with the same size as the teacher see the discussion in the next section
knowledge distillation has been also shown to improve results for various nlp tasks
tan et al
use it to transfer knowledge from bert to smaller models helping them approach or the quality of much larger pretrained neural networks
aside from distilling large models into smaller ones kim and rush mou et al
or ensembles of models into single els kuncoro et al
liu et al
edge distillation has been further used in task learning e

to teach a multi task student from single task teachers clark et al

self knowledge distillation for text summarization self knowledge distillation refers to the cial case where the teacher and student have identical neural network architectures
prisingly perhaps it has been consistently served furlanello et al
yang et al
ahn et al
that students trained with knowledge distillation outperform their teachers by signicant margins in several computer vision and language modeling tasks
recent efforts have also focused on understanding why this happens e

by observing that knowledge transferred by the teacher is localized mainly in higher layers and does not affect early feature extraction ers much gotmare et al
by interpreting the teacher s knowledge as importance weighting furlanello et al
by showing that stopping is crucial dong et al
and by studying how self distillation modies tion mobahi et al

for text summarization we argue that knowledge distillation can potentially alleviate problems in conventional maximum likelihood summarization models are typically training
trained on single reference document summary pairs however considering a single summary as the only correct reference during maximum likelihood training can harm model ization elbayad et al
and is there can be multiple valid intuitive
harman and over maries for a source input nenkova and even the single ence summaries available are not entirely dard due to the inherent noise in the automatic construction of large scale summarization datasets kryscinski et al

with self knowledge tillation teacher outputs provide softened tions of the reference summaries which can be viewed as an enrichment of the single reference setting and a reweighting of gold summaries to vent the student from becoming over condent in its predictions
the standard objective for an abstractive marization model is negative log likelihood lnll t x where indicates the source document yt cates the t token in the target summary and are the rst t tokens in the target summary
we further assume that the teacher is a fully trained neural model the student has the same ture with the teacher and access to the learned teacher s output distribution pt x lkd t x where pt and are model outputs from the teacher and student spectively
where the input is usually a long document we design the following perturbation policies it is common practice to compensate for no rect access to the training data see equation by interpolating between the two losses in tions and
so the nal objective for ing the student becomes lfinal where is a mixture parameter combining the hot distribution and the teacher distribution
we further want our summarization systems to be robust to natural noise found in existing datasets
injecting noise onto training samples has been proven useful for improving model ization et al

we extend this idea for knowledge distillation and propose a novel work for introducing noise to both distillation nals and training data
we design different noise mechanisms for the teacher and student and select the best noise conguration experimentally
noisy teacher to inject noise into the lation signals we incorporate a teacher dropout mechanism et al
where dropout is kept active while generating teacher predictions for training the student
in this manner the teacher generates variable supervision labels for the dent with some degree of uncertainty alleviating the problem of overtting to the teacher tions
meanwhile it can also be considered as proximating an average ensemble from many ral networks et al

with dropout rate the knowledge distillation loss now becomes lkd t ps t x
word drop a word in the source input is moved with probability pd

word replacement for each word xi in the source input we calculate a candidate placement list by selecting k words most ilar to xi from the vocabulary
the ity is calculated as the cosine distance tween the embedding of xi and embeddings of all other words in the vocabulary
then a source word is replaced with a word domly selected from its candidate ment list with probability pr

sentence drop a sentence in the source input is removed with probability ps

gaussian noise a gaussian noise vector e is multiplied with the embeddings of input words e e n i
these perturbation policies can be applied multaneously or successively as a pipeline
we experimentally found the best combination for our task to be the sequential application of word drop followed by word replacement and sentence drop
although gaussian noise has been effective in natural language understanding tasks zhang and yang we found it not to be helfpul in our summarization experiments
the knowledge distillation loss with a student trained on noisy data becomes lkd t x t x where x indicates perturbed source input
where p teacher model with active dropout
t indicates the predictions from the experimental setup noisy student to inject noise into the ing data we propose various mechanisms to turb the source input
random perturbation is effective in enforcing local smoothness for ing text generation models under the assumption that semantically similar inputs can be mapped to the same or similar targets
a related approach has been shown to improve the performance of machine translation models in self training tings he et al

for text summarization in this section we describe the summarization datasets used in our experiments and discuss ious implementation details

summarization datasets on two evaluated our model we document summarization datasets namely the cnn dailymail news highlights hermann et al
and xsum narayan et al
and one multi document dataset i
e
wikicatsum perez beltrachini et al

summarization without pretraining lead ptrnet transformerabs skd skd noisy t skd noisy t noisy s base size pretrained models massbase bertsumabs m m m m skd skd noisy t m skd noisy t noisy s m large size pretrained models unilmlarge bartlarge m m cnn dailymail





























rl











rl











rl



xsum












rl





rl





rl
table rouge results on cnn dailymail and xsum test sets and are shorthands for unigram and bigram overlap rl is the longest common subsequence
skd refers to a system trained with self knowledge distillation noisy t are skd models trained with noisy signals while noisy s are student models trained on noisy data
results for comparison systems are taken from the authors respective papers or obtained on our data by running publicly released software
represent different these datasets summary styles ranging from highlights to very brief one sentence summaries
the summaries also vary with respect to the type of rewriting operations they exemplify e

cnn dailymail showcases more cut and paste operations while xsum is genuinely abstractive
two of these datasets xsum and wikicatsum were created assumptions automatically following various about the correspondence of purported summaries to the source input
finally cnn dailymail contains news articles and sociated highlights i
e
a few bullet points ten by journalists which give a brief overview of the article
we used the standard splits training of hermann et al
tion and testing cnn ments and dailymail uments
we did not anonymize entities
tences were split with the stanford corenlp toolkit manning et al
and the dataset was pre processed following see et al

input documents were truncated to tokens
for question what is this article about
we used the splits of narayan et al
for training tion and testing and lowed the pre processing introduced in their work
input documents were also truncated to kens
wikicatsum is a multi document tion dataset derived from wikisum liu et al

the target summary is the lead tion of a wikipedia article and the source put are webpages related to this article
catsum perez beltrachini et al
represents three domains from the original wikisum dataset under the assumption that these vary in terms of the topics the summaries discuss and their tic characteristics
aside from the summaries the dataset contains the input webpages whose length is truncated to the rst tokens
contains samples for the company domain samples for the film domain and samples for the animal domain

implementation details xsum contains news articles nied with a one sentence summary answering the for all datasets we evaluated our self knowledge in the distillation framework in two settings
all animal without pretraining company rl


cv


cv tf





skd skd noisy t


skd noisy t noisy s


rl rl rl



































rl rl























skd skd noisy t











skd noisy t noisy s











film rl











rl





with pretraining table rouge results on wikicatsum test sets and are shorthands for unigram and bigram overlap rl is the longest common subsequence
results are reported separately on three domains and in combination all
skd refers to systems trained with self knowledge distillation noisy t are skd systems trained with noisy signals and noisy s are skd students trained on noisy data
results for comparison systems are taken from the authors respective papers or obtained on our data by running publicly released software
rst setting our models are non pretrained while in the second setting we take advantage of trained language models which have strated impressive improvements in tion lewis et al
liu and lapata bao et al

specically we adopt bao et al
as the pretrained model
is a transformer based neural network vaswani et al
with transformer layers and attention heads
it is pretrained as a pseudo masked guage model on a large corpus label smoothing is applied with smoothing factor

we tuned our teacher models following the procedure in bao et al

in the non pretrained setting we adopt a transformer encoder decoder model with layers hidden size and forward lter size
label smoothing was also used with smoothing factor

all teacher models in this setting were trained from randomly initialized parameters following liu and lapata
in all knowledge distillation experiments dent models have the same neural network tecture with their teachers and are trained with the same hyperparameters as the teacher models
the best teacher and student model are selected by evaluating perplexity on the development set
for noisy distillation models word drop probability pd was set to

the candidate length k for word replacement was and word replacement ability pr was

sentence drop probability ps was

during decoding we used beam search size and tuned for the length penalty wu et al
between
and on the validation set we decode until an end of sequence token is ted
repeated trigrams are blocked paulus et al

results
automatic evaluation we evaluated summarization quality automatically using rouge lin
we report unigram and bigram overlap and as a means of assessing informativeness and the longest common subsequence rouge l as a means of assessing uency
examples of system output are shown in table
table summarizes our results on the cnn dailymail and xsum single document datasets
the rst block includes the results of non pretrained models
we present the lead baseline which simply selects the rst three tences in a document for cnn dailymail and the rst sentence for
we also report the results of see et al
s pointer ator network ptrnet and an abstractive tem from liu and lapata based on formers transformerabs see section
for tails
the latter forms the backbone of our knowledge distillation models skd
we present a variant without noise skd a variant with noise in the teacher training signal noisy t models transformerabs noisy skd noisy skd cnn dailymail xsum







table factual correctness on cnn dailymail and xsum test set
noisy skd are students trained on noisy signals and noisy data
and a third variant where the student is ally trained on noisy data noisy s
the second and third blocks in table include the results of pretrained models
to make isons fairer we separate second block from base size third block pretrained models based on parameter size shown within ses
with regard to large size models we port the results of three very strong summarization systems netuned with unilmlarge bao et al
bartlarge lewis et al
and raffel et al

our base size models include bertsumbase liu and lapata a summarizer based on a base size bert encoder and a randomly initialized decoder massbase song et al
and unilmbase which are both netuned with base size trained models
as can be seen in table skd improves over teacher models in both pretrained size and non pretrained settings
we also observe that injection of noise brings further improvements with noise in the training signal noisy t ing more effective compared to noisy data tation noisy s
overall we obtain competitive results with skd and base size pretrained els and even manage to outperform unilmlarge and on the cnn dailymail dataset
table presents experimental results on the kicatsum dataset
the rst block in the table includes results for non pretrained models
and cv perez beltrachini et al
are convolutional encoder decoder models
the former is a standard convolutional decoder while the latter adopts a hierarchical convolutional coder which rst generates target sentence tors and then generates target words based on sentence vectors
tf is a standard former encoder decoder model trained on catsum perez beltrachini et al

tf is the model used in our skd system and its noisy version noisy t noisy s
the second block includes the results of a system using the size pretrained model unilmbase on its own and with skd
results are reported per domain pany film and animal and across domains all
under pretrained and non pretrained settings we skd boosts the performance of the teacher model unilmbase and tf respectively and that the injection of noise is benecial
provements in performance vary across domains with film showing the least gains
column all in table shows average rouge across domains
although skd and noise injection improve sults we observe that non pretrained models et more

factual consistency evaluation we also use besides rouge factcc kryscinski et al
to evaluate the factual correctness of the generated factcc is a bert based classier maries
trained to identify conicts between a source document and a generated summary
given a document sentence pair as input it assigns a information mentioned positive label in a summary sentence is consistent with the document otherwise it assigns a negative label
we view the percentage of positive labels assigned by factcc to all generated summaries as a factual correctness score for a summarization system
if factual we performed experiments with the publicly released version of factcc
our results on the cnn dailymail and xsum datasets are presented in table
here we only focus on document summarization as there is no version of factcc trained on multi document datasets
as can be seen the application of skd trained with noisy signals and on noisy data improves tual consistency for non pretrained and pretrained models on both datasets
all noisy skd dents are signicantly p
more factually correct compared to their teachers abs and using a paired student t test

human evaluation in addition to automatic evaluation we also sessed system output by eliciting human ments
we compared the quality of the summaries produced by a teacher model against its distilled student noisy skd
for
com salesforce factcc cnn daily mail gold lz granderson millennials say they ll marry if and when they want
he says that s not the case they re happily single and happy
granderson says marriage is about family not money
lz granderson millennials say they do nt care what their generation thinks about marriage
he says they ll get married if and when they want
lz marriage is linked to economic well being but it s not clear if that s true
noisy skd carol costello talk to any millennial and you can envision an america virtually marriage free
in countries like sweden or denmark people do nt feel pressured to marry even if they have kids together
xsum more than half of pupils in wales have passed their gcse exam for the third year running
gold more than
of pupils in wales have achieved the top grades in their gcse exams
noisy skd two thirds of welsh pupils who took gcses got a to c grades according to this year s results
animal gold the conception bank silver boa chilabothrus argentum is a species of boa described in may
it is only known from the conception island bank in the bahamas
it is the rst known discovery of a west indian boa species in years
it is named for its unique silver color
the conception bank silver boa chilabothrus argentum is a species of snake in the family boidae
it is demic to the bahamas
the species was discovered on conception island bank which comprises uninhabited islets
noisy skd the conception bank silver boa chilabothrus argentum is a species of nonvenomous boa endemic to the bahamas
it was discovered in on conception island bank an uninhabited islet in the bahamas
table gold reference summaries and automatic summaries produced by and its distilled student on the cnn dailymail xsum and datasets
cnn dailymail noisy skd succinct

inform fluent



xsum noisy skd succinct

inform fluent



noisy skd company

film animal



table human evaluation on cnn dailymail xsum and wikicatsum test sets
noisy skd is trained with self knowledge distillation on noisy signals and noisy data
all pairwise ences between systems are signicant p
using a paired t test
cnn dailymail and xsum human participants were presented with the output of two systems and the original document and asked to cide which one was better according to the lowing criteria succinctness does the summary avoid repetition informativeness does the mary capture the document s most important formation and fluency is the summary uent and grammatical
evaluation was conducted on the amazon mechanical turk crowdsourcing platform
we used the same test documents in total from liu and lapata for both cnn dailymail and xsum
we elicited ve sponses per hit
systems were rated along each dimension and assigned a score corresponding to the proportion of times a system was selected as better against another
human evaluation results are shown in table upper part
on both datasets participants ceive the student noisy skd as signicantly p
more succinct and informative pared to the teacher
however on fluency the student tends to be worse
upon spection we found student summaries to be rather telegraphic and hypothesize that crowdworkers tend to penalize them in terms of uency even though they are grammatical
human evaluation was performed slightly ferent for
recall that this is a document dataset where input documents are continuous webpage fragments
to allow pants to perform the experiment in a timely ion we used the gold summary as a proxy for the content of the input
crowdworkers were presented with the output of two systems again and noisy skd and asked to decide which one was better according to the formation contained in the gold summary
uation was conducted on amt we randomly lected samples from the test set and elicited three responses per hit
for each domain we port the proportion of times a system was chosen as better
human evaluation results are shown in table lower part
amt crowdworkers prefer the maries produced by the student for the animal and film domains but not for company we found that the distilled model tends to generate too many tities in one sentence which render the summaries too dense for this domain
conclusions in this paper we advocated the use of knowledge distillation for abstractive tion as a means to alleviate problems associated with maximum likelihood training for this task
we also introduced several noise functions in the training signal and training data which help larize training and further boost performance
periments on three benchmark datasets strate that our framework can improve both pretrained and pretrained summarizers
in the ture we would like to investigate more thoroughly which aspects of pretrained models improve and how self knowledge distillation can be enhanced with more sophisticated noise functions
references sungsoo ahn shell xu hu andreas c
damianou neil d
lawrence and zhenwen dai

ational information distillation for knowledge in the ieee conference on computer vision fer
and pattern recognition cvpr pages long beach california
chine learning pages new york new york
asli celikyilmaz antoine bosselut xiaodong he and yejin choi

deep communicating agents for in proceedings of the abstractive summarization
conference of the north american chapter of the association for computational linguistics man language technologies volume long pers pages new orleans louisiana
kevin clark minh thang luong urvashi wal christopher d
manning and quoc v
le

bam born again multi task networks for natural language understanding
in proceedings of the annual meeting of the association for tional linguistics pages florence italy
association for computational linguistics
wojciech m
czarnecki simon osindero max berg grzegorz swirszcz and razvan pascanu

sobolev training for neural networks
in i
guyon u
v
luxburg s
bengio h
wallach r
fergus s
vishwanathan and r
garnett editors advances in neural information processing systems pages
curran associates inc
jacob devlin ming wei chang kenton lee and kristina toutanova

bert pre training of deep bidirectional transformers for language in proceedings of the conference standing
of the north american chapter of the association for computational linguistics human language technologies volume long and short papers pages minneapolis minnesota
ation for computational linguistics
bin dong jikai hou yiping lu and zhihua zhang

distillation early stopping harvesting dark knowledge utilizing anisotropic information in neurips workshop on machine trieval
learning with guarantees vancouver canada
jimmy ba and rich caruana

do deep nets in advances in neural ally need to be deep formation processing systems pages
curran associates inc
maha elbayad laurent besacier and jakob verbeek

token level and sequence level loss arxiv preprint ing for rnn language models


hangbo bao li dong furu wei wenhui wang nan yang xiaodong liu yu wang songhao piao feng gao ming zhou and hsiao wuen hon

pseudo masked language models for ed language model pre training
arxiv preprint

cristian bucilu rich caruana and alexandru niculescu mizil

model compression
in ceedings of the acm sigkdd international conference on knowledge discovery and data ing kdd page new york ny usa
association for computing machinery
samuel rota lorenzo porzi and peter in kontschieder

dropout distillation
ceedings of the international conference on tommaso furlanello zachary lipton michael nen laurent itti and anima anandkumar

born again neural networks
in international ference on machine learning pages stockholm sweden
sebastian gehrmann yuntian deng and alexander rush

bottom up abstractive summarization
in proceedings of the conference on cal methods in natural language processing pages brussels belgium
akhilesh gotmare nitish shirish keskar caiming xiong and richard socher

a closer look at deep learning heuristics learning rate restarts warmup and distillation
in international ference on learning representations iclr new orleans la usa may
view
net
max grusky mor naaman and yoav artzi

newsroom a dataset of
million summaries with diverse extractive strategies
in proceedings of the conference of the north american chapter of the association for computational linguistics man language technologies volume long pers pages new orleans louisiana
sociation for computational linguistics
sangchul hahn and heeyoul choi

knowledge distillation in natural language ing
in proceedings of the international conference on recent advances in natural language ing ranlp pages varna bulgaria
incoma ltd
donna harman and paul over

the effects of human variation in duc summarization in text summarization branches out pages tion
barcelona spain
association for tional linguistics
junxian he jiatao gu jiajun shen and marcaurelio revisiting self training for arxiv preprint sequence generation
ranzato

neural

karl moritz hermann tomas kocisky edward stette lasse espeholt will kay mustafa suleyman and phil blunsom

teaching machines to read and comprehend
in c
cortes n
d
lawrence d
d
lee m
sugiyama and r
garnett editors advances in neural information processing systems pages
curran associates inc
geoffrey hinton oriol vinyals and jeff dean

distilling the knowledge in a neural network
arxiv preprint

yoon kim and alexander m
rush

level knowledge distillation
in proceedings of the conference on empirical methods in ral language processing pages austin texas
association for computational linguistics
wojciech kryscinski nitish shirish keskar bryan cann caiming xiong and richard socher

neural text summarization a critical evaluation
arxiv preprint

adhiguna kuncoro miguel ballesteros lingpeng kong chris dyer and noah a
smith

ing an ensemble of greedy dependency parsers into one mst parser
in proceedings of the ference on empirical methods in natural language processing pages austin texas
ciation for computational linguistics
and comprehension
in proceedings of the nual meeting of the association for computational linguistics pages online
association for computational linguistics
chin yew lin

rouge a package for in text matic evaluation of summaries
rization branches out proceedings of the workshop pages barcelona spain
tion for computational linguistics
peter j liu mohammad saleh etienne pot ben goodrich ryan sepassi lukasz kaiser and noam shazeer

generating wikipedia by ing long sequences
in proceedings of the national conference on learning representations vancouver canada
xiaodong liu pengcheng he weizhu chen and feng gao

multi task deep neural networks for natural language understanding
in proceedings of the annual meeting of the association for putational linguistics pages florence italy
association for computational linguistics
yang liu and mirella lapata

text in proceedings of tion with pretrained encoders
the conference on empirical methods in ural language processing and the international joint conference on natural language processing emnlp ijcnlp pages hong kong china
association for computational linguistics
christopher manning mihai surdeanu john bauer jenny finkel steven bethard and david mcclosky

the stanford corenlp natural language in proceedings of annual cessing toolkit
meeting of the association for computational guistics system demonstrations pages timore maryland
association for computational linguistics
joshua maynez shashi narayan bernd bohnet and ryan mcdonald

on faithfulness and ality in abstractive summarization
in proceedings of the annual meeting of the association for computational linguistics pages line
association for computational linguistics
seyed iman mirzadeh mehrdad farajtabar ang li and hassan ghasemzadeh

improved edge distillation via teacher assistant bridging the gap between student and teacher
arxiv preprint

hossein mobahi mehrdad farajtabar and peter l
bartlett bartlett

self distillation es regularization in hilbert space
arxiv preprint

mike lewis yinhan liu naman goyal jan ghazvininejad abdelrahman mohamed omer levy veselin stoyanov and luke zettlemoyer

bart denoising sequence to sequence training for natural language generation translation lili mou ran jia yan xu ge li lu zhang and zhi jin

distilling word embeddings an ing approach
proceedings of the acm national on conference on information and edge management pages
ramesh nallapati bowen zhou cicero dos santos caglar gulcehre and bing xiang

tive text summarization using sequence to sequence in proceedings of the rnns and beyond
signll conference on computational natural guage learning pages berlin germany
association for computational linguistics
shashi narayan shay b
cohen and mirella lapata

do nt give me the details just the summary topic aware convolutional neural networks for in proceedings of the treme summarization
conference on empirical methods in natural guage processing pages brussels gium
association for computational linguistics
ani nenkova

summarization evaluation for text and speech issues and approaches
in ninth tional conference on spoken language processing
emilio parisotto jimmy ba and ruslan salakhutdinov

actor mimic deep multitask and transfer inforcement learning
corr

romain paulus caiming xiong and richard socher

a deep reinforced model for abstractive marization
in proceedings of the international conference on learning representations ver canada
laura perez beltrachini yang liu and mirella lapata

generating summaries with topic templates and structured convolutional decoders
in ings of the annual meeting of the association for computational linguistics pages florence italy
association for computational guistics
mary phuong and christoph lampert

towards in understanding knowledge distillation
ings of the international conference on chine learning pages long beach ifornia
colin raffel noam shazeer adam roberts katherine lee sharan narang michael matena yanqi zhou wei li and peter j
liu

exploring the limits of transfer learning with a unied text to text former
arxiv e prints
gj rath a resnick and tr savage

the tion of abstracts by the selection of sentences
part i
sentence selection by men and machines
american documentation
adriana romero nicolas ballas samira ebrahimi hou antoine chassang carlo gatta and yoshua bengio

fitnets hints for thin deep nets
arxiv preprint

alexander m
rush sumit chopra and jason weston

a neural attention model for abstractive in proceedings of the tence summarization
conference on empirical methods in natural guage processing pages lisbon portugal
association for computational linguistics
evan sandhaus

the new york times annotated corpus
linguistic data consortium philadelphia
abigail see peter j
liu and christopher d
manning

get to the point summarization with generator networks
in proceedings of the nual meeting of the association for computational linguistics volume long papers pages vancouver canada
kaiqiang song lin zhao and fei liu

infused copy mechanisms for abstractive tion
in proceedings of the international ference on computational linguistics pages santa fe new mexico usa
association for computational linguistics
kaitao song xu tan tao qin jianfeng lu and yan liu

mass masked sequence to quence pre training for language generation
in ceedings of international conference on machine learning pages long beach nia
jiwei tan xiaojun wan and jianguo xiao

abstractive document summarization with a in proceedings based attentional neural model
of the annual meeting of the association for computational linguistics volume long papers pages vancouver canada
association for computational linguistics
xu tan yi ren di he tao qin zhou zhao and tie yan liu

multilingual neural machine corr translation with knowledge distillation


yee teh victor bapst wojciech m
czarnecki john quan james kirkpatrick raia hadsell nicolas heess and razvan pascanu

distral robust multitask reinforcement learning
in i
guyon u
v
luxburg s
bengio h
wallach r
fergus s
wanathan and r
garnett editors advances in ral information processing systems pages
curran associates inc
gregor urban krzysztof j geras samira ebrahimi kahou ozlem aslan shengjie wang rich ana abdelrahman mohamed matthai philipose and matt richardson

do deep convolutional nets really need to be deep and convolutional arxiv preprint

ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan n gomez ukasz kaiser and illia polosukhin

attention is all you need
in i
guyon u
v
luxburg s
bengio h
wallach r
fergus s
vishwanathan and r
nett editors advances in neural information cessing systems pages
curran ciates inc
yonghui wu mike schuster zhifeng chen quoc v le mohammad norouzi wolfgang macherey maxim krikun yuan cao qin gao klaus macherey et al

google s neural machine translation system bridging the gap between in arxiv preprint man and machine translation


qizhe xie eduard hovy minh thang luong and quoc v le

self training with noisy student arxiv preprint improves imagenet classication


chenglin yang lingxi xie siyuan qiao and alan yuille

training deep neural networks in erations a more tolerant teacher educates better dents
in proceedings of the aaai conference on articial intelligence volume pages honolulu hawaii
sergey zagoruyko and nikos komodakis

ing more attention to attention improving the formance of convolutional neural networks via tion transfer
in proceedings of the international conference on learning represenatations toulon france
dongxu zhang and zhichao yang

word ding perturbation for sentence classication
arxiv preprint


