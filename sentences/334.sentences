multi mode translation of natural language and python code with transformers colin b
clement microsoft cloud and ai
com dawn drain microsoft cloud and ai
com jonathan stanford university
edu alexey svyatkovskiy microsoft cloud and ai
com neel sundaresan microsoft cloud and ai
com abstract simultaneously modeling source code and natural language has many ing applications in automated software development and understanding
suant to achieving such technology we introduce the python method text to text transfer transformer which is trained to translate between all pairs of python method feature combinations a single model that can both predict whole methods from natural language tation strings docstrings and summarize code into docstrings of any common style
we present an analysis and modeling fort of a large scale parallel corpus of million python methods and
lion method docstring pairs ing that for docstring and method eration outperforms sized auto regressive language models which were english pre trained or randomly initialized
on the searchnet test set our best model dicts
syntactically correct method bodies achieved a bleu score of
for method generation and
for docstring corresponding author work done during a microsoft internship generation summarization and achieved a rouge l f score of
for method generation and
for docstring tion
introduction software is a keystone of modern society touching billions of people through services and devices daily
writing and documenting the source code of this software are ing and labor intensive tasks software opers need to repeatedly refer to online umentation resources in order to understand existing code bases to make progress
oper productivity can be improved by the ence of source code documentation and a velopment environment featuring intelligent machine learning based code completion and analysis tools
recent progress in natural language ing nlp especially encoder decoder based transformer models vaswani et al
and pre training radford et al
lewis et al
has led to state of the art formance on language modeling tion devlin et al
translation raffel et al
summarization liu and t c o g l
s c v
v i x r a ata grammar correction bryant et al
entity recognition dialogue tion budzianowski and vulic and more
along with these quantitative advances have come deeper understanding of the learned hidden representations which power ers kovaleva et al
voita et al
clark et al
ethayarajh
while they are arguably not natural programming languages are increasingly becoming ing playgrounds for nlp modeling
since these languages by denition have a mar syntax and known relationships between entities they offer enticing opportunities for an even deeper probing of nlp models and tasks
beyond theoretical importance many nlp tasks have practical utility in software development environments language ing or generation can be used for code pletion raychev et al
bruch et al
svyatkovskiy et al
tion summarization to generate documentation or natural language summaries moreno et al
scalabrino et al
wan et al
alon et al
or even summarize a set of code changes moreno et al
tion and grammar error correction to patch and detect bugs zhai et al
and joint bedding of code and natural language for code search husain et al
gu et al

in this work we focus on jointly modeling both source code python and concomitant natural language documentation docstrings with transformers through the study of dual tasks generating method code bodies from signatures and docstrings and generating strings from signatures and method code ies
while previous work allamanis et al
yin and neubig has leveraged the grammar of code to extract features like the stract syntax tree for modeling treating code and natural language as separate modalities we follow examples like barone and sennrich and treat python and its docstrings as fundamentally no different than other ral languages representing both source code and natural language docstrings as sequences of tokens sharing the same vocabulary
here we present a multi mode translation method resulting in the python method text to text transfer transformer inspired by the text to text transfer transformer raffel et al

our single model can both learn code language generation and understand the relationships between them
the paper is organized as follows we begin in sec
by presenting examples of the performance of our novel multi mode the python method text to text transfer transformer model which we trained to translate between all pairs of combinations of method signatures docstrings and ies which do not have the same feature in both the source and target
in sec

we scribe our training data and the pre processing steps for source code and natural language we followed and compared it to existing allel docstring method corpora like searchnet et al
and that presented by barone al barone and nrich
in sec

we explain our like lewis et al
pre training scheme demonstrating a speed up in training time for docstring generation
next in sec

we analyze and classify python docstrings abling style conditioned docstring generation in
in sections and we discuss results on method generation and string generation respectively and compare it to two models randomly initialized and pre trained on english
multi mode training figure shows examples of inputs and puts of our model for example tasks top blue predicting a body from a method figure real examples of performing method generation using combinations of signatures and docstrings
a leading comment in the input sequence instructs the model to output a particular target signature and body instructs to predict combination of features e

both a signature and body
target docstring style oneline def count for example in lst count return count if example target docstring style numpydoc def count for example in lst count return count if example count the number of even numbers in a list count the number of even numbers in a list
the list to count even numbers in
parameters lst list returns int the number of even numbers in the list
figure performing docstring generation on an example method showing the output when the target prex indicates one line top blue and numpydoc docstring bottom red styles
signature middle red predicting a whole method from a natural language docstring and bottom green predicting a body from a signature and docstring
note that the ment target specification structs the model to choose a particular form of output
further note that correctly learns to interpret natural language it prets even as being related to example and greater than as number
the model also duces syntactically correct code as we will discuss later we never show the model tically incorrect code and correctly infers the types of lst and numbers to be iterables containing numbers
can also be prompted with source code to produce a docstring summary in various styles
figure shows the model prompted with one of the methods generated by in fig
top blue in both a one line top blue style and a numpydoc bottom red style
it infers the intent from the signature name and code and even infers that type of the argument is a list and return type int
it produces the same terse one sentence summary of the function in both cases
in order to teach to maximally late the separate method features signatures docstrings bodies we trained it to translate between all pairs of feature combinations in which the same feature does not appear in both the source and target
this scheme is also vantageous as our corpus is unbalanced with only methods featuring docstrings and so the model can learn to leverage all the features whether they are present or not
additionally it has been shown that code is more predictable than natural language hindle et al

if the method and argument names are a nating signal due to their relatively rigid ture the model may learn to ignore the content of docstrings
this multi mode method comes that by training the model to generate method bodies from docstrings alone
see the appendix for a more detailed description of the multi mode training scheme

dataset our data consists of github ries which includes all public repositories belled as containing primarily python source code featuring at least stars and which have had a commit in the past years
we successfully cloned of these repositories extracting
million python les from the default head state of each repository
we then removed literal duplicate les resulting in
million unique les but did not remove grained clones
after removing license from the les the literal contents were used in the pre training step comprising about gb of raw text
in order to extract method level tion for ne tuning we used the
standard library ast to produce the level abstract syntax tree ast for each python le extracting every individual and class method
for each le which failed to parse we used and to come the issue of different styles and white space or tab conventions successfully parsing
of the
million unique python les
we used the python module astunparse to take the ast for each method and unparse them back into source code so that our tuned model was never trained on syntactically incorrect code
the statistics of our docstring corpus are summarized in table

our parallel method docstring corpus is twice as large as the next largest irrespective of guage and over as large as the next largest python parallel corpus both in csn
for each method we ignored comments as they generally represent trivia and are not part of the normal language syntax
we cleaned the docstrings by removing non ascii characters normalizing unicode and replacing commit hashes le paths and urls with placeholder tokens
in all studies here we randomly split the les at the repository level to prevent data leakage with for training for tion and for a test set

pre training the majority of our python methods over million methods do not possess strings
this imbalance is in fact an tunity in light of the recent trend for nlp unsupervised pre training of language els on vast amounts of raw text devlin et al

using these pre trained models as ing points for downstream tasks like cation translation summarization and tion answering consistently yields state the art results lewis et al
raffel et al

following this trend we use a similar masking objective used by the recent text text transfer transformer raffel et al

as shown in figure after ing the inputs we sample a random subset of the token spans up to length to be replaced with e

a token and then teach dataset methods docstring languages csn husain et al
ciurumelea et al
barone and sennrich







python python al
python python table summary statistics of our python parallel corpus compared to others presented in the literature
csn contains python methods with docstrings among other languages
our parallel corpus is as large as the next largest and over the size of the next largest python parallel corpus
figure denoising auto encoder pre training for sequence to sequence tasks based on the masking objective used by the raffel et al

python les are rst tokenized with spaces replaced by the character g which is in ordinal above the space character similarly for newlines tabs

note that indentation is a token of multiple g s
we replace random sub sequences of tokens with numbered masks and train the model to return each mask followed by the tokens it replaced
the sequence to sequence model to replace the missing tokens
the training target is prised of numbered mask tokens followed by the tokens that mask represents
the architecture of is an decoder transformer with a vocabulary of byte pair bpe encoder trained on raw python les self attention encoder decoder layers in each encoder layers and a hidden mension of totaling million ters
all the experiments in this paper ing were done using this same extended gpt tokenizer
we pre trained on gb of raw source code in total for weeks on sixteen gb tesla gpus or epochs total
when training on docstring eration alone we observed faster gence to a lower loss when starting with this pre trained model as compared to a random tialization
see the appendix for details
in all experiments is trained starting with this pre trained model

docstring analysis when examining docstring samples from our corpus one of the most salient features is the different styles of documentation
the python community has no prescribed or de facto style for docstrings but python hancement protocol goodger and van rossum does describe one line and multi line docstrings and mandates tion as well
most modern large scale projects utilize docstring styles which are parseable lowing the automatic creation and tion of source code and documentation sites see e

sphinx
therefore a number of standard styles have evolved in the nity
the currently dominant parseable docstring styles and the ones supported by sphinx are restructuredtext rest jones the ofcial google style google numpy style also technically es rest maintainers and javadoc style jav
the difference tween each style is mainly in the syntax they exist and of denoting sections if the name type description annotation of the method arguments and returned yielded tities if they exist
we dened in tion to these styles one line containing only one line one paragraph containing no empty lines and other to label any docstring not described so far which includes informal user docstring styles and a few project specic styles like the sage mathematics toolkit brary
table shows the breakdown of the fraction of each of these styles in our corpus
the rality of docstrings are one line
the next most common style is one paragraph at
the next four most common styles are the machine parseable styles discussed above comprising
of the total number of strings
the appendix contains detailed tributions of method signature docstring and method body character and line lengths
style one line one paragraph rest google numpy javadoc other fraction of methods


table docstring style statistics from
million pythondocstrings
to visualize the space of these styles we used fasttext vector embeddings of the strings obtaining dimension continuous vector representations of each
we then used pca to reduce the dimensionality to and plied the t distributed stochastic neighbor bedding t sne to obtain a two dimensional visualization
figure shows of our docstrings embedded colored by docstring style as dened above
we can see clear clustering of styles indicating that similar docstrings use the same style for the parseable styles
there is also a natural chotomy between parseable and non parseable styles the left side is dominated by one line one paragraph and other styles and the four parseable styles are largely on the right side
this observation can be used to generate mentation consistent with the style of a given project or it could be used to translate ods into more informal descriptions useful for search indices
figure visualization of continuous dings of of our docstring corpus strings colored by docstring style
embeddings were obtained using fasttext and the dimensional embedding was obtained via pca for dimensionality reduction and initialization and t sne
model med random med english csn test med random ppl
bleu syntax





prec
rec




prec
rec


stat
prec
rec
prec
rec
prec
rec
prec
rec




































rl

















barone and sennrich test barone et al

table comparing models with a random weight initialization pre trained on english and on the task of method generation from a signature and natural language docstring
the rst three rows use our test set consisting of methods
the fourth and fth rows compare the performance of and medium on the codesearchnet python test set
the nal rows compare the performance of on the parallel test set of barone and sennrich
syntax is the fraction of predicted methods which had correct syntax using the python
grammar
method generation now we turn our attention to method ation predicting a whole method code body from either a method signature a natural guage docstring or both
we rst discuss a benchmark of this task using a medium model million parameters see the pendix for details training from scratch and starting with the publicly released openai glish pre trained checkpoint with weights from et al

in all ments we used an extended tokenizer including white space one tab two tabs
tokens for a total vocabulary size of and we used beam decoding with a beam width of
the third row of tab
shows has more than double the bleu score overall better recall and signicantly better and rouge l f scores than our baselines
further
of the methods generated by were syntactically correct python
whereas only of methods were syntactically correct
was trained on tesla gb gpus for epochs or weeks training time see the appendix for its hyper parameters and the baselines were trained on the same hardware for week training time achieving the same or better validation loss perplexity as
the english pre trained initialization of only slightly beats the random ization of which could indicate that the learned biases of english are not particularly benecial for writing python code the rics are almost all within our margin of error
note that barone and sennrich also modeled methods from docstrings obtaining a similar bleu score of
on their own python parallel corpus
on the barone et al
test set obtains nearly double these scores at
such a large discrepancy could be explained by data leaking from their test set model med random med english ppl
bleu




csn test med random barone test barone et al




p r p r p r p r p r p r



































rl

















table comparing models with a dom weight initialization pre trained on english and on the task of natural guage docstring generation from a signature and method body
the rst three rows are evaluated on our test set of methods
the fourth and fth rows shows performance of and medium on the csn python test set and the last two rows compare our model to barone et al
on their test set
into our training set
barone s test set is also smaller than ours and may not be a resentative sample of the whole python code domain
the third and fourth rows of tab
show the performance of using the publicly available csn python test set from which we nd notably worse results than on our own test set
csn curated their whole set by ing any methods with test in the name and any methods with fewer than lines of code
we calculated the performance of only on a subset of our test set curated the same way as csn observing f scores for and r l on our test set of

and
which is lower than our nominal test set mance of

and
and closer to the csn performance of

and

we believe this curating choice explains the ence between our test set and the csn test set
we also conclude that tests and short methods are easier to complete which is plausible and bodes well for automatic code completion applications
docstring generation we now examine results from the docstring generation task which for evaluation poses were conditioned on both signatures and method bodies
as in method generation we set a benchmark with random ization and pre trained english initialization as well as the same hyperparameters
table shows that the rouge scores of the baselines are within the margin of error a somewhat surprising result given the english domain of docstrings
the third row shows to be superior to medium in terms of bleu and all of the rouge metrics
we again present the results from the licly available csn test set
similar to the method generation task performs worse on the csn data than our own likely for the same reasons we discussed in sec

we also evaluated on the barone et al
parallel test set as shown in the second to last row of tab
and nd performs notably worse on barone s test set than our own test set contradicting the hypothesis that our doubling of the method generation bleu score is due to data leakage
has a much higher bleu score than that reported by barone et al perhaps indicating real progress in the code summarization eld
docstring generation is similar to code marization though the domains are different as docstrings also contain structured annotations of arguments return values raised exceptions and even in line unit tests doctest
by wang et al
wang et al
reports a best rouge l of
on the same test set for code summarization but does not specify which statistic they are reporting so we not make strong conclusions about the mance of compared to the state of the art
conclusion in this work we presented a novel multi mode python method text to text transfer former model well as the largest parallel corpus of python source code and docstrings reported in the literature to date
we have trained to translate between all pairs of combinations of method tures docstrings and method bodies which do not have the same feature in both the source and target
further we introduced control token prexes for docstring generation to cilitate docstring generation of various styles
focusing on two modeling tasks ing python methods from docstrings and summarizing python source code methods into docstrings of various commonly ring styles we have compared this new proach to the auto regressive baselines trained on individual docstring or method eration tasks
on the codesearchnet test set achieves a bleu score of
for method generation and
for docstring generation and a rouge l f score of
for method generation and
for docstring generation
we have demonstrated the fectiveness of dynamic masked pre training reducing docstring generation training time by
looking forward we plan to age for various downstream mated software engineering tasks including code documentation and method generation from natural language statements and velop more model evaluation criteria to age the unique properties of source codes
acknowledgements we would like to thank the microsoft cloud and ai smartml engineering team for help in preparing the data shao kun deng for the development of compelling user experiences leveraging and christian bird for ful discussions
a appendix a
docstring statistics figure shows the distributions of various tures of docstrings in our corpus
the top row is the distribution of total character level length of the method signatures left docstrings ter and code bodies
the blue lines are for methods possessing a docstring and we can see that the vast majority of these methods have docstrings with more than characters
the bottom row shows the distribution of line lengths of the concomitant features from the top row
while the most common line length of docstrings is comprising the vast majority of docstrings have multiple lines
a
pre training details figure is the complete training script using the facebook ai research quence fairseq modeling library with which we pre trained
the data was pre noised and processed using the fairseq preprocess command and placed in the directory indicated by dir
the architecture and training hyper parameters are set in this script
was trained with the same hyperparameters but with data described in sec
a

figure shows learning curves of a gle model of the same architecture as trained only on docstrings starting from random initializations and starting from our pre trained model
as the gure shows the pre trained initialization converged to a better figure histogram of the number of characters top row in the python signatures left docstrings middle and method body right
the blue lines are for methods with docstrings the yellow lines are for methods without docstrings
the vast majority of docstrings have more than characters
the bottom row shows histograms of the number of lines for the same features described in the top row
validation loss faster than the randomly initialized model
a
training details our experiments also used the fairseq library with the openai english checkpoint supplied by the huggingface library
ure shows the complete training script where for the english pre trained initialization a trained checkpoint was provided
each models was trained on tesla gpus with gb of memory each for days
a
multi mode training details in order to better teach to stand the relationships between all the ent features of code signatures docstrings and bodies we taught it to translate between all pairs of combinations of these features figure learning curves for training a to sequence transformer translating from python method denitions to their docstrings
blue curves represent the training and validation loss and show that convergence validation loss stops decreasing occurs after
steps or epochs
the optimization of the pre trained model with cal hyperparameters reaches and beats the best idation loss at
steps or epochs

dir fairseq train tokens translation lang src lang tgt all embeddings decoder input output embed transformer
dropout
dropout
embed dim embed dim target positions source positions ffn embed dim ffn embed dim attention heads attention heads smoothing

dropout
decay
adam norm
scheduler lr updates freq invalid size inputs valid test dir dir models interval betas

eps logdir dir tensorboard learned pos learned pos figure the fairseq train script used to pre train setting all the relevant parameters
fairseq train dir adam betas

decay
norm

optimizer scheduler updates init lr
decay
per sample break mode complete tokens freq target positions invalid size inputs valid test figure the fairseq train script we used to train our gpt model baselines where the solid black line is the ing loss and all the other curves are the tion loss for each of the tasks indicated in tab

the dashed lines indicate tasks where strings are present in the target showing that these are generally less predictable than only targets as the validation loss is larger
trained on tesla gb gpus for epochs or weeks training time
which do not contain the same feature in both the source and target
in this way the model can learn to produce method bodies ing both signatures and docstrings or one or the other
table spells out exactly which combinations were provided to the model for each source as a source and target
example the comment string target feature style was added structing the model which feature combination e

signature and body
only if a docstring was in the target a style imperative was added where the styles are dened and discussed in the main text
figure shows the training curves for references
java doc
technical report
miltiadis allamanis daniel tarlow andrew d
gordon and yi wei

bimodal modelling in of source code and natural language
ceedings of the international conference on international conference on machine ing volume page
jmlr
org
uri alon shaked brody omer levy and eran yahav

generating sequences from structured representations of code
arxiv preprint

antonio valerio miceli barone and rico sennrich

a parallel corpus of python functions and figure learning curve for the multi mode training where the black line is the training loss and the other lines are the validation loss for each mode of translation
dashed lines indicate the docstrings are in the target solid lines have only code in the target
documentation strings for automated code umentation and code generation
arxiv preprint

marcel bruch martin monperrus and mira mezini

learning from examples to in prove code completion systems
ings of the joint meeting of the european software engineering conference and the acm sigsoft symposium on the foundations of ware engineering pages
christopher bryant mariano felice and edward briscoe

automatic annotation and uation of error types for grammatical error rection
association for computational guistics
pawe budzianowski and ivan vulic

hello it s how can i help you towards the use of pretrained language models for arxiv preprint oriented dialogue systems


adelina ciurumelea sebastian proksch and ald gall

suggesting comment tions for python using neural language models
in edition of the ieee international ference on software analysis evolution and reengineering saner
ieee
kevin clark urvashi khandelwal omer levy and christopher d manning

what does bert look at arxiv preprint

an analysis of bert s attention
jacob devlin ming wei chang kenton lee and kristina toutanova

bert pre training of deep bidirectional transformers for language derstanding
arxiv preprint

kawin ethayarajh

how contextual are textualized word representations comparing the geometry of bert elmo and dings
arxiv preprint

david goodger and guido van rossum

docstring conventions
pep
google

google python style guide
cal report
xiaodong gu hongyu zhang and sunghun kim

deep code search
in proceedings of the international conference on software gineering icse page new york ny usa
association for computing ery
abram hindle earl t barr zhendong su mark gabel and premkumar devanbu

on the in naturalness of software
national conference on software engineering icse pages
ieee
o g i s y o b c o d y o b g i s sources r u t a n g i s g n i r t c s o d y o b signature docstring body sig s t e g r a t sig body doc body table a table of all possible translation ities between the features of a function the nature sig docstring and body
we train our model to translate between sources and targets indicated with a which were chosen as all pairs of feature combinations which do not contain the same feature in both the source and target
the tem is then instructed to target code bodies when performing function completion
hamel husain ho hsiang wu tiferet gazit miltiadis allamanis and marc brockschmidt

codesearchnet challenge evaluating the state of semantic code search
arxiv preprint

richard jones

a restructuredtext primer
docutils
sourceforge
net march
olga kovaleva alexey romanov anna rogers revealing arxiv preprint and anna rumshisky

the dark secrets of bert


mike lewis yinhan liu naman goyal marjan ghazvininejad abdelrahman mohamed omer levy ves stoyanov and luke zettlemoyer

bart denoising sequence to sequence pre training for natural language generation translation and comprehension
arxiv preprint

yang liu and mirella lapata

text arxiv marization with pretrained encoders
preprint

numpydoc maintainers

numpydoc string guide
technical report
laura moreno jairo aponte giriprasad sridhara andrian marcus lori pollock and k shanker

automatic generation of ural language summaries for java classes
in international conference on gram comprehension icpc pages
ieee
laura moreno gabriele bavota massimiliano di penta rocco oliveto andrian marcus and gerardo canfora

automatic in proceedings of the tion of release notes
acm sigsoft international symposium on foundations of software engineering pages
alec radford karthik narasimhan tim salimans and ilya sutskever

improving language understanding by generative pre training
url us
amazonaws
com assets researchcovers languageunsupervised language understanding paper
pdf
colin raffel noam shazeer adam roberts katherine lee sharan narang michael matena yanqi zhou wei li and peter j liu

exploring the limits of transfer learning with a unied text to text transformer
arxiv preprint

veselin raychev martin vechev and eran hav

code completion with statistical in proceedings of the language models
acm sigplan conference on programming language design and implementation pages
simone scalabrino gabriele bavota pher vendome mario linares vasquez denys poshyvanyk and rocco oliveto

tomatically assessing code understandability how far are we in ieee acm ternational conference on automated software engineering ase pages
ieee
alexey svyatkovskiy shao kun deng shengyu intellicode fu and neel sundaresan

compose code generation using transformer
arxiv preprint

alexey svyatkovskiy ying zhao shengyu fu and neel sundaresan

pythia ai assisted code completion system
in proceedings of the acm sigkdd international conference on knowledge discovery data mining pages
ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan n gomez ukasz kaiser and illia polosukhin

tention is all you need
in advances in neural information processing systems pages
elena voita rico sennrich and ivan titov

the bottom up evolution of representations in the transformer a study with machine tion and language modeling objectives
arxiv preprint

yao wan zhou zhao min yang guandong xu haochao ying jian wu and philip s yu

improving automatic source code in tion via deep reinforcement learning
ceedings of the acm ieee international conference on automated software ing pages
wenhua wang yuqun zhang zhengran zeng and guandong xu

trans a based framework for unifying code arxiv preprint rization and code search


thomas wolf lysandre debut victor sanh julien chaumond clement delangue thony moi pierric cistac tim rault remi louf morgan funtowicz and jamie brew

huggingface s transformers state arxiv the art natural language processing


pengcheng yin and graham neubig

a tactic neural model for general purpose code generation
in proceedings of the annual meeting of the association for computational linguistics volume long papers pages vancouver canada
association for computational linguistics
juan zhai xiangzhe xu yu shi minxue pan shiqing ma lei xu weifeng zhang lin tan and xiangyu zhang

cpc automatically classifying and propagating natural language comments via program analysis

