improving zero and few shot abstractive summarization with intermediate fine tuning and data augmentation alexander r
fabbri haoran li marjan ghazvininejad simeng han haoyuan li shaq joty dragomir radev yashar mehdad yale university facebook ai nanyang technological university renmin university of china alexander
fabbri dragomir

edu

edu
sg
edu
cn aimeeli ghazvini
com abstract models pretrained with self supervised tives on large text corpora achieve state of art performance on text summarization tasks
however these models are typically ne tuned on hundreds of thousands of data points an infeasible requirement when applying rization to new niche domains
in this work we introduce a general method called itransfer for ne tuning pretrained models for summarization in an unsupervised specic manner which makes use of istics of the target dataset such as the length and abstractiveness of the desired summaries
we achieve state of the art zero shot tive summarization performance on the dailymail dataset and demonstrate the tiveness of our approach on three additional diverse datasets
the models ne tuned in this unsupervised manner are more robust to noisy data and also achieve better few shot performance using and training ples
we perform ablation studies on the effect of the components of our unsupervised tuning data and analyze the performance of these models in few shot scenarios along with data augmentation techniques using both matic and human evaluation
introduction automatic text summarization aims to distill the most salient content of a given text in a compact form
recent advances in summarization have been driven by the availability of large scale datasets such as the cnn dailymail cnndm corpus nallapati et al
and the new york times corpus sandhaus as well as by the duction of large pretrained models such as bart lewis et al
and pegasus zhang et al
in some cases resulting in summaries which are even favored over the human written reference summaries
fine tuning such models however ically requires a large corpus of labeled summaries and creating data for every domain is infeasible and highly costly
thus the ability to transfer large trained models to new domains with only a small amount of data is necessary especially as such models make their way into production ments
unsupervised summarization approaches clude autoencoders to mirror the information pression inherent in summarization baziotis et al
chu and liu brazinskas et al
as well as large scale pretraining for specic adaptation yang et al

however little work has focused on domain adaptation in summarization
wang et al
examine main adaptation for extractive summarization
hua and wang showed that summarization els have difculty generating text in the style of the target domain while more recently zhang et al
report strong performance of trained models when trained in few shot settings and brazinskas et al
ne tune specic components of a model for few shot ing
we aim to build off of recent work in trained models and improve unsupervised and shot summarization by encoding characteristics of the target summarization dataset in unsupervised intermediate ne tuning data
in one view summarization can be seen as a function of several sub functions called aspects which determine the output form
jung et al
dene three subaspects for rization position importance and diversity and study how these subaspects manifest themselves in summarization corpora and model outputs
for example a common subaspect for the cnndm dataset is position earlier sentences tend to give a good summary
inspired by this view of rization as subaspects we aim to encode subaspects of a target dataset into unlabeled data to allow a model ne tuned on this data to learn t c o l c
s c v
v i x r a tics of the target dataset to improve zero shot and few shot transfer of the model
in our work we focus on the subaspects of extractive diversity as determined by how well an extractive model forms on the data compression ratio between the source document and summary and in the case of cnndm the lead bias
we assume knowledge of the target dataset such as the size of input ments the size of the desired summaries and the extent to which the summary is abstractive all of which are prior knowledge if the task is to be well dened
we encode this knowledge into wikipedia article data by extracting summaries of the desired output length and ltering examples based on the desired level of abstraction
our contributions are the following we troduce a method called wikitransfer to create pseudo summaries with subaspects of the target dataset which can be used as unlabeled data for termediate ne tuning we show that this method improves zero shot domain transfer over transfer from other domains achieving state of the art supervised abstractive summarization performance on the cnndm dataset while generalizing to other domains and we perform extensive ablation studies on the factors inuencing zero shot performance we demonstrate additional improvements in ferring our wikitransfer models in the few shot ting and analyze differences in performance when using data augmentation techniques on datasets with different level of extractiveness
related work while advances have been made in neural niques for summarization due in part to large datasets less work has focused on domain tion of such methods for summarization in the zero and few shot settings
wang et al
examine domain adaptation but in extractive summarization
hua and wang examine domain adaptation between opinion and news summarization ing that models trained on one domain and applied to another domain can capture relevant content but differ in style in generating the summary
brazinskas et al
introduce plug in works small netune able layers added to a larger model that aims to reproduce characteristics of the target dataset as seen in a small set of labeled amples
in contrast we aim to encode the teristics of our target dataset such as ness and compression a priori in the intermediate training phase for better adaptation
in other work lebanoff et al
adapt a single document summarization model to the multi document ting while zhu et al
use the references from wikipedia data for downstream query based summarization similar to the task of wikipedia paragraph generation as dened in liu et al

several approaches for unsupervised rization have made use of variational coders baziotis et al
chu and liu brazinskas et al

zhou and rush makes use of pretrained language models for pervised text summarization by aligning the erage of the generated summary to the source ument
laban et al
train an unsupervised summarization model by guiding the model with reinforcement learning rewards
in another line of work extractive models such as textrank cea and tarau lexrank erkan and radev and more recently pacsum zheng and ata aim to use graph centrality in order to extract important sentences for a document
the power of pretrained models for few shot transfer was shown in zhang et al

our work focuses on the zero shot case as well as the transferability of models ne tuned on given datasets across multiple datasets rather than just the transferability of a single pre trained model
the closest work to ours for zero shot transfer is yang et al
which makes use of the bias in news articles to pretrain an unsupervised model on a large dataset of news articles
our proach however focuses on ne tuning an already pretrained model specic for the task of rization on a downstream dataset and shows the generalizability of such ne tuning across domains
bart lewis et al
is a pretrained denoising autoencoder and achieved state of the art mance when ne tuned on summarization tasks at the time
in this work we use bart as our base pretrained model but in future work will ment with intermediate ne tuning and few shot transfer with other pretrained models
methods in this section we introduce our methods to prove zero and few shot abstractive summarization
let


xt


xn be a source ument with n words and n sentences where xi represents the i th word in
it could also be represented as


st


sn where st resents the t th sentence in
the corresponding target summary y contains m words and m tences and yt denotes the t token of y
standard training minimizes the negative likelihood loss using supervised teacher forcing williams and zipser which we label lsup y m where represents the distribution among the vocabulary predicted by our model with eter
will be ignored for the following equations for simplicity

intermediate fine tuning we propose a method for ne tuning pretrained models using unsupervised wikipedia data
to create data for this intermediate ne tuning we assume knowledge of characteristics of the target dataset such as the average length of input ments the average summary length and the general bin of whether the summaries desired are very stractive or very extractive
such specications are necessary a priori so that the summarization problem is not underconstrained kryscinski et al

assume that we want a summary of m tences from source documents of n sentences on average
assume that we know approximately how well an oracle extractive model performs on the get dataset as dened as bins of extractive oracle rouge scores ranging from extremely abstractive rouge oracle more abstractive rouge oracle more extractive rouge oracle and extremely extractive rouge oracle
we then iterate the following procedure on all wikipedia articles available in a wikipedia dump we remove the rst m sentences from the wikipedia article for use as a summary
then we select the m sentences in the remaining article with the highest individual rouge scores against the pseudo summary and calculate the rouge score between those m sentences joint and the pseudo summary which amounts to a greedy upper bound of the performance of an extractive model on this example
the example will be kept if this rouge score falls into the general range of the extractive oracle of the target dataset dened previously and otherwise discarded
we use knowledge of how abstractive a dataset is as a type of summary style which an end user would know ahead of time
we lter the data points from wikipedia so that only those which fall into the bin for a given dataset are used for ne tuning
for datasets that are extremely abstractive such examples may be hard to nd so we remove high rouge sentences from the input until the desired rouge oracle score is reached
from here on we refer to data created through this process as wikitransfer
we then ne tune a trained model on this dataset specic wikitransfer data to transfer to a target domain

data augmentation via round trip translation in addition to ne tuning on wikitransfer data for zero shot domain transfer we test the ability of our model to transfer when we have few examples and whether data augmentation further improves these results
in few shot ne tuning we conduct data augmentation to reduce brute force memorization and introduce a regularization effect
specically we perform round trip translation yu et al
to generate paraphrases of both the source ments and summaries
given a dataset of size n we translate the source and target sentence wise into a non english language and keep the top k beam hypotheses from beam search as output
we then do likewise for the backtranslation to english
this results in n n total data points

data augmentation consistency while data augmentation may introduce a ization effect naively training with augmented data does not necessarily account for noise introduced in the augmented examples
to balance learning from the examples while not overtting to the small number of supervised samples the model must learn to be robust to small changes in input amples
we thus investigate the effect of using a consistency loss for few shot training by building off of ideas in unsupervised data augmentation uda xie et al

in our formulation the output distribution given an augmented example should not diverge much from the distribution given the original document with teacher forcing so that the model learns to be resilient to small tions
let x be a paraphrase of input document generated via round trip translation as described in the previous section
in addition to the vised loss y we introduce another loss x y m x where kl is the kl divergence which penalizes the loss if the probability distribution of the output using the original input is far from the distribution using the round trip translated input document
as in xie et al
the gradient does not agate through the model for the distribution of the original input while it does propagate through to the round trip translated input
as a result the total loss for training with consistency is x y y y we note that the original formulation of uda forces consistency in a semi supervised framework
we also experimented with this setup using beled examples from the target dataset with pseudo labels for teacher forcing generated by a model trained on the associated few shot subset although this approach is very sensitive to the quality of the pseudo labels see appendix
experimental settings in this section we describe our experimental tings for data usage intermediate netuning as well as zero shot and few shot domain transfer
datasets we experiment with four datasets ndm xsum narayan et al
reddit tifu reddit kim et al
and bigpatent sharma et al

the datasets were chosen as they all differ in their abstractiveness output length ing from one sentence in xsum to on average four sentences in bigpatent and cover multiple mains from news cnndm and xsum to social media reddit to patent documents bigpatent to show the generalizability of our results
model selection and metric for the ments which follow we rst choose the model with the best zero shot performance on a given domain
we test the zero shot performance from all four domains onto every other domain
for els from our wikitransfer subset we choose the best model based on performance on an vised validation subset
we found that ne tuning the model longer did not result in performance gains in few shot transfer and the checkpoints sen were typically ne tuned from to epochs
results from ablation studies for the fer subset are shown on the validation set of that given target dataset
unless otherwise stated all results reported are and rouge l
we run all few shot transfer ments on ve subsets of supervised data as we found results may vary from run to run and the reported numbers unless zero shot are the average of the top three results of the ve runs
the data point sets are subsets of the data point sets
data augmentation parameters for data mentation via round trip translation we use a beam size of and of on german and russian translation models fairseq provides bidirectional pretrained translation models edunov et al
from ng et al
for these language pairs
for both and data points we use a k of resulting in and total data points
we label the model ne tuned on these settings aug and aug
for consistency loss we make use of the same augmented data
model hyperparameters we use the fairseq codebase ott et al
for our experiments
our base abstractive text summarization model is bart lewis et al
a pretrained denoising coder that builds off of the sequence to sequence transformer of vaswani et al

we tune bart using a polynomial decay learning rate scheduler using the adam optimizer kingma and ba
we mainly vary the learning rate uler warm up updates and total updates
as in the previous few shot summarization work zhang et al
and work in unsupervised machine translation lample and conneau we make use of the validation set for early stopping based on the validation loss
we used the following learning rates warmup updates and total parameters based on an examination of the validation curves in initial experiments aug aug
for consistency loss experiments we use the value of
for experiments with data points and of
for experiments with data points
see the appendix for additional training details
zero shot transfer results in this section we compare transferring from a bart model ne tuned on wikitransfer data to one transferred from summarization datasets in terms of zero shot performance
we also show ablations of different choices for wikitransfer tuning data across the cnndm and xsum datasets
each of the datasets falls into a different extractive bin ranging from the most extractive cnndm dataset to the more abstractive xsum we discuss these settings further in the appendix

zero shot transfer comparison we ne tune bart on wikitransfer data for each of the four datasets described above and also tune a model on the fully supervised datasets
we compare the zero shot performance of transferring from wikitransfer against the best zero shot fer performance from another dataset as well as the current state of the art fully supervised results in table
we see that zero shot transfer from wikitransfer data outperforms transfer from other datasets in terms of
we also mented with training a model on data combined from multiple datasets we leave one dataset out and train on the others and then test in a zero shot setting
this setting however did not give proved results except for bigpatent transfer where the zero shot transfer increases to
from
in still lower than our wikitransfer model so for the experiments which follow we use the best performing single domain transfer model
note that the difference in performance for our pervised bigpatent results from bart likely is due to differences in capacity and training batch size when compared to the current state of the art gasus large model zhang et al

our result on bigpatent is comparable to that of the pegasus base model zhang et al

additionally in table we compare the shot performance of our model to the state of art unsupervised abstractive model on cnndm as this dataset has seen the most comparison in pervised summarization literature of the datasets in our study
we outperform the recently introduced ted model yang et al
which was cally motivated for the news domain showing the generalizability of our approach

ablation studies on wikitransfer data we conduct ablation studies to determine what fect the characteristics of our intermediate tuning data have on downstream zero shot mance
we perform these ablation studies on ndm and xsum to show the effect on both ends of the extractive and abstractive dataset spectrum
target dataset sota full dataset wikitransfer






cnndm xsum reddit bigpatent



transfer best
from reddit
from reddit
from cnndm
from cnndm table comparison of zero shot fer performance from dataset specic wikitransfer vs
transfer from another dataset
the best performing zero shot model is shown the right column in these s
we show the state of the art supervised formance on that dataset and in parentheses the mance of our bart model trained on that dataset
model wikitransfer ted yang et al
l





table a comparison of our approach to the pervised pretraining of yang et al
showing the superior performance and generalizability of our proach versus the ted model which focused on only the news domain
ablation no bin bin bin cnndm














xsum














table ablation studies on the effect of learning rate the use of extractive bin for data ltering and the choice of m in intermediate ne tuning on rouge mance on cnndm and xsum validation sets
effect of learning rate in intermediate tuning we examine the extent to which overtting to the unsupervised wikitransfer data occurs by examining the effect of the learning rate in diate ne tuning on zero shot transfer performance
we netune the models on the cnndm and xsum wikitransfer data respectively each with a mum learning rate of and
results are shown in table
using a smaller learning rate in intermediate ne tuning improves results on ndm but not on xsum likely due to the simple extractive and lead bias objective which can ily overt during ne tuning as opposed to the abstractive object in the xsum wikitransfer data
we see a similar trend with the effect of dataset size
for datasets other than cnndm we use a learning rate of in intermediate ne tuning
effect of extractive oracle bin use and the choice of m we tested whether using the tive bin to lter examples in the unsupervised data affected zero shot transfer
for this ablation periment we used the rst m sentences from the wikipedia article as the summary and the ing n as the source but do not lter examples according to how extractive they are
from table we see that the extractive bin has a very noticeable effect on transfer results for xsum and a ate effect on cnndm
this is to be expected as the model otherwise is missing information about xsum s distinctive output style
we examined how the choice of m affected formance
we set m for cnndm and m for xsum and ltered examples in a similar way based on the extractive bin of the target dataset
we see that the choice of m has a large impact on cnndm performance but no decrease on xsum
this result combined with the effect of ltering examples based on extractive bin gives insight into the importance of the subaspect of abstractiveness over compression for xsum performance
effect of intermediate pretraining dataset size we examined the effect of the size of the transfer data on downstream performance
for this experiment we take a single subset of vised data points as validation data and then vary the amount of data used for training for and examples
results are shown in ble
we see a general increase with the addition of more data although smaller increases after data points and even a decrease in on xsum likely due to noise variation
when compared to xsum we see that the performance with data points on cnndm is already much closer to the best performance
we believe that this is due to the highly extractive nature of cnndm
such an objective is especially easy for a model such as bart to learn as it is pretrained as a denoising autoencoder
for xsum we see a noticeable provement from to examples
we suspect that the abstractive objective is harder for the model to learn with small datasets
as we add more amples we do not see a noticeable improvement
such observations agree with our observation of the effect of learning rate and the cnndm objective being easier for the model to overt to and learn
for the remaining experiments we use data points since this worked well in initial experiments
effect of summary sentence choice the rst m sentences of a given wikipedia article were chosen as this introduction intuitively form a coherent mary of the article
we examine the effect of ing the rst sentences compared to choosing based intermediate dataset size cnndm











xsum











table a comparison of the effect of dataset size of the unsupervised intermediate ne tuning data on the zero shot transfer rouge performance
target dataset cnndm xsum first m sents





ind orig





ind orig p





table a comparison of the effect of summary tence choice for wikitransfer on zero shot transfer
on other criteria
as an alternative we pick the sentences with the highest self rouge rouge score of a sentence when using all other sentences as the reference summary in a greedy fashion the equivalent of the ind orig settings in zhang et al

as in zhang et al
we use for this setting
the sentences sen under this heuristic consistently corresponded to those which were longest and the resulting maries were hence longer
thus we also mented with choosing important sentences by using precision ind orig p
the ison of these methods is shown in table
we see that the choice of the summary sentence has a noticeable impact on performance
we esize that the coherence lost in the summaries is especially important for the longer cnndm maries
using important sentences other than the rst sentence likely adds more diversity in the data and nding a balance between coherence and put style is an interesting direction for additional work christensen et al

effect of lead bias on cnndm ne tuning we examined the effect of selecting the m sentences greedily chosen for calculating the extractive oracle and inserting them at the beginning of the vised source document versus leaving them in place for cnndm
this insertion is meant to mirror the lead bias present in the dataset
this had a slight impact on performance
vs
without this bias and thus we keep the lead bias
wikipedia vs target domain unlabeled data while wikipedia is a natural source of unlabeled data we tested whether creating unsupervised data from unlabeled in domain data improved results
we performed the same dataset creation treating the source data of the target domain as we did the wikipedia data
this resulted in about ples for cnndm and examples for xsum
fine tuning on this data however resulted in a performance of about
vs
on transfer data for cnndm and
vs
on wikitransfer data for xsum
the removal of the rst sentences may remove too much mation in the case of cnndm while for xsum which already has an initial sentence headline moved as the summary the rst sentence may not constitute a very good summary of the remainder of the document
wikipedia data often contains multi paragraph introductions thus the removal of the rst few sentences may still leave a structured document with coherent informative tent placed at the front
this result supports the emphasis on learning the subaspects of the target domain over simply in domain training
an ysis of the output of intermediate ne tuning on cnndm revealed that the output was more tive due to information present in the summary not being directly stated in the source than ne tuning on wikipedia
we also experimented with further in domain pretraining of the denoising autoencoder objective before zero shot transfer but this did not result in consistent improvements across datasets
few shot transfer results we examine whether the improvements in shot transfer also carry over to the few shot setting as well as the effect of data augmentation niques
the results of our experiments with varying training data sizes and augmentation methods for all datasets are shown in table and appendix
and shot performance with round trip translation augmentation we see that in shot settings without data augmentation or tency training our model outperforms transferring from another domain or vanilla bart
we see in transfer to reddit that despite similar zero shot formance with transfer from cnndm there is a more sizeable gap with shot transfer which gests that our intermediate ne tuning does more closely align the bart model with the target main
furthermore when training on augmented data from round trip translation we see the best performance in transfer from wikitransfer in all cases except bart transfer to cnndm on aug which is likely due to the autoencoder pretraining objective of bart which biases it towards ing and lead bias allowing it to perform well in applications to cnndm
we see improvements target dataset transfer from aug cons aug cons target dataset transfer from aug cons aug cons target dataset transfer from aug cons aug cons target dataset transfer from aug cons aug cons wikitransfer




















wikitransfer




















wikitransfer




















wikitransfer




















cnndm reddit




















xsum reddit




















reddit cnndm




















bigpatent cnndm




















bart




















bart




















bart




















bart




















table a comparison of transfer results across training dataset size data augmentatation datasets techniques showing the generalizable and robust formance of our models transferred from wikitransfer
when training with augmented data in example cases and example cases for wikitransfer
less improvement is seen in the aug setting when transferring from bart or another domain
we hypothesize that the noise present in the larger augmented dataset causes this occasional mance drop while the wikitransfer models appear more robust to potential noise
interestingly for transfer from bart and another domain aug only improves on cnndm the most extractive dataset while the largest drop in performance from augmented data occurs on xsum
this xsum formance drop may be caused by the high pression in the xsum summaries which leaves less room for noisy output when compared to the longer cnndm and bigpatent summaries which may still preserve the main meaning of the original summary better despite backtranslation noise
in cases aug with wikitransfer results in the best formance only several points away from the of the art supervised performance in table
transfer with consistency training we nd contrasting trends with the added consistency loss target dataset cnndm xsum relevance consistency relevance consistency aug aug full supervision















table summary relevance and factual consistency across cnndm and xsum datasets with varying amounts of training data
all results except those with an asterisks do not differ in a statistically signicant way p value of
from the full supervision score
compared to data augmentation via round trip lation
we note the most sizeable improvements in the more abstractive cases of xsum and reddit
we hypothesize that the consistency loss promotes better abstraction as the model learns to be ant to noise which does not change the meaning of the text and is thus equipped with a better tion of paraphrasing
the consistency loss allows for better training of vanilla bart as well as in general better transfer from other domains than without consistency loss
the loss likely provides a regularization factor which prevents the models from overtting to the supervised examples
as the wikitransfer model is already more closely tuned to the target domain this regularization may not make as large of a difference
this aligns with our observation of wikitransfer models being more robust to noisy backtranslated data on xsum and reddit
transfer to reddit shows similar results across models for consistency loss with amples better rouge l for wikitransfer better for reddit vanilla bart s strong performance at examples suggests that the formation provided in this subset is sufcient for good performance thus diminishing the gains from the head start the wikitransfer model provides in zero and shot transfer
we leave aspects of the consistency training such as the role of the quality of the round trip translation data and its relation to the transfer domain to future work

human quality assessment we examine how the improved performance from wikitransfer manifests itself in qualitative tations when varying the amount of training data
we collect human judgment annotations for two of the four quality dimensions studied in kryscinski et al
fabbri et al
namely tency and relevance
consistency is dened as the factual alignment between the summary and the summarized source text while relevance is dened as the selection of important content only relevant information should be included in the summary
we did not include uency as a dimension as an initial inspection of the data found uency to be of very high quality and we did not include ence due to our inclusion of single sentence xsum summaries where coherence is not a factor
we domly select examples per dataset and collect the model output from the best performing shot aug aug and fully supervised models on cnndm and xsum
the annotator sees the source article and randomly ordered output from the four models rates the summaries for relevance and consistency on a likert from with being the best score
we averaged the score of two tive english speaking annotators on each example and then across examples and found moderate and strong annotator correlations for relevance and sistency respectively
results are shown in table
for cnndm we see an increase in consistency as more training data is added but not a statistically signicant difference using a student s t test with a value of
between and full supervision for any of the relevance or consistency results
we see that the relevance of the full model does not outperform the others likely because the model output was more concise and was judged as not including source information while the zero shot output more closely resembles the lead three bias and is longer so was judged as more informative
for xsum we see that relevance improves ably as more training data is used
we see varied sults for consistency although without statistically signicant differences
this uctuation in scores may be due to the transition of the model from ing knowledge from pretraining in its output versus knowledge from the target dataset obtained during ne tuning which we discuss in the appendix
conclusion we show improved performance when ne tuning pretrained models on dataset specic unsupervised data on both zero and few shot transfer ments
we also demonstrate the benets and backs of data augmentation and consistency ing techniques
for future work we plan to porate additional subaspects into the intermediate ne tuning data such as redundancy diversity and more explicitly token overlap
we also plan to pand our experiments across more domains and compare other pretrained models for transfer
references christos baziotis ion androutsopoulos ioannis konstas and alexandros potamianos

differentiable sequence to sequence to sequence autoencoder for unsupervised abstractive sentence in proceedings of the compression
ference of the association for computational linguistics human language technologies volume long and short papers pages minneapolis minnesota
association for computational linguistics
the north american chapter of arthur brazinskas mirella lapata and ivan titov
few shot learning for abstractive
document opinion summarization
in emnlp
arthur brazinskas mirella lapata and ivan titov

unsupervised opinion summarization as in proceedings of the copycat review generation
annual meeting of the association for tational linguistics pages online
sociation for computational linguistics
janara christensen mausam stephen soderland and towards coherent oren etzioni

in proceedings of the document summarization
conference of the north american chapter of the association for computational linguistics man language technologies pages lanta georgia
association for computational guistics
eric chu and peter liu

meansum a neural model for unsupervised multi document abstractive summarization
in international conference on chine learning pages
sergey edunov myle ott michael auli and david grangier

understanding back translation at scale
in conference of the association for tational linguistics acl
gunes erkan and dragomir r radev

lexrank graph based lexical centrality as salience in text summarization
journal of articial intelligence search
alexander r fabbri wojciech kryscinski bryan socher summeval arxiv mccann caiming xiong richard and dragomir radev

evaluating summarization evaluation
preprint

xinyu hua and lu wang

a pilot study of main adaptation effect for neural abstractive in proceedings of the workshop on marization
new frontiers in summarization pages copenhagen denmark
association for tional linguistics
on empirical methods in natural language ing and the international joint conference on natural language processing emnlp ijcnlp pages hong kong china
association for computational linguistics
byeongchang kim hyunwoo kim and gunhee kim

abstractive summarization of reddit posts with multi level memory networks
diederick p kingma and jimmy ba

adam a method for stochastic optimization
in international conference on learning representations iclr
wojciech kryscinski nitish shirish keskar bryan cann caiming xiong and richard socher

neural text summarization a critical evaluation
in proceedings of the conference on empirical methods in natural language processing and the international joint conference on natural guage processing emnlp ijcnlp pages hong kong china
association for tional linguistics
philippe laban andrew hsi john canny and marti a
hearst

the summary loop learning to write in abstractive summaries without examples
ceedings of the annual meeting of the ciation for computational linguistics pages online
association for computational guistics
guillaume lample and alexis conneau

lingual language model pretraining
advances in neural information processing systems neurips
logan lebanoff kaiqiang song and fei liu

adapting the neural encoder decoder framework from single to multi document summarization
in proceedings of the conference on empirical methods in natural language processing pages brussels belgium
association for computational linguistics
mike lewis yinhan liu naman goyal jan ghazvininejad abdelrahman mohamed omer levy veselin stoyanov and luke zettlemoyer

bart denoising sequence to sequence training for natural arxiv preprint lation

language generation and comprehension
mike lewis yinhan liu naman goyal jan ghazvininejad abdelrahman mohamed omer levy veselin stoyanov and luke zettlemoyer

bart denoising sequence to sequence training for natural language generation translation and comprehension
in proceedings of the nual meeting of the association for computational linguistics pages online
association for computational linguistics
taehee jung dongyeop kang lucas mentch and uard hovy

earlier is nt always better aspect analysis on corpus and system biases in marization
in proceedings of the conference peter j
liu mohammad saleh etienne pot ben goodrich ryan sepassi lukasz kaiser and noam shazeer

generating wikipedia by ing long sequences
iclr
qizhe xie zihang dai eduard hovy minh thang ong and quoc v
le

unsupervised data mentation for consistency training
arxiv preprint

ziyi yang chenguang zhu robert gmyr michael zeng xuedong huang and eric darve

ted a pretrained unsupervised summarization model with theme modeling and denoising
arxiv preprint

adams wei yu david dohan quoc le thang luong rui zhao and kai chen

fast and accurate reading comprehension by combining self attention in international conference on and convolution
learning representations
jingqing zhang yao zhao mohammad saleh and ter j
liu

pegasus pre training with tracted gap sentences for abstractive summarization
hao zheng and mirella lapata

sentence trality revisited for unsupervised summarization
in proceedings of the association for computational linguistics pages florence italy
association for tational linguistics
the annual meeting of jiawei zhou and alexander rush

simple supervised summarization by contextual matching
in proceedings of the annual meeting of the association for computational linguistics pages florence italy
association for tational linguistics
haichao zhu li dong furu wei bing qin and ting liu

transforming wikipedia into augmented data for query focused summarization
arxiv preprint

rada mihalcea and paul tarau

textrank bringing order into text
in proceedings of the conference on empirical methods in natural guage processing pages barcelona spain
association for computational linguistics
ramesh nallapati bowen zhou cicero dos santos c aglar and bing xiang

tive text summarization using sequence to sequence in proceedings of the rnns and beyond
signll conference on computational natural guage learning pages berlin germany
association for computational linguistics
shashi narayan shay b
cohen and mirella lapata

do nt give me the details just the summary topic aware convolutional neural networks for treme summarization
arxiv

nathan ng kyra yee alexei baevski myle ott michael auli and sergey edunov

facebook fair s news translation task submission
in proceedings of the fourth conference on chine translation volume shared task papers day pages florence italy
association for computational linguistics
myle ott sergey edunov alexei baevski angela fan sam gross nathan ng david grangier and fairseq a fast extensible michael auli

in proceedings of toolkit for sequence modeling
the conference of the north american ter of the association for computational linguistics demonstrations pages minneapolis nesota
association for computational linguistics
evan sandhaus

the new york times annotated corpus
linguistic data consortium philadelphia
eva sharma chen li and lu wang

bigpatent a large scale dataset for abstractive and coherent summarization
christian szegedy vincent vanhoucke sergey ioffe jon shlens and zbigniew wojna

rethinking the inception architecture for computer vision
in proceedings of the ieee conference on computer sion and pattern recognition pages
ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan n gomez ukasz kaiser and illia polosukhin

attention is all in advances in neural information you need
cessing systems pages
danqing wang pengfei liu ming zhong jie fu xipeng qiu and xuanjing huang

exploring domain shift in extractive text summarization
ronald j williams and david zipser

a ing algorithm for continually running fully recurrent neural networks
neural computation
a appendix a
comparison to previous work we show a comparison of our best performing itransfer few shot results with those from zhang et al
in table
note that the reddit dataset does not have a predened subset so this son is not exact
also the pegasus numbers were obtained by a single run as opposed to our average of the best three over subsets
we show large improvements with our few shot approach pared to previous numbers except for the shot experiment on xsum
the xsum dataset has the highest overlap with the pegasus pretraining dataset of all datasets explored in zhang et al
though that work states that the effect of removing this overlap does not affect the full dataset mance
we hope that this comparison promotes future benchmarking of few shot results
a
sample summary outputs we include an example of model output summaries on the xsum dataset in table
the example serves to demonstrate how output style varies as the amount of training data is increased and how the source of pretraining or ne tuning data affects this style and model hallucinations
the source ment does not state the rst name of ms
jones yet every model output and the gold target gives her one
for zero and aug the model outputs raine jones likely still under the inuence of bart wikipedia pretraining as there is a wikipedia ticle on the welsh politician ruth lorraine jones although it does not appear in our intermediate ne tuning subset
the zero and aug also most resemble wikipedia introduction sentences although the output is compact and abstractive like an xsum target sentence the x is y format of wikipedia appears
we see at aug examples that the model output is stylistically already much like that of the fully supervised output and gold summary
this stylistic change is also reected in the change in hallucination the use of rachel jones is likely caused by the appearance of the name of a minister rachel haves in an article on welsh tics found in the aug subset
the model at this point is already tting strongly to the target domain
for the fully supervised output we see the use of carwyn jones which does not match the gender of ms jones but which is found times in the ing source documents
caroline jones the actual person in question only appears times in the training set
this phenomenon points to two esting research directions for future work how to properly preserve world knowledge from ing and improvement faithfulness to the source text in knowing when to insert world knowledge
a
semi supervised uda experiments we experimented with the original formulation of uda in a semi supervised setting
in this work the label summary outputted by the model for an augmented example should be the same as the label of the original document on unlabeled examples
let xu be an unsupervised source ment from the target dataset other than our vised few shot examples
let xu be a paraphrase of input xu generated via round trip translation as in our above data augmentation experiments
to apply teacher forcing we require a label yu which we obtain for each model by applying the model ne tuned on the analogous few shot subset
in addition to the supervised loss y we thus introduce another loss xu yu m xu xu in practice for an epoch we iterate through the supervised examples with loss lsup followed by iterating over the unsupervised examples luda
we sampled unlabeled data points for uda periments and unlabeled data points for uda
results of initial experiments are shown in table
we nd that the performance of the uda models is very dependent on the quality of the pseudo labels generated
we chose the model trained on the rst data subset of the runs to erate the pseudo labels and if this model had higher performance then this model likely performed ter in uda
as a result as the quality of the labels improves with shot training the uda performance improves and is more comparable to the unaugmented performance in
a
additional training setting details we found that full precision oating point gave slightly better and more stable results so we port full precision oating point numbers
we set a maximum tokens per batch of and use dient accumulation with an update frequency of for all experiments with data points and for aug as well as all experiments with mented data points
for cnndm examples target dataset training samples cnndm xsum reddit bigpatent











wikitransfer



































pegasus zhang et al
























table a comparison of zero and few shot performance between our best performing wikitransfer model in the case of cnndm and bigpatent and for xsum and reddit and the zero and few shot results reported in zhang et al

source document ms jones told bbc radio wales she did not want to give up being an am to go to brussels to replace nathan gill ukip wales leader
mr gill has been told by the ukip assembly group and the ukip party chairman steve crowther to stop double jobbing as an am and mep
mr gill said those making such calls were doing it out of malice
we ve got brexit now and i think that possibly it may be best to leave that role unlled ms jones told the good morning wales programme
i m surprised i ve not been formally asked what i d like to do
ms jones the south wales west am is one of two people who could take up the role of ukip wales mep if mr gill made it vacant the other being south wales east am david rowlands


lorraine jones is a welsh labour party member of the welsh assembly for south wales west
aug lorraine jones is a welsh labour member of the welsh assembly for south wales west
aug wales assembly member for south wales west rachel jones says she has not been formally asked to become a ukip mep
full supervision first minister carwyn jones has said she is surprised she has not been asked to become a ukip mep
gold summary ukip s welsh mep post may be better left unlled as a result of brexit party am caroline jones has said
table an example of wikitransfer model output across dataset size used in ne tuning illustrating how model output style and hallucinated entities differ as the model moves from wikipedia pretraining as a source of knowledge to the target dataset
text not stated in the source document is highlighted in red
target dataset transfer from uda target dataset transfer from uda target dataset transfer from uda target dataset transfer from uda wikitransfer


cnndm reddit


bart


wikitransfer


wikitransfer


wikitransfer


xsum reddit


reddit cnndm


bigpatent cnndm


bart


bart


bart


table results from experiments using the original formulation of uda xie et al
on examples
we found it necessary to use a smaller learning rate to avoid immediate overtting
we figure and rouge l scores across datasets training dataset size data tion and consistency loss showing the eralizable and robust performance of our models ferred from wikitransfer
form validation after each model update as the models typically converge in under iterations
for the aug setting we begin validation ing after iterations as the models typically verged around iterations
we train with smoothed cross entropy szegedy et al
loss for few shot transfer
we found that models can be sensitive to the choice of hyperparameters in the few shot settings hence the averaging over subsets to reduce variation
while we did not serve large differences in preliminary experiments on varying the size of this validation set we leave the task of training without or with very little dation data for future work
we use the statistics from the original papers to determine the extractive bin of the dataset except for the case of reddit upon seeing the strong shot performance of the cnndm we investigated the extractive oracle of the reddit dataset and found it to be much higher about r than that stated in the original paper
we select the rst m sentences for the pseudo summaries from wikipedia except in the case of reddit where we choose the orig setting this did not result in a difference in zero shot performance but upon a qualitative inspection of the output we found the ind orig to be less biased towards wikipedia style with the coherence of the summaries not being an issue

