towards information rich logical text generation with knowledge enhanced neural models hao bin wei and zhiwen polytechnical university xian china research beijing china
nwpu
edu
cn guob
edu
cn
com r a m i a
s c v
v i x r a abstract text generation system has made massive ing progress contributed by deep learning niques and has been widely applied in our life
however existing end to end neural models suffer from the problem of tending to generate tive and generic text because they can not ground input context with background knowledge
in der to solve this problem many researchers begin to consider combining external knowledge in text generation systems namely knowledge enhanced text generation
the challenges of enhanced text generation including how to select the appropriate knowledge from large scale edge bases how to read and understand extracted knowledge and how to integrate knowledge into generation process
this survey gives a hensive review of knowledge enhanced text eration systems summarizes research progress to solving these challenges and proposes some open issues and research directions
introduction text generation also known as natural language generation nlg aims to make machines express like humans which has the capability to produce smooth meaningful and mative textual contents
from the original template based and statistical methods to the deep learning based methods text generation has attracted the attention of massive searchers and made many remarkable advances
depending on the data sources text generation can be divided into to text data to text and image to text generation
we focus on the text to text generation in this survey because it still has massive research challenges and a wider range of cations
text to text generation takes natural language text as input understands the input text to obtain semantic tations and generates corresponding output text according to task requirements
most advances in text generation in recent years are ted from deep neural networks such as recurrent neural work rnn elman and transformer vaswani et al
contact author
with the help of these technologies text generation systems have been able to generate smooth topic consistent and even personalized text
however existing text tion systems lack interactions with the real world and have little access to the external knowledge making them easy to generate the short and meaningless text
we humans are constantly acquiring understanding and storing knowledge and will automatically combine our knowledge to understand the current situation in communicating writing or reading which is a huge challenge faced by text generation systems
to generate more informative diverse and logical text text generation systems must have the ability to combine external knowledge which is a promising research direction
fig
is an example of a dialogue system with or without sense knowledge where we can see that combining with monsense knowledge including structured knowledge graph kg composed of triples and unstructured knowledge base kb composed of natural language text the dialogue agent can generate more informative diverse and logical response
there are many researchers from both academia and dustry have begun to explore knowledge enhanced text eration system by incorporating different types of knowledge
due to the complexity of the real world the scale of edge base is usually extremely large
how to extract the most relevant knowledge from massive knowledge facts based on the simple text input is a huge challenge because of the sity of natural languages
meanwhile how to effectively derstand the extracted knowledge and integrate it into neural network models to facilitate text generation is also a difcult problem
to our best knowledge this is the rst survey to summarize knowledge enhanced text generation systems in detail
to sum up we summarize contributions of our work as follows
we briey introduce the development process of text generation formalize the denition of enhanced text generation and analyze a number of key challenges in this research area
we summarize the current in knowledge enhanced text generation systems by tematically categorizing the state of the art works cording to research challenges
research progress we propose some open issues and research directions for the reference of the community
figure two dialogue examples with the rst line or combining external knowledge background and formalized denition in this section we briey introduce the development of text generation formalize the denition of general and knowledge enhanced text generation and summarize some research challenges

nnlm and rnnlm the neural network language model nnlm bengio et al
is rstly proposed for text generation tasks which leverages neural networks to model language representation
nnlm maps the input into a low dimensional space thereby reducing the parameters of the model
given the text quence sn


and a parametrized language model p nnlm can be proximated as eq
where t represents the t th word in sn
p p


nnlm has achieved promising results in experiments but as a typical feedforward neural network the length of the text it receives must be xed in advance so it can not capture context information of variable length
to solve this problem the rnn language model rnnlm mikolov et al
is proposed which is an auto regressive language model that utilizes rnn to encode variable length inputs into vector resentations
this process can be formulated as eq

p p n


theoretically rnnlm can model text sequences of any length
however due to the xed dimension hidden vector the information in excessively long context may not be stored efciently
therefore variants of rnn including long term memory lstm and gated recurrent unit gru have been utilized to improve the performance of rnnlm which perform well in capturing long term dependencies

encoder decoder framework the length of input and output text of above language models are equal but there are many cases where the length of them are different e

the question and answer in the qa system
to deal with this problem the encoder decoder framework sutskever et al
is proposed composed of an encoder and a decoder
the encoder uses rnn to encode the input sequence x


xm into the intermediate tics representation c where m is the length of input sequence
the decoder utilizes another rnn to generate the t th output word yt according to c and



this process can be dened as eq
where y


xn and n is the length of the output text sequence
y t t encoder decoder is a very general computing framework whose specic model in the encoder and decoder can be justed based on tasks
it has been widely used in various text generation tasks since proposed such as machine translation text summarization and dialogue system

knowledge enhanced text generation in addition to the encoder decoder framework various ral network models and algorithms have been applied to text generation tasks to improve the quality of generated text
the development of text generation systems requires the ated text to be more informative diverse and logical rather than simple smooth or surface correct
combined with ternal knowledge text generation systems can deeply stand the input and generate more informative more sistent with the logic of human expression and with less common sense mistakes
given a set of knowledge facts f


k where each fi may be a text sequence or a knowledge triple and k is the number of facts the knowledge types challenges structured kg representing knowledge graphs with vectors incorporating knowledge vectors into models unstructured kb extracting knowledge reading and understanding knowledge existing solutions word embedding based knowledge representation wang et al
distance based knowledge representation gunel et al
graph attention based knowledge representation guan et al
concatenated with input vector liu et al
attention based knowledge graph decoder qiu al
gcn based knowledge incorporation de cao et al
keyword matching ghazvininejad et al
semantical level knowledge extraction lian et al
memory network based knowledge understanding madotto et al
transformer based knowledge understanding kim et al
rl based knowledge understanding xu et al
table a summary of challenges in knowledge enhanced text generation system knowledge enhanced text generation model can be lated as eq

f f y t
t there are two forms of external knowledge that is tured kg and unstructured kb
the kg is essentially a mantic network containing multiple types of entities and lations with the form of head relation tile where head and tile are different entities
entities refer to things in the real world and relations express connections between entities
the kb has no xed form and usually store edge related to specic concepts in textual sequence form
there are many challenges in combining external knowledge into text generation systems as shown in table
when combining structured knowledge the rst challenge is how to obtain vector representations of knowledge triples as ceptable inputs to neural network models
neural network models need input data with vector form while the tion stored in structured kb is symbolized
it is a difcult problem to map these symbols into low dimensional dense vector spaces
and how to incorporate knowledge vectors as additional input into neural network models to guide the eration process is also a challenge
because knowledge facts in the unstructured kb are stored with the form of natural language text mapping knowledge facts to vector tions does not pose a research challenge
the rst challenge in combining unstructured knowledge is how to extract the most appropriate knowledge from massive knowledge facts due to the possible semantic duplication of different edge
the understanding of sentence level natural language text is a long term research challenge in nlp so how to ciently read and understand textual knowledge facts to tegrate them into generation systems is another challenge in combining unstructured knowledge
researchers have made great efforts to address these challenges which will be tailed summarized in following sections
text generation with structured kg structured kgs can store a wider range of knowledge types but less information due to its simple representation of triples
however its unique symbolic storage form is quite different from vectors required by neural network models
therefore how to map knowledge triples into low dimensional vector representations and efciently incorporate knowledge vectors into neural network models are key directions of the research which will be summarized in this section

word embedding based knowledge representation the simplest way to obtain vector representations of tured kbs is to directly treat entities and relations in edge triples as common words and then use word embedding methods to obtain vector representations which has been widely used at the initial research stage
in order to comprehensive understand the content of ument in reading comprehension tasks mihaylov et al
haylov and frank utilize a bigru to encode edge triples as text sequences to get the key value memory
and then the key value retrieval algorithm selects a single sum of weighted fact representations for each token to hance the understanding of the document context
wang et al
wang et al
propose a kb based single relation qa system
the entity linking module determines the optimal subject in the question to select knowledge facts which will be encoded into vectors using a bilstm
the relation tion module calculates similarity scores of each question and its relation candidates to select the triple with highest score to answer the question

distance based knowledge representation there is a gap between kg of symbolic form and vector resentation so directly encoding entities as common words may lead to certain information loss
the concept of edge representation learning is proposed to represente entities and relations in low dimensional dense vector spaces for culation and reasoning
bordes et al
bordes et al
propose the transe algorithm which uses the translation variant phenomenon of word vector and distance based ing function to obtain vector representations of entities and relations which can be better integrated with text generation system to provide more powerful knowledge support
moussallem et al
moussallem et al
incorporate external knowledge into machine translation system to prove the quality of results
knowledge facts are linked based on the translated document and encoded by the modied trane and then concatenated vectors into the internal tors of nmt embeddings as the input of the decoder
gune et al
gunel et al
incorporate entity level knowledge from knowledge graph into transformer encoder decoder chitecture to produce coherent summaries
extracted entities are initialized with the pretrained algorithm to get the vector representations and then are fed into separate head attention channel for generating summaries

graph attention based knowledge representation to obtain more accurate vector representations the graph tention algorithm zhou et al
is proposed which uses relation information to aggregate entities to generate new tity representations
the attention mechanism makes better use of the interconnections between graph entities and guishes the hierarchy of connections which can enhance the effective information needed in text generation tasks
to generate more informative responses zhou et al
zhou et al
propose the static graph attention mechanism to generate a static representation for a retrieved graph to augment the semantics of input words
the dynamic graph attention mechanism is designed for attentively reading all knowledge triples for text generation
guan et al
guan et al
propose an incremental encoding scheme for story ending generation to mine context clues hidden in the story context and adopt graph attention and contextual attention respectively to obtain the graph vectors
the multi source attention mechanism combines commonsense knowledge for facilitating story comprehension to generate coherent and sonable story endings
for comprehensive understanding paragraph level ment qiu et al
qiu al
construct sub graphs for entities to capture the structural information in the kb
resentations of nodes in a sub graph are updated using the graph attention network with the document context which are combined with document representations to generate nal answer

concatenating knowledge with input vector after vector representations of knowledge triples are tained the next challenge facing by knowledge enhanced text generation systems is how to integrate knowledge vectors into neural network models
the simplest method is to directly concatenate knowledge vectors with input vectors to enhance vector representations of the input and then send them into the decoding stage for text generation
for instance young et al
young et al
extract triples according to entity keys in the input to form a text sequence which is encoded by lstm to obtain vector sentations
then knowledge vectors are added with the input vector to calculate the degree of correlation with the tive responses
liu et al
liu et al
propose the idea of entity diffusion which means that conversation usually drifts from one entity to another
the similarity between extracted entities and other entities is calculated to retrieve relevant tities
word vectors of entities and relations are averaged to obtain vector representations of each triple which is nated with the input vector to guide the response generation

attention based knowledge graph decoder the attention mechanism can focus on the important contents among the numerous input information and select the key formation while ignoring other unimportant
through the tention mechanism knowledge enhanced text generation tems can focus on most critical parts of knowledge instead of feeding all the selected knowledge directly into neural works to produce more informative text
for example moon et al
moon et al
construct kg embeddings to represent entities with algorithm and aggregate input contexts with relevant entities
to ate candidate kg entities efciently an attention based graph decoder is proposed to walk an optimal path in a large kg to select candidate entities
for paragraph level essay generation yang et al
yang et al
present a memory augmented neural model to bine commonsense knowledge
knowledge concepts are tracted using input topics as query and stored into a memory matrix
the model will attend on the memory and ically update it to incorporate information of the generated text for diverse and topic consistent essay generation
cel et al
koncel kedziorski et al
introduce a graph transforming encoder to leverage the relational structure of knowledge graphs to encode knowledge graphs into vectors and then the decoder will attend on the input title and edge graphs to generate informative and topic coherent text

gcn based knowledge incorporating gcn kipf and welling is a natural extension of cnn in the graph domain which learns node feature and structure information in the end to end manner
it is a very powerful neural network framework on graphs so it has begun to attract researchers attention in text generation systems combining structured knowledge graphs
de et al
de cao et al
consider question answering as an inference problem on a graph of the document tion
nodes in the graph are entities appeared in the ment and edges in the graph represent the relations between entities
the gcn is used to capture reasoning chains by propagating local contextual information along edges to form multi step reasoning for generating answers
lv et al
et al
extract evidence from knowledge graph and make predictions based on the evidence
the graph based contextual word representation learning module is used to dene the distance between words for learning better textual word representations using graph structural tion
the graph based inference module is applied to encode neighbor information into the representations of nodes using gcn and aggregate evidence to generate answers
text generation with unstructured kb unstructured kbs are composed of natural language text lated to concepts which express rich semantic information
because of its textual form the unstructured kb can be ily combined with text generation systems whose input is text sequences
however the scale of knowledge base is usually extremely huge which contains too redundant information
therefore how to extract the knowledge required by text generation systems and efciently understand the knowledge to integrate it into the generation process are main research challenges
there have been many researches of grounded text generation with unstructured kb which will be discussed in detail in this section

key word matching based knowledge extraction the simplest way to extract knowledge from unstructured kb is the key matching method using words in the input as keywords
this method is simple and direct but can only extract knowledge according to the surface information of words and can not combine deeper semantic information into the knowledge extraction
for instance ghazvininejad et al
ghazvininejad et al
rstly introduce external knowledge into the fully driven neural conversation model
given the dialogue history relevant knowledge facts are identied by keyword matching method using entities in the context as keys
then retrieved know facts are fed into the memory network to retrieve and weight facts based on the input and dialogue context to hance the semantic representation of the input

semantical level knowledge extraction the simple key word matching method may make it hard to accurately select the required knowledge due to the less information contained in single word
therefore many searchers focus on the knowledge selection in the semantic level and put forward many novel ideas
the same query in human conversation may be related to different responses so different knowledge may be utilized
lian et al
to solve this problem lian et al
pose the idea of the posterior distribution over knowledge which is calculated from both the input query and response to provide more accurate guidance on knowledge selection
by minimizing the distance between the prior and the posterior distribution over knowledge the prior distribution can be lized to select appropriate knowledge so as to generate mative responses even the actual response is unknown
ren et al
ren et al
propose a global to local edge selection mechanism using the global perspective to select appropriate background knowledge
a topic tion vector is learned from the dialogue context and external knowledge by a distantly supervised learning schema to lect the most likely text fragments
the vector is then used to guide the local knowledge selection module at decoding stage to generate uency and appropriate responses
zhao et al
zhao et al
represent a disentangled response coder to separate parameters relying on knowledge grounded dialogues from the whole model to solve the problem of ing knowledge grounded training data
the decoder is posed of three components including language model to generate common words context processor to generate text words and knowledge processor to generate words from knowledge document by a hierarchical attention mechanism

memory network based knowledge understanding after extracting relevant knowledge facts the most import is to read and understand the textual knowledge to enhance the input representation and guide text generation
memory work sukhbaatar et al
is proposed to improve the poor memory ability of rnn using external memory ponent to realize the storage of long term memory
because of its powerful memory storage capacity memory network is widely used in knowledge enhanced text generation systems to retrieve read and condition on external knowledge
madott et al
madotto et al
augment the memory network with a sequential generative architecture
edge facts are fed into memory network to update the input query vector
a gru is used as a dynamic query generator to generate the output words which will produce two tion to decide whether to generate common words or memory contents
in order to carefully read and understand the trieved knowledge dinan et al
dinan et al
combine the memory network and transformer to encode the selected knowledge and the dialogue context to get the higher level semantic representation
then the dot product attention tween the knowledge and context is performed to retrieved most relevant knowledge for generating the next response

transformer based knowledge understanding transformer is an emerging sequential model which has caused great repercussions in nlp
it exceeds rnn in tic information abstraction long term feature extraction and task comprehensive feature representation
many researches have begun to use transformer to read and attend on external knowledge in text generation systems
for example zhao et al
zhao et al
make use of multi head attention mechanism in transformer to encode the dialogue context response candidate and the relevant ment
through the hierarchical interaction in the context and document the importance of different parts of the document and context is determined to select the most appropriate sponse
li et al
li et al
employ the multi head tention to get the vector representation of external knowledge and input
the incremental transformer incorporates the tor representation of knowledge and context into the encoding process to encode knowledge utterances span in the turn dialogue
the decoder contains two processes where the rst pass focuses on contextual coherence and the pass renes the results of the rst pass by attending on the knowledge to increase the knowledge relevance and ness
kim et al
kim et al
propose a sequential latent model which sequentially conditions on previously selected knowledge to produce informative responses
the input terance and knowledge sentences are encoded into vectors and then model the knowledge selection as latent variables to joint inference knowledge selection of multi turn dialogue

rl based knowledge understanding reinforcement learning is an subeld of machine learning that emphasizes how to act based on the state to maximize the expected rewards
through continuously interacting with the environment rl can use the rewards and punishments given by the environment to continuously improve the strategy that is what kind of actions to take in what kind of state for imum cumulative rewards
rl is actually very close to the way of human thinking which is why it is likely to become the future general articial intelligence paradigm
based on deep q network dqn a typical rl algorithm xu et al
xu et al
propose the knowledge routed dqn to age topic transitions during the dialogue
the relational nement branch encodes relations among different symptoms and the knowledge routed graph branch decides policy in rl under different medical knowledge
the two branches ensure that the dialogue manager the agent interacting with the vironment can make more reasonable decisions from edge guiding and relation encoding
conclusion and future directions this survey makes a systematic literature review of the search trends of knowledge enhanced text generation
with the help of external knowledge text generation system can understand input text more deeply and comprehensively and generate more informative text which is a very promising search direction
as an emerging research direction there are many open issues in the research of knowledge enhanced text generation system which will be briey discussed here

combining structured and unstructured knowledge at present researches mainly focus on incorporating one form of external knowledge
if the two forms of knowledge are combined more appropriately and informatively text may be generated
structured kg can narrow down knowledge candidates using the prior information such as entities and graph paths
unstructured kb can provide abundant mation to enhance text generation but we need strong pability of natural language understanding to select useful information
both forms of knowledge have their own vantages and disadvantages
due to their structural ences it is a challenging research direction to combine the structured and unstructured knowledge into generation tems which will certainly bring promising progress to the knowledge enhanced text generation system

lifelong learning we humans continuously learn new knowledge update our knowledge base to adapt to the fast changing pace of society
however existing text generation systems mostly utilize xed knowledge bases whose knowledge do not keep updating in real time
to make text generation models more morphic they should have the ability of continuous lifelong learning
a meaningful exploration of this is discussed by mazumder et al

they propose mazumder et al
the lifelong interactive learning and inference model which will actively ask users questions when encountering unknown concepts and update its knowledge base after corresponding answers are reached
how to continuously obtain information from numerous external inputs and achieve lifelong learning is a important research direction in text generation
acknowledgments this work was supported by the national key program of china and the national natural ence foundation of china no

references bengio et al
yoshua bengio rejean ducharme pascal vincent and christian jauvin
a neural tic language model
journal of machine learning research
bordes al
antoine bordes nicolas usunier and oksana alberto garcia duran yakhnenko
translating embeddings for modeling multi relational data
in advances in neural information processing systems pages
jason weston de cao et al
nicola de cao wilker aziz and ivan titov
question answering by reasoning across documents in proceedings of with graph convolutional networks
naacl hlt pages
dinan et al
emily dinan stephen roller kurt shuster angela fan michael auli and jason weston
wizard of wikipedia knowledge powered conversational agents
in international conference on learning sentations
elman jeffrey l elman
finding structure in time
cognitive science
ghazvininejad et al
marjan ghazvininejad chris brockett ming wei chang bill dolan jianfeng gao wen tau yih and michel galley
a knowledge grounded neural conversation model
in thirty second aaai ference on articial intelligence
guan et al
jian guan yansen wang and minlie huang
story ending generation with incremental ing and commonsense knowledge
in proceedings of the aaai conference on articial intelligence volume pages
gunel et al
beliz gunel chenguang zhu michael zeng and xuedong huang
mind the facts boosted coherent abstractive text summarization
in neurips
kim et al
byeongchang kim jaewoo ahn and gunhee kim
sequential latent knowledge selection for in international knowledge grounded dialogue
ence on learning representations
kipf and welling thomas n kipf and max welling
semi supervised classication with graph convolutional networks
arxiv preprint

koncel kedziorski et al
rik koncel kedziorski dhanush bekal yi luan mirella lapata and hannaneh hajishirzi
text generation from knowledge graphs with in proceedings of naacl hlt graph transformers
pages
et al
zekang li cheng niu fandong meng yang feng qian li and jie zhou
incremental former with deliberation decoder for document grounded conversations
in proceedings of the annual meeting of the association for computational linguistics pages
lian et al
rongzhong lian min xie fan wang jinhua peng and hua wu
learning to select knowledge for response generation in dialog systems
in proceedings of the international joint conference on articial telligence pages
aaai press
liu et al
shuman liu hongshen chen zhaochun ren yang feng qun liu and dawei yin
knowledge diffusion for neural dialogue generation
in proceedings of the annual meeting of the association for tational linguistics volume long papers pages
et al
shangwen lv daya guo jingjing xu duyu tang nan duan ming gong linjun shou daxin jiang guihong cao and songlin hu
based reasoning over heterogeneous external knowledge arxiv preprint for commonsense question answering


madotto et al
andrea madotto chien sheng wu and pascale fung
effectively incorporating knowledge bases into end to end task oriented dialog tems
in proceedings of the annual meeting of the sociation for computational linguistics volume long papers pages
mazumder et al
sahisnu mazumder nianzu ma and bing liu
towards a continuous knowledge ing engine for chatbots
arxiv preprint

knowledgeable reader mihaylov and frank todor mihaylov and anette enhancing frank
style reading comprehension with external commonsense knowledge
in proceedings of the annual meeting of the association for computational linguistics volume long papers pages
mikolov et al
tomas mikolov martin karaat lukas burget jan and sanjeev khudanpur
current neural network based language model
in eleventh annual conference of the international speech cation association
moon et al
seungwhan moon pararth shah anuj kumar and rajen subba
opendialkg explainable conversational reasoning with attention based walks over in proceedings of the annual knowledge graphs
meeting of the association for computational linguistics pages
et al
diego moussallem mihael arcan axel cyrille ngonga ngomo and paul buitelaar
augmenting neural machine translation with knowledge graphs
arxiv preprint

qiu al
delai qiu yuanzhe zhang xinwei feng xiangwen liao wenbin jiang yajuan lyu kang liu and jun zhao
machine reading comprehension using in proceedings tural knowledge graph aware network
of the conference on empirical methods in ral language processing and the international joint conference on natural language processing ijcnlp pages
ren et al
pengjie ren zhumin chen christof monz jun ma and maarten de rijke
thinking globally acting locally distantly supervised global to local edge selection for background based conversation
arxiv preprint

sukhbaatar et al
sainbayar sukhbaatar jason ston rob fergus al
end to end memory networks
in advances in neural information processing systems pages
sutskever et al
ilya sutskever oriol vinyals and quoc v le
sequence to sequence learning with neural networks
in advances in neural information processing systems pages
vaswani et al
ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan n gomez ukasz kaiser and illia polosukhin
attention is all you need
in advances in neural information processing tems pages
wang et al
run ze wang zhen hua ling and yu hu
knowledge base question answering with ieee access tive pooling for question representation

xu et al
lin xu qixian zhou ke gong dan liang jianheng tang and liang lin
end to end knowledge routed relational dialogue system for matic diagnosis
in proceedings of the aaai conference on articial intelligence volume pages
yang et al
an yang quan wang jing liu kai liu yajuan lyu hua wu qiaoqiao she and sujian li
enhancing pre trained language representations with rich knowledge for machine reading comprehension
in ceedings of the annual meeting of the association for computational linguistics pages
young et al
tom young iti chaturvedi hao zhou subham biswas and minlie huang
augmenting end to end dialogue systems with in thirty second aaai commonsense knowledge
conference on articial intelligence
erik cambria zhao et al
xueliang zhao chongyang tao wei wu can xu dongyan zhao and rui yan
a document grounded matching network for response arxiv preprint lection in retrieval based chatbots


zhao et al
xueliang zhao wei wu chongyang tao can xu dongyan zhao and rui yan
low resource in knowledge grounded dialogue generation
tional conference on learning representations
et al
hao zhou tom young minlie huang haizhou zhao jingfang xu and xiaoyan zhu
monsense knowledge aware conversation generation with graph attention
in ijcai pages

