user guided aspect classication for domain specic texts peiran fang jingbo department of computer science and engineering university of california san diego ca usa halcoglu data science institute university of california san diego ca usa department of computer science university of illinois at urbana champaign il usa
edu
edu r a l c
s c v
v i x r a abstract aspect classication identifying aspects of text segments facilitates numerous tions such as sentiment analysis and review summarization
to alleviate the human fort on annotating massive texts in this paper we study the problem of classifying aspects based on only a few user provided seed words for pre dened aspects
the major challenge lies in how to handle the noisy misc aspect which is designed for texts without any dened aspects
even domain experts have culties to nominate seed words for the misc aspect making existing seed driven text sication methods not applicable
we propose a novel framework arya which enables mutual enhancements between pre dened pects and the misc aspect via iterative sier training and seed updating
specically it trains a classier for pre dened aspects and then leverages it to induce the supervision for the misc aspect
the prediction results of the misc aspect are later utilized to lter out noisy seed words for pre dened aspects
experiments in two domains demonstrate the superior performance of our proposed work as well as the necessity and importance of properly modeling the misc aspect
introduction aspect classication is a fundamental task in text understanding aiming at identifying aspects of text segments he et al

it can facilitate ous downstream applications including sentiment analysis and product review summarization
for stance understanding aspects of a product s review sentences can help to deliver a holistic summary of this product without missing any important pect angelidis and lapata
following the supervised paradigm to extract aspects requires extensive human effort on ing massive domain specic texts because aspects vary across domains
for example in restaurant reviews possible aspects include food service and location
when it comes to laptop reviews aspects become battery display
fore to alleviate such effort we study the problem of user guided aspect classication which only relies on very limited supervision only a small number e

of seed words per aspect
the major challenge of this problem lies in how to handle the misc aspect
the misc aspect is designed to capture two types of text segments which makes it noisy text segments about some specic aspects out of the pre dened scope which are quite common in the real world and text segments talking nothing about any specic aspect e

this is one of my favorite restaurants

due to this noisy nature even domain experts have difculties to nominate seed words for the misc aspect making existing seed driven text tion methods agichtein and gravano riloff et al
kuipers et al
tao et al
meng et al
not applicable here
in this per we aim to better incorporate the misc aspect into user guided aspect extraction
we make two intuitive crucial observations which shed light on the development of our posed framework
first given a text segment if its distribution over pre dened aspects is at it likely belongs to the misc aspect
this provides us a chance of inducing supervision from the classier trained for pre dened aspects
ond given a word if it is a strong indicator of the misc aspect it is unlikely to be a good seed word of any pre dened aspect
excluding such words from the seed words of pre dened aspects would reduce ambiguity thus becoming a wise decision
acknowledging these observations we propose a novel framework incorporating the misc aspect in a systematic manner
as shown in figure it is an iterative framework alternatively training the figure overview of our proposed framework arya
it enables mutual enhancements between the pre dened aspects and the misc aspect via iterative classier training and seed updating
pre dened aspects help to induce supervision for the misc aspect the misc aspect helps to lter out noisy seed words for pre dened aspects
classier for all aspects and updating seed words of pre dened aspects
we name it as arya
more specically we rst train a classier for k dened aspects based on user provided seed words
this k aspect classier further induces supervision for the misc aspect based on normalized entropy estimation enabling a classier
this facilitates our comparative sis which updates seed words of pre dened pects using strong aspect indicative words
the predicted misc aspect information is utilized to ensure those noisy words will never appear in seed words of pre dened aspects
as one can see here arya achieves mutual enhancements between dened aspects and the misc aspect
to our best knowledge we are the rst to tematically handle the misc aspect in user guided aspect extraction
our main contributions are we identify the keystone towards user guided aspect extraction as the noisy misc aspect
we develop arya based on two intuitive servations making pre dened aspects and the misc aspect mutually enhance each other
experiments in two domains demonstrate the superiority of arya and the necessity of ering the misc aspect systematically
reproducibility
we will release our code and datasets in our github
framework is named after arya stark in game of thrones who kills the night king bringing an end to the others i
e
white walkers wights
forever

com peiranli arya overview problem formulation
given a domain specic corpus d of n text segments


sn k pre dened aspects


ak and a small number of seed words per aspect


vak this paper aims to build an aspect classier for domain specic text segments
a domain here refers to a relatively consistent category of products or services such as the hotel domain the restaurant domain and the laptop domain
in this paper we assume that there is at most one specic aspect in each text segments
in practice one can always segment the text in a ne grained way to ensure that this assumption holds
in other words for any input text segment si our classier aims to predict its corresponding aspect label yi
is either an id of the pre dened aspects between and k or the number denoting that si focuses on none of the pre dened aspects
our framework
arya is an iterative framework as illustrated in figure and algorithm
in each iteration we apply the following four steps in order
pseudo label generation
given seed words for k aspects we generate k aspect pseudo bels for all text segments in the raw corpus
classier training
we train a k aspect er based on the generated pseudo labels
our framework is compatible with all text classiers
as an illustration we choose to use d cnn in this paper
we will brief its neural architecture for the self contained purpose
thisismyfavoriterestaurantaspect seed wordsfoodservice foodspicypizzasushitastytipswaitressmanagerwaitservers text pseudo labelgenerationusing multi head attentions foodservice classifier trainingconvolutional seed tuning expansion miscaspect handling k aspect labelsk aspect labelsfoodservice food spicy pizza sushi tasty steak noodle tips waitress manager wait servers fast courteous
algorithm overall algorithm input a corpus d of n text segments


sn user provided seed words for k pre dened aspects


vak
output a classier
train word embedding ew on d
while seed words are not converged do compute aspect embedding aj eq get k aspect supervision qi j eq train k aspect classier mk sec get supervision qi j eq tune expand and lter seed words sec return the last classier
misc aspect handling
we leverage the tions of the trained k aspect classier to produce pseudo labels for the misc aspect
after that we train a new classier which makes an end to end aspect extraction
seed tuning expansion and filtering
we conduct a comparative analysis to compare and contrast the text segments projected to ent aspects to nd new and discriminative seed words for each aspect
the misc aspect is lized here to further lter out noisy seed words for pre dened aspects
we will discuss the details of the four major components in the following sections
before that here are some basic notations
notations
each text segment consists of a quence of tokens i
e
si


where is the number of tokens in si
please note that token here includes not only single word words and punctuation but also multi word phrases e

battery life chocolate cake and subword pieces e

nt
the tokens are pre processed from raw texts by applying both tokenization and phrasal segmentation shang et al

let v be the vocabulary set of all tokens
for each token w v we denote its dimensional embedding vector as ew
the embedding representation matrix of text segment si is then dened as xi


by concatenating each row vector
pseudo label generation we generate pseudo labels following a multi head attention mechanism where each attention head focuses on a specic aspect
it helps our model focus on aspect indicative words and ignore evant ones and derive aspect oriented tation
the outputs from all attention heads are nally aggregated to derive the prominent aspect of the text segment
first we assume that the user provided seed words can characterize the aspect s semantics
so we compute aj the aspect representation of aj by averaging embedding of its seed words
aj ew wvaj a higher embedding similarity between a word and an aspect implies that the word is more closely related to the aspect and it should be paid greater attention to
therefore given a word w its tion weight is dened as its maximum similarity over k aspects
since text segments are usually short we use the erage of its tokens following the attention weights as its aspect oriented representation zi
w k max at ew zi w w wsi wsi based on the similarity between text segment representation zi and aspect representation aj we derive the pseudo label assignments as j zi we normalize into a label distribution over all k aspects
aspect classier training our framework is generally compatible with any text classiers
in this paper we choose to use a cnn model because the multi head attention mechanism in our pseudo label generation can be viewed as applying a few corresponding tional lters
specically every aspect tation ai is equivalent to a convolutional lter of window size one
as mentioned before we have xi as the ding representation matrix of text segment si
we feed xi to our cnn model as illustrated in figure
specically we employ various lters of window sizes two three and four ing to bi grams tri grams and four grams
we be a and assign a higher pmisc if the hnorm is higher
therefore we propose to leverage a relu like function to quantify pmisc based on hnorm
pi misc hnorm hnorm hnorm we choose the value of as the quantile of the hnorm scores of all documents because based on figure quantile will give a suitable pivot point
specically in figure the values on the restaurant and laptop datasets are
and
respectively
after getting this we combine pi misc and pi j qi j pi j pi misc k j k finally we obtain the pseudo labels qi j for all aspects including the misc aspect
we then train a cnn classier
seed tuning expansion and filtering besides the user provided seed words there are usually more strong aspect indicator words ded in the raw input corpus
it could be helpful to discover and add such words into the seed sets
seed tuning
not every word could be a date seed word e

stopwords
therefore we build a candidate pool based on the k aspect sier
specically we try to replace each word by the special unk token and compute the kl divergence between the prediction results before and after
given a word if there exists one text segment where this word leads to a kl divergence difference more than
the word becomes a didate
the intuition here is we want to prepare a candidate pool with high recall and reasonable precision
also as further ranking and ltering will be applied this threshold is fairly easy to decide
seed expansion
then we expand the seed sets by ranking and adding words from the candidate pool
given an aspect aj and its candidate pool cj we mainly consider two measurements indicative
as the pseudo label generation cess can be viewed as a soft version of string matching using embedding we want to select words whose presence strongly indicate a tain aspect
mathematically we want to select the word w if it has a high posterior probability p
p means that given the ence of a word w how likely the text segment restaurant dataset laptop dataset figure hnorm distribution and pmisc visualization
apply these lters on the input matrix and then add a dropout layer after convolutional layers to viate
finally we use a softmax layer to transform the output to probabilities as pi j noting the probability of si belonging to aspect aj
the pseudo label distribution qi generated in the previous step serves as supervision here using the kl divergence loss as below
l pi qi j log k pi j qi j the same classication logic applies to the ing of both k aspect and classiers
misc aspect handling in aspect extraction two types of text segments long to the misc aspect text segments about some specic aspects different from the k dened aspects and text segments talking ing about any specic aspects
these text segments are expected to have a relatively at distribution in the predictions of the k aspect classier
fore it is intuitive to leverage normalized entropy hnorm which measures how chaotic the tion is to estimate the likelihood of si belonging to the misc aspect i
e
pmisc
specically hnorm pi j log k k as shown in figure we plot the distribution of hnorm for all text segments on both the restaurant and laptop datasets
one can easily observe that a large volume of the text segments have low hnorm indicating that they belong to some pre dened aspects
at the same time those misc aspect text segments follow a long tail distribution over large hnorm values
ideally we want to classify text segments with low enough hnorm values to











of text











of text segmentspmisc table dataset statistics
dataset unlabeled segments test segments restaurant laptop belongs to the aspect aj
therefore we dene the indicative measure as table user provided seed words for the restaurant dataset
by default we randomly sample seed words from each aspect and run experiments
aspect seed word list location drinks street convenient block avenue river subway neighborhood downtown bus drinks beverage wines margaritas sake beer wine list cocktail vodka soft drinks w food food spicy sushi pizza tasty steak delicious bbq seafood noodle faj w faj where faj w is the frequency of the word w peared in text segments of the aspect aj and faj refers to the total text segments of the aspect aj
the frequency is calculated based on the prediction results on the training set
distinctive
ideally a seed word should be only frequent in its own aspect
therefore we pose a distinctive measure to capture this
it measures how distinctive this word w in aspect aj is compared with all other aspects
w faj w fw ak since these two scores are of different scales we aggregate them using the geometric mean which has been shown effective in other comparative yses tao et al

ranking by the aggregated score we replace the seed words of the aspect aj by the top words here
seed filtering
it is worth noting that the same ranking heuristic can be applied to the misc pect as well
we observe that highly ranked words in the misc aspect are mostly general words or some noisy words that are related to multiple dened aspects
by checking some examples on the restaurant dataset we observe that restaurant is ranked high in the misc aspect as it can appear in text segments of any aspects the word place is also a top ranked word for the misc aspect
other than location related text segments it also appears frequently in text segments like this restaurant is such a great place
intuitively the user may lect this word as a seed word for location aspect however it is in fact very noisy
therefore when replacing the seed words we propose to maintain a new pool of noisy words following the ranking in the misc aspect and exclude top words in this pool from seed words in pre dened aspects
experiments ambience romantic atmosphere room seating small spacious dark cozy quaint music service tips manager wait waitress servers fast prompt friendly courteous attentive methods
we also explore the effects of the number of iterations and the number of seeds
a case study about seed word evolution will be presented too

datasets we have prepared two review datasets in the rant and laptop domains for evaluation
table presents you some statistics
these two datasets can be found in our
restaurant
there are aspects in our rant dataset food service ambience drinks and location
for training we have collected unlabeled restaurant reviews from the yelp dataset challenge
laptop
there are aspects in our top dataset support display battery software keyboard os mouse
for training we are using unlabeled zon reviews on laptop collected by mcauley et al
he and mcauley
user provided seed words
for both datasets we ask three domain experts to provide seed words for each pre dened aspect
table shows the seed word list provided by one expert for the restaurant dataset
by default we will randomly choose seed words from them to train all the models including both ours and baselines
we report the average of these test results
for one tricky aspect the keyboard aspect of the laptop dataset we have only collected seed words
pre processing
we pre process the corpus using the
special characters such as and redundant punctuations are removed
we learn word embedding on the unlabeled training corpus

com peiranli arya
yelp
com in this section we empirically evaluate our posed framework arya against many compared challenge
table evaluation results on the restaurant and laptop datasets
all precision recall and scores are averaged in the macro weighted manner
underlines highlight the best compared models
restaurant laptop method precision recall precision recall cossim abae mate westclass dataless bert arya arya noiter arya notuning arya nofilter


































































compared models we compare our model with a wide range of line models described as follows
cossim assigns the most similar aspect to each text segment according to the cosine similarity between the average word embedding of the text segment and the average word embedding of all seeds in each aspect
dataless song and roth accepts aspect names as supervision and leverages wikipedia and explicit semantic analysis esa to derive vector representation of both aspects and ments
the class is assigned based on the vector similarity between aspects and documents
abae he et al
is an unsupervised ral topic model
we extend the abae by ing user provided seed words for each aspect to align its topics to pre dened aspects
mate angelidis and lapata is an tended version of abae which accepts seed information for guidance and replaces abae s aspect dictionary with seed matrices
meng et al
is the of the art weakly supervised text classication model which accepts seed words as supervision
bert devlin et al
is a powerful tualized representation learning technique
we use seed words matching and majority voting to generate sentence labels and then ne tune the bert for classication
most of these models do not take care of the misc aspect systematically
therefore we ne tune the best compared method using our proposed aspect handling referred as
we denote our model as arya
in addition we have a few ablated versions as follows
noiter uses our proposed misc aspect handling technique to generate the probability of misc pect based on k aspect classier however without any further steps
arya notuning refers to the version of our model without the seed tuning nique i
e
no kl divergence threshold for seed word candidates
arya nofilter is our model without the seed ltering technique i
e
no noisy seed words removal in pre dened aspects based on misc aspect information

experiment setup default parameters
we set the word embedding dimension d
for the classier training we x the number of epoch as since the training error tends to converge after epochs
the kl gence threshold for seed tuning is set to

this value is set based on some human efforts
one can easily observe that words lead to a kl divergence difference less than
are not very representative for that aspect
based on the raw corpus sizes we set the maximum number of seed words per each aspect as on the restaurant dataset and on the laptop dataset
evaluation metrics
we use macro weighted erage precision recall and scores

experiment results we present the evaluation results on the rant and laptop datasets in table
it is clear that our proposed method arya outperforms all other methods with signicant margins on both datasets because none of these models considers the misc aspect systematically
even compared with the ne tuned second best models arya results in and in absolute provements over it on the restaurant and laptop table seed word evolution examples
the th iteration indicates the user provided seeds
dataset aspect iter seed words restaurant laptop food location keyboard os spicy pizza sushi food tasty pizza spicy variety tasty tuna sushi portion food specials bland avenue convenient river street block located block view convenient river avenue located block street view convenient park river avenue located block street view convenient park river york avenue keyboard key space keyboard keys key keys keyboard numeric volume palm key layout keyboards system os ios windows mac system os ios operating mac windows lion interface decided automatically
figure shows that the score increases w

t
iterations on both datasets
this suggests that our framework truly enables tual enhancements between pre dened aspects and the misc aspect over iterations
table presents the seed words of each aspect w

t
different iterations on both datasets
we can observe that the seed words become much better after the seed expansion than the initial seed words
as mentioned before even domain experts feel challenging to provide seed words for the keyboard aspect
only three seed words board key and space are given at the very beginning
after a few rounds of seed tuning pansion and ltering some interesting words are added to its seed set such as layout numeric and palm which make sense for the keyboard aspect
for example palm describes the how comfortable the palms are when typing on a board or how big the keyboard is compared with palms
it is interesting to see that our model can automatically discover these words beyond typical examples come up by experts
we also observe that the seed word sets of lar aspects converge faster than infrequent aspects
for example on the restaurant dataset the food ambience and service aspects converge after the iteration and the drinks and location pects requires and iterations respectively
the rst three have signicantly more text segments than the latter two
another observation is that the tricky aspects converge slower than the other aspects
for ple on the laptop dataset the keyboard aspect converges much slower than the other aspects cause it is very counter intuitive to come up with the seed words such as palm and numeric
on the contrary the os aspect is relatively easy figure scores in different iterations
arya keeps iterating until the seed words converge
datasets respectively
it is also worth noting that arya noiter signicantly outperforms all pared methods
all these observations show the importance of properly handling the misc aspect
among all compared methods mate is guably the second best method
it utilizes the head attention mechanism which is the same as our pseudo label generation step
this implies that attention mechanism is very important for aspect extraction tasks
arya generalizes attentions to more convolutional lters thus being able to train a more powerful model
the advantage of arya over arya noiter demonstrates the importance of progressively ne the model by updating seed words at every iteration
comparing arya notuning and noiter one can see that if we do not carefully limit the scope of seed word candidates there is a risk of adding noisy seed words that will lead to even worse performance e

on the laptop dataset
the improvement of arya over arya nofilter reveals the effectiveness of ltering the seed words in pre dened aspects by the misc aspect

seed word evolution arya keeps iterating until the seed words verge
so the number of iterations in arya is



scorerestaurantlaptop pared with other aspects

misc text segment examples we present two successfully classied text ments of the different types of misc aspect
the rst example is from the restaurant dataset there is nothing more pleasant than that
without any specic aspect
arya detects that the word pleasant as a noisy word because it can refer to service or ambience
therefore it is ltered for these two aspects
eventually arya predicts the probabilities of this segment belong to misc service and ambience as

and
respectively
therefore misc wins in the end
the second example is from the laptop dataset the only problem is that i had to add gb ram the computer was kinda slow
about the out pre dened hardware aspect
arya predicts it as misc and os with chances
and
respectively mainly because the word slow is widely used to complain about os
related works aspect extraction was originated at a level task instead of working on text segments
rule based methods hu and liu liu et al
zhuang et al
scafdi et al
zhang et al
qiu et al
are the pioneers along this direction
a number of unsupervised learning methods based on the lda topic model and its variants titov and mcdonald zhao et al
brody and elhadad jee and liu zhang et al
shams and baraani dastjerdi treat extracted topics as aspects
more recently a neural model extra luo et al
is proposed to further improve the aspect extraction at the document level
ever since our problem focuses on text segments directly applying these document level methods leads to some unsatisfactory results
there are several recent unsupervised attempts on aspect extraction for text segments
abae he et al
employs an attention module to learn embedding for text segments and an auto encoder framework to build aspect dictionaries
however it requires users to rst set the number of topics as a much larger number than the number of sired aspects and then manually merge and map the extracted topics back to the aspects
building upon abae angelidis and lapata further proposed a multi seed aspect extractor mate ing seed aspect words as guidance
this model keeps the human effort at a minimal degree and ts our problem setting well
however even with its multi task counterpart the reconstruction objective in mate model is not able to provide adequate training signals
our proposed method leverages the seed word tuning and expansion to overcome this issue thus outperforming mate signicantly in the extensive experiments
our problem shares certain similarities with the weakly supervised text classication problem
isting methods can build document classiers by taking either hundreds of labeled training ments tang et al
miyato et al
xu et al
class category names song and roth li et al
or user provided seed words meng et al
as the source of weak supervision
however all these methods assume that users can always provide seeds for all classes while overlooking the noisy misc aspect in our problem
we incorporate the misc aspect atically into our framework
conclusions and future work in this paper we explore to build an aspect traction model for text segments using only a few user provided seed words per aspect
we identify the key challenge lies in how to properly handle the misc aspect for which even domain experts can not easily design seed words
we propose a novel framework arya which incorporates the misc aspect systematically
in our framework we induce supervision for the misc aspect using seed words of pre dened aspects
at the same time we utilize the misc aspect information to lter out the noisy words from the seed list of pre dened aspects
extensive experiments have demonstrated the effectiveness of arya and veried the sity of modeling the misc aspect
in the future we would like to integrate the tracted aspect information with downstream tasks such as sentiment analysis and opinion tion
we also want to explore the use of alized representation in weakly supervised aspect extraction further disambiguating words based on contexts
in addition we are interested in ing our work to document classications even with multiple labels per document
references eugene agichtein and luis gravano

snowball extracting relations from large plain text collections
in proceedings of the fth acm conference on tal libraries pages
acm
stefanos angelidis and mirella lapata

marizing opinions aspect extraction meets ment prediction and they are both weakly supervised


samuel brody and noemie elhadad

an pervised aspect sentiment model for online reviews
in naacl pages
jacob devlin ming wei chang kenton lee and kristina toutanova

bert pre training of deep bidirectional transformers for language ing


ruidan he wee sun lee hwee tou ng and daniel dahlmeier

an unsupervised neural attention in acl pages model for aspect extraction

ruining he and julian mcauley

ups and downs modeling the visual evolution of fashion trends with one class collaborative ltering
in www
minqing hu and bing liu

mining and in sigkdd pages rizing customer reviews

benjamin j kuipers patrick beeson joseph modayil and jefferson provost

bootstrap learning of foundational representations
connection science
keqian li hanwen zha yu su and xifeng yan

unsupervised neural categorization for in siam data mining pages entic publications

siam
bing liu minqing hu and junsheng cheng

opinion observer analyzing and comparing ions on the web
in www pages
zhiyi luo shanshan huang frank f xu bill yuchen lin hanyuan shi and kenny zhu

extra extracting prominent review aspects from customer feedback
in emnlp pages
julian mcauley christopher targett qinfeng shi and anton van den hengel

image based mendations on styles and substitutes
in sigir
yu meng jiaming shen chao zhang and jiawei han

weakly supervised neural text classication
in cikm pages
takeru miyato andrew m dai and ian low

adversarial training methods for supervised text classication


guang qiu bing liu jiajun bu and chun chen

opinion word expansion and target extraction through double propagation
computational tics
ellen riloff janyce wiebe and theresa wilson

learning subjective nouns using extraction pattern bootstrapping
in proceedings of the seventh ence on natural language learning at hlt naacl volume pages
association for putational linguistics
christopher scafdi kevin bierhoff eric chang mikhael felker herman ng and chun jin

red opal product feature scoring from reviews
in proceedings of the acm conference on tronic commerce pages
mohammadreza shams and ahmad baraani dastjerdi

enriched lda elda combination of latent dirichlet allocation with word co occurrence sis for aspect extraction
expert systems with cations
jingbo shang jialu liu meng jiang xiang ren clare r voss and jiawei han

automated phrase mining from massive text corpora
tkde
yangqiu song and dan roth

on dataless chical text classication
in aaai
jian tang meng qu and qiaozhu mei

pte dictive text embedding through large scale in sigkdd pages geneous text networks

fangbo tao chao zhang xiusi chen meng jiang tim hanratty lance kaplan and jiawei han

automated document allocation to text cube via dimension aware joint embedding
sion
ivan titov and ryan mcdonald

modeling online reviews with multi grain topic models
in www pages
acm
weidi xu haoze sun chao deng and ying tan

variational autoencoder for semi supervised text classication
in aaai
chen zhang hao wang liangliang cao wei wang a hybrid term term topic detection
and fanjiang xu

relations analysis approach for knowledge based systems
lei zhang bing liu suk hwan lim and eamonn obrien strain

extracting and ranking uct features in opinion documents
in proceedings of the international conference on tional linguistics posters pages
ciation for computational linguistics
arjun mukherjee and bing liu

aspect in acl tion through semi supervised modeling
pages
wayne xin zhao jing jiang hongfei yan and ing li

jointly modeling aspects and opinions with a maxent lda hybrid
in emnlp pages
li zhuang feng jing and xiao yan zhu

movie review mining and summarization
in cikm pages

