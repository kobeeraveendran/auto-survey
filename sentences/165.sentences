iterative document representation learning towards summarization with polishing xiuying shen chongyang yan dongyan and rui for data science peking university beijing china of computer science and technology peking university beijing china ai lab

abstract
in this paper we introduce iterative text marization its an iteration based model for supervised extractive text summarization spired by the observation that it is often essary for a human to read an article multiple times in order to fully understand and rize its contents
current summarization proaches read through a document only once to generate a document representation ing in a sub optimal representation
to
dress this issue we introduce a model which iteratively polishes the document tion on many passes through the document
as part of our model we also introduce a tive reading mechanism that decides more curately the extent to which each sentence in
the model should be updated
experimental results on the cnn dailymail and datasets demonstrate that our model cantly outperforms state of the art extractive systems when evaluated by machines and by humans
introduction
a summary is a shortened version of a text ument which maintains the most important ideas from the original article
automatic text rization is a process by which a machine gleans the most important concepts from an article removing secondary or redundant concepts
nowadays as there is a growing need for storing and digesting large amounts of textual data automatic rization systems have signicant usage potential in society
extractive summarization is a technique for generating summaries by directly choosing a set of salient sentences from the original
ment to constitute the summary
most efforts made towards extractive summarization either rely corresponding author rui yan on human engineered features such as sentence length word position and frequency cohen radev et al woodsend and lapata yan et al b or use neural networks to automatically learn features for tence selection cheng and lapata ati al

although existing extractive summarization methods have achieved great success one tion they share is that they generate the summary after only one pass through the document
in real world human cognitive processes ever people read a document multiple times in order to capture the main ideas
browsing through the document only once often means the model can not fully get at the document s main ideas leading to a subpar summarization
we share two examples of this
consider the situation where we almost nish reading a long article and forget some main points in the beginning
we are likely to go back and review the part that we forget
to write a good summary we usually rst browse through the document to obtain a general understanding of the article then perform a more intensive reading to select salient points to include in the summary

in terms of model design we believe that letting a model read through a document multiple times polishing and updating its internal representation of the document can lead to better understanding and better summarization
to achieve this we design a model that we call iterative text summarization its consisting of a novel iteration mechanism and selective
its is an iterative process ing module
ing through the document many times
there is one encoder one decoder and one iterative unit in each iteration
they work together to polish ument representation
the nal labeling part uses outputs from all iterations to generate summaries

the selective reading module we design is a a m l c
s
c
v
v
i x r a ed version of a gated recurrent unit gru work which can decide how much of the hidden state of each sentence should be retained or dated based on its relationship with the document

overall our contribution includes
we propose iterative text summarization
its an iteration based summary generator which uses a sequence classier to extract salient sentences from documents

we introduce a novel iterative neural work model which repeatedly polishes the distributed representation of document stead of generating that once for all
besides we propose a selective reading mechanism which decides how much information should be updated of each sentence based on its lationship with the polished document resentation
our entire architecture can be trained in an end to end fashion

we evaluate our summarization model on representative cnn dailymail corpora and benchmark dataset

tal results demonstrate that our model performs state of the art extractive systems when evaluated automatically and by human
related work
our research builds on previous works in two elds summarization and iterative modeling
text summarization can be classied into tractive summarization and abstractive rization
extractive summarization aims to ate a summary by integrating the most salient tences in the document
abstractive tion aims to generate new content that concisely
paraphrases the document from scratch

with the emergence of powerful neural work models for text processing a vast majority of the literature on document summarization is icated to abstractive summarization
these els typically take the form of convolutional ral networks cnn or recurrent neural networks rnn
for example rush et al
propose an encoder decoder model which uses a local tention mechanism to generate summaries
lapati et al
further develop this work by addressing problems that had not been adequately solved by the basic architecture such as keyword modeling and capturing the hierarchy of to word structures
in a follow up work nallapati al
propose a new summarization model which generates summaries by sampling a topic one sentence at a time then producing words ing an rnn decoder conditioned on the sentence topic
another related work is by see et al
where the authors use pointing and coverage techniques to generate more accurate summaries

despite the focus on abstractive summarization extractive summarization remains an attractive method as it is capable of generating more matically and semantically correct summaries

this is the method we follow in this work
in tractive summarization cheng and lapata propose a general framework for single document text summarization using a hierarchical article coder composed with an attention based extractor

following this nallapati et al
propose a simple rnn based sequence classier which outperforms or matches the state of art models at the time

in another approach narayan et al

use a reinforcement learning method to
timize the rouge evaluation metric for text marization
the most recent work on this topic is wu and hu where the authors train a reinforced neural extractive summarization model called rnes that captures cross sentence ence patterns
due to the fact that they use a ferent dataset and have not released their code we are unable to compare our models with theirs
the idea of iteration has not been well explored for summarization
one related study is xiong et al
s work on dynamic memory works which designs neural networks with ory and attention mechanisms that exhibit certain reasoning capabilities required for question
swering
another related work is yan where they generate poetry with iterative ing sn chema
similiar method can also be applied on couplet generation as in yan et al
we take some inspiration from their work but focus on document summarization
another related work is singh et al where the authors present a deep network called hybrid memnet for the gle document summarization task using a ory network as the document encoder
compared to them we do not borrow the memory network structure but propose a new iterative architecture

methodology problem formulation
in this work we propose iterative text marization its an iteration based supervised model for extractive text summarization
we treat the extractive summarization task as a sequence labeling problem in which each sentence is ited sequentially and a binary label that determines whether or not it will be included in the nal mary is generated

wi its takes as input a list of sentences s
sns where ns is the number of tences in the document
each sentence is a list of words wi nw where nw is the word length of the sentence
the goal of its is to generate a score vector y
yns for each sentence where each score yi
denotes the sentence s extracting probability that is the probability that the corresponding sentence will be extracted to be included in the mary
we train our model in a supervised ner using a corresponding gold summary written by human experts for each document in training set
we use an unsupervised method to convert the human written summaries to gold label tor

ns where
i denotes
whether the i th sentence is selected or not

next during training process the cross entropy loss is calculated between y and which is imized to optimize finally we select three tences with the highest score according to y to be the extracted summary
we detail our model
low

model architecture
its is depicted in
it consists of multiple erations with one encoder one decoder and one iteration unit in each iteration
we combine the outputs of decoders in all iterations to generate the extracting probabilities in the nal labeling ule
our encoder is illustrated in the shaded region in the left half of
it takes as input all sentences as well as the document representation from the previous unit processes them through eral neural networks and outputs the nal state to the iterative unit module which updates the ment representation
our decoder takes the form of a bidirectional rnn
it takes the representation of sentence erated by the encoder as input and its initial state is the polished document representation dk
our last module the sentence labeling module catenates the hidden states of all decoders together to generate an integrated score for each sentence

as we apply supervised training the objective is to maximize the likelihood of all sentence labels


ns given the input document and model parameters log log our model encoder in this subsection we describe the encoding cess of our model
for brevity we drop the script when focusing on a particular layer
all the w and b in this section with different perscripts or subscripts are the parameters to be learned
sentence encoder
given a discrete set of tences s
sns we use a word ding matrix m rv d to embed each word wi in sentence into continuous space wi where v is the vocabulary size d is the dimension of word embedding
the sentence encoder can be based on a variety of encoding schemes
simply taking the average of embeddings of words in a sentence will cause too much information loss while using grus or long short term memory lstm requires more computational resources and is prone to ting
considering above we select positional
coding described in sukhbaatar et al as our sentence encoding method
each sentence resentation si is calculated by si

lj wi
j where is element wise multiplication lj is a umn vector computed as lj j d nw nw lj denotes the th dimension of lj
note that throughout this study we use grus as our rnn cells since they can alleviate the tting problem as conrmed by our experiments

as our selective reading mechanism which will be explained later is a modied version of inal gru cell we give the details of the gru here
gru is a gating mechanism in recurrent neural networks introduced in cho et al

their performance was found to be similar to that of lstm cell but using fewer parameters as scribed in hochreiter and schmidhuber

the gru cell consists of an update gate vector
figure model structure there is one encoder one decoder and one iterative unit which is used to polish document representation in each iteration
the nal labeling part is used to generating the extracting probabilities for all sentences combining hidden states of decoders in all iterations
we take a document consists of three sentences for example here
ui a reset gate vector ri and an output vector
hi
for each time step i with input xi and vious hidden state the updated hidden state hi is computed by ui
u ri u
hi
ri u
hi ui hi ui
the is sigmoid w u w r w h activation
where function rnh ni u u u r u rnh nh nh is the hidden size ni is the size of input xi
to further study the interactions and tion exchanges between sentences
we establish a bi directional gru bi gru network taking the sentence representation as input si
si si si
where si is the sentence representation input at time step i si is the hidden state of the forward
gru at time step i and si is the hidden state of the backward gru
this architecture allows formation to ow back and forth to generate new sentence representation si
document encoder we must initialize a ument representation before polishing it
ating the document representation from sentence representations is a process similar to
ing the sentence representation from word dings
this time we need to compress the whole document not just a sentence into a vector

cause the information a vector can contain is ited rather than to use another neural network we simply use a non linear transformation of the erage pooling of the concatenated hidden states of the above bi gru to generate the document resentation as written below
si si ns where is the concatenation operation

selective reading module now we can mally introduce the selective reading module in
this module is a bidirectional rnn sisting of modied gru cells whose input is the sentence representation s sns
in the original version of gru the update gate ui in equation is used to decide how much of den state should be retained and how much should be updated
however due to the way ui is
lated it is sensitive to the position and ordering of sentences but loses information captured by the polished document representation

herein we propose a modied gru cell that

reading iterative unititerative unit
replace the ui with the newly computed update gate gi
the new cell takes in two inputs the tence representation and the document tion from the last iteration rather than merely the sentence representation
for each sentence the lective network generates an update gate vector gi in the following way fi
si
fi w

gi where si is the i th sentence representation is the document representation from last eration
equation now becomes hi gi hi gi
we use this selective reading module to matically decide to which extent the information of each sentence should be updated based on its relationship with the polished document
in this way the modied gru network can grasp more accurate information from the document
iterative unit
after each sentence passes through the selective reading module we wish to update the document representation with the newly constructed sentence representations
the iterative unit also depicted above in is designed for this pose
we use a gruiter cell to generate the ished document representation whose input is the nal state of the selective reading network from the previous iteration hns and whose initial state is set to the document representation of the ous iteration
the updated document resentation is computed by dk decoder next we describe our decoders which are picted shaded in the right part of
ing most sequence labeling task xue and palmer carreras and where they learn a feature vector for each sentence we use a bidirectional grudec network in each iteration to output features so as to calculate extracting bilities
for k th iteration given the sentence resentation s as input and the document sentation
dk as the initial state our decoder codes the features of all sentences in the hidden state hk hk ns
hk hk hk
dk sentence labeling module
next we use the feature of each sentence to ate corresponding extracting probability
since we have one decoder in each iteration if we directly transform the hidden states in each iteration to tracting probabilities we will end up with several scores for each sentence
either taking the age or summing them together by specic weights is inappropriate and inelegant
hence we nate hidden states of all decoders together and
ply a multi layer perceptron to them to generate the extracting probabilities w hk

where yns yi is the extracting
ability for each setence

in this way we let the model learn by itself how to utilize the outputs of all iterations and assign to each hidden state a liable weight
in section we will show that this labeling method outperforms other methods experiment setup in this section we present our experimental setup for training and estimating our summarization model
we rst introduce the datasets used for training and evaluation and then introduce our perimental details and evaluation protocol
datasets
in order to make a fair comparison with our lines we used the cnn dailymail which was constructed by hermann et al

we used the standard splits for training validation and testing in each corpus uments for cnn and for dailymail
we followed previous studies in
ing the human written story highlight in each
cle as a gold standard abstractive summary
these highlights were used to generate gold labels when training and testing our model using the greedy search method similar to nallapati al

we also tested its on an out of domain pus which consists of documents

documents in this corpus belong to various clusters and each cluster has a unique topic
each document has two gold summaries written by

man experts of length around words
implementation details we implemented our model in tensorow abadi al
the code for our models is able
we mostly followed the settings in nallapati al and trained the model ing the adam optimizer kingma and ba with initial learning rate and anneals of every epochs until reaching epochs
we lected three sentences with highest scores as mary
after preliminary exploration we found that arranging them according to their scores
sistently achieved the best performance
periments were performed with a batch size of documents
we used dimension glove
pennington et al embeddings trained on wikipedia as our embedding initialization with a vocabulary size limited to for speed purposes
we initialized out of vocabulary word embeddings over a uniform distribution within
we also padded or cut sentences to tain exactly words
each gru module had layer with dimensional hidden states and with either an initial state set up as described above or a random initial state
to prevent overtting we used dropout after each gru network and ding layer and also applied loss to all ased variables
the iteration number was set to if not specied
a detailed discussion about tion number can be found in section baselines
on all datasets we used the method as a baseline which simply chooses the rst three tences in a document as the gold summary
on dailymail datasets we report the performance of summarunner in nallapati al and the model in cheng and lapata as well as a logistic regression classier lreg that they used as a baseline
we reimplemented the brid memnet model in singh et al as one of our baselines since they only reported the formance of samples in their paper
also narayan al
released their for the refresh model we used their code to produce rouge recall scores on the dailymail dataset as they only reported results on cnn dailymail joint dataset
baselines on cnn dataset are similar
ve document representation learning tow ards summarization with polishing esh on corpus we compare our model with several baselines such as integer linear ming ilr and lreg
we also report the mance of the newest neural networks model

cluding nallapati al cheng and ata singh et al
evaluation
in the evaluation procedure we used the rouge scores and rouge l responding to the matches of unigram bigrams and longest common subsequence lcs
tively to estimate our model
we obtained our rouge scores using the standard pyrouge
to compare with other related works we used full length score on the cnn corpus ited length of bytes and bytes recall score on dailymail corpus
as for the corpus following the ofcial guidelines we examined the rouge recall score at the length of words
all results in our experiment are statistically ca nt using condence interval as estimated by rouge script
schluter noted that only using the rouge metric to evaluate summarization quality can be misleading
therefore we also evaluated our model using human evaluation
five highly ucated participants were asked to rank maries produced by four models the line hybrid memnet its and human authored highlights
we chose hybrid memnet as one of the human evaluation baselines since its mance is relatively high compared to other lines

judging criteria included informativeness and coherence
test cases were randomly sampled from dailymail test set
experiment analysis table shows the performance comparison of our model with other baselines on the dailymail dataset with respect to rouge score at bytes and bytes of summary length
our model performs consistently and signicantly better than other models on bytes while on bytes the improvement margin is smaller
one possible terpretation is that our model has high precision on top rank outputs but the accuracy is lower for lower rank sentences

in addition cheng and lapata used additional supervised training
rouge l rouge l dailymail
cheng summarunner refresh
hybrid memnet
its table comparison with other baselines on dailymail test dataset using rouge recall score with respect to the abstractive ground truth at bytes and at bytes
cnn

cheng
hybrid memnet refresh
its rouge l table comparison with other baselines on cnn test dataset using full length variants of rouge
to create sentence level extractive labels to train their model while our model uses an unsupervised greedy approximation instead
we also examined the performance of our model on cnn dataset as listed in table
to compare with other models we used full length rouge metric as reported by narayan et al


results demonstrate that our model has a consistently best performance on different datasets

in table we present the performance of its on the out of domain duc dataset
our model performs or matches other basic models including lreg and ilr as well as neural network baselines such as summarunner with respect to the ground truth at bytes which shows that our model can be adapted to different copora maintaining high accuracy

in order to explore the impact of internal ture of its we also conducted an ablation study in table
the rst variation is the same model without the selective reading module
the ond one sets the iteration number to one that is a model without iteration process
the last variation is to apply mlp on the output from the last tion instead of concatenating the hidden states of all decoders
all other settings and parameters are the same
performances of these models are worse than that of its in all metrics which demonstrates lreg ilp
cheng
summarunner hybrid memnet
its rouge l table comparison with other baselines on dataset using rouge recall score with spect to the abstractive ground truth at bytes
variations
its selective reading iteration concatenation l table ablation study on dailymail test dataset with respect to the abstractive ground truth at bytes
the preeminence of its
more importantly by this controlled experiment we can verify the tion of different module in its further discussion analysis of iteration number we did a broad sweep of experiments to further investigate the uence of iteration process on the generated mary quality
first we studied the inuence of iteration number
in order to make a fair ison between models with different iteration ber we trained all models for same epochs without tuning
illustrates the relationship between iteration number and the rouge score at bytes of summary length on dailymail test dataset
the result shows that the rouge score increases with the number of iteration to begin with
after
ing the upper limit it begins to drop
note that models hybrid memnet
its gold table system ranking comparison with other lines on dailymail corpus
rank is the best and rank is the worst
each score represents the percentage of the summary under this rank
sitivity between iterations as shown in
to be specic the sentences which are not preferred by iteration remain low probabilities in the next two iterations while sentences with relatively high scores are still preferred by iteration and
human evaluation we gave human tors three system generated summaries generated by hybrid memnet its as well as the human written gold standard summary and asked them to rank these summaries based on summary informativeness and coherence
table shows the percentages of summaries of different models der each rank scored by human experts
it is not surprising that gold standard has the most maries of the highest quality
our model has the most summaries under rank thus can be sidered best following are hybrid memnet and as they are ranked mostly and

by case study we found that a number of maries generated by hybrid memnet have two sentences the same as its out of three however the third distinct sentence from our model always leads to a better evaluation result considering all informativeness and coherence
readers can refer to the appendix to see our case study
conclusion in this work we introduce its an iteration based extractive summarization model inspired by the observation that it is often necessary for a man to read the article multiple times to fully derstand and summarize it
experimental results on cnn dailymail and duc corpora demonstrate the effectiveness of our model

acknowledgments we would like to thank the anonymous
ers for their constructive comments
we would also like to thank jin ge yao and zhengyuan ma for their valuable advice on this project
this work was supported by the national key figure relationship between number of iteration and rouge score on dailymail test dataset with respect to the ground truth at bytes
figure the predicted extracting probabilities for each sentence calculated by the output of each iteration
the result of training the model for only one epoch outperforms the state of the art in singh et al which demonstrates that our selective ing module is effective
the fact that
ing this process increase the performance conrms that the iteration idea behind our model is useful in practice
based on above observation we set the default iteration number to be
analysis of polishing process next to fully investigate how the iterative process inuences the extracting results we draw heatmaps of the
tracting probabilities for each decoder at each eration
we pick two representative cases in where the axis represents the sentence index and y axis is the iteration number axis labels are omitted
the darker the color is the higher the
in it can be extracting probability is
seen that when the iteration begins most sentences have similar probabilities
as we increase the number of iteration some probabilities begin to fall and others saturate
this means that the model already has preferred sentences to select
another interesting feature we found is that there is a search and development program of china no
the national science dation of china nsfc no
no
rui yan was sponsored by tencent open research fund and microsoft search asia msra
collaborative research gram
references martn abadi paul barham jianmin chen zhifeng

chen andy davis jeffrey dean matthieu devin sanjay ghemawat geoffrey irving michael isard al

tensorow
a system for large scale machine learning
in osdi volume pages
xavier carreras and llus

duction to the shared task semantic role labeling
in proceedings of the ninth ence on computational natural language ing conll pages stroudsburg pa usa
association for computational linguistics
jianpeng cheng and mirella lapata
neural summarization by extracting sentences and words
kyunghyun cho bart van merrienboer caglar cehre dzmitry bahdanau fethi bougares holger
schwenk and yoshua bengio

learning phrase representations using rnn encoder decoder for statistical machine translation
computer ence

kevin bretonnel cohen

natural language cessing for online applications text retrieval
language traction and categorization review


karl moritz hermann edward grefenstette
lasse
peholt will kay mustafa suleyman and phil som
teaching machines to read and hend
pages
sepp hochreiter and jurgen schmidhuber


neural computation long short term memory


diederik kingma and jimmy ba

adam a method for stochastic optimization
computer ence
ramesh nallapati igor melnyk abhishek kumar and bowen zhou

sengen sentence generating neural variational topic model
ramesh nallapati feifei zhai and bowen zhou


summarunner a recurrent neural network based sequence model for extractive summarization of documents
ramesh nallapati bowen zhou
cicero nogueira
dos santos caglar gulcehre and bing xiang

abstractive text summarization using sequence sequence rnns and beyond
shashi narayan shay cohen and mirella lapata


ranking sentences for extractive tion with reinforcement learning jeffrey pennington richard socher and christopher manning

glove
global vectors for word
in conference on empirical representation
ods in natural language processing pages

dragomir r radev timothy allison sasha
goldensohn john blitzer arda celebi stanko
dimitrov elliott drabek ali hakim wai lam danyu liu al

mead a platform for tidocument multilingual text summarization

in lrec
alexander rush sumit chopra and jason weston


a neural attention model for abstractive tence summarization
computer science
natalie schluter

the limits of automatic
in proceedings of marisation according to rouge
the conference of the european chapter of the association for computational linguistics volume short papers volume pages

abigail see peter liu and christopher manning


get to the point summarization with generator networks
corr
abhishek kumar singh manish gupta and vasudeva varma
hybrid memnet for extractive marization
pages
sainbayar sukhbaatar arthur szlam jason weston and rob fergus

end to end memory works
computer science
kristian woodsend and mirella lapata


in meeting matic generation of story highlights
of the association for computational linguistics pages
yuxiang wu and baotian hu

learning to extract coherent summary via deep reinforcement learning
arxiv preprint
caiming xiong stephen merity and richard socher


dynamic memory networks for visual and textual question answering
nianwen xue and martha palmer

calibrating features for semantic role labeling
in proceedings of the conference on empirical methods in natural language processing
rui yan

i poet automatic poetry composition through recurrent neural networks with iterative ishing schema
in ijcai pages
rui yan cheng te li xiaohua hu and ming zhang


chinese couplet generation with neural work structures
in meeting of the association for computational linguistics pages
rui yan jian yun nie and xiaoming li

marize what you are interested in an optimization framework for interactive personalized tion
in conference on empirical methods in ral language processing emnlp july john mcintyre conference centre edinburgh uk a meeting of sigdat a special interest group of the acl pages
rui yan xiaojun wan mirella lapata wayne xin
zhao pu jen cheng and xiaoming li

sualizing timelines evolutionary summarization via iterative reinforcement between text and image
in proceedings of the acm streams
national conference on information and knowledge management pages
acm
rui yan xiaojun wan jahna otterbacher liang kong xiaoming li and yan zhang

ary timeline summarization a balanced
in tion framework via iterative substitution
national acm sigir conference on research and development in information retrieval pages

