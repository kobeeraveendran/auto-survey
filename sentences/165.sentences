iterative document representation learning towards summarization with polishing xiuying shen chongyang yan dongyan and rui for data science peking university beijing china of computer science and technology peking university beijing china ai lab xy chen chongyangtao zhaody
edu
cn
com abstract in this paper we introduce iterative text marization its an iteration based model for supervised extractive text summarization spired by the observation that it is often essary for a human to read an article multiple times in order to fully understand and rize its contents
current summarization proaches read through a document only once to generate a document representation ing in a sub optimal representation
to dress this issue we introduce a model which iteratively polishes the document tion on many passes through the document
as part of our model we also introduce a tive reading mechanism that decides more curately the extent to which each sentence in the model should be updated
experimental results on the cnn dailymail and datasets demonstrate that our model cantly outperforms state of the art extractive systems when evaluated by machines and by humans
introduction a summary is a shortened version of a text ument which maintains the most important ideas from the original article
automatic text rization is a process by which a machine gleans the most important concepts from an article removing secondary or redundant concepts
nowadays as there is a growing need for storing and digesting large amounts of textual data automatic rization systems have signicant usage potential in society
extractive summarization is a technique for generating summaries by directly choosing a set of salient sentences from the original ment to constitute the summary
most efforts made towards extractive summarization either rely corresponding author rui yan
edu
cn on human engineered features such as sentence length word position and frequency cohen radev et al
woodsend and lapata yan et al
b or use neural networks to automatically learn features for tence selection cheng and lapata ati et al

although existing extractive summarization methods have achieved great success one tion they share is that they generate the summary after only one pass through the document
in real world human cognitive processes ever people read a document multiple times in order to capture the main ideas
browsing through the document only once often means the model can not fully get at the document s main ideas leading to a subpar summarization
we share two examples of this
consider the situation where we almost nish reading a long article and forget some main points in the beginning
we are likely to go back and review the part that we forget
to write a good summary we usually rst browse through the document to obtain a general understanding of the article then perform a more intensive reading to select salient points to include in the summary
in terms of model design we believe that letting a model read through a document multiple times polishing and updating its internal representation of the document can lead to better understanding and better summarization
to achieve this we design a model that we call iterative text summarization its consisting of a novel iteration mechanism and selective its is an iterative process ing module
ing through the document many times
there is one encoder one decoder and one iterative unit in each iteration
they work together to polish ument representation
the nal labeling part uses outputs from all iterations to generate summaries
the selective reading module we design is a a m l c
s c v
v i x r a ed version of a gated recurrent unit gru work which can decide how much of the hidden state of each sentence should be retained or dated based on its relationship with the document
overall our contribution includes
we propose iterative text summarization its an iteration based summary generator which uses a sequence classier to extract salient sentences from documents

we introduce a novel iterative neural work model which repeatedly polishes the distributed representation of document stead of generating that once for all
besides we propose a selective reading mechanism which decides how much information should be updated of each sentence based on its lationship with the polished document resentation
our entire architecture can be trained in an end to end fashion

we evaluate our summarization model on representative cnn dailymail corpora and benchmark dataset
tal results demonstrate that our model performs state of the art extractive systems when evaluated automatically and by human
related work our research builds on previous works in two elds summarization and iterative modeling
text summarization can be classied into tractive summarization and abstractive rization
extractive summarization aims to ate a summary by integrating the most salient tences in the document
abstractive tion aims to generate new content that concisely paraphrases the document from scratch
with the emergence of powerful neural work models for text processing a vast majority of the literature on document summarization is icated to abstractive summarization
these els typically take the form of convolutional ral networks cnn or recurrent neural networks rnn
for example rush et al
propose an encoder decoder model which uses a local tention mechanism to generate summaries
lapati et al
further develop this work by addressing problems that had not been adequately solved by the basic architecture such as keyword modeling and capturing the hierarchy of to word structures
in a follow up work nallapati al
propose a new summarization model which generates summaries by sampling a topic one sentence at a time then producing words ing an rnn decoder conditioned on the sentence topic
another related work is by see et al
where the authors use pointing and coverage techniques to generate more accurate summaries
despite the focus on abstractive summarization extractive summarization remains an attractive method as it is capable of generating more matically and semantically correct summaries
this is the method we follow in this work
in tractive summarization cheng and lapata propose a general framework for single document text summarization using a hierarchical article coder composed with an attention based extractor
following this nallapati et al
propose a simple rnn based sequence classier which outperforms or matches the state of art models at the time
in another approach narayan et al
use a reinforcement learning method to timize the rouge evaluation metric for text marization
the most recent work on this topic is wu and hu where the authors train a reinforced neural extractive summarization model called rnes that captures cross sentence ence patterns
due to the fact that they use a ferent dataset and have not released their code we are unable to compare our models with theirs
the idea of iteration has not been well explored for summarization
one related study is xiong et al
s work on dynamic memory works which designs neural networks with ory and attention mechanisms that exhibit certain reasoning capabilities required for question swering
another related work is yan where they generate poetry with iterative ing sn chema
similiar method can also be applied on couplet generation as in yan et al

we take some inspiration from their work but focus on document summarization
another related work is singh et al
where the authors present a deep network called hybrid memnet for the gle document summarization task using a ory network as the document encoder
compared to them we do not borrow the memory network structure but propose a new iterative architecture
methodology
problem formulation in this work we propose iterative text marization its an iteration based supervised model for extractive text summarization
we treat the extractive summarization task as a sequence labeling problem in which each sentence is ited sequentially and a binary label that determines whether or not it will be included in the nal mary is generated



wi its takes as input a list of sentences s


sns where ns is the number of tences in the document
each sentence is a list of words wi nw where nw is the word length of the sentence
the goal of its is to generate a score vector y


yns for each sentence where each score yi denotes the sentence s extracting probability that is the probability that the corresponding sentence will be extracted to be included in the mary
we train our model in a supervised ner using a corresponding gold summary written by human experts for each document in training set
we use an unsupervised method to convert the human written summaries to gold label tor ns where i denotes whether the i th sentence is selected or not
next during training process the cross entropy loss is calculated between y and which is imized to optimize y
finally we select three tences with the highest score according to y to be the extracted summary
we detail our model low




model architecture its is depicted in fig

it consists of multiple erations with one encoder one decoder and one iteration unit in each iteration
we combine the outputs of decoders in all iterations to generate the extracting probabilities in the nal labeling ule
our encoder is illustrated in the shaded region in the left half of fig

it takes as input all sentences as well as the document representation from the previous unit processes them through eral neural networks and outputs the nal state to the iterative unit module which updates the ment representation
our decoder takes the form of a bidirectional rnn
it takes the representation of sentence erated by the encoder as input and its initial state is the polished document representation dk
our last module the sentence labeling module catenates the hidden states of all decoders together to generate an integrated score for each sentence
as we apply supervised training the objective is to maximize the likelihood of all sentence labels


ns given the input document and model parameters log log our model
encoder in this subsection we describe the encoding cess of our model
for brevity we drop the script when focusing on a particular layer
all the w and b in this section with different perscripts or subscripts are the parameters to be learned
sentence encoder given a discrete set of tences s


sns we use a word ding matrix m rv d to embed each word wi in sentence into continuous space wi where v is the vocabulary size d is the dimension of word embedding
the sentence encoder can be based on a variety of encoding schemes
simply taking the average of embeddings of words in a sentence will cause too much information loss while using grus or long short term memory lstm requires more computational resources and is prone to ting
considering above we select positional coding described in sukhbaatar et al
as our sentence encoding method
each sentence resentation si is calculated by si lj wi j where is element wise multiplication lj is a umn vector computed as lj j d nw nw lj denotes the th dimension of lj
note that throughout this study we use grus as our rnn cells since they can alleviate the tting problem as conrmed by our experiments
as our selective reading mechanism which will be explained later is a modied version of inal gru cell we give the details of the gru here
gru is a gating mechanism in recurrent neural networks introduced in cho et al

their performance was found to be similar to that of lstm cell but using fewer parameters as scribed in hochreiter and schmidhuber
the gru cell consists of an update gate vector figure model structure there is one encoder one decoder and one iterative unit which is used to polish document representation in each iteration
the nal labeling part is used to generating the extracting probabilities for all sentences combining hidden states of decoders in all iterations
we take a document consists of three sentences for example here
ui a reset gate vector ri and an output vector hi
for each time step i with input xi and vious hidden state the updated hidden state hi is computed by ui u ri u hi ri u hi ui hi ui the is sigmoid w u w r w h activation where function rnh ni u u u r u rnh nh nh is the hidden size ni is the size of input xi
to further study the interactions and tion exchanges between sentences we establish a bi directional gru bi gru network taking the sentence representation as input si si si si where si is the sentence representation input at time step i si is the hidden state of the forward gru at time step i and si is the hidden state of the backward gru
this architecture allows formation to ow back and forth to generate new sentence representation
document encoder we must initialize a ument representation before polishing it
ating the document representation from sentence representations is a process similar to ing the sentence representation from word dings
this time we need to compress the whole document not just a sentence into a vector
cause the information a vector can contain is ited rather than to use another neural network we simply use a non linear transformation of the erage pooling of the concatenated hidden states of the above bi gru to generate the document resentation as written below si si ns where is the concatenation operation
selective reading module now we can mally introduce the selective reading module in fig

this module is a bidirectional rnn sisting of modied gru cells whose input is the sentence representation s


sns
in the original version of gru the update gate ui in equation is used to decide how much of den state should be retained and how much should be updated
however due to the way ui is lated it is sensitive to the position and ordering of sentences but loses information captured by the polished document representation
herein we propose a modied gru cell that reading iterative unititerative unit replace the ui with the newly computed update gate gi
the new cell takes in two inputs the tence representation and the document tion from the last iteration rather than merely the sentence representation
for each sentence the lective network generates an update gate vector gi in the following way fi si fi w gi where si is the i th sentence representation is the document representation from last eration
equation now becomes hi gi hi gi we use this selective reading module to matically decide to which extent the information of each sentence should be updated based on its relationship with the polished document
in this way the modied gru network can grasp more accurate information from the document

iterative unit after each sentence passes through the selective reading module we wish to update the document representation with the newly constructed sentence representations
the iterative unit also depicted above in fig
is designed for this pose
we use a gruiter cell to generate the ished document representation whose input is the nal state of the selective reading network from the previous iteration hns and whose initial state is set to the document representation of the ous iteration
the updated document resentation is computed by dk
decoder next we describe our decoders which are picted shaded in the right part of fig

ing most sequence labeling task xue and palmer carreras and where they learn a feature vector for each sentence we use a bidirectional grudec network in each iteration to output features so as to calculate extracting bilities
for k th iteration given the sentence resentation s as input and the document sentation dk as the initial state our decoder codes the features of all sentences in the hidden state hk


hk ns hk hk hk dk
sentence labeling module next we use the feature of each sentence to ate corresponding extracting probability
since we have one decoder in each iteration if we directly transform the hidden states in each iteration to tracting probabilities we will end up with several scores for each sentence
either taking the age or summing them together by specic weights is inappropriate and inelegant
hence we nate hidden states of all decoders together and ply a multi layer perceptron to them to generate the extracting probabilities w


where


yns yi is the extracting ability for each setence
in this way we let the model learn by itself how to utilize the outputs of all iterations and assign to each hidden state a liable weight
in section we will show that this labeling method outperforms other methods
experiment setup in this section we present our experimental setup for training and estimating our summarization model
we rst introduce the datasets used for training and evaluation and then introduce our perimental details and evaluation protocol

datasets in order to make a fair comparison with our lines we used the cnn dailymail which was constructed by hermann et al

we used the standard splits for training validation and testing in each corpus uments for cnn and for dailymail
we followed previous studies in ing the human written story highlight in each cle as a gold standard abstractive summary
these highlights were used to generate gold labels when training and testing our model using the greedy search method similar to nallapati et al

we also tested its on an out of domain pus which consists of documents
documents in this corpus belong to various clusters and each cluster has a unique topic
each document has two gold summaries written by man experts of length around words

implementation details we implemented our model in tensorow abadi et al

the code for our models is able
we mostly followed the settings in nallapati et al
and trained the model ing the adam optimizer kingma and ba with initial learning rate
and anneals of
every epochs until reaching epochs
we lected three sentences with highest scores as mary
after preliminary exploration we found that arranging them according to their scores sistently achieved the best performance
periments were performed with a batch size of documents
we used dimension glove pennington et al
embeddings trained on wikipedia as our embedding initialization with a vocabulary size limited to for speed purposes
we initialized out of vocabulary word embeddings over a uniform distribution within

we also padded or cut sentences to tain exactly words
each gru module had layer with dimensional hidden states and with either an initial state set up as described above or a random initial state
to prevent overtting we used dropout after each gru network and ding layer and also applied loss to all ased variables
the iteration number was set to if not specied
a detailed discussion about tion number can be found in section

baselines on all datasets we used the method as a baseline which simply chooses the rst three tences in a document as the gold summary
on dailymail datasets we report the performance of summarunner in nallapati et al
and the model in cheng and lapata as well as a logistic regression classier lreg that they used as a baseline
we reimplemented the brid memnet model in singh et al
as one of our baselines since they only reported the formance of samples in their paper
also narayan et al
released their for the refresh model we used their code to produce rouge recall scores on the dailymail dataset as they only reported results on cnn dailymail joint dataset
baselines on cnn dataset are similar

com yingtaomj iterati ve document representation learning tow ards summarization with polishing
com edinburghnlp refr esh on corpus we compare our model with several baselines such as integer linear ming ilr and lreg
we also report the mance of the newest neural networks model cluding nallapati et al
cheng and ata singh et al


evaluation in the evaluation procedure we used the rouge scores i
e
and rouge l responding to the matches of unigram bigrams and longest common subsequence lcs tively to estimate our model
we obtained our rouge scores using the standard pyrouge
to compare with other related works we used full length score on the cnn corpus ited length of bytes and bytes recall score on dailymail corpus
as for the corpus following the ofcial guidelines we examined the rouge recall score at the length of words
all results in our experiment are statistically ca nt using condence interval as estimated by rouge script
schluter noted that only using the rouge metric to evaluate summarization quality can be misleading
therefore we also evaluated our model using human evaluation
five highly ucated participants were asked to rank maries produced by four models the line hybrid memnet its and human authored highlights
we chose hybrid memnet as one of the human evaluation baselines since its mance is relatively high compared to other lines
judging criteria included informativeness and coherence
test cases were randomly sampled from dailymail test set
experiment analysis table shows the performance comparison of our model with other baselines on the dailymail dataset with respect to rouge score at bytes and bytes of summary length
our model performs consistently and signicantly better than other models on bytes while on bytes the improvement margin is smaller
one possible terpretation is that our model has high precision on top rank outputs but the accuracy is lower for lower rank sentences
in addition cheng and lapata used additional supervised training
python
org pypi pyrouge

rouge l rouge l dailymail cheng et
summarunner refresh hybrid memnet its





































table comparison with other baselines on dailymail test dataset using rouge recall score with respect to the abstractive ground truth at bytes and at bytes
cnn cheng et
hybrid memnet refresh its rouge l














table comparison with other baselines on cnn test dataset using full length variants of rouge
to create sentence level extractive labels to train their model while our model uses an unsupervised greedy approximation instead
we also examined the performance of our model on cnn dataset as listed in table
to compare with other models we used full length rouge metric as reported by narayan et al

results demonstrate that our model has a consistently best performance on different datasets
in table we present the performance of its on the out of domain duc dataset
our model performs or matches other basic models including lreg and ilr as well as neural network baselines such as summarunner with respect to the ground truth at bytes which shows that our model can be adapted to different copora maintaining high accuracy
in order to explore the impact of internal ture of its we also conducted an ablation study in table
the rst variation is the same model without the selective reading module
the ond one sets the iteration number to one that is a model without iteration process
the last variation is to apply mlp on the output from the last tion instead of concatenating the hidden states of all decoders
all other settings and parameters are the same
performances of these models are worse than that of its in all metrics which demonstrates lreg ilp cheng et
summarunner hybrid memnet its rouge l




















table comparison with other baselines on dataset using rouge recall score with spect to the abstractive ground truth at bytes
variations its
selective reading

iteration
concatenation l







table ablation study on dailymail test dataset with respect to the abstractive ground truth at bytes
the preeminence of its
more importantly by this controlled experiment we can verify the tion of different module in its
further discussion analysis of iteration number we did a broad sweep of experiments to further investigate the uence of iteration process on the generated mary quality
first we studied the inuence of iteration number
in order to make a fair ison between models with different iteration ber we trained all models for same epochs without tuning
fig
illustrates the relationship between iteration number and the rouge score at bytes of summary length on dailymail test dataset
the result shows that the rouge score increases with the number of iteration to begin with
after ing the upper limit it begins to drop
note that models hybrid memnet its gold















table system ranking comparison with other lines on dailymail corpus
rank is the best and rank is the worst
each score represents the percentage of the summary under this rank
sitivity between iterations as shown in fig

to be specic the sentences which are not preferred by iteration remain low probabilities in the next two iterations while sentences with relatively high scores are still preferred by iteration and
human evaluation we gave human tors three system generated summaries generated by hybrid memnet its as well as the human written gold standard summary and asked them to rank these summaries based on summary informativeness and coherence
table shows the percentages of summaries of different models der each rank scored by human experts
it is not surprising that gold standard has the most maries of the highest quality
our model has the most summaries under rank thus can be sidered best following are hybrid memnet and as they are ranked mostly and
by case study we found that a number of maries generated by hybrid memnet have two sentences the same as its out of three however the third distinct sentence from our model always leads to a better evaluation result considering all informativeness and coherence
readers can refer to the appendix to see our case study
conclusion in this work we introduce its an iteration based extractive summarization model inspired by the observation that it is often necessary for a man to read the article multiple times to fully derstand and summarize it
experimental results on cnn dailymail and duc corpora demonstrate the effectiveness of our model
acknowledgments we would like to thank the anonymous ers for their constructive comments
we would also like to thank jin ge yao and zhengyuan ma for their valuable advice on this project
this work was supported by the national key figure relationship between number of iteration and rouge score on dailymail test dataset with respect to the ground truth at bytes
figure the predicted extracting probabilities for each sentence calculated by the output of each iteration
the result of training the model for only one epoch outperforms the state of the art in singh et al
which demonstrates that our selective ing module is effective
the fact that ing this process increase the performance conrms that the iteration idea behind our model is useful in practice
based on above observation we set the default iteration number to be
analysis of polishing process next to fully investigate how the iterative process inuences the extracting results we draw heatmaps of the tracting probabilities for each decoder at each eration
we pick two representative cases in fig
where the axis represents the sentence index and y axis is the iteration number axis labels are omitted
the darker the color is the higher the in fig
it can be extracting probability is
seen that when the iteration begins most sentences have similar probabilities
as we increase the number of iteration some probabilities begin to fall and others saturate
this means that the model already has preferred sentences to select
another interesting feature we found is that there is a






search and development program of china no
the national science dation of china nsfc no
no

rui yan was sponsored by tencent open research fund and microsoft search asia msra collaborative research gram
references martn abadi paul barham jianmin chen zhifeng chen andy davis jeffrey dean matthieu devin sanjay ghemawat geoffrey irving michael isard et al

tensorow a system for large scale machine learning
in osdi volume pages
xavier carreras and llus

duction to the shared task semantic role labeling
in proceedings of the ninth ence on computational natural language ing conll pages stroudsburg pa usa
association for computational linguistics
jianpeng cheng and mirella lapata

neural summarization by extracting sentences and words
kyunghyun cho bart van merrienboer caglar cehre dzmitry bahdanau fethi bougares holger schwenk and yoshua bengio

learning phrase representations using rnn encoder decoder for statistical machine translation
computer ence
kevin bretonnel cohen

natural language cessing for online applications text retrieval language traction and categorization review

karl moritz hermann edward grefenstette lasse peholt will kay mustafa suleyman and phil som

teaching machines to read and hend
pages
sepp hochreiter and jurgen schmidhuber

neural computation long short term memory

diederik kingma and jimmy ba

adam a method for stochastic optimization
computer ence
ramesh nallapati igor melnyk abhishek kumar and bowen zhou

sengen sentence generating neural variational topic model
ramesh nallapati feifei zhai and bowen zhou

summarunner a recurrent neural network based sequence model for extractive summarization of documents
ramesh nallapati bowen zhou cicero nogueira dos santos caglar gulcehre and bing xiang

abstractive text summarization using sequence sequence rnns and beyond
shashi narayan shay b
cohen and mirella lapata

ranking sentences for extractive tion with reinforcement learning
jeffrey pennington richard socher and christopher manning

glove global vectors for word in conference on empirical representation
ods in natural language processing pages
dragomir r radev timothy allison sasha goldensohn john blitzer arda celebi stanko dimitrov elliott drabek ali hakim wai lam danyu liu al

mead a platform for tidocument multilingual text summarization
in lrec
alexander m
rush sumit chopra and jason weston

a neural attention model for abstractive tence summarization
computer science
natalie schluter

the limits of automatic in proceedings of marisation according to rouge
the conference of the european chapter of the association for computational linguistics volume short papers volume pages
abigail see peter j
liu and christopher d
manning

get to the point summarization with generator networks
corr

abhishek kumar singh manish gupta and vasudeva varma

hybrid memnet for extractive marization
pages
sainbayar sukhbaatar arthur szlam jason weston and rob fergus

end to end memory works
computer science
kristian woodsend and mirella lapata

in meeting matic generation of story highlights
of the association for computational linguistics pages
yuxiang wu and baotian hu

learning to extract coherent summary via deep reinforcement learning
arxiv preprint

caiming xiong stephen merity and richard socher

dynamic memory networks for visual and textual question answering
nianwen xue and martha palmer

calibrating features for semantic role labeling
in proceedings of the conference on empirical methods in natural language processing
rui yan

i poet automatic poetry composition through recurrent neural networks with iterative ishing schema
in ijcai pages
rui yan cheng te li xiaohua hu and ming zhang

chinese couplet generation with neural work structures
in meeting of the association for computational linguistics pages
rui yan jian yun nie and xiaoming li

marize what you are interested in an optimization framework for interactive personalized tion
in conference on empirical methods in ral language processing emnlp july john mcintyre conference centre edinburgh uk a meeting of sigdat a special interest group of the acl pages
rui yan xiaojun wan mirella lapata wayne xin zhao pu jen cheng and xiaoming li

sualizing timelines evolutionary summarization via iterative reinforcement between text and image in proceedings of the acm streams
national conference on information and knowledge management pages
acm
rui yan xiaojun wan jahna otterbacher liang kong xiaoming li and yan zhang

ary timeline summarization a balanced in tion framework via iterative substitution
national acm sigir conference on research and development in information retrieval pages

