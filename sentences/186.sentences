pretraining based natural language generation for text summarization haoyu jianjun ji of computer national university of defense technology changsha china jjxu
edu
cn r a l c
s c v
v i x r a abstract in this paper we propose a novel pretraining based encoder decoder framework which can generate the output sequence based on the input sequence in a two stage manner
for the encoder of our model we encode the input sequence into text representations using bert
for the decoder there are two stages in our model in the rst stage we use a transformer based decoder to generate in the second stage we a draft output sequence
mask each word of the draft sequence and feed it to bert then by combining the input sequence and the draft representation generated by bert we use a transformer based decoder to predict the rened word for each masked position
to the best of our knowledge our approach is the rst method which applies the bert into text generation tasks
as the rst step in this direction we evaluate our proposed method on the text summarization task
mental results show that our model achieves new state of the art on both cnn daily mail and new york times datasets
introduction text summarization generates summaries from input ments while keeping salient information
it is an important task and can be applied to several real world applications
many methods have been proposed to solve the text marization problem see et al
nallapati et al
zhou et al
gehrmann et al

there are two main text summarization techniques extractive and tive
extractive summarization generates summary by ing salient sentences or phrases from the source text while abstractive methods paraphrase and restructure sentences to compose the summary
we focus on abstractive tion in this work as it is more exible and thus can generate more diverse summaries
recently many abstractive approaches are introduced based on neural sequence to sequence framework paulus et al
see et al
gehrmann et al
li et al

based on the sequence to sequence model with copy contact author
mechanism see et al
incorporates a coverage vector to track and control attention scores on source text
paulus et al
introduce intra temporal attention processes in the encoder and decoder to address the repetition and incoherent problem
there are two issues in previous abstractive methods these methods use left context only decoder thus do not have complete context when predicting each word
they do not utilize the pre trained contextualized language models on the decoder side so it is more difcult for the decoder to learn summary representations context interactions and language modeling together
recently bert has been successfully used in various ural language processing tasks such as textual entailment name entity recognition and machine reading sions
in this paper we present a novel natural language generation model based on pre trained language models we use bert in this work
as far as we know this is the rst work to extend bert to the sequence generation task
to dress the above issues of previous abstractive methods in our model we design a two stage decoding process to make good use of bert s context modeling ability
on the rst stage we generate the summary using a left context only decoder
on the second stage we mask each word of the summary and predict the rened word one by one using a rene decoder
to further improve the naturalness of the generated sequence we cooperate reinforcement objective with the rene decoder
the main contributions of this work are
we propose a natural language generation model based on bert making good use of the pre trained language model in the encoder and decoder process and the model can be trained end to end without handcrafted features

we design a two stage decoder process
in this tecture our model can generate each word of the summary considering both sides context information

we conduct experiments on the benchmark datasets cnn daily mail and new york times
our model achieves a
average of and rouge l on the cnn daily mail which is state of the art
on the new york times dataset our model achieves about
relative improvement over
background
text summarization in this paper we focus on single document multi sentence summarization and propose a supervised abstractive model based on the neural attentive sequence to sequence work which consists of two parts a neural network for the encoder and another network for the decoder
the encoder encodes the input sequence to intermediate representation and the decoder predicts one word at a time step given the input sequence representation vector and previous decoded output
the goal of the model is to maximize the probability of ating the correct target sequences
in the encoding and ation process the attention mechanism is used to concentrate on the most important positions of text
the learning tive of most sequence to sequence models is to minimize the negative log likelihood of the generated sequence as shown in following equation where y t is the t th ground truth mary token
loss log p y t t x however with this objective traditional sequence ation models consider only one direction context in the coding process which could cause performance degradation since complete context of one token contains preceding and following tokens thus feeding only preceded decoded words to the decoder so that the model may generate unnatural quences
for example attentive sequence to sequence els often generate sequences with repeated phrases which harm the naturalness
some previous works mitigate this problem by improving the attention calculation process but in this paper we show that feeding bi directional context stead of left only context can better alleviate this problem

bi directional pre trained context encoders recently context encoders such as elmo gpt and bert have been widely used in many nlp tasks
these models are pre trained on a huge unlabeled corpus and can generate better contextualized token embeddings thus the approaches built on top of them can achieve better performance
since our method is based on bert we illustrate the cess briey here
bert consists of several layers
in each layer there is rst a multi head self attention sub layer and then a linear afne sub layer with the residual connection
in each self attention sub layer the attention scores eij are rst calculated as eq
in which is output dimension and w q w k w v are parameter matrices
put hi
the last layer outputs is context encoding of input sequence
oi hi n despite the wide usage and huge success there is also a mismatch problem between these pre trained context coders and sequence to sequence models
the issue is that while using a pre trained context encoder like bert they model token level representations by conditioning on both rection context
during pre training they are fed with plete sequences
however with a left context only decoder these pre trained language models will suffer from plete and inconsistent context and thus can not generate good enough context aware word representations especially ing the inference process
model in this section we describe the structure of our model which learns to generate an abstractive multi sentence summary from a given source document
based on the sequence to sequence framework built on top of bert we rst design a rene decoder at word level to tackle the two problems described in the above section
we also introduce a discrete objective for the rene decoders to reduce the exposure bias problem
the overall structure of our model is illustrated in figure

problem formulation we denote the input document as x


xm where x represents one source token
the corresponding mary is denoted as y


yl l represents the mary length
given input document x we rst predict the summary draft by a left context only decoder and then using the erated summary draft we can condition on both context sides and rene the content of the summary
the draft will guide and constrain the rene process of summary

summary draft generation the summary draft is based on the sequence to sequence model
on the encoder side the input document x is coded into representation vectors h


hm and then fed to the decoder to generate the summary draft a



aij eij exp eij exp eik encoder we simply use bert as the encoder
it rst maps the input sequence to word embeddings and then computes document embeddings as the encoder s output denoted by following equation
then the output is calculated as eq
which is the weighted sum of previous outputs h added by previous h bert


xm figure model overview n represents decoder layer number and l represents summary length
summary draft decoder in the draft decoder we rst introduce bert word bedding matrix to map the previous summary draft outputs


into embeddings vectors


at time step
note that as the input sequence of the decoder is not complete we do not use the bert network to predict the context vectors here
then we introduce an n layer transformer decoder to learn the conditional probability p
transformer s encoder decoder multi head attention helps the decoder learn soft alignments between summary and source document
at the t th time step the draft decoder predicts output ability conditioned on previous outputs and encoder den representations as shown in eq
in which q t



each generated sequence will be truncated in the rst position of a special token
the total mary draft decoder progress is shown in stage of figure
this issue exists when using any other contextualized word representations so we design a rene process to mitigate it in our approach which will be described in the next sub section
copy mechanism as some summary tokens are out of vocabulary words and occurs in input document we incorporate copy nism gu et al
based on the transformer decoder we will describe it briey
at decoder time step t we rst calculate the attention ability distribution over source document x using the linear dot product of the last layer decoder output of former ot and the encoder output hj shown in eq

uj t j t otwchj exp uj t exp uk t p vocab ldec t w t h log p at y as eq
shows the decoder s learning objective is to minimize negative likelihood of conditional probability in which y t is the t th ground truth word of summary
t t h however a decoder with this structure is not sufcient enough if we use the bert network in this decoder then during training and inference in complete of sentence is fed into the bert module and although we can ne tune bert s parameters the input distribution is quite different from the pre train process and thus harms the ity of generated context representations
if we just use the embedding matrix here it will be more difcult for the decoder with fresh parameters to learn to model representations as well as vocabulary probabilities from a relative small corpus compared to bert s huge training corpus
in a word the decoder can not utilize bert s ability to generate high quality context vectors which will also harm performance
we then calculate copying gate gt which makes a soft choice between selecting from source and generating from vocabulary wc wg bg are parameters gt ot bg using gt we calculate the weighted sum of copy ity and generation probability to get the nal predicted ability of extended vocabulary v x where x is the set of out of vocabulary words from the source document
the nal probability is calculated as follow vocab w gt t i t i wi w
summary rene process the main reason to introduce the rene process is to enhance the decoder using bert s contextualized representations so we do not modify the encoder and reuse it during this process
on the decoder side we propose a new word level rene decoder
the rene decoder receives a generated summary multi head attention decoderbertsummary draft draft outputbertmulti head attention embeddingdocumentsummary draft embeddingsummary outputmask draft as input and outputs a rened summary
as figure stage shows it rst masks each word in the summary draft one by one then feeds the draft to bert to generate context vectors
finally it predicts a rened summary word using an n layer transformer decoder which is the same as the draft decoder
at t th time step the t th word of input summary is masked and the decoder predicts the rened word given other words of the summary
the learning objective of this process is shown in eq
t for the ground truth yt is the t th summary word and y mary word and






lref ine log p yt y t h from the view of bert or other contextualized dings the rene decoding process provides a more complete input sequence which is consistent with their pre training cesses
intuitively this process works as follows the draft decoder writes a summary draft based on a document and then the rene decoder edits the draft
it concentrates on one word at a time based on the source document as well as other words
we design the word level rene decoder because this cess is similar to the cloze task in bert s pre train cess therefore by using the ability of the contextual language model the decoder can generate more uent and natural quences
the parameters are shared between the draft decoder and rene decoder as we nd that using individual parameters the model s performance degrades a lot
the reason may be that we use teach forcing during training and thus the level rene decoder learns to predict words given all the other ground truth words of summary
this objective is similar to the language model s pre train objective and is probably not enough for the decoder to learn to generate rened maries
so in our model all decoders share the same ters
mixed objective for summarization rouge is usually used as the tion metric however during model training the objective is to maximize the log likelihood of generated sequences
this mis match harms the model s performance
similar to vious work kryscinski et al
we add a discrete jective to the model and optimize it by introducing the icy gradient method
the discrete objective for the summary draft process is as shown in eq
where as is the draft summary sampled from predicted distribution and is the reward score compared with the ground truth mary we use rouge l in our experiment
to balance tween optimizing the discrete objective and generating able sequences we mix the discrete objective with likelihood objective
eq
shows the nal objective for the draft process note here ldec is logp
in the rene process we introduce similar objectives

learning and inference during model training the objective of our model is sum of the two processes jointly trained using teacher forcing gorithm
during training we feed the ground truth summary to each decoder and minimize the following objective
lmodel ldec lref ine at test time each time step we choose the predicted word by y use beam search to generate the draft summaries and use greedy search to generate the ned summaries
experiment
settings in this work all of our models are built on bertbase though another larger pre trained model with better mance bertlarge has published but it costs too much time and gpu memory
we use wordpiece embeddings with a vocabulary which is the same as bert
we set the layer of transformer decoders to on and set the attention heads number to on set fully connected sub layer hidden size to
we train the model using an adam optimizer with learning rate of

and and use a dynamic learning rate during the training process
for regularization we use dropout srivastava et al
and label ing szegedy et al
in our models and set the dropout rate to
and the label smoothing value to

we set the rl objective factor to

during training we set the batch size to and train for epochs for since it has many fewer ing samples after training the best model are selected from last models based on development set performance
due to gpu memory limit we use gradient accumulation set cumulate step to and feed samples at each step
we use beam size and length penalty of
to generate logical form sequences
we lter repeated tri grams in beam search process by ting word probability to zero if it will generate an tri gram which exists in the existing summary
it is a nice method to avoid phrase repetition since the two datasets seldom contains repeated tri grams in one summary
we also ne tune the erated sequences using another two simple rules
when there are multi summary sentences with exactly the same content we keep the rst one and remove the other sentences we also remove sentences with less than words from the result
datasets to evaluate the performance of our model we conduct ments on cnn daily mail dataset which is a large collection of news articles and modied for summarization
ing see et al
we choose the non anonymized version of the dataset which consists of more than training samples and test set samples
we also conduct experiments on the new york dataset which also consists of many news articles
the original dataset can be applied here
in our lrl ldec lrl dec dec ldec
nist
rouge l r avg model extractive see et al
summmarunner nallapati et al
refresh narayan et al
deepchannel shi et al
rnn ext rl chen and bansal mask lm global chang et al
neusum et al
abstractive see et al
attn et al
inconsistency et al
bottom up summarization gehrmann et al
dca celikyilmaz et al
ours one stage two stage two stage rl



























































table rouge results for various models and ablations on the cnn daily mail test set
r avg calculates average score of and rouge l
experiment we follow the dataset splits and other pre process settings of durrett et al

we rst lter all samples without a full article text or abstract and then remove all samples with summaries shorter than words
then we choose the test set based on the date of examples published after january
the nal dataset contains training samples and test samples and is called since all summaries are longer than words
we tokenize all sequences of the two datasets using the wordpiece tokenizer
after tokenizing the average article length and summary length of cnn daily mail are and and s average article length and summary length are and
we truncate the article length to and the summary length to in our summary length is set to on as its average golden summary length is longer
evaluation metrics on cnn daily mail dataset we report the full length score of the and rouge l rics calculated using pyrouge and the porter mer option
on following paulus et al
we evaluate limited length rouge recall the ated summary length to the ground truth length
we split summaries into sentences by semicolons to calculate the rouge scores

results and analysis table gives the results on cnn daily mail dataset we pare the performance of many recent approaches with our model
we classify them to two groups based on whether they are extractive or abstractive models
as the last line of the ble lists the and score of our full model
python
org pypi

is comparable with dca and outperforms on rouge l
also compared to extractive models neusum and lm global we achieve slight higher
except the four scores our model outperforms these models on all the other scores and since we have condence interval of at most
these improvements are statistically signicant
ablation analysis as the last four lines of table show we conduct an tion study on our model variants to analyze the importance of each component
we use three ablation models for the periments
one stage a sequence to sequence model with copy mechanism based on bert two stage adding the ne decoder to the one stage model two stage rl full model with rene process cooperated with rl objective
first we compare the two model with stage ablation we observe that the full model outperforms by
on average rouge suggesting that the ment objective helps the model effectively
then we analyze the effect of rene process by removing it from the two stage model we observe that without the rene process the average rouge score drops by

the ablation study proves that each module is necessary for our full model and the ments are statistically signicant on all metrics
effects of summary length to evaluate the impact of summary length on model formance we compare the average rouge score ments of our model with different length of ground truth maries
as the above of figure shows compared to pointer generator with coverage on length interval about of test set the improvements of our model are higher than shorter samples conrms that with ter context representations in longer documents our model can achieve higher performance
as shown in the below of figure compared related work
text summarization text summarization models are usually classied to stractive and extractive ones
recently extractive models like deepchannel shi et al
rnn chen and bansal and neusum et al
achieve higher performances using well designed structures
for ample deepchannel propose a salience estimation network and iteratively extract salient sentences
zhang et al
train a sentence compression model to teach another latent variable extractive model
also several recent works focus on improving abstractive methods
gehrmann et al
design a content selector to over determine phrases in a source document that should be hsu et al
introduce part of the summary
tency loss to force words in less attended termined by extractive model to have lower generation li et al
extend model with an abilities
information selection network to generate more informative summaries

pre trained language models pre trained word vectors mikolov et al
pennington et al
bojanowski et al
have been widely used in many nlp tasks
more recently pre trained language models elmo gpt and bert have also achieved great success on several nlp problems such as textual entailment semantic similarity reading comprehension and question answering peters et al
radford et al
devlin et al

some recent works also focus on leveraging pre trained language models in summarization
radford et al
pretrain a language model and use it as the sentiment kryscinski et al
yser when generating reviews of goods
train a language model on golden summaries and then use it on the decoder side to incorporate prior knowledge
in this work we use is a pre trained language model using large scale unlabeled data on the encoder and decoder of a model and by designing a two stage coding structure we build a competitive model for abstractive text summarization
conclusion and future work in this work we propose a two stage model based on sequence to sequence paradigm
our model utilize bert on both encoder and decoder sides and introduce reinforce jective in learning process
we evaluate our model on two benchmark datasets cnn daily mail and new york times the experimental results show that compared to previous tems our approach effectively improves performance
although our experiments are conducted on tion task our model can be used in most natural language generation tasks such as machine translation question ation and paraphrasing
the rene decoder and mixed tive can also be applied on other sequence generation tasks and we will investigate on them in future work
figure average rouge l improvement on cnn daily mail test set samples with different golden summary length
to extractive baseline see et al
the tage of our model will fall when golden summary length is greater than
this probably because that we truncate the long documents and golden summaries and can not get full formation it could also because that the training data in these intervals is too few to train an abstractive model so simple extractive method will not fall too far behind
model first sentences first k words full durrett et al
attn et al
two stage rl ours









table limited length rouge recall results on the test set

additional results on table reports experiment results on the corpus
since the short summary samples are ltered has erage longer summaries than cnn daily mail
so the model needs to catch long term dependency of the sequences to erate good summaries
the rst two lines of table show results of the two baselines introduced by durrett et al
these lines select rst n sentences or select the rst k words from the original document
also we compare performance of our model with two recent models we see
improvements compared to the with intra attn sota carries over to this dataset which is a large margin
on our model also get an provement of

the experiment proves that our approach can outperform competitive methods on different data butions
references bojanowski et al
piotr bojanowski edouard grave armand joulin and tomas mikolov
enriching word tors with subword information
transactions of the ciation for computational linguistics
celikyilmaz et al
asli celikyilmaz antoine lut xiaodong he and yejin choi
deep ing agents for abstractive summarization
arxiv preprint

chang et al
ming wei chang kristina toutanova language model kenton lee and jacob devlin
pre training for hierarchical document representations
arxiv preprint

chen and bansal yen chun chen and mohit bansal
fast abstractive summarization with reinforce selected arxiv preprint
sentence rewriting

devlin et al
jacob devlin ming wei chang ton lee and kristina toutanova
bert pre training of deep bidirectional transformers for language ing
arxiv preprint

durrett et al
greg durrett taylor berg kirkpatrick and dan klein
learning based single document rization with compression and anaphoricity constraints
in proceedings of the annual meeting of the tion for computational linguistics acl august berlin germany volume long papers
gehrmann et al
sebastian gehrmann yuntian deng and alexander m rush
bottom up abstractive summarization
arxiv preprint

gu et al
jiatao gu zhengdong lu hang li and incorporating copying mechanism in victor o
k
li
sequence to sequence learning
in acl
hsu et al
wan ting hsu chieh kai lin ying lee kerui min jing tang and min sun
a unied model for extractive and abstractive summarization ing inconsistency loss
arxiv preprint

al
wojciech kryscinski paulus caiming xiong and richard socher
ing abstraction in text summarization


romain arxiv preprint li et al
wei li xinyan xiao yajuan lyu and yuanzhuo wang
improving neural abstractive document summarization with explicit information selection eling
in emnlp pages
mikolov et al
tomas mikolov ilya sutskever kai chen greg s corrado and jeff dean
distributed sentations of words and phrases and their ity
in advances in neural information processing systems pages
nallapati et al
ramesh nallapati feifei zhai and bowen zhou
summarunner a recurrent neural work based sequence model for extractive summarization in proceedings of the thirty first aaai of documents
conference on articial intelligence pages
narayan et al
shashi narayan shay b cohen and mirella lapata
ranking sentences for extractive marization with reinforcement learning
arxiv preprint

et al
romain paulus caiming xiong richard socher and palo alto
a deep reinforced model for abstractive summarization
iclr pages
pennington et al
jeffrey richard socher and christopher manning
glove global vectors in proceedings of the for word representation
conference on empirical methods in natural language processing emnlp pages
pennington peters et al
matthew e peters mark neumann hit iyyer matt gardner christopher clark kenton lee and luke zettlemoyer
deep contextualized word sentations
arxiv preprint

radford et al
alec radford rafal jozefowicz and ilya sutskever
learning to generate reviews and ering sentiment
corr

radford et al
alec radford karthik narasimhan improving language tim salimans and ilya sutskever
understanding by generative pre training

see et al
abigail see peter j
liu and pher d
manning
get to the point summarization with in proceedings of the pointer generator networks
annual meeting of the association for computational guistics acl pages
shi et al
jiaxin shi chen liang lei hou juanzi li zhiyuan liu and hanwang zhang
deepchannel salience estimation by contrastive learning for tive document summarization
corr
srivastava et al
nitish srivastava geoffrey hinton alex krizhevsky ilya sutskever and ruslan nov
dropout a simple way to prevent neural networks from overtting
journal of machine learning research
szegedy et al
christian szegedy vincent houcke sergey ioffe jonathon shlens and zbigniew wojna
rethinking the inception architecture for computer in ieee conference on computer vision vision
and pattern recognition cvpr pages
zhang et al
xingxing zhang mirella lapata furu wei and ming zhou
neural latent extractive document summarization
arxiv preprint

et al
qingyu zhou nan yang furu wei shaohan huang ming zhou and tiejun zhao
neural document summarization by jointly learning to score and select sentences
in proceedings of the annual ing of the association for computational linguistics acl melbourne australia july volume long papers pages

