v o n l c
s c v
v i x r a topic centric unsupervised multi document summarization of scientic and news articles amanuel alambo computer science and engineering wright state university dayton oh alambo

edu cori lohstroh oneil center wright state university dayton oh lohstroh

edu erik madaus oneil center wright state university dayton oh madaus

edu swati padhee computer science and engineering wright state university dayton oh padhee

edu brandy foster oneil center wright state university dayton oh brandy

edu tanvi banerjee computer science and engineering wright state university dayton oh tanvi

edu krishnaprasad thirunarayan computer science and engineering wright state university dayton oh t


edu michael raymer computer science and engineering wright state university dayton oh michael

edu abstract recent advances in natural language processing have enabled automation of a wide range of tasks including machine translation named entity recognition and sentiment analysis
automated summarization of documents or groups of ments however has remained elusive with many efforts limited to extraction of keywords key phrases or key sentences
accurate abstractive summarization has yet to be achieved due to the inherent difculty of the problem and limited availability of training data
in this paper we propose a topic centric pervised multi document summarization framework to generate extractive and abstractive summaries for groups of scientic articles across fields of study fos in microsoft academic graph mag and news articles from task
the posed algorithm generates an abstractive summary by developing salient language unit selection and text generation techniques
our approach matches the state of the art when evaluated on automated extractive evaluation metrics and performs better for abstractive summarization on ve human evaluation metrics entailment coherence conciseness readability and grammar
we achieve a kappa score of
between two co author linguists who evaluated our results
we plan to publicly share a human validated gold standard dataset of topic clustered research articles and their summaries to promote research in abstractive summarization
index terms abstraction language units multi document summarization text generation hierarchical clustering this effort was sponsored in whole or in part by the air force search laboratory usaf under memorandum of understanding partnership intermediary agreement no
the u
s
government is authorized to reproduce and distribute reprints for governmental purposes notwithstanding any copyright notation thereon
i
introduction with the large number of articles published in the research and media community there is an increasing demand to produce summaries that are coherent concise informative and matical
summarization comes in two forms extractive and abstractive
extractive summarization is focused on extracting signicant sentences from source documents and has been well studied
abstractive summarization aims at ways of fusing or paraphrasing sentences in source documents to form abstractive sentences
due to the challenges of capturing abstractive concepts shared among sentences across source documents and synthesizing an informative summary there has been limited progress in abstractive summarization
while there are recent advances in unsupervised multi document abstractive summarization they are usually limited to forming summaries by copying words in source documents and re arranging the words to form new sentences
these approaches identify salient phrases from sentences in source documents and fuse them to form abstractive summaries
thus they do not perform abstraction of sentences
our framework consists of two phases an extractive phase and an abstractive phase
in the extractive phase we follow a three fold approach
first we identify the core article and peripheral articles for each set of related articles
second we instantiate clusters using the language units of a core article and perform centroid based clustering to place language units from peripheral articles into the clusters initialized by the language units in a core article
third we fuse language units in a cluster using an enhanced multi sentence compression msc technique with a novel algorithm to maximize topical coverage and relevance of a path to the language units in the cluster
in the abstractive summarization phase we employ text generation to generate abstractive language units alus and we use msc to fuse the generated alus into an abstractive summary
unlike where articles are topically clustered scientic articles in do not come topically grouped
therefore for we use topical hierarchical agglomerative clustering hac to cluster articles
the key contributions of our study are an alu generation technique using a novel msc based algorithm for selection of informative paths and a gold standard dataset of topical clusters of articles from and their doc abstractive summaries
we use abstracts of articles from mag for this study
ii
related work different techniques have been proposed for unsupervised including sequence to sequence abstractive summarization models neural models with and without attention abstract meaning representation amr and centroid based summarization
our approach techniques in centroid based extends the state of the art summarization by employing language unit identication from articles and a novel text generation technique
abstractive summarization has received signicant attention due to progress in deep representation learning
propose meansum which consists of an autoencoder and a summarization module to produce abstractive summaries
the abstractive summarization approach of called ilpsumm includes identication of informative content and clustering of similar sentences from the documents to form summaries
extended the technique proposed in by introducing a paraphrastic fusion model called parafuse based on sensitive substitution of target words
while lexical tion enables the generation of novel words it is limited when it comes to capturing the context in the source document
propose a pointer generator network to summarize news articles from the cnn dailymail dataset
further propose a framework that takes an article and a topic and generates a summary specic to the topic
however their work is supervised and relies on the availability of human generated training corpus to train their model
iii
data collection we work with two datasets to better understand and evaluate our proposed approach the benchmark dataset and scientic articles from mag
we queried mag for the most cited abstracts for each of the fos published in
the fos we used are articial intelligence articial neural network big data case based reasoning cybernetics cyberwarfare data mining data science decision support system electronic warfare expert system human machine interaction ligent agent knowledge based systems machine learning multi agent system prediction algorithms predictive lytics predictive modeling and sensor fusion
iv
proposed method a
extractive phase fig
shows the sequence of steps we devised for and extractive summarization
the difference in the extractive phase of these tasks is has topic modeling and hierarchical agglomerative clustering in its pipeline
fig
extractive summarization
the pipeline following cluster of abstracts is for each cluster of topics
topic modeling for we determine groups of topically related abstracts for each fos
we rst build lda topic models for an fos using number of topics in the range of to
we then determine the optimal number of topics from an ensemble of the lda models that maximizes the coherence score
the topics and thus keywords generated using the lda model that gives the highest coherence score are used for topical hac
topical hierarchical agglomerative clustering different topics generated using an lda model can have semantically redundant keywords
we thus cluster topics having high similarity among their keywords using hac
we use scibert embeddings to represent each keyword in a topic
a topic is then represented as a concatenation of its keywords
once each topic is the representations of represented we conduct topical hac
to determine the number of clusters for a collection of topics we ran hac for several clusters ranging from to the total number of topics
we use silhouette coefcient to determine the optimal number of clusters
we introduce a topical similarity metric for suring the similarity between a pair of topics
each keyword in a topic is compared with all the keywords in another topic and the sum of highest similarity scores is preserved
i topic j topic j itopic i where topic j maximum of cosine similarities between term i and terms in topic j cluster topic ids table i topics and their cluster membership a abstract is assigned to a topic that is the most dominant among all possible topics the abstract addresses
table i shows the three clusters and their constituent topic ids
table ii shows topical distribution among abstracts for a selected eld of study
it can be seen two abstracts have the same dominant topic
these abstracts form a set of documents on which we perform multi document summarization
abstract id dominant topic dominant
topic topic keywords
inspire state device accelerator small size high power ved advantage inspire state device accelerator small size high power ved advantage table ii abstracts and dominant topic
core and peripheral articles identication we identify the core article from a cluster of articles based on how similar an article is to other articles
computes the article similarity score of an article
an article with the highest cumulative similarity with other articles in a cluster is chosen as the core article
the rest of the articles in the cluster are peripheral articles
casi i jc j n where i j n number of articles in the cluster the cluster of articles sim based cosine similarity centroid based clustering after core and peripheral ticles are identied we generate extractive language units elus from the core and the peripheral articles
recent studies in centroid based summarization utilized sentences in documents as standalone elus to initiate clusters and to quantify semantic relatedness
however this approach breaks the interdependence among sentences in a document and eventually leads to incoherent summaries
we address this issue by identifying the sentences that are interdependent using neural coreference resolution and preserving them as one elu
once the elus from the core article have instantiated clusters the elus from the peripheral articles are placed into a cluster based on the cosine similarity between the embedding of an elu from the peripheral article and the embeddings of the elus from the core article
an elu embedding is constructed by concatenating the embeddings of the sentences using sent bert and performing dimensionality tion to units using t sne
the purpose of dimensionality reduction is to have a uniform dimension among elus even when they contain different number of sentences so that cosine similarity can be computed
multi sentence compression the number of clusters formed in the centroid based clustering stage is the same as the number of elus in the core article
after clusters of elus are formed we build word graphs for each cluster
fig
shows a sample word graph constructed for a cluster consisting of the following elus radars are required to limit emissions in adjacent bands but traditional rectangular pulses have high out of band emissions
millimeter wave radars are popularly used in last mile radar based defense systems
fig
word graph for two elus using
tokens and pos tags of the tokens are used for a node
we develop an algorithm for extracting paths based on topical coverage and relevance
a path is selected using an additional criterion that a candidate path should at least span two elus in the cluster
next we generate topically informative and relevant paths from the word graph while maintaining the word summary limit
topical coverage measures how well a path covers the dominant topics discussed by the articles of the elus
relevance measures how relevant a path is to the elus
the cumulative score of a path is determined by a weighted sum of topical coverage and relevance
we experimented with values of in the range of to
a topical coverage formulation ctopics icpath cpath kcctopics kc where cpath candidate path ctopics cluster of topics topical coverage is measured with respect to the cluster of topics
path relevance formulation celu where cpath candidate path celu cluster of elus vectorial representation of candidate path vectorial representation of cluster of elus path relevance is measured with respect to the elus
c cumulative score ctopics celu a path is selected from the word graph if the path is longer than the average minimum length of a sentence in an fos or topic and smaller than the average maximum length of a sentence if the combined topical coverage and relevance for the path meets or exceeds a threshold of

if a path picked from the word graph is semantically similar to an already selected path by an order of threshold of
or more we compare the combined topical coverage and relevance of the two paths and keep the one with a higher score and remove the other
the selection of
is based on empirical observations
b
abstractive phase fig
shows the steps we followed for abstractive tion
the difference in the abstractive phase of and is has headline generation component
elu the ability to repair ship and work together will be the key to a stable coalition
alu it s a good time for a new political party that can bring ity and development
table iii alu generated using
m parameters and generates candidate alus
while ne tuning we set the temperature to
number of generated samples to top k random sampling to to generate more alus and minimize redundancy
we train the for epochs with a batch size of and attain a loss of

we select an alu that maximizes semantic similarity and minimizes syntactic similarity with the elu used for generation
we use the normalized sum of and for syntactic similarity
we introduce an abstractiveness score for an alu as shown in
we use bart for headline generation for each article that is later used for alu generation along with an elu
fig
alus generation using
elu elu elu elu alu alu where alu abstractive language unit elu extractive language unit cossimxbert cosine similarity on x dimension bert embeddings we select an alu that gives the highest abstractiveness score from candidate alus
table iii shows a sample elu and highest scoring alu generated
multi sentence compression after generating alus for a cluster we build a word graph and run our msc algorithm as used in the extractive phase i
e
the same ranking formulation and path selection algorithm is used for selecting informative paths from a word graph built this time from a cluster of alus
fig
shows a cluster of alus and the generated fused paths that form the nal abstractive summary
v
results and discussion a
extractive evaluation we use rouge metrics for evaluating extractive summaries taking the source articles as the reference summary
fig
abstractive summarization pipeline
abstractive language unit alu generation we start our abstractive phase with a pragmatic assumption that the title headline of an article is an abstraction of the individual extractive language units elus within the same article
we propose a method to generate an alu for an elu using the elu and title headline as prompts for generating text
combining bidirectional encodings of the title headline with an elu enables generating abstractive text
for elus consisting of two or more sentences we encode each sentence using sentence bert and then we concatenate these representations
next we perform dimensionality reduction using t sne to encode an elu
for encoding a title headline we use sentence bert without dimensionality reduction
we ne tune a model for an fos fig
and use the tuned model to generate alus given a concatenation of the bidirectional encodings of the elu and the title headline of an article
we ne tune a model such that it has in addition to the human evaluation metrics we also use copy rate for evaluating abstractive summaries
copy rate assesses the rate of novel word generation
as shown in table vi our framework achieves the lowest copy rate indicating that we are able to generate more novel words
task model ilpsumm parafuse our approach ilpsumm parafuse our approach copy rate





table vi copy rate evaluation
fig
comparison of abstractive summaries
human evaluator evaluator i evaluator ii model ilpsumm parafuse our approach ilpsumm parafuse our approach entailment





coherence





conciseness





readability grammar























table vii abstractive summarization results
human evaluator evaluator i evaluator ii model ilpsumm parafuse our approach ilpsumm parafuse our approach entailment





coherence





conciseness





readability grammar table viii abstractive summarization results
experimental results show that our proposed approach forms signicantly better in human abstractive evaluation rics and copy rate
this is mainly due to the alu generation using a ne tuned model and minimizing the syntactic similarity of generated alus
abstractive evaluation for our proposed approach consistently performs better than summ or parafuse on the human evaluation criteria
summ and parafuse show better results in entailment
in contrast our approach generally performs comparably across the criteria
thus we can clearly infer generating summaries that are entailed by source articles is easier than generating summaries that are coherent concise readable and ical
this is because if summaries have words copied from the source articles it is highly likely that they are entailed by the source articles
since the baseline approaches ilpsumm and parafuse have higher copy rate they do well in entailment
however with our approach having a low copy rate and fig
candidate alus and compressed alu paths
model ilpsumm parafuse our proposed method





r l


table iv extractive evaluation
and extractive evaluation results are shown in tables iv and v respectively
it can be seen that our proposed method performs comparably to the baseline approaches on and rouge l metrics
b
abstractive evaluation since metrics based on lexical overlap such as rouge favor extractive summaries we conduct abstractive summary evaluation using ve human evaluation metrics we propose for this study
the metrics have been developed in consultation with two co author linguists
the ve human evaluation metrics are entailment coherence conciseness readability and grammar
our co author linguists evaluated the abstractive summaries on a scale of to on each of the human evaluation metrics
our co author linguists independently reviewed the and results generated using our approach ilpsumm and parafuse
when determining the rating for each criterion they used the source articles to validate the summary
then they used their own compiled summaries to compare to the resulting abstractive summary
the closer the tive summary was to the details in their notes the higher the entailment
the human evaluators judged coherence by sentence structure and whether the sentences showed logical progression
when examining conciseness they looked for areas of the abstractive summary that were repeated
they also noted whether a sentence carried the logical progression of the paragraph
for readability they did not take grammar or spacing into consideration they looked for sentence fragments word order and took note of instances of missing subjects or verbs
when rating grammar they gave the abstractive summary a lower rating for comma splices or extra spacing than if there were fragments or inappropriate punctuation
model ilpsumm parafuse our proposed method





r l


table v extractive evaluation
generating summaries that are entailed by the sources articles is difcult yet our proposed approach still has the best entailment score for task
abstractive evaluation for our proach performs better than the baseline approaches in ence conciseness readability and grammar across two of our human evaluators while marginally losing to the baselines cording to one of our evaluators
as for entailment ilpsumm performs the best which is attributed to the high copy rate by ilpsumm
even though our approach generates signicantly more novel words than ilpsumm or parafuse we lose to the best entailment score by only
further ilpsumm parafuse and our proposed approach perform generally better on than on
we surmise this is due to the headline generation task for while we use provided titles for
vi
conclusion and future work we proposed an unsupervised multi document abstractive summarization framework that when given a set of documents from mag automatically clusters the documents and then generates summaries for each cluster
our framework consists of extractive and abstractive phases
in the extractive phase we use coreference resolution to extract groups of dependent sentences from source articles and centroid based clustering followed by an enhanced multi sentence sion algorithm to generate topically informative and relevant summaries
in the abstractive phase we use text generation technique to generate abstractive language units that are thesized into an abstractive summary
the number of maries in our proposed method is adaptively determined based on the semantic analysis of the topics discussed in the ments
we introduce a dataset of topically clustered groups of scientic articles across fields of study and their abstractive summaries
results show that our proposed approach performs better than state of the art centroid based summarization techniques on human evaluation metrics and copy rate
in the future we plan to use additional knowledge and metadata such as citation relationships among scientic articles for document summarization
vii
acknowledgment the authors are deeply grateful to daniel foose for helping with developing scripts for efcient data collection from mag
references q
zhou n
yang f
wei s
huang m
zhou and t
zhao a joint sentence scoring and selection framework for neural extractive document summarization ieee acm transactions on audio speech and language processing vol
pp

e
chu and p
liu meansum a neural model for unsupervised document abstractive summarization in international conference on machine learning pp

m
t
nayeem t
a
fuad and y
chali abstractive unsupervised multi document summarization using paraphrastic sentence fusion in proceedings of the international conference on computational linguistics pp

s
banerjee p
mitra and k
sugiyama multi document abstractive summarization using ilp based multi sentence compression in fourth international joint conference on articial intelligence
k
filippova multi sentence compression finding shortest paths in word graphs in proceedings of the international conference on computational linguistics coling pp

y
zhao x
shen w
bi and a
aizawa unsupervised rewriter for multi sentence compression in proceedings of the annual meeting of the association for computational linguistics pp

a
sinha z
shen y
song h
ma d
eide b

hsu and k
wang an overview of microsoft academic service mas and applications in proceedings of the international conference on world wide web pp

t
shi y
keneshloo n
ramakrishnan and c
k
reddy neural stractive text summarization with sequence to sequence models arxiv preprint

c
khatri g
singh and n
parikh abstractive and extractive text summarization using document context vector and recurrent neural networks arxiv preprint

p
j
liu m
saleh e
pot b
goodrich r
sepassi l
kaiser and n
shazeer generating wikipedia by summarizing long sequences arxiv preprint

a
see p
j
liu and c
d
manning get to the point tion with pointer generator networks arxiv preprint

k
liao l
lebanoff and f
liu abstract meaning representation for multi document summarization arxiv preprint

f
liu j
flanigan s
thomson n
sadeh and n
a
smith toward stractive summarization using semantic representations arxiv preprint

a
vaswani n
shazeer n
parmar j
uszkoreit l
jones a
n
gomez
kaiser and i
polosukhin attention is all you need in advances in neural information processing systems pp

j
devlin m

chang k
lee and k
toutanova bert pre training of deep bidirectional transformers for language understanding arxiv preprint

k
krishna and b
v
srinivasan generating topic oriented summaries the conference of using neural attention in proceedings of the north american chapter of the association for computational linguistics human language technologies volume long papers pp

d
m
blei a
y
ng and m
i
jordan latent dirichlet allocation journal of machine learning research vol
no
jan pp

i
beltagy k
lo and a
cohan scibert a pretrained language model for scientic text arxiv preprint

k
lee l
he m
lewis and l
zettlemoyer end to end neural coreference resolution arxiv preprint

n
reimers and i
gurevych sentence bert sentence embeddings using siamese bert networks arxiv preprint

y
liu fine tune bert for extractive summarization arxiv preprint f
boudin and e
morin keyphrase extraction for n best reranking in

multi sentence compression
s
narayan s
b
cohen and m
lapata ranking sentences for extractive summarization with reinforcement learning arxiv preprint

a
radford j
wu r
child d
luan d
amodei and i
sutskever language models are unsupervised multitask learners openai blog vol
no
p

k
thirunarayan t
immaneni and m
v
shaik selecting labels for news document clusters in international conference on application of natural language to information systems
springer pp

m
lewis y
liu n
goyal m
ghazvininejad a
mohamed o
levy v
stoyanov and l
zettlemoyer bart denoising sequence to sequence pre training for natural language generation translation and sion arxiv preprint


