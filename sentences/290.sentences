combining word embeddings and n grams for unsupervised document summarization zhuolin jiang manaj srivastava sanjay krishna gouda david akodes richard schwartz raytheon bbn technologies cambridge ma zhuolin
jiang manaj
srivastava sanjaykrishna
gouda david
akodes rich

com r a l c
s c v
v i x r a abstract graph based extractive document summarization relies on the quality of the sentence similarity graph
bag words or tf idf based sentence similarity uses exact word matching but fails to measure the semantic similarity tween individual words or to consider the semantic ture of sentences
in order to improve the similarity measure between sentences we employ off the shelf deep embedding features and tf idf features and introduce a new text ilarity metric
an improved sentence similarity graph is built and used in a submodular objective function for tractive summarization which consists of a weighted erage term and a diversity term
a transformer based pression model is developed for sentence compression to aid in document summarization
our summarization approach is extractive and unsupervised
experiments demonstrate that our approach can outperform the tf idf based approach and achieve state of the art performance on the dataset and comparable performance to the fully vised learning methods on the cnn dm and nyt datasets

introduction state of the art summarization performance has been achieved by using supervised learning methods which are mainly based on neural network architectures and require a large corpus of document summary pairs and
alternative approaches to document tion employ unsupervised techniques
those include graph based extractive summarization ods such as which require a similarity graph between sentences as input to the summarization system
the similarity between sentences is usually computed ing bag of words or tf idf features which do not porate similarity in the semantics
modeling sentence mantic similarity is challenging because of the variability of linguistic expression where different words in different orders can express the same meanings or the same set of words in different orders can express totally different ings
due to this traditional sparse and hand crafted tures such as bag of words and tf idf vectors fail to tively capture the similarity between individual words and semantic structure and context of sentences
alternatively distributed semantic representations or word embeddings of each word such as and glove do a better job of capturing the word or sentence level tics and have been widely used in many nlp tasks
and represent the embedding of a sentence by ing the embedding vectors for each word in the sentence
but there is limited work that uses these deep word dings in an unsupervised setting for extractive document summarization
introduces a summarization method that estimates kl divergence between the document and its summary based on embedding distributions
in this paper we explore two popular deep embedding features and bert in a submodular work for document extractive summarization
our ument summarization framework is unsupervised and is therefore useful for the case of limited or no reference summary pairs
in order to use the strengths of these two types of features we combine them to further prove the similarity measure
in addition we investigate the effect of using abstractive sentence compression for tive document summarization
towards that end we train a transformer model to compress the sentences from a document before performing submodular sentence tion
our main contributions are we improve the sentence similarity graph by ing off the shelf neural word embedding models for graph based submodular sentence selection where a similarity graph for pair wise sentences is required
we provide thorough experimental comparisons tween different sentence similarity measures
we show that combining off the shelf neural word beddings and tf idf features can improve the mance of document summarization
we show that a transformer based sentence sion method can improve the performance of ment summarization

unsupervised document summarization similar to we extract a subset of sentences a from the whole set of sentences v in a document d as the mary by maximizing a submodular objective function


similarity graph construction

text semantic similarity given a document d we construct an undirected ilarity graph g v e where the vertices v v are sentences in d and the edges e e model pairwise lation between the sentences
the weight wi associated with the edge ei j measures the similarity between tices or sentences vi and vj
wi j is computed as wi j xj where xi is the feature descriptor of vi and xj measures the difference between xi and xj
as suggested in we set the normalization tor to ij and select the scaling parameter i for vi through the local statistic of vi s neighborhood
we set i xk where xk corresponds to the k th nearest neighbor of vi


sentence selection via submodularity the selected subset a should be representative and should cover other unselected sentences in the whole set v
we associate a nonnegative cost to each sentence s
we introduce a weighted coverage term for selecting sentences max jav x iv wi j s
t
b psa denotes the total cost of selecting where a and b is a budget for selecting sentences
maximizing this term encourages the selected subset a to be tive and compact
in addition the selected sentences should be diverse
we used the diversity term introduced in piv wi j where


pk is a p partition of v and is the number of elements in v
k qpjpk we combine two terms to obtain the nal objective tion maxa maxa s
t
a v b
the objective function is submodular and monotonically increasing
we solve the problem via a greedy algorithm
given the selected sentences from step i during optimization in step i we lect the element with the highest marginal gain
the marginal gain argmaxsv takes the element cost into account
the element cost of sentence s is related to its position m in a document

it is dened as the greedy algorithm is guaranteed to nd a solution that is at least e of the optimal solution as proved in but with a complexity of
the tion steps can be further accelerated using a lazy greedy approach
we construct a max heap for all elements in v then evaluate them in the max heap order
with this approach the time complexity becomes log stead of quadratic
the edge weight in g serves as the similarity between sentences
we compute the similarity between two tences i and by ri j pwsi sj pwsj where sj is the maximal cosine similarity between input word w and any words in the sentence sj
tion words are ltered out when computing the ity
this similarity value ri j measures the semantic lap between two sentences
then we compute the distance between these two sentences for the similarity graph via xj ri j


combination of different features in order to leverage the strengths of deep word beddings and n grams features we combine them by graph fusion the weight assigned to each edge in a similarity graph is computed by the similarity measure tween pairwise sentences
we combine the graphs from ferent features by using a simple weighted average of edge late fusion the ranking lists from weights
ent features are combined by the popular borda count rithm


sentence compression in order to obtain compressed or summarized form of sentences which could then be fed into our unsupervised extractive algorithm we trained a standard transformer model both encoder and decoder were composed of six stacked layers
transformer is a neural ture that has shown promising results for many tasks
we applied it to the problem of sentence sion
we also used byte pair encoding for subword tation in order to handle unseen words and named entities
at the time of decoding

experiments our approach is evaluated on a multi document marization dataset and two single document datasets cnn dm news and


multi document summarization the dataset was constructed for the document summarization task using english news articles with multiple reference summaries
there are document clusters with documents per cluster
for the evaluation we used f score and recall
the mary length is bytes per summary
baselines we compare our approach with eight lines
lead simply uses the rst bytes from

with options
methods lead centroid submodular mckp lexrank rnn cnn ours tf idf ours ours wmd ours tss ours bert ours latefusion ours graphfusion ours tss with compression ours bert with compression ours graphfusion with compression










r

















table
document summarization performance on the dataset
the most recent document in each cluster
is the winning system in
centroid uses embeddings for summarization
three unsupervised marization methods are also compared submodular mckp and lexrank
another two methods that learn sentence embeddings are compared
uses recursive neural networks rnn and uses convolutional neural networks cnn for learning sentence embeddings
we include the results of our approach using different bert similarity measures with word embeddings the sentence embedding is computed by using the mean of word embeddings from the pretrained bert model
the pairwise similarity between sentences is the cosine similar to bert the embeddings from larity
model are used
note that we did not ne tune bert or embeddings
wmd the sentence similarity measure is the word mover distance introduced in
tss the text semantic similarity sure in equation is used
graphfusion latefusion tf idf bert wmd and tss are combined
we summarize the results that use different features and compare our results with those from state of the art proaches in table
the cnn rnn models achieve better results than our bert and models
this is because they are trained on the and datasets while our approach is totally unsupervised and uses off shelf neural word bert or embeddings only without any ne tuning
our results using graph fusion are better than the results of other approaches including and comparable to
methods ours







r l



table
sentence compression performance on gigaword dataset
methods ours gig only ours









r l




table
sentence compression performance on dataset


sentence compression we used gigaword sentence compression dataset to train the transformer model
gigaword dataset comprises nearly
m training sentence pairs rst lines of word news articles paired with the headlines
we also used byte pair encoding for subword segmentation
in der to determine the efcacy of trained model we used the sentence pairs from gigaword test set as well as the sentence pairs from sentence sion dataset
our results on gigaword beat the current sentence compression baselines by nearly points absolute on f scores of and rouge l rics
on we get additional ments on the three variants of rouge metrics by using publicly released subset of google sentence compression dataset in addition to gigaword dataset
google pression dataset comprises nearly k sentence pairs we used k pairs as train set and k as validation set
in all metrics on dataset we get point lute improvement on the three rouge metrics over current baselines
our approach to summarization uses the compressed sentences from a document to do sentence selection for document level summarization
with sentence sion the document summarization performance of our proach is further improved and outperforms other compared approaches as shown in table
the sentence compression model used to aid in document level summarization used only the gigaword dataset for training
we did not see any additional improvements on dataset by using the additional google compression dataset


single document summarization the cnn dm dataset consists of online news articles from cnn and daily mail websites
the corpus contains a total of article summary pairs out of which methods oracle pointer refresh ours tf idf ours ours wmd ours tss ours bert ours graphfusion ours latefusion cnn dm





















r l










nyt





















r l










table
document summarization performance on the cnn dm dataset
pairs are used for validation articles as test pairs and the remaining for training
however we use about sand validation pairs for tuning our meta parameters and completely ignored the training set
the dataset is a subset of the new york times corpus introduced by
we use a subset of the documents that have summaries with at least words a subset known as
the nal test dataset includes test examples out of the nal articles
we evaluate these two datasets in terms of and rouge l l f
for both datasets we use a budget of three sentences per
baselines we compare our approach with two state the art supervised learning methods pointer and fresh we also provide results from the extractive cle system which maximizes the rouge score against the reference summary and the baseline that creates a summary by selecting the rst three sentences in a ment
the results on both datasets are summarized in table
on both datasets the results of deep features are marginally better than those of the tf idf features
note that our proach is unsupervised and does not use the training data our results are surprisingly comparable to the results from the supervised learning methods including and

conclusions we explore two popular deep word embeddings for the extractive document summarization task
compared with tf idf based features deep embedding features are better in capturing the semantic similarity between sentences and achieve better document summarization performance
the

with options
note that we do nt report the summarization results with tence compression on the cnn dm dataset since a compressed sentence may lose some information and the nal performance may not be proved with the constraint on the number of selected sentences
sentence similarity measure is further improved by bining the word embeddings with n gram features
a transformer based sentence compression model is duced and evaluated with our summarization approach showing improvement in summarization performance on the dataset
our summarization approach is pervised but achieves comparable results to the supervised learning methods on the cnn dm and nyt datasets
acknowledgement this work was supported by the intelligence advanced research projects activity iarpa via department of fense us air force research laboratory contract number
references z
cao f
wei l
dong s
li and m
zhou
ranking with recursive neural networks and its application to document summarization
in twenty ninth aaai conference on articial intelligence
z
cao f
wei s
li w
li m
zhou and w
houfeng
learning summary prior representation for extractive marization
in proceedings of the annual meeting of the association for computational linguistics and the national joint conference on natural language processing volume short papers volume pages
s
chopra m
auli and a
m
rush
abstractive sentence summarization with attentive recurrent neural networks
in proceedings of the conference of the north american chapter of the association for computational linguistics human language technologies pages san diego california june
association for computational guistics
j
devlin m
chang k
lee and k
toutanova
bert training of deep bidirectional transformers for language derstanding
corr

g
durrett t
berg kirkpatrick and d
klein
based single document summarization with compression and s
narayan s
b
cohen and m
lapata
ranking sentences for extractive summarization with reinforcement learning
arxiv preprint

g
l
nemhauser l
a
wolsey and m
l
fisher
an analysis of approximations for maximizing submodular set functions i
math
program
dec

j
pennington r
socher and c
manning
glove global vectors for word representation
in proceedings of the conference on empirical methods in natural language cessing pages
g
rossiello p
basile and g
semeraro
centroid based text summarization through compositionality of word beddings
in proceedings of the multiling workshop on summarization and summary evaluation across source types and genres pages
a
m
rush s
chopra and j
weston
a neural tion model for abstractive sentence summarization
corr

a
see p
j
liu and c
d
manning
get to the point marization with pointer generator networks
arxiv preprint

r
sennrich b
haddow and a
birch
neural chine translation of rare words with subword units
corr

h
takamura and m
okumura
text summarization model based on maximum coverage problem and its variant
in ceedings of the conference of the european chapter of the association for computational linguistics pages
a
vaswani n
shazeer n
parmar j
uszkoreit l
jones a
n
gomez l
u
kaiser and i
polosukhin
attention is all you need
in advances in neural information processing systems pages

l
zelnik manor and p
perona
self tuning spectral ing
in proceedings of the international conference on neural information processing systems
anaphoricity constraints
arxiv preprint

c
dwork r
kumar m
naor and d
sivakumar
rank aggregation methods for the web
in proceedings of the international conference on world wide web www pages
g
erkan and d
r
radev
lexrank graph based lexical centrality as salience in text summarization
journal of cial intelligence research
k
filippova e
alfonseca c
colmenares l
kaiser and o
vinyals
sentence compression by deletion with lstms
in proceedings of the conference on empirical methods in natural language processing
s
gehrmann y
deng and a
m
rush
bottom up tive summarization
arxiv preprint

m
kgebck o
mogren n
tahmasebi and d
hashi
extractive summarization using continuous vector in proceedings of the workshop on space models
continuous vector space models and their ity cvsc pages
h
kobayashi m
noguchi and t
yatsuka
in proceedings of tion based on embedding distributions
the conference on empirical methods in natural guage processing pages
m
j
kusner y
sun n
i
kolkin and k
q
weinberger
from word embeddings to document distances
in ings of the international conference on international conference on machine learning volume pages
j
leskovec a
krause c
guestrin c
faloutsos c
sos j
vanbriesen and n
glance
cost effective outbreak in proceedings of the acm detection in networks
sigkdd international conference on knowledge discovery and data mining kdd pages
h
lin and j
bilmes
multi document summarization via budgeted maximization of submodular functions
in human language technologies the annual conference of the north american chapter of the association for tional linguistics hlt pages
h
lin and j
bilmes
a class of submodular functions for document summarization
in proceedings of the annual meeting of the association for computational linguistics human language technologies pages
r
mihalcea and p
tarau
textrank bringing order into text
in proceedings of the conference on empirical methods in natural language processing
t
mikolov i
sutskever k
chen g
corrado and j
dean
distributed representations of words and phrases and their in proceedings of the international compositionality
conference on neural information processing systems ume pages
r
nallapati b
zhou c
gulcehre b
xiang al
tive text summarization using sequence to sequence rnns and beyond
arxiv preprint


