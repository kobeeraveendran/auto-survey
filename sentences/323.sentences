unsupervised abstractive dialogue summarization for tete a tetes xinyuan ruiyi manzil amr inc
university research
com p e s l c
s c v
v i x r a abstract high quality dialogue summary paired data is expensive to produce and domain sensitive making abstractive dialogue summarization a challenging task
in this work we propose the rst unsupervised abstractive dialogue rization model for tete a tetes sutat
unlike standard text summarization a dialogue marization method should consider the speaker scenario where the speakers have ferent roles goals and language styles
in a tete a tete such as a customer agent versation sutat aims to summarize for each speaker by modeling the customer utterances and the agent utterances separately while taining their correlations
sutat consists of a conditional generative module and two supervised summarization modules
the ditional generative module contains two coders and two decoders in a variational toencoder framework where the dependencies between two latent spaces are captured
with the same encoders and decoders two vised summarization modules equipped with sentence level self attention mechanisms erate summaries without using any tions
experimental results show that sutat is superior on unsupervised dialogue rization for both automatic and human tions and is capable of dialogue classication and single turn conversation generation
introduction tete a tetes conversations between two pants have been widely studied as an importance component of dialogue analysis
for instance a tetes between customers and agents contain mation for contact centers to understand the lems of customers and improve the solutions by agents
however it is time consuming for ers to track the progress by going through long and sometimes uninformative utterances
matically summarizing a tete a tete into a shorter customer agent i am looking for the hamilton lodge in bridge
sure it is at chesterton road postcode
customer please book it for people nights ning on tuesday
done
your reference number is
agent customer thank you i will be there on tuesday agent is there anything more i can assist you with today customer thank you that s everything i needed
agent you are welcome
any time
customer summary agent summary i would like to book a hotel in cambridge on tuesday
i have booked you a hotel
the reference number is
can i help you with anything else table an example of sutat generated summaries
version while retaining its main points can save a vast amount of human resources and has a number of potential real world applications
summarization models can be categorized into two classes extractive and abstractive
extractive methods select sentences or phrases from the input text while abstractive methods attempt to ate novel expressions which requires an advanced ability to paraphrase and condense information
spite being easier extractive summarization is often not preferred in dialogues for its limited ity to capture highly dependent conversation ries and produce coherent discourses
therefore abstractively summarizing dialogues has attracted recent research interest goo and chen pan et al
yuan and yu liu et al

however existing abstractive dialogue rization approaches fail to address two main lems
first a dialogue is carried out between tiple speakers and each of them has different roles goals and language styles
taking the example of a contact center customers aim to propose lems while agents aim to provide solutions which leads them to have different semantic contents and choices of vocabularies
most existing methods process dialogue utterances as in text tion without accommodating the multi speaker nario
second high quality annotated data is not readily available in the dialogue summarization main and can be very expensive to produce
topic descriptions or instructions are commonly used as gold references which are too general and lack any information about the speakers
moreover some methods use auxiliary information such as dialogue acts goo and chen semantic scaffolds yuan and yu and key point sequences liu et al
to help with summarization adding more burden on data annotation
to our edge no previous work has focused on vised deep learning for abstractive dialogue marization
we propose sutat an unsupervised abstractive dialogue summarization approach specically for tete a tetes
in this paper we use the example of agent and customer to represent the two speakers in tete a tetes for better understanding
in addition to summarization sutat can also be used for alogue classication and single turn conversation generation
to accommodate the two speaker scenario tat processes the utterances of a customer and an agent separately in a conditional generative module
inspired by zhang et al
where two latent spaces are contained in one variational autoencoder vae framework the conditional generative ule includes two encoders to map a customer terance and the corresponding agent utterance into two latent representations and two decoders to construct the utterances jointly
separate encoders and decoders enables sutat to model the ences of language styles and vocabularies between customer utterances and agent utterances
the pendencies between two latent spaces are captured by making the agent latent variable conditioned on the customer latent variable
compared to using two standard autoencoders that learn tic representations for input utterances using the vae based conditional generative module to learn variational distributions gives the model more pressive capacity and more exibility to nd the correlation between two latent spaces
the same encoders and decoders from the ditional generative module are used in two pervised summarization modules to generate tomer summaries and agent summaries
divergent from meansum chu and liu where the combined multi document representation is simply computed by averaging the encoded input texts sutat employs a setence level self attention anism vaswani et al
to highlight more nicant utterances and neglect uninformative ones
we also incorporate copying factual details from the source text that has proven useful in supervised summarization see et al

dialogue maries are usually written in the third person point of view but sutat simplies this problem by ing the summaries consistent with the utterances in pronouns
table shows an example of sutat generated summaries
experiments are conducted on two dialogue datasets multiwoz budzianowski et al
and taskmaster byrne et al

it is assumed that we can only access utterances in the datasets without any annotations including dialogue acts descriptions instructions
both automatic and human evaluations show sutat outperforms other unsupervised baseline methods on dialogue marization
we further show the capability of tat on dialogue classication with generated maries and single turn conversation generation
methodology sutat consists of a conditional generative ule and two unsupervised summarization modules
let x xn denote a set of customer utterances and y denote a set of agent utterances in the same dialogue
our aim is to generate a customer summary and an agent summary for the utterances in x and y
figure shows the entire architecture of sutat
given a customer utterance and its consecutive agent utterance y the conditional generative ule embeds them with two encoders and obtain latent variables zx and zy from the variational tent spaces then reconstruct the utterances from zx and zy with two decoders
in the latent space the agent latent variable is conditioned on the customer latent variable during decoding the generated tomer utterances are conditioned on the generated agent utterances
this design resembles how a a tete carries out the agent s responses and the customer s requests are dependent on each other
the encoded utterances of a dialogue are the inputs of the unsupervised summarization modules
we employ a sentence level self attention mechanism figure block diagram of sutat
architectures connected by a blue dashed line are the same
the red arrow represents the conditional relationship between two latent spaces
on the utterances embeddings to highlight the more informative ones and combine the weighted dings
a summary representation is drawn from the low variance latent space using the combined utterance embedding which is then decoded into a summary with the same decoder and a partial copy mechanism
the whole process does not require any annotations from the data

conditional generative module we build the conditional generative module in a sivae based framework zhang et al
to capture the dependencies between two latent spaces
the goal of the module is to train two coders and two decoders for customer utterances and agent utterances y by maximizing the evidence lower bound lgen log zx log log y where q is the variational posterior distribution that approximates the true posterior distribution
the lower bound includes two reconstruction losses and two kullback leibler kl divergences tween the priors and the variational posteriors
by assuming priors and posteriors to be gaussian we can apply the reparameterization trick kingma and welling to compute the kl divergences in closed forms
zx zx and represent customer encoder agent coder customer decoder and agent decoder
the correlation between two latent spaces are captured by making the agent latent variable zy conditioned on the customer latent variable zx
we dene the customer prior to be a standard gaussian n i
the agent prior is also a gaussian n where the mean and the variance are functions of zx
this process resembles how a tete a tete at contact centers carries out the response of an agent is conditioned on what the customer says
encoding given a customer utterance sequence wt we rst encode it into an terance embedding ex using bidirectional lstm graves et al
or a transformer encoder vaswani et al

the bi lstm takes the hidden states hi h i as contextual representations by h i ing a sequence from both directions h h
the transformer encoder produces the contextual representations that have the same dimensions as word embeddings wt wt
customer encoderagent encodercustomerdecoderagentdecodercustomer latent spaceagent latent spacecustomerutterances xagentutterances ycustomerutterancesagentutterancescustomer decoder sentence levelself attentionpartialcopysxzxzysummaryrepresentationcustomerlatent sentence levelself attentionpartialcopysysummaryrepresentationagentlatent generative moduleunsupervised summarization moduleunsupervised summarization moduleencoded customer utterancesencoded agent utterances the customer utterance embedding ex is obtained by averaging over the contextual representations
similarly we can obtain the agent utterance bedding ey
the customer latent variable zx is rst pled from n x x using ex then the agent latent variable zy is sampled from zx n y y using ey and
the gaussian parameters x x y and y are puted with separate linear projections x y lineary zx x y lineary ey zx
decoding we rst decode zy into the agent terance from the using lstm sutskever et al
or a transformer decoder vaswani et al

the decoded sequence and the latent variable zx are then used in zx to generate the customer utterance
in the lstm decoder y zy zx y y
while in the transformer decoder i zy i where y i and i are the embeddings of the viously decoded sequence
the decoded tations are put in feedforward layers to compute the vocabulary distributions y and y wt i zy i zx y y by wt bx where wx wy rl and by rl are learnable parameters
and are the vocabulary sizes for customer utterances and agent utterances

unsupervised summarization module given the encoded utterances of a dialogue an unsupervised summarization module learns to erate a summary that is semantically similar to the input utterances using trained components from the conditional generative module
sentence level self attention some utterances like greetings or small talk do not contribute to the content of a dialogue
therefore we employ a sentence level self attention mechanism which is built upon multi head attention vaswani et al
to highlight the most signicant utterances in a dialogue
the multi head attention partitions the queries q keys k and values v into h heads along their dimensions d and calculates h scaled dot product attention for the linear projections of the heads
k v headi i kwk i vwv where wo wq wk and wv are trainable rameters
the scaled dot product attention outputs a weighted sum of values k v softmax v
qkt in sutat the sentence level self attention is achieved by making the queries keys and values all be the set of encoded agent customer utterances of a dialogue
the self attention module assigns weights on the input utterances such that more nicant and informative ones have higher weights
the output is a weighted combined utterance bedding ex or ey that highlights more informative utterances from the dialogue
summary generation summary tions and sy are sampled from the latent spaces taking the weighted combined utterance tations ex and ey as inputs
to limit the amount of novelty in the generated summary we set the variances of the latent spaces close to zero so that x and sy y
and sy containing key information from the dialogue are decoded into a customer summary and an agent summary using the same decoders from the conditional generative module which makes the generated summaries similar to the utterances in pronouns and language styles
we re encode the generated summaries into ex and ey with the same encoders and compare them with each of the utterance embeddings using age cosine distance
to constrain the summaries to be semantically close to input utterances the summarization modules are trained by maximizing a similarity loss n n lsum y where d denotes the cosine distance
however the summarization modules are prone to produce inaccurate factual details
we design a simple but effective partial copy mechanism that employs some extractive summarization tricks to address this problem
we automatically make a list of factual information from the data such as dates locations names and numbers
whenever the decoder predicts a word from the factual mation list the copy mechanism replaces it with a word containing factual information from the input utterances
if there are multiple factual tion words in the dialogue the one with the highest predictive possibility will be chosen
note that this partial copy mechanism does not need to be trained and is not activated during training

training process the objective function we optimize is the weighted sum of the reconstruction loss in equation and the similarity loss in equation l lgen where controls the weights of two objectives
sutat involves re encoding the generated agent utterance to help with generating the customer terance in equation and re encoding the ated summary to compare with utterance dings in equation
directly sampling from the multinomial distribution with argmax is a differentiable operation so we use the soft argmax trick chen et al
to approximate the ministic sampling scheme yi y where is the annealing parameter
adam kingma and ba is adopted for stochastic optimization to jointly train all model parameters by maximizing equation
in each step adam samples a mini batch of dialogues and then updates the parameters zhang et al

related works cardie
abstractive dialogue tion has been recently explored due to the success of sequence to sequence neural networks
pan et al
propose an enhanced interaction dialogue encoder and a transformer pointer decoder to marize dialogues
li et al
summarize modal meetings on another encoder decoder ture
some approaches design additional nisms in a neural summarization model to leverage auxiliary information such as dialogue acts goo and chen key point sequences liu et al
and semantic scaffolds yuan and yu
however these supervised methods can only use concise topic descriptions or instructions as gold references while high quality annotated dialogue summaries are not readily available
unsupervised summarization many extractive summarization models do not require summary paired data and instead they tackle a sentence selection problem
textrank cea and tarau and lexrank erkan and radev encode sentences as nodes in a graph to select the most representative ones as a mary
zheng and lapata and rossiello et al
advance upon textrank and lexrank by using bert devlin et al
to compute tence similarity and replacing tf idf weights with embeddings respectively
in abstractive summarization some approaches focus on ing unsupervised sentence compression with scale texts fevry and phang baziotis et al
west et al
while ted yang et al
proposes a transformer based architecture with pretraining on large scale data
meansum chu and liu generates a multi document summary by decoding the average encoding of the input texts where the autoencoder and the rization module are interactive
brainskas et al
and amplayo and lapata extend meansum by using a hierarchical variational toencoder and denoising a noised synthetic dataset
however none of these methods accommodate the multi speaker scenario in dialogues
experimental details dialogue summarization early dialogue marization works mainly focus on extractively marizing using statistical machine learning ods galley xie et al
wang and we perform experiments with two variants of tat one equipped with lstm encoders and coders sutat lstm and the other equipped with transformer encoders and decoders sutat tran
model lexrank meansum copycat vae sutat lstm sutat tran






multiwoz taskmaster customer r l r l r l customer









































ablation study with lstm encoders and decoders sutat ls sutat att sutat copy




















agent





























agent









r l









table rouge scores on the multiwoz and taskmaster test sets

dataset the experiments are conducted on two dialogue datasets
budzianowski et al
and byrne et al

tiwoz consists of goal oriented human written dialogues between customers and agents spanning over domains such as booking hotels booking taxis
of them are label and of them are multi label
in the periment we split the dataset into and dialogues for training testing and tion
taskmaster consists of goal oriented dialogues including spoken and written dialogues
in this work we only use the written logues which is created by human workers based on scenarios outlined for one of the six tasks such as ordering pizza ordering movie tickets
the dataset is split into and dialogues for training testing and validation

baselines to validate the effectiveness of sutat we compare the two variants against the following baselines unsupervised extractive summarization methods lexrank erkan and radev and rossiello et al
unsupervised abstractive summarization methods meansum chu and liu and copycat brainskas et al

in dition we train a vanilla text vae model bowman et al
with our unsupervised summarization module as another baseline
since we are the rst work that summarizes for each speaker in a dialogue some modications need to be made on baselines to make fair parisons with our model
to make the vised summarization baseline models adapt to the two speaker scenario in tete a tetes we train two models for each baseline with either customer terances or agent utterances
during testing the customer summaries and agent summaries are erated by the two trained models of each baseline which are used either separately for automatic and human evaluation or concatenated together for the classication experiment

settings we ne tune the parameters of sutat on the tion set
vae based text generative models can fer from posterior collapse where the model learns to ignore the latent variable bowman et al

we employ kl term annealing and dropping out words during decoding to avoid posterior collapse
for kl annealing the initial weights of the kl terms are and then we gradually increase the weights as training progresses until they reach the kl threshold of
the rate of this increase is set to
with respect to the total number of batches
the word dropout rate during decoding is

the latent variable size is for both customer and agent latent variables
that controls weights of two objective functions in equation is set to

the word embedding size is
for the rectional lstm encoder and lstm decoder the number of hidden layers is and the hidden unit size is
for the transformer encoder and coder the number of hidden layers is and the number of heads in the multi head attention is set to
the number of heads in the sentence level self attention is also
the hidden unit size of the mlps in is
the annealing parameter for soft argmax in equation is set to

ing training the learning rate is
the batch size is and the maximum number of epoch is
sutat is implemented in pytorch and trained using a nvidia tesla gpu with gb

reference summaries in this work we dene the dialogue summary as summarizing for each speaker in a dialogue and there is no such annotated dataset available
to validate the effectiveness of sutat and compare with baselines we follow the setting in chu and liu to collect abstractive summaries for a subset of each dataset
workers were sented with dialogues from multiwoz and dialogues from taskmaster and asked to write maries that best summarize both the content and the sentiment for each speaker
we asked ers to write your summaries as if your were the speaker e

i want to book a hotel
instead of the customer wants to book a hotel
and keep the length of the summary no more than one tence
the collected summaries are only used as reference summaries for testing and not used for model tuning
these reference summaries cover all domains in both datasets and will be released later
results we conduct the majority of experiments to show the superiority of sutat on unsupervised dialogue summarization
we use the labeled reference maries for rouge score based automatic tion and human evaluation to compare with line methods
we further demonstrate the ness of sutat by analyzing the language modeling results and using generated summaries to perform dialogue classication
in addition we show that sutat is capable of single turn conversation ation

unsupervised dialogue summarization automatic evaluation rouge lin is a standard summarization metric to measure the face word alignment between a generated summary and the reference summary
in the experiments we use and rouge l to sure the word overlap bigram overlap and longest common sequence respectively
table shows the rouge scores for two sutat variants and the baselines
as we can see our proposed sutat with lstm encoders and decoders outperforms all other baselines on both datasets
sutat lstm performs better than sutat transformer on rouge scores the reason could be that transformer decoders are too strong so the encoders are weakened during training
in general the unsupervised abstractive models perform better than unsupervised tive models
compared with other unsupervised abstractive summarization baselines equipped with lstm encoders and decoders sutat lstm has a big performance improvement
we believe this is because sutat accommodates the two speaker nario in tete a tetes so that the utterances from each speaker and their correlations are better modeled
in addition we evaluate reconstruction mances of the language modeling based methods with perplexity ppl and check the posterior lapse for the vae based methods with kl gence
the results for multiwoz and taskmaster are shown in table
as can be seen sutat tran has much better ppl scores than other ing methods on both datasets showing the former decoders are effective at reconstructing tences
consequently due to the powerful coders sutat tran has smaller kl divergences which can lead to posterior collapse where the coders tend to be ignored
human evaluation human evaluation for the generated summaries is conducted to quantify the qualitative results of each model
we sample alogues that are labeled with reference summaries from the multiwoz and taskmaster test set each
with the sampled dialogues summaries are generated from the unsupervised abstractive approaches meansum copycat vae lstm and sutat tran
we recruit three workers to rank the generated summaries and reference maries from the best to the worst based on three criteria informativeness a summary should present the main points of the dialogue in a concise version readability a summary should be matically correct and well structured correlation the customer summary should be correlated to the agent summary in the same dialogue
the average ranking scores are shown in ble
as we can see sutat lstm achieves the best informativeness and correlation results on both datasets while sutat tran also has good mances further demonstrating the ability of sutat on generating informative and coherent dialogue summaries
in general the two sutat models have better human evaluation scores than baseline model multiwoz agent customer kl ppl taskmaster agent customer kl ppl meansum copycat vae


sutat lstm

sutat tran



ppl




kl












ppl




kl



table language modeling results on multiwoz and taskmaster
lower is better for ppl
multiwoz read taskmaster read corr model reference info
meansum

copycat
vae sutat lstm sutat tran

corr





info























table human evaluation results on informativeness readability and correlation of generated summaries
model multiwoz taskmaster meansum copycat vae sutat unsupervised sutat supervised









table auc scores for domain classcation with erated summaries where multiwoz is multi label and taskmaster is single label
els especially on correlation scores where the sults are close to reference summaries
this is cause sutat exploits the dependencies between the customer latent space and the agent latent space which results in generating more correlated tomer summaries and agent summaries
ablation study we perform ablations to date each component of sutat by removing the variational latent spaces sutat ls so the encoded utterances are directly used for ding removing the sentence level self attention mechanism sutat att and removing the tial copy mechanism sutat copy
we use lstm encoders and decoders for all ablation els
the results for ablation study in table show that all the removed components play a role in tat
removing the latent spaces has the biggest inuence on the summarization performance cating that the variational latent space is necessary to support our design which makes the agent latent variable dependent on the customer latent variable
the performance drop after removing the level self attention mechanism shows that using weighted combined utterance embedding is better than simply taking the mean of encoded utterances
removing the partial copy has the smallest quality drop
however taking the dialogue example in ble without the partial copy mechanism sutat can generate the following summaries customer summary book a hotel in cambridge on tuesday
i would like to agent summary i have booked you a hotel
the reference number is lzludtvi
can i help you with anything else the generated summaries are the same except for the wrong reference number which is crucial mation in this summary

classication with summaries a good dialogue summary should reect the key points of the utterances
we perform dialogue sication based on dialogue domains to test the lidity of generated summaries
first we encode the generated customer summary and agent summary into ex and ey using the trained encoders of each model which are then concatenated as features of the dialogue for classication
in this way the logue features are obtained unsupervisedly
then we train a separate linear classier on top of the encoded summaries
we use sutat with lstm encoders and decoders for this task
as shown in table sutat outperforms other baselines on dialogue classication indicating the sutat erated summaries have better comprehension of domain information in the dialogue
we can also perform supervised classication by using and sy from sutat as features to train a linear classier
the cross entropy loss is combined with equation as the new objective customer yes yes
are there any multiple sports places that i can visit in sorry there are none locations in the center of town
would you like a different area customer yes please
book for the same group of people at on thursday
your booking was successful and your ence number is minorhoq
agent agent customer hi i am looking for a place to stay
the west should be cheap and does nt need to have internet
there are no hotels in the moderate price range
would you care to expand other criteria agent table examples of single turn conversations ated by the conditional generative module of sutat
function where all parameters are jointly optimized
as can be seen in table the supervised cation results are as high as
on multiwoz and
on taskmaster further demonstrating the effectiveness of sutat

single turn conversation generation the design of the conditional generative module in sutat enables generating novel single turn sations
by sampling the customer latent variable from the standard gaussian zx n i and then sampling the agent latent variable zy sutat can produce realistic looking novel dialogue pairs using the customer decoder and agent decoder
table shows three examples of novel single turn conversations generated by sutat using randomly sampled latent variables
we can see that the alogue pairs are closely correlated meaning the dependencies between two latent spaces are cessfully captured
conclusion we propose sutat an unsupervised abstractive dialogue summarization model accommodating the two speaker scenario in tete a tetes and marizing them without using any data annotations
the conditional generative module models the tomer utterances and agent utterances separately using two encoders and two decoders while taining their correlations in the variational latent spaces
in the unsupervised summarization module a sentence level self attention mechanism is used to highlight more informative utterances
the mary representations containing key information of the dialogue are decoded using the same decoders from the conditional generative module with the help of a partial copy mechanism to generate a customer summary and an agent summary
the experimental results show the superiority of sutat for unsupervised dialogue summarization and the capability for more dialogue tasks
references reinald kim amplayo and mirella lapata

supervised opinion summarization with noising and in proceedings of the annual meeting denoising
of the association for computational linguistics acl
christos baziotis ion androutsopoulos ioannis stas and alexandros potamianos

seq differentiable sequence to sequence to sequence autoencoder for unsupervised abstractive sentence compression
in proceedings of the conference of the north american chapter of the association for computational linguistics naacl
samuel r bowman luke vilnis oriol vinyals drew m dai rafal jozefowicz and samy gio

generating sentences from a continuous space
in proceedings of the conference on tational natural language learning conll
arthur brainskas mirella lapata and ivan titov
unsupervised opinion summarization as
in proceedings of the copycat review generation
annual meeting of the association for tional linguistics acl
pawe budzianowski tsung hsien wen bo hsiang tseng inigo casanueva stefan ultes osman madan and milica gaic

multiwoz a scale multi domain wizard of oz dataset for oriented dialogue modelling
proceedings of the conference on empirical methods in natural guage processing emnlp
bill byrne karthik krishnamoorthi chinnadhurai sankar arvind neelakantan daniel duckworth semih yavuz ben goodrich amit dubey andy cedilnik and kyu young kim

toward a realistic and diverse dialog dataset
in proceedings of the conference on empirical methods in natural language processing emnlp
liqun chen yizhe zhang ruiyi zhang chenyang tao zhe gan haichao zhang bai li dinghan shen changyou chen and lawrence carin

improving sequence to sequence learning via in proceedings of the international mal transport
conference on learning representations iclr
eric chu and peter j liu

meansum a neural model for unsupervised multi document abstractive summarization
in proceedings of the international conference on machine learning icml
jacob devlin ming wei chang kenton lee and kristina toutanova

bert pre training of deep bidirectional transformers for language ing
in proceedings of the conference of the north american chapter of the association for tional linguistics naacl
gnes erkan and dragomir r radev

lexrank graph based lexical centrality as salience in text summarization
journal of articial intelligence search
thibault fevry and jason phang

vised sentence compression using denoising encoders
in proceedings of the conference on putational natural language learning conll
michel galley

a skip chain conditional dom eld for ranking meeting utterances by in proceedings of the conference on tance
pirical methods in natural language processing emnlp
chih wen goo and yun nung chen

tive dialogue summarization with sentence gated modeling optimized by dialogue acts
in ieee ken language technology workshop slt
alex graves navdeep jaitly and abdel rahman hamed

hybrid speech recognition with deep bidirectional lstm
in ieee workshop on automatic speech recognition and understanding
diederik p kingma and jimmy ba

adam a method for stochastic optimization
in proceedings of the international conference on learning sentations iclr
gaetano rossiello pierpaolo basile and giovanni meraro

centroid based text summarization through compositionality of word embeddings
in proceedings of the multiling workshop on marization and summary evaluation across source types and genres
abigail see peter j liu and christopher d manning

get to the point summarization with in proceedings of the annual generator networks
meeting of the association for computational guistics acl
ilya sutskever oriol vinyals and quoc v le

sequence to sequence learning with neural networks
in advances in neural information processing tems neurips
ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan n gomez ukasz kaiser and illia polosukhin

attention is all in advances in neural information you need
cessing systems neurips
lu wang and claire cardie

independent abstract generation for focused meeting summarization
in proceedings of the annual ing of the association for computational linguistics acl
peter west ari holtzman jan buys and yejin choi

bottlesum unsupervised and self supervised sentence summarization using the information tleneck principle
in proceedings of the conference on empirical methods in natural language ing emnlp
diederik p kingma and max welling

in proceedings of the encoding variational bayes
international conference on learning tions iclr
shasha xie yang liu and hui lin

evaluating the effectiveness of features and sampling in tive meeting summarization
in ieee spoken language technology workshop slt
manling li lingyu zhang heng ji and richard j radke

keep meeting summaries on topic abstractive multi modal meeting summarization
in proceedings of the annual meeting of the tion for computational linguistics acl
ziyi yang chenguang zhu robert gmyr michael zeng xuedong huang and eric darve

ted a pretrained unsupervised summarization model with theme modeling and denoising
arxiv preprint

chin yew lin

rouge a package for automatic evaluation of summaries
in acl workshop on text summarization branches out
lin yuan and zhou yu

abstractive dialog marization with semantic scaffolds
arxiv preprint

chunyi liu peng wang jiang xu zang li and jieping ye

automatic dialogue summary generation for customer service
in acm sigkdd international conference on knowledge discovery and data mining
rada mihalcea and paul tarau

textrank ing order into text
in proceedings of the conference on empirical methods in natural language ing emnlp
haojie pan junpei zhou zhou zhao yan liu deng cai and min yang

end end dialogue description generation
arxiv preprint

xinyuan zhang yitong li dinghan shen and lawrence carin

diffusion maps for textual in advances in neural network embedding
mation processing systems neurips
xinyuan zhang yi yang siyang yuan dinghan shen and lawrence carin

syntax infused tional autoencoder for text generation
in ings of the annual meeting of the association for computational linguistics acl
hao zheng and mirella lapata

sentence trality revisited for unsupervised summarization
in proceedings of the annual meeting of the tion for computational linguistics acl

