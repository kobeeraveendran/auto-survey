text summarization using abstract meaning representation shibhansh dohare cse department iit kanpur
iitk
ac
in harish karnick cse department iit kanpur
iitk
ac
in vivek gupta microsoft research bangalore t
com l u j l c
s c v
v i x r a abstract with an ever increasing size of text present on the internet automatic summary eration remains an important problem for in this natural language understanding
work we explore a novel pipeline for text summarization with an termediate step of abstract meaning resentation amr
the pipeline proposed by us rst generates an amr graph of an input story through which it extracts a summary graph and nally generate mary sentences from this summary graph
our proposed method achieves state the art results compared to the other text summarization routines based on amr
we also point out some signicant lems in the existing evaluation methods which make them unsuitable for ing summary quality
introduction summarization of large texts is still an open lem in language processing
people nowadays have lesser time and patience to go through large pieces of text which make automatic tion important
automatic summarization has nicant applications in summarizing large texts like stories journal papers news articles and even larger texts like books
existing methods for summarization can be broadly categorized into two categories tive and abstractive
extractive methods picks up words and sometimes directly sentences from the text
these methods are inherently limited in the sense that they can never generate human level summaries for large and complicated documents which require rephrasing sentences and rating information from full text to generate maries
most of the work done on summarization in past has been extractive
on the other hand most abstractive methods take advantages of the recent developments in deep learning
specically the recent success of the sequence to sequence learning models where recurrent networks read the text encodes it and then generate target text
though these methods have recently shown to be competitive with the tractive methods they are still far away from ing human level quality in summary generation
the work on summarization using amr was started by liu et al

abstract meaning representation amr was as introduced by narescu et al

amr focuses on capturing the meaning of the text by giving a specic ing representation to the text
amr tries to ture the who is doing what to whom in a tence
the formalism aims to give same tation to sentences which have the same ing meaning
for example he likes apple and apples are liked by him should be assigned the same amr
liu et al
s approach aimed to produce a summary for a story by extracting a summary subgraph from the story graph and nally ate a summary from this extracted graph
but cause of the unavailability of amr to text tor at that time their work was limited till ing the summary graph
this method extracts a single summary graph from the story graph
tracting a single summary graph assumes that all of the important information from the graph can be extracted from a single subgraph
but it can be difcult in cases where the information is spread out in the graph
thus the method compromises between size of the summary sub graph and the amount of information it can extract
this can be easily solved if instead of a single sub graph we extract multiple subgraphs each focusing on background amr parsing and generation amr was introduced by banarescu et al
with the aim to induce work on statistical natural language understanding and generation
amr represents meaning using graphs
amr graphs are rooted directed edge and vertex labeled graphs
figure shows the graphical representation of the amr graph of the sentence i looked carefully all around me generated by jamr parser gan et al

the graphical representation was produced using amrica saphra and lopez
the nodes in the amr are labeled with concepts as in figure around represents a cept
edges contains the information regarding the relations between the concepts
in figure tion is the relation between the concepts and around
amr relies on propbank for tic relations edge labels
concepts can also be of the form where the index represents the rst sense of the word run
further details about the amr can be found in the amr guidelines narescu et al

a lot of work has been done on parsing tences to their amr graphs
there are three main approaches to parsing
there is alignment based parsing flanigan et al
parser zhou et al
which uses graph based algorithms for concept and relation tication
second grammar based parsers like wang et al
camr generate output by performing shift reduce transformations on output of a dependency parser
neural parsing konstas et al
peng et al
is based on using models for parsing the main problem for neural methods is the absence of a huge corpus of human generated amrs
peng et al
duced the vocabulary size to tackle this problem while konstas et al
used larger external corpus of external sentences
recently some work has been done on ducing meaningful sentences form amr graphs
flanigan et al
used a number of tree to string conversion rules for generating sentences
song et al
reformed the problem as a eling salesman problem
konstas et al
used learning methods
datasets we used two datasets for the task amr bank knight et al
and cnn dailymail figure the graphical representation of the amr graph of the sentence i looked carefully all around me using amrica mation in a different part of the story
we propose a two step process for extracting multiple summary graphs
first step is to select few sentences from the story
we use the idea that there are only few sentences that are tant from the point of view of summary i
e
most of the information contained in the summary is present in very few sentences and they can be used to generate the summary
second step is to extract important information from the selected sentences by extracting a sub graph from the selected tences
our main contributions in this work are three folds we propose a pipeline for text summarization providing strong baseline for future work on summarization using amr
present a novel approach for extracting ple summary graphs that outperforms the vious methods based on a single sub graph extraction
expose some problems with existing uation methods and datasets for abstractive summarization
the rest of the paper is organized as follows
section contains introduction to amr section and contains the datasets and the algorithm used for summary generation respectively
tion has a detailed step by step evaluation of the pipeline and in section we discuss the problems with the current dataset and evaluation metric
mann al
nallapati et al

we use the proxy report section of the amr bank as it is the only one that is relevant for the task cause it contains the gold standard human ated amr graphs for news articles and the maries
in the training set the stories and maries contain
sentences and
sentences on an average respectively
the training and test sets contain and summary document pairs respectively
cnn dailymail corpus is better suited for marization as the average summary size is around or sentences
this dataset has around document summary pairs with stories having sentences on average
the dataset comes in sions one is the anonymized version which has been preprocessed to replace named entities e

the times of india with a unique identier for ample
second is the non anonymized which has the original text
we use the anonymized version of the dataset as it is more suitable for amr parsing as most of the parsers have been trained on non anonymized text
the dataset does not have gold standard amr graphs
we use automatic parsers to get the amr graphs but they are not gold standard and will effect the quality of nal summary
to get an idea of the error introduced by using automatic parsers we compare the results after using gold standard and automatically generated amr graphs on the standard dataset
pipeline for summary generation the pipeline consists of three steps rst convert all the given story sentences to there amr graphs followed by extracting summary graphs from the story sentence graphs and nally generating tences from these extracted summary graphs
in the following subsections we explain each of the methods in greater detail

step story to amr as the rst step we convert the story sentences to their abstract meaning representations
we use jamr parser version flanigan et al
as its openly available and has a performance close to the state of the art parsers for parsing the dailymail corpus
for the amr bank we have the gold standard amr parses but we still parse the input stories with jamr parser to study the effect of using graphs produced by jamr parser instead figure graph of the best recall scores for summaries around sentences in the cnn dailymail corpus
y axis is the rogue score and x axis is the cumulative percentage of sentence with the corresponding score of the gold standard amr graphs

step story amr to summary amr after parsing step we have the amr graphs for the story sentences
in this step we extract the amr graphs of the summary sentences using story sentence amrs
we divide this task in two parts
first is nding the important sentences from the story and then extracting the key information from those sentences using their amr graphs


selecting important sentences our algorithm is based on the idea that only few sentences are important from the point of view of summary i
e
there are only a few sentences which contain most of the important information and from these sentences we can generate the mary
hypothesis most of the information sponding to a summary sentence can be found in only one sentence from the story
to test this hypothesis for each summary tence we nd the sentence from the story that tains maximum information of this summary tence
we use lin recall scores measures the ratio of number of words in the get summary that are contained in the predicted summary to the total number of words in the target summary as the metric for the information tained in the story sentence
we consider the story sentence as the predicted summary and the mary sentence as the target summary
the results that we obtained for randomly chosen ment summary pairs from the cnn dailymail pus are given in gure
the average recall score that we obtained is
the score will be fectly when the summary sentence is directly picked up from a story sentence
upon manual inspection of the summary sentence and the sponding best sentence from the story we realized when this score is more than
or
almost ways the information in the summary sentence is contained in this chosen story sentence
the score for in these cases is not perfectly because of stop words and different verb forms used in story and summary sentence
around of summary tences have score above

so our hypothesis seems to be correct for most of the summary tences
this also suggests the highly extractive ture of the summary in the corpus
now the task in hand is to select few tant sentences
methods that use sentence tion for summary generation can be used for the task
it is very common in summarization tasks specically in news articles that a lot of mation is contained in the initial few sentences
choosing initial few sentences as the summary produces very strong baselines which the state the art methods beat only marginally
ex
on the cnn dailymail corpus the state of the art tive method beats initial sentences only by
as reported by nallapati et al

using this idea of picking important sentences from the beginning we propose two methods rst is to simply pick initial few sentences we call this rst n method where n stands for the ber of sentences
we pick initial sentences for the cnn dailymail corpus i
e
and only the rst sentence for the proxy report section amr bank i
e
as they produce the best scores on the rogue metric compared to any other rst n
second we try to capture the relation between the two most important entities we dene importance by the number of occurrences of the entity in the story of the document
for this we simply nd the rst sentence which contains both these entities
we call this the rst co occurrence based sentence selection
we also select the rst sentence along with rst co occurrence based sentence selection note that methods rst n and rst co are by default followed by the summary graph extraction step and they are not just sentence selection methods
as the important sentences
we call this the rst co based sentence selection


extracting summary graph as the datasets under consideration are news ticles
the most important information in them is about an entity and a verb associated with it
so to extract important information from the tence
we try to nd the entity being talked about in the sentence we consider the most referred tity one that occurs most frequently in the text now for the main verb associated with the entity in the sentence we nd the verb closest to this entity in the amr graph
we dene the closest verb as the one which lies rst in the path from the entity to the root
we start by nding the position of the most ferred entity in the graph then we nd the closest verb to the entity
and nally select the subtree hanging from that verb as the summary amr

step summary generation to generate sentences from the extracted amr graphs we can use already available generators
we use neural amr konstas et al
as it provides state of the art results in sentence ation
we also use flanigan et al
generator in one of the experiments in the next section
generators signicantly effect the results we will analyze the effectiveness of generator in the next section
results and analysis
baselines for the cnn dailymail dataset in this section we present the baseline models and analysis method used for each step of our pipeline
the model is considered a strong baseline both the stractive paulus et al
and extractive lapati et al
state of the art methods on this dataset beat this baseline only marginally
the model simply produces the leading three sentences of the document as its summary
the key step in our pipeline is i
e
mary graph extraction
directly comparing the baseline with amr based pipeline to evaluate the effectiveness of is an unfair comparison because of the errors introduced by imperfect parser and tor in the amr pipeline
thus to evaluate the fectiveness of against baseline we table comparison with previous methods and baselines
this table reports rogue scores on the proxy report section using alignment based generator
recall precision method liu et al
amr st co occurrence rst











table table for analyzing the effect of using jamr parser in
this table has rogue scores after using neural amr for sentence generation i
e

first half contains scores by using standard amr graphs second half has amr graphs generated by jamr parser method recall precision rogue l amr rst co occurrence rst using gold standard amr in





using jamr parser for amr rst co occurrence rst























need to nullify the effect of errors introduce by amr parser and generator
we achieve this by trying to introduce similar errors in the leading thre sentences of each document
we generate the amr graphs of the leading three sentences and then generate the sentences using these amr graph
we use parser and generator that were used in our pipeline
we consider these generated tences as the new baseline summary we shall now refer to it amr baseline in the remaining of the paper
for the proxy report section of the amr bank we consider the amr model as the line
for this dataset we already have the standard amr graphs of the sentences
therefore we only need to nullify the error introduced by the generator

procedure to analyze and evaluate each step for the evaluation of summaries we use the dard rogue metric
for comparison with vious amr based summarization methods we report the recall precision and scores for
since most of the literature on marization uses scores for and rogue l for comparison we also report scores for and rogue l for our method
recall and precision are sured for uni gram overlap between the reference and the predicted summary
on the other hand uses bi gram overlap while l uses the longest common sequence between the target and the predicted summaries for evaluation
in rest of this section we provide methods to lyze and evaluate our pipeline at each step
amr parsing to understand the fects of using an amr parser on the results we compare the nal scores after the following two rst when we use the gold standard amr graphs and second when we used the amr graphs generated by jamr parser in the pipeline
tion
contains a comparison between the two
summary graph extraction for uating the effectiveness of the summary graph traction step we compare the nal scores with the lead n amr baselines described in section

in order to compare our summary graph traction step with the previous work liu et al
we generate the nal summary using the same generation method as used by them
their method uses a simple module based on alignments for generating summary after
the ments simply map the words in the original tence with the node or edge in the amr graph
to generate the summary we nd the words aligned with the sentence in the selected graph and put them in no particular order as the predicted summary
though this does not generate matically correct sentences we can still use the metric similar to liu et al
as it is based on comparing uni grams between the target and predicted summaries
generation for evaluating the ity of the sentences generated by our method we compare the summaries generated by the model and amr model on the standard dataset
however when we looked at the scores given by rogue we decided to do get the above summaries evaluated by humans
this duced interesting results which are given in more detail in section


results on the proxy report section in table we report the results of using the pipeline with generation using the alignment based ation module dened in section
on the proxy report section of the amr bank
all of our ods out perform liu et al
s method
we obtain best scores using the rst model for important sentences
this also out perform our amr baseline by
points

effects of using jamr parser in this subsection we analyze the effect of ing jamr parser for instead of the standard amr graphs
first part of table has scores after using the gold standard amr graphs
in the second part of table we have included the scores of using the jamr parser for amr graph generation
we have used the same neural amr for sentence generation in all methods
scores of all methods including the amr baseline have dropped signicantly
the usage of jamr parser has affected the scores of rst co and more than that for the amr
the drop in rogue score when we use rst is around two rogue points more than when amr
this is a surprising result and we believe that it is worthy of further research

effectiveness of the generator in this subsection we evaluate the effectiveness of the sentence generation step
for fair comparison at the generation step we use the gold standard amrs and do nt perform any extraction in instead we use full amrs this allows to move any errors that might have been generated in and
in order to compare the ity of sentences generated by the amr we need a gold standard for sentence generation step
for this we simply use the original sentence as standard for sentence generation
thus we pare the quality of summary generated by and amr
the scores using the rogue metric are given in bottom two rows of table
the results show that there is signicant drop in amr when compared to
we perform human evaluation to check whether the drop in rogue scores is because of drop in information contained and human readability or is it because of the inability of the rogue ric to judge
to perform this evaluation we domly select ten test examples from the three test cases of the proxy report section
for each example we show the summaries generated by four different models side by side to the man evaluators
the human evaluator does not know which summaries come from which model
a score from to is then assigned to each summary on the basis of readability and mation contained of summary where sponds to the lower level and to the highest
in table we compare the scores of these four cases as given by rogue along with human uation
the parser generator pairs for the four cases are gold neural gold neural and the original sentence spectively
here gold parser means that we have used the gold standard amr graphs
the scores given by the humans do not relate with rogue
human evaluators gives most similarly scores to summary generated by the and amr with amr tually performing better on readability though it dropped some information as clear from the scores on information contained
on the other hand rogue gives very high score to while models and get almost same scores
the similar scores of model and shows that tors are actually producing meaningful sentences
thus the drop in rogue scores is mainly due to the inability of the rogue to evaluate abstractive summaries
moreover the rogue gives model higher score compared to model while human evaluators give the opposite scores on information table comparion of the scores given by rogue and human evaluators on different models
scores suggest that rogue do nt co relate with the human evaluators parser generator jamr gold neural jamr gold neural original sentence information contained readability











r l







table results on cnn dailymail corpus
table has parts
first part contains baselines our method and the state of the art on the non anonymized dataset second part has scores on the anonymized dataset
method amr baseline non anonymized see et al
pointer generator coverage see et al
anonymized nallapati et al
rl with intra attention paulus et al
recall

rogue





precision







l





contained in the sentence
a possible reason for the inability of the rogue metric to properly evaluate the maries generated by our method might be due to its inability to evaluate restructured sentences
amr formalism tries to assign the same amr graphs to the sentences that have same meaning so there exists a one to many mapping from amr graphs to sentences
this means that the automatic generators that we are using might not be trying to generate the original sentence instead it is ing to generate some other sentence that has the same underlying meaning
this also helps in plaining the low and rogue l scores
if the sentences might be getting rephrased they would loose most of the and tri grams from the original sentence resulting in low and rogue l scores

analyzing the effectiveness of amr extraction the aim of extracting summary graphs from the amr graphs of the sentence is to drop the not so important information from the sentences
if we are able to achieve this perfectly the recall scores that we are getting should remain almost the same since we are not add any new information and the precision should go up as we have thrown out some useless formation thus effectively improving the overall score
in the rst two rows of ble we have the scores after using the full amr and extracted amr for generation respectively
it is safe to say that extracting the amr results in improved precision whereas recall reduces only slightly resulting in an overall improved

results on the cnn dailymail corpus in table we report the results on the dailymail corpus
we present scores by using the model
the rst row contains the amr baseline
the results we achieve are petitive with the amr baseline
the rest of the table contains scores of baseline followed by the state of the art method on the anonymized and non anonymized versions of the dataset
the drop in the scores from the anonymized to amr is signicant and is largely because of the error introduced by parser and generator
related work and discussion
related work dang and owczarzak showed that most of the work in text summarization has been tive where sentences are selected from the text which are then concatenated to form a summary
vanderwende et al
transformed the input to nodes then used the pagerank algorithm to score nodes and nally grow the nodes from value to low value using some heuristics
some of the approaches combine this with sentence pression so more sentences can be packed in the summary
mcdonald martins and smith almeida and martins and gillick and favre among others used ilps and proximations for encoding compression and traction
recently some abstractive approaches have also been proposed most of which used sequence to sequence learning models for the task
rush nallapati chopra et al
et al
et al
see et al
used standard encoder decoder models along with their variants takase et al
to generate summaries
incorporated the amr information in the dard encoder decoder models to improve results
our work in similar to other graph based tive summarization methods penn and cheung and gerani et al

penn and ung used dependency parse trees to produce summaries
on the other hand our work takes vantage of semantic graphs

need of an new dataset and evaluation metric rogue metric by it is design has lots of ties that make it unsuitable for evaluating tive summaries
for example rogue matches exact words and not the stems of the words it also considers stop words for evaluation
one of the reasons why rogue like metrics might never become suitable for evaluating abstractive maries is its incapabilities of knowing if the tences have been restructured
a good evaluation metric should be one where we compare the ing of the sentence and not the exact words
as we showed section
rogue is not suitable for evaluating summaries generated by the amr pipeline
we now show why the cnn dailymail corpus is not suitable for abstractive summarization
the nature of summary points in the corpus is highly extractive section

for details where most of the summary points are simply picked up from some sentences in the story
tough this is a good enough reason to start searching for better dataset it is not the biggest problem with the dataset
the dataset has the property that a lot of important formation is in the rst few sentences and most of the summary points are directly pick from these sentences
the extractive methods based on tence selection like summarunner are not ally performing well the results they have got are only slightly better than the baseline
the work does nt show how much of the selected tences are among the rst few and it might be the case that the sentences selected by the extractive methods are mostly among the rst few sentences the same can be the problem with the abstractive methods where most the output might be getting copied from the initial few sentences
these problems with this corpus evoke the need to have another corpus where we do nt have so much concentration of important information at any location but rather the information is more spread out and the summaries are more abstractive in nature
possible future directions as this proposed algorithm is a step by step cess we can focus on improving each step to duce better results
the most exciting ments can be done in the summary graph tion method
not a lot of work has been done to extract amr graphs for summaries
in order to make this pipeline generalizable for any sort of text we need to get rid of the hypothesis that the summary is being extracted exactly from one tence
so the natural direction seems to be joining amr graphs of multiple sentences that are similar and then extracting the summary amr from that large graph
it will be like clustering similar tences and then extracting a summary graph from each of these cluster
another idea is to use amr graphs for important sentence selection
conclusion in this work we have explored a pipeline using amr for summarization for the rst time
we propose a new method for ing summary graph which outperformed previous methods
overall we provide strong baseline for text summarization using amr for possible future works
we also showed that rogue ca nt be used for evaluating the abstractive summaries generated by our amr pipeline
references miguel b
almeida and andre f
t
martins

fast and robust compressive summarization with dual composition and multi task learning
in ings of acl
laura banarescu claire bonial shu cai madalina georgescu kira griftt ulf jakob kevin knight philipp koehn martha palmer and nathan schneider

guidelines

com amrisi guidelines blob master amr

laura banarescu claire bonial shu cai madalina georgescu kira griftt ulf hermjakob kevin knight martha palmer philipp koehn and abstract nathan schneider

ing ceedings of linguistic annotation workshop

aclweb
org anthology
representation sembanking
for sumit chopra michael auli and alexander m
rush

abstractive sentence summarization with tentive recurrent neural networks
hoa trang dang and karolina owczarzak

overview of the tac update summarization task
in proceedings of text analysis conference tac
jeffrey flanigan jaime carbonell chris dyer generation from and noah a
smith

tree using abstract meaning in proceedings of the transducers
ference of the north american chapter of the association for computational linguistics

org anthology
representation jeffrey flanigan sam thomson jaime carbonell chris dyer and noah a
smith

a inative graph based parser for the abstract ing representation
in proceedings of the annual meeting of the association for tional linguistics
association for computational linguistics baltimore maryland pages

aclweb
org anthology
shima gerani yashar mehdad giuseppe carenini raymond t
ng and bita nejat

stractive summarization of product reviews using discourse structure
in proceedings of emnlp

org papers pdf
pdf
dan gillick and benoit favre

a scalable global model for summarization
in ings of the naacl workshop on integer ear programming for natural langauge processing

acm
org citation

kevin knight laura baranescu claire bonial madalina georgescu kira griftt ulf hermjakob daniel marcu martha palmer and nathan der

deft phase amr annotation
philadelphia linguistic data tium
abstract meaning representation amr notation release

web download
philadelphia linguistic data consortium
ioannis konstas srinivasan iyer mark yatskar yejin choi and luke zettlemoyer

neural amr sequence to sequence models for parsing and eration
in proceedings of the annual ing of the association for computational guistics
association for computational linguistics

org

c
lin

rouge a package for automatic tion of summaries
text summarization branches out post conference workshop of acl
barcelona spain
fei liu flanigan jeffrey thomson sam sadeh man and smith noah a

toward abstractive in summarization using semantic representations
proceedings of the conference of the north american chapter of the association for tational linguistics
association for computational linguistics denver colorado pages

aclweb
org anthology
andre f
t
martins and noah a
smith

summarization with a joint model in tence extraction and compression
ings of the acl workshop on integer linear programming for natural language processing

aclweb
org anthology
for ryan mcdonald

a study of global inference gorithms in multi document summarization
in ceedings of ecir
ramesh nallapati feifei zhai and bowen zhou

summarunner a recurrent neural network based quence model for extractive summarization of uments
proceedings of the thirty first aaai conference on articial intelligence

org anthology
ramesh nallapati bowen zhou cicero a glar g ulehre abstractive text santos
sequence to sequence rnns and beyond
computational natural
aclweb
org anthology
dos and bing xiang
summarization using in learning
language romain paulus caiming xiong and richard socher

a deep reinforced model for abstractive marization

org

karl moritz hermann tomas kocisky ward grefenstette lasse espeholt will kay and phil blunsom

mustafa suleyman teaching machines to read and comprehend

org

pdf
xiaochang peng chuan wang daniel gildea and anwen xue

addressing the data sparsity sue in neural amr parsing
in proceedings of the conference of the european chapter of the ation for computational linguistics
association for computational linguistics valencia spain pages
aclweb
org anthology

gerald penn and jackie chi kit cheung

for automatic in proceedings of emnlp
supervised sentence enhancement summarization

aclweb
org anthology
alexander m
rush sumit chopra and jason weston

a neural attention model for sentence marization

org

naomi saphra and adam lopez

rica an amr inspector for cross language the system demonstrations of ments
conference of the north american chapter of the association for computational linguistics

aclweb
org anthology
j
liu abigail see peter get manning

marization with pointer generator
org anthology
and christopher d
networks
to the point linfeng song yue zhang xiaochang peng zhiguo wang and daniel gildea

amr to text eration as a traveling salesman problem
ings of the conference on empirical ods in natural language processing
association for computational linguistics austin texas pages

org anthology
sho takase jun suzuki naoaki okazaki tomu hirao and masaaki nagata

ral headline generation on abstract meaning representation
in proceedings of emnlp

org anthology
lucy vanderwende michele banko and arul menezes

event centric summary generation
in proceedings of duc
chuan wang sameer pradhan xiaoman pan heng ji and nianwen xue

camr at task an extended transition based amr parser
in proceedings of the international workshop on semantic evaluation
association for compuational linguistics

aclweb
org anthology
junsheng zhou feiyu xu hans uszkoreit weiguang qu ran li and yanhui gu

amr parsing with an incremental joint model
in proceedings of the conference on empirical methods in ural language processing
association for tational linguistics austin texas pages

org anthology

