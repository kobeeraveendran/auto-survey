from standard summarization to new tasks and beyond summarization with manifold information shen xiuying zhaochun dongyan and rui institute of computer technology peking university beijing china academy of articial intelligence of computer science and technology shandong university qingdao china shengao xy chen zhaody
edu
cn zhaochun

edu
cn a m l c
s c v
v i x r a abstract text summarization is the research area aiming at creating a short and condensed version of the inal document which conveys the main idea of the document in a few words
this research topic has started to attract the attention of a large nity of researchers and it is nowadays counted as one of the most promising research areas
in eral text summarization algorithms aim at using a plain text document as input and then output a mary
however in real world applications most of the data is not in a plain text format
instead there is much manifold information to be rized such as the summary for a web page based on a query in the search engine extreme long ument e

academic paper dialog history and so on
in this paper we focus on the survey of these new summarization tasks and approaches in the real world application
introduction the rapid growth of world wide web means that document oods spread throughout the internet
readers get drown in the sea of documents wondering where to access
text marization system aims at generating a condensed version of a document and conveying the main idea to the reader
users can save a lot of time by reading the summary instead of the whole document to capture the main idea
hence many sites and applications deploy automatic summarization tems and researchers in natural language processing nlp eld also focus on the text summarization task
generally speaking there are two types of text rization
one is designed for the most common scenario that summarizes a plain text with hundreds of words and the most popular usage is the news summarization
the other one on the contrary is designed for generating summary with fold information in which input may be a structured document or document with additional knowledge
different from the plain text summarization task these new summarization tasks aim to produce a better and appropriate summary by rating manifold information in many real world applications corresponding author rui yan
edu
cn figure new summarization tasks introduced in this paper
and scenarios
for example for a search engine it is better to summarize the web page according to the user s query stead of just summarizing the web page ignoring the query
another example is dialog summarization with the ment of online chatting people always chat with people on the web for business or chitchat e

on slack or whatsapp
especially in the business scenario it is helpful to give the user a summary of what has been talked in the past days fore they starting a new dialog session or give a brief duction to the people who just join the group chat about what has been discussed in this group
in contrast to the prosperity of survey on plain text marization task there are no systematic introductions to proaches about how to build an efcient summarization tem which can leverage the manifold information such as structure information of document and additional knowledge in the research community
thus in this survey paper we present a literature review for new summarization tasks and their corresponding methods the tasks are listed in figure
there are eight summarization tasks introduced in this per stream document timeline document treme long document dialog query based document incorporating reader comment template based multi media in these tasks the rst ve tasks can be ed into summarization task with special incorporating ument structure
and the last three tasks can be classied into incorporating additional knowledge into summarization
unlike the conventional plain text summarization methods which are built all with hand crafted rules or feature new summrization tasks stream document summarization timeline summarization template based summarization reader aware summarization multi modal summarization extreme long document summarization query based summarization dialog summarization incorporating document structure incorporating additional knowledge table leaderboard of document summarization task on dailymail dataset

neural methods duc rouge l extractive methods nallapati et al
narayan and et al
wu and hu zhang et al
liu and lapata abstractive methods see et al
paulus and et al
hsu and et al
celikyilmaz and et al
liu and lapata
































ing recently many researchers begin to develop some driven approaches to build summarization systems
since these approaches can leverage the publicly available large scale dataset and the rapid progress of deep learning proaches instead of using time consuming hand crafted ture engineering
therefore we believe it is useful and able to summarize recent progress on new summarization tasks
this survey paper is partially based on our ous efforts on building summarization models for new marization tasks
we will introduce the problem formulation data collection and the proposed methods for these tasks
preliminary standard summarization in this section we will introduce some generally used marization frameworks based on conventional and based learning methods
these frameworks are used as the basis of the methods for new summarization tasks

conventional methods early conventional approaches to extractive summarization include centroid based methods radev and et al
lin and et al
supervised and semi supervised ods wong and et al
tree and graph based ods kikuchi and et al
qian and et al
morita and et al
filippova and et al
submodular methods morita and et al
li and et al
lin and et al
and ilp based methods gillick and et al
li and et al
banerjee and et al

less extractive approaches only extract some phrases or tences from the original document as the summary and it can not produce the condensed and uent summary yates and et al

on the contrary the conventional stractive summarization methods usually extract some words from document and then reorder and perform motivated transformations to the words dorr and al
banko and et al
cohn and lapata barzilay and mckeown tanaka and et al

however these paraphrase based generation method are easy to produce uent sentences
figure techniques used in neural based summarization methods
in contrast to the conventional learning methods based approaches provide an end to end method to rization task
in this section we introduce some widely used techniques in these methods as shown in figure and we split the these methods into two categories extractive and abstractive
most of these works conduct the experiments on a benchmark dataset cnn dailymail and we list the mance in terms of rouge score lin at table
first we will introduce the extractive methods which tract sentences from the document as the summary
since the extractive methods use the sentence as the basic unit the rst step is to obtain a sentence representation
the most common way cheng and lapata narayan and et al
is to employ a recurrent neural network rnn or convolutional neural network cnn to encode the words in a sentence and then obtain a vector representation
ter obtaining the sentence representation cheng and ata rst propose a framework with encoder decoder using an rnn to tackle the extractive summarization task which uses the encoder to obtain the vector representation of sentences and use the decoder with an attention nism to extract sentence
since the encoder decoder marization framework narayan and et al
needs two rnn that computes slowly many researchers start to use the sequence labeling framework nallapati et al
zhou and et al
for this task which use an rnn to read the sentences only once
to deeply understand the document researchers incorporate the memory network chen et al
into summarization framework which gives the ing ability to the model
since previous works use the entropy as the loss function to train the model which has a gap with the testing stage that use the rouge lin score wu and hu propose to use the reinforcement ing method to directly optimize the rouge score
in recent two years the pre training techniques growth rapidly in nlp eld
researchers employ the language model pre training model like elmo bert into summarization task liu and lapata zhang et al
to achieve better mance
from table we can nd that liu and lapata achieve the state of the art performance on the cnn mail dataset
the sequence to sequence based text generation ods sutskever et al
li et al
gao et al
neural summarization methods sequence to sequence copy mechanism reinforcement learning selective encoding multi agent pre train pre train reinforcement learning sequence labeling encoder decoder abstractive methods extractive methods make generating uent and concise summary sible and nallapati and et al
rstly apply the text generation method to the abstractive summarization task
next many extensions on the text generation framework are proposed to achieve better performance in generating summary
to avoid the out of vocabulary oov problem copy mechanism gu and et al
see et al
gulcehre and et al
is proposed which directly copy the oov words like the name of person place or tion from the source document into the generated summary
similarly researchers also use the reinforcement learning method in abstractive summarization paulus and et al
wang and et al
liu and et al
for the same son as the extractive methods
to help the summary tion module focus on the salient parts of source document selective encoding zhou and et al
hsu and et al
are proposed to encode the important semantic parts and ignore the trivial parts
encoding the source document is a crucial step in summarization researchers celikyilmaz and et al
propose to divide the hard document ing task into several sub tasks and solve it by multiple laborating encoder agents
after encoding the document an attention mechanism is used to fuse all the document sentations produced by all the agents and then generate the summary
pre training language model helps the model to capture the semantic of text that motivates the researchers to use the it as the encoder of summarization framework
searchers employ the pre training technique into document reading module liu and lapata to capture the main idea of the document and achieve the state of the art mance in abstractive summarization task
these methods use a plain text as input and they can not utilize the document structure or other easily acquired knowledge to improve the summarization performance
summarization by incorporating document structure in this section we will introduce some summarization tasks in which input is structural text instead of a plain text
these special structures will help the summarization model to ture the document main idea

stream document summarization stream summarization task was rst introduced in tac dang and owczarzak which targets at rizing new documents in a continuously growing text stream such as news events and twitters
when the new document rives in a sequence the stream summary needs to be updated along with considering previous information meantime
initial works include boudin and et al
which poses a scalable sentence scoring method for query oriented update summarization
in this method candidate sentences are selected according to a combined criterion of query vance and dissimilarity with previously read sentences
lort and alfonseca present an unsupervised tic approach to model novelty in a document collection and apply it to the generation of update summaries
ge and et al
propose a graph ranking based method burst mation networks as a novel representation of a text stream
in this method the graph node is a burst word including tities with the time span of one of its burst periods and an edge between two nodes indicates how strongly they are lated
mnasri and et al
is the state of the art work on the duc and tac dataset which examines how integrating a semantic sentence similarity into an update summarization system can improve its results
current state of the art methods for this task are all based on human engineered and extractive methods hong and et al
mnasri and et al

nowadays there are many stream data provided on the internet such as tweets focus on the same news topic and news of a big event like a presidential election a natural disaster
consequently the abstractive based summarization methods will be a hot search area and will be explored in the near future

timeline summarization classic news summarization plays an important role with the exponential document growth on the web
many approaches are proposed to generate summaries but seldom ously consider evolutionary characteristics of news plus to traditional summary elements
timeline summarization is an important research task which can help users to have a quick understanding of the overall evolution of any given topic
it thus attracts much attention from research communities in cent years
to solve this task one should rst identify which sub events are salient and then generate a summary
the big difference between timeline summarization and stream marization task is whether the model can see all the sub event
timeline summarization task is rstly proposed by allan and et al
in this paper they propose a method that tracts a single sentence from each event within a news topic
later a series of works et al
yan et al
yan et al
zhao et al
further investigate the timeline summarization task and all of them are based on conventional learning method to extract sentences from the timeline data
for instance et al
formulate the timeline summarization task as a balanced optimization lem via iterative substitution
the objective function used in this method is measured by four properties relevance erage coherence and diversity
in recent years as an tant case of timeline information social media data is used by many timeline summarization research works
for example ren et al
considered the task of time aware tweets summarization based on a user s history and collaborative social inuences from social circles
the previous works are all based on extractive methods which are not as exible as abstractive approaches
chen et al
rstly propose a key value memory based architecture to store the events described in the line
in this key value memory network they use the event time representation as the key and split the value into two slots global value and local value
the local value only captures event information from current event and the global value stores the global characteristics of events in different time position
finally an rnn based decoder is employed to generate the summary abstractively
to train their model they release the rst large scale timeline summarization dataset which contains document summary pairs collected from a cyclopedia website

extreme long document summarization different from the previous summarization task in some narios the input document can be very long such as an demic paper or a patent document which is longer than the news article
thus summarizing such an extreme long ment is still a challenging problem when using existing marization methods
the biggest challenge of this task is to extract the salient information and central idea from a large amount of information
first we introduce some benchmark datasets used the treme long document summarization
in the era of using conventional machine learning methods in this task all of the researchers use the small scale scientic papers as the dataset teufel and moens teufel and moens liakata et al
and the number document summary data pairs is less than
in recent years most of the searchers employ the neural based methods which require a large amount of data to train the model
thus many scale long document datasets have been proposed and the data comes from wikipedia liu and et al
scientic papers cohan and et al
patent documents sharma et al

liu and et al
propose to use a wikipedia web page with all the reference articles and the sults fetched from google as the long text input and there are document summary data pairs in this dataset
cohan and et al
propose a large scale scientic per summarization dataset which is collected from arxiv and pubmed and it contains document and summary pairs
the average document length is and words in arxiv and pubmed respectively which is times longer than the news dataset cnn dailymail
sharma et al
propose a larger long document summarization dataset patent which is times larger than the pubmed and arxiv
it contains us patent document and tion pairs and the average document length is words
this dataset is more suitable for abstractive summarization task since the summary has more novel n grams than other long document summarization datasets
in the initial works of this task researchers teufel and moens liakata et al
use a supervised er to select content from a scientic paper based on some human engineered features
then researchers have extended these works by applying more sophisticated classiers to identify more ne grain categories
to determine whether a sentence should be included in the summary collins et al
directly use the section each sentence appears in as a categorical feature with values like highlight abstract troduction
as for the neural network based methods liu and et al
rstly use an extractive summarization method to coarsely identify salient information and then employ a ral abstractive model to generate the summary
cohan and et al
propose a hierarchical model that uses two level rnn to encode the words and sections respectively and then they use an attention decoder to forms a context vector from both word and sentence level information
they also duct experiments on arxiv and pubmed datasets and their model outperforms the baseline methods on these datasets
xiao and carenini propose an extractive method for this task using both the global context of the whole ment and the local context within the current topic and this method achieves state of the art performance on the previous two datasets

dialog summarization in recent years online chatting becomes more and more ular qiu al
tao et al
gao et al

when the chatting history becomes very long it is consuming for people to review all the context before starting a new dialog
thus some researchers focus on the task of summarizing the dialog history
different from summarizing a document the salient information is scattered in the whole dialog history
ganesh and dingliwal rst propose this task and they propose a pipeline method that consists of a sequence beling module to identify the salient utterance and a module with attention and copy mechanism
since their dataset is in a small scale they use a news summarization dataset cnn dailymail to train the abstractive tion module and evaluate on a small scale dialog tion dataset with only sessions
to leverage the based text generation method gliwa and et al
pose the rst large scale dataset samsum for this task
ferent from previous papers working on chit chats liu and et al
propose a framework to generate a summary for online customer service which can help the staff to know what was happen without going through long and sometimes twisted utterances
as another branch of dialog summarization task the ing summarization task is to generate a summary of meeting transcriptions
shang and et al
propose an vised abstractive meeting summarization using a graph based model and budgeted submodular maximization
in recent years people usually hold a meeting using video calls instead of just using audio
consequently additional visual tion can be used in meeting summarization such as the ipant s head pose and eye gaze
li and et al
propose a multi modal encoding framework that incorporates this sual information and employs a topic segmentation method to identify the topic transition in a dialog ow
finally they employ the pointer generator see et al
network to fusion the encoded information and generate the summary

query based summarization in the typical web search scenario the search engine provides a list of web pages associated with their summaries
ferent from the traditional document summarization in this scenario the summary should summarize the query focused aspect of the web page instead of the main idea
inspire by this application many researchers start to focus on the based summarization task whose goal is to generate a mary that highlights those points that are relevant in the text of a given query
in this task most of the methods are based on tional machine learning methods
li and li propose a semi supervised graph based model and incorporate the lda topic model into summarization
feigenblat and al
propose an unsupervised multi document query based marization method using a cross entropy method which is a generic monte carlo framework for solving hard torial optimization problems
different from previous tence extraction methods wang and et al
employ a sentence compression method which uses three compression strategy rule based sequence based and tree based to duce the summary
to avoid generating repeated phrases and increasing the versity of summary nema and et al
rstly propose a neural based framework which ensures that context vectors in attention mechanism are orthogonal to each other
specically to alleviate the problem of repeating phrases in the summary we treat successive context vectors as a quence and use a modied lstm cell to compute the new state at each decoding time step
in decoding steps the tion mechanism is also used to focus on different portions of the query at different time steps
summarization by incorporating additional knowledge in some summarization applications there are many ent types of additional knowledge that can be used to help the model enhance the performance
the model can age this additional knowledge to capture the main idea of the document or generate more uent summaries

reader aware summarization in most of the news websites they provide an area for the readers to post their comments on the news article
in most cases the reader comments concentrate on the main idea of the news article
thus these comments can be used to help the summarization model to capture the main idea of the news and then the model can generate a better summary with this help
in this section we will introduce two kinds of methods which are based on conventional learning methods and neural networks respectively
in the beginning et al
rstly propose to derstand the input document with the feedback of readers ing a graph based method where they identify three relations topic quotation and mention by which comments can be linked to one another
li et al
employ a sparse coding based framework for this task which jointly considers news documents and reader comments via an unsupervised data construction strategy
next we turn to the methods using neural networks
li et al
propose a sentence salience estimation framework based on a neural generative model called variational encoder vae
in contrast to the previous methods which use sentence extraction methods on a small scale dataset gao et al
rst propose a large scale dataset and a neural generative method rasg on this task
this dataset contains data samples and each data sample has eral a document a summary and several reader comments the average comments number of a document is

the proposed rasg method is a generative adversarial fellow et al
based learning method which conducts the interaction between the reader comments and news cle to capture the reader attention distribution on the article and then use the reader focused article information to guide the summary generation process

template based summarization to generate a uent summary template based summarization method rst retrieves a summary template and then edits it into the new summary of the current document
existing methods can be classied into two categories hard editing and soft editing
more specically hard editing methods force the system to generate the summary which is in the same language pattern as the template
conversely editing methods can use partial words in the template and generate more exible summaries
wang and cardie introduced a template based cused abstractive meeting summarization system
their tem rst applies a multiplesequence alignment algorithm to generate templates and extracts all summary worthy relation instances from the document
then the templates are lled oya et al
propose a with these relation instances
hard editing method that employs a multi sentence fusion gorithm in order to generate summary templates
since the hard editing methods are not very exible editing methods become popular in recent years due to the velopment of the neural text generation method
cao et al
employ existing summaries as soft templates to ate a new summary
in this method they use an information retrieval system to retrieve summaries of a similar document and then use an attention based generator to fuse the mation from the template and current document
wang et al
leverages template discovered from training data to softly select key information from each source article to guide its summarization process
however this method nores the dependency between the template document and the input document
following these works gao et al
propose to analyze the dependency and use this dependency to help the model identify which facts in the input document are the salient facts that should be mentioned in the summary
furthermore they use the relationship between template ument and template summary to extract the summary in plate that can be reused in generating a new summary
gao et al
they also propose a large scale dataset contains document and summary pairs in which summaries are all in patternized language and their method achieves state of the art performance on this dataset

multi modal summarization with the increase of multi media data on the web many searchers focus on the multi modal summarization task zhu et al
li et al
chen and zhuge li et al
palaskar et al
li et al
in recent years
compared to the traditional text summarization task setting in the multi modal summarization task the visual tion is incorporated along with the input document into the text summarizing process to improve the quality of the erated summary
in the beginning we rst introduce some datasets of li et al
image based multi modal summarization
chen and zhuge propose two large scale modal summarization datasets and each data sample in these datasets contains a source sentence an image collected from the webpage and a summary
different from the previous two datasets which output is only text zhu et al
propose the rst large scale multi modal input and modal output summarization dataset which input is a ment with several images and the ground truth is a text mary with the most relevant image selected from the input images
li et al
palaskar et al
propose two video based multi modal summarization dataset which tains document video and summary pairs and the number of data sample are and respectively
next we will introduce some existing methods of modal summarization task
for the image based multi modal summarization task li et al
chen and zhuge zhu et al
propose to use a based tive model which has image and document encoders to obtain the representations of multi modal input and an rnn based decoder with multi modality attention to generate the mary
since there are some abstract concepts in the source document which can not nd a counterpart in the image and not all the visual information is useful for generating mary
to avoid introducing noise into summarization li et al
propose to use an image attention lter and an image context lter
as for the video based summarization palaskar et al
employ a hara et al
convolutional neural network to model the video frames and then fuse this video information into the using a hierarchical attention mechanism
conclusion we have witnessed a rapid surge of summarization studies recently especially the research in the many new rization tasks
summarization systems are catching on re the state of the art performance of summarization tasks has been pushed higher and higher
in the real world applications most of them are not in a traditional summarization setting and they usually leverage manifold information
since scale big data become more easily available in our living era and it requires much time for people to obtain the overall formation for an event
we may stand at the entrance of future success in more advanced summarization systems
it is our hope that this survey provides an overview of the challenges and the recent progress as well as some future directions in these new summarization tasks which leverages the manifold information
acknowledgements we would like to thank the anonymous reviewers for their constructive comments
this work was supported by the national key research and development program of china no
the national science foundation of china nsfc no
and nsfc no

rui yan is partially supported as a young fellow of beijing institute of articial intelligence baai
references allan and et al
james allan and et al
temporal summaries of new topics
in sigir pages
acm
banerjee and et al
siddhartha banerjee and et al
multi document abstractive summarization using ilp based multi sentence compression
in ijcai
banko and et al
michele banko and et al
headline generation based on statistical translation
in acl
barzilay and mckeown regina barzilay and leen mckeown
sentence fusion for multidocument news summarization
computational linguistics
boudin and et al
florian boudin and et al
a scalable mmr approach to sentence scoring for document update summarization
in coling
cao et al
ziqiang cao wenjie li sujian li and furu wei
retrieve rerank and rewrite soft template based neural summarization
in acl
celikyilmaz and et al
asli celikyilmaz and et al
deep communicating agents for abstractive tion
in naacl
chen and zhuge jingqiang chen and hai zhuge
abstractive text image summarization using multi modal attentional hierarchical rnn
in emnlp
chen et al
xiuying chen shen gao chongyang tao yan song dongyan zhao and rui yan
iterative document representation learning towards summarization with polishing
in emnlp
chen et al
xiuying chen zhangming chan shen gao meng hsuan yu dongyan zhao and rui yan
learning towards abstractive timeline summarization
in ijcai
cheng and lapata jianpeng cheng and mirella pata
neural summarization by extracting sentences and words
in acl pages
cohan and et al
arman cohan and et al
a discourse aware attention model for abstractive rization of long documents
in naacl hlt
cohn and lapata trevor cohn and mirella lapata
sentence compression as tree transduction
j
artif
intell
res

collins et al
ed collins isabelle augenstein and sebastian riedel
a supervised approach to extractive summarisation of scientic papers
in conll pages august
dang and owczarzak hoa trang dang and karolina owczarzak
overview of the tac update tion task
in tac
delort and alfonseca jean yves delort and enrique alfonseca
dualsum a topic model based approach for update summarization
in eacl
dorr and al
bonnie j
dorr and al
hedge mer a parse and trim approach to headline generation
in hlt naacl
hong and et al
kai hong and et al
a repository of state of the art and competitive baseline summaries for generic news summarization
in lrec
feigenblat and al
guy feigenblat and et al
supervised query focused multi document summarization using the cross entropy method
in sigir
filippova and et al
katja filippova and et al
dency tree based sentence compression
in inlg
ganesh and dingliwal prakhar ganesh and saket dingliwal
abstractive summarization of spoken and ten conversation
arxiv

gao et al
shen gao xiuying chen piji li ming chan dongyan zhao and rui yan
how to write learning towards abstractive summaries with patterns in emnlp summarization through prototype editing

gao et al
shen gao xiuying chen piji li zhaochun ren lidong bing dongyan zhao and rui yan
abstractive text summarization by incorporating reader comments
in aaai
gao et al
shen gao zhaochun ren yihong zhao dongyan zhao dawei yin and rui yan
product aware answer generation in e commerce question answering
in wsdm
gao et al
shen gao xiuying chen chang liu li liu dongyan zhao rui yan shen gao xiuying chen chang liu li liu dongyan zhao and rui yan
learning to respond with stickers a framework of unifying modality in multi turn dialog
in www
ge and et al
tao ge and et al
news stream marization using burst information networks
in emnlp
gillick and et al
dan gillick and et al
a scalable global model for summarization
in ilp
gliwa and et al
bogdan gliwa and et al
samsum corpus a human annotated dialogue dataset for tive summarization
in emnlp
goodfellow et al
ian goodfellow jean abadie mehdi mirza bing xu david warde farley sherjil ozair aaron courville and yoshua bengio
erative adversarial nets
in advances in neural information processing systems pages
gu and et al
jiatao gu and et al
incorporating ing mechanism in sequence to sequence learning
acl page
gulcehre and et al
caglar gulcehre and et al
ing the unknown words

hara et al
kensho hara hirokatsu kataoka and yutaka satoh
can spatiotemporal cnns retrace the tory of cnns and imagenet ieee cvf ence on computer vision and pattern recognition pages
hsu and et al
wan ting hsu and et al
a unied model for extractive and abstractive summarization using inconsistency loss
in acl
et al
meishan hu aixin sun and ee peng lim
comments oriented document summarization derstanding documents with readers feedback
in sigir
kikuchi and et al
yuta kikuchi and et al
single document summarization based on nested tree structure
in acl
li and et al
jingxuan li and et al
multi document summarization via submodularity
applied intelligence
li and et al
chen li and et al
using supervised in acl bigram based ilp for extractive summarization

li and et al
manling li and et al
keep meeting summaries on topic abstractive multi modal meeting summarization
in acl
li and li yanran li and sujian li
query focused multi document summarization combining a topic model with graph based semi supervised learning
in coling pages august
et al
piji li lidong bing wai lam huang li and yi liao
reader aware multi document tion via sparse coding
in ijcai
et al
haoran li junnan zhu cong ma jiajun zhang and chengqing zong
multi modal tion for asynchronous collection of text image audio and video
in emnlp
et al
piji li lidong bing and wai lam
reader aware multi document summarization an in hanced model and the rst dataset

et al
haoran li junnan zhu tianshang liu ajun zhang and chengqing zong
multi modal sentence summarization with modality attention and image ltering
in ijcai
et al
h
li j
zhu c
ma j
zhang and c
zong
read watch listen and summarize modal summarization for asynchronous text image audio ieee transactions on knowledge and data and video
engineering may
et al
juntao li lidong bing lisong qiu dongmin chen dongyan zhao and rui yan
learning to write stories with thematic consistency and wording elty
in aaai
liakata et al
maria liakata simon dobnik masree saha colin r
batchelor and dietrich schuhmann
a discourse driven content model for marising scientic articles evaluated in a complex question answering task
in emnlp
lin and et al
chin yew lin and et al
from single to multi document summarization a prototype system and its evaluation
in acl
lin and et al
hui lin and et al
multi document summarization via budgeted maximization of submodular functions
in hlt naacl
lin chin yew lin
rouge a package for in text summarization matic evaluation of summaries
branches out pages july
liu and et al
linqing liu and et al
generative in versarial network for abstractive text summarization
aaai
liu and et al
peter j
liu and et al
generating wikipedia by summarizing long sequences
in iclr
liu and et al
chunyi liu and et al
automatic logue summary generation for customer service
in kdd
liu and lapata yang liu and mirella lapata
text in emnlp summarization with pretrained encoders

mnasri and et al
maali mnasri and et al
taking into account inter sentence similarity for update tion
in ijcnlp
morita and et al
hajime morita and et al
subtree extractive summarization via submodular maximization
in acl
nallapati and et al
ramesh nallapati and et al
stractive text summarization using sequence to sequence rnns and beyond
in conll
nallapati et al
ramesh nallapati feifei zhai and bowen zhou
summarunner a recurrent neural work based sequence model for extractive summarization of documents
in aaai
narayan and et al
shashi narayan and et al
ing sentences for extractive summarization with ment learning
in naacl
nema and et al
preksha nema and et al
diversity driven attention model for query based abstractive marization
in acl
oya et al
tatsuro oya yashar mehdad giuseppe carenini and raymond t
ng
a template based tive meeting summarization leveraging summary and source text relationships
in inlg
palaskar et al
shruti palaskar jindrich libovicky spandana gella and florian metze
multimodal tive summarization for videos
in acl
paulus and et al
romain paulus and et al
a deep reinforced model for abstractive summarization
in iclr
qiu al
lisong qiu juntao li wei bi dongyan zhao and rui yan
are training samples correlated learning to generate dialogue responses with multiple erences
in acl
radev and et al
dragomir r
radev and et al
centroid based summarization of multiple documents
volume pages
ren et al
zhaochun ren shangsong liang edgar meij and maarten de rijke
personalized time aware tweets summarization
in sigir pages
acm
see et al
abigail see peter j
liu and pher d
manning
get to the point summarization with pointer generator networks
in acl
shang and et al
guokan shang and et al
pervised abstractive meeting summarization with sentence compression and budgeted submodular mization
in acl
sharma et al
eva sharma chen li and lu wang
bigpatent a large scale dataset for abstractive and ent summarization
in acl
sutskever et al
ilya sutskever oriol vinyals and quoc v
le
sequence to sequence learning with neural networks
in nips
tanaka and et al
hideki tanaka and et al
driven sentence revision for broadcast news tion
in acl ijcnlp
tao et al
chongyang tao shen gao mingyue shang wei wu dongyan zhao and rui yan
ing towards effective responses with multi head attention mechanism
in ijcai
teufel and moens simone teufel and marc moens
summarizing scientic articles experiments with vance and rhetorical status
computational linguistics
wang and cardie lu wang and claire cardie
domain independent abstract generation for focused meeting summarization
in acl pages gust
wang and et al
lu wang and et al
a sentence compression based framework to query focused document summarization
in acl pages gust
wang and et al
li wang and et al
a reinforced topic aware convolutional sequence to sequence model for abstractive text summarization
in ijcai
wang et al
kai wang xiaojun quan and rui wang
biset bi directional selective encoding with plate for abstractive summarization
in acl pages july
qian and al
xian qian and et al
fast joint in emnlp pression and summarization via graph cuts

wong and et al
kam fai wong and et al
tive summarization using supervised and semi supervised learning
in coling
wu and hu yuxiang wu and baotian hu
learning to extract coherent summary via deep reinforcement ing
in aaai
xiao and carenini wen giuseppe and carenini
long extractive summarization of ments by combining global and local context
in emnlp
xiao et al
rui yan liang kong congrui huang xiaojun wan xiaoming li and yan zhang
timeline eration through evolutionary trans temporal tion
in emnlp pages
acl
et al
rui yan xiaojun wan jahna bacher liang kong xiaoming li and yan zhang
tionary timeline summarization a balanced optimization framework via iterative substitution
in sigir pages
acm
et al
rui yan xiaojun wan mirella lapata wayne xin zhao pu jen cheng and xiaoming li
sualizing timelines evolutionary summarization via in ative reinforcement between text and image streams
cikm pages
acm
yates and et al
alexander yates and et al
in ner open information extraction on the web
naacl
zhang et al
xingxing zhang furu wei and ming zhou
hibert document level pre training of cal bidirectional transformers for document tion
in acl
zhao et al
xin wayne zhao yanwei guo rui yan yulan he and xiaoming li
timeline generation with cial attention
in sigir pages
acm
zhou and et al
qingyu zhou and et al
selective in acl coding for abstractive sentence summarization

zhou and et al
qingyu zhou and et al
neural ment summarization by jointly learning to score and select sentences
arxiv

zhu et al
junnan zhu haoran li tianshang liu yu zhou jiajun zhang and chengqing zong
msmo in multimodal summarization with multimodal output
emnlp

