stepwise extractive summarization and planning with structured transformers shashi narayan joshua maynez shashinarayan joshuahm
com jakub adamek daniele pighin blaz bratanic biondo blazb
com ryan mcdonald google research abstract we propose encoder centric stepwise els for extractive summarization using tured transformers hibert zhang et al
and extended transformers ainslie et al

we enable stepwise rization by injecting the previously generated summary into the structured transformer as an auxiliary sub structure
our models are not only efcient in modeling the structure of long inputs but they also do not rely on task specic redundancy aware modeling making them a general purpose extractive tent planner for different tasks
when ated on cnn dailymail extractive tion stepwise models achieve state of the art performance in terms of rouge without any redundancy aware modeling or sentence ing
this also holds true for rotowire to text generation where our models surpass previously reported metrics for content tion planning and ordering highlighting the strength of stepwise modeling
amongst the two structured transformers we test stepwise extended transformers provides the best formance across both datasets and sets a new standard for these challenges
introduction extractive document summarization is the task of creating a summary by identifying and sequently concatenating the most important tences in a document erkan and radev nenkova and mckeown
in recent years this task has matured signicantly mostly thanks to advances in deep neural networks
cheng and lapata conceptualize extractive tion as a sequence labeling task in which rst a archical long short term memory network lstm equal contribution
code and data are available at
com google research google tree master etcsum
hochreiter and schmidhuber is used to code a document and then another lstm is used to predict for each sentence whether it should be included in the summary
this architecture was later adopted by nallapati et al
nallapati et al
narayan et al
zhang et al
and dong et al

following the success of pre trained based architectures for many tasks vaswani et al
devlin et al
the current state of art approach to extractive summarization uses formers to learn sentence representations and to rank sentences by their saliency liu liu and lapata zhang et al
zhong et al
bi et al

the top scoring sentences are then assembled to produce an extract of the ument
summaries built in this fashion cheng and lapata narayan et al
zhang et al
dong et al
are prone to contain dant information
several recent approaches have explored mechanisms to better handle redundancy such as heuristic based trigram blocking triblk liu and lapata wang et al
crafted feature driven models ren et al
and redundancy aware neural sequence models zhou et al
bi et al

one common lem with these models is that their focus is limited to content overlap and to respecting length gets
however these are but a small subset of the dimensions necessary to produce informative and coherent summaries
ideally models would utilize enriched document and summary representations in order to implicitly learn better extractive plans for producing summaries liu et al
mendes et al

one such method is stepwise marization liu et al
where a summary is constructed incrementally by choosing new content conditioned on previously planned content
in this paper we propose encoder centric wise models for extractive summarization using t c o l c
s c v
v i x r a structured transformers
structured transformers are transformer based architectures that have the exibility to model some form of structure of the input e

hierarchical document structure
in this paper we specically study two such architectures hibert zhang et al
and extended formers construction etc ainslie et al

details of these are given in sections and
we enable stepwise summarization by injecting the previously planned summary content into the tured transformer as an auxiliary sub structure
the model then can holistically learn any level coherence properties such as saliency dancy and ordering embodied in the gold maries
this differs from other methods which are either task specic e

redundancy aware ing in bi et al
or not holistic e

manually curated features in liu et al

an added vantage of structured encoders is that they break the quadratic attention mechanism of transformers vlin et al
making them more efcient and able to process longer inputs instead of truncating the inputs to tokens liu and lapata bi et al
which is critical for long inputs and outputs which require non trivial planning
when evaluated on the cnn dailymail summarization dataset hermann et al
we achieve of the art performance in terms of rouge lin and hovy without any redundancy zhou et al
bi et al
or sentence selection nisms liu and lapata
our model s task agnostic approach allows it to implicitly learn and leverage content plans directly from the data
moreover structured transformers form the basis of our model which are exible in terms of content type e

text or tables that can be modeled
we demonstrate this by learning intricate extractive content plan for the rotowire table to text generation task wiseman et al

this task requires the generation of long summaries from large score tables detailing the the specics of a sports match which often necessitates dedicated content selection and planning models to ate a high quality summary wiseman et al
puduppully et al

we show that our wise framework achieves higher content selection planning and ordering scores relative to prior work with task specic planning mechanisms
the contributions of the paper are as follows this is rst study to use etc ainslie et al
for summarization for its ability and exibility to better model long and structured inputs we pose augmentions of two structured transformers hibert and etc in order to enable stepwise els for extractive planning we demonstrate pirically that our models are general purpose and can be adapted as an extractive document rizer or as a content planner for table to text tion our experiments highlight the effectiveness of stepwise modeling specically stepwise etc which sets a new standard for both tasks
related work redundancy
summarization models often use a dedicated sentence selection step after sentence scoring to address redundancy
maximal marginal relevance carbonell and goldstein based methods select the content that has the maximal score and is minimally redundant with the ously constructed partial summary
others treated sentence selection as an optimization problem der some constraints such as summary length donald lin and bilmes
liu and pata and wang et al
used based trigram blocking triblk for redundancy elimination
ren et al
trained two ral networks with handcrafted features one is used to rank sentences and the other one is used to model redundancy during sentence selection
zhou et al
and bi et al
proposed redundancy aware models by modeling redundancy and saliency jointly during the scoring process ing neural sequence models
in contrast to these proaches our models are not redundancy aware
stead they implicitly model redundancy by ing previously generated summary representations
by virtue of this our models are not text specic and can be applied to other tasks see section
partial summary representations
ultilizing representations of partially generated summaries is relatively less studied in summarization
mendes et al
proposed to dynamically model the generated summary using an lstm to iteratively increment summaries based on previously tracted information
liu et al
used a forward neural network driven by hand curated tures capturing the prevalence of domain subtopics in the source and the summary
to the best of our knowledge our models are rst to use mary representations with structured transformers for summarization
our models learn to make summary informed next sentence predictions out any hand curated features
long form summarization
it is well known that a better content selection benets abstractive summarizers to generate summaries that are not only uent but also informative gehrmann et al
hsu et al
xiao et al

it can be particularly important when generating long stractive summaries liu et al
liu and pata or summarizing multiple documents yasunaga et al

earlier multi document summarization methods have addressed the issue of long form input by graph based representations of sentences or passages erkan and radev christensen et al

recently yasunaga et al
proposed a neural version of this work using graph convolutional networks kipf and welling
liu and lapata used cross document attention mechanism to share formation as opposed to simply concatenating text spans using hierarchical transformers
similar to this motivation we also explore better encoding of long inputs with structured transformers
table to text content planning
wiseman et al
introduced the rotowire dataset which requires multi sentence summaries of large tables
several works found that the key to generate uent and informative summaries for this task is to have dedicated content planning and realization steps puduppully al
c miculicich et al

miculicich et al
and gong et al
used a transformer encoder and gong et al
used multi dimensional hierarchical lstm encoders to compute better table entry representations
following these lines of work we evaluate our models to generate long content plans for this task using structured transformers
problem stepwise content extraction m we dene a general paradigm for stepwise content extraction that can be easily tailored to both tractive summarization and table to text generation
given an input d


sn with n tent units the goal is to learn an extractive content plan i
e
j m j of length m m is an empty unit denoting the end of the plan
we formulate this as an tive ranking problem liu et al
bi et al
where at each k th step k m given the input d and the previously selected plan we select d with a probability d with model parameters
the selected content is then added to to construct
the best plan s can be dened as s arg p d
for extractive document summarization let d


sn be a document with n sentences
our goal is to learn an extractive plan or summary in this case s which best summarizes d
for table to text generation we represent a table with n records as d


sn
we aim to erate a plan m that can be used by a text generator to generate a meaningful and coherent summary
for exposition we use the extractive document summarization setup to introduce our stepwise models with hibert zhang et al
and etc ainslie et al
in the following sections
specically we use sentence as a content unit and previously or partially generated summary for a previously selected content plan
stepwise hibert hierarchical encodings have been used to model put structure with lstms nallapati et al
cheng and lapata narayan et al

zhang et al
proposed hibert with two stacked transformer encoders vaswani et al
for extractive summarization see the middle diagram in figure a sentence encoder that pendently builds representations for each sentence in the document and a document encoder that erates over sentence encodings to build contextual representations for all sentences
these contextual sentence representations are then ingested by a sier to predict the salience score of each sentence in the document
as in standard transformers both encoders have multiple layers with each layer posed of a multi head self attention layer followed by a feed forward sub layer with residual tions he et al
and layer normalizations ba et al

for stepwise hibert at time step k we modify the document encoder with the content plan which is the previously selected sentences in the summary
this is depicted in ure left and allows the model to implicitly select new sentences relative to the previously generated summary
wi


wi sentence and document encoders
let d


sn be a document where wi j is a token in si
si is rst mapped to a ous space esi j
j ptoken are the token is a sentence in d and wi ei


j and ptoken where ei j j figure memory usage and attentions in standard transformers devlin et al
hibert zhang et al
and etc ainslie et al

and positional embeddings of token wi j tively
our transformer based sentence encoder then transforms esi into a list of hidden tations hi j is the hidden representation for wi j
following the standard tice devlin et al
liu and lapata we take the rst hidden representation hi as the representation for the sentence
where hi


hi hi
hi zhang et al
use a standard transformer document encoder
it takes the document sentation hd


hn where hi hi psent are the representation i from the sentence encoder and the positional bedding for sentence si in the document tively and builds contextual sentence tions


dn
and psent i let i


stepwise modeling
at step k be the partial summary with k previously extracted sentences
in tion to hd our document encoder takes the mary representation


where xi hi
hi psum is the representation from the sentence encoder for sentence and psum i is the positional embedding for sentence in
at each layer the document encoder employs three levels of nested multi headed attentions vaswani et al
to build summary informed contextual sentence representations n ument self attention summary self attention and document summary attention see figure left
the rst two operate in parallel followed by the document summary attention



si while document self attention learns the textual hidden representation hdocdoc of each sentence in the document d summary attention learns the contextual hidden tion hsumsum of each sentence in
we share i the parameters of the document and summary attention layers
the document summary attention then builds the contextual hidden representation si si hdocsum ing linear projections of hdocdoc hsumsum of each sentence in the document d as query and as key and values vaswani et al

i in addition to the introduction of stepwise anism to hibert our positional embeddings ptoken are not shared to better j model individual sentences the document and the different styles of summary
zhang et al
shared their token ptoken sitional embeddings
but we both use the lute position encodings used in the original bert model devlin et al

and sentence psent and psum j j j j stepwise etcsum there has been growing interest in addressing the limitation of the transformer architecture used in bert devlin et al
where memory age scales quadratically with the size of the put guo et al
dai et al
ye et al
child et al
rae et al
beltagy et al
roy et al

hibert alleviates this problem by modeling each sentence dently the memory usage in hibert scales with the square of the number of sentences and the square of the maximum length of any sentence
however the main disadvantage of this approach is that token level attention across sentences is hibited and long range attention only happens rectly at the second stage encoder see the middle diagram in figure
recently extended former construction etc ainslie et al
provides an alternative
it alleviates the quadratic memory growth by introducing sparsity to the tion mechanism via its novel global local attention mechanism see the rightmost diagram in figure
this not only permits encoding of long but also enables a mechanism to model structure directly through nodes in the global attention layer
do other recent architectures yang et al
kitaev et al




nodesinput tokensinput tokensinput tokenslayer tokensblockembeddingsblocksinput tokensspecialglobal token


tokenslayer transformer hierarchical attention xl bp transformer star transformer compressive transformer long inputglobal inputfull attentionfull and g attention global local attention etc


nodesinput tokensinput tokensinput tokenslayer tokensblockembeddingsblocksinput tokensspecialglobal token


tokenslayer transformer hierarchical attention xl bp transformer star transformer compressive transformer long inputglobal inputfull attentionfull and g attention global local attention etc figure stepwise hibert left and etcsum right models
hibert builds summary informed representation by jointly modeling partially generated summary and the document during document encoding while etcsum takes as input the document appended with the partially generated summary
global local attention
the etc model tecture receives two inputs a long input which in most cases corresponds to the text to be encoded and an auxiliary global input which serves as ductive bias features
first the model builds an attention map called long to long across the long input with a sparse local attention of xed length this bypasses the quadratic memory complexity and allows to scale input lengths to the thousands of tokens but limits the attention span of tokens to their nearest neighbors
to overcome this limitation the global local tention denes three other attention parts to global global to long and long to global all with unrestricted attention
this allows tokens trarily far apart to attend to each other with at most one hop through the global input tokens
we refer the reader to ainslie et al
for more details
the right parts of figures and illustrate these four types of attentions and the sparsity diagrams where each cell in a row i and column j is ent than white input token wi can attend to input token wj same relative position embeddings are indicated by using the same color
stepwise modeling
given the document d and its partial summary at step k we construct an input i by concatenating the document d and the partial summary
etc replaces absolute position encodings with relative position encodings shaw


al
to easily adapt to greater input lengths than seen during pretraining
in addition to ing relative positions in an input sequence relative position encodings in etc are also used to model arbitrary pairwise token relations useful for tured inputs
we used the auxiliary global input to represent sentence structure
specically following ainslie et al
we placed one auxiliary token in the global input per each sentence in the input i
we linked the global tokens with the input tokens by using relative position labels to represent whether each token belongs to that sentence
global global attention is left unrestricted allowing all sentences to attend to each other
this result is summary informed contextualized input token resentations via attention through the global nodes
in the rest of the paper we refer to this rizer by stepwise etcsum
similar to hibert we take the rst token hidden representation hi as the representation for the sentence
finally tence embeddings are passed to the softmax layer for salience scoring
both hibert and etcsum are then trained with the cross entropy loss
extractive document summarization
experimental setup dataset
we evaluate our models on the cnn and dailymail news highlights datasets mann et al

we used standard splits documents for training validation and testing
we did not anonymize tities or lower case tokens as in narayan et al
zhou et al
zhang et al
liu and lapata
the documents in the cnn dailymail dataset are long the average lengths are
words sentences for cnn and
words
sentences for dailymail
the human written abstracts have and words for cnn and dailymail respectively
we ated summarization quality using rouge
baselines
we compared our stepwise hibert and etcsum models to lead and oracle baselines
lead selects the rst sentences to form the mary while oracle baselines creates a summary by selecting the best possible set of sentences in the document that gives the highest average of and rouge l scores with respect to the human written summary
the oracle cates the input document to tokens
we further compared our models against several aware models neusum zhou et al
and aredsum bi et al
and models that uses trigram blocking triblk liu and lapata for redundancy elimination during sentence tion see the second block in table
to understand the importance of modeling long documents for extractive summarization we also trained bertsum similar to liu and lapata with a receptive capacity of tokens initialized with the bert checkpoint
our sum differs slightly from liu and lapata in that we do nt use segment embeddings
we also report on roberta liu et al
initialized version of bertsum
we also trained non stepwise variants of ert and etcsum models the third block in ble
in this setting hibert and etc do not take partial summaries as input
instead they simply take the input document and generate salient scores using a sigmoid layer for each sentence in the document the top three sentences are then bled to generate the summary
our implementation of hibert differs from zhang et al

for example we do nt pretrain hibert from scratch for document modeling as in zhang et al

instead we initialize our hibert models with publicly available roberta liu et al
checkpoints following the superior performance of models lead oracle oracle full latent zhang et al
refresh narayan et al
banditsum dong et al
neusum zhou et al
exconsum mendes et al
jecs xu and durrett zhong et al
her luo et al
hibert zhang et al
pnbert zhong et al
bertsum liu and lapata aredsum ctx bi et al
hsg wang et al
bertsum large


















bertsum robertasum hibert etcsum our non stepwise models







our stepwise models





stepwise robertasum stepwise stepwise hibert stepwise stepwise etcsum stepwise
































rl
































table rouge scores on the cnn dailymail test set
boldfaced numbers are the best results among comparable models
bertsum large builds on bertlarge layers architectures whereas ours build on bertbase layers architectures
robertasum over bertsum
we use different number of layers in the document encoder ldoc and in the sentence encoder lsent as opposed to equal number of layers l in both encoders of zhang et al

the layers in the document and sentence encoders were initialized with the top and the bottom layers of roberta respectively
all etcsum models were ized with the uncased version of etc pretrained checkpoints ainslie et al
pretrained using the standard masked language model task and the contrastive predictive coding van den oord et al

we also report on the effect of triblk with all our models
we only experiment with the sized models and therefore have layers a den size of lter size of and attention lowercased candidate and reference summaries and used pyrouge with parameters

thank the authors ainslie et al
for sharing their etc checkpoints with us
human etcsum stepwise etcsum stepwise y t i s n e d summary length figure length distributions in etcsum summaries on the cnn dailymail test set
heads
for comparison we report results from bertsum large liu and lapata which uses layers
finally we employ a beam coding to predict summaries using our stepwise models we use a beam size of for a maximum of steps
we do nt allow repeated sentences though this is not a requirement
we refer the reader to the supplementary material for implementation and reproducibility details
generating extractive oracles
following narayan et al
we train models to predict all sentences in oracle full for non stepwise stepwise training learns to do this training
gradually at each step we train model to predict the next sentence in oracle full using the earlier predicted sentences and the document
during testing human written abstracts are used as reference summaries to evaluate our models

results long form summarization
in our experiments etcsum appears to be far more superior than ert when modeling long documents for extractive summarization etcsum outperformed hibert in all cases including stepwise or non stepwise dictions and with or without trigram blocking
the downside of hibert where token level tion across sentences is not possible is not mal for modeling documents
both etcsum and performed better than bertsum and respectively
these results suggest the importance of modeling the whole ument with etcsum rather than truncating it to only tokens to t bertsum
however the improvement may not be attributed solely to sum s ability to model long inputs but also to its better initialization with etc checkpoints ainslie et al
specially when the improvement minishes when compared against robertasum
stepwise vs non stepwise models
first of all trigram ltering seems to be the key in ing redundancy in generated summaries in stepwise models
it helps almost all models cluding our hibert and etcsum except for the single case of robertasum on
estingly we do nt observe the same pattern for our stepwise models
we observe that our wise models both hibert and etcsum out triblk consistently improve over their stepwise counterparts
but when stepwise is plied with triblk we do nt always see ments
we conjecture that our stepwise models themselves are inherently better at avoiding dancy in generated summaries due to the edge of previously generated summary at each diction step and improvements with triblk are not always complementary
the same is also strated in figure density curves show that wise etcsum

follows the human distribution

better than etcsum


with stepwise

we do nt see signicant improvement over stepwise sum
we also report on stepwise robertasum lines and performance dropped compared to sponding non stepwise models
perhaps without any structure in the transformer simple summary concatenation is not a good method for stepwise robertasum to distinguish the document from the summary
there might be better ways than the vanilla concatenation but with stepwise etcsum or hibert it is very natural
stepwise tasum also loses access to the end of the input as the partial summary grows for documents that are already close to tokens in length
finally our stepwise etcsum model out any explicit redundancy or sentence selection mechanisms achieved comparable performance to the state of the art on the cnn dailymail may consider to access the modeling of long puts in etcsum against the truncated inputs in bertsum and robertasum by initializing etcsum with bert or roberta checkpoints and not etc checkpoint
however this is not fair to etcsum as bert or roberta uses absolute position embeddings devlin et al
whereas etc uses relative position embeddings shaw et al

models cc wiseman et al
puduppully al
hierenc gong et al
puduppully al
ms miculicich et al
ms end to end miculicich et al
systran ai detok gong et al
nle saleh et al
hierarchical t rebuffel et al
stepwise hibert realized stepwise hibert planning only stepwise etcsum realized stepwise etcsum planning only























cs












co bleu


































table standard metrics for rotowire relation generation rg precision content selection cs precision and recall content ordering co via the complement of normalized damerau levenshtein distance and bleu score
models marked with a are not directly comparable
boldfaced numbers are the best results among comparable models
extractive summarization task with a smaller model bertsum large liu and lapata with m parameters achieved


rl scores whereas ours with m eters achieved



comparatively stepwise hibert did not do equally well on ument summarization due to the sequential nature of the input
however we demonstrate in section that it is well suited as an extractive content planner for table to text generation
rouge scores in table are computed with a condence interval of
as such wise is signicantly better than all variants of hierbert etcsum and stepwise
for other models such as and this condence interval is not a deciding factor hence we performed one way anova with posthoc tukey hsd tests p

our best model stepwise etcsum performs icantly better than and stepwise on the average of rouge scores
table to text generation task
we further explore our model s ability to learn content plans for the rotowire data to text generation task wiseman et al

the task is to generate a summary of an nba game from its box score a table of statistics detailing the mance of the two teams and of each player
the dataset consists of pairs of box scores and summaries of nba games played from to
the data is split into train tion and test examples
on average there are rotowire dataset is available for download at https
com harvardnlp boxscore data
records in a box score per game
the average summary has
words and
sentences
similar to puduppully al
we pose the problem into two sub problems which we solve independently content planning which sists of selecting which records in the table should be mentioned in the summary in what order and how they should be organized into sentences and realization which uses the content plan to create a human readable summary
we refer the reader to the supplementary material for an example
our main focus in this paper is to demonstrate our els ability to model long and structured rotowire input tables and generate long meaningful content plans
for realization we simply use a roberta liu et al
initialized sequence to sequence transformer model rothe et al
trained to emit the realization sentence by sentence
we train our stepwise models to take a score table and the partially generated content plan and predict the next element in the content plan
this can be either one of the entries in the score table a sentence break or a token marking the end of the plan
unlike extractive summarization here an optimal extractive content plan can have repeated entries from the input table e

team names to better preserve and generate discourse relations among sentences in the target summary pully al
making it a challenging task for other iterative models that prohibit redundancy e

bi et al

for details about model implementation realization and the induction of oracle content plans for training we refer the reader to the supplementary material
we report typical rotowire metrics wiseman et al
using the standard information tion system described by puduppully et al
to extract the box score table relations mentioned in the generated g and in the target t summary
the metrics measure text quality bleu score between g and t relation generation quality the precision of the relations extracted from g against the box score table content selection quality the precision and recall of the relations extracted from g against those extracted from t and content dering quality the complement of the normalized damerau levenshtein distance on the sequences of relations extracted from g and t
we also ducted human evaluation of rotowire summaries
results
we focus on evaluating our stepwise hibert and etcsum models
our results are presented in table
the realized scores assess the quality of our realized summaries and are parable to systems in the rst block in table
we found both stepwise hibert and stepwise sum do content selection particularly well
their very high precision scores
and
respectively combined with good recall
and
respectively outperform puduppully et al
and other recent models on score
in terms of content ordering and bleu score wise hibert
bleu
dld forms worse than puduppully et al

bleu
dld while stepwise etcsum performs signicantly better
bleu
dld
it s possible that a higher bleu score could be achieved by improving our simple sentence sentence realization method
we also report content selection scores for the output of the content planning modules see ning only models in table
we drop name city and date entries from our content plans before puting the metrics in order to make them ble with others in table
we see the roundtrip of realization and subsequent information extraction decreases cs quality slightly for both models the absolute drop of score is
for stepwise hibert and
for stepwise etcsum
human evaluation
participants were shown two summaries of an nba game and asked to pare them with respect to informativeness does a summary present a better selection of the do nt reproduce bertsum or robertasum lines here for two reasons these sequential models are not optimal for tabular data and they are also bounded by an input length of tokens the average length of linearized score tables is tokens per game
we also do nt report on our non stepwise models as they are not suitable to generate ordered content plans as required for this task
informativeness readability models baseline stepwise hibert truncated stepwise etcsum truncated gold











table human evaluation of rotowire summaries
vant facts about the game and readability which summary has a better narrative ow and is ier to read
we randomly selected nba bles and evaluated summaries from baseline man et al
stepwise hibert stepwise etc and gold
the number of sentences were

and
for baseline stepwise hibert stepwise etc and gold respectively
we also included truncated summaries from stepwise bert and stepwise etc to match the number of sentences in corresponding gold summaries
we elicited judgements from three different tors for each pair
we report the scaling scores louviere and woodworth louviere et al

results are presented in table
overall stepwise etc summaries were ranked most informative but they performed worst on readability
the off the shelf sentence level izer see the supplementary material favors the statistics dense sentences of the baseline maries as it tends to hallucinate on less dense plans
future work will aim to address this itation
for infromativeness stepwise etc maries are signicantly better than gold stepwise etc truncated and stepwise hibert truncated summaries
stepwise hibert summaries are nicantly better than both truncated variants
all other differences are not signicant p

for readability baseline summaries are signicantly better than both etc variants and stepwise ert
all other differences are not signicant
conclusion the stepwise structured transformer paradigm emplied by hibert and etcsum can be easily adapted both to extractive document tion or content planning for table to text generation
stepwise etcsum in particular sets a new dard for both tasks
future work will focus on extending our models to generate extractive plans for better abstractive summarization of long or tiple documents liu et al

acknowledgments we thank joshua ainslie and santiago ontanon for sharing their etc code and checkpoints and also giving us feedback on an early draft of this paper
we thank annie louis the london and zurich generation teams the reviewers and the action editor for invaluable feedback
we thank enrique alfonseca and hadar shemtov for their support for longform summarization
references joshua ainslie santiago ontanon chris alberti philip pham anirudh reddy ravula and sumit sanghai

etc encoding long and structured data in in proceedings of the transformers
ence on empirical methods in natural language processing
online
jimmy ba jamie ryan kiros and geoffrey e
hinton

layer normalization
corr

iz beltagy matthew e
peters and arman cohan

longformer the long document transformer
corr

keping bi rahul jha w
bruce croft and asli likyilmaz

aredsum adaptive aware iterative sentence ranking for extractive ment summarization
corr

jaime carbonell and jade goldstein

the use of mmr diversity based reranking for reordering ments and producing summaries
in proceedings of the annual international acm sigir ence on research and development in information retrieval pages new york ny usa
jianpeng cheng and mirella lapata

neural marization by extracting sentences and words
in proceedings of the annual meeting of the sociation for computational linguistics pages berlin germany
rewon child scott gray alec radford and ilya sutskever

generating long sequences with sparse transformers
corr

janara christensen mausam stephen soderland and towards coherent oren etzioni

in proceedings of the document summarization
conference of the north american chapter of the association for computational linguistics man language technologies pages lanta georgia
zihang dai zhilin yang yiming yang jaime bonell quoc le and ruslan salakhutdinov

transformer xl attentive language models beyond in proceedings of the a xed length context
annual meeting of the association for tional linguistics pages florence italy
jacob devlin ming wei chang kenton lee and kristina toutanova

bert pre training of deep bidirectional transformers for language in proceedings of the conference standing
of the north american chapter of the association for computational linguistics human language technologies pages minneapolis nesota
yue dong yikang shen eric crawford herke van hoof and jackie chi kit cheung

sum extractive summarization as a contextual dit
in proceedings of the conference on pirical methods in natural language processing pages brussels belgium
gunes erkan and dragomir r radev

lexrank graph based lexical centrality as salience in text journal of articial intelligence summarization
search
sebastian gehrmann yuntian deng and alexander rush

bottom up abstractive summarization
in proceedings of the conference on cal methods in natural language processing pages brussels belgium
heng gong xiaocheng feng bing qin and ting liu

table to text generation with effective archical encoder on three dimensions row column and time
in proceedings of the conference on empirical methods in natural language cessing and the international joint conference on natural language processing pages hong kong china
li gong josep crego and jean senellart

in tran wngt dgt task
ings of the workshop on neural generation and translation pages hong kong
qipeng guo xipeng qiu pengfei liu yunfan shao xiangyang xue and zheng zhang

in proceedings of the transformer
ence of the north american chapter of the ation for computational linguistics human guage technologies pages lis minnesota
kaiming he xiangyu zhang shaoqing ren and jian sun

deep residual learning for image nition
corr

karl moritz hermann tomas kocisky edward stette lasse espeholt will kay mustafa suleyman and phil blunsom

teaching machines to read in advances in neural and comprehend
tion processing systems pages
ran associates inc
sepp hochreiter and jurgen schmidhuber

neural computation long short term memory

wan ting hsu chieh kai lin ming ying lee kerui min jing tang and min sun

a unied model for extractive and abstractive summarization using inconsistency loss
corr

jordan j louviere and george g woodworth

best worst scaling a model for the largest ence judgments
university of alberta working per
thomas kipf and max welling

supervised classication with graph convolutional networks
corr

nikita kitaev lukasz kaiser and anselm levskaya

reformer the efcient transformer
corr

chin yew lin and eduard hovy

automatic uation of summaries using n gram co occurrence statistics
in proceedings of the human guage technology conference of the north can chapter of the association for computational linguistics pages
hui lin and jeff bilmes

a class of lar functions for document summarization
in ceedings of the annual meeting of the ation for computational linguistics human guage technologies pages portland gon usa
jingyun liu jackie chi kit cheung and annie louis

what comes next extractive corr marization by next sentence prediction


peter j
liu mohammad saleh etienne pot ben goodrich ryan sepassi lukasz kaiser and noam shazeer

generating wikipedia by ing long sequences
in proceedings of the national conference on learning representations vancouver canada
yang liu

fine tune bert for extractive marization
corr

yang liu and mirella lapata

hierarchical transformers for multi document summarization
in proceedings of the annual meeting of the ciation for computational linguistics pages florence italy
yang liu and mirella lapata

text tion with pretrained encoders
in proceedings of the conference on empirical methods in natural language processing and the international joint conference on natural language processing pages hong kong china
yinhan liu myle ott naman goyal jingfei du dar joshi danqi chen omer levy mike lewis luke zettlemoyer and veselin stoyanov

roberta a robustly optimized bert pretraining approach
corr

jordan j louviere terry n flynn and anthony fred john marley

best worst scaling ory methods and applications
cambridge sity press
ling luo xiang ao yan song feiyang pan min yang and qing he

reading like her man reading inspired extractive summarization
in proceedings of the conference on empirical methods in natural language processing and the international joint conference on natural guage processing pages hong kong china
ryan mcdonald

a study of global inference gorithms in multi document summarization
in ceedings of the european conference on ir search page berlin heidelberg
springer verlag
afonso mendes shashi narayan sebastiao miranda zita marinho andre f
t
martins and shay b
hen

jointly extracting and compressing uments with summary state representations
in ceedings of the conference of the north ican chapter of the association for computational linguistics human language technologies pages minneapolis minnesota
lesly miculicich marc marone and hany hassan

selecting planning and rewriting a ular approach for data to document generation and translation
in proceedings of the workshop on neural generation and translation pages hong kong
ramesh nallapati feifei zhai and bowen zhou

summarunner a recurrent neural network based sequence model for extractive summarization of in proceedings of the thirty first aaai uments
conference on articial intelligence pages
ramesh nallapati bowen zhou and mingbo ma

classify or select neural architectures corr for extractive document summarization


ramesh nallapati bowen zhou cicero dos santos caglar gulcehre and bing xiang

tive text summarization using sequence to sequence in proceedings of the rnns and beyond
signll conference on computational natural guage learning pages berlin germany
shashi narayan ronald cardenas nikos topoulos shay b
cohen mirella lapata sheng yu and yi chang

document eling with external attention for sentence extraction
in proceedings of the annual meeting of the association for computational linguistics pages melbourne australia
shashi narayan shay b
cohen and mirella lapata

ranking sentences for extractive tion with reinforcement learning
in proceedings of the conference of the north american ter of the association for computational linguistics human language technologies pages new orleans louisiana
association for tional linguistics
ani nenkova and kathleen mckeown

matic summarization
foundations and trends in information retrieval
aaron van den oord yazhe li and oriol vinyals

representation learning with contrastive dictive coding
corr

ratish puduppully li dong and mirella lapata

data to text generation with content tion and planning
in proceedings of the aaai ference on articial intelligence volume pages
ratish puduppully li dong and mirella lapata

data to text generation with entity in proceedings of the annual meeting ing
of the association for computational linguistics pages florence italy
ratish puduppully jonathan mallinson and mirella lapata

university of edinburgh s sion to the document level generation and in proceedings of the tion shared task
shop on neural generation and translation pages hong kong
jack w
rae anna potapenko siddhant m
jayakumar and timothy p
lillicrap

compressive formers for long range sequence modelling
corr

laure clement rebuffel soulier geoffrey a scoutheeten and patrick gallinari

hierarchical model for data to text generation
in advances in information retrieval pages cham
springer international publishing
pengjie ren zhumin chen zhaochun ren furu wei jun ma and maarten de rijke

leveraging contextual sentence relations for extractive rization using a neural attention model
in ings of the international acm sigir ence on research and development in information retrieval new york ny usa
sascha rothe shashi narayan and aliaksei leveraging pre trained checkpoints eryn

transactions for sequence generation tasks
of the association for computational linguistics

aurko roy mohammad taghi saffar ashish vaswani and david grangier

efcient content based sparse attention with routing transformers
corr

fahimeh saleh alexandre berard ioan calapodescu and laurent besacier

naver labs europe s systems for the document level generation and in proceedings of the lation task at wngt
workshop on neural generation and tion pages hong kong
peter shaw jakob uszkoreit and ashish vaswani

self attention with relative position in proceedings of the conference of tations
the north american chapter of the association for computational linguistics human language nologies pages new orleans louisiana
ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan n gomez lukasz kaiser and illia polosukhin

attention is all you need
in advances in neural information cessing systems pages
curran ciates inc
danqing wang pengfei liu yining zheng xipeng qiu and xuanjing huang

heterogeneous graph neural networks for extractive document marization
in proceedings of the annual ing of the association for computational linguistics pages online
sam wiseman stuart shieber and alexander rush

challenges in data to document generation
in proceedings of the conference on cal methods in natural language processing pages copenhagen denmark
liqiang xiao lu wang hao he and yaohui jin

copy or rewrite hybrid summarization with chical reinforcement learning
in proceedings of the thirty fourth aaai conference on articial gence
jiacheng xu and greg durrett

neural extractive text summarization with syntactic compression
in proceedings of the conference on empirical methods in natural language processing and the international joint conference on natural guage processing pages hong kong china
zhilin yang zihang dai yiming yang jaime g
bonell ruslan salakhutdinov and quoc v
le

xlnet generalized autoregressive pretraining for language understanding
corr

michihiro yasunaga rui zhang kshitijh meelu ayush pareek krishnan srinivasan and dragomir radev

graph based neural multi document summarization
in proceedings of the ence on computational natural language learning pages vancouver canada
zihao ye qipeng guo quan gan xipeng qiu and zheng zhang

bp transformer modelling long range context via binary partitioning
corr

xingxing zhang mirella lapata furu wei and ming zhou

neural latent extractive document marization
in proceedings of the conference on empirical methods in natural language ing pages brussels belgium
xingxing zhang furu wei and ming zhou

bert document level pre training of hierarchical bidirectional transformers for document in proceedings of the annual meeting tion
of the association for computational linguistics pages florence italy
ming zhong pengfei liu danqing wang xipeng qiu and xuanjing huang

searching for tive neural extractive summarization what works and what s next
in proceedings of the annual meeting of the association for computational guistics pages florence italy
ming zhong danqing wang pengfei liu xipeng qiu and xuanjing huang

a closer look at data bias in neural extractive summarization models
in proceedings of the workshop on new frontiers in summarization pages hong kong china
qingyu zhou nan yang furu wei shaohan huang ming zhou and tiejun zhao

neural ment summarization by jointly learning to score and select sentences
in proceedings of the annual meeting of the association for computational guistics pages melbourne australia
a implementation and reproducibility details a
hibert j j we did a wide range of hyperparameter search for hibert
we experimented with the number of layers in the document encoder ldoc the number of layers in the sentence encoder lsent ldoc lsent the initialization and sharing of position embeddings ptoken and psum the initialization and sharing of document j and sentence encoder parameters with bert and roberta checkpoints and the representation of sentence token embedding or average of all token embeddings from the sentence encoder
for extractive summarization we used hibert with a transformer layer sentence encoder and a transformer layer document encoder
the model has parameters
the word position embedding in the sentence encoder is initialized ing the roberta checkpoint but the document and summary sentence position embeddings are learned from scratch
the document self attention and mary self attentions are shared and initialized using the roberta checkpoint the document summary attention is also initialized using the roberta checkpoint
we truncate each document to tences and each sentence to words
we trained all hibert models for steps saving points every steps with a batch size of
following liu and lapata we choose the best model based on the mle loss on the whole validation set
for rotowire we use hibert with a former layer sentence encoder and a former layer document encoder
the model has trainable parameters
we do nt use the document sentence position embeddings for towire as the input consists of a set of entries in a table
we use the summary sentence position embedding to capture the order in the content plan
we use the roberta vocabulary but as discussed in b
we do nt use roberta pretraining instead initializing with random weights
we trained the model with a batch size of until the auc score for predicting the next content plan entry on the validation dataset attened out which came after k steps
since the dataset has examples one for each element in the target content plan for each rotowire example the model saw the entire dataset approximately times
for all hibert models we used cloud tpu accelerators for training and the adam optimizer with a learning rate of

a
etcsum the etcsum model for both extractive rization and table to text generation uses a layer transformer as described in ainslie et al

the model is pretrained with mlm and cpc tives as described in ainslie et al

in total the model has trainable parameters which mostly comes from the long input of tokens and the full attention of of the global tokens
we trained our model with a batch size of for steps approximately equivalent to epochs
we used cloud tpu accelerators for training and inference was done on a taking hours to get predictions for the test set
model selection was done over models performance in the validation set for all models except stepwise models where a subset of the dation set was used instead consisting of the rst examples given the longer inference times
we did a wide range of hyperparameter search where we experimented with learning rate


relative position coding vocabulary size the representation of sentences token embedding or average of all token embeddings from the sentence coder and in additionally non stepwise models we experimented with positive label weight used to for loss calculation
finally we used an adam optimizer with learning rate of

a
realization model we use a robertashare model following rothe et al

the model has trainable parameters
we trained the model until we reached the maximum bleu score on tion data
we trained our model with a batch size of for k steps
since the dataset has amples one for each element in the target content plan in each rotowire example the model saw the entire dataset approximately times
we used cloud tpu accelerators for training
we used the adam optimizer with a learning rate of

b table to text generation b
task table shows a prototypical input table from the rotowire along with a possible content plan and its realization
as shown in the example a well formed content plan can repeat some of the entries from the input table
b
generating oracle content plans the rotowire dataset does not contain ground truth content plans for its summaries
instead we infer them following a similar approach to puduppully et al
but with a few minor modications we use just a single convolutional model stead of an ensemble of convolutional models and lstms our plans maintain the within sentence order of information and may include repetitions if a piece of information is repeated within a sentence in the target summary our plans include tence breaks though we remove sentences with no table entries our content plans can include the match date if it s mentioned in the text e

on saturday when we resolve a pronoun we emit the corresponding player or team name to the tent plan
with respect to table if the realization at the bottom was a reference summary then by applying this process we would obtain the content plan shown in the middle of the table
on average the plans inferred in this fashion have
table entries and
sentences
are not presenting an actual example for legal reasons
b
content planning technical details hibert
conceptually the input to hibert is a sequence of strings
we use three special strings i
e
beg eos eot to explicitly mark the beginning of the content plan the end of a tence and the end of the plan text respectively
the other strings are the values from the table e

points in the same order in which they appear in the text
in practice in an attempt to leverage roberta pre training we place value strings with natural language sentences that we generate from each value using the plates listed in table
for numeric values such as the number of points of a team or player larly to puduppully al
we compute the rank of the value among the instances of the same table entry type and include that in the templated sentence in the form of a which is


nth best
with respect to the example in table the value points would then be represented as the natural language sentence team points scored of chicago bulls is which is best
as we did not observe a signicant benet in terms of auc when predicting the next content plan entry on validation data we eventually ized our model with random weights but retained the natural language representation of the value strings
because hibert has a sentence limit of we do a step by discarding the table entries that are less likely to be mentioned in the summary i
e
all player entries valued n a and as many entries valued as needed
since the table entries are nt naturally ordered we do nt feed a positional embedding psent in the document encoder but we still feed it for the summary encoder
i given the table entries and partial summary bert computes a distribution over the input tences where eos corresponds to emitting a sentence break eot corresponds to ending the content plan and beg is not used
we sample content plans from a trained model by greedy decoding with one modication entries are not allowed to repeat in the content plan except for sentence breaks team names and team cities
if the highest probability sentence would have been a repeat we instead emit the second highest
use the words which is nth best even when a high number is logically detrimental to the team e

when it represents losses
input table match date saturday october team chicago bulls la lakers player michael jordan shaquille o neal


name bulls lakers name michael shaquille


city chicago los angeles surname jordan oneal


at home home away team chicago bulls la lakers


wins points losses points rebounds assists








rebounds




















content plan chicago bulls city chicago bulls name la lakers city la lakers name chicago bulls points la lakers points match date eos
la lakers name la lakers wins la lakers losses shaquille o neal surname shaquille o neal points eos
chicago bulls city chicago bulls name chicago bulls wins chicago bulls losses michael jordan name michael jordan surname michael jordan points michael jordan rebounds eos
the chicago bulls won against the los angeles lakers on saturday
it was a poor showing for the lakers in spite of oneal s point contribution
the chicago bulls best player was predictably michael jordan with points and rebounds
realization table an hypothetical example from the rotowire dataset for an nba game possible sentence content plan and sponding realized sentences below
etcsum
etc models used the same ltered set of table entries used in hibert as input
we catenated these entries into a at input sequence
similarly we used special strings eos eot and beg which correspond to the same cepts as in hibert end of sentence end of text and beginning of text respectively
these special strings are appended at the beginning of the at input sequence
the partial summary input is constructed by catenating the special string beg and the entries that have been predicted so far in order of tion with eos indicating sentences breaks
the full input sequence is then constructed by concatenating a cls delimiter the at input sequence a special separator sep the partial summary and nally a separator sep
both the input sequence and the partial summary are padded to and respectively adding up in total to strings for the full input including the special delimiters
the model uses additional inputs to construct the global local attention
one global token is assigned to each segment in the full input each special limiter gets assigned a global token as well as ery sentence in the input and partial summary
the model has a maximum global token i d of this has to be taken into account for examples where the number of segments input sequence sentences special delimiters and partial inputs is larger than
for those examples we do nt assign global tokens to the tail of the input sequence
to be consistent we use the same decoding egy where we sample content plans greedily but without repeated entries allowed in the content plan except for sentence breaks team names and team cities
b
rotowire realization model the generated content plans are realized via a sequence to sequence transformer model ized with roberta liu et al
following rothe et al
trained to emit the realization sentence by sentence
the input to the model is the concatenation of the following
the text of the previous sentence or the empty string for the rst sentence
the model can use this to pronominalize team and player names if they were already introduced

the literal string beg as a separator

the templated realizations cfr
table of the entries in the sentence s content plan space arated

the literal string context as a rator

the templated representation of the match date

for both teams the templated representations of a the team name the team city c pts d team wins e team losses whether the team was playing at home or away
these are space separated

for each player in the sentence s content plan the templated representations of a start position and which team the player was on
these are space separated
the input after the context tor is provided because we noticed that sometimes the content plan does nt provide all the necessary information for realizing a sentence
for ple sometimes the target text may refer to a player template used match date of match is year yyyy month mm day dd day of week w team name of t is v team city of t is v team quarter points of t is v team quarter points of t is v team quarter points of t is v team quarter points of t is v team free throw percentage of t is v team points scored of t is v team assists of t is v team losses of t is v team wins of t is v team rebounds of t is v team turnovers of t is v team point eld goal percentage of t is v team eld goal percentage of t is v table entry type match date team name team city team pts team pts team pts team pts team ft pct team pts team ast team losses team wins team reb team tov team pct team fg pct team playing at home or away t is home away team of match player rst name player second name player pts player fgm player fga player min player m player player stl player ftm player fta player blk player ast player to player pf player reb player start position player oreb player dreb player fg pct player pct player ft pct the team a player belongs to player rst name of p is v player second name of p is v player points scored of p is v player eld goals made of p is v player eld goals attempted of p is v player minutes played of p is v player point eld goals made of p is v player point eld goals attempted of p is v player steals of p is v player free throws made of p is v player free throws attempted of p is v player blocks of p is v player assists of p is v player turnovers of p is v player fouls of p is v player rebounds of p is v player starting position of p is v player offensive rebounds of p is v player defensive rebounds of p is v player eld goals percentage of p is v player point eld goals percentage of p is v player free throws percentage of p is v p is player of t table the templates we use to create textual representations of the table entries
in the templates yyyy mmm dd w t p and v are placeholders
w encodes the day of week monday is sunday is
x is the name of a team or of a player
v is the value that the team t or player p has for the given table entry in the dataset
the names in the table entry column correspond to the names of properties in the rotowire dataset where possible
models stepwise hibert realized stepwise hibert planning only stepwise etcsum realized stepwise etcsum planning only





cs



co





bleu

table standard metrics for rotowire on validation data
by their starting position and team which is mation that would nt otherwise be provided to the realizer
we create training data from the rotowire maries and their inferred content plans by ting them into sentences together with our inferred content plans
we realize content plans by gressively feeding the sentence produced in the previous step as input to the next step
b
validation data performance we report performance of our best models on the rotowire validation data in table

