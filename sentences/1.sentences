v o n l c
s c v s c v i x r a utilizing the world wide web as an encyclopedia extracting term descriptions from semi structured texts atsushi fujii and tetsuya ishikawa university of library and information science kasuga tsukuba japan
ac
jp abstract in this paper we propose a method to extract descriptions of cal terms from web pages in order to utilize the world wide web as an encyclopedia
we use linguistic patterns and html text structures to extract text fragments ing term descriptions
we also use a language model to discard neous descriptions and a clustering method to summarize resultant scriptions
we show the ness of our method by way of iments
introduction reecting the growth in utilization of readable texts extraction and linguistic knowledge from large sition of corpora has been one of the major ics within the natural language processing nlp community
a sample of linguistic knowledge targeted in past research includes grammars word classes hatzivassiloglou and mckeown and bilingual lexicons smadja et al

while human experts nd it dicult to produce exhaustive and consistent linguistic knowledge automatic methods can help viate problems associated with manual struction
kupiec and maxwell term descriptions which are usually fully organized in encyclopedias are able linguistic knowledge but have seldom been targeted in past nlp literature
as with other types of linguistic knowledge ing on human introspection and supervision constructing encyclopedias is quite expensive
additionally since existing encyclopedias are usually revised every few years in many cases users nd it dicult to obtain descriptions for newly created terms
to cope with the above limitation of ing encyclopedias it is possible to use a search engine on the world wide web as a tute expecting that certain web pages will describe the submitted keyword
however since keyword based search engines often trieve a surprisingly large number of web pages it is time consuming to identify pages that satisfy the users information needs
in view of this problem we propose a method to automatically extract term scriptions from web pages and summarize them
in this paper we generally use web pages to refer to those pages containing textual contents excluding those with only image audio information
besides this we specically target descriptions for technical terms and thus terms generally refer to technical terms
in brief our method extracts fragments of web pages based on patterns or templates typically used to describe terms
web pages are in a sense semi structured data because html hyper text markup language tags provide the textual information contained in a page with a certain structure
thus our method relies on both linguistic and tural description patterns
we used several nlp techniques to automatically produce linguistic patterns
we call this approach nlp based method
we also produced several heuristics ated with the use of html tags which we call html based method
while the former method is language dependent and currently applied only to japanese the latter method is theoretically language independent
our research can be classied from several dierent perspectives
as explained in the beginning of this section our research can be seen as linguistic knowledge extraction
specically our research is related to web mining methods nie et al
resnik
from an information retrieval point of view our research can be seen as constructing domain specic or task oriented web search engines and software agents etzioni mccallum et al

overview our objective is to collect encyclopedic knowledge from the web for which we signed a system involving two processes
as with existing web search systems in the background process our system periodically updates a database consisting of term tions a description database while users can browse term descriptions anytime in the ground process
in the background process depicted as in figure a search engine searches the web for pages containing terms listed in a lexicon
then fragments such as paragraphs of retrieved web pages are extracted based on linguistic and structural description patterns
note that as a preprocessing for the tion process we discard newline codes dant white spaces and html tags that our extraction method does not use in order to standardize the layout of web pages
however in some cases the extraction cess is unsuccessful and thus extracted ments are not linguistically understandable
in addition web pages contain some linguistic information such as special ters symbols and e mail addresses for tact along with linguistic information
sequently those noises decrease extraction curacy
web extraction patterns lexicon search engine extraction description database browser clustering filtering language model figure the control ow of our extraction system
in view of this problem we perform a tering to enhance the extraction accuracy
in practice we use a language model to measure the extent to which a given extracted ment can be linguistic and index only ments judged as linguistic into the description database
at the same time the urls of web pages from which descriptions were extracted are also indexed in the database so that users can browse the full content in the case where descriptions extracted are not satisfactory
in the case where a number of tions are extracted for a single term the sultant description set is redundant because it contains a number of similar descriptions
thus it is preferable to summarize tions rather than to present all the tions as a list
for this purpose we use a clustering method to divide descriptions for a single term into a certain number of clusters and present only descriptions that are tive for each cluster
as a result it is expected that descriptions resembling one another will be in the same cluster and that each cluster corresponds to dierent viewpoints and word senses
possible sources of the lexicon include ing machine readable terminology ies which often list terms but lack tions
however since new terms unlisted in existing dictionaries also have to be ered newspaper articles and magazines tributed via the web can be possible sources
in other words a morphological analysis is performed periodically e

weekly to tify word tokens from those resources in order to enhance the lexicon
however this is not the central issue in this paper
in the foreground process given an input term a browser presents one or more scriptions to a user
in the case where the database does not index descriptions for the given term term descriptions are dynamically extracted as in the background process
the background process is optional and thus term descriptions can always be obtained cally
however this potentially decreases the time eciency for a real time response
figure shows a web browser in which our prototype page presents several japanese descriptions extracted for the word mainingu data mining
for example an english translation for the rst description is as follows data mining is a process that collects data for a certain task and retrieves relations latent in the data
in figure each description uses various expressions but describes the same content data mining is a process which discovers rules latent in given databases
it is expected that users can understand what data mining is by browsing some of those descriptions
in dition each headword deeta mainingu in this case positioned above each description is linked to the web page from which the scription was extracted
in the following sections we rst orate on the nlp html based extraction methods in section
we then elaborate on noise reduction and clustering methods in sections and respectively
finally in tion we investigate the eectiveness of our extraction method by way of experiments
extracting term descriptions
nlp based extraction method the crucial content for the nlp based tion method is the way to produce linguistic figure example japanese descriptions for deeta mainingu data mining
patterns that can be used to describe cal terms
however human introspection is a dicult method to exhaustively enumerate possible description patterns
thus we used nlp techniques to automatically collect description patterns from machine readable encyclopedias cause they usually contain a signicantly large number of descriptions for existing terms
in practice we used the japanese rom world encyclopedia heibonsha which includes approximately entries related to various elds
before collecting description patterns through a preliminary study on the pedia we used we found that term tions frequently contain salient patterns sisting of two japanese bunsetsu phrases
the following sentence which describes the term x contains a typical bunsetsu nation that is x toha and de aru x toha y de aru x is y
de aru itself is not a bunsetsu phrase we use bunsetsu phrases to refer to combinations of several words
in other words we collected description terns based on the co occurrence of two setsu phrases as in the following method
first we collected entries associated with technical terms listed in the world pedia and replaced headwords with a able x
note that the world dia describes various types of words including technical terms historical people and places and thus description patterns vary depending on the word type
for example entries for historical people usually contain when where the people were born and their major butions to the society
however for the purposes of our tion it is desirable to use entries solely ated with technical terms
we then consulted the edr machine readable technical nology dictionary which contains mately terms related to the tion processing eld japan electronic nary research institute and obtained entries associated with terms listed in the edr dictionary
second we used the chasen cal analyzer matsumoto et al
which has commonly been used for much japanese nlp research to segment collected entries into words and assign them parts of speech
we also developed simple heuristics to duce bunsetsu phrases based on the part speech information
finally we collected combinations of two bunsetsu phrases and sorted them according to their co occurrence frequency in ing order
however since the resultant setsu co occurrences even with higher ings are extraneous we supervised veried corrected or discarded the top dates and produced description patterns
figure shows a fragment of the resultant patterns and their english glosses
in this ure x and y denote variables to which technical terms and sentence fragments can be unied respectively
here we are in a position to extract tences that match with description patterns from web pages retrieved by the search engine see figure
in this process we do not english gloss japanese x toha y dearu
x ha y dearu
y wo x to iu
x wo y to sadameru
x is dened as y
y wo x to yobu
x is y
x is y
y is called x
y is called x
figure a fragment of linguistic description patterns we produced
duct morphological analysis on web pages because of computational cost
instead we rst segment textual contents in web pages into sentences based on the japanese tuation system and use a surface pattern matching based on regular expressions
however in most cases term descriptions consist of more than one sentence
this is especially salient in the case where anaphoric expressions and itemization are used
thus it is desirable to extract a larger fragment taining sentences that match with description patterns
in view of this problem we rst use guistic description patterns to briey identify a zone and sequentially search the following fragments relying partially on html tags until a certain fragment is paragraph tagged with p


or p


p in the case where is missing itemization tagged with ul


n sentences identied with the japanese punctuation system where the sentence that matched with a description pattern is positioned as near center as possible where we empirically set n

html based extraction method through a preliminary study on existing web pages we identied two typical usages of we use html tags to identify priate text fragments we call the method described in this section nlp based method in a comparison with the method in section
that relies solely on html tags
html tags associated with describing nical terms
in the rst usage a term in question is lighted as a heading by way of h


b


or tag and followed by its description in a short fragment
in the ond usage terms that are potentially miliar to readers are tagged with the anchor a tag providing hyperlinks to other pages or a dierent position within the same page where they are described
the crucial factor here is to determine which fragment in the page is extracted as a description
for this purpose we use the same rules described in section

however unlike the nlp based method in the based method we extract the fragment that follows the heading and the position linked from the anchor
however in the case where a term in question is tagged with dt we extract the following fragment tagged with dd
note that dt and dd are ently provided to describe terms
the html based method is expected to extract term descriptions that can not be tracted by the nlp based method and vice
in fact in figure the third and fourth descriptions were extracted with the based method while the rest were extracted with the nlp based method
language modeling for filtering given a set of web page fragments extracted by the nlp html based methods we lect fragments that are linguistically standable and index them into the tion database
for this purpose we perform a language modeling so as to quantify the extent to which a given text fragment is guistically acceptable
there are several alternative methods for language modeling
for example grammars are relatively strict language modeling ods
however we use a model based on n gram which is usually more robust than that based on grammars
in other words text ments with lower perplexity values are more linguistically acceptable
in practice we used the cmu cambridge toolkit clarkson and rosenfeld and produced a trigram based language model from two years of mainichi shimbun japanese newspaper articles mainichi shimbun which were automatically segmented into words by the chasen morphological alyzer matsumoto et al

in the current implementation we cally select as the nal extraction results text fragments whose perplexity values are lower than
clustering term descriptions for the purpose of clustering term tions extracted using methods in sections and we use the hierarchical bayesian tering hbc method iwayama and naga which has been used for ing news articles and constructing thesauri
as with a number of hierarchical ing methods the hbc method merges similar items i
e
term descriptions in our case in a bottom up manner until all the items are merged into a single cluster
that is a tain number of clusters can be obtained by splitting the resultant hierarchy at a certain level
at the same time the hbc method also determines the most representative item troid for each cluster
then we present only those centroids to users
the similarity between items is computed based on feature vectors that characterize each item
in our case vectors for each term description consist of frequencies of tent words e

nouns and verbs identied through a morphological analysis appearing in the description
experimentation
methodology we investigated the eectiveness of our traction method from a scientic point of view
however unlike other research topics where benchmark test collections are able to the public e

information retrieval there are two major problems for the purpose of our experimentation as follows production of test terms for which
results scriptions are extracted judgement for descriptions extracted for those test terms
for test terms possible sources are those listed in existing terminology dictionaries
however since the judgement can be erably expensive for a large number of test terms it is preferable to selectively sample a small number of terms that potentially reect the interest in the real world
in view of this problem we used as test terms those contained in queries in the sis test collection kando et al
which consists of japanese queries and imately abstracts in either a bination of english and japanese or either of the languages individually collected from technical papers published by japanese sociations for various elds
this collection was originally produced for the evaluation of information retrieval tems where each query is used to retrieve technical abstracts
thus the title eld of each query usually contains one or more nical terms
besides this since each query was produced based partially on existing nical abstracts they reect the real world terest to some extent
as a result we tracted test terms as shown in table
in this table we romanized japanese terms and inserted hyphens between each morpheme for enhanced readability
note that unlike the case of information trieval e

a patent retrieval where every relevant document must be retrieved in our case even one description can potentially be sucient
in other words in our experiments more weight is attached to accuracy sion than recall
for the search engine in figure we used which is one of the major japanese web search engines
then for each extracted description one of the authors judged it rect or incorrect

rd
nacsis
ac
index en
html
goo
ne
out of the test terms extracted from the nacsis collection for terms goo retrieved one or more web pages
among those test terms our method extracted at least one term description for terms disregarding the judgement
thus the coverage or plicability of our method was

in ble the third column denotes the number of web pages identied by goo
however goo retrieves contents for only the top pages
table also shows the number descriptions judged as correct the column c the total number of descriptions extracted the column t and the accuracy the umn a for both cases with without the trigram based language model
table shows that the nlp html based methods extracted appropriate term tions with a
accuracy and that the trigram based language model further proved the accuracy from
to

in other words only two descriptions are cient for users to understand a term in tion
reading a few descriptions is not consuming because they usually consist of short paragraphs
we also investigated the eectiveness of clustering where for each test term we tered descriptions into three clusters in the case where there are less than four tions individual descriptions were regarded as dierent clusters and only descriptions determined as representative by the hbc method were presented as the nal result
we found that
of descriptions presented were correct ones
in other words users can obtain descriptions from dierent viewpoints and word senses maintaining the extraction accuracy obtained above i
e


however we concede that we did not tigate whether or not each cluster corresponds to dierent viewpoints in a rigorous manner
for the polysemy problem we investigated all the descriptions extracted and found that only korokeishon collocation was ated with two word senses that is word collocations and position of machinery
among the three representative descriptions table extraction accuracy for the test terms c the number of correct descriptions t the total number of extracted descriptions a accuracy
japanese term english gloss zipf s law access control document image understanding intelligent agent data mining digital watermark digital library image retrieval groupware optical ber position measurement genetic algorithm articial intelligence autonomous mobile robot next generation internet zipf no housoku akusesu seigyo bunsho gazou rikai chiteki eejento deeta mainingu denshi sukashi denshi toshokan gazou kensaku guruupuwea hikari faibaa ichi keisoku identeki arugorizumu jinkou chinou jiritsu idou robotto jisedai intaanetto kiiwaado jidou chuushutsu keyword automatic extraction kikai honyaku korokeishon koshou shindan maruchikyasuto media douki nettowaaku toporojii nyuuraru nettowaaku ringu gata nettowaaku shisourasu souraa kaa teromea machine translation collocation fault diagnosis multicast media synchronization network topology neural network ring network thesaurus solar car telomere total trigram w trigram pages c t a c t a







































for korokeishon collocation two sponded to the rst sense and one sponded to the second sense
to sum up the hbc clustering method correctly ed polysemy
conclusion in this paper we proposed a method to tract encyclopedic knowledge from the world wide web
for extracting fragments of web pages taining term descriptions we used tic and html structural patterns typically used to describe terms
then we used a language model to discard irrelevant tions
we also used a clustering method to summarize extracted descriptions based on dierent viewpoints and word senses
we evaluated our method by way of ments and found that the accuracy of our traction method was practical that is a user can understand a term in question by ing two descriptions on average
we also found that the language model and the tering method further enhanced our work
future work will include experiments using a larger number of test terms and tion of extracted descriptions to other nlp research
acknowledgments the authors would like to thank hitachi ital heibonsha inc
for their support with the cd rom world encyclopedia makoto iwayama and takenobu tokunaga for their report naist is naist
japanese
in andrew mccallum kamal nigam jason rennie and kristie seymore

a machine learning approach to building domain specic search engines
in ings of the international joint ence on articial intelligence pages
jian yun nie michel simard pierre abelle and richard durand

language information retrieval based on parallel texts and automatic mining of allel texts from the web
in proceedings of the annual international acm sigir conference on research and development in information retrieval pages
philip resnik

mining the web for bilingual texts
in proceedings of the annual meeting of the association for computational linguistics pages
frank smadja kathleen r
mckeown and vasileios hatzivassiloglou

ing collocations for bilingual lexicons a statistical approach
computational guistics
support with the hbc clustering software and noriko kando national institute of formatics japan for her support with the nacsis collection
references philip clarkson and ronald rosenfeld

statistical language modeling using the cmu cambridge toolkit
in proceedings of pages
oren etzioni

moving up the tion food chain
ai magazine
vasileios hatzivassiloglou and kathleen r
mckeown

towards the automatic identication of adjectival scales ing adjectives according to meaning
in proceedings of the annual meeting of the association for computational tics pages
hitachi digital heibonsha

cd rom world encyclopedia
in japanese
makoto iwayama and takenobu tokunaga

hierarchical bayesian clustering for in automatic text classication
ings of the international joint ence on articial intelligence pages
japan electronic dictionary research
edr electronic dictionary tute
technical guide
noriko kando kazuko kuriyama and hiko nozue

nacsis test collection in proceedings of workshop
the annual international acm sigir conference on research and development in information retrieval pages
julian kupiec and john maxwell

training stochastic grammars from in workshop on labelled text corpora
statistically based natural language gramming techniques
aaai technical ports
mainichi shimbun

mainichi shimbun cd rom
in japanese
yuji matsumoto akira kitauchi tatsuo mashita osamu imaichi and tomoaki imamura

japanese morphological analysis system chasen manual
technical
