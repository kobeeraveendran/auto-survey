p e s l c
s c v
v i x r a xte explainable text entailment vivian s
andre freitasb siegfried handschuha c adepartment of computer science and mathematics university of passau germany bschool of computer science university of manchester uk cinstitute of computer science university of st
gallen switzerland abstract text entailment the task of determining whether a piece of text logically follows from another piece of text is a key component in nlp providing input for many semantic applications such as question answering text marization information extraction and machine translation among others
entailment scenarios can range from a simple syntactic variation to more complex semantic relationships between pieces of text but most approaches try a one all solution that usually favors some scenario to the ment of another
furthermore for entailments requiring world knowledge most systems still work as a black box providing a yes no answer that does not explain the underlying reasoning process
in this work we troduce xte explainable text entailment a novel composite approach for recognizing text entailment which analyzes the entailment pair to decide whether it must be resolved syntactically or semantically
also if a tic matching is involved we make the answer interpretable using external knowledge bases composed of structured lexical denitions to generate ural language justications that explain the semantic relationship holding between the pieces of text
besides outperforming well established ment algorithms our composite approach gives an important step towards explainable ai allowing the inference model interpretation making the mantic reasoning process explicit and understandable
keywords textual entailment knowledge graph interpretability corresponding author email addresses vivian
passau
de vivian s
silva andre

ac
uk andre freitas siegfried

ch siegfried handschuh preprint submitted to articial intelligence september
introduction many natural language processing tasks such as question answering text summarization information retrieval and machine translation among others need to deal with language variability
inputs and outputs can be expressed in dierent forms and determining whether such forms are alent that is if a variant can be inferred from the standard input or output is an important task by itself
however inference mechanisms built within specic nlp applications used to cover only the application s needs and could not be easily reused by other applications since researchers in one area might not be aware of relevant methods developed in the context of another plication
the text entailment paradigm was established as a unifying framework for applied inference providing a means of delivering other nlp task from handling inference issues in an ad manner using instead the outputs of an inference dedicated mechanism
text entailment is dened as a directional relationship between a pair of text expressions denoted by t the entailing text and h the entailed hypothesis
we say that t entails h if typically a human reading t would infer that h is most likely true
three dierent scenarios can be observed t and h are equivalent statements but expressed in dierent ways
ex
t the badger is burrowing a hole
h a hole is being burrowed by the badger
h generalizes information from t
ex
t a dog is riding a skateboard
h an animal is riding a skateboard
h present new information derived from t
ex
t iran is a signatory to the chemical weapons convention
h the chemical weapons convention is an agreement
while can usually be resolved syntactically given that only the tence structure is altered and requires only shallow semantic information such as synonyms and hypernyms requires knowledge that goes beyond what is expressed in t and h demanding the use of external world knowledge to solve the entailment
some text entailment approaches focus on exploring the syntactic tures of t and h trying to transform the syntactic representation of t into that of h to determine whether they are equivalent and conrm the tailment
this kind of approach can fall short of identifying more complex semantic variations like that observed in
on the other hand techniques concentrating purely on nding semantic relations between t and h will struggle to deal with pairs like the one shown in where only a syntactic variation holds
to overcome these issues and better address the variety of entailment phenomena we propose the use of dierent methods to tackle dierent narios integrated as components into a composite approach that performs a routing that is it analyzes the entailment pair identies the most relevant phenomenon present and sends it to the most suitable component to solve it
we split the entailment phenomena into two broad categories syntactic and semantic
for solving syntactic entailments we adopt a tree edit tance algorithm which operates over a dependency tree representation of t and h
for semantic entailments we look for the semantic relationships ing between t and h employing a distributional word embedding based navigation algorithm that explores a graph knowledge base composed of ural language dictionary denitions
by nding paths is this graph linking t and h we can provide human readable justications that shows explicitly what the semantic relationship holding between them is
given the ing importance of explainable ai the ability to explain how decisions are reached is becoming a key demand for intelligent systems and ing natural language justications is an important feature for meeting this requirement and rendering a system interpretable
the contributions of this work intended to equally address and balance the benets for both the nlp and explainable ai elds are a more exible way to deal with dierent entailment scenarios ing the most suitable method for each entailment phenomenon an interpretable denition based commonsense reasoning model which through the generation of natural language explanations allows the nal users to understand and assess the inference process leading to a decision a quantitative and qualitative analysis of dierent knowledge bases erated from various lexical resources showing how they compare cially from the interpretability point of view

related work the introduction of the rte recognizing text entailments stimulated the development of a large number of text entailment frameworks
starting in the rte challenges encouraged the creation of systems capable of capturing textual inferences and given the low accuracy achieved by the rst participants showed that much improvement was still required in the area
the rst text entailment systems were mainly based on shallow methods relying only on word overlap and statistical lexical relations
later approaches have moved to more sophisticated approaches or deep methods combining the analysis of the sentence structure with logical features and linguistic resources like wordnet framenet and verbnet which can add some shallow semantic information to the syntactic data
edit distance alignment and transformation are some examples of such approaches
as a common starting point they translate the text and hypothesis to some kind of syntactic or semantic representation and then try to determine if the representation of the hypothesis is subsumed by that of the text
machine learning classication techniques are also employed where t and h are represented as feature vectors and multiple similarity measures computed over lexical syntactic and shallow semantic tions are used to train a supervised machine learning model
regarding the use of external world knowledge silva et al
proposed an approach that focuses on semantic entailments also exploring structured knowledge bases to nd semantic relationships that conrm and explain the entailment
regarding the generation of natural language justications only a few text entailment systems provide such feature most approaches only output a yes no answer and sometimes a condence score but no explanation or evidence that support the entailment decision
the third rte challenge proposed an optional task which required a system to make
gl three way entailment decisions entails contradicts neither and to justify its response
human evaluators judging the outputs provided by the competing systems pointed out a number of problems notably the use of vague and abstract phrases such as there is a relation between and there is a match which shows that rather than simply detecting that the semantic relation exists it is also very important to describe what exactly this specic semantic relation is so users can understand and trust the system s decisions
with respect to the entailment recognition methods adopted in this work although tree edit distance has already been used in text entailment graph traversal approaches have been explored more often in other areas such as information retrieval text mining text summarization and semantic similarity
graphs were also already used in text entailment but as a set of potential entailment rules extracted from text
the use of graph traversal methods for exploring independent external knowledge bases for injecting world knowledge in the entailment recognition process and for generating explanations for a system s decisions is still an emerging eld

text entailment vs
natural language inference in the last years with the end of the rte challenges the development of new textual entailment approaches have slowed down
on the other hand a new subtask derived from it have emerged leveraged by a new set of datasets and machine learning methods
as opposed to the original text entailment task which is a binary classication task where the answer is either yes or no corresponding to entailment or non entailment the natural language inference nli subtask is intended to perform a three way classication labeling entailment pairs as entailment neutral or contradiction
nli is usually associated with deep learning methods which was abled by the introduction of large machine learning oriented datasets such as the stanford natural language inference snli corpus and the genre natural language inference multinli corpus
beneting from the large amount of training data nli systems can make use of models and techniques now widely adopted for natural language text processing among which stand out the long short term memory lstm models and the attention mechanisms integrated into deep neural networks
in the nli task the text t sentence is usually called the premise and sentence embedding is a common way of representing the premise and the pothesis
lstm models be it plain or enriched with attention weighted representations word by word matching between sentences rectional lstm or a decomposable attention model are the most common architectures
a number of variants regarding the attention anism have also been proposed including inner attention to detect the most important portions of one sentence in the pair regardless of the content of the other one self attention to model the long term dependencies in a sentence and co attention to preserve information from all the work layers
despite the dierences in the way they attend sentences and align their words these approaches build upon similar architectures using lstm or bilstm models with no or almost no feature engineering and no external resources
even though nli datasets are semantically simpler than text entailment ones still not all the knowledge needed for the inference is self contained within the training data
some approaches try to address this issue by porating external knowledge in the inference process
chen et al
do this by extracting synonymy antonymy hypernymy and co hyponymy relation between sibling words that is words having the same hypernym relations from wordnet
wang et al
goes further and besides wordnet use also conceptnet and dbpedia as knowledge sources
although nli systems show great quantitative improvement when pared with text entailment applications since all of them use very advanced models the increasing number of dierent approaches present only mental improvements among them
furthermore even though the advances introduced in the nli eld are arguably invaluable the high accuracy the approaches achieve may nevertheless be partly inuenced by bias in the ing datasets
in a study conducted by gururangan et al
in which samuel r
bowman one of the researchers responsible for the creation of both snli and multinli also participated it is shown that nli datasets contain a signicant number of annotation artifacts that can help a classier detect the correct class without ever observing the premise
the presence of such artifacts is a result of the crowdsourcing process adopted for the dataset creation because crowd workers adopt heuristics in order to ate hypotheses quickly and eciently producing certain patterns in the data
through a shallow statistical analysis of the data focusing on lexical choice and sentence length they found for example that entailed hypotheses tend to contain gender neutral references to people purpose clauses are a sign of neutral hypotheses and negation is correlated with contradiction
besides the dataset statistical analysis they also built a hypothesis only classier showing that a signicant portion of snli and multinli test sets can be correctly classied without looking at the premise
then they evaluated high performing nli models on the subset of examples on which the hypothesis only classier failed which were considered to be hard showing that the performance of these models on the hard subset is matically lower than their performance on the rest of the instances
they conclude that supervised models perform well on these datasets without tually modeling natural language inference because they leverage annotation artifacts and these artifacts inate model performance so the success of nli models to date has been overestimated
poliak et al
reinforces these conclusions implementing a similar hypothesis only classier but extending the study to other eight datasets besides snli and multinli underlining that such statistical irregularities lead models to skimp over a fundamental principle of textual entailment and by extension of nli that the truth of the hypothesis necessarily follows from the premise and then the premise must be indispensable if actual inference is to be performed
the problems evidenced by the bias in the datasets show that nli is still an open challenge but one more issue can be highlighted due to their creasingly more complex architectures nli models will invariably show poor interpretability making it even harder for users to know how decisions are reached and therefore if they are reliable or not
as deep neural network models they are not transparent and commonly do not provide post hoc explanations
regarding explanations the recently released e snli dataset an extension of snli with human annotated natural language nations for each premise hypothesis pair can leverage the developments in this area
so far a single approach has used this dataset and only for token level explanation that is only the tokens in the premise and in the pothesis that are relevant for the inference are presented which is basically the output of the attention mechanism
fully human readable explanations made up of full concise and connected natural language sentences are yet to be addressed in nli

syntactic semantic composite text entailment this work builds upon a previous version of the approach presented in extending the entailment framework to consider context information in order to better capture the semantics of the sentences as a whole and also performing more extensive experiments therefore introducing novel and hanced results
the central point is the notion that text entailment can involve syntactic or semantic phenomena and each of these phenomena egories requires specic approaches to be solved
in the rst case an analysis of the syntactic structure of the sentences may be enough while in the second it is necessary to identify the semantic relationship holding between the text and the hypothesis
on the other hand looking for semantic relationships where only a syntactic variation occurs or comparing syntactic structures of very syntactically dierent sentences can be highly counterproductive hence the importance of choosing the suitable method rst and foremost
to pick the best approach we need to answer the following question can there be a semantic relationship between t and h we assume that a semantic relationship must hold between two entities and both referring to a third entity which we call the referent or
the routing mechanism that will check these conditions relies on the notion of overlap between the text and the hypothesis
the overlap o is computed over the bag of words representation of t and h denoted by where ti are tokens in t and n is the size of t and t


tn h


hm where hi are tokens in h and m is the size of h
therefore o t h


wk where k is the size of o
formalizing the aforementioned conditions for the existence of a semantic relationship between t and h we have that c t h a d e after computing o three scenarios may occur total overlap where all the tokens of h are contained in t or less monly vice versa that is k m or n
in this case the condition is not satised r o partial overlap where some but not all of the of tokens of t are contained in h so n and m
both conditions and e are met in this scenario null overlap that is no tokens of t are contained in h so o and
since o is empty the condition e ca nt be satised
given that we can look for a semantic relationship between t and h solely when both conditions and e are met the entailment pair will be solved semantically only when a partial overlap occurs
otherwise the pair will be solved syntactically because if there is a total overlap there are no entities and such that for which a semantic relationship may hold and in the case of a null overlap there is no referent r so even if there are some potential candidates and that could be semantically related it is more likely although not certain that they are referring to completely dierent entities
for solving entailments syntactically we use the tree edit distance ted model and for dealing with entailments involving semantic ena we employ the distributional graph navigation dgn model
an additional context analysis module feeds both models with extra tion extracted from the entailment pair
following a preprocessing stage that generates t and h the router computes o and sends the entailment pair either to the ted or to the dgn model according to the aforementioned conditions
after the entailment is solved by the suitable model ing yes or no as the output an interpretability module uses the evidence produced by the entailment algorithm to generate a natural language cation explaining the algorithm decision
the general architecture of the xte explainable text entailment approach is shown in figure
figure general architecture of xte explainable text entailment

tree edit distance the tree edit distance algorithm computes the minimal cost sequence of operations namely insertion deletion and replacement of nodes necessary to transform the tree representation of t into the tree that represents h
we use the all paths tree edit distance apted which improves over the classical algorithm of zhang and shasha by being tree shape pendent
the edit distance is computed over the syntactic dependency trees of t and h generated by the stanford dependency parser
this parser generates a dependency graph but it can be easily converted to an acyclic tree where nodes with more than one incoming edge are expanded only at the rst time they are referenced and represented as childless nodes in sequent references similar to the pretty print string representation provided by the parser for the original graph
we represent dependencies between terms which are labeled edges in the original graph as intermediary nodes between the two nodes they link that is the two dependency s arguments
figure shows the graph generated by the dependency parser for the text sentence t in the example in section the badger is burrowing a hole and the resulting dependency tree which will be sent as input to the ted algorithm
figure dependency graph left and the resulting dependency tree right which is sent to the tree edit distance algorithm given that we represent dependencies as nodes in the tree our ted model penalizes node replacement more than insertion and deletion because replacing a node between nodes a and b in t by a node between the same nodes a and b in h means changing the dependency between them or ing one of the arguments of a dependency if the replacement comes before or after a sequence of two nodes a and b which are identical in t and h
this is done by a weighted cost model with higher weight for replacements than for insertions and deletions and by the calculation of the relative edit distance reldist which is the edit distance dist relative to the dierence dif between the sizes of the two trees given by reldist dist
if the two trees are roughly the same size but many edit operations are performed they are probably replacements which means many dependencies arguments are being changed so dif is low and reldist increases
on the other hand if approximately the same number of operations are performed for trees ing dierent sizes usually t larger than h there will be more insertions deletions
in this case is higher and reldist decreases which favors scenarios where the tree for h is a subtree of the tree for t and fore insertions deletions will occur more often and aect the validity of the entailment less than replacements
the reldist is then compared against a threshold t and the pair is classied as an entailment if reldist t and as a non entailment otherwise


distributional graph navigation the distributional graph navigation model is based on two main pillars the use of a knowledge graph automatically extracted from natural language lexical denitions as a world knowledge base and a navigation mechanism based on distributional semantics to traverse this graph and nd paths tween the text and the hypothesis
a path between t and h explains the semantic relationships holding between them conrming the entailment by showing that a relationship indeed exists while also providing evidence that it is true by enabling the generation of an explanation from its nodes tents



the denition knowledge graph generating natural language justications is an important feature for increasing a system s interpretability and the generation of such tions can be leveraged by the use of external sources of world knowledge
dictionary style denitions are a rich source of such knowledge and ferent from formal structured resources like ontologies they are independent and largely available
many nlp systems including text tailment systems already explore lexicons among which the most used is wordnet but they usually look only at its structured tion that is links such as synonyms hypernyms
the natural language denitions are left aside although they contain the largest amount of relevant information about an entity its type essential attributes primary functions and often many non essential but very informative attributes as well
we rely on the knowledge provided by lexical dictionary denitions for looking for relationships between the text and the hypothesis whenever the entailment is solved semantically
to make use of natural language denitions in our approach we structure them converting a whole dictionary into a knowledge graph following the conceptual representation model proposed by silva et al

in this model the denitions are split into entity centered semantic roles
dierently from the commonly used event centered semantic roles which dene the semantic relations holding among a predicate the main verb in a clause and its associated participants and properties denition s semantic roles express the part played by an expression in a denition showing how it relates to the deniendum that is the entity being dened
table lists the semantic roles for lexical denitions present in the model
this model allows a structured semantic representation of natural guage denitions and enables the selection of the portions of information that are relevant for a given reasoning task
for building the denition knowledge graph dkg we followed the methodology introduced in which comprises the following steps denitions sample selection we selected a random sample of noun and verb denitions to be annotated so we could train a supervised machine learning model to classify the data
we used the glosses extracted from the wordnet database from which we randomly selected denitions being noun denitions and verb denitions the verb database size in wordnet is around of the noun database size
automatic pre annotation using the syntactic patterns identied by tistical analysis described by silva et al
we implemented a rule based heuristic to automatically pre annotate the set of denitions
this heuristic links each phrasal node in a sentence s syntactic tree to the mantic role most often associated with it
for example the supertype for a noun denition is usually the innermost and leftmost noun phrase np that contains at least one noun nn a dierentia event is usually either a subordinate clause sbar or a verb phrase vp and so on
the syntactic parse trees for each denition were generated and queried with the aid of the stanford parser
data curation after the automatic pre annotation the denitions were role supertype dierentia quality dierentia event event location event time origin location quality modier purpose associated fact accessory determiner accessory quality role particle description the immediate or ancestral entity s superclass a quality that distinguishes the entity from the others under the same supertype an event action state or process in which the tity participates and that is mandatory to guish it from the others under the same supertype the location of a dierentia event the time in which a dierentia event happens the entity s location of origin degree frequency or manner modiers that strain a dierentia quality the main goal of the entity s existence or rence a fact whose occurrence is was linked to the tity s existence or occurrence a determiner expression that does nt constrain the supertype dierentia scope a quality that is not essential to characterize the entity a particle such as a phrasal verb complement contiguous to the other role components table semantic roles for dictionary denitions manually curated so misclassications were xed and segments missing a role were assigned the appropriate one
the manual data curation ensured that the whole denition was consistently classied that is that every segment was associated with the most suitable semantic role label
classier training the curated dataset was then used to train a recurrent neural network rnn machine learning model designed for sequence ing
we used the rnn implementation provided by mesnil et al
which reports state of the art results for the slot lling task
the dataset was split into training validation and test sets
the best accuracy reached during training was of

database classication the trained classier was then used to label the whole set of denitions from dierent lexical resources as detailed in section

each resource gave origin to a dierent dkg which allowed us to test our approach with dierent congurations and compare the resources among them see section


data post processing we then passed the classied data through a processing phase aimed at xing classication that missed the supertype role
this role is a mandatory component in a well formed denition and as detailed in the next step the rdf model is structured around it
missing supertypes were identied according to the same syntactic rules used for pre annotation keeping the remaining classication unchanged
rdf conversion nally the labeled denitions were serialized in rdf mat
in the rdf graph the entity being dened is a node the entity node and each semantic role in its denition is another role the role nodes
the entity node is linked to its supertype role node which is in turn linked to all the other role nodes
consider for example the denition from wordnet for the concept planet any of the nine large celestial bodies in the solar system that revolve around the sun and shine by reected light
the output of the labeling stage is depicted in figure and the resulting rdf subgraph is shown in figure
figure the labeled denition for the concept planet figure the graph representation of a lexical denition
the node labeled planet is an entity node the entity being dened and all the other ones are role nodes when using this graph to recognize semantic text entailments we assume that whenever we can nd a path between two entities and t and h see condition d in section the entailment is true and this path is used to justify the entailment showing explicitly what the relationship between the entities is



the distributional navigation algorithm distributional semantic models dsms are grounded in the tional hypothesis which states that words that occur in similar contexts tend to have similar meanings
dsms allow the approximation of a word meaning representing it as a vector summarizing its pattern of co occurrence in large text corpora
dsms can be used to compute the semantic similarity relatedness sure between words
this computation is used as a heuristic to navigate in a graph knowledge base in the approach proposed by freitas et al
where they dene the distributional navigation algorithm dna which sponds to a selective reasoning process in the knowledge graph
given a pair of terms namely a source and a target and a threshold the dna nds all paths from source to target with length l formed by concepts semantically related to target wrt
in the text entailment context and more specically for semantic ments the source and target are the entities t and h which we assume have some kind of semantic relationship between them
a path in the denition knowledge graph linking these entities then explains what this relationship is conrming the entailment or rejecting it in case no path is found
we implement the dna as a search algorithm exploring rst the paths whose next node to be visited has the highest semantic similarity value wrt the target
given a node in the dkg starting from the source s the algorithm retrieves all its neighbors


xn and computes the similarity relatedness target keeping only the nodes for which sr in the set of nodes to be visited next
each of these nodes generates a new path and for each path the search goes on until the next node to be visited is equal to the target or a synonym of it or until the maximum path length is reached
if no path reaches the target before the maximum number of paths is reached the search stops
the distributional graph navigation mechanism is schematized in figure
the dgn algorithm which takes as inputs a denition knowledge graph g a source word s a target word t a threshold a maximum path length l and a maximum number of paths m and outputs the set p of paths from s to t is listed in algorithm
figure the distributional navigation algorithm
gray nodes for which t make up valid paths between the source node s and the target node t
the path s t is the shortest one depending on the lexical resource from which the denitions are tracted entity nodes in a dkg can be identied by a single word or phrase or by a synset that is a set of synonym words or phrases
starting from the source word s the dgn retrieves all entity nodes identied by s or having s as one of the words in its identifying synset line
then it retrieves all the neighbors of each entity node that is the role nodes that make up its denition line and keeps only the best ones line
the next nodes to be visited are given by words present in a role node which we call the head words
for getting the head words we rst remove all stop words and words with low inverse document frequency idf which are words that occur too frequently for example verbs such as get put cause or make and can be reached from almost any node in the graph leading to diverting paths
idf is calculated using as the the same linguistic resource that gave origin to the knowledge graph being explored by the algorithm
after removing the irrelevant words we compute the semantic adds the newp ath to the stack pulls the path at the top of the stack end if odes ei if nextn ode then odes entityn odes for all g do end for for all rolen odes do if t then end for rolen odes bestroles for all entityn odes do path nextn ode path
lastn ode while nextn ode t and path
length do p stack newp ath newp ath while stack and p
size m do algorithm distributional graph navigation algorithm procedure s t l m end procedure end for nextn ode nextn ode if nextn ode t or ode t then end for bestroles nextn odes for all bi bestroles do end for nextn odes odes for xi nextn odes i n do newp ath path ath newp ath odes end while return p end if end while ri path end if similarity sr between each remaining word and the target word t sort the results and keep only the top k words
the highest scoring head word will be the next node to be visited that is the next entity node to be searched and all the other head words are added to a copy of the current path generating a new path which will be pushed to the stack to be explored later
as mentioned earlier the search stops successfully when the next node to be visited is equal to the target but also when it is one of the target s synonyms line
this is done with the aid of a synonym table built from synonym lists gathered across all the tested lexical resources see section
and other online
word sense disambiguation comes as a natural consequence of the butional navigation mechanism while choosing the next nodes to be visited in the graph by looking for the word phrases that are more semantically lated to the target t the algorithm naturally selects the correct or at least the closest word senses since unrelated word meanings will have lower ilarity scores wrt the target and the paths containing them will be excluded by the algorithm
according to freitas et al
the worst case time complexity of the rithm implemented as a search is where is the branching factor and l is the depth limit
they show that the algorithm selectivity ensures that the number of paths does not grow exponentially even when the depth limit increases
in our implementation the maximum number of paths and the maximum path length depth limit were obtained empirically in order to optimize the search



recognizing and justifying entailments through the dgn for applying the distributional graph navigation algorithm section
over denition knowledge graphs section

for recognizing and justifying semantic entailments we rst need to identify the source target pairs that is the pairs of entities ei which will be sent as input to the dgn
using the information from the sets t equation a h equation b and o equation c we compute the sets t and h where t t o

also using dsms we then compute the semantic similarity measures between t and h as the cartesian product p h h o p t h g h the results are then sorted and the highest scoring pairs are selected making up the set p
each pair ei ej p is sent to the dgn algorithm which returns a set of paths between ei and ej
from the set of all paths returned for all pairs we choose the shortest one which is the one that oers the shortest distance between a source and a target and therefore shows that their meanings are more closely related
if no path is found at all then the entailment is rejected
a path in the dkg is composed by a sequence of entity nodes and the relevant role nodes linked to them see section


this path is then sent to the interpretability module which formats them into a human readable justication which shows what the relationship between the source and therefore the text t and the target the hypothesis h is making clear what was the reasoning followed by the algorithm
the procedure for recognizing an entailment through the dgn is listed in algorithm for readability further parameters for the dgn procedure were omitted in line
to illustrate the steps followed by the dgn while traversing a knowledge graph to solve an entailment consider the entailment pair
from the dataset
t iran is a signatory to the chemical weapons convention

h the chemical weapons convention is an agreement
in this example the best source target pair is signatory and agreement and the referent chemical weapons convention since both and refer to this concept
the best path between the source and the target in a dkg as well as all the semantic similarity measures between each node retrieved by the algorithm and the target are shown in figure
in this path nodes are linked either by the has supertype property which
cs
utexas
edu users pclark bpi test t h o t h t t o h h o p t h p for all ei ej p do algorithm semantic entailment recognition through the dgn algorithm procedure h end procedure entailment true bestp ath aths justif ication ath end if return entailment justif ication end for if allp aths then aths dgn ei ej entailment alse else denes the kind of an entity or by the has di event or has di qual ties which introduce qualiers for the entities they describe
in the second case the supertype node is also included in the path because dierentia role nodes as well as almost all the other role nodes do nt make much sense without the supertype they refer to
since the justication takes into count the content of the nodes and the relationships between them that is the role names see section

the nal human readable explanation generated by the algorithm from this sequence of nodes is a signatory is someone who signs and is bound by a document a document is an account of ownership or obligation an obligation is a kind of agreement the natural language justications are not a formal logical proof of the entailment but rather explanations based on commonsense intended to be figure a path indicated by the gray nodes between source node signatory and target node agreement in a dkg
full lines represent actual edges in the graph while dashed lines represent the algorithm s internal operations in this case the extraction of head words for multi word expression nodes
numbers show the semantic relatedness between each node and the target close to what a human being employing their accumulated world knowledge would use to justify the link between concepts


context analysis the goal of the context analysis module is to provide both the tree edit distance and the distributional graph navigation models with information they ca nt easily grasp and that if missed can lead to erroneous conclusions
in the tree edit distance case this can happen when minimal slight cations completely changes the meaning of a sentence
such modications may yield only a small edit distance between t and h resulting in a wrong entailment classication for the pair
for semantic entailments the extraction of further contextual tion is even more critical since the distributional graph navigation model though looking for common referents primarily considers pairs of terms in isolation and not the sentences as a whole
this means that even when there are two entities t and h with a high semantic relatedness score there may also be at another point in the sentences contradictory or inconsistent information which invalidates the entailment but that the dgn wo nt catch
the context analysis module receives as input the tokenized output from the preprocessing stage discards the overlapping entities in the set o and using syntactic and semantic features analyzes the remaining words phrases in order to look for the following simple negation t is a simple negation of h
example t a sea turtle is not hunting for sh h a sea turtle is hunting for sh a no negation adverbs will mostly be considered as stop words so entailment pairs where these are the only divergent words will be sent to the tree edit distance model
detecting the negation allows the model to classify the pair as a non entailment even if the nal edit distance is well below the threshold
opposition t contains a term which is an antonym of a term in h
ple t a woman is taking o eyeshadow h a woman is putting make up on a no detecting opposition is an important step when entailments are solved through the distributional navigation model
in the above example the dgn would detect eyeshadow in t and make up in h as a candidate source target pair and most likely nd a path in a dkg linking both entities since eyeshadow is a kind of make up but the presence of antonym terms take o in t and put in h prevents the pair from being ed as an entailment
opposition detection is performed with the aid of an antonym table built from antonym lists extracted from all the tested lexical resources see section
and other online
inverse specialization h specializes some information in t
since text entailment is a directional relationship from t to h specializations are valid only in this direction not the other way around
example from the sick dataset
cimec
unitn
it composes sick
html





t a person is rinsing a steak with water h a man is rinsing a large piece of meat a no in this example h specializes t since person is more general than man
as much as opposition detection inverse specialization detection plays an important part in preventing the dgn from misclassifying the tailment pair in the above example by nd a relationship between steak and meat
in the correct direction that is from t to h specializations can be easily detected also by the dgn model since they are a kind of semantic relationship for detecting only inverse specializations we use the hypernym links from wordnet
unsatisable clauses h has more information than what can be satised by t
example t a large group of cheerleaders is walking in a parade h the cheerleaders are parading and wearing black pink and white uniforms a no coordinated or subordinated clauses in h can be unsatisable if t has fewer clauses that h
in the above example t is composed by a single clause while h has two coordinated clauses and although the rst h s clause can be fully entailed by t the second one ca nt be satised
mismatching number of clauses between t and h is detected through the analysis of the sentences syntactic parse trees
any of the above described phenomena is considered enough to reject the entailment so the ted and dgn models always take into account the output of the context analysis module and in case their conclusions diverge the decision made on the basis of the contextual information prevails

evaluation to evaluate our proposed composite syntactic semantic text entailment approach we tested the xte on several datasets experimenting with varying knowledge bases built from dierent lexical resources
the details are given next


datasets we tested xte on four datasets dataset the dataset from the third rte is one of the most traditional and popular text entailment datasets
it contains t h pairs split into dev pairs and test pairs sets and is balanced with half positive and half negative examples
sick dataset sentences involving compositional knowledge is a dataset aimed at the evaluation of compositional distributional semantic models which besides the semantic relatedness between sentences also cludes annotations about the entailment relation for the sentence pairs
it is composed of pairs split into train pairs trial pairs and test pairs
instead of the binary entailment cation there are three dierent relations entailment contradiction and neutral
for coherence with the other datasets we considered both the tradiction and neutral labels as non entailment leading to positive and negative examples the original classication is also unbalanced around of the pairs have the label neutral
bpi the boeing princeton isi textual entailment test suite was developed specically to look at entailment problems requiring world edge being syntactically simpler than rte datasets but more challenging from the semantic viewpoint
it is composed of pairs positive and negative
ghs dataset the guardian headlines sample is a subset of the guardian headlines a set of entailment pairs automatically extracted from the guardian newspaper but not validated
the ghs is a random sample of pairs which have been manually curated leading to a balanced set of positive and negative examples
it also requires a reasonable amount of world knowledge and is the only dataset fully composed of world data without articially assembled hypotheses in positive examples

org project third recognising textual entailment
cimec
unitn
it composes sick
html
cs
utexas
edu users pclark bpi test

gl t is the rst sentence of a story and h is its headline and in negative examples t and h are two random sentences from the same story


knowledge bases to evaluate the impact of dierent lexical resources in the entailment sults especially in the justications generated we extracted the denitions from four dictionaries wordnet the webster s unabridged and the set of denitions extracted from wikipedia pages provided by faralli and navigli
each of the four resulting dkgs diers from the others in some way the webster is an older conventional dictionary dating from
wordnet and wiktionary are modern on line lexicons but the former is developed by professional lexicographers while the latter is built collaboratively by lay users
last wikipedia is also built collaboratively but is an encyclopedic rather than lexical resource
the original webster s dictionary text le was processed so besides the denitions the part of speech and list of synonyms when available for each word could also be
all the four sets of denitions were submitted to a pre processing stage intended at ltering potential invalid denitions including but not restricted to verb denitions not beginning with a verb or an adverbial phrase followed by a verb noun denitions beginning with verbs or prepositions
for the wikipedia dataset denitions for named entities were also excluded with the aid of the stanford named entity recognizer ner so the nal content could be closer to a regular dictionary
due to the natural limitations of the ner many named entity denitions remained in the nal set but this additional lter helped to set a manageable size for the nal graph without leaving out potentially relevant information
finally all the ltered sets of denitions were labeled and converted to an rdf graph as described in section


table shows the dimensions of each of the resulting graphs


computing the thresholds two of the most important parameters of the proposed approach are the tree edit distance model s threshold t section
and the distributional graph navigation algorithm s semantic relatedness threshold section


gutenberg
org
wiktionary
in json format available at
com ssvivian webstersdictionary resource noun denitions verb denitions total wordnet webster s wiktionary wikipedia table final dimensions of the denition knowledge graphs used in the interpretable composite text entailment approach ted threshold t is computed previously through a training procedure which performs a sequential search to look for the distance that better separates positive examples from the negative ones and is aimed at maximizing the algorithm s accuracy in our case the f measure score
for training the model we combined the training portions of the and sick datasets
this combined dataset herein called train dataset compensates for the lack of training data in the other two datasets while still being resentative of their syntactic characteristics the is closer in format to the ghs both having very long text sentences and usually short hypothesis while the sick data is more similar to the bpi entries with both datasets having short to medium sized text and hypothesis sentences and usually not a big dierence in size between the two sentences composing an ment pair
after the training is performed over the dataset the learned threshold t is used to compute the syntactic entailments for all the four tested datasets for the and sick datasets the evaluation is performed on their test portions
the dgn threshold is computed dynamically so the algorithm can always retrieve the highest scoring entries from a list of candidates
while navigating the knowledge graph the dgn always retrieves a set of nodes and computes the semantic relatedness sr between each node and the target see section
which results in a ranked list of scores
over this list we perform a semantic dierential analysis adapting the method proposed in to identify score gaps which discriminate between highly semantically related nodes and non related ones
given a list of ranked nodes is the score for the node with the maximum relatedness value sk is the score for the ranked node and sk is the semantic dierential between two adjacent ranked nodes that is sk i the gap in the list occurring between sn and is given by smax the maximum semantic dierential
sn and dene the top and bottom relatedness values of smax denoted by
figure illustrates the semantic dierential model
n and s figure the semantic dierential model
smax denes a gap in a ranked list of scores to determine at each step of the graph navigation we compute smax over the current ranked list of nodes and select the bottom value as the semantic threshold therefore j we choose the bottom value and not the top which is the one ately before the gap in order to keep at least one moderately related node in the list
since semantic similarity scores depend on the dsm model used to compute them and by extension on the corpus from where the dsm was learned average immediately after the gap scores can have varying meanings when considered in dierent contexts
by including the bottom latedness value in the list we ensure we wo nt miss potential relevant nodes
if such nodes prove to be irrelevant the dgn manages to abort their paths at subsequent steps eliminating any eventual noise


baselines to show the improvements provided by our composition of entailment methods we compare our results with three single technique approaches a purely syntactic algorithm a syntactic approach that employs linguistic resources from where shallow semantic information is extracted and a purely semantic algorithm
the edit distance is the state of the art implementation of the tree edit distance algorithm for recognizing textual entailment and only ers the syntactic structures of t and h given by their dependency trees
the maximum entropy classier also uses the syntactic dependency trees as features in a classier which also employs lexical semantic features from wordnet and verbocean we use the en conguration
both implementations are provided by the text entailment framework excitement open platform eop and were also trained on the combined training dataset section

as the only baseline we use the graph navigation algorithm using the wordnet denition graph described in


results table shows the precision recall and f measure rounded to two decimal places obtained by each of the xte congurations which are given by the dkg used by the distributional graph navigation component that is the knowledge bases derived from wordnet wn webster s dictionary wbt wiktionary wkt and wikipedia wkp as well as the baselines results
ghs bpi sick pr re pr re pr re pr re



















































































ed mec gn xte wn xte wbt xte wkt xte wkp table evaluation results
the upper part shows the baselines and at the bottom are the proposed composite entailment approach s results
ed editdistance mec maxentclassier gn graphnavigation xte explainabletextentailment the rst thing that can be noticed is how the results vary across datasets for those baselines that rely on training data that is the editdistance and the maxentclassier algorithms
both approaches present homogeneous sults for the rte and sick datasets for which training data is available but their accuracy falls signicantly for the bpi and ghs datasets
while xte presents consistent results across all datasets because we do nt rely exclusively on the syntactic information learned at the training phase but rather balance it with the semantic knowledge extracted from the dkgs
since these knowledge graphs are commonsense and independent resources they work homogeneously for any unseen data making our approach much less training data dependent
moreover for the bpi and ghs datasets which require more world knowledge both algorithms show low recall and are surpassed by xte adding to the importance of combining and balancing syntactic and semantic information while solving the entailments
when compared with the semantic only graphnavigation approach xte also presents much better results especially for the and sick datasets which do nt have a heavy focus on world knowledge based entailments
sides not dealing well with entailments that do nt show any semantic tionship that is purely syntactic entailments the graphnavigation also uses a syntax based heuristic to dene the source target input pairs and the head words for multi word expression graph nodes
this heuristic uses part of speech tags to nd the main components subject verb objects in a sentence which may not work well for long complex sentences as is the case in the and ghs datasets
by using a semantic similarity based heuristic we can retrieve better source target pairs and a higher number of relevant head words what leads to much better recall and overall f measure



justications we analyzed the justications generated for the positive entailments solved by the distributional graph navigation model to assess their correctness and consistency
this evaluation was intended to assess the explanations from a functional point of view that is to determine if they were accomplishing the task of establishing the right relationship between the right terms in t and h
a deeper psychological evaluation to assess trustworthiness was out of the scope of this evaluation and is included as future work
that means justications were evaluated to be right or wrong on a high level but not good or bad according to a more subjective user s judgment
evaluators were asked to rst point the entities establishing a tion between t and h
then they should judge if the justication met two requirements it linked the previously identied entities and the lationship it describes is the same one intended by the context given by t and h according to the human judgment
they then classied justications into correct or incorrect
correct tications meet both conditions establishing the right relationship between the relevant entities t and h presenting the pertinent information about it and making the reasoning clear

t a council worker cleans up after tuesday s violence in budapest

h there was damage in budapest

a yes entailment yes justication a violence is a state resulting in injuries and destruction
a destruction is a termination of something by causing so much damage to it that it can not be repaired or no longer exists t vasquez rocks natural area park is a northern los angeles county park acquired by la county government in the
h the vasquez rocks natural area park is a property of the la county government
a yes entailment yes justication to acquire is to come into the possession a possession is an act of having and controlling property incorrect justications do not meet one or both aforementioned tions establishing a relationship between the wrong pair of entities that is entities that although being semantically related do not establish a logical link between t and h or being too vague linking the correct pair of entities t and h but giving only supercial and insucient information about their semantic relationship
t joining pinkerton at the chamber of commerce is lindsey beverly who will be the new executive assistant
from the bpi and datasets respectively
from the dataset
h pinkerton works with beverly
a yes entailment yes justication an assistant is a person who contributes to the fulllment of a need or therance of an eort or purpose eort is synonym of work t leloir was promptly given the premio de la sociedad cientca gentina one of few to receive such a prize in a country in which he was a foreigner
h leloir won the premio de la sociedad cientca argentina
a yes entailment yes justication to receive is a way of to take to take is synonym of to win in the rst example even though the semantic relationship between sistant and work may seem consistent the expressions that establish the entailment relation are join and work with
in the second case although the justication links the correct entities establishing the entailment that is receive and win the explanation made through the verb take with no complements sounds vague and not informative enough
denitions for verbs tend to be less detailed than those for nouns and many times expressed in terms of very broad supertypes like take in the second example ing justications generated from paths containing verb entity nodes to be more prone to vagueness
the distribution of correct and incorrect justications is given in table
the evaluation was performed over the results obtained with the wordnet graph which is the knowledge base that yields the overall best results across datasets
a more detailed comparison of all the tested dkgs is given in the section


as can be seen in table the distribution of correct and incorrect tications varies depending on the dataset with sick and bpi showing the best results
in common these two datasets have relatively short text and hypothesis sentences which favors the correct identication of source target dataset correct justications sick bpi ghs



incorrect justications



table distribution of correct and incorrect justications word pairs
on the other hand the and ghs datasets have short hypothesis sentences but very long text sentences and the larger the tence the bigger the number of possible source target pairs so the likelihood of nding a relationship that does nt necessarily leads to the entailment creases
the ghs is a particularly challenging dataset because the text t corresponds to the the rst sentence of a news story usually expanding the idea highly condensed in the headline that is the hypothesis h
the two sentences are very semantically related as a whole so many concepts in t will have some strong relationship with sometimes the same concepts in h leading to many explanations where although the relationship by itself may be right it is not the most suitable answer for the entailment decision as ready shown in the example above hence the higher number of incorrect justications
although there is still much room for improvement our approach for generating natural language justications proved to be a viable solution pecially in view of the fact that it employs an unsupervised technique and relies on already existing knowledge sources yielding reasonable results out the costs and training data dependency of supervised methods



comparing denition knowledge graphs besides the improvements in the quantitative results the interpretable characteristic of xte represents an important contribution providing like explanations for the entailment decisions whenever a more complex mantic relationship is involved translates into a concrete gain for the nal user who can understand and judge the system s rationale
the tions though depend heavily on the graph knowledge base employed in the entailment recognition
overall wordnet webster s and wiktionary graphs deliver close results for the and sick datasets but wordnet stands out for the more world knowledge demanding bpi and ghs datasets as can be seen in table
the impact of each dkg can be better measured by the recall obtained when they are queried the more useful information the graph contains the more paths meaning semantic relationships between source and target words can be found and consequently more entailments can be recognized
again wordnet webster s and wiktionary graphs show comparable recalls for the and sick datasets but wordnet presents a much better recall for bpi and ghs
the wikipedia graph on the other hand shows lower recall for all of the datasets especially for the more knowledge oriented bpi and ghs despite being the larger knowledge base
this happens because wikipedia besides not dening verbs privileges the denitions of people places arts and entertainment artifacts lms books songs
and other entities expressed by proper nouns
in fact wikipedia lacks denitions for many concepts present in the datasets for which relationships are sought violence and damage signatory and agreement decontamination and contaminants or bet and gamble to name a few
this shows that for the entailment task the content type is more relevant than the amount of information in the graph
the wordnet graph for example corresponds to roughly only of the wikipedia graph but contains far more common nouns denoting basic language concepts better matching the task requirements
the wiktionary graph has a good coverage of common nouns ble to wordnet but in some cases the completeness of its denitions may represent an issue if not enough information is contained in the denition that is the denition of an entity fails to mention other entities it is tially related to paths will start but wo nt reach the target
built by expert lexicographers the wordnet denitions tend to follow some patterns and are more prone to cover essential attributes
on the other hand in a orative environment despite the larger volume of information that can be generated high quality standards ca nt always be ensured
this is the son why in spite of its much larger dimensions the wiktionary graph ca nt always surpass the wordnet one yielding lower recall for the bpi and ghs datasets
again the coverage and regularity of the knowledge base contents prove to be more important than its size
as for the webster s graph what was observed as an issue is the oldness of its source dating from the webster s unabridged dictionary naturally also covers all the most common language concepts but besides sometimes registering obsolete forms like camera obscura for era lacks many modern concepts or concepts that were not of widespread use back then such as wmd weapons of mass destruction website terrorist act or recall in the sense of defective products callback
such modern concepts are frequent in press content so datasets like the ghs totally generated from newspaper content and the bpi also derived from news content may pose a challenge to this knowledge graph which is conrmed by the lower recall compared to wordnet returned for these two datasets when the webster s graph is used
the justications generated by each of the graphs are comparable in quality with wordnet and wikipedia graphs oering slightly more detailed explanations
an example from the ghs dataset explained by the wordnet graph t glaxosmithkline has been forced to set aside m to settle trust cases in the us over its anti inammatory drug relafen
h glaxo hit by m us court blow a yes entailment yes justication a case is a term for any proceeding in a court of law whereby an individual seeks a legal remedy court of law is synonym of court from the dataset explained by the webster s graph t mr
gotti who is already serving nine years on extortion charges was sentenced to an additional years by judge richard d
casey of federal district court
h gotti was accused of extortion
a yes entailment yes justication a charge is a kind of accusation an accusation is an act of accusing or charging with a crime or with a lighter oense from the sick dataset explained by the wiktionary graph t a man is exercising h a man is doing physical activity a yes entailment yes justication to exercise is to perform physical activity for health or training from the bpi dataset explained by the wikipedia graph
t a union pacic freight train hit ve people

h the train was moving along a railroad track

a yes entailment yes justication a freight train is a group of freight cars or goods wagons hauled by one or more locomotives on a railway


railway is synonym of railroad track in summary the wordnet graph presents the best recall across all datasets due to its good term coverage and denitions completeness
the wiktionary and webster s graphs also show good recall but their performance can be weakened by the incompleteness resulting from the amateur nature of the denitions creation process in the wiktionary case or by the lack of ern terms which are frequent in contemporary language in the webster s instance
the wikipedia graph due to its encyclopedic nature has well structed and complete denitions but lacks many basic concepts yielding the lowest recall regardless of the dataset


limitations despite outperforming state of the art text entailment algorithms the proposed approach still does not present very high accuracy especially if pared with accuracy driven modern nli models
in fact solving demanding semantic entailments in an interpretable way is a complex task with still much room for new developments
analyzing our approach results to understand the cause of false positive and false negatives we identied some limitations that can be better addressed in future work
wrong entailment decisions can be caused by both internal and nal factors
internal factors indicate the current limitations of the proposed approach while external factors refer to third party resources for which no ne tuning is possible
undetected context information and wrong tic relationship assumption are the most common internal sources of errors
external sources include syntactic parser error and knowledge base pleteness
knowledge base related issues aect not only the system accuracy but also the quality of the resulting justications
we observed that the errors caused by internal factors occur much more often when there is a neutral relationship between t and h that is there is no entailment but also no contradiction
in such cases more obvious context information is not always available indicating that our approach needs to adopt further analyses and resources to better deal with these scenarios
regarding external factors improving knowledge base coverage could also bring considerable gains both quantitatively and qualitatively

conclusion recognizing textual entailment is an important task whose outputs feeds many other nlp applications
it can involve a wide range of dierent guistic and semantic phenomena and identifying such phenomena and using the most suitable techniques for each of them is key to better accuracy
we presented xte an interpretable composite approach for recognizing text entailment that employing a routing mechanism that analyzes the overlap between the text and hypothesis decides whether entailment pairs should be dealt with syntactically or semantically
for those pairs predominantly ing structural that is syntactic dierences we use a relative tree edit distance model over dependency parse trees and for pairs where a semantic relationship exists we employ a distributional graph navigation model over knowledge bases composed of structured dictionary denitions
whenever the entailment is solved semantically the paths found in the graph kbs by the dgn model are used to render the entailment pretable providing natural language human like justications for the tailment decision
by explaining the system s reasoning steps we make it possible to interpret and understand its underlying inference model taking the entailment decision out of the numerical score black box
we built denition knowledge graphs from four dierent lexical resources wordnet the webster s unabridged dictionary wiktionary and wikipedia and assessed how each of them impacted our approach results and enced its interpretability
given that text entailment deals with language variability we observed that knowledge graphs covering the most basic eryday language concepts yield the best results so regular dictionaries such as wordnet webster s and wiktionary are more useful than encyclopedic kbs like wikipedia for this task
we also found that denitions created by lexicographers under a controlled environment tend to be more complete and consequently provide better recall and somewhat more detailed justications than those created in collaborative environments by lay users
furthermore contemporary resources can show some advantage over older dictionaries for containing modern terms frequently occurring in the present day language but absent from ancient lexicons like webster s dictionary
our interpretable composite approach outperforms entailment algorithms that employ a single technique be it syntactic or semantic to tackle all types of entailments and is less dependent on training data since it bines learned parameters with independent external commonsense edge which works well regardless of the dataset
by testing and comparing several graph knowledge bases we show that the use of external world edge not only improve quantitative results but is also a valuable feature for increasing intelligent systems interpretability
recent advancements such as the new nli techniques which presents good results but are mostly driven could benet from these resources to also explain themselves staying in line with explainable ai requirements
as future work especially to overcome the current limitations we believe that the system performance can be improved by the exploration of more advanced graph analysis techniques to enable the aggregation of multiple knowledge bases for better information coverage
the use of further resources and features for capturing more complex or subtler context information can also improve the overall accuracy
finally a complementary qualitative uation of the justications for assessing their trustworthiness from the user s perspective through a more sophisticated psychological study can contribute to reinforcing the usefulness of the system s explainable dimension
references i
dagan d
roth m
sammons f
m
zanzotto recognizing textual entailment models and applications synthesis lectures on human language technologies
i
dagan o
glickman b
magnini the pascal recognising textual entailment challenge in machine learning challenges evaluating dictive uncertainty visual object classication and recognising textual entailment springer pp

d
gunning explainable articial intelligence xai defense advanced research projects agency darpa
s
ghuge a
bhattacharya survey in textual entailment center for indian language technology
o
glickman i
dagan a probabilistic setting and lexical cooccurrence model for textual entailment in proceedings of the acl workshop on empirical modeling of semantic equivalence and entailment tion for computational linguistics pp

d
perez e
alfonseca application of the bleu algorithm for recognising in proceedings of the first challenge workshop textual entailments recognising textual entailment pp

e
newman n
stokes j
dunnion j
carthy ucd iirg approach to the textual entailment challenge in proceedings of the pascal challenges workshop on recognising textual entailment pp

b
maccartney m
galley c
d
manning a phrase based alignment model for natural language inference in proceedings of the conference on empirical methods in natural language processing association for computational linguistics pp

r
wang g
neumann an accuracy oriented divide and conquer in proceedings of the text egy for recognizing textual entailment analysis conference tac
m
sammons v
v
vydiswaran t
vieira n
johri m

chang d
goldwasser v
srikumar g
kundu y
tu k
small j
rule q
do d
roth relation alignment for textual entailment recognition in proceedings of the text analysis conference tac
s
harmeling inferring textual entailment with a probabilistically sound calculus natural language engineering
a
stern i
dagan a condence model for syntactically motivated tailment proofs in proceedings of the international conference recent advances in natural language processing pp

m
kouylekov b
magnini recognizing textual entailment with tree edit distance algorithms in proceedings of the first challenge shop recognising textual entailment pp

r
wang g
neumann an divide and conquer strategy for recognizing in proceedings of the text analysis conference textual entailment gaithersburg md
r
zanoli s
colombo a transformation driven approach for ing textual entailment natural language engineering
s
jimenez g
duenas j
baquero a
gelbukh unal nlp ing soft cardinality features for semantic textual similarity relatedness and entailment in proceedings of the international workshop on semantic evaluation semeval pp

j
zhao t
zhu m
lan ecnu one stone two birds ensemble of erogenous measures for semantic relatedness and textual entailment in proceedings of the international workshop on semantic evaluation semeval pp

k
zhang e
chen q
liu c
liu g
lv a context enriched neural network method for recognizing lexical entailment in aaai pp

v
s
silva a
freitas s
handschuh recognizing and justifying text entailment through distributional navigation on denition graphs in aaai
p
clark p
harrison an inference based approach to recognizing tailment in tac
r
raina a
y
ng c
d
manning robust textual inference via ing and abductive reasoning in aaai pp

e
m
voorhees contradictions and justications extensions to the textual entailment task in acl pp

m
e
frisse searching for information in a hypertext medical handbook communications of the acm
v
n
gudivada v
v
raghavan w
i
grosky r
kasanagottu formation retrieval on the world wide web ieee internet computing
c
c
aggarwal h
wang text mining in social networks in social network data analytics springer pp

k
ganesan c
zhai j
han opinosis a graph based approach to in abstractive summarization of highly redundant opinions ings of the international conference on computational linguistics association for computational linguistics pp

c
paul a
rettinger a
mogadala c
a
knoblock p
szekely cient graph based document similarity in international semantic web conference springer pp

l
kotlerman i
dagan b
magnini l
bentivogli textual entailment graphs natural language engineering
s
r
bowman g
angeli c
potts c
d
manning a large annotated corpus for learning natural language inference in conference on pirical methods in natural language processing emnlp association for computational linguistics acl
a
williams n
nangia s
bowman a broad coverage challenge corpus for sentence understanding through inference in proceedings of the conference of the north american chapter of the association for computational linguistics human language technologies volume long papers volume pp

t
rocktaschel e
grefenstette k
m
hermann t
p
som reasoning about entailment with neural attention in proceedings of the international conference on learning representations
s
wang j
jiang learning natural language inference with lstm in proceedings of the conference of the north american ter of the association for computational linguistics human language technologies pp

q
chen x
zhu z

ling s
wei h
jiang d
inkpen enhanced lstm for natural language inference in proceedings of the nual meeting of the association for computational linguistics volume long papers pp

a
parikh o
tackstrom d
das j
uszkoreit a decomposable tention model for natural language inference in proceedings of the conference on empirical methods in natural language ing pp

y
liu c
sun l
lin x
wang learning natural language inference using bidirectional lstm model and inner attention arxiv preprint

j
i m s
cho distance based self attention network for natural guage inference arxiv preprint

y
gong h
luo j
zhang natural language inference over interaction space arxiv preprint

s
kim i
kang n
kwak semantic sentence matching with connected recurrent and co attentive information in proceedings of the aaai conference on articial intelligence volume pp

q
chen x
zhu z

ling d
inkpen s
wei neural natural language inference models enhanced with external knowledge in proceedings of the annual meeting of the association for computational tics volume long papers pp

x
wang p
kapanipathi r
musa m
yu k
talamadupula i
laziz m
chang a
fokoue b
makni n
mattei m
witbrock ing natural language inference using external knowledge in the science questions domain in proceedings of the aaai conference on articial intelligence volume pp

s
gururangan s
swayamdipta o
levy r
schwartz s
bowman n
a
smith annotation artifacts in natural language inference data in proceedings of the conference of the north american ter of the association for computational linguistics human language technologies volume short papers pp

a
poliak j
naradowsky a
haldar r
rudinger b
van durme pothesis only baselines in natural language inference in proceedings of the seventh joint conference on lexical and computational semantics pp

o

camburu t
rocktaschel t
lukasiewicz p
blunsom e snli natural language inference with natural language explanations in vances in neural information processing systems pp

j
thorne a
vlachos c
christodoulopoulos a
mittal generating token level explanations for natural language inference in proceedings of the conference of the north american chapter of the ciation for computational linguistics human language technologies volume long and short papers pp

v
s
silva a
freitas s
handschuh exploring knowledge graphs in an interpretable composite approach for text entailment in aaai
m
pawlik n
augsten tree edit distance robust and ecient information systems
k
zhang d
shasha simple fast algorithms for the editing distance between trees and related problems siam journal on computing
d
chen c
manning a fast and accurate dependency parser using in proceedings of the conference on empirical neural networks methods in natural language processing emnlp pp

p
clark c
fellbaum j
hobbs using and extending wordnet to support question answering in proceedings of the global wordnet conference
j
herrera a
penas f
verdejo textual entailment recognition based on dependency analysis and wordnet in machine learning lenges
evaluating predictive uncertainty visual object classication and recognising textual entailment springer pp

c
fellbaum wordnet wiley online library
v
s
silva s
handschuh a
freitas categorization of semantic in cognitive aspects of the lexicon roles for dictionary denitions cogalex v workshop at coling pp

l
x
carreras k
c
litkowski s
stevenson semantic role labeling an introduction to the special issue computational linguistics
v
s
silva a
freitas s
handschuh building a knowledge graph from natural language denitions for interpretable text entailment tion in proceedings of the eleventh international conference on guage resources and evaluation lrec
c
d
manning m
surdeanu j
bauer j
r
finkel s
bethard d
closky the stanford corenlp natural language processing toolkit in acl system demonstrations pp

g
mesnil y
dauphin k
yao y
bengio l
deng d
hakkani tur x
he l
heck g
tur d
yu al
using recurrent neural networks for slot lling in spoken language understanding ieee acm transactions on audio speech and language processing taslp
p
d
turney p
pantel from frequency to meaning vector space models of semantics journal of articial intelligence research
m
marelli s
menini m
baroni l
bentivogli r
bernardi r
parelli a sick cure for the evaluation of compositional distributional semantic models in lrec pp

a
freitas j
c
p
da silva e
curry p
buitelaar a distributional mantics approach for selective reasoning on commonsense graph edge bases in international conference on applications of natural language to data bases information systems springer pp

s
faralli r
navigli a java framework for multilingual denition and in proceedings of the annual meeting of hypernym extraction the association for computational linguistics system demonstrations pp

a
freitas e
curry s
a distributional approach for logical semantic search on the linked data web in proceedings of the annual acm symposium on applied computing acm pp

b
magnini r
zanoli i
dagan k
eichler g
neumann t

noh s
pado a
stern o
levy the excitement open platform for textual inferences in acl system demonstrations pp


