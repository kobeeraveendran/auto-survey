n u j l c
s c v
v i x r a neural network based abstract generation for opinions and arguments lu wang college of computer and information science northeastern university boston ma
neu
edu wang ling google deepmind london
com abstract we study the problem of generating tive summaries for opinionated text
we pose an attention based neural network model that is able to absorb information from ple text units to construct informative concise and uent summaries
an importance based sampling method is designed to allow the coder to integrate information from an tant subset of input
automatic evaluation dicates that our system outperforms state the art abstractive and extractive tion systems on two newly collected datasets of movie reviews and arguments
our system summaries are also rated as more informative and grammatical in human evaluation
introduction collecting opinions from others is an integral part of our daily activities
discovering what other ple think can help us navigate through different pects of life ranging from making decisions on ular tasks to judging fundamental societal issues and forming personal ideology
to efciently absorb the massive amount of opinionated information there is a pressing need for automated systems that can erate concise and uent opinion summary about an entity or a topic
in spite of substantial researches in opinion summarization the most prominent proaches mainly rely on extractive summarization methods where phrases or sentences from the nal documents are selected for inclusion in the mary hu and liu lerman et al

one of the problems that extractive methods suffer from movie the martian reviews one the smartest sweetest and most satisfyingly suspenseful lms in years



an intimate epic that is smart spectacular and stirring
the martian is a thrilling human and moving picture that is easily the most emotionally engaging lm ridley scott has made


it s pretty sunny and often funny a space oddity for a director not known for pictures with a sense of humor
the martian highlights the book s best qualities tones down its worst and adds its own style


opinion consensus summary smart thrilling and prisingly funny the martian offers a faithful adaptation of the bestselling book that brings out the best in leading man matt damon and director ridley scott
topic this house supports the death penalty
arguments the state has a responsibility to protect the lives of innocent citizens and enacting the death penalty may save lives by ducing the rate of violent crime
while the prospect of life in prison may be frightening surely death is a more daunting prospect
a study by stephen k
layson at the university of north carolina showed that a single execution deters murders
reducing the wait time on death row prior to execution can dramatically increase its deterrent effect in the united states
claim summary the death penalty deters crime
figure examples for an opinion consensus of fessional reviews critics about movie the martian from www
rottentomatoes
com and a claim about death penalty supported by arguments from idebate
org
tent with similar meaning is highlighted in the same color
is that they unavoidably include secondary or dant information
on the contrary abstractive marization methods which are able to generate text beyond the original input can produce more ent and concise summaries
in this paper we present an attention based ral network model for generating abstractive maries of opinionated text
our system takes as put a set of text units containing opinions about the same topic e

reviews for a movie or arguments for a controversial social issue and then outputs a one sentence abstractive summary that describes the opinion consensus of the input
specically we investigate our abstract tion model on two types of opinionated text movie reviews and arguments on controversial topics
amples are displayed in figure
the rst ple contains a set of professional reviews or ics about movie the martian and an opinion sensus written by an editor
it would be more ful to automatically generate uent opinion sus rather than simply extracting features e

plot music and opinion phrases as done in previous summarization work zhuang et al
li et al

the second example lists a set of arguments on death penalty where each argument supports the central claim death penalty deters crime
guments as a special type of opinionated text tain reasons to persuade or inform people on certain issues
given a set of arguments on the same topic we aim at investigating the capability of our abstract generation system for the novel task of claim ation
existing abstract generation systems for ated text mostly take an approach that rst es salient phrases and then merges them into tences bing et al
ganesan et al

those systems are not capable of generating new words and the output summary may suffer from ungrammatical structure
another line of work quires a large amount of human input to enforce summary quality
for example gerani et al
utilize a set of templates constructed by human which are lled by extracted phrases to generate grammatical sentences that serve different discourse functions
to address the challenges above we propose to use an attention based abstract generation model a data driven approach trained to generate tive concise and uent opinion summaries
our method is based on the recently proposed work of neural encoder decoder models ner and blunsom sutskever et al
which translates a sentence in a source language into a target language
different from previous work our summarization system is designed to port multiple input text units
an attention based model bahdanau et al
is deployed to low the encoder to automatically search for salient information within context
furthermore we pose an importance based sampling method so that the encoder can integrate information from an portant subset of input text
the importance score of a text unit is estimated from a novel regression model with pairwise preference based regularizer
with importance based sampling our model can be trained within manageable time and is still able to learn from diversied input
we demonstrate the effectiveness of our model on two newly collected datasets for movie reviews and arguments
automatic evaluation by bleu ineni et al
indicates that our system forms the state of the art extract based and based methods on both tasks
for example we achieved a bleu score of
on rotten toes movie reviews compared to
by an stractive opinion summarization system from et al

rouge evaluation lin and hovy also indicates that our system summaries have reasonable information coverage
human judges further rated our summaries to be more informative and grammatical than compared systems
data collection we collected two datasets for movie reviews and arguments on controversial topics with standard abstracts
rotten tomatoes www
rottentomatoes
com is a movie review site that aggregates both professional critics and user generated reviews henceforth toes
for each movie a one sentence critic sensus is constructed by an editor to summarize the opinions in professional critics
we crawled critics and their opinion consensus for movies i
e
around reviews per movie on average
we select movies for training movies for idation and movies for testing
the opinion sensus is treated as the gold standard summary
we also collect an argumentation dataset from idebate
org henceforth idebate which is a wikipedia style website for gathering pro and con arguments on controversial issues
the arguments under each debate or topic are organized into datasets can be downloaded from
ccs
neu
edu home
ferent for and against points
each point tains a one sentence central claim constructed by the editors to summarize the corresponding arguments and is treated as the gold standard
for instance on a debate about death penalty one claim is the death penalty deters crime with an argument acting the death penalty may save lives by reducing the rate of violent crime figure
we crawled debates with claims
we treat each tence as an argument which results in ments in total
debates are used for training debates for validation and debates for testing
the neural network based abstract generation model in this section we rst dene our problem in tion
followed by model description
in eral we utilize a long short term memory network for generating abstracts section
from a latent representation computed by an attention based coder section

the encoder is designed to search for relevant information from input to ter inform the abstract generation process
we also discuss an importance based sampling method to low encoder to integrate information from an tant subset of input sections
and

processing section
is conducted to re rank the generations and pick the best one as the nal mary

problem formulation in summarization the goal is to generate a summary y composed by the sequence of words



unlike previous neural encoder decoder approaches which decode from only one input our input sists of an arbitrary number of reviews or arguments henceforth text units wherever there is no ity denoted as


xm
each text unit xk is composed by a sequence of words xk
each word takes the form of a representation vector which is initialized randomly or by pre trained beddings mikolov et al
and updated during training
the summarization task is dened as ing y which is the most likely sequence of words


yn such that


xk where log p denotes the conditional likelihood of the output sequence y given the input text units
in the next sections we describe the attention model used to model log p

decoder similar as previous work sutskever et al
bahdanau et al
we decompose log p into a sequence of word level predictions log p log p





where each word is predicted conditional on the previously generated


and input
the probability is estimated by standard word softmax


sof hj is the recurrent neural networks rnns state variable at timestamp j which is modeled as hj s here g is a recurrent update function for generating the new state hj from the representation of ously generated word obtained from a word lookup table the previous state and the input text representation s see section

in this work we implement g using a long term memory lstm network hochreiter and schmidhuber which has been shown to be fective at capturing long range dependencies
here we summarize the update rules for lstm cells and refer readers to the original work hochreiter and schmidhuber for more details
given an bitrary input vector uj at timestamp j and the previous state a typical lstm denes the lowing update rules ij bi fj uuj wf wf bf fj ij bc oj woccj bo hj oj y argmaxy log p is component wise logistic sigmoid function and denotes hadamard product
projection matrices w and biases b are parameters to be learned ing training
long range dependencies are captured by the cell memory cj which is updated linearly to avoid the vanishing gradient problem
it is accomplished by predicting two vectors ij and fj which determine what to keep and what to forget from the current timestamp
vector oj then decides on what mation from the new cell memory cj can be passed to the new state
finally the model concatenates the representation of previous output word and the input representation s see section
as uj which serves as the input at each timestamp

encoder the representation of input text units s is computed using an attention model bahdanau et al

given a single text unit


and the previous state hj the model generates s as a weighted sum aibi


where ai is the attention coefcient obtained for word xi and bi is the context dependent sentation of xi
in our work we construct bi by building a bidirectional lstm over the whole put sequence


and then combining the ward and backward states
formally we use the lstm formulation from eq
to generate the ward states hf by setting uj xj the jection word xj using a word lookup table
wise the backward states hb are generated using a backward lstm by feeding the input in the reverse order that is uj
the cients ai are computed with a softmax over all input


hb


hf ai sof where function v computes the afnity of each word xi and the current output context how likely the input word is to be used to ate the next word in summary
we set ws where w and w are parameters to be learned

attention over multiple inputs a key distinction between our model and isting sequence to sequence models sutskever et al
bahdanau et al
is that


xk input consists of multiple separate text our given an input of n text units i
e
units
xk a simple extension would be to concatenate them into one sequence as z





where seg is a special token that delimits inputs
seg xn seg


xn however there are two problems with this proach
firstly the model is sensitive to the order of text units
moreover may contain thousands of words
this will become a bottleneck for our model with a training time of since attention efcients must be computed for all input words to generate each output word
we address these two problems by sub sampling from the input
the intuition is that even though the number of input text units is large many of them are redundant or contain secondary information
as our task is to emphasize the main points made in the input some of them can be removed without ing too much information
therefore we dene an importance score for each document xk see section

during training k candidates are sampled from a multinomial distribution which is constructed by normalizing xk for input text units
notice that the training process goes over the training set multiple times and our model is still able to learn from more than k text units
for ing top k candidates with the highest importance scores are collapsed in descending order into z

importance estimation we now describe the importance estimation model which outputs importance scores for text units
in general we start with a ridge regression model and add a regularizer to enforce the separation of summary worthy text units from others
given a cluster of text units


xm and their summary y we compute the number of overlapping content words between each text unit and summary as its gold standard importance score
the scores are uniformly normalized to
each text unit xk is represented as an ddimensional feature vector rk rd with label lk
text units in the training data are thus denoted with a feature matrix r and a label vector l
we aim at learning xk rk w by mizing rw
this is a standard formulation for ridge regression and we use tures in table
furthermore pairwise preference constraints have been utilized for learning ranking models joachims
we then consider adding a pairwise preference based regularizing constraint to incorporate a bias towards summary worthy text units xp xqt rq w where t is a cluster of text units to be rized
term rp rq w enforces the separation of summary worthy text from the others
we further construct to contain all the pairwise differences rp rq
is a vector of the same size as with each element as
the objective function becomes t rw are tuned on development set
with i d and closed form solution for w is w rt r rt l num of words unigram num of pos tags num of named entities centroidness radev mpqa wilson et al
avg max tf idf scores category in general inquirer stone et al
num of positive negative neutral words general inquirer table features used for text unit importance estimation

post processing for testing phase we re rank the n best summaries according to their cosine similarity with the input text units
the one with the highest similarity is cluded in the nal summary
uses of more ticated re ranking methods charniak and johnson konstas and lapata will be gated in future work
pre trained embeddings and features
the size of word representation is set to both for put and output words
these can be initialized randomly or using pre trained embeddings learned from google news mikolov et al

we also extend our model with additional features described in table
discrete features such as pos tags are mapped into word representation via lookup tables
for continuous features e
g tf idf scores they are attached to word vectors as additional values
part of a named entity category in general inquirer capitalized pos tag dependency relation sentiment polarity general inquirer mpqa tf idf score table token level features used for abstract generation
hyper parameters and stop criterion
the lstms equation for the decoder and encoders are dened with states and cells of dimensions
the attention of each input word and state pair is computed by being projected into a vector of dimensions equation
training is performed via adagrad duchi et al

it terminates when performance does not prove on the development set
we use bleu up to grams papineni et al
as evaluation ric which computes the precision of n grams in erated summaries with gold standard abstracts as the reference
finally the importance based sampling rate k is set to for experiments in sections
and

decoding is performed by beam search with a beam size of i
e
we keep most probable put sequences in stack at each step
outputs with end of sentence token are also considered for re ranking
decoding stops when every beam in stack generates the end of sentence token
experimental setup results data pre processing
we pre process the datasets with stanford corenlp manning et al
for tokenization and extracting pos tags and dency relations
for rottentomatoes dataset we place movie titles with a generic label in training and substitute it with the movie name if there is any generic label generated in testing

importance estimation evaluation we rst evaluate the importance estimation nent described in section

we compare with support vector regression svr smola and nik and two baselines a length baseline that ranks text units based on their length and a centroid baseline that ranks text units according to their centroidness which is computed as the sine similarity between a text unit and centroid of the cluster to be summarized erkan and radev
figure evaluation of importance estimation by mean ciprocal rank mrr and normalized discounted cumulative gain at top and returned results and
our regression model with pairwise preference based izer uniformly outperforms baseline systems on both datasets
we evaluate using mean reciprocal rank mrr and normalized discounted cumulative gain at top and returned results
text units are considered relevant if they have at least one ping content word with the gold standard summary
from figure we can see that our importance timation model produces uniformly better ranking performance on both datasets

automatic summary evaluation for automatic summary evaluation we consider three popular metrics
rouge lin and hovy is employed to evaluate n grams recall of the summaries with gold standard abstracts as erence
rouge measures unigram and bigrams separated by up to four words is reported
we also utilize bleu a precision based metric which has been used to evaluate various language generation systems chiang angeli et al
karpathy and fei fei
we further consider meteor denkowski and lavie
as a recall oriented metric it calculates similarity between generations and references by considering synonyms and paraphrases
for comparisons we rst compare with an stractive summarization method presented in et al
on the rottentomatoes dataset
ganesan et al
utilize a graph based rithm to remove repetitive information and merge opinionated expressions based on syntactic tures of product reviews
for both datasets we sider two extractive summarization approaches lexrank erkan and radev is an vised method that computes text centrality based on pagerank algorithm sipos et al
propose a supervised submodular summarization model which is trained with support vector machines
in addition longest sentence is picked up as a line
four variations of our system are tested
one uses randomly initialized word embeddings
the rest of them use pre trained word embeddings additional features in table and their combination
for all systems we generate a one sentence summary
results are displayed in table
our system with pre trained word embeddings and additional tures achieves the best bleu scores on both datasets in boldface with statistical signicance two tailed wilcoxon signed rank test p

notice that our system summaries are conciser i
e
shorter on average which lead to higher scores on precision based metrics e

bleu and lower scores on recall based metrics e

meteor and rouge
on rottentomatoes dataset where summaries erated by different systems are similar in length our system still outperforms other methods in meteor and rouge in addition to their signicantly ter bleu scores
this is not true on idebate since the length of summaries by extract based systems is signicantly longer
but the bleu scores of our system are considerably higher
among our four systems models with pre trained word embeddings in general achieve better scores
though additional features do not always improve the performance we nd that they help our systems converge faster

human evaluation on summary quality for human evaluation we consider three aspects formativeness that indicates how much salient mation is contained in the summary grammaticality that measures whether a summary is grammatical and compactness that denotes whether a summary contains unnecessary information
each aspect is rated on a to scale is the best
the judges are do not run this model on idebate because it relies on high redundancy to detect repetitive expressions which is not observed on idebate
rottentomatoes idebate length bleu meteor rouge length bleu meteor rouge extract based systems longest lexrank submodular abstract based systems opinosis our systems words words pre trained words features words pre trained features



























































table automatic evaluation results by bleu meteor and rouge scores multiplied by for abstract generation systems
the average lengths for human written summaries are
and
for rottentomatoes and idebate
the best performing system for each column is highlighted in boldface where our system with pre trained word embeddings and additional features achieves the best bleu scores on both datasets
our systems that are statistically signicantly better than the comparisons are highlighted with two tailed wilcoxon signed rank test p

our system also has the best meteor and rouge scores in italics on rottentomatoes dataset among learning based systems








info gram comp avg rank

lexrank


opinosis



our system human abstract


table human evaluation results for abstract generation tems
inter rater agreement for overall ranking is
by pendorff s
informativeness info grammaticality gram and compactness comp are rated on a to scale with as the best
our system achieves the best informativeness and grammaticality scores among the three learning based systems
our summaries are ranked as the best in of the evaluations and are also ranked higher than compared systems on average
also asked to give a ranking on all summary tions according to their overall quality
we randomly sampled movies from tomatoes test set each of which was evaluated by distinct human judges
we hired procient glish speakers for evaluation
three system maries lexrank opinosis and our system and human written abstract along with representative reviews were displayed for each movie
reviews with the highest gold standard importance scores were selected
results are reported in table
as it can be seen our system outperforms the abstract based tem opinosis in all aspects and also achieves ter informativeness and grammaticality scores than lexrank which extracts sentences in their nal form
our system summaries are ranked as the best in of the evaluations and has an average ranking of
which is higher than both opinosis and lexrank on average
an inter rater ment of krippendorff s of
is achieved for overall ranking
this implies that our based abstract generation model can produce maries of better quality than existing summarization systems
we also nd that our system summaries are constructed in a style closer to human abstracts than others
sample summaries are displayed in figure

sampling effect we further investigate whether taking inputs pled from distributions estimated by importance scores trains models with better performance than the ones learned from xed input or sampled input
recall that we sample k text units based on their importance scores importance based sampling
here we consider two other setups one is sampling k text units uniformly from the put uniform sampling another is picking k text units with the highest scores top k
we try ous k values
results in figure demonstrates that importance based sampling can produce ble bleu scores to top k methods while both of them outperform uniform sampling
for meteor score importance based sampling uniformly performs the other two

further discussion finally we discuss some other observations and tential improvements
first applying the re ranking component after the model generates n best stracts leads to better performance
preliminary periments show that simply picking the observe similar results on the idebate dataset movie the neverending story reviews here is a little adventure that fed on our tivated need to think and wonder


magical storytelling targeted at children still fascinates



the art direction volved a lot of imagination
human a magical journey about the power of a young boy s imagination to save a dying fantasy land the neverending story remains a much loved kids adventure
lexrank it pokes along at times and lapses occasionally into dark moments of preachy philosophy but this is still a ing amusing and harmless lm for kids
opinosis the neverending story is a silly fantasy movie that often shows its age
our system the neverending story is an entertaining dren s adventure with heart and imagination to spare
movie joe strummer the future is unwritten reviews the late punk rock legend joe strummer is rendered fully human in julian temple s engrossing and encompassing portrait
the movie fascinates not so much because of strummer


but because of the way temple ganized and edited the lm
one of the most compelling documentary portraits of a musician yet made
human displaying joe strummer warts and all the ture is unwritten succeeds as both an engrossing tary and a comprehensive examination of one of music s most legendary gures
lexrank joe strummer the future is unwritten is a lm for fans really big fans
opinosis joe strummer the future is unwritten is for fans really big fans
our system fascinating and insightful joe strummer the future is unwritten is a thoroughly engrossing documentary
topic this house would detain terror suspects without trial
arguments governments must have powers to protect their citizens against threats to the life of the nation
one would recognise that rules that are applied in peacetime may not be appropriate during wartime
human governments must have powers to protect citizens from harm
lexrank this is not merely to directly protect citizens from political violence but also because political violence caps the process of reconstruction in nation building efforts
our system governments have the obligation to protect izens from harmful substances
topic this house would replace christmas with a festival for everyone
arguments christmas celebrations in the western world


do not respect the rights of those who are not gious
states should instead be sponsoring and celebrating events that everyone can join in equally regardless of religion race or class
human states should respect the freedom from religion as well as the freedom of religion
lexrank for school children who do not share the christian faith christmas celebrations require either their ticipation when they do not want to through coercion or their non participation and therefore isolation whilst everyone else celebrations their inclusiveness
our system people have a right to freedom of religion
figure sample summaries generated by different systems on movie reviews and arguments
we only show a subset of reviews and arguments due to limited space
figure sampling effect on rottentomatoes
ations produces inferior results than re ranking them with simple heuristics
this suggests that the current models are oblivious to some task specic issues such as informativeness
post processing is needed to make better use of the summary candidates
for example future work can study other sophisticated re ranking algorithms charniak and johnson konstas and lapata
furthermore we also look at the difcult cases where our summaries are evaluated to have lower formativeness
they are often much shorter than the gold standard human abstracts thus the information coverage is limited
in other cases some generations contain incorrect information on domain dependent facts e

named entities numbers
for stance a summary a poignant coming of age tale marked by a breakout lead performance from cate shortland is generated for movie lore
this mary contains cate shortland which is the tor of the movie instead of actor
it would require semantic features to handle this issue which has yet to be attempted
related work our work belongs to the area of opinion rization
constructing uent natural language ion summaries has mainly considered product views hu and liu lerman et al
munity question answering wang et al
and editorials paul et al

extractive tion approaches are employed to identify worthy sentences
for example hu and liu rst identify the frequent product features and then attach extracted opinion sentences to the sponding feature
our model instead utilizes stract generation techniques to construct natural guage summaries
as far as we know we are also the rst to study claim generation for arguments
recently there has been a growing interest in generating abstractive summaries for news cles bing et al
spoken meetings wang and cardie and product reviews ganesan et al
di fabbrizio et al
gerani et al

most approaches are based on phrase tion from which an algorithm concatenates them into sentences bing et al
ganesan et al

nevertheless the output summaries are not guaranteed to be grammatical
gerani et al
then design a set of manually constructed realization templates for producing grammatical sentences that serve different discourse functions
our approach does not require any human annotated rules and can be applied in various domains
our task is closely related to recent advances in neural machine translation kalchbrenner and som sutskever et al

based on the sequence to sequence paradigm rnns based els have been investigated for compression al
and summarization filippova et al
rush et al
hermann et al
at sentence level
built on the attention based lation model in bahdanau et al
rush et al
study the problem of constructing abstract for a single sentence
our task differs from the els presented above in that our model carries out stractive decoding from multiple sentences instead of a single sentence
conclusion in this work we presented a neural approach to generate abstractive summaries for opinionated text
we employed an attention based method that nds salient information from different input text units to generate an informative and concise summary
to cope with the large number of input text we ploy an importance based sampling mechanism for model training
experiments showed that our tem obtained state of the art results using both tomatic evaluation and human evaluation
references et al
gabor angeli percy liang and dan klein

a simple domain independent in proceedings of abilistic approach to generation
the conference on empirical methods in ral language processing pages
association for computational linguistics
bahdanau et al
dzmitry bahdanau kyunghyun
neural machine cho and yoshua bengio
translation by jointly learning to align and translate
corr

bing et al
lidong bing piji li yi liao wai lam weiwei guo and rebecca passonneau

stractive multi document summarization via phrase lection and merging
in proceedings of the nual meeting of the association for computational linguistics and the international joint conference on natural language processing volume long pers pages beijing china july
ation for computational linguistics
charniak and eugene charniak and mark johnson

coarse n best parsing and in proceedings of maxent discriminative reranking
the annual meeting on association for tional linguistics acl pages burg pa usa
association for computational guistics
david chiang

a hierarchical phrase based model for statistical machine translation
in proceedings of the annual meeting on ation for computational linguistics pages
association for computational linguistics
denkowski and michael denkowski and alon lavie

meteor universal language specic translation evaluation for any target language
in proceedings of the eacl workshop on statistical machine translation
di fabbrizio et al
giuseppe fabbrizio amanda j stent and robert gaizauskas

a hybrid approach to multi document summarization of opinions in reviews
inlg page
di duchi et al
john duchi elad hazan and yoram singer

adaptive subgradient methods for j
mach
line learning and stochastic optimization
learn
res
july
erkan and gunes erkan and dragomir r
radev

lexrank graph based lexical centrality as salience in text summarization
j
artif
int
res
december
filippova et al
katja filippova enrique seca carlos a
colmenares lukasz kaiser and oriol vinyals

sentence compression by deletion with lstms
in proceedings of the conference on empirical methods in natural language processing pages lisbon portugal september
ation for computational linguistics
ganesan et al
kavita ganesan chengxiang zhai and jiawei han

opinosis a graph based proach to abstractive summarization of highly in proceedings of the dant opinions
tional conference on computational linguistics pages
association for computational linguistics
gerani et al
shima gerani yashar mehdad giuseppe carenini raymond t
ng and bita nejat

abstractive summarization of product reviews the using discourse structure
conference on empirical methods in natural language processing emnlp pages doha qatar october
association for computational linguistics
in proceedings of hermann et al
karl moritz hermann tomas cisky edward grefenstette lasse espeholt will kay mustafa suleyman and phil blunsom

teaching machines to read and comprehend
corr

hochreiter and sepp hochreiter and jurgen schmidhuber

long short term memory
neural comput
november
hu and minqing hu and bing liu

ing and summarizing customer reviews
in ings of the tenth acm sigkdd international ence on knowledge discovery and data mining kdd pages new york ny usa
acm
thorsten joachims

in ing search engines using clickthrough data
ceedings of the eighth acm sigkdd international conference on knowledge discovery and data ing kdd pages new york ny usa
acm
kalchbrenner and nal kalchbrenner and phil blunsom

recurrent continuous translation models
in emnlp pages
acl
karpathy and fei andrej karpathy and li deep visual semantic alignments arxiv preprint fei
for generating image descriptions



konstas and ioannis konstas and mirella lapata

concept to text generation via inative reranking
in proceedings of the annual meeting of the association for computational tics volume long papers pages jeju land korea july
association for computational guistics
lerman et al
kevin sasha lerman goldensohn and ryan mcdonald

sentiment summarization evaluating and learning user in proceedings of the conference of erences
the european chapter of the association for putational linguistics eacl pages stroudsburg pa usa
association for computational linguistics
et al
fangtao li chao han minlie huang aoyan zhu ying ju xia shu zhang and hao yu

structure aware review mining and in proceedings of the international tion
ference on computational linguistics coling pages stroudsburg pa usa
association for computational linguistics
lin and chin yew lin and eduard hovy

automatic evaluation of summaries using in proceedings of the gram co occurrence statistics
conference of the north american chapter of the association for computational linguistics on human language technology volume pages
manning et al
christopher manning mihai deanu john bauer jenny finkel steven bethard and david mcclosky

the stanford corenlp in proceedings of ural language processing toolkit
annual meeting of the association for tional linguistics system demonstrations pages baltimore maryland
association for tional linguistics
mikolov et al
tomas mikolov kai chen greg corrado and jeffrey dean

efcient tion of word representations in vector space
corr

papineni et al
kishore papineni salim roukos todd ward and wei jing zhu

bleu a method in for automatic evaluation of machine translation
proceedings of the annual meeting on association for computational linguistics pages
ation for computational linguistics
paul et al
michael j
paul chengxiang zhai and roxana girju

summarizing contrastive points in opinionated text
in proceedings of the conference on empirical methods in natural guage processing emnlp pages burg pa usa
association for computational guistics
dragomir r
radev

experiments in single and multidocument summarization using mead
in in first document understanding conference
rush et al
alexander m
rush sumit chopra and jason weston

a neural attention model for in proceedings of stractive sentence summarization
the conference on empirical methods in natural language processing pages lisbon gal september
association for computational guistics
sipos et al
ruben sipos pannaga shivaswamy and thorsten joachims

large margin learning of submodular summarization models
in proceedings of the conference of the european chapter of the association for computational linguistics eacl pages stroudsburg pa usa
association for computational linguistics
smola and alex smola and vladimir support vector regression machines
nik
advances in neural information processing systems

stone et al
philip j
stone dexter c
dunphy marshall s
smith and daniel m
ogilvie

the general inquirer a computer approach to content analysis
mit press cambridge ma
sutskever et al
ilya sutskever oriol vinyals and quoc v
le

sequence to sequence ing with neural networks
in advances in neural formation processing systems annual conference on neural information processing systems cember montreal quebec canada pages
sutskever et al
ilya sutskever oriol vinyals and quoc v
le

sequence to sequence learning with neural networks
corr

wang and lu wang and claire cardie

domain independent abstract generation for cused meeting summarization
in proceedings of the annual meeting of the association for tational linguistics volume long papers pages soa bulgaria august
association for computational linguistics
wang et al
lu wang hema raghavan claire cardie and vittorio castelli

query focused opinion summarization for user generated content
in proceedings of coling the international conference on computational linguistics technical papers pages dublin ireland august
dublin city university and association for tional linguistics
wilson et al
theresa wilson janyce wiebe and paul hoffmann

recognizing contextual larity in phrase level sentiment analysis
in ings of the conference on human language ogy and empirical methods in natural language cessing hlt pages stroudsburg pa usa
association for computational linguistics
zhuang et al
li zhuang feng jing and xiao yan zhu

movie review mining and summarization
in proceedings of the acm international ference on information and knowledge management cikm pages new york ny usa
acm

