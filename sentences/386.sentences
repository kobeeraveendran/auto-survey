mapgn masked pointer generator network for sequence to sequence pre training mana ihori naoki makishima tomohiro tanaka akihiko takashima shota orihashi ryo masumura ntt media intelligence laboratories ntt corporation japan e f l c
s c v
v i x r a abstract this paper presents a self supervised learning method for generator networks to improve spoken text normalization
text normalization that converts spoken style text into style malized text is becoming an important technology for improving subsequent processing such as machine translation and tion
the most successful spoken text normalization method to date is sequence to sequence mapping using pointer generator networks that possess a copy mechanism from an input sequence
however these models require a large amount of paired data of spoken style text and style normalized text and it is difcult to prepare such a volume of data
in order to construct spoken text normalization model from the limited paired data we focus on self supervised learning which can utilize unpaired text data to prove models
unfortunately conventional self supervised learning methods do not assume that pointer generator networks are utilized
therefore we propose a novel self supervised ing method masked pointer generator network mapgn
the proposed method can effectively pre train the pointer generator work by learning to ll masked tokens using the copy mechanism
our experiments demonstrate that mapgn is more effective for pointer generator networks than the conventional self supervised learning methods in two spoken text normalization tasks
index terms sequence to sequence pre training generator networks self supervised learning spoken text ization
introduction with the rise of various automatic speech recognition asr cations such as smart speakers and automatic dictation tems it has become increasingly important to accurately cess spoken style text i
e
the transcribed text from spoken ances
spoken style text often includes disuencies such as dant expressions and various minority spoken expressions e

alects because asr systems convert speech into text in a literal manner
spoken style text adversely affects subsequent natural guage processing e

machine translation summarization because these technologies are often developed to handle written style text which is text with majority expressions and no disuencies or dundant expressions
thus it is required to convert spoken style text including disuencies and dialects into style normalized text which excludes disuencies and dialects
in this paper we aim to improve spoken text normalization
spoken text normalization tasks are considered as monolingual translation that is regarded as sequence to sequence mapping from text to text
in recent studies fully neural based models have shown effective performance in ous monolingual translation tasks such as summarization phrase generation and disuency detection
in ular models based on pointer generator networks have been utilized recently
pointer generator networks are tive for monolingual translation tasks because they contain a copy mechanism that copies tokens from a source text to help generate infrequent tokens
pointer generator networks have reportedly performed attention based encoder decoder networks in spoken text normalization task
to construct models for spoken text normalization a large amount of paired data of spoken style text and style ized text are necessary
however to make these paired data we need to prepare manual transcriptions of spoken utterances and the text style of these transcriptions needs to be normalized manually
thus it is costly and time consuming to make a large amount of paired data
to mitigate this problem we use self supervised ing which has been gaining a lot of attention in recent years
supervised learning is one form of unsupervised learning where paired data is only employed for designing supervised training tings
in natural language processing self supervised learning has been improving in natural language generation and natural language understanding
unfortunately conventional self supervised learning methods for models do not assume that generator networks are utilized
in practice the tional methods are insufcient for pointer generator networks cause they can not learn to copy tokens from a source text explicitly
in this paper we propose a novel self supervised learning method for pointer generator networks
the proposed method masked pointer generator network mapgn is an extension of masked sequence to sequence pre training mass
mass pre trains a model by predicting the masked tokens taking the masked sequence as input
in contract mapgn can pre train a copy mechanism efciently by learning to choose whether to copy or generate tokens with masked tokens
our experiments demonstrate that the proposed method is effective for pointer generator networks with less paired data in two spoken text normalization tasks dialect conversion task and spoken to written style conversion task

pointer generator networks this section denes spoken style normalization with pointer generator networks
we dene spoken style text as x xm and style normalized text as y yn where xm and yn are tokens in the spoken style and style normalized text respectively
the pointer generator network predicts generation probabilities of a written style text y given a spoken style text x
the tion probability of y is dened as p y p x n where enc dec represents model parameter sets
enc and dec are trainable parameter sets with an encoder and a decoder spectively
p x can be computed with the encoder fig
token span masking method
text by using unpaired data
the model can be ne tuned with paired data for each subsequent task
in this paper we use token span masking which is common with mass to corrupt the original text for the basic self supervised learning strategy
we propose a self supervised learning method for pointer generator networks by devising a masking method for the token span masking


basic self supervised learning strategy we use token span masking in mass as a basic self supervised learning strategy
in this method given unpaired sentence y we get y a where its fragment from position a to b are masked
here a n and n is the number of tokens of sentence y
the number of tokens that are masked from position a to is and the length k is roughly of n
position a is selected from between the rst token and the n token
ya indicates the sentence fragment of y from a to b
in addition each of the selected masked token is either replaced with a mask token a random token or left unchanged
we describe details of this replacing method for the masked token in section

the model is pre trained by predicting the sentence fragment ya taking the masked sequence y a as input as shown in fig

the model parameter set can be optimized from unpaired data du y y
a loss function to pre train the model parameter set is dened as l log p yd a y d fig
the network structure of a pointer generator network
and the decoder with a copy mechanism
fig
shows the network structure of the pointer generator network
the encoder converts an input sequence x into the hidden resentations h hm
these hidden representations are produced by an arbitrary network such as bidirectional rent neural networks rnns or a transformer encoder
the decoder computes both copying tokens via pointing and ating tokens from a xed vocabulary based on a copy mechanism to compute the generation probabilities
first the decoder converts tokens from the rst token to the n token into hidden vector vn
the hidden vector is produced by an arbitrary network such as unidirectional rnns or a transformer decoder
next we compute attention distribution m n from the function that computes the attention distribution with h and vn
the tion distribution produces a weighted sum of the encoder states dn
generation probabilities for the n token are produced with dn and vn by p x pgen m n m xm t a log p yd t y d
t vn dec dec pgen t vn dec dec t t t t where tanh softmax and sigmoid are linear mational functions with a tanh softmax and sigmoid activation
the copy mechanism enables switching probability pgen to choose whether to copy or generate tokens
thus pgen computes the weighted sum with generator distribution and attention tribution and produces the prediction probability of the n token
the model parameter set can be optimized from paired data dp x y x y
a loss function to optimize the model parameter set is dened as l n log p yd x d

self supervised learning for pointer generator networks this section details self supervised learning for pointer generator networks
the denoising auto encoder task is widely used for self supervised learning
in this task the model learns to reconstruct the original text given the corrupted original in this self supervised learning the encoder is encouraged to derstand the meaning of unmasked tokens
furthermore the decoder is encouraged to extract more useful information from the encoder side by masking the decoder input tokens which are not masked in the encoder
if the decoder input tokens are not masked at all it is assumed that the decoder uses abundant information from the ceding tokens rather than information from the encoder side
in dition the decoder can learn more effective language modeling by predicting consecutive tokens in the decoder side rather than dicting discrete tokens


masking methods we propose a self supervised learning method for pointer generator networks
in token span masking each of the selected masked token is either replaced with a mask token a random token or left unchanged
we vary the percentage of these tokens and the replacing method to develop a suitable masking method for pointer generator networks
this section describes the conventional method mass two methods that varies the percentage of these replaced tokens in mass and our proposed method mapgn
table summarizes each masking method
mass in mass of the masked tokens in the encoder are replaced by mask tokens are replaced by random tokens and are unchanged
the random tokens are introduced on behalf table summary of masking methods
mask random unchanged mapgn select random tokens from all tokens all tokens masking span of the mask token considering that the mask token does not pear during ne tuning
these tokens are randomly selected from the vocabulary
in this paper this masking method is referred to as
moreover we prepare two methods that varies the age of these replaced tokens and
in of the masked tokens in the encoder are replaced by mask kens are replaced by random tokens and are unchanged
in of the masked tokens in the encoder are replaced by mask tokens and are unchanged
mapgn our masked pointer generator network mapgn is an extension of mass
in mapgn of the masked tokens in the encoder are replaced by mask tokens are replaced by random tokens and are unchanged
key advance of mapgn is that the random tokens are not selected from all tokens but from tokens in the masking span
for example in fig
the random tokens are randomly selected from
a reason why we use this masking method is detailed in section



key idea we presume that is not suitable for pointer generator work pre training because the percentage of unchanged tokens in the masking span of the encoder is small and the model can not learn to copy tokens explicitly
we assume that pointer generator networks can learn to copy appropriate tokens from the input by increasing the percentage of unchanged tokens
thus it is assumed that is suitable for pointer generator network
however if only unchanged tokens are increased there is a possibility that the copy mechanism will be overtting
thus mapgn utilizes random tokens to learn to choose to copy or generate tokens effectively
by comparing with mapgn we can verify that increasing unchanged tokens is not enough to pre train the copy mechanism
in addition mapgn selects random tokens from masking span
in other words although random tokens include tokens in the output sequence these tokens do not have information on appropriate positions
thus these tokens are used to aid for copying tokens and conceal the position of tokens that should be copied
the role of the random tokens in mapgn is not only to mask the token but also encourage to determine whether to copy
by comparing with mapgn we can validate the selecting of random tokens from the masking span

experiments this section describes the experimental details of pre training and ne tuning on spoken text normalization tasks
in particular we chose dialect conversion and spoken to written style conversion tasks in japanese
in the dialect conversion task japanese dialect is converted into standard japanese
in the spoken to written style conversion task spoken style text produced by an automatic speech recognition system is converted into written style text with correct punctuations and no disuencies


datasets pre training we prepared a large scale japanese web text as the unpaired written style text data
the web text was downloaded from various topic web pages using our home made crawler
the loaded pages were ltered in such a way that html tags javascript codes and other parts that were not useful for these tasks were cluded
finally we prepare one million sentences for pre training
fine tuning on dialect conversion task we prepared paired data of a japanese dialect tohoku ben and standard japanese using crowd sourcing
we divided the data into a training set validation set and test set
the training set contained sentences and we divided the training set into and sentences to investigate the difference of performance with different amount of training data
the validation set have sentences and the test set have sentences
fine tuning on spoken to written style conversion task we used the parallel corpus for japanese spoken to written style sion cjsw
although the cjsw has four domains we only used the data from one domain call center dialogue for training to compare the in domain id task and the out of domain ood task
the training set has sentences which we divided into increments of between and sentences for the same reason as the dialect conversion task
a validation set of call center dialogue containing sentences was used for training
we used all domain test sets the call center dialogue test set for the id task and all other test sets for the ood task
the test sets were divided in accordance with
the datasets were paired data of spoken style text manual transcriptions of speech and written style text created by crowd sourcing


setups we pre trained attention based encoder decoder networks and pointer generator networks with four pre training methods
we used the following congurations
we used pre trained sional word embeddings using continuous bag of words
in the encoder a layer bidirectional long short term memory rnn lstm rnn with units was introduced
in the decoder a layer unidirectional lstm rnn with units was introduced
we used an additive attention mechanism
the output unit size which corresponded to the number of tokens in the training set of word embeddings was set to
to train these networks we used the adam optimizer and label smoothing with a smoothing rameter of

we set the mini batch size to sentences and the dropout rate in each lstm rnn to

for the mini batch training we truncated each sentence to tokens
all trainable parameters were randomly initialized
we used characters as tokens and these pre trained networks for two tasks in common
in ne tuning we used the attention based encoder decoder work and the pointer generator network which are transferred trained model parameter sets
we constructed these two networks without pre training as the baseline
these model congurations were the same as that of the pre training model and all trainable parameters were randomly initialized
in the evaluation we lated automatic evaluation scores in three metrics rouge l and meteor


results table and table show the experimental results of the dialect version task and the spoken to written style conversion task tively
fig
shows the score with all training data in spoken to written style conversion task
table shows that mapgn for pointer generator networks outperformed other masking methods in all evaluation metrics
although was the best mance in encoder decoder networks it was the least performance in pointer generator networks
the results were the same whether or sentences were used as training however the formance of mapgn improved when sentences were used
table results of dialect conversion task with and sentences of training data
masking method baseline mapgn sentences sentences









rouge l meteor





























rouge l meteor






























attention based encoder decoder network
pointer generator network table results of spoken to written style conversion task with and sentences of training data
masking method baseline sentences rouge l id










attention based encoder decoder network id









ood



















mapgn meteor id









ood










pointer generator network ood









id









ood









sentences rouge l id









ood









meteor id









ood









table shows that mapgn for pointer generator networks formed other masking methods in all metrics
fig
shows that in encoder decoder networks all masking methods improved the line performance signicantly
moreover was the best formance for any amount of training data
in pointer generator works mapgn yield the best performance and the performance was improved more signicantly in the ood task than in the id task
in both id and ood tasks the performance difference between each masking method decreased as the amount of paired training data creased
the results of the two spoken text normalization tasks largely followed the same trend
the results of encoder decoder networks show that outperformed all other methods
in other words the pre training method which learned to actively output the same tokens in the input sequence was more effective
it is assumed that the method to actively copy tokens is effective for spoken text normalization tasks using encoder decoder networks
next a table point of pre training in pointer generator networks results is that each masking method performed differently
this indicates that the masking method is important for pointer generator network pre training
for example was the least performance and was less effective than mapgn
we can infer that even if the pointer generator network simply pre trains to copy tokens actively the copy mechanism learns to copy tokens that do not need to be copied
moreover we assume that the networks can be pre trained to copy or not effectively by selecting random tokens from tokens in the masking span rather than from all tokens in the model
finally the results of the pointer generator networks show that mapgn outperformed other masking methods
however in the id task since pointer generator networks are suitable for style normalization task and the baseline performance is improved as the amount of paired training data increases the effectiveness of pre training for pointer generator networks decreases
on the other hand in the ood task pre training is effective signicantly even if the amount of paired training data increases
thus mapgn is an effective pre training method for pointer generator networks if the amount of paired training data is small or in ood tasks
fig
score of spoken to written style conversion task

conclusion this paper proposed masked pointer generator network mapgn a self supervised learning method for pointer generator networks
while conventional self supervised learning methods do not port to explicitly train a copy mechanism in the pointer generator networks the proposed method can train the copy mechanism ciently by learning to choose whether to copy or generate tokens against masking span
experiments demonstrated that mapgn outperformed the conventional methods in two spoken text malization tasks and was especially effective if the amount of paired training data is small and in ood tasks
we concluded that mapgn is suitable for pre training pointer generator networks and effective when paired data set is limited

references bo li tara sainath arun narayanan joe caroselli michiel bacchiani ananya misra izhak shafran hasim sak golan pundak kean chin khe chai sim ron j
weiss kevin wilson ehsan variani chanwoo kim olivier siohan mitchel weintraub erik mcdermott rick rose and matt shannon acoustic modeling for google home in proc
annual conference of the international speech communication ciation interspeech
amanda purington jessie taft shruti sannon natalya natalie bazarova and samuel taylor alexa is my new bff social roles user satisfaction and personication of the amazon echo in proc
conference extended abstracts on human factors in computing tems chi pp

guokan shang wensi ding zekun zhang antoine jean pierre tixier polykarpos meladianos michalis vazirgiannis and jean pierre lorre unsupervised abstractive meeting summarization with multi sentence compression and budgeted submodular maximization in proc
annual meeting of the association for computational linguistics acl
manling li lingyu zhang heng ji and richard j radke keep ing summaries on topic abstractive multi modal meeting tion in proc
annual meeting of the association for computational linguistics acl pp

sander wubben antal van den bosch and emiel krahmer phrase generation as monolingual translation data and evaluation in proc
international natural language generation conference inlg pp

ilya sutskever oriol vinyals and quoc v le sequence to sequence learning with neural networks in proc
international conference on neural information processing systems nips pp

romain paulus caiming xiong and richard socher a deep forced model for abstractive summarization in proc
international conference on learning representations iclr
yen chun chen and mohit bansal fast abstractive summarization with reinforce selected sentence rewriting in proc
association for computational linguistics acl pp

aaditya prakash sadid a hasan kathy lee vivek datla ashequl qadir joey liu and oladimeji farri neural paraphrase generation with stacked residual lstm networks in proc
international conference on computational linguistics coling pp

shuming ma xu sun wei li sujian li wenjie li and xuancheng ren query and output generating words by querying distributed word representations for paraphrase generation in proc
conference of the north american chapter of the association for computational linguistics human language technologies naacl hlt pp

shaolei wang wanxiang che and ting liu a neural attention model for disuency detection in proc
international conference on putational linguistics coling pp

qianqian dong feng wang zhen yang wei chen shuang xu and bo xu adapting translation models for transcript disuency tion in proc
thirty third aaai conference on articial intelligence aaai pp

abigail see peter j liu and christopher d manning get to the point summarization with pointer generator networks in proc
nual meeting of the association for computational linguistic acl pp

zhengyuan liu angela ng sheldon lee ai ti aw and nancy f chen topic aware pointer generator networks for summarizing spoken versations in proc
ieee automatic speech recognition and standing workshop asru
wenjun zhao meina song and e
haihong summarization with highway condition radom pointer generator network in proc
tional conference on algorithms computing and articial intelligence acai pp

mana ihori akihiko takashima and ryo masumura large context pointer generator networks for spoken to written style conversion in proc
international conference on acoustics speech and signal cessing icassp pp

jacob devlin ming wei chang kenton lee and kristina toutanova bert pre training of deep bidirectional transformers for language derstanding arxiv preprint

matthew peters mark neumann mohit iyyer matt gardner pher clark kenton lee and luke zettlemoyer deep contextualized word representations in proceedings of the conference of the north american chapter of the association for computational tics human language technologies volume long papers pp

alec radford jeffrey wu rewon child david luan dario amodei language models are unsupervised multitask and ilya sutskever learners openai blog p

kaitao song xu tan tao qin jianfeng lu and tie yan liu mass masked sequence to sequence pre training for language generation in proc
international conference on machine learning icml pp

yinhan liu jiatao gu naman goyal xian li sergey edunov jan ghazvininejad mike lewis and luke zettlemoyer multilingual denoising pre training for neural machine translation arxiv preprint

liang wang wei zhao ruoyu jia sujian li and jingming liu noising based sequence to sequence pre training for text generation in proc
conference on empirical methods in natural language cessing and the international joint conference on natural guage processing emnlp ijcnlp pp

mike schuster and kuldip k paliwal bidirectional recurrent neural networks ieee transactions on signal processing pp

ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan n gomez ukasz kaiser and illia polosukhin attention is all you need in proc
advances in neural information processing systems nips pp

sepp hochreiter and jurgen schmidhuber long short term memory neural computation pp

mana ihori akihiko takashima and ryo masumura parallel for japanese spoken to written style conversion in proc
language resources and evaluation conference lrec pp

thang luong hieu pham and christopher d
manning effective approaches to attention based neural machine translation in proc
conference on empirical methods in natural language processing emnlp pp

tomas mikolov kai chen greg corrado and jeffrey dean efcient estimation of word representations in vector space in proc
shop at international conference on learning representations iclr
dzmitry bahdanau kyunghyun cho and yoshua bengio neural chine translation by jointly learning to align and translate in proc
international conference on learning representations iclr pp

kishore papineni salim roukos todd ward and wei jing zhu bleu a method for automatic evaluation of machine translation in proc
annual meeting on association for computational linguistics acl pp

chin yew lin and franz josef och automatic evaluation of machine translation quality using longest common subsequence and skip bigram statistics in proc
annual meeting on association for computational linguistics acl pp

satanjeev banerjee and alon lavie meteor an automatic metric for mt evaluation with improved correlation with human judgments in proc
the acl workshop on intrinsic and extrinsic evaluation sures for machine translation summarization pp


