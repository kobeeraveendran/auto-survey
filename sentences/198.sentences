ai powered text generation for harmonious machine interaction current state and future directions qiuyun zhang bin guo hao wang yunji liang shaoyang hao zhiwen yu school of computer science northwestern polytechnical university xian p
r
china
edu
abstract in the last two decades the landscape of text generation has undergone tremendous changes and is being reshaped by the success of deep learning
new technologies for text generation ranging from template based methods to neural network based methods emerged
meanwhile the research objectives have also changed from generating smooth and coherent sentences to infusing personalized traits to enrich the diversification of newly generated content
with the rapid development of text generation solutions one comprehensive survey is urgent to summarize the achievements and track the state of the arts
in this survey paper we present the general systematical framework illustrate the widely utilized models and summarize the classic applications of text generation
keywords text generation deep learning dialog system research on personalized unprecedented attention
text generation is receiving different from prior survey papers on text generation in this overview we introduce the most recent progress from the methodology perspective and summarize the emerging applications of text generation
according to the difference of data modalities tasks of text generation can be divided into data to text text to text and image to text
among them data to text include weather forecast generation financial report generation and so on
text to text tasks include news generation text summarization text retelling and review generation are widely studied
while the to text tasks include image captioning image questioning answering
tasks i
introduction in short the main contributions of this paper are shown text generation is an important research field in natural language processing nlp and has great application prospects which enables computers to learn to express like human with various types of information such as images structured data text
so as to replace human to complete a variety of tasks
the first automatically generated text is dated back to march when the los angeles times reported the small earthquake occurred near beverly hills california by providing detailed information about the time location and strength of the earthquake
the news was automatically generated by a robot reporter which converted the automatically registered seismic data into text by filling in the blanks in the predefined template text
since then the landscape of text generation is rapidly expanding
at the initial stage majority studies focused on how to reduce the grammatical errors of the text to make the generated text more accurate smooth and coherence
in recent years deep learning achieved great success in many applications ranging from computer vision speech processing and natural language processing
most recent advances in text generation field are based on deep learning technology
not only the most basic recurrent neural networks rnn and sequence to sequence but even the generative adversarial networks gan and the reinforcement learning are widely used in the field of text generation
with the help of these technologies the generated text is more coherent logical and emotionally harmonious
many dialogue systems have brought great convenience to people lives such as microsoft xiaoice contona and apple siri
they not only help people to accomplish specific tasks but also communicate with people as a virtual partner
nowadays researchers start to consider the research of personalized text generation
just as we adjust our speaking style according to the characteristics of each other in the daily communication the text generation process should also dynamically adjust the generation strategy and the final generated content according to the different profiles of the user
therefore now the below we summarize the most recent progress in text generation and present the widely used models in this field
we provide one comprehensive collection of primary applications text including dialogue summarization review generation and image caption visual question answer and the key techniques behind them
systems finally we provide a promising research direction of text generation the personalized text generation
the remaining of this paper is organized as follows
section introduces the commonly used models in the field of text generation
section presents the application scenarios of these models in detail
section highlights application of personalized text generation in various fields
section summarizes the evaluation and section concludes this paper with future work
ii
the text generation models in this section we will introduce the basic frameworks of the widely applied neural networks models for text generation including recurrent neural networks rnn sequence to sequence generative adversarial networks gan and reinforcement learning
a
recurrent neural network rnn is a special neural network structure which is proposed according to the view that people cognition is based on past experience and memory
different from deep neural networks dnn and convolutional neural networks cnn rnn not only considers the input of the previous moment but also endows the network with a memory function of the previous content
the rnn structure is shown in figure
rnn can remember the previous information and apply it to the calculation of the current output
thus the nodes between the hidden layer are no longer connectionless but connected and the input of the hidden layer includes not only the output of the input layer but also the output of the hidden layer at the previous moment
the idea of smooth approximation was used to approximate the output of the generator lstm to solve the gradient inducibility problem caused by the discrete data
there are also many variations of rnn networks such as long short term memory lstm and gated recurrent unit gru
the study in is the pioneering application of rnn for the construction of language models
the experimental results show that the rnn language model outperforms the traditional method
figure
the model structure of gan reinforcement learning is usually a decision process in which performing an action in each state will be rewarded or get negative reward punishment
the goal of reinforcement learning is to find the optimal policy to maximize rewards
the dialogue generation task is in line with the operating mechanism of reinforcement learning
the dialogue generation process can be seen as a process of maximizing the expected rewards of generated dialogue content
through the combination of reinforcement learning and gan excellent results have been achieved in the field of text generation
used discriminator in gan as the source of reward in reinforcement learning
before the discriminator was updated the generator continuously optimized itself according to the return score of the current discriminator until the texts generated by the generator were absolutely true
by using the reward mechanism and the policy gradient technologies in reinforcement learning the problem that the gradient can not be back propagated when gan faces discrete data was skillfully avoided
in the interval of training generator with reinforcement learning method the discriminator was trained with the original method of gan
iii
the text generation applications generation in this section we summarize the classic applications of text text summarization review generation and image caption visual question answer
including dialogue systems a
task oriented dialogue systems dialogue systems attracted more and more attention in recent years
according to different application fields dialogue systems can be divided into two categories oriented and non task oriented dialogue systems also known as chatbots
task oriented dialogue systems help users carry out specific tasks such as restaurant reservations travel itineraries
apple siri and microsoft cortana are the representatives of the task oriented dialogue system
recently deep learning algorithm has been applied to the construction of task oriented dialogue systems
deep learning can automatically learn dimensional distributed feature representation and reduce the burden of manual design
using a large amount of dialogue data to build a pure data driven end to end dialogue system to directly map user input to system output is a very popular research direction now
wen al
constructed a task oriented dialogue system by using a modular neural generation model
neural network is used to realize the process of all modules and figure
the model structure of rnn b
sequence to sequence structure standard model used two rnn networks to compose the encoder decoder structure
the first rnn encoded a sequence of symbols into a fixed length vector representation and the representation into another sequence of symbols
the encoder and decoder were jointly trained to maximize the conditional probability of a target sequence given a source sequence
the structure is shown in figure
second rnn decoded the figure
the model structure of while cnn can not process sequence data with variable length and the input and output sequence length of rnn must be the same using model to encode with rnn in encoder stage can receive sequences with indefinite length as input and in decoder stage can transform the representation vector into sequences without being affected by the input sequence length
thus is widely used in a variety of tasks including machine translation text summarization reading comprehension and speech recognition
c
gan and reinforcement learning proposed by goodfellow consists of two parts one generator and one discriminator
the generator is to generate a false sample distribution that is closest to the real samples
the discriminator is used to distinguish generated samples and real samples
the model structure of gan is shown in figure
while the original gan supports well for the continuous data instead of discrete data such as text
to address this problem researchers have made some fine tuning to gan structure which brings hope for the generation of discrete data
zhang al
used lstm as generator and cnn as discriminator to implement the task of text generation
random inputreal datareal fakeoptimizegeneratorgenerated ot specific tasks of restaurant reservation was achieved
bordes et al
used the neural generation model to treat the dialogue process as a mapping between the user input content and the model reply content and used the encoder decoder structure to train the mapping relationship
in order to solve the problem of dependence on external knowledge bases in oriented dialogue systems eric et al
proposed the end to end key value retrieval network in which was equipped with an attention based key value retrieval mechanism over entries of a knowledge base and could extract relevant information from the knowledge base
in addition the memory network a variant of rnn was proposed to store the current user dialogue context and similar user conversation history with external memory module
by matching user input and context appropriate replies could be selected from the alternative reply set
b
non task oriented dialogue systems known as chatbots non task oriented dialogue systems aim to communicate with humans naturally in the open context
microsoft xiaoice is a typical chatbots
there are two main design methods for the non task oriented dialogue systems retrieval based method and generative method
rtrieval based methods selects retrieval based method directly the the corresponding reply from the alternative replies of the given according to the matching principle
in dual encoder model was proposed by lowe et al
for semantic representation of context and reply content
context and reply were respectively encoded into semantic vectors by dual rnn model and then semantic similarity was calculated by matrix transformation
it was found that matching only from the perspective of words could not achieve good results so zhou et al
proposed matching through multiple levels word level and utterance level and its multi dimensional thinking provided direction for the following papers
in zhou et al
used the encoder part of the transformer model obtain the multi granularity text representation of each context and reply and then two matching matrices were calculated for the representation under each granularity of utterance response pair and the dependency information between the words in utterance and the words in response was also added to the calculation of the alignment matrix as the expression of the words so as to model a deeper semantic relationship
generative methods recently the data driven model has been widely studied in the dialogue system
the pure data driven model directly trains from a large amount of dialogue data without relying on external knowledge
ritter et al
took the reply generation problem as a translation problem in which the process of generating replies was regarded to translate the query into corresponding replies
based on the statistical machine translation model a generating probability model was proposed to model the dialogue system
the disadvantages of that model are obvious
the most important one is that only one user query is translated into the reply in the translation process without considering the context information in the dialogue which is obviously unable to work properly in multiple rounds dialogues
with the development of deep learning neural generation model began to receive attention
sordoni et al
and vinyals et al
began to apply rnn to construct the dialogue model and applied the neural network method to the end end dialogue model for the first time
based on the model the past dialogue history was mapped to the reply
many existing researches have realized the importance of context
the simplest method is to use rnn to directly encode the dialogue sentences as a whole sequence and obtain the semantic representation vector of the context which is treated as additional input in the decoding stage
this method is used by yan al
to utilize the context information
direct concatenation of all sentences may the relative relationship between sentences so researchers have proposed more complex methods to extract context information
using the multi layer model to extract context information the first level is a model of the sentence level to encode the semantic information of a single sentence and the second is a sentence level model using the first layer s output as input to integrate all the contextual information
tian et al
carried out experiments on three different cross sentence methods and came to the conclusion that the performance of the multi layer context information extraction model outperformed the single layer model
lose as stated in in daily human communication people often associate a dialogue content with related topics in their mind
based on this assumption xing et al
organized content and selected words according to the topics for generating responses
the latent dirichlet allocation lda topic model was used to obtain the topical information in the dialogue sentences which was taken into consideration as additional input in the decoding process
the experimental results showed that the introduction of topics into the dialogue model is constructive with improved performance
choudhary et al
recently gan and reinforcement learning have been applied to dialogue systems
li et al
through combining gan and reinforcement learning jointly trained two models
the generation model aimed to generate reply sequences and the discriminator was used to distinguish between generated and machine generated dialogue
li et al
simulated dialogues between two virtual agents using policy gradient to reward sequences showing three useful dialogue attributes informativity coherence and ease of answering related to forward looking function
c
text summarization text summarization in is another important research direction text generation which provides concise description for users by compressing and refining the original text
text summarization can be regarded as a process of information synthesize in which one or more input documents are integrated into a short abstract
banko al
viewed summarization as a problem analogous to statistical machine translation and generated headlines using statistical models for selecting and ordering the summary words
there are two methods to realize text summarization retrieval based method and generative method which will be described in detail in the rest of this chapter
retrieval based methods retrieval based method is a simple method by selecting a subset of sentences in the original document
this process can be thought of selecting the most central sentences in the document which contain the necessary and sufficient information related to the subject of the main theme
nenkova et al
used the word frequency as a feature of the summarization
three attributes related to word frequency were studied word frequency compound function estimating sentence importance from word frequency and word frequency weight adjusted based on context
erkan et al
proposed a model based on the centrality prestige of eigenvectors which is known as lexpagerank
this model constructed the sentence connectivity matrix based on cosine similarity
svore et al
proposed a new automatic summarization method based on neural network whose name was netsum
the model retrieved a set of characteristics from each sentence to help determine the importance in the document
in the cheng et al
used neural network to extract abstract and word and sentence contents were extracted respectively
what is special about this work is the use of the attention mechanism
they directly used the scores in attention to select sentences in a document and was actually similar to pointer networks
cao al
used the attention mechanism to weight the sentences
the weighted basis was the correlation of document sentences to query based on attention and thus extracted the summary by ranking the sentences
the disadvantages of retrieval based method include the similarity of selected sentences and the lack of logic among the selected sentences
generative methods different from retrieval based method the generative method is able to generate sentences that are not in the original text which requires the generative model to have stronger ability of understanding and representation
it is difficult for traditional methods to achieve these abilities
paulus al
the application of reinforcement learning method based on architecture in abstract generation
pasunuru et al
also used reinforcement learning to generate the summarization of the article
introduced the theme of from facebook was attention based nn to generate sentence summarization
alexander m
rush et al
proposed a sentence model under the framework of encoder decoder
later this method was used in many works to construct training data
nallapati al
not only included work on sentence compression but also presented a new data set about document into a multi sentence
this paper added a lot of features on it such as pos tag tf idf ner tag
the feature rich encoder proposed in this paper was also of great significance for other work
d
review generation review generation belongs to data to text natural language generation
within the field of recommender systems a promising application is to estimate or generate personalized reviews that a user would write about a product to discover their nuanced opinions about each of its individual aspects
in order to recommend products to users we need to ultimately predict how users will react to new products
however traditional methods often discard comment text which makes the underlying dimensions of users and products difficult to explain
after identifying the product domain name and user rating the model could generate a review of the corresponding rating like i love disney movies but this one was not at all what i expected
the story line was so predictable it was dumb
jaech et al
made full use of rnn and concatenated the context with the word embedding at the input layer of rnn
experiments on language modeling and classification tasks using three different corpora demonstrated the advantages of this method
almahairi et al
developed two new models bowlf and lmlf to normalize the rating predictions for the amazon review data set using text reviews
lei zheng et al
proposed a new method for modeling ratings reviews and their temporal dynamics in conjunction with rnn was proposed
a recurrent network was used to capture the temporal evolution of user and movie states which were directly used to predict ratings
the user s movie rating history was used as the input of the updated status
the problem with review generation is how to use grained attributes as input to generate more diverse and specific comments
generation of long comments is also a challenge
e
image captioning visual question answering with the development of social network the task that generates captions for images received a lot of attention
image captioning image caption is a basic multimodal problem in the field of artificial intelligence which connects computer vision with natural language generation
it can be divided into two steps feature extraction and natural language generation
cnn is usually used as the feature extraction sub model
it can extract significant features usually represented by the context vector of fixed length
this is followed by a rnn model to generate the corresponding sentence
the whole structure is similar to encoder decoder structure
jaech et al
proposed a deep boltzmann machine in to learn how to generate such multimodal data and shows that the model can be used to create a fused representation by combining features across modes
kiros et al
introduced the neural language model of multimodal constraint and used cnn to learn the word representation and image features together
vinyals al
proposed a generation model based on deep rnn architecture
given the training image the model could be trained to maximize the probability of the target sentence
socher et al
introduced a model that recognized objects in images even when there was no training data available in the object class
and in the completely unsupervised model the accuracy was up to
mao al
proposed a multimodal rnn m rnn model to generate new sentence descriptions explaining the content of images
the model was composed of two subnetworks sentence depth rnn and image depth cnn
huang al
determined the meaning of words through the context of local and global documents of words and explained homonyms and polysemy by learning multiple embedding of each word
tang et al
generated natural language in a specific context or context which introduced two text generation models and
the model produced semantically and syntactically coherent sentences and did better when the sequence became very long
kulkarni al
introduced an automatic natural language description generation system based on image which used a lot of statistical information of text data and computer vision recognition algorithm
the system was very effective in generating image related sentences
mitchel et al
used a new method to generate language in which syntactic models were linked to computer vision detection to generate well formed descriptions of images by filtering out unlikely attributes and putting objects into ordered syntactic structures
frome al
proposed a new deep visual semantic embedding model which used annotated image data and semantic information extracted from unannotated text to identify visual objects
visual question answering visual question answering vqa aims to answer questions about image
the inputs are one image and one question associated with the image and the output is one answer to the question
the deep learning model of vqa usually uses cnn to acquire image s information and rnn to encode the question
ma et al
applied cnn to vqa tasks and provided an end to end convolutional framework for learning not only images and problem representations but also the modal interactions between them to generate answers
malinowsk et al
also used cnn to encode image and feed the question together with the image representation into the lstm network
the system was trained to give correct answers to questions about images
ren et al
used neural networks and visual semantic embedding and not included intermediate stages such as object construction and image segmentation
there are other methods besides cnn to implement vqa task
noh et al
used an independent parametric predictive network with a gru with the question as input and a fully connected layer generating as output
by combining hashing techniques they reduced the complexity of constructing a parameter prediction network with a large number of parameters
yang et al
proposed a learning method based on hierarchical attention network which could help models to answer natural language questions from images
zhu et al
evaluated several basic patterns of personnel performance and qa tasks and proposed a new lstm model with spatial attention that can handle quality assurance tasks
iv
personalized text generation with the development of deep learning techniques we hope computers to automatically write high quality natural language text but much of the previous research has focused on the generated text content not the user personality
in our daily conversation we will not only consider the fact content to produce the corresponding dialogue content but also consider other s personalized profiles to adjust our dialogue style and strategy
the purpose of personalized text generation is to let the computer imitate the behavior of human beings and take the personalized characteristics of users into text content so as to consideration when generating dynamically adjust the generated text content and generate more high quality text
personalized text generation are embodied in many applications which will be briefly introduced below
a
personalized dialogue systems dialogue system is one of the text generation applications that best reflects the user personalized profiles
in the process of chatting with different users if chatbots want to bring pleasant interactive experience to users it needs to adjust its dialogue strategies and reply content according to different characteristics of users
in the personalized characteristics of the user were modeled for the first time by li et al
and the persona based model was proposed
different user was embedded into the hidden vector space by the similar word embedding method
the user embedding vector was used to adjust the dialogue style and content of the dialogue agent
kottur al
extended the previous model and also carried out vector embedding of user features
combined with the hierarchical recurrent encoder decoder structure the model could better capture context related information and considered user personalized features to generate more high quality dialogue content
the considering lack of dialogue data with user personalized characteristics luan et al
applied the task learning mechanism to the personalized reply generation
a small amount of personalized dialogue data was used to train the reply generation model firstly and then an encoder model was trained with non conversational data and the parameters of the two models were shared by the task learning mechanism to obtain the generation model of personalized reply
mo et al
and yang et al
made use of the idea of transfer learning
first they trained a large number of general dialogue data to generate a general reply model and then used a small amount of personalized dialogue data to fine tune the model with transfer learning so that users personalized information could be considered when generating reply
considering the different influence of user characteristics on the reply content qian et al
applied the supervision mechanism to judge when to express the appropriate user profiles in the reply generation process
liu et al
built a two branch neural network to automatically learn user profiles from user dialogues and then the deep neural network was used to further learn fusion representation the user queries replies and user profiles so as to realize the dialogue process from the user perspective
zhang et al
all the work above was carried out based on chatbots and the task oriented dialogue system is also an important research direction in the dialogue system but there are few studies that consider the user personalized information in it
joshi et al
published the dataset of task oriented dialogue system in which each conversation contained the user personalized information providing data support for subsequent research
luo al
made use of a variant of rnn memory network to realize the task oriented personalized dialogue system
the profile model was used to encode the user personalized information and the preference model was used to solve the ambiguity problem of the same query when facing different users
at the same time the similar user s dialogue history was stored
when the reply content was extracted the personalized reply content for different users was generated by combining the similar user dialogue history and the user personalized feature information
b
personalized review generation the findings of tintarev al
indicated that users mentioned different movie features when describing their favorite movies and short personalized arguments for users were more persuasive
therefore personalized user review generation contributes to better recommend products
radford et al
demonstrated the direct influence of emotional units on the process of model generation
lipton et al
built a system of giving user item combinations to generate the comments that users would write when reviewing the product
they designed a character level rnn to generate personalized product reviews
the model learned the styles and opinions of nearly a thousand different authors using a large number of comments from beeradvocate
com
the model in was able to generate sentences which was close to a real user written comments and could identify spelling errors and domain specific words
besides rnn the decoder structure can be lstm and gru
zang et al
introduced a deep neural network model to generate chinese comments from emotional scores representing user opinions
in this paper a hierarchical lstm decoder with attention consistency was proposed
dong et al
proposed an attention enhanced attribute sequence model to generate product reviews for given attribute information such as users products and ratings
attribute encoder learned to represent input attributes as vectors
the sequence decoder then generated comments by adjusting the output of these vectors
they also introduced an attention mechanism to syndicate comments and align words with input attributes
sharma et al
used the model similar to and added loss terms to generate more compliant comments
ni et al
designed a review generation model that could make use of user and project information as well as auxiliary text input and aspect perception knowledge
in the encoding stage of the model there were three encoders sequence encoder attribute encoder and aspect encoder for information integration
the decoder processing of the encoded information biased the gru model toward generating phrases and statements closest to the input
in addition gan can also be used to generate personal reviews
wang et al
proposed a new punishment based goal that took a more rational approach to minimizing overall punishment rather than maximizing rewards
experiments and theories have shown that based on the punishment it could force each generator to produce multiple texts of specific emotional tags rather than producing repeated but safe and good examples
in addition multi class discriminator target allowed the generator to focus more on generating its own specific emotional tag examples
the model could generate a variety of different emotional tags of high quality text perceptual language
v
evaluation metrics with the continuous development of text generation technology the corresponding evaluation method has gradually become an active research direction
researchers need to use the established evaluation method to judge the quality of the proposed models
good evaluation metric is a key factor to promote the research progress
to date there are two main methods for text generation evaluation objective evaluation metric and artificial evaluation
objective evaluation metric is mainly divided into two aspects the first is the word overlapping evaluation matrix such as bleu and rouge the second is based on word vector evaluation matrix such as greedy matching and embedding business
a
word overlap evaluation metrics bleu bilingual evaluation understudy bleu is a method to compare the n gram of model output and reference output and calculate the number of matched fragments
to calculate this metric you need to use translated text called candidate docs and some text translated by professional translators called reference docs
in essence bleu is used to measure the degree of similarity between machine translation text and reference text
its value ranges from to and the closer the value is to the better the machine translation results will be
bleu adopts the n gram matching rule through which it calculates a proportion of n groups of words similar between the comparison translation and the reference translation
bleu algorithm can give relatively valuable evaluation scores quickly
rouge recall oriented understudy for gisting evaluation rouge evaluates the abstract based on the co occurrence information of n gram which is an evaluation method oriented to the recall rate of n gram words
its basic idea is to generate an abstract set of standard abstracts by a number of experts respectively and then to compare that automatically generated abstract of the system with the artificially generated standard abstract
the quality of abstract is evaluated by counting the number of overlapping basic units n gram word sequence and word pair between the two kind of abstract
the stability and robustness of the system are improved by comparing the multi expert manual abstracts
this method has become one of the general notes of abstract evaluation technique
b
word vectors evaluation metrics in addition to the word overlap another way to evaluate the response effect is to judge the relevance of the response by knowing the meaning of each word and the word vector is the basis of this evaluation method
in accordance with semantic distributions a vector is assigned to each word by a method such as which is used to represent that word which is represented approximately by calculating the frequency that the word appears in the corpus
all the word vector metrics can be approximated as sentence vectors at the sentence level through vector connection
in this way the sentence vectors of candidate and target reply sentences can be obtained respectively and the similarity between them can be obtained by comparing them with cosine distance
greedy matching the greedy matching method is a matrix matching method based on word level
in the two sentences given and every word w r will converse into word vector after a conversion
at the same time cosine similarity matching is carried out to the maximum extent with the word vector of each word sequence in and the final result is the mean value after all words are matched
greedy matching was first proposed in the intelligent navigation system and subsequent studies have found that the optimal solution of this method tends to be the result with a large semantic similarity between the center word and the reference answer
embedding average the embedding average is the way to calculate a sentence eigenvector by the word vector in the sentence
the vector of a sentence is calculated by averaging the vectors of each word in the sentence
it a method that has been used in many nlp domains beyond the dialogue system like calculating the similarity of the text
when comparing two sentences embedding average can be calculated respectively and then put the cosine similarity of both as indicators to evaluate their similarity
vi
future directions and conclusion references within the last two decades although great achievements of deep learning spur the development of text generation it is still at the preliminary stage with a large number of open issues
in this section we will highlight some pending questions to underpin the future of research work
a
dataset deficiency different from computer vision and machine translation there is a lack of high quality data in the field of text generation and it is difficult to manually label data
how to use a small amount of data to complete the efficient training of the model is the primary research direction in the future
b
ultra long dialogue context human computer dialogue is a hot area of text generation research
although current chatbots can preliminarily understand the context it is still difficult to grasp the long text
how to effectively capture the semantic information in the text and ensure the consistency of language and logic in the whole dialogue process is a hot topic in the future
c
co textual information language most of the existing studies only focus on the text content but ignore their co textual information
however in reality in a specific natural environment such as time place emotion or emotion
therefore only by considering these co textual information can the syntactically correct semantically reasonable and reasonable text content in a specific context be generated
is usually generated d
evaluation metrics text generation field is lack of unified evaluation metrics system the best evaluation method is conducted by artificial judgement
high quality evaluation metric is crucial to the research in the field of artificial intelligence
only through reasonable and unified evaluation metric can researchers know whether their research work is reasonable or not
this is a major research gap in the future
e
personalized text generation research on personalized text generation is attracting more and more attention
most of the existing research is based on the encoding of user personalized profiles
how to effectively obtain the relationship between personalized profiles and text content is the focus of future research
another problem is the impact of the lack of personalized data on model training
how to use a small amount of personalized data to achieve personalized text generation is the focus of researchers
this paper gives a comprehensive introduction to the basic concepts commonly used models and popular applications in the text generation
at the same time some unsolved problems are put forward
since there are many researchers in each of these work the relevant research results are also endless so the text is inevitably missing
we hope that this paper will provide some help to relevant researchers in this field
acknowledgment this work was partially supported by the national key program of the national natural science foundation of china no

almahairi a
kastner k
cho k
and courville a

learning distributed representations from reviews for collaborative filtering
in proceedings of the acm conference on recommender systems
arjovsky m
chintala s
and bottou l

wasserstein gan
arxiv preprint

banko m
mittal v
o
and witbrock m
j

headline generation based on statistical translation
in proceedings of the annual meeting on association for computational linguistics
bordes a
boureau y

and weston j

learning end end goal oriented dialog
arxiv preprint

cao z
li w
li s
wei f
and li y

attsum joint learning of focusing and summarization with neural attention
arxiv preprint

cheng j
and lapata m

neural summarization by extracting sentences and words
arxiv preprint

cho k
van merrinboer b
gulcehre c
bahdanau d
bougares f
schwenk h
al

learning phrase representations using rnn encoder decoder for statistical machine translation
arxiv preprint

choudhary s
srivastava p
ungar l
and sedoc j

domain aware neural dialog system
arxiv preprint

costa f
ouyang s
dolog p
and lawlor a

automatic generation of natural language explanations
in proceedings of the international conference on intelligent user interfaces companion
dong l
huang s
wei f
lapata m
zhou m
and xu k

learning to generate product reviews from attributes
in proceedings of the conference of the european chapter of the association for computational linguistics volume long papers
eric m
and manning c
d

key value retrieval networks for task oriented dialogue
arxiv preprint

erkan g
and radev d
r

lexpagerank prestige in document text summarization
in proceedings of the conference on empirical methods in natural language processing
frome a
corrado g
s
shlens j
bengio s
dean j
and mikolov t

devise a deep visual semantic embedding model
in advances in neural information processing systems
gatt a
and krahmer e

survey of the state of the art in natural language generation core tasks applications and evaluation
journal of artificial intelligence research
goodfellow i
pouget abadie j
mirza m
xu b
warde farley d
ozair s
al

generative adversarial nets
in advances in neural information processing systems
huang e
h
socher r
manning c
d
and ng a
y

improving word representations via global context and multiple word prototypes
in proceedings of the annual meeting of the association for computational linguistics long papers volume
jaech a
and ostendorf m

improving context aware language models
joshi c
k
mi f
and faltings b

personalization in oriented dialog
arxiv preprint

kiros r
salakhutdinov r
and zemel r

multimodal neural language models
in international conference on machine learning
kottur s
wang x
and carvalho v

exploring personalized neural conversational models
in ijcai
kulkarni g
premraj v
dhar s
li s
choi y
berg a
c
al

baby talk understanding and generating image descriptions
in proceedings of the cvpr
kusner m
j
and hernndez lobato j
m

gans for sequences of discrete elements with the gumbel softmax distribution
arxiv preprint

lei z
noroozi v
and yu p
s

joint deep modeling of users and items using reviews for recommendation
in tenth acm international conference on web search data mining
li j
galley m
brockett c
spithourakis g
p
gao j
and dolan b

a persona based neural conversation model
arxiv preprint

li j
monroe w
ritter a
galley m
gao j
and jurafsky d

deep reinforcement learning for dialogue generation
arxiv preprint

li j
monroe w
shi t
jean s
ritter a
and jurafsky d

adversarial learning for neural dialogue generation
arxiv preprint

lipton z
c
vikram s
and mcauley j

generative concatenative nets jointly learn to write and classify reviews
arxiv preprint

liu b
xu z
sun c
wang b
wang x
wong d
f
al

content oriented user modeling for personalized response ranking in chatbots
ieee acm transactions on audio speech and language processing taslp
lowe r
pow n
serban i
and pineau j

the ubuntu dialogue corpus a large dataset for research in unstructured turn dialogue systems
arxiv preprint

luan y
brockett c
dolan b
gao j
and galley m

multi task learning for speaker role adaptation in neural conversation models
arxiv preprint

luo l
huang w
zeng q
nie z
and sun x

learning personalized end to end goal oriented dialog
arxiv preprint

ma l
lu z
and li h

learning to answer questions from image using convolutional neural network
in thirtieth aaai conference on artificial intelligence
malinowski m
rohrbach m
and fritz m

ask your neurons a deep learning approach to visual question answering
international journal of computer vision
mao j
xu w
yang y
wang j
and yuille a
l

explain images with multimodal recurrent neural networks
arxiv preprint

mcauley j
and leskovec j

hidden factors and hidden topics understanding rating dimensions with review text
in proceedings of the acm conference on recommender systems
mikolov t
karafit m
burget l
ernock j
and khudanpur s

recurrent neural network based language model
in speech eleventh annual communication association
conference of international the mitchell m
han x
dodge j
mensch a
goyal a
berg a
et al

midge generating image descriptions from computer vision detections
in proceedings of the conference of the european chapter of the association for computational linguistics
mo k
zhang y
li s
li j
and yang q

personalizing a dialogue system with transfer reinforcement learning
in second aaai conference on artificial intelligence
nallapati r
zhou b
gulcehre c
and xiang b

abstractive text summarization using sequence to sequence rnns and beyond
arxiv preprint

nenkova a
vanderwende l
and mckeown k

a compositional context summarizer exploring the factors that influence summarization
in proceedings of the annual international acm sigir conference on research and development in information retrieval
sensitive multi document ni j
and mcauley j

personalized review generation by expanding phrases and attending on aspect aware representations
in proceedings of the annual meeting of the association for computational linguistics volume short papers
noh h
hongsuck seo p
and han b

image question answering using convolutional neural network with dynamic parameter prediction
in proceedings of the ieee conference on computer vision and pattern recognition
oremus w

the first news report on the la earthquake was written by a robot
slate
in
pasunuru r
and bansal m

multi reward reinforced summarization with saliency and entailment
arxiv preprint

paulus r
xiong c
and socher r

a deep reinforced model for abstractive summarization
arxiv preprint

qian q
huang m
zhao h
xu j
and zhu x

assigning personality profile to a chatting machine for coherent conversation generation
in ijcai
radford a
jozefowicz r
and sutskever i

learning to reviews and discovering sentiment
arxiv preprint generate

ren m
kiros r
and zemel r

exploring models and data for image question answering
in advances in neural information processing systems
ritter a
cherry c
and dolan w
b

data driven response generation in social media
in proceedings of the conference on empirical methods in natural language processing
rush a
m
chopra s
and weston j

a neural attention model for abstractive sentence summarization
arxiv preprint

sharma v
sharma h
bishnu a
and patel l

cyclegen cyclic consistency based product review generator from attributes
in proceedings of the international conference on natural language generation
socher r
ganjoo m
manning c
d
and ng a

zero shot learning through cross modal transfer
in advances in neural information processing systems
sordoni a
galley m
auli m
brockett c
ji y
mitchell m
et al

a neural network approach to context sensitive generation of conversational responses
arxiv preprint

srivastava n
and salakhutdinov r
r

multimodal learning with deep boltzmann machines
in advances in neural information processing systems
svore k
vanderwende l
and burges c

enhancing document summarization by combining ranknet and third party sources
in proceedings of the joint conference on empirical methods in natural language processing and computational natural language learning emnlp conll
tang j
yang y
carton s
zhang m
and mei q

context aware natural language generation with recurrent neural networks
arxiv preprint

tian z
yan r
mou l
song y
feng y
and zhao d

how to make context more useful an empirical study on aware neural conversational models
in proceedings of the annual meeting of the association for computational linguistics volume short papers
tintarev n
and masthoff j

effective explanations of recommendations user centered design
in proceedings of the acm conference on recommender systems
vinyals o
and le q

a neural conversational model
arxiv preprint

vinyals o
toshev a
bengio s
and erhan d

show and tell a neural image caption generator
in proceedings of the ieee conference on computer vision and pattern recognition
wang k
and wan x

sentigan generating sentimental texts via mixture adversarial networks
in ijcai
wen t

vandyke d
mrksic n
gasic m
rojas barahona l
m
su p

al

a network based end to end trainable task oriented dialogue system
arxiv preprint

xing c
wu w
wu y
liu j
huang y
zhou m
al

topic aware neural response generation
in thirty first aaai conference on artificial intelligence
yan r
song y
and wu h

learning to respond with deep neural networks for retrieval based human computer conversation system
in proceedings of the international acm sigir conference on research and development in information retrieval
yang m
zhao z
zhao w
chen x
zhu j
zhou l
al

personalized response generation via domain adaptation
in proceedings of the international acm sigir conference on research and development in information retrieval
yang z
he x
gao j
deng l
and smola a

stacked attention networks for image question answering
in proceedings of the ieee conference on computer vision and pattern recognition
yu l
zhang w
wang j
and yu y

seqgan sequence generative adversarial nets with policy gradient
in thirty first aaai conference on artificial intelligence
zang h
and wan x

towards automatic generation of product reviews from aspect sentiment scores
in proceedings of the international conference on natural language generation
zhang s
dinan e
urbanek j
szlam a
kiela d
and weston j

personalizing dialogue agents i have a dog do you have pets too arxiv preprint

zhang y
gan z
and carin l

generating text via adversarial training
in nips workshop on adversarial training
zhou x
dong d
wu h
zhao s
yu d
tian h
al

multi view response selection for human computer conversation
in proceedings of the conference on empirical methods in natural language processing
zhou x
li l
dong d
liu y
chen y
zhao w
x
al

multi turn response selection for chatbots with deep attention matching network
in proceedings of the annual meeting of the association for computational linguistics volume long papers
zhu y
groth o
bernstein m
and fei fei l

grounded question answering in images
in proceedings of the ieee conference on computer vision and pattern recognition

