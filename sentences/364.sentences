dialogue discourse aware graph convolutional networks for abstractive meeting summarization xiachong feng xiaocheng feng bing qin xinwei geng ting liu research center for social computing and information retrieval harbin institute of technology china xiachongfeng xcfeng qinb xwgeng
hit
edu
abstract sequence to sequence methods have achieved promising results for textual abstractive ing summarization
different from documents like news and scientic papers a meeting is naturally full of dialogue specic structural formation
however previous works model a meeting in a sequential manner while ing the rich structural information
in this per we develop a dialogue discourse aware graph convolutional networks dda gcn for meeting summarization by utilizing logue discourse which is a dialogue specic structure that can provide pre dened tic relationships between each utterance
we rst transform the entire meeting text with dialogue discourse relations into a discourse graph and then use dda gcn to encode the semantic representation of the graph
finally we employ a recurrent neural network to in addition we utilize erate the summary
the question answer discourse relation to struct a pseudo summarization corpus which can be used to pre train our model
mental results on the ami dataset show that our model outperforms various baselines and can achieve state of the art performance
c e d l c
s c v
v i x r a figure an example of meeting with its sponding summary where id is short for industrial designer ui for user interface pm for project manager
summary generated by sentence gated goo and chen which models utterances tially
summary generated by our dda gcn which models utterances by incorporating dialogue discourse
introduction automatic summarization is a fundamental task in natural language generation and computational guistics
it is crucial to help the user quickly read and understand daily events and has been uously studied for decades
paice kupiec et al

in this paper we focus on meeting summarization which is an extensively studied task in the eld of automatic summarization
given tiple speakers and corresponding utterances in text the task calls for generating a shorter transcript covering salient information of the entire meeting
an example is shown in figure which includes speakers and their utterances and as well as a human written summary
meeting summarization is typically regarded as a kind of abstractive summarization problem in the literature
the majority of existing studies build summarization systems based on the sequence sequence model which adopts a sequence eling strategy for encoding utterances goo and chen ganesh and dingliwal liu et al
li et al
zhu et al

despite id what about if we had like power cradle ui you could have some neat little sexy design for the cradle and the remote itself
pm it would increase the cost
pm we have to change the end cost
parts of the meetingground truth summarythe industrial designer proposed including a battery charging stand with the device but it was decided that it was not a useful feature
sentence gatedthe industrial designer research the necessary of power cradle for the device
gcnthe industrial designer presented the design for power cradle but the project manager decided it was not useful
the effectiveness of these approaches they typically only use sequential text information while ing the important inuences of dialogue structure
we claim that dialogue specic structural tion is important for meeting summarization
for example dialogue discourse is an effective tural feature
as shown in figure contrast question answer and continuation are three dialogue discourse relations which can provide more precise semantic relationships between each utterance
specically we can see that the existing sequence modeling method is unable to generate correct summary results shown in figure which can be attributed to the system not knowing the and are opposed to the s proposal
differently the dialogue discourse can provide this key information via labeling the contrast tionship as shown in figure
accordingly how to effectively integrate the discourse ship into the existing summarization model become a crucial step in meeting summarization
in this paper we propose dialogue aware graph convolutional networks gcn to address this problem
in detail we rst convert the entire meeting with dialogue discourse labeling into a discourse graph which represents both utterances and discourse relationships as tices
afterwards we additionally design six types of directed edges and one global vertex in the course graph to facilitate information ow
nally we employ a graph convolutional network schlichtkrull et al
to encode the graph and pass the semantic representation to the rnn decoder
besides we further use the answer discourse relationship to construct a summarization corpus for pre training dda gcn
in a conversation a question often sparks a sion so naturally the question can be used as a pseudo summary for subsequent discussions
we conduct experiments on the widely used ami benchmark carletta et al

our proach outperforms various baselines
moreover we analyze the effectiveness of dialogue discourse and pseudo summarization corpus
in the end we give a brief summary of our contributions to the best of our knowledge we are the rst to ply dialogue discourse to model the structure of a meeting for meeting summarization we design a discourse aware graph model to encode the entire meeting our model achieves a new sota on the ami dataset
figure levi graph transformation background in this section we rst describe the task tion for meeting summarization then give a brief introduction to dialogue discourse and levi graph

task denition meeting summarization task aims at producing a summary y for the input meeting u where u consists of utterances


and y consists of words



the i terance of the meeting can be represented as a quence of words ui


where ui j denotes the j th word of i th utterance
each utterance ui associates with a speaker pi p p being a set of participants

dialogue discourse dialogue discourse indicates relations between discourse units in a conversation utterances in a meeting
different from constituency based discourse structures such as rhetorical structure theory rst mann and thompson this dependency based structure allows relations tween non adjacent utterances which is applicable for multi party conversions
there are discourse relations in total comment clarication question elaboration acknowledgment continuation nation conditional question answer alternation question elaboration result background tion correction parallel and contrast

levi graph a graph can be transformed into the levi graph levi gross et al
by turning labeled edges into additional vertices
let g v e r denote a directed graph the corresponding levi graph is dened as gl vl el rl where vl v e
in order to facilitate information ow over the graph previous works marcheggiani and titov beck et al
dene three types of edges reverse self which refer to the original forward direction new reverse tion and self loop direction separately
with this approach we can encode utterances and relations in the same way
an example is shown in figure
vivjrvivjrdefaultreverseselflevi graphoriginal graph figure illustration of discourse graph
discourse graph construction in this section we rst give the notation for course graph and then describe the details of our discourse graph construction process
an example of discourse graph is shown in figure

notation let gd vd ed rd denote a discourse graph with vertices vi vd and labeled edges vi r vj ed where r rd is the relation type of the edge
comes default in discourse default out discourse reverse in discourse reverse out discourse global self
dialogue discourse aware graph convolutional networks in this section we describe the details of our logue discourse aware graph convolutional works dda gcn which consists of three nents an utterance encoder a graph encoder and a pointer decoder
the model is shown in figure

discourse graph
vertex representation given a meeting and its corresponding dialogue discourse we construct discourse graph based on the levi graph transformation described in tion

we now have two types of vertices cluding utterance vertices and edge vertices
for example an edge continuation in the original graph becomes default tion and continuation default in the levi graph
note that different types of vertices may have different features which fall in different space beck et al

specically previous works ignore the type of source and target vertices and use the same type of edge to pass information such as default which may reduce the ness of discourse information
to this end we propose our discourse graph which transforms the default edge into default in discourse and out discourse edges shown in figure and the reverse edges into reverse in discourse and reverse out discourse edges shown in figure
furthermore to aggregate non local information a global vertex is added which connects all tices by global edges and will be used to ize the decoder shown in figure
finally there are six types of relations where rd for global vertex and relation vertices we obtain the initial representation i by looking up from an embedding table
for utterance vertices we ploy a bilstm as the utterance encoder which updates its hidden state upon each received word sequentially
as hi j bilst m hi j where hi j and ei j denote the hidden state and the embedding of word ui j respectively
in a party conversation the speaker additionally plays an important part so we encode speaker pi using a one hot vector and get ei j by concating the responding word embedding and one hot speaker embedding
the concatenation of the forward and backward nal hidden states in the utterance coder indicated as the representation i which is used as input to the graph encoder

graph encoder after getting the initial feature i of each vertex vi vd we feed them into the graph encoder to digest the structural information
we use tional graph convolutional networks schlichtkrull et al
to capture high level hidden features considering different types of edge
the lution computation for vertex vi at the l answercontinuationexplanationdefault discoursedefault discoursereverse discoursereverse discourseggg figure illustration of our dda gcn model
the utterance encoder encodes each utterance in a meeting into hidden vectors
the graph encoder performs convolutional computation over the discourse graph
the pointer decoder attends to the updated utterance representations and the word representations to generate the summary words either from the xed length vocabulary or copy from the input
layer takes the representation at the l th layer as input can be dened as i relu w l r j r i where nr vi denotes the set of neighbors of tex vi under relation r and w l r denotes specic learnable parameters at the l th layer
however uniformly accepting information from different discourse relations is not suitable for tifying important discourse
as shown in figure contrast is more important than answer
thus we use the gate mechanism marcheggiani and titov to control the formation passing j sigmoid w l r g j where w l r g denotes a learnable parameter under relation type r at the l th layer
equipped with the gate mechanism the convolution computation can be dened as i relu w l r j j r i
pointer decoder we use a standard lstm decoder with attention and copy mechanism to generate the summary bahdanau et al
see et al

the global representation described in section
is used to initialize the decoder
at each step t the coder receives the word embedding of the previous word and has decoder state st
the attention bution is calculated as in luong et al

we consider both word level attention and level attention
the word level context vector hwl t is computed as i j i j et at i at hwl t i j i j where wa is a learnable parameter and i j is obtained from utterance encoder for ui j
the utterance level context vector hul is calculated t ilarly to the word level context vector except that we use the nal outputs of the graph encoder i which represent utterances to calculate the attention distribution
the nal context vector is the nation of word level and utterance level context vector h which is then used to late generation probability and the nal probability distribution see et al

t hwl t
training objective we use maximum likelihood training to train our model
given the ground truth summary y y for an input meeting u
we


y y and copyutterance encodergraph encoderpointer decoderfinal attentionutterance experiments dataset we evaluate our model on the ami ing dataset carletta et al
in which the participants play different roles in a design team including a project manager a marketing expert an industrial designer and a user interface designer
they take a remote control design project from kick off to completion
we preprocess and divide the dataset into training meetings ment meetings and test meetings sets as done by shang et al

since the current meeting summarization dataset has no dialogue course annotation we get the dialogue discourse for one meeting based on deep sequential shi and huang a sota dialogue discourse parser trained on the stac corpus asher et al

implementation details for our model the mension of hidden states is set to for encoder and decoder and both use dimensional glove vectors pennington et al
which are updated during training
we train using adam kingma and ba with the learning rate of
and using gradient clipping with a maximum gradient norm of
the vocabulary size is set to and dropout rate is set to

in test process beam size is set to
for pre training we stop training until the model converges on the pseudo data
for discourse parser we use the default rameters except that vocabulary size is set to
evaluation metrics we adopt the standard metrics rouge lin for evaluation and obtain the scores for and l that measures the word overlap bigram overlap and longest common sequence between the truth and the generated summary respectively
the package is used to evaluate the mance of the model
baseline models we compare our model with eral baselines including extractive and abstractive methods
textrank mihalcea and tarau is a graph based extractive method that selects portant sentences from the input document
cheng and lapata is an extractive method based on sequence to sequence framework
its decoder receives sentence embeddings and puts sentence labels
summarunner nallapati et al
is an extractive method based on erarchical rnn which iteratively constructs mary representation to predict sentence labels

com shizhouxing dialoguediscourseparsing
python
org pypi

figure an instance of our pseudo summarization data samples
mize the negative log likelihood of the target words sequence l log p t


y u pre training in this section we introduce how to struct pseudo summarization corpus based on the question answer discourse relation which can be used to pre train our model
given a meeting and its corresponding discourse relations we nd a question often sparks a cussion
as shown in figure the user interface asked what s the standard colour other ipants start to discuss this small topic
thus we can view the discussion as a small meeting and the question as a pseudo summary of this small meeting
according to this observation we collect pseudo summarization data from the meeting marization dataset where the question identied by dialogue discourse serves as the pseudo summary and n utterances in experiments n after the question serve as the pseudo input
in detail there are some uninformative and normal questions such as what is this here which are not suitable for pseudo summarization corpus construction
thus we lter out questions that contain no noun and adjective to make the pseudo data cleaner
given this pseudo corpus we rst pre train our model and then ne tune it on the meeting marization dataset
our motivations are two fold we can potentially augment the training data since the pseudo data is constructed from the meeting summarization dataset the pre training can give the model a warm start
we have different colour
is there standard colour you got different colours but you should have standard colour
how many colours are we going to black
you should have black one because black is standard
black is the standard
with the yellow just regular remote colour
but if you want to be different then silver
dark grey something like this colour
and then have different covers to use
or silver
it better to have silver nowadays
you see more silver than black
me id ui me pm me ui pm me ui me me mewhat the standard colour ui meetingpseudo summarywhat the standard colour uiwe have different colour
is there standard colour you got different colours but you should have standard colour
how many colours are we going to black
you should have black one because black is standard
black is the standard
with the yellow just regular remote colour
but if you want to be different then silver
dark grey something like this colour
and then have different covers to use
or silver
it better to have silver nowadays
you see more silver than black
me id ui me pm me ui pm me ui me me mepseudo inputdiscussion about colorquestion about color erank shang et al
is a unsupervised stractive method which generates summaries by combining several approachs
pointer generator see et al
is an abstractive method equips with copy mechanism its decoder can either erate from the vocabulary or copy from the put
hred serban et al
is a hierarchical sequence to sequence model which is composed of a word level lstm and a sentence level lstm
sentence gated goo and chen is an stractive method that incorporates dialogue acts by the sentence gated mechanism
topicseg li et al
is an abstractive method using a archical attention mechanism at three levels topic utterance word
hmnet zhu et al
is an abstractive method that incorporates part of speech and entity information while pretraining on scale news summary data then ne tuning it on meeting datasets

automatic evaluation the dda gcn in table stands for the model that is based on the discourse graph and pre trained using pseudo summarization data
the dda gcn pre train stands for the model that is rectly trained using the meeting summarization dataset
the dda gcn pre stands for the model that is based on the levi graph and without pre training
the dda gcn zero shot means directly testing the pre trained model on the meeting summarization test set
we can see that our model dda gcn outperforms various baselines
compared with dda gcn our model dda gcn pre train achieves better performance which indicates the effectiveness of taking the type of source and target vertices into account
by pre training on summarization data our model dda gcn can further boost the performance by a large margin and achieves
improvement on
on
on rouge l pared with dda gcn pre
ditionally dda gcn zero shot still achieves a basic effect which indicates the effectiveness of our pseudo data
model textrank summarunner corerank pointer generator hred sentence gated topicseg hmnet dda shot dda gcn pre train pre



























r l












table test set results on the ami meeting dataset ing rouge where is short for for r l for rouge l
dg is short for discourse graph

human evaluation to further assess the quality of the generated maries we conduct a human evaluation study paring summaries generated by our model and lines
we choose three metrics uency flu no grammatical problems relevance rel consistent with the original input informativeness info preserves the meaning expressed in the truth
we hired six graduates who passed mediate english test and are familiar with summarization tasks to perform the human tion
they were asked to rate each summary on a scale of worst to best for each metric
the results are shown in table
model flu rel info ground truth


summarunner pointer generator sentence gated dda gcn pre train














table human evaluation results
et al
also proposed a model named by incorporating vision features in a multi modal setting
in this paper we compare our model with baselines using only textual features
our model increases the performance on tomatic evaluation metrics entails better human evaluation scores
we can see that our method achieves higher scores in all three evaluation rics compared with other baselines
we can see that dda gcn has a signicant improvement in informativeness which indicates pre training can help our model select more key information
ever dda gcn pre train scored better than dda gcn on uency
we argue that due to the ference between pseudo summary and the ground truth summary pre training may have some impact on the language model on the decoder side
ground truth is still obtained signicant high scores pare with model generated summaries indicating the challenge of this task

analysis effect of dialogue discourse to verify the effectiveness of dialogue discourse we randomly provide parts of the discourse lations to the model in the test process
the sults are shown in figure
we can see that the more dialogue discourse the higher the rouge l score which indicates discourse can do good to summary generation
when given no discourse formation our model gets similar scores compared with hred serban et al
which models utterances sequentially
figure rouge l score with respect to the age of dialogue discourse
furthermore to verify the importance of each discourse relation type we then test our model by giving different discourse relations
the results are shown in figure
we can see that contrast and acknowledgement are more important than other discourse relations we attribute this to the fact that these two relations strongly indicate the change of viewpoint which have inuences on meeting information ow

analysis effect of pseudo summarization data to further study the effectiveness of summarization data instead of using questions figure rouge l score with respect to dialogue course relations
model r l pre train rule based discourse based








table the results of pre training the dda gcn on different types of pseudo summary data
identied by dialogue discourse as pseudo maries we extract questions following two rules utterances which begin with wh words such as what who and why utterances which end with a question mark
we call the pseudo data discourse based and rule based according to the way the question was obtained
we rst pre train our model on the two types of pseudo data arately and then ne tune it on the ami dataset
the results are shown in table
we can see that pre training on discourse based data is better than rule based data demonstrating the effectiveness of dialogue discourse
besides pre training on rule based data still achieves a good result which indicates the rationality of our pre training strategy

analysis case study table shows an example summary generated by different models and the visualization of ance attention weights when generating this mary
the darker the color the higher the weight
sentence gated goo and chen focuses more on the second utterance than the third terance shown in table
the second one mainly talks about fruit shape which leads to the omission of the keyword vegetable
differently we can observe that our model pays more tion to the rst and third utterances which form a




lpercentage of dialogue discourserouge




elaborationcontinuationnarrationquestion answer correctionelaborationclarification questionparallelresultalternation ground truth the marketing expert presented trends in the remote control market and the fruit and vegetable and spongy material trends in fashion
pointer generator they discussed the possibility of a fruit or fruit and fruit
sentence gated the need to incorporate a fruit theme into the design of the remote
the buttons will be included in a fruit and vegetable theme into the shape of the remote control
dda gcn table example summaries generated by different models and utterance attention weights visualization of a dda gcn and sentence gated
contrast discourse structure shown in table
they both mention fruit and vegetables which help our model generate the correct summary that contains both keywords
related work meeting summarization previous works focused on extractive meeting summarization xie et al

a recent study shows that for meeting marization people prefer abstract summaries to extracted ones murray et al

shang et al
proposed a unied framework for fully pervised abstractive meeting summarization
goo and chen incorporated dialogue acts which indicate the effect of utterances
ganesh and wal used crf to tag each utterance a course label and then remove utterances that do not contribute to the meeting based on rules
liu et al
incorporated topic information which serves as a coarse grained structure of the meeting
li et al
proposed a hierarchical model der the multi modal setting by incorporating vision features
liu et al
generated a summary in a two stage manner by rst producing a sequence of keywords and then a full summary
zhu et al
made use of large scale news datasets to rst pretrain the model and then ne tune it it on ing dataset
koay et al
revealed domain terminology has a substantial impact on meeting summarization performance
in this paper we rst propose to transform the utterances of a meeting into a graph via dialogue discourse
graph to sequence generation recent research efforts for text generation consider utilizing graph neural networks gnn to better model structured data such as amr beck et al
ribeiro et al
sql xu et al
and edge graph koncel kedziorski et al

ditionally there are many works employed gnn in non structural scenarios such as summarization yasunaga et al
fernandes et al
tognini and faltings and comment tion li et al
by transforming the input into a meaningful graph
we propose the discourse graph to facilitate information ow over the graph
conclusions in this paper we apply the dialogue discourse to model the structure of a meeting for meeting marization
we rst transform the entire meeting text and corresponding dialogue discourse relations into a discourse graph
specically both the ances and discourse relations are constructed as tices and we design six types of edge and a global vertex to facilitate the information ow
moreover we develop a dialogue discourse aware graph convolutional networks dda gcn which sists of an utterance encoder a graph encoder and a pointer decoder
in addition we construct a summarization corpus by utilizing the answer discourse relation which can be used to train our model
experiments on the ami dataset show the effectiveness of our model which can achieve the sota performance
the fashion trends are that people want sort of clothes and shoes and things with fruit and vegetables theme
if you start making the buttons fruit shaped it might make it more complicated to use
fruit and vegetables may be popular at the moment but as we know how fickle the fashion markets are
it just seems realistic that the remote control market is the thing which takes in those kinds of fashion trends
marketing expertuser interfaceproject managerproject references diego antognini and boi faltings

learning to create sentence semantic relation graphs for in proceedings of the document summarization
workshop on new frontiers in summarization pages hong kong china
association for computational linguistics
nicholas asher julie hunter mathieu morey farah benamara and stergos afantenos

discourse structure and dialogue acts in multiparty dialogue the stac corpus
dzmitry bahdanau kyunghyun cho and yoshua gio

neural machine translation by jointly arxiv preprint learning to align and translate


daniel beck gholamreza haffari and trevor cohn
graph to sequence learning using gated
the graph neural networks
annual meeting of the association for putational linguistics volume long papers pages melbourne australia
association for computational linguistics
in proceedings of jean carletta simone ashby sebastien bourban mike flynn mael guillemot thomas hain jaroslav kadlec vasilis karaiskos wessel kraaij melissa kronenthal al

the ami meeting corpus a pre announcement
in international workshop on machine learning for multimodal interaction pages
springer
jianpeng cheng and mirella lapata

neural marization by extracting sentences and words
in proceedings of the annual meeting of the sociation for computational linguistics volume long papers pages berlin germany
sociation for computational linguistics
patrick fernandes miltiadis allamanis and marc brockschmidt

structured neural tion
arxiv preprint

prakhar ganesh and saket dingliwal

tive summarization of spoken and written tion
arxiv preprint

chih wen goo and yun nung chen

stractive dialogue summarization with gated modeling optimized by dialogue acts
in ieee spoken language technology workshop slt pages
ieee
jonathan l gross jay yellen and ping zhang

handbook of graph theory
chapman and hall crc
diederik p kingma and jimmy ba

adam a method for stochastic optimization
arxiv preprint

jia jin koay alexander roustai xiaojin dai dillon burns alec kerrigan and fei liu

how domain terminology affects meeting summarization in proceedings of the performance
national conference on computational linguistics pages barcelona spain online
national committee on computational linguistics
rik koncel kedziorski dhanush bekal yi luan mirella lapata and hannaneh hajishirzi

text generation from knowledge graphs with in proceedings of the graph transformers
conference of the north american chapter of the association for computational linguistics human language technologies volume long and short papers pages minneapolis minnesota
association for computational linguistics
julian kupiec jan pedersen and francine chen

a trainable document summarizer
advances in tomatic summarization pages
friedrich wilhelm levi

finite geometrical tems six public lectues delivered in february at the university of calcutta
the university of cutta
manling li lingyu zhang heng ji and richard j
radke

keep meeting summaries on topic abstractive multi modal meeting summarization
in proceedings of the association for computational linguistics pages florence italy
association for tational linguistics
the annual meeting of wei li jingjing xu yancheng he shengli yan fang wu and xu sun

coherent ments generation for chinese articles with a to sequence model
in proceedings of the nual meeting of the association for computational linguistics pages florence italy
ciation for computational linguistics
chin yew lin

rouge a package for matic evaluation of summaries
in text tion branches out pages barcelona spain
association for computational linguistics
chunyi liu peng wang jiang xu zang li and jieping ye

automatic dialogue summary generation for customer service
in proceedings of the acm sigkdd international conference on knowledge discovery data mining pages
acm
zhengyuan liu angela ng sheldon lee ai ti aw and nancy f chen

topic aware generator networks for summarizing spoken sations
arxiv preprint

minh thang luong hieu pham and christopher d manning

effective approaches to based neural machine translation
arxiv preprint

william c mann and sandra a thompson

rhetorical structure theory toward a functional ory of text organization
text interdisciplinary nal for the study of discourse
guokan shang wensi ding zekun zhang toine tixier polykarpos meladianos michalis giannis and jean pierre

vised abstractive meeting summarization with sentence compression and budgeted submodular the maximization
nual meeting of the association for computational linguistics volume long papers pages melbourne australia
association for tational linguistics
in proceedings of zhouxing shi and minlie huang

a deep tial model for discourse parsing on multi party logues
in proceedings of the aaai conference on articial intelligence volume pages
shasha xie yang liu and hui lin

ating the effectiveness of features and sampling in in ieee extractive meeting summarization
spoken language technology workshop pages
ieee
kun xu lingfei wu zhiguo wang yansong feng and vadim sheinin

sql to text generation in proceedings of with graph to sequence model
the conference on empirical methods in ural language processing pages sels belgium
association for computational guistics
michihiro yasunaga rui zhang kshitijh meelu ayush pareek krishnan srinivasan and dragomir radev

graph based neural multi document summarization
in proceedings of the ence on computational natural language learning conll pages vancouver canada
association for computational linguistics
chenguang zhu ruochen xu michael zeng and dong huang

a hierarchical network for stractive meeting summarization with cross domain pretraining
in proceedings of the conference on empirical methods in natural language ing findings pages
diego marcheggiani and ivan titov

coding sentences with graph convolutional works for semantic role labeling
arxiv preprint

rada mihalcea and paul tarau

textrank bringing order into text
in proceedings of the conference on empirical methods in natural guage processing pages barcelona spain
association for computational linguistics
gabriel murray giuseppe carenini and raymond ng

generating and validating abstracts of ing conversations a user study
in proceedings of the international natural language generation conference pages
association for tational linguistics
ramesh nallapati feifei zhai and bowen zhou

summarunner a recurrent neural network based quence model for extractive summarization of ments
chris d paice

constructing literature abstracts by computer techniques and prospects
information processing management
jeffrey pennington richard socher and christopher manning

glove global vectors for word resentation
in proceedings of the conference on empirical methods in natural language ing emnlp pages doha qatar
ciation for computational linguistics
leonardo f
r
ribeiro claire gardent and iryna gurevych

enhancing amr to text in tion with dual graph representations
ings of the conference on empirical methods in natural language processing and the national joint conference on natural language cessing emnlp ijcnlp pages hong kong china
association for computational guistics
michael schlichtkrull thomas n kipf peter bloem rianne van den berg ivan titov and max welling

modeling relational data with graph tional networks
in european semantic web ence pages
springer
abigail see peter j
liu and christopher d
manning

get to the point summarization with generator networks
in proceedings of the nual meeting of the association for computational linguistics volume long papers pages vancouver canada
association for tional linguistics
iulian v serban alessandro sordoni yoshua bengio aaron courville and joelle pineau

building end to end dialogue systems using generative archical neural network models
in thirtieth aaai conference on articial intelligence

