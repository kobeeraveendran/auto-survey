tone biased mmr text summarization chaudhari nelson mattukoyya graduate student of computer science information systems pilani k
k
birla goa campus goa india abstract text summarization is an interesting area for researchers to develop new techniques to provide human like summaries for vast amounts of information
summarization techniques tend to focus on providing accurate representation of content and often the tone of the content is ignored
tone of the content sets a baseline for how a reader perceives the content
as such being able to generate summary with tone that is appropriate for the reader is important
in our work we implement maximal marginal relevance mmr based multi document text summarization and propose a nave model to change tone of the summarization by setting a bias to specific set of words and restricting other words in the summarization output
this bias towards a specified set of words produces a summary whose tone is same as tone of specified words
keywords text summarization maximal marginal relevance tone bias
i
introduction with vast amounts of information being generated every day automated text summarization is used to represent such vast information in compact form
there are many techniques that have been developed over recent times to improve the accuracy of summary and provide summaries that are human like
various features of sentences are used to rank the sentences which should be included in the summary
the top ranked sentences are refined and reordered to form a coherent summary
feature based ranking means that we try improving the query relevance of the sentences selected for summarization but this in turn might increase redundancy in summary as many query relevant sentences may have similar content
in our work we implemented maximal marginal relevance mmr based text multi document summarization which along with sentence ranking considers the novelty of the sentence there by reducing the redundancy in final summarization
current summarization techniques try to make sure that all information in content is accurately represented in the summary
such summarization gives us accurate representation of data however the summarization is nt human like
daily life human summaries usually tend to be customized for reader by using bias in the tone of summarization
we propose a nave model for biasing the tone of summary by using a set of words which have defined polarity tag to decide whether to include or discard the sentence in the summary
the rest of the paper is organized as follows section ii gives an overview of current work done in the proposed area
section iii discusses our approach and implementation of mmr multi document text summarization and nave tone biasing
section iv briefly discusses the applications of tone biasing
section v we discuss the results and observations obtained by implementing our approach
section vi describes future scope of our work and conclusions
ii
literature survey the ability to get deeper insights without having to manually read through huge amount of data has fueled research in field of text summarization
carbonell and goldstein proposed maximal marginal relevance and discussed the mmr based text summarization in detail
long et al
discuss ways to apply learning models for optimizing diversity evaluation measure in training and proposes a novel modelling approach perceptron
xie and liu compare the various knowledge based similarity measures that can be used with mmr in summarization of large corpus of meeting recording according to their experimental rogue scores
radev et al
present a multi document summarizer mead which uses cluster centroids produced by topic detection and tracking to generate summaries
goldstein et al
propose a new approach based on domain independent techniques for document text summarization which has few operations based on single document text summarization
yulita and pribadi implemented with simple modifications such as using tf idf df for ranking sentences
yadav and chatterjee discuss the application of sentiment analysis for text summarization using various summary techniques and compared them
gupta et al
surveyed the existing text summarization methods which integrate well with ml and ai techniques for sentiment analysis for online product reviews
we see that most works in mmr text summarization work on augmenting mmr with other algorithms to improve accuracy
iii
approach our approach starts by data preprocessing then using mmr to fetch relevant sentences and remove redundancy to improve novelty
now we have a content accurate novel text summary of the document but the tone of the summary just reflects the tone of the content
we bias the tone of the generated summary using proposed nave approach
following subsections describe the above mentioned approach step by step a
preprocessing firstly we must clean our data set to be able to run our algorithms efficiently and remove any unwanted unnecessary data
in this step we scan through all the documents to remove stop words and xml tags which are unnecessary while processing
the sentences are classified into separate entities as our next steps will process text as sentences
lastly we reduce the words of each sentence into its stem word using the porter stemming algorithm
b
sentence ranking using tf idf values tf idf values give us the relevance of a sentence with respect to the query vector so that we can pick the top k relevant sentences
the query vector is generated by finding most frequent words in the document that reflect the subject of document
since our query vector is based on frequent words the tf idf sentence ranking returns the sentences that are most close to the general summary of the document by using cosine similarity
equation for calculation of tf idf is given below log wi j is the weight of the word i in the sentence j
tfi j term frequency is the term frequency of word i in sentence j
is the equation of inverse document frequency idf n is the number of the total sentences
dfi document frequency is the number of sentences which contain the word i
we can also vary the tf idf method to use tf idf df as used in or use other similarity measures like pearson s coefficient but the main objective of our work is to explore tone biasing and evaluate a nave approach
c
maximal marginal relevance the maximal marginal relevance mmr technique tries to reduce the redundancy while maintaining relevance to the query when reordering sentences
we calculate the relevance of the query and sentences using cosine similarity then we calculate the similarity of sentences among themselves and remove the similar sentences to remove redundancy
the below equation shows the mmr

as explained in last subsection is rank of sentence in terms of best word query
is the cosine similarity of current sentence among the list of top n sentences s that we get from
is the tunable parameter which allows the user to tune the mmr equation
value ranges between to where indicated maximum similarity and indicates maximal diversity
mmr works iteratively to get the best possible redundant summary
mmr process stops when becomes less than zero
we start by varying from
to
and observe that we get best accuracy when is between
and
for given dataset
we used dataset for performing the text summarization
d
tone biasing as discussed in the introduction we propose a nave approach for biasing the tone of summary
we use textblob library in python which provides functions to compute the polarity of sentences and tag them as positive negative and neutral
polarity of the sentence is calculated by summing the polarity of words in the sentence
textblob internally does stemming and lemmatization to provide accurate polarity information
we use this method to analyze the polarity of the sentence retrieved after tf idf sentence ranking then if the sentence has negative polarity we discard the sentence
finally only the sentences with either neutral or positive polarity are populated in the n list passed to mmr
this results in mmr producing positive summaries due to the positive bias
we can also flip the tone by simply discarding positive polarity sentences instead of negative polarity sentences
polarity is context sensitive and is referred abstractly in the paper it needs to be explicitly defined by the users according to their use case
users can learn the polarity of the words and define the tone according to the context by tweaking their classifiers used to assign polarity tags in textblob
we ve developed this nave approach as we are exploring the tone biasing approach
we can make this approach more sophisticated and robust by augmenting other text summarization techniques and polarity tagging techniques we discuss a few such approaches in future work section
iv
applications system generated summaries are generally consistent and content accurate
while these are desirable properties in the quest to make summaries as human like as possible we need bias the summary according to our audience
tone biasing has lot of applications where text summaries should be modified to convey the same message in different flavor
we could think of many applications of tone biasing here we present two such examples
censoring content in a graceful manner if a similar content should be displayed to different age groups it might be beneficial to change the tone of summary accordingly
apart from just adding removing sentences based on polarity we can work on an natural language processing approach to use similar words with less negativity as replacement for existing words to improve the polarity of sentence towards required direction
summarizing product service reviews we can summarize reviews positively or critically for different audiences
positive summary could be provided to prospective buyers while critical summary could be provided to backend teams as feedback to improve service
we can also extend this approach by having a multiclass summary instead of just negative or positive summary
the multiple classes would depend on the context and subject of the documents we re trying to summarize
v
result we implemented the mmr approach described by carbonell and goldstein and experimental results were observed to be lower than the original paper as the original paper uses normalized recall and score
we evaluated our implementation using the dataset which contains news articles on various topics
dataset also provides words words and words summary of topics prepared by humans as a benchmark to evaluate the accuracy of our approaches
we use rogue score which is the combination of recall precision and f score to measure the accuracy of our approach
also we ve implemented our nave tone biasing approach for biasing the text and compared it with polarity of non biased summary
we present our observations below figure rouge score of mmr summary of length words figure rouge score of mmr summary of length words figure rouge score of mmr summary of length words figures show plot of the rouge scores for document cluster of documents of dataset
we see that mmr multi document text summarization technique provides an average recall of
average precision of
and average f score of

average rouge score for our implementation of mmr summarization would be



this is lower when compared to the average rouge score of original paper which was


this is as the original paper uses normalized recall and f score to provide better accuracy and use tf idf id instead of tf idf for sentence ranking
in best case scenario our approach has the rogue score of


this is similar to the original paper s rogue score
we also observe that as the length of summary increases from words to words the recall value and score decrease while precision increases
recall value is the total number of correct words that are present in summary versus total correct words as the length of summary increases the probability of fetching incorrect words increases and as the number of sentences we pick to calculate mmr remains same the recall value is impacted
the same goes for f score
however precision is the total number of correct words in the summary versus total words in the summary this means that when summary length increases most correct words could be included in summary this leads to increase in precision value
figure polarity of document clusters with and without tone biasing figure is the plot comparing the polarity of sentences in mmr with nave tone biasing approach indicated in blue color and human summary without any tone bias dataset denoted in red color
mmr was implemented using
and summary chosen for evaluation was word length summary provided in dataset
we see that few documents have negative polarity in human summary and mmr with nave tone biasing successfully changes the polarity of all documents to positive polarity
we can observe that few document clusters find have negative polarity higher than positive polarity this indicates that the document has high negative polarity and converting such document to positive polarity meant dropping lot of negative sentences which results in loss of information
vi
conclusion and future work we observed that mmr based multi document text summarization provides good accuracy and is superior to other approaches removing redundant information in summary
nave tone biasing approach works well but might result in information loss when the tones we do nt need are too high in the document
such loss of information can be mitigated by rephrasing sentences with required tone instead of discarding them this is part of a larger nlp problem and there is future scope for us to work this area
nave tone biasing approach does binary biasing i
e
either positive bias or negative bias and meaning of positive and negative might vary with the subject
as such instead of having a static lexicon of polarity scores of words we need develop a model to dynamically generate a lexicon with polarities according the subject
such a model would give us better accuracy as the lexicon is customized to the subject topic in point
also in some subjects topics it might be better to have multiclass biasing instead of having only two classes
we need to develop a model to identify different classes of information and construct a polarity lexicon
references jaime carbonell and jade goldstein

the use of mmr diversity based reranking for reordering documents and producing summaries
in proceedings of the annual international acm sigir conference on research and development in information retrieval sigir
acm new york ny usa
long xia jun xu yanyan lan jiafeng guo and xueqi cheng

learning maximal marginal relevance model via directly optimizing diversity evaluation measures
in proceedings of the international acm sigir conference on research and development in information retrieval sigir
acm new york ny usa
shasha xie and yang liu using corpus and knowledge based similarity measure in maximum marginal relevance for meeting summarization ieee international conference on acoustics speech and signal processing las vegas nv pp

dragomir r
radev hongyan jing and malgorzata budzikowska

centroid based summarization of multiple documents sentence extraction based evaluation and user studies
in proceedings of the naacl anlp workshop on automatic summarization naacl anlp autosum
association for computational linguistics stroudsburg pa usa
jade goldstein vibhu mittal jaime carbonell and mark kantrowitz

multi document summarization by sentence extraction
in proceedings of the naacl anlpworkshop on automatic summarization volume naacl anlp autosum vol

association for computational linguistics stroudsburg pa usa
yulita w
and pribadi f
s
the implementation of maximum marginal relevance method on online national and local news portal proceeding of international conference on green technology semarang indonesia
n
yadav and n
chatterjee text summarization using sentiment analysis for duc data international conference on information technology icit bhubaneswar pp

p
gupta r
tiwari and n
robert sentiment analysis and text summarization of online reviews a survey international conference on communication and signal processing iccsp melmaruvathur pp


