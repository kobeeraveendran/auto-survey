l u j l c
s c v
v i x r a conducting sparse feature selection on arbitrarily long phrases in text corpora with a focus on interpretability luke miratrix robin ackerman july disclaimer the analyses opinions and conclusions expressed in this paper do not necessarily reect the views of the united states department of labor
note this paper has been accepted to statistical analysis and data mining
please see proofed
version there
abstract we propose a general framework for topic specic summarization of large text pora and illustrate how it can be used for analysis in two quite dierent contexts an osha database of fatality and catastrophe reports to facilitate surveillance for patterns in circumstances leading to injury or death and legal decisions on workers compensation claims to explore relevant case law
our summarization framework built on sparse classication methods is a compromise between simple word frequency based methods currently in wide use and more heavyweight model intensive ods such as latent dirichlet allocation lda
for a particular topic of interest e

mental health disability or carbon monoxide exposure we regress a labeling of uments onto the high dimensional counts of all the other words and phrases in the documents
the resulting small set of phrases found as predictive are then harvested as the summary
using a branch and bound approach this method can be extended to allow for phrases of arbitrary length which allows for potentially rich summarization
we discuss how focus on the purpose of the summaries can inform choices of tuning parameters and model constraints
we evaluate this tool by comparing computational time and summary statistics of the resulting word lists to three other methods in the literature
we also present a new r package textreg
overall we argue that sparse methods have much to oer text analysis and is a branch of research that should be considered further in this context
keywords concise comparative summarization sparse classication regularized sion lasso text summarization text mining key phrase extraction text classication dimensional analysis normalization introduction regularized high dimensional regression can extract meaningful information from large text corpora by producing key phrase summaries that capture how a specic set of documents of interest dier from some baseline collection
this text summarization approach has been called concise comparative summarization ccs underscoring two fundamental tures of this tool the comparison of a class of documents to a baseline or complete set in order to remove generic terminology and characteristics of the overall corpus and the resulting production of a short easy to read summary comprised of key phrases
such summaries can be useful for understanding what makes a document collection distinct and can be used to inform media analysis understand incident reports or investigate trends in legal decisions
many classic methods of text summarization tend to focus on single words or short phrases only
approaches such as latent dirichlet allocation also do not extend naturally to phrases
on the other hand one regression based method that does allow for longer phrases does not allow for rescaling of the counts of phrases in the text based on the overall frequency of appearance of such phrases which can negatively impact the quality of resulting summaries
in this paper we merge two ccs approaches to allow for rescaled arbitrary length key phrases that can include gaps
we briey discuss how this is done below
our new ccs tool be easily used via our new r package textreg which allows for rapid exploration of text corpora of up to a few gigabytes in size
even given these tools when a researcher desires to conduct a specic analysis he or she is faced with many choices
in particular the implementation and regularization of the regression itself can be done in several ways and the impact of choosing among these ways is one of the foci of this paper
in particular we argue that if the researcher has specic goals for interpretation in mind these goals can inform choice of tuning parameters
for example when faced with a corpus where only a few documents are of interest and the rest are to be used as a baseline a researcher may choose to allow only positive weights on phrases in order to simplify interpretation
similarly choice of tuning parameter can be governed by a researcher s level of interest in pruning rare phrases
we also oer a method for testing for a signicant relationship between the text and the labeling that also provides a threshold regularization value
we compare this tool to other related state of the art methods
first we compare to multinomial inverse regression mnir a text regression method that is primarily signed to be distributed across many cores in order to be able to handle massive data
we also compare to a classic linear lasso approach see e

which is similar to this method run on pre computed document term matrices without some of the exibility
we nally compare to the original ifrim et al
method that is one of the building blocks of this work
in these comparisons we investigate computation time prediction accuracy and dierent features of the resulting word lists
the dierent approaches give very dierent types of lists and we hope this work gives some guidance to the practitioner as to how to sort through the options
as a case study we use this tool to examine a large collection of occupational fatality and catastrophe reports generated by the occupational safety and health administration osha in the united states
as a motivating example we examine hazardous exposure to methylene chloride a neurotoxin during bathtub renishing operations
in osha and the national institute for occupational safety and health niosh jointly issued a hazard alert calling attention to a recurring pattern of this nature following the deaths of least workers since in related circumstances
however the sheer volume of information describing occupational fatalities and catastrophes may have initially obscured this pattern in the years preceding its detection
although osha maintains a database of narrative reports describing fatalities and catastrophes fat cats similar patterns of preventable exposure to occupational hazards may be dicult to identify eciently through manual review alone given the large number of narratives in this database
thus using methylene chloride as a case study we consider whether text mining techniques can help identify important patterns in circumstances of hazardous exposures
in our framework a summary list of key words and phrases ideally represents and reveals the overall content of a collection of narrative reports
for example one summary for all narratives related to methylene chloride contained the words bathtub and stripper
to qualitatively evaluate our tool we manually examine these words and phrases in the context of the original reports and consider whether our text summarization tool eectively characterizes the circumstances of the bathroom renishing fatalities
in general we explore whether we can construct text mining algorithms that when applied to an entire corpus can uncover needles in the haystack patterns such as the connection between bathroom renishing and overexposure to methylene chloride
at this stage we are not focused on rates or relative risks of particular patterns we are instead focused on the crude detection of textual patterns that may represent meaningful information about how certain types of injuries and fatalities occur
such ndings even if they involve only a few recorded deaths or injuries may facilitate the prevention of many future fatalities particularly in the context of emerging hazards
we also examine our tool s ability to extract information from a collection of legal cisions from the employees compensation appeals board ecab which handles appeals of determinations of the oce of workers compensation programs owcp in the u
s
department of labor dol
here we investigate what information we can extract about ferent categories of cases
in particular we examine cases involving a question as to whether the work environment caused a mental health condition an emotional condition in the parlance of ecab
we nd that while the ccs tool does extract meaningful information relating to the cases of interest further work needs to be done to obtain more nuanced summaries
overall the ccs approach does allow for exploration of text and does extract meaningful information
extending earlier xed length phrase tools to allow for longer phrases and phrases with gaps does increase the expressive capability of the summaries
the methods for picking a tuning parameter while possibly a bit aggressive and conservative do provide an alternative paradigm for data analysis with an eye to extracting human meaning from text
overview of summarization this paper extends the concept of concise comparative summarization ccs discussed in incorporating a prior approach proposed by to result in an overall improved ology
concise comparative summarization involves comparing a pre specied set of uments to a baseline set
one can think of it as a regularized regression of some labeling of the m documents normally and onto the collection of all possible summary key phrases
for example in one analysis we label documents relating to incidents involving bon monoxide co as and the remaining documents as
each potential key phrase or feature is considered to be a covariate in the regression and is in principle represented as an m vector of measures of the feature s presence e

appearance count in each of the m documents
by using sparse regularization only a small subset of these possible summary key phrases is selected
these phrases are taken as the nal summary
so for example the resulting phrases of our co related regression would ideally indicate what is dierent about co related events when compared to other workplace injuries and fatalities
at root we are taking those phrases most useful for classifying a set of documents by the given labeling as the summary of the positive set of documents as compared to the negative baseline set
it is worth emphasizing that the focus is not predictive quality the selected features themselves are the object of interest and the quality can only be measured by their usefulness in the original human interpretation based question that motivated the exploration
thus these methods in principle require human analog experiments to validate their ndings
this can be done see or for examples
we are using text classication tools but the classication is a byproduct
there are many possible choices for how to implement this regression including whether to use logistic or linear regression and whether to rescale the frequency appearance of the phrases before regression
prior work has shown that rescaling phrase frequency is quite important failing to appropriately do so can result in summaries that have little informative content even while predictive accuracy is maintained
this is not surprising term frequency in text is a known and serious concern when data mining large text corpora e

as was rst illustrated in the information retrieval literature e


in text classication and by extension key phrase extraction via text classiers there is some desire to allow for phrases as well as unigram single word features
one approach is to calculate and use all phrases up to n words long as the overall feature set
for long phrases this can quickly become intractable as there is a blowup in the number of possible phrases a corpus may contain
to solve this problem ifrim et al
allow for arbitrary length phrases by generating the features on as part of the optimization
as an added benet this approach easily allows for gaps i
e
phrases with wildcard words which greatly enhances the potential expressiveness of the resultant summaries
ifrim et al
s algorithm based on work of and even earlier ts an elastic penalized logistic regression with the features consisting of the entire space of all phrases
also see or for other examples of regression on text and or for an overview of elastic nets and other regularized regression methods in general
ifrim et al
initially propose an algorithm to solve a penalized logistic regression of arg min


p m i log i with ci being the feature vector for document i with the cij as binary indicators of the presence of feature j in document i yi being the class label being the number of features including all phrases c being a regularization tuning parameter and being some regularization function
they later extend this to allow for alternate loss functions such as a hinge loss
however they do not allow for rescaling features
by modifying their methods we show how rescaling can be incorporated into their overall approach
they also do not allow for an intercept term which can introduce diculty with the summarization process if the number of positive features is not close to
we extend their algorithm to allow for a non penalized intercept term as discussed in
we implement these modications by extending their code and then wrap the resulting algorithm in a new r package textreg to make it easier to use in a rapid and exploratory manner
we also provide some useful tools and visualizations to help researchers understand the resulting summaries
the core idea behind the algorithm is a greedy coordinate descent coupled with a and bound algorithm
with each step the algorithm searches the entire feature space for the feature that has the highest gradient at the current
this is obviously a very large search space but it can be pruned using a relationship that bounds the size of a gradient of a sub phrase by a known calculation on any parent phrase
in the search we track the current optimal feature and then for each new feature considered if the bound on all the children of that feature is smaller than the current optimum prune all those children from the search

related work ccs is distinct from classication
classication is focused on sorting documents such as for attributing authorship or counting types of news events
text classication has been attempted using a wide array of machine learning methods such as naive bayes linear discriminant analysis or support vector machines which easily allow for large numbers of features the words and phrases to be incorporated
for comparisons of these dierent methods on text see or
for svms as an approach in particular for text see the book of
for such methods and these evaluations however the features themselves are not of primary interest classication is
we instead attempt to extract meaning from documents by contrasting sets to each other
this is most similar to key phrase extraction a literature in its own right
see for example or
interpreting text is a dicult task and can be done in a variety of ways
for example use text to predict roll call votes with an lda latent dirichlet allocation algorithm in order to understand how language of law is correlated with political support
model political text to explore who dominates policy debates
truly validating a nding is generally quite dicult and requires a great deal of eort to do properly as these papers illustrate quite well
our tools are primarily intended for exploration validation is not within ccs s scope without additional human validation eorts or alternative techniques
of the many approaches to text analysis variations of lda in particular have cently been widely investigated and used
these consist of a bayesian hierarchical model that describe documents in a corpus as a mixture of some number of dierent distributions on the words
they can reveal structure in a corpus and have been shown to capture human meaning
however generating novel models for specic circumstances is dicult
even mild changes to the model can be technically quite challenging and consist of an entire research topic in its own right
they are either computationally expensive or only solved mately via e

variational methods
in the spirit of diversity in research approaches we take a dierent path
this is not to say that using sparse regression methods on text is new see for example and
use sparsity to model topics by representing them as small collections of phrases that stand out against a background
showcase several methods such as sparse pca to investigate large corpora
there are many others
one aspect of our approach that we believe is novel is allowing for complex features in the form of overlapping phrases especially phrases with wildcard words while maintaining the ability to rescale features
this allows great exibility in the expressiveness of the possible summaries generated and it is not obvious how to naturally extend methods such as lda which rely on a generative model where words are picked i
i

from some distribution to do this
rescaled n gram regression initial methods regress the yi on the ci where ci is either the vector of counts with cij being how often phrase j appears in document i or of binary indicators of appearance with the elements cij indicating the presence of phrase j in document i
this can be problematic in that common phrases e

the or less obviously usually end up having much higher variance than less common ones and thus it is easier to pick them up due to random uctuations
see section for further discussion
rescaling the features can correct this as pointed out in e


in particular rescaling for transforms the vectors ci into new covariate vectors xi as xij where zj cij n cq ij q
this is similar to standardizing columns in a lasso regression if you do not then phrases with high variability need smaller coecients to have similar impacts on prediction
this makes them cheaper under the regularization and therefore appear more frequently due to random chance
once our feature space has been standardized with each phrase having an lq length of we regress our y onto these rescaled and an intercept
this is a high dimensional problem with p m
as we want a small number of phrases we use a sparse regularization penalty
we also use a squared hinge loss to obtain i c n with a denoting the maximum of a and
for this loss function an over is not penalized
from a prediction standpoint we wish i yi if we fall short we have quadratic loss if it does the loss is zero and if we overshoot we still have zero loss
there is no penalty for over predicting a document s label
we use squared hinge loss as this is similar to the lasso shown to be eective in but also monotonic which is needed for the optimization algorithm
also note the penalty term does not include the intercept
to generalize this framework taking notation from ifrim et al
let our loss for an vidual document be with mi i
we can use any monotonic loss functions with everywhere
the squared hinge loss from above is m this is very similar to an ols type penalty of
logistic would be em
regardless of the choice of the loss term can be expressed in the original counts as yi j c p cij n n c
we then obtain as arg min alternatively by letting j j zj we can move the zj to the penalty term and regress on on the counts ci this gives the identical loss as seen by considering mi i the rescaled columns scaling a column by zj is the same as penalizing the associated j by zj
this has ties to weighted lasso approaches such as the adaptive lasso in that we now have feature specic penalties
the gradients change however which can aect the optimization
see appendix b
solving equation is done with greedy coordinate descent
see algorithm
for greedy coordinate descent we repeatedly nd the feature with the highest gradient and then mize its corresponding j with a line search over the loss function
because this is a convex problem this will converge on the global maximum as each iteration will decrease and since the gradient along all the coordinates can only be if we are at a maximum
for a proof see e

or
we keep a cache of all the non zero features in our model we do not need to ever calculate or store all possible features
the main computational cost of the algorithm is in nding the feature with the largest gradient
to do this we dynamically generate the features by exploiting the nested structure of any multiword phrase having a smaller phrase as a prex
this inner algorithm is shown on algorithm
here we rst examine all unigrams then bigrams and so forth until there are no more eligible phrases
we rst calculate the gradient for all unigrams and enter them into the queue
phrases in the queue are placeholders for their family of superphrases
when we pull a phrase out of the queue we check to see if we can prune all of its children and if we can not we determine the phrases children calculate gradients for these children and nally enter them into the queue
this algorithm would work without pruning but if we were able to prune all the at zero children of a feature before examining them we could achieve large speed ups
and indeed some pruning is possible due to a trick of bounding a child s gradient based on the structure of its parent although the rescaling makes keeping this bound tight more dicult than in the original ifrim et al
presentation
the main idea is if a bound on the gradients of a family of features is less in magnitude than our current best gradient we can prune them all
we discuss nding such bounds next
algorithm greedy coordinate descent eatures while not converged do ndhighestgradient eatures
end while algorithm ndhighestgradient f eatures all non zero features so far
bestf arg maxf f eatures


all unigrams in dictionary q queue a queue of all features to check for u


do if then bestf u end if q
add u end for while q is not empty do q
next if not bestf then for c do bestf c end if q
add c end for end if end while if then
bounding gradients take any feature j with corresponding appearance pattern across the documents cj
for any feature with feature j as a prex we know that cki cji for i


n which we write as ck cj
we also know that cki for i


n because they are counts so ck
i
e
given a phrase j any phrase with phrase j as a prex can only have a count vector bounded between and phrase j s count vector
during our search we consider phrases from shorter to longer
for each phrase j we based on that phrase s appearance pattern in the text calculate a bound bj on the magnitude of the highest gradient a best case hypothetical superphrase with that prex could have
if this bj is smaller than the current best achieved gradient then we can prune from consideration all phrases with phrase j as a prex because if bj we have for any superphrase of j therefore we want bj to be as small as possible i
e
tight to make phrases easier to prune
bj
dk as derived in appendix b one such overall bound is for any q max dk max c r r where r q
for any phrase j these bounds can be computed by summing over only those documents that have phrase j rendering them computationally tractable
because the rescaling allows for theoretically very predictive yet relatively rare phrases these bounds are unfortunately quite loose making it hard to substantially trim the search tree
one possible avenue for improvement would be to integrate the preprocessing step suggested by
as special cases q gives r and a bound of the maximum for any document i that has phrase j and q gives r and a bound of the maximum of the two sums of the across the negative and positive documents containing phrase j which is ifrim s bound corresponding to no rescaling up to the scaling of the maximum occurrence of the phrase in any single document
all of the above is easily extended to an elastic net
see note in appendix b
choices of rescaling and additional constraints choices of rescaling e

the q in the lq rescaling and further restrictions on the tion problem can focus the ccs tool on dierent aspects of the summary
we can seek to generate summaries with more general or more specic words for example or enforce a contrast of a target set to a larger background set which eases interpretability
we discuss how in the following subsections

rescaling phrases vary greatly in their overall appearance in text with a very long tail of words and phrases that appear in nearly every document and the bulk of phrases appearing only one or two times
a phrase s rate of appearance is connected to its underlying variance if we represent the phrase with its count vector
this can cause problems when selecting the most meaningful phrases
in particular common phrases can easily dominate because they have greater variance
typically this is handled with stop word lists which are lists of words that are a priori deemed low information and dropped before analysis
for a thorough discussion of this see
and as discusses stop word removal is nicky not general and imperfect
they can not easily be adapted to diering contexts or languages
furthermore how to implement stop word removal when phrases are the object of interest is unclear
rescaling however can not only serve the function of a stop word list but do a superior job
rescaling is critical as is widely known in information retrieval
without rescaling stop words are easily selected by virtually all text mining procedures
even with stop words being dropped typically the runner up most common phrases are then selected primarily due to random variation in their appearance pattern
to see this test nearly any shelf text mining tool without removing stop words rst more often than not these methods will fail and their results will be dominated by these low information words
stop word lists are a hard threshold solution when a soft threshold tapering is more appropriate
rescaling oers such a tapering approach
the question then becomes which rescaling to use or alternatively how much tapering do we want
we use lq rescaling because it oers a class of choices and integrates well into the gradient descent algorithm
with lq rescaling dierent choices of q weight phrases relatively dierently allowing for focus on more common or uncommon phrases at the desire of the researcher
overall lower q means generally higher normalization factors z which will change the appropriate c for regularization
the main point of concern however is the relative sizes of the weights for rare phrases compared to common phrases
in general a
rescaling heavily penalizes common phrases while a rescaling does not
on the other hand rescaling penalizes rare phrases slightly more than lower choices of q
to illustrate see figure here we consider a sequence of phrases that appear once in each of m out of documents
the dierent series of weights have been rescaled so a phrase which appears of the time times has the same weight for all choices
tf idf rescaling
a related strategy for rescaling is tf idf rescaling from information trieval
it is typically something like variations exist xij rj with rj log cij ni n dj with dj being the total number of documents containing the phrase j
it diers in that it corrects for each document length with the ni we do not do so
if documents are roughly the same length this becomes less relevant
tf idf also puts more of an extreme dierence between weights for rare and common phrases scaled by the total number of documents
for rare and mid range phrases the tf idf rescaling is similar to a lq rescaling with large q
figure
impact of dierent choices of rescaling
here we see dierent rescaling factors z for phrases with a single appearance for each of m documents out of total documents
more common phrases are penalized more greatly relative to rare phrases

interpretation and negative coecients ccs returns a list of phrases with non zero coecients
interpreting these coecients can be quite dicult
just as in ols a model based interpretation for k would be that changing feature by would change the prediction by k holding other features xed which given normalization means changing feature by a count of would change the outcome by k zk
however given the lack of a well motivated model in our context interpreting the magnitude of these coecients is somewhat dubious
nonetheless we still wish to interpret the sign of the coecients positive indicates a feature is associated with our positive class of documents and negative indicates the negative class
when the negative group is a baseline however it is not an object of direct interest
this is especially the case if it is much larger and more diverse than the positive class
in this case the regression is ideally subtracting out baseline characteristics leaving the researcher with what makes the positive class distinct and noteworthy
here interpreting negative coecients can be dicult
one interpretation would be that such features are conspicuous in their absence
unfortunately even when there is a mix of positive and negative features we can still end up with unclear interpretations of the sign due to the holding other features constant aspect of the above
for example a negative weight for a feature might be osetting the positive weight for a highly correlated alternate feature and in fact both features may have a positive correlation with the labeling
in this case interpreting the negative sign as e

conspicuous in its absence is erroneous
it is more accurate to say the feature is conspicuous in its absence given its normal association with the second feature
this can be hard to communicate
one solution is to extend the optimization to consider only the set of positive forcing negative coecients to not exist
this is an easy extension of the above algorithm simply rescaling results in different
drop the lower bound on the gradient search and truncate any line search update of a k at
this is not to say that negative features are useless
for example if we allow negative features and nd all the coecients are positive it would suggest that the positive group has a clearer signal than the negative group
only phrases found in the positive group are dierentiating the groups
this might suggest distinct language use larger vocabulary or specic turns of phrase on the part of the positive group which could be of interest in its own right
picking the regularization parameter for most regularized regression settings picking the regularization parameter c is a riously dicult problem
in general higher cs lead to fewer features i
e
more concise summaries
low c summaries will be more verbose
however an overly low c allows for which in our context means obtaining features that are detected soley due to random uctuations in the appearance patterns of phrases
we need to ensure that c is suciently large to mostly prune out such noise
classically selection of c is done using methods such as cross validation to optimize prediction accuracy on out of sample predictions
as prediction is not our primary focus we look for other methods to select c that enhance the quality or interpretability of the maries generated
the lack of appropriateness of prediction accuracy is somewhat motivated by the literature prediction accuracy is for example not the same as model selection as is illustrated by the choice between aic and bic selection methods in regression
we present two methods rooted in the goals of ccs to select c
the rst is to conduct a permutation test to select a c that gives a statistically signicant summary in that the summary being non empty indicates the presence of systematic dierences in the text between the positively and negatively marked documents
the second is to select a minimum c to guarantee the pruning of very rare phrases
we discuss how one might select which approach to use in the discussion after the case studies below

a permutation test on the summary one might wonder if the phrases returned by ccs are simply due to random chance
there are so many dierent phrases it is reasonable to believe some will randomly be associated with any document labeling
we can control this with a permutation test
this is an exact test and the resulting p value is easy to interpret
to test whether it is meaningful to generate a summary at all repeatedly randomize the labeling across the documents regress and nd the corresponding c that zeros out all the features given our random permutation of the labels
this gives a null distribution of what c is appropriate if there were no signal
finally compare our originally observed c obs to this distribution of fake c s
we calculate a value of p pr obs c
if p is small we conclude that the needed regularization to zero out our originally observed signal is greater than that for a random signal i
e
there is a relationship between the labeling and the text
similarly if we pick a c that is at the percentile of our permutation distribution we are condent that the resulting summary being non empty is due to the relationship of the labeling and the text and not due to random chance
the individual phrases however are not specically tested as being signicant it is possible that they would change for example given mild perturbations to the data
theless this test provides a useful minimum bound for the nal c
any c lower than this bound could result in a non empty summary purely due to random chance
in a similar manner one can check the coherence of nal summaries by generating maries under dierent permutations of the labeling potentially adjusting c with each ation to get similar length summaries for all permutations and creating a list of lists with if humans can then reliably pick out the actual the actual summary randomly inserted
summary from the fake ones this is indication that the structure of the summary is not due to random chance
this idea is based on assessing the quality of eda visualizations see

pruning rare phrases most potential phrases are rare showing up only a handful of times in even very large corpora
selecting from such phrases introduces a severe multiple testing problem and we seek to appropriately regularize the regression with c to solve it
in particular rare phrases that show up only a few times can be selected if they happen to fall only in the positive set
more generally with improper rescaling of features a term that shows up once in the positive examples with a high count and several times in the negative examples with a low count can also be selected
this often is contrary to the interpretive goal behind selecting predictors
we want phrases that are general summaries informing the researcher of aspects across multiple documents
these problems can be partially remedied by proper selection of the tuning parameter c
here we investigate minimal c to guarantee that quite rare phrases are dropped
we nd such c by investigating so called perfect predictors
consider a feature j such that cij if yi and cij for some of the documents where yi
for the moment assume we do not have multiple counts in any document
this is a perfect predictor predicting r i cij s of the s positive documents
these perfect predictors could be used to identify a subset of the positive examples while incurring no loss for the negative examples
the only cost of including such a predictor is due to the regularization term c
if we set c high enough the cost will be prohibitive and we will not select
in fact the cut o of c q with m phrase suces
see appendix b for a derivation
yi with yi being the prediction for document i without any such hypothetical for this c any perfect predictor of r documents will be pruned
for comparison see table which shows for both the case of few positive and many negative examples and the case of roughly equal numbers of positive and negative documents the needed cut os for r
this cuto will generally be overly aggressive if other predictors also predict these documents then the gain of including the perfect predictor is potentially less
c q r r r



r r



table
needed c to prune rare perfect predictors
no rescaling
as discussed at the end of section
no rescaling is bad for appropriately handling common phrases
no rescaling is also bad for appropriately handling rare ones as we can see by its connection to the innity norm and the top row of the table
no rescaling of features makes it very dicult to prune perfect predictors
singleton predictors
as a special case singleton predictors are those that appear only once in the entire corpus and appear for a positive example
normally if such a rare phrase appears once in a positive example it can be pruned as described above
regardless of q in order to remove singletons that predict for a document with no other predictors we must have c
this can be generalized somewhat
consider for q if the count of a phrase for a single positive document is s and the count for t negative documents for that same phrase are each
the normalizing constant is then z cki t n t s t for the single positive document and xki and xki for the few negative documents
this is approximately the same as the singleton phrase circumstance and will therefore be pruned as above
proper selection of the tuning parameter is a better approach for pruning than cutting by dropping phrases with low counts ignoring computational issues as it can also prune near singleton phrases with high counts
this circumstance indeed arises
in a study of the fat cats corpus below the word lion used times for a report involving a plague ridden mountain lion corpse positive example and times in various negative examples was kept as a predictor in the nal summary of disease when c was too low c


regularization with cross validation the traditional form of selecting a tuning parameter is via cross validation where some metric of predictive performance is optimized
for example in our context we could calculate the predicted labeling of set aside documents and select c such that the average squared distance between predictions and actual is minimized
as we will see this tends to give longer lists which can be less interpretable as more important signal can be buried amongst less relevant terms
generally predictive performance is not directly a measure of our primary focus the interpretability and signicance of the selected phrases

regularization with early stopping and the elastic net the original ifrim et al method includes both a penalty term in the loss function but also regularizes by stopping before full convergence
dierent choices of c do aect the resulting model but early stopping is an easy way to obtain a list of specied length quite quickly although if the list is non sensical then this is obviously not a good move
ifrim s initial paper in fact has no penalty term in the loss function at all their entire regularization is due to early stopping
the relationship of these forms of regularization is unclear early stopping clearly has great computational advantages and there is no need for convergence checks we simply stop when we have found enough features of interest
however it is unclear for example whether this approach alone will successfully prune out rare phrases
computational comparisons we compare our ccs tool to three other methods in two studies
the rst compares running time and general characteristics of the nal word lists generally using default values and recommended approaches for setting tuning parameters
the second compares prediction accuracy for the four methods
in a third study we also examine the ccs tool under dierent choices of q
for our data we use our fat cats corpus and our ecab corpus both of which we describe more fully in the case studies section below

the four methods the main comparison method is multinomial inverse regression mnir a text sion method that is primarily designed to be distributed across many cores in order to be able to handle massive data
it parallelizes the regression by conducting individual inverse regressions of the counts of phrases onto the feature space which is in our case the binary labeling
it is regularized giving a sparse feature vector
the recommendation of mnir regarding tuning is to use aic or bic penalization
we selected bic because we are more interested in interpreting the covariates than in the quality of prediction and bic is known to be superior for model recovery as compared to aic see e

or more generally
we used the textir package
mnir resulted in very long lists of often more than a thousand words so we truncated the list by taking the words with the top weights
for practical use we would advocate greater regularization via e

cross validation to restrict the list further
we also compare to a classic linear lasso approach see e


earlier work shows that the gains from logistic over linear are minimal and the computational expense is large
we use the glmnet package selecting the regularization parameter with cross validation on the rmse test error
the standard package automatically standardizes the columns so this method uses rescaling
generally the lasso lists were short but if they were above words we truncated as above
we nally compare to the original n gram method of ifrim et al
other than our own there does not seem to be an r package implementing this approach so we replicated their method by using our binary features option and not rescaling the columns
for a more direct comparison we select the tuning parameter with our permutation approach
they initially advocated early stopping to regularize but in later work and in their package which we extended and modied for our package they introduced direct regularization
in the rst two studies we used normalization for our method
we select c two ways the permutation approach discussed above and to a xed c to prune perfect predictors for or fewer documents
we did not allow gaps in phrases and did not upper bound phrase length
for the permutation we permuted times and took the maximum as the c as the c from the permutations do not vary much
we ran our trials primarily on the cleaned fat cat dataset stemmed with porter ming via the tm package
see section
for further details
for the latter two methods the data was stored in a at le of cleaned and stemmed text with each line responding to a single document
for the rst two methods we generated a document term matrix from this text dropping all terms that appeared fewer than times to keep the matrix manageable
this resulted in unigrams bigrams and trigrams
there are documents in total
we ran the lasso twice rst with unigrams and second with all unigrams through trigrams
for mnir we only use unigrams the number of tures generated when we expanded to trigrams was computationally prohibitive on a single computer
to obtain labeling we selected a random sample of of the keywords associated with the fat cat reports weighted by the frequency of the keywords s appearance to form the labeling schemes to evaluate
for each keyword we dropped any phrases from eration as a feature e

phrases with carbon or monoxide for the carbon monoxide keyword were dropped by either passing these words as banned words to our algorithm or dropping the relevant columns from the document term matrix
to further understand computational timing we also replicated our rst comparison study on the ecab legal decisions discussed below
here we had judges that were part of overlapping subsets of decisions and we compared each judge to baseline this is not our labeling of primary interest in the case study below
for the rst computational comparison we ran the four methods on the full data and compared runtimes and characteristics of the nal word lists
for the second computation comparison we set aside a random of the data and then predicted the labeling of the set aside data using each method
the lasso the ifrim et al
and our method all produce nominal predictive scores often overly low due to the massive imbalance of the positive and negative classes
to nd a cut point for classication we t a logistic regression model on whatever predictive value came out of the method on the training set and then classied by determining whether the predicted log odds were above or below
for the mnir method we rst projected the results of the inverse regression as described in and used the resulting score as the covariate in the logistic regression modeling step
we also calculated auc scores using the raw predictive scores for all methods
our third investigation was on the impact of dierent choices of q for lq rescaling when using our method
we again generated word lists for each of our keywords and calculated average length and average frequency of words for the resulting lists for q
and
we put no upper bound on phrase length
simulations were run on a macbook pro with a
ghz intel core processor gb of memory and a solid state drive
reproducible r code for all simulations is available on request

comparison results for each labeling and each method we calculated dierent statistics on the word lists
we also timed the total time to generate a list
table has the average of these measures across the labelings
a few general trends are evident
list characteristics
overall the resulting lists are very dierent in character
we culated average list length and then for lists truncated at the top terms calculated the mean and median frequency of the words to gauge how common or rare selected phrases were
we nally averaged these means and medians across the dierent labeling runs
we see the dierent methods are quite dierent in terms of these scores
mnir gives very long lists of very specic words this is probably due to the inverse regression which selects all phrases that are relevant without much regard to correlation structure between them
the lasso also tends to have longer lists but the average ance of the selected words is on par with the n gram feature rescaled regressions
however when the lasso had access to trigrams which can be highly targeted and specic the median frequency plummeted to
for pentagrams it went to
as expected the ifrim method due to no column rescaling selects very general terms with a median frequency of around
for the osha dataset the permutation c values tended to be close to the xed c giving overall word lists that were similar as well
for the ecab dataset with longer documents the permutation selected c was much higher and thus the word lists were much shorter due to the greater regularization
the resulting words were also typically more general
overall it is clear that the practitioner can use dierent methods to get dierent types of lists
we believe generally that one wants short lists of phrases and that those phrases should not be overly general
runtime
runtimes were generally comparable for our osha data but widely dierent for the ecab data
we discuss the osha data rst
the lasso method even including its cross validation step was quite comparable to the textreg method with respect to time due to its very fast implementation
it is also robust to very wide feature matrices note the average time for using all trigrams is the same as for just unigrams
the ifrim method is faster than the textreg methods likely due to improved pruning
mnir is also quite fast event though it is designed for parallel systems and we ran on a single core
generally the computational times to generate summaries are comparable
however mnir on the trigrams was not workable
as mnir is linear in the number of features the blow up of features was too much of a time increase
of course multiple machines and its parallel structure could avoid this
we were quite surprised by the time statistics being insensitive to number of terms for the glmnet package
to investigate further we calculated a document term matrix for all phrases up to words giving unique terms giving the extra row in table
here average runtimes for the lasso increased modestly by about to a mean of seconds
the computational times were more spread out for the ecab data see the bottom half of table
we now see a substantially increased time from moving to unigrams to trigrams for the lasso and the timing of the textreg methods exploded
in investigating this time dierential further we found many selected phrases had or more words
furthermore most runs reached the maximum number of iterations before convergence indicating at surfaces
this is a weakness of greedy coordinate descent which we discuss further below
overall we potentially have due to the use of boilerplate language in legal writing long yet informative phrases that we wish to see in our summaries
this could make pre generation of candidate phrases prohibitively expensive
the mnir method selected all unigrams each unigram apparently has enough dierence in use across judges to be selected under the bic penalization
the times for the lasso and mnir do not include the time to generate the document term matrix which was
minutes for grams and
minutes for trigrams
method ifrim lasso lasso trigrams lasso pentagrams mnir textreg textreg xed c ifrim lasso lasso trigrams mnir textreg textreg xed c time avg sec












list length avg word freq avg median table
results of comparison study on osha reports top and ecab legal decisions bottom
runtime does not include time to generate the document term matrix for the lasso and mnir

predictive accuracy results to assess predictive accuracy we used the macro averaged f statistic f calculated for each of the trials and then averaged
see table
all methods would sometimes score none of the test set documents as positive giving undened f scores
these are also indicated on the table
figure shows these scores along with the component precision and recall as well as auc scores for those keywords where all methods had dened f
the predictive accuracy is comparable across the methods although our text regularization does suer some due to over regularization from the permutation method
overall predictive accuracy is low recall in particular tends to run around for all methods although the lasso with trigrams was noticeably superior
we also examined auc scores
here we again see the cost of textreg s over regularization conservative classication reduces sensitivity greatly lowering the roc and auc scores
the auc scores are deceptively high due to the imbalance between positive and negative examples
generally the superior performance of the lasso on trigrams indicates that rescaling tures coupled with the richer feature set of multiple words is useful for prediction tasks
our methods results indicate over regularization is detrimental to prediction due to primarily diminished ability to predict positive documents as positive shown by the recall scores
method mnir lasso lasso trigrams ifrim textreg




sd missing




table
f scores for dierent methods averaged across keywords examined with standard deviations
number of keywords with no dened f out of also indicated

selection of q results as anticipated dierent values of q produce lists of dierent quality
results are on table
higher q corresponds to lists with relatively more common words and phrases
low q duces lists that tend to have phrases with more words
for the q
list more than half the phrases were words or longer
we also found that the length of the list increased with q
for q we had longer lists with more common words
for low q the algorithm selected very targeted phrases and not too many of them
the regularization parameter c is not comparable across q
therefore we recalculated it via the permutation approach for each run and value of q
mean values for c are shown on the table for reference
generally we nd a rule of thumb worth remembering longer lists tend to have more general terms
this can happen by adjusting either c or q see rst simulation and compare dierent c for example
figure
out of sample classication rates for dierent methods
lasso on trigrams is generally the best although there is substantial variability
keywords where any of the methods failed to return f scores were dropped
charts substantively the same when these keywords included
method textreg

textreg
textreg
time avg sec


list length phrase length word freq word freq median avg avg


avg c avg


table
word list characteristics of dierent rescaling norms
length is length of list mean size is average number of words in the selected phrases frequency refers to phrase occurrence in the corpus
mean c is the average regularization value

discussion overall our method succeeds in accommodating multiple word phrases while also allowing for an intercept and the rescaling of features
as shown these extensions are critical for generating manageable lists with phrases that are not overly common
further picking c and q does control both list length and commonality of words on the lists as predicted by our initial discussion
in terms of computational time our method did not perform particularly well when compared to the lasso of the glmnet package especially considering that the lasso s overall time includes that of cross validation
as our method is similar to lasso regression on a pre constructed phrase matrix diering only in that it uses a hinge loss instead of a squared loss one might naturally ask whether an alternative approach would be to simply generate the full document term matrix and use glmnet
we are not entirely convinced as we discuss next
llllllllllllllllifrimtextregmnirlassolasso
















first generating full document term matrices including phrases with wildcards could be computationally tedious especially if we imagine extensions such as optionally included words
at the very least it is expensive in both time and memory and grows increasingly so with the number of possible phrases the minutes to generate all grams is well over the times
the stored matrix was twice the size of the initial raw text
second the hinge loss is dierent from the classic loss and there could be core dierences in overall behavior here this is an area for future investigation
third the speed of the glmnet package is partially due to the lars approach where the entire regularization path is computed at once
it is arguably also due to the package being particularly well developed and optimized for eciency
there may be similar ways to substantially speed up our package and method to make it more competitive
for example one potential slow down determined from examining the convergence paths of many runs is that we often see an initial selection of a small list of phrases and then a slow hill climb where with each step a dierent already selected phrase is selected for adjustment
this comes from the coordinate descent the optimal gradient path is at an angle and so following it requires small steps in the associated coordinate directions to trace that path
unfortunately with each such step the algorithm conducts a full search for the highest gradient across all phrases which is expensive
we might instead after each phrase is selected nd the maximum point given the set of all phrases selected at that point
then only if a new phrase were introduced would the algorithm have further steps
this is eectively the lars approach and is an important area for enhancement
furthermore the greedy ascent iteration often involves the intercept ratcheting down as features ratchet up
this suggests another possible direction of forcing the intercept to be for corpora with rare positive labeling rates rather than estimating it a intercept corresponds to predicting all documents as negative by default distinct from no intercept which predicts uncertainty as default or allowing the intercept to move which tends to predict the overall base rate as default
specifying the intercept value would save an intercept update with each step the optimization problem is then nding a collection of phrases to give positive documents positive weight without aecting the negative documents too heavily
regardless for truly massive data especially when the number of documents grows large neither our approach or the lasso will work
instead methods that allow for easy parallelization such as mnir will be key
another area for future exploration therefore is to determine how to over regularize mnir to get shorter lists which also might induce lists with more common phrases
case studies we illustrate the ccs tool with two case studies
for the rst case study we also compare the resulting summaries from ccs to the three other methods discussed above
an overview of the code to generate these results using our textreg r package available on cran is in the appendix
full scripts and data are available on our website
for other studies using address is
harvard
edu lmiratrix software textreg r package similar tools see for example or
before presenting our results we discuss data representation

data representation and cleaning there are many choices one might make in preparing a corpus for statistical analysis
it is common to for example convert text to lowercase and to drop all punctuation
we take that approach here although we convert all digits to x to preserve the presence of numbers in case that is informative and convert hyphens to blank spaces so the sequence of hyphenated words would coincide with a non hyphenated similar phrase something not possible with single word analyses
most text analysis packages would then convert the raw text into an m p matrix of counts dropping any stop words but because of the greedy coordinate descent algorithm unknown phrase length and the related generation of features on we store the text as raw strings with one string per document
there is some controversy as to whether to stem documents which is where the tails of many words are cropped so as to collapse the number of possible words
for example clean cleaning and cleaner might all be cropped to this has the disadvantage of making resulting text output somewhat dicult to read especially when considering phrases
stemming can also lose textual meaning if the dierent suxes are in fact important in the context
it has the advantage particularly for phrases of collapsing several dierent versions of phrases into one
we provide stemming as an option but to enhance readability of output append a to the end of all stemmed words and their roots to indicate they have been potentially cropped
we also provide tools to wildcard search for stemmed phrases in the original text so as to recover examples of the complete phrases
sometimes a given context involves words that are known a or nearly a priori to have no meaning
we therefore provide a option for custom made short stop word lists i
e
a list of banned words that are prohibited from being in any summary phrase
generally these lists are built on the y as an iterative process
the rst summary generated will often contain words that are immediately recognizable as inconsequential to a researcher with pre existing contextual knowledge even though they are correlated with the labeling
the researcher would then drop these words rerun the algorithm and repeat as necessary
we do not see any way to avoid this the case studies below illustrate why

fat cats our main investigation relies on osha s publicly available summaries of occupational talities and catastrophes fat in the united states from to
these summaries describe workplace incidents that have resulted in death or the inpatient talization of three or more workers
when such an event occurs an employer must report it to osha
in the course of conducting the resulting investigation osha generates a address is

gov views

c
f
r

recently expanded the list of reportable events to include the loss of an eye amputation or inpatient hospitalization
occupational injury and illness reporting requirements naics update and narrative report part of which becomes publicly available and is annotated with any of a set of about keywords to categorize the narrative reports in terms of specic chemicals involved machinery involved body parts aected and other salient features
the publicly available records primarily consist of a title a short paragraph summary of the incident along with the date whether the incident involved a fatality and several other covariates
we concatenated the title and paragraph description to form the documents
these documents tend to be to words long these are the and quantiles with a minimum length of words and a longest report of words
after stemming there were unique unigrams word stems of which appeared or more times and appeared or more times
to investigate this corpus we can for any keyword generate a labeling of the narrative reports by setting those reports tagged with the keyword as and the remainder as
using ccs on this would then summarize the collection of reports marked with a keyword by comparing them to all other reports
ideally this would take out words common to these reports e

employee or other general work place terms leaving us with phrases that make the identied set stand out
we would interpret this summary as a distillation of what is distinct about this category of fat cats as compared to fat cats in general
by periodically summarizing reports for each keyword of interest researchers may gain information about emerging hazards and trends in circumstances
hopefully the resulting summaries would be faster to read than the individual narratives but still contain hints as to general themes within these narratives
as chemical exposure is an area of particular interest for enhanced surveillance and standing we generated a background comparison set of documents by identifying keywords that we deemed to be at least loosely associated with chemical exposure
we then dened the chemical family of narratives as all narratives that were labeled with at least one of these keywords
this allowed us to compare various categories of narratives within the ited context of this chemical family as well as within the larger context of all other types of narratives
changing the background set highlights dierent aspects of what sets apart a marked collection of reports
as an overview of the overall number of narratives of dierent topics of interest table shows the appearance pattern of the categories examined
we discuss methylene chloride and carbon monoxide here and defer chemical reaction to a supplementary document
the table also shows how many narratives involved a fatality


methylene chloride as it is our motivating example we rst examined methylene chloride
we initially selected a value of c to ensure that we prune all singleton perfect predictors see section

there are reports marked with the methylene chloride keyword
running ccs on these reports returns two words methylene and chloride
as these words are not of interest we immediately added these words to the ban list and reran
table displays the resulting summary comparing these narratives to all the other narratives
reporting revisions fed
reg
september c
f
r


methylene chloride carbon monoxide chemical reaction general fatality




total chemical




table
number of narratives with dierent keyword labelings
second pair of columns restrict database to only reports related to chemical exposure
the summarizer picks up on the coherent theme across these reports of bathroom nishing
this example is quite encouraging given our prior knowledge of the dangers of methylene chloride but the utility of ccs in detecting yet unknown patterns remains to be seen
if we select c based on the percentile of permutations we obtain c

the needed c to result in a null summary is by comparison c obs

we conclude that there is a statistically signicant relationship between the text and the keyword beyond the presence of the banned words and that the summary is thus informative
the sponding summary for c
is quite succinct containing only a bathtub and stripper contained
picking c to give statistical signicance appears here to drop informative phrases
phrase a bathtub paint stripper stripper contained and reglazing from a bathtub remover contained stripping agent tub head phrase reports tag tag phrase table
methylene chloride rescaling against all reports
phrase is total occurrence of phrase reports is number of reports containing phrase tag is number of methylene chloride reports containing phrase tag is percent of phrase appearances in methylene chloride reports and phrase is percent of methylene chloride reports containing phrase
the summaries do not necessarily capture information in all tagged documents
in this case for example six of the methylene chloride reports do not have any of the phrases on table and so are not represented
a manual review of these reports revealed that four involved strippers for tile oors and furniture
one involved an explosion and one quite terse only referred to methylene chloride gas


carbon monoxide we also examined reports relating to carbon monoxide an asphyxiant odorless gas
we ran ccs with dierent values of q for lq rescaling to examine the impact of dierent levels of rescaling
we compared the co cases to all other cases involving any of a set of keywords predetermined to be related to some sort of chemical exposure i
e
those narratives marked as members of the chemical family
to reduce computational time we limited attention to phrases that appear at least ve times in the corpus
results are in table
to obtain these results we summarized in an iterative process words such as bon monoxide gas poisoning exposed exposure overexposed hemoglobin ppm levels partspermillion overcome and co were eventually dropped
we also removed the more specialized hyperbaric having to do with a ical intervention for co poisoning and cohb an abbreviation for carboxyhemoglobin a molecular complex that hemoglobin and carbon monoxide form in the body
all of these words appeared in initial summaries and are due to the technical obvious aspects of co poisoning they do not reveal trends or characteristics of interest and thus obscure the desired results
none of these words would have appeared on any conventional stop word list
as they are in fact correlated with the category we see no way of automatically removing them
the nal results reect several known patterns in co poisoning
for example the exhaust from gasoline and propane powered engines are major culprits of these exposures particularly in combination with poorly ventilated enclosed spaces
the appearance of the phrase cold room appears to reect incidents in which propane fueled forklifts and oor cleaning devices were the source of carbon monoxide exposure within cold storage areas where ventilation can be poor
in investigating hospitalization we found of the co poisoning cases contained were hospitalized versus only of the other chemical related narratives and of the chemical narratives
the re department was mentioned in nearly of these narratives versus a baseline of

this all may be due to lower rates of fatality with only of the co poisoning cases involving fatalities as compared to for other chemical family reports and for non chemical family reports
interestingly dead appears in of the co narratives as compared to in the remainder of the chemical family
dierent rescalings give dierent styles of summaries
the smaller q
and q
have very specic phrases e

were using a gasoline and their blood that appear only in the positively marked documents
larger q give more phrases overall and give phrases that appear at higher rates in both the positive and negative class
for example employees with more than appearances appears for q
overall infrequent and specic phrases are relatively easy to interpret and the more common phrases less so
but their patterns of appearance are striking
employees for example appears in of the co narratives versus a baseline of a mere less prisingly enclosed appears of the time versus less than
at baseline
table contrasts carbon monoxide to all incidents labeled with other chemical related keywords
we also compared co cases to the full set of cases in the database
that is we summarized the same collection of reports but used a dierent baseline point of comparison
results are in table in the appendix
they are broadly similar
we also analyzed the data using stemming
see table in the appendix
results are again broadly similar but possibly harder to read
stemming collapses phrases which can be helpful but hampers human readability
phrase a propane powered their blood gasoline powered concrete saw an x hour the re department measured in a cold operating a propane propane operated were using a gasoline propane powered powered forklifts for fresh the generator was x hour overexposure exhaust generator was blood a gasoline average found were treated the re department hour source of the ventilation enclosed were taken employees employees were were re department a propane were hospitalized dead warehouse










































reports tagged



















tagged





































phrase

























































table
dierent summaries of carbon monoxide compared to chemical family narratives finally we compare ccs to the other methods of mnir the lasso and the ifrim et al
approach
the methods returned summaries of very dierent lengths mnir was lasso ifrim and textreg
we had to truncate mnir and lasso to display the lists but we did this by taking the union of the top words of each list and then displaying weights before truncation to see maximal overlap
see table with words sorted by frequency of appearance in the corpus
we see the lists are quite dierent with mild overlap
mnir generally targets rare phrases most of which are not displayed ifrim very general ones
mnir restricted to unigrams in this instance due to computational concerns has less overlap than it might otherwise
textreg has mid range phrases with a few very rare phrases which are perfect predictors
these phrases were not included in the document term matrix due to their rarity so could not be on the lasso or mnir lists
many of the phrases have similar meanings
for example ifrim has the general versions e

for the more specic of the lasso and textreg

legal decisions in the context of legal decisions our motivating question is whether we can eciently learn about characteristics of certain types of cases by extracting associated phrases and topics from a corpus of those cases
as an exploratory case study we chose to examine publicly available decisions from the employees compensation appeals board ecab which siders appeals to determinations by the oce of workers compensation programs owcp in the u
s
department of labor dol
owcp handles compensation claims from federal mnir lasso ifrim textreg

phrase of for their blood vanguard decatur mek twa carbonyl newton qa transient stratton three are fd by by when his was

































































num
phrase









table
dierent summaries of carbon monoxide comparing dierent methods workers injured during the course of employment
ebac handles as many as appeals per year
within this case law one particular area of interest is how ecab handles compensation claims for so called emotional conditions
these cases can be challenging for a number of interesting reasons
for example establishing whether an employee s condition was mately caused by workplace conditions requires an analysis of causation that is unique in many ways from that which is appropriate in the context of a physical condition
to further probe the potential utility of css in extracting useful information from large bodies of technical text we performed an exploratory analysis on the collection of ecab decisions that relate to both mental health conditions and causality
we sought to mine whether automated summaries would reveal meaningful patterns
these decisions are publicly available through
we examined years to by scraping them from the website
address is

gov ecab decisions main
html we ended up with legal decisions documents generally ranging in length from to words these being the and quartiles and a median length of words
the shortest was words and the longest
there are unique words of which appear or more times and appear or more times
these counts include case identiers and other character strings as words
we do not attempt to remove them directly
we automatically labeled all of the decisions with two sets of dummy variables one for emotional condition and one for discussion of causality or work relatedness of the injury
for each we labeled documents if they contained any of a set of handpicked key phrases
once we tuned our collection of key phrases we took a random sample of the positively and negatively marked documents and conducted a manual review
the labeling is clearly not perfect as is illustrated in table
ideally the ccs method will still be able to produce relevant summaries despite the noise of the missed labels
although it is possible that specic types of positive decisions are systematically missed due to the labeling discovery of meaningful summary phrases would nevertheless be suggestive
labeling emotional condition no emotional condition total cases causality work relatedness no work relatedness total cases total correct sample estimated positive negative sense
spec




table
manual review of labeling quality for legal decisions column of table shows a rst pass summary of those cases that both involve an emotional condition and revolve around issues of causality
we see fairly general terms and some boilerplate language
here it is necessary to explore the raw text to discover the contexts for these phrases
this is easily done using our package
for example one positively marked decision has not every injury or illness that is related to a claimant s employment is compensable
in the case of lillian cutler the board explained some distinctions as to the type of employment situations giving rise to a compensable emotional condition under feca
c
e
docket no
issued november emphasis added
another is to establish that an emotional condition arose in the performance of duty a claimant must submit the following medical evidence establishing that she has an emotional or psychiatric disorder factual evidence identifying employment factors or incidents alleged to have caused or contributed to the condition and rationalized medical opinion evidence establishing that the emotional condition is causally related to the identied compensable employment factors
t
g
ecab emphasis added
illness that is cutler xx ecab requirement imposed by the psychiatrist incidents alleged to have caused lillian cutler compensable disorder factor of employment and a factor of employment not covered reaction to anxiety cutler xx ecab xxx xxxx depression lillian cutler xx compensable factor of employment and results from an environment or an administrative or personnel requirement imposed allegations


















main ecab depression xx many



















































table
summary of cases involving an emotional condition and a discussion of causality
dierent columns correspond to the number of dropped phrases
first column is only emotion and condition
rest are adding more and more phrases
column b adds column c depression and column d xx for illustration purposes
column d includes many other terms
as an illustration of stability of ccs consider the other columns of table
each column corresponds to dropping more and more terms from consideration
note that the transition from the third to fourth column drops one of the case references lillian cutler even though we did not explicitly drop those words and phrases
ccs selected phrases are picked in the context of other phrases
because we removed xx indicating a case number lillian cutler is no longer selected along with other phrases including parts of this phrase and xx
instead we obtain a cluster of phrases showcasing dierent aspects of these cases
dropping phrases can only aect a summary if those phrases are in the summary
the nal two columns are the same because none of the additional phrases were in the summary from column
care must be taken to understand the complex dependencies between phrases
thus in the context of ecab decisions the ccs tool provides phrases that ag plate language and case citations
to some degree these phrases appear to reect precedent and common statements of law that characterize a given category of cases
while our results are exploratory inexact and not particularly revealing in and of themselves they do suggest that a rened ccs tool might one day facilitate the development of automated case content analysis or aid the development of rened legal taxonomies

discussion as the above studies illustrate using these tools to understand text is a very dierent and far less precise activity than working to correctly classify text
the common problems with machine learning approaches selecting methods selecting tuning parameters
are only exacerbated by this uncertain area
the researcher is left with many decisions to make and only vague guidance on how to make them
with our method two such decisions are prominent what method to use for selection of the regularization parameter c and what method to use for selection of the normalization parameter q
we also need to determine how to remove domain specic stop words
picking c
for selecting c we have several options especially if we include picking a regularization parameter by optimizing predictive performance
which option to select is a dicult especially since there is no easy metric of nal quality if one s focus is not prediction
the computational investigations shed some light on this problem
ideally one would use the maximum of the permutation approach and the rare phrase pruning approach
this will guarantee only nding a summary if one should be found and also will discard rare phrases that do not speak of general trends across the positive documents
the free test from the permutation selected c of whether the phrases as a whole are in fact signicantly associated with the text is a real boon in our view
it moves towards presentation of results that are known to not be entirely noise
that being said future work on stability where documents are perturbed to see how the selected phrases change for example is a must
furthermore we acknowledge that the above examples suggest that the permutation selected c is severe more severe than from cross validation or similar
this means we can lose human meaning as illustrated by the methylene chloride example the richness of the summary was much greater with a slightly reduced c
relaxing regularization towards what would be achieved with oriented approaches to achieve longer lists may be informative but other than improved prediction this could undermine the guarantees provided by the above
perhaps work on testing individual phrases via false discovery rates could nd a better balance
regardless one should always compare the nally chosen c to table to see to what degree it is discarding perfect predictors and to what degree it would leave the remainder to be potentially picked up
this provides a human interpretation of the impact of the regularization
picking q
selecting q is also admittedly dicult
by design it gives dierent views of the data from the quite general to the very specic
we advocate for exploring a range of values as the best practice
in the above case studies for example the full range of terms on the tables provided the most complex and rich story perhaps pooling the lists and exploring these pooled lists would be one way forward
we underscore that we view these tools as exploratory the researcher can extract small snippits of text to see if they oer some clue towards a more thorough investigation
this is similar in spirit to for example xgobi
stop words
even though we avoid using stop word lists as a rst pass approach we still needed to generate specialized stop word lists
we see no way to have removed these words without human intervention as from a prediction standpoint the removed words are key indicators of a given labeling
unfortunately selecting them occludes terms that could enhance human understanding
we can see this with the calculated c thresholds as we add words to the ban list the c plummets because we are removing the words that are most correlated with the labeling
eventually we could reach a point where we have conditioned out all the connection of the labeling to text
this is another potential avenue for exploring such data
overall we advocate generating modest length stop word ban lists using substance matter knowledge coupled with rescaling over using generic stop lists that allow milquetoast words through due to their being not obviously wrong
conclusions we present a method for comparing sets of documents that is simple sparse and fast
we argue that these qualities are important for text analysis especially if it is to be used for surveillance or other exploratory tasks
here simple means the summaries can not be too technical in nature
for example the presence absence of features is easier to interpret than regression weights
we need sparsity as humans are lazy the number of phrases in a summary must be few
the faster summaries can be computed the better
otherwise exploratory analysis and discovery are bogged down
we do not however argue that the results of these tools or in fact any other text analysis tools that we know of can be taken as ultimate proof of any particular substantive theme or meaning
summaries can be quite suggestive but researchers would need to investigate further to substantiate those suggestions
alternatively secondary analyses such as blind scoring of the key phrases for sentiment could lead to traditional statistical conclusions
in these cases ccs should be viewed as a dimension reduction tool providing a targeted small number of informative features for a very complicated form of data
on the technical side we have eectively provided an implementation of lasso style gression where the full set of features are dynamically created and the loss is a squared hinge loss rather than a normal quadratic loss
this work shows that implementing sparse sion with greedy coordinate oers a viable direction for summarization using phrases rather than words
furthermore the approach of dynamically building features shows promise for other customizations such as skipping or dropping words to automatically detect related phrases collapsing them into single features
admittedly more work needs to be done to optimize this particular implementation to see exactly how fast it could be currently the algorithm is sub optimal because it does not fully t currently selected features at each iteration
that being said compared to methods with a pre computed design matrix it is comparably fast is more exible in the n grams considered and allows for some trickiness such as having gaps in the key phrases i
e
card words and the enforcement of non negative weights
additionally the textreg package is quite natural to use allowing users to avoid calculating the phrase matrix and instead works with raw text
it also easily allows for customization of dierent rescaling schemes other than
going beyond text analysis these methods also hint at ways of incorporating many interaction terms among features in high dimensional regression
phrase features are simply interactions of nearby word features and thus similar bounding methods may exist
this is another area for future exploration
acknowledgements we would like to thank three anonymous reviewers and an ae for their detailed comments the paper has been much improved by this feedback
also enormous thanks to matt taddy for his invaluable and speedy support with his textir package used in our comparison studies
this work builds on conversations and ideas discussed in the statnews research in particular thanks to group in uc berkeley led by bin yu and laurent el ghaoui
garvesh raskutti for his ideas on the impact of dierent rescaling choices
the authors are very grateful for these opportunities and inspirations
also thanks to kevin wu for a portion of the code in the r package and to janet ackerman for collaboration on an earlier incarnation of this project and initial data collection
appendix a appendix a consists of supplemental tables showing alternate summaries of the case studies discussed above
table compares carbon monoxide narratives to all the other narratives
table demonstrates a series of summaries on a stemmed corpus
reports tagged tagged


































phrase


































phrase were using a gasoline propane powered oor their blood hemoglobin in the cold room they were found powered forklifts gasoline powered propane powered overexposure exhaust fumes exhaust calculated generator was employees were treated evacuated the re department ventilation were treated a propane powered blood were hospitalized found employees stratton headaches source of the a gasoline passed out enclosed re department cold hours room


































































table
dierent summaries of carbon monoxide when comparing against all other cases phrase for x was the cold xxx the was the cold of the x cold are

































reports
















tagged tagged phrase

























table
dierent summaries of carbon monoxide using stemming compared to chemical family appendix b derivations we here show three derivations used in the above work
we rst show how to obtain the bound on the gradient
second we give an alternate formulation of the loss function which gives a dierent approach for nding the feature with the maximal gradient
we then show how to obtain the minimal to ensure perfect predictors are pruned
bound on the gradient the gradient for phrase j is dj c cij dj n cij c cij dj now consider all phrases with phrase j as a prex that are currently not in the model i
e
which currently have k and maximize over the possible gradients a similar argument gives a bound below for negative gradients
for vectors a let a b a component wise relationship of ai bi for i


m
then the set a a cj contains all potential appearance patterns for a phrase with phrase j as a prex
we do not wish to calculate what the actual phrases are hence we optimize over this set of potential phrases
this results in the optimization problem u max max ai ai za c ai za dj c max c za with za being the lq norm of a w being a m vector of weights with wi and being the inner product
because is everywhere nonpositive the rst term in the rst line above is negative
the second line follows because setting ai for any document with yi only increases the gradient as doing so will simultaneously drop negative terms and shrink
the penalty term is negative because we are examining gradients at if we step in the negative direction as indicated by the rst term the gradient will immediately be shrunk towards
the mi which include the intercept are xed constants determined by the current location of our optimization path
we are eectively maximizing over an inner product of a and a vector of weights w with wi for i


n
overall this bound is assessing the maximum possible utility of a hypothetical super phrase which boils down to maximizing weights put on positive examples
the normalization za renders this problem dicult
we can bound the optimization using the following relationship for q r such that q r
this inequality gives u max cos c cos c cos c max max c with being the angle between a and w
coupling this with a similar argument for minimization gives the overall bound
a note on the elastic net
the elastic net is where we penalize our loss function with using ifrim et al
s notation ca a p p
this regularization tends to keep groups of correlated features rather than picking one it can borrow from the stability of ridge regression
it can be potentially useful when many small features have weak signals
setting a corresponds to regularization
for our problem the gradient search is just changed to a subtraction of ca rather than c for the at zero potential new features
the gradients calculated for features already in the model have an extra term of due to the derivative of the second term above
alternate gradient formulation
by redening we can change the optimization problem to have a lq rescaling term in the penalty
this gives dierent bounds on the gradients for super phrases based on a phrase
however it also changes the gradients themselves which would change the path of the optimization problem
that is dierent features will initially have the largest gradient
assuming true convergence the nal solution will be identical however
in particular dene j
the loss term can then be reexpressed as c
n the gradient for phrase j is then dj n czj dj dj dj czj
again consider all phrases with phrase j as a prex and maximize over the gradient yielding the optimization problem czj u max max ai c n p
ap i as before the second line comes from noticing that setting ai for any document with yi will only increase the gradient due to dropping the negative terms and shrinking
the penalty term is still negative because if we step in the negative direction from which is indicated by the rst term the gradient will immediately shrink towards
if we consider only phrases that have at least r occurrences in our corpus then we can roughly bound with max k dk cj p
this is from maximizing both terms separately
for the rst we simply add maximum weight without regard to the normalizing constant
for the normalizing constant given a total count of r occurrences the maximum zj would be putting singletons on each of r documents giving the p total
similarly bounding from below gives an overall bound of max k dk max cij cij p this bound appears to be less useful than the one presented in the main paper
more not rescaling by zj tend to make more common phrases be selected rst as we are not rescaling the rst term allowing it to grow quite large
perfect predictors take the count vector for a perfect predictor cj
it has r and m r
for the regression the count vector cj is q rescaled giving xj cj zj q as zj q
q m assume feature cj has been set aside and we have optimized without it
we have x with j j zj except for the intercept for our current set of predictions and an overall yi
now reintroduce feature cj
our loss function when only predicted mean m considering feature cj is then where we have dropped those terms not dependent on j
this is convex
take the derivative and set equal to to nd the minimum yi yi m m i yi xi xijj xi j q xi j q q q q i i i xi q i j q xi q
the s will not be negative and hence we examine the positive case allowing us to drop the sgn term
set equal to and solve giving j q r xi i
the term in the outer parenthesis is the average prediction for the documents having the perfect predictor cj
if for a document i with cik there are not really any other predictive features then
if this is true for all of the documents predicted by ck then the above is then yi xi approximately j
if some documents are predicted by other features included in the model then the sum will be less and the necessary c for pruning will be reduced
rearrange to obtain an approximate cut o for c to drop all perfect predictors that perfectly predict r documents
the q case
for q we have j r r xi i c c
for the positive examples xij r giving prediction of yi r c r
c the rst term of j takes our prediction perfectly to and the second term shrinks the coecient away from by half of c
predictions from predictors that predict for more documents will be shrunk less than those for fewer
the raw coecients will also be larger
appendix c using the textreg package our text regression package textreg on cran is a wrapper for an extensively modied version of the code written by georgiana ifrim
it is also designed to integrate with the tm package a commonly used r package for dealing with text
our package is fully documented but it is research code meaning gaps and errors are possible the author would appreciate notication of anything that is out of order
the primary method in this package is the regression call textreg
this method takes a corpus and a labeling vector and returns a textreg
result object that contains the nal regression result along with diagnostic information that can be of use
the somewhat edited function heading along with default values is textreg labeling banned null c
lq maxiter verbosity positive
only false binary
features false no
regularization false min
support min
pattern max
pattern gap convergence

the main arguments to this method are listed below corpus a vector of strings or a corpus object built out of strings
labeling a vector of values where means drop from consideration
banned a vector of unigrams words that should not be allowed in any summary phrase
c the c tuning parameter for regularization
lq the q for the lq rescaling of terms
or above is treated as innity
maxiter the maximum number of iterations allowed before terminating even under no convergence
verbosity means silent
larger numbers mean more diagnostic printout
positive
only only allow positive features other than the intercept
useful if there are few positive documents and many negative baseline documents
binary
features the feature vectors are vectors indicating whether a phrase is in or not in any given document
this is compared to vectors of counts of how many times a phrases in a document
these feature vectors are lq rescaled regardless
no
regularization if true then features will not be rescaled which recovers the ifrim et al
algorithm
min
support phrases that do not appear this many times are not considered viable features
increasing this number can greatly decrease the running time of the algorithm but it will force the dropping of very rare phrases regardless of rescaling or regularization choice
min
pattern max
pattern minimum and maximum lengths for phrases that are gap number of words that can appear in a gap
a phrase can have multiple gaps of this ered
length
the resulting textreg
result object can be printed plotted and explored
try in an r console typing rs by itself or plot rs
the method reformat
textreg
gives a nice table see e

table of summary statistics for the nal phrases
the by side summary table such as table is made by passing a list of textreg
result objects to make
list
table
the method calc
loss rs gives the nal loss of a result rs and predict rs will return individual document level predictions of the labeling
the method rule
to
matrix rs gives back the n r design matrix for the nal selected r phrases including intercept
to pick a tuning parameter one can use cs find
threshold
c labeling ban
words this method returns a r length list of numbers
the rst number is the choice of c that will return a null model for the labeling given and the subsequent r numbers constitute our found c values that return a null model under a random permutation of the labeling holding the zeros xed
it takes the same parameters as textreg except for maxiter and c
be sure to use the same remaining values for both calls so that find
threshold
c culminates with a c corresponding to the correct model family
for exploring text sample
fragments phrase labeling corpus is useful
see also grab
fragments
to prole specic phrases possibly even phrases not in the results use make
phrase
count
table
one can make cluster plot of how phrases relate with cluster
phrases rs or make matrices of co occurrence of phrases using make
phrase
correlation
chart rs
all of the above and a bit more is demonstrated and more fully explained in the vignette bathtub demo that comes with the package
please read through it for further discussion and ideas
references jinzhu jia luke miratrix bin yu brian gawalt laurent el ghaoui luke moore and sophie clavier
concise comparative summaries ccs of large text corpora with a human experiment
the annals of applied statistics
david m blei andrew y ng and michael i jordan
latent dirichlet allocation
the journal of machine learning research
georgiana ifrim gokhan bakir and gerhard weikum
fast logistic regression for text categorization with variable length n grams
in acm sigkdd international conference on knowledge discovery and data mining pages
georgiana ifrim and carsten wiuf
bounded coordinate descent for biological sequence classication in high dimensional predictor space
in acm sigkdd international conference on knowledge discovery and data mining pages
matt taddy
multinomial inverse regression for text analysis
journal of the american statistical association
robert tibshirani iain johnstone b efron and t hastie
least angle regression
the annals of statistics
j grimmer
a bayesian hierarchical topic model for political texts measuring pressed agendas in senate press releases
political analysis january
jonathan chang jordan boyd graber sean gerrish chong wang and david m blei
in neural information reading tea leaves how humans interpret topic models
processing systems nips
jonathan m bischof and edoardo m airoldi
capturing semantic content with word frequency and exclusivity
in international conference on machine learning inburgh scotland february
gerard salton and christopher buckley
term weighting approaches in automatic text retrieval
information processing management
g salton
developments in automatic text retrieval
science
taku kudo and yuji matsumoto
a boosting algorithm for classication of structured text
in conference on empirical methods in natural language processing pages barcelona spain
association for computational lingusitics
robert schapire and yoram singer
boostexter a boosting based system for text categorization
machine learning
alexander genkin david d lewis and david madigan
large scale bayesian logistic regression for text categorization
technometrics
tong zhang and frank j oles
text categorization based on regularized linear ication methods
information retrieval
t hastie robert tibshirani and j h friedman
the elements of statistical learning
springer
hui zou and trevor hastie
regularization and variable selection via the elastic net
journal of the royal statistical society series b statistical methodology
f mosteller and d l wallace
applied bayesian and classical inference the case of the federalist papers
springer verlag
e m airoldi a g anderson and s e fienberg
who wrote ronald reagan s radio addresses bayesian analysis
gary king and will lowe
an automated information extraction tool for international conict data with performance as good as human coders a rare events evaluation design
international organization september
daniel j hopkins and gary king
a method of automated nonparametric content analysis for social science
american journal of political science
c cortes and v vapnik
support vector networks
machine learning
george forman
an extensive empirical study of feature selection metrics for text classication
the journal of machine learning research
s dumais j platt and d heckerman
inductive learning algorithms and tions for text categorization
in proceedings of the seventh international conference on information and knowledge management acm pages
thorsten joachims
learning to classify text using support vector machines
the springer international series in engineering and computer science
springer
stuart rose dave engel nick cramer and wendy cowley
automatic keyword traction from individual documents
in michael w berry and jacob kogan editors text mining applications and theory pages
john wiley sons ltd unknown
eibe frank gordon w paynter ian h witten carl gutwin and craig g manning
domain specic keyphrase extraction
in the sixteenth international joint conference on articial intelligence pages california
gan kaufmann
jilin chen benyu zhang dou shen qiang yang zheng chen and qiansheng cheng
diverse topic phrase extraction from text collection
in world wide web conference pages edinburgh uk may
citeseer
sean gerrish and david m blei
predicting legislative roll calls from text
in the international conference on machine learning pages
li wei lee and shyi ming chen
new methods for text categorization based on a new feature selection method and a new similarity measure between documents
in advances in applied articial intelligence pages
advances in applied articial intelligence
y yang and i o pendersen
a comparative study on feature selection in text tion
in international conference on machine learning pages nashville us
jacob eisenstein amr ahmed and eric p xing
sparse additive generative models of text
in international conference on machine learning pages bellevue wa usa february
laurent el ghaoui guan cheng li viet an duong vu pham ashok n srivastava and kanishka bhaduri
sparse machine learning methods for understanding large text corpora application to flight reports
in conference on intelligent data derstanding pages june
burt l monroe michael p colaresi and kevin m quinn
fightin words lexical ture selection and evaluation for identifying the content of political conict
political analysis
hui zou
the adaptive lasso and its oracle properties
journal of the american statistical association
tong tong wu and kenneth lange
coordinate descent algorithms for lasso penalized regression
the annals of applied statistics
z q luo and p tseng
on the convergence of the coordinate descent method for convex dierentiable minimization
journal of optimization theory and applications
laurent el ghaoui vivian viallon and tarek rabbani
safe feature elimination in sparse supervised learning
uc berkeley
hadley wickham dianne cook heike hofmann and andreas buja
graphical inference for infovis
ieee transactions on visualization and computer graphics
t p speed and bin yu
model selection and prediction normal regression
annals of the institute of statistical mathematics
jerome friedman trevor hastie and rob tibshirani
regularization paths for eralized linear models via coordinate descent
journal of statistical software
martin f porter
an algorithm for sux stripping
program
david meyer kurt hornik and ingo feinerer
text mining infrastructure in r
journal of statistical software
yiming yang and xin liu
a re examination of text categorization methods
in ceedings of the annual international acm sigir conference on research and development in information retrieval pages
acm
andreas buja dianne cook and deborah f swayne
interactive high dimensional data visualization
journal of computational and graphical statistics

