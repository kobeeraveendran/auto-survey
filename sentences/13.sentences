r a r i
s c v
v i x r a improving update summarization by revisiting the mmr criterion florian juan manuel torres and marc el diro universite de montreal cp
succursale centre ville montral quebec canada laboratoire informatique davignon bp avignon cedex france ecole polytechnique de montreal cp
succursale centre ville montral quebec canada october abstract this paper describes a method for multi document update tion that relies on a double maximization criterion
a maximal marginal relevance like criterion modied and so called smmr is used to select sentences that are close to the topic and at the same time distant from sentences used in already read documents
summaries are then generated by assembling the high ranked material and applying some ruled based linguistic post processing in order to obtain length reduction and tain coherency
through a participation to the text analysis ence tac evaluation campaign we have shown that our method achieves promising results
introduction text summarization is the process of automatically creating a compressed sion of a given text that provides useful information for the user
oriented summaries focus on a user s need and extract the information related to the specied topic given explicitly in the form of a query
on the other hand generic summaries try to cover as much as possible the information tent
over the past few years extensive experiments on query oriented document summarization have been carried out
extractive summarization duces summaries by choosing a subset of sentences in the original documents
sentences are then ordered and assembled according to their relevance to erate the summary
this contrasts with abstractive summarization that involves rephrasing information in the text
although human beings typically produce summaries in an abstractive way most of the research is on extractive summarization
this is due to the fact that tools needed to construct semantic representations or generate natural language have not reached a mature stage today
moreover existing abstractive summarizers often depend on an tive component
for example use a language generation component on top of a multi document extractive summarizer to produce the nal summary
in this paper we focus on query oriented multi document text summarization where the goal is to produce a summary of multiple documents about a specied topic
with the ever increasing popularity of news search engines displaying the information in a more practical and pleasant way is becoming a challenging and important issue
one possible solution is to summarize multiple news so as to propose only one short text instead of raw aggregated headlines
this is intuitively a reasonable solution though producing summaries from large collection of documents is a very complicated task
however as the number of documents increases facts that are considered as important and have to appear in the summary also become more numerous
in this case a choice must then be made to drop important facts in order to satisfy size constraints
one way to tackle this problem is to remove facts that the user is already aware of
this variant of text summarization is called update summarization
more formally update summarization is the task of producing summaries while minimizing redundancy with previously read documents from now on history
recently introduced at the document understanding conference duc update summarization is an emerging summarization task that brings new challenges to sentence ranking algorithms
indeed segments have to be selected according to their salience but also to their ability to capture elty
existing approaches are derived from state of the art query oriented document summarizers by the addition of some constraints about redundancy and novelty detection
these include machine reading graph based marization maximal marginal relevance mmr and novelty boosting
the fact that most of them are relying on linguistic resources or tools such as taggers and parsers is a limiting factor for the adaptation to other languages or domains
in this paper we propose a sentence ranking algorithm inspired by the well known mmr re ordering algorithm
sentences are scored thanks to a double maximization criterion that strives to maximize sentence s relevance while imizing non redundancy with the previously read documents
our formulation combines word level similarity measures in an information retrieval approach ranking sentences by their similarity to the topic and the inverse ity to other sentences in history
we show that our method although using minimal linguistic resources can achieve good results among state of the art summarizers
preliminary results about the sentence re ranking process were published in
the remainder of this paper is organized as follows
an overview of related work is provided in section
section presents our three understanding conferences are conducted since by the national institute of standards and technology nist nlpir
nist
gov steps summarization method pre processing sentence ranking and linguistic post processing
experimental results are presented in section followed by discussions and conclusions
related work introduced by luhn in the fties research on automatic summarization can be qualied as a long tradition
in the strategy proposed by luhn source tences are scored for their component word values as determined by type weights
scored sentences are then ranked and selected from the top until some summary length threshold is reached
finally the summary is generated by sembling the selected sentences in original source order
although fairly simple this extractive methodology is still used in current approaches
later on tended this work by adding simple heuristic features of sentences such as their position in the text or some key phrases indicating the importance of the tences
as the range of possible features for source characterization widened choosing appropriate features feature weights and feature combinations have became a central issue
a natural way to tackle this problem is to consider sentence extraction as a classication task
to this end several machine ing approaches that uses document summary pairs have been proposed
summarization then started gaining more momentum with the uation followed by the duc evaluation conferences
new tasks have been continuously added to the summarization issue as proaches became more robust and resources grew larger
were amongst the rst to tackle the update summarization problem
their approach originally developed as a tool to monitor changes in news coverage over time uses topic detection and tracking techniques to determine which sentences capture ness and novelty
the most intuitive way to go about update summarization would be to be identify temporal references within documents dates elapsed times temporal expressions
and to construct a timeline of the events
it is a complex task as temporal references depend on surrounding elements in the discourse but also require an understanding of the ontological and cal foundations of temporal reference construction
assuming the timeline is constructed update summaries could be produced by assembling sentences containing the most recent events
however most recently written material is not necessarily latest facts
this way focusing the summaries on information that the user is not aware of can be seen as identifying unseen facts
ing approaches rely exclusively on content based redundancy removal without recourse to temporal detection
propose a machine reading method to struct knowledge representations from clusters of documents
sentences that are containing new facts i
e
that could not be inferred by any document from the history are selected to generate the summary
a rule based method using fuzzy coreference cluster graphs was introduced by
this approach can be text summarization evaluation conference summac conducted in may nlpir
nist
gov related projects tipster summac index
html applied to various summarization tasks but requires to manually write the tence ranking scheme
rst use a nave similarity ratio to select sentences that are relevant and dissimilar to sentences from history
on top of this ranking approach a second method called novelty boosting is used
the latter extends the topic by the unique terms in the cluster thus biasing the ranking towards maximizing relevance not only with respect to the topic but also to the novel aspects of the topic in the cluster
method in this section we present the details of the proposed text summarization method
as mentioned earlier our work models sentence ranking as a double tion criterion
we dene h to represent the previously read set of documents history q to represent the query and s the candidate sentence
the ing subsections formally dene document pre processing the sentence scoring method and the summary generation process

pre processing the rst step is to prepare documents for the ranking process
as we use tractive summarization documents have to be chunked into cohesive textual segments that will be assembled to produce the summary
the importance of pre processing is predominant because the selection of segments is based on words they contains
the choice was made to split documents into full sentences in this way obtaining textual segments that are likely to be matically correct
afterwards sentences are going through several basic malization steps in order to reduce computational complexity
an example of document pre processing is given in table
the process is composed by the following steps
sentence splitting a simple rule based method is used for sentence
documents are chunked at the dot exclamation and tion mark signs
prior to that ambiguous composed person names i
e
george w
bush are detected to reduce segmentation errors

sentence ltering words are converted to lowercase and cleared up from sloppy punctuation
words that do not carry meaning such as tional or very common words are removed

date normalization dates are rewritten and extended with time related words
for example december is replaced by and
standardized dates allow to minimize enriched with december software is available from
nist
gov
the scoring function bias i
e
considering only one word for one concept instead of three in this example while enrichment is useful to link facts that were happening at the same period of time month or year

word normalization remaining words are replaced by their simplied forms i
e
inected forms go goes went gone


are replaced by go using a word root database entries
in case of ambiguity the most frequent word is chosen
washington a federal judge monday found president clinton in civil contempt of court for lying in a deposition about the nature of his sexual relationship with former white house intern monica s
lewinsky
clinton in a january deposition in the paula jones sexual harassment case swore that he did not have a sexual relationship with lewinsky
clinton later explained that he did not believe he had lied in the case because the type of sex he had with lewinsky did not fall under the denition of sexual relations used in the case
a federal judge monday found president clinton in civil contempt of court for lying in a deposition about the nature of his sexual relationship with former white house intern monica s
lewinsky
clinton in a january deposition in the paula jones sexual ment case swore that he did not have a sexual relationship with lewinsky
clinton later explained that he did not believe he had lied in the case because the type of sex he had with lewinsky did not fall under the denition of sexual relations used in the case
judge monday president clinton civil contempt court federal lie deposition nature sex relation former white house intern monica clinton january deposition paula jones sex harassment case swear sex relation clinton late explain believe lie case type sex lewinsky fall dene sex relation use l a n g i r o e t t i l s e s s e c o r p table example of pre processing applied to the document
from cluster of duc
news agency name is removed document is segmented into sentences words are normalized punctuation and case are removed dates are standardized end enriched

ranking sentences are scored according to the fact that they contain material satisfying the need formulated in the user s query
ranking sentences for query oriented summarization can be seen as a passage retrieval task in information retrieval
in this paradigm sentences sharing most of their vocabulary with the query are likely to be informational for the reader
each sentence is then scored by computing a combination of two similarity measures with the query
the rst similarity measure is the well known cosine computed on the sentence and the query vectorial representations in the documents term space denoted spectively and
the decision was made not to use the classical tf idf weighting scheme because of the diculty to nd similar data and generate pertinent weight lists
the main weakness of cosine and more generally of all similarity measures using words for tokens is that they are relying too much on term normalization
their performance dramatically decreases with wrongly or non normalized words
that is why we propose a second similarity measure based on the jaro winkler distance that can bridge morphologically similar words in order to smooth normalization and misspelling errors
this measure can be classied as an improved edit distance between two word sequences
the jaro winkler distance denoted jw calculates the number of operations required to transform a string into another one
it uses the number of ing characters and transpositions to compute a similarity score between two terms giving more favourable ratings to terms that match from the beginning
originally introduced to tackle normalization issues in automatic tion of chemistry articles this distance was extended to compute a similarity measure between a sentence and the query q q q qq m max where is the term set of s in which the terms m that already have maximized m during the previous steps of the summation are removed
the nal score is calculated using a linear combination of the two similarity measures
equation shows how to compute the relevance score between a sentence s and a query q
q q the maximal marginal relevance mmr algorithm has been successfully used in query oriented summarization
it strives to reduce redundancy while maintaining query relevance in selected sentences
the summary is constructed incrementally from a list of ranked sentences at each iteration the sentence which maximizes mmr is chosen mmr arg max q ss max sj e sj where s is the set of candidates sentences and e is the set of selected sentences
represents an interpolation coecient between relevance and redundancy
in the original formulation and were computed using the cosine ilarity measure
although this measure has been proven to be ecient any other similarity measure between sentences remains appropriate
we propose an interpretation of mmr to tackle the update summarization issue
unlike previous work such as our approach does not require ative re ranking
to remove sentences containing redundant material the set of selected sentences e is replaced by the set of sentences in history
in terms of computational complexity this means that each candidate is compared to all sentences from h
since and are ranged in they can be seen as probabilities even though they are not
this way is considered as the probability to be relevant to the topic and as the probability to be redundant with history
we propose to rewrite by adding the constant as nr stands for novelty relevance nr arg max q max shh sh arg max q sh max shh ss ss this makes more sense because it combines relevance and non redundance instead of focusing on redundancy penalization
according to our intuition we presume that is more or less corresponding to an or combination
but we are obviously looking for a criterion corresponding to and
since the similarities are independent we can use the product combination
sentences are scored thanks to a double maximization criterion in which the best ranked one will be the most relevant to the query and the most dierent to the sentences in h q h max shh sh decreasing parameter in with the length of the summary was suggested by and successfully used in the duc by thereby emphasizing the relevance at the outset but increasingly prioritizing redundancy removal as the process continues
similarly we propose to follow this assumption in smmr that as the amount of data in history increases using a function denoted
we have dened this parameter prioritizes non redundancy function as novelty factor
h n n a special breed of redundancy is proliferating in news articles as ists increasingly rely on the fact that news articles have to be as universally understandable as possible
this means that most of the news articles contain previous facts pointers to previous articles in order for a reader that does not know anything on the subject to catch on
this is why we think that a normalized longest common substring lcs measure between two sentences n is well adapted to be used as the non redundancy measure
for ple lcs can easily detect sentence rewritings specially when the sentence is structured around a redundant sub sentence

post processing once sentences are selected to be assembled in the nal summary some tic treatments are applied
indeed once out of their contexts discursive forms are considerably decreasing summary coherence
for example two sentences one next to the other in the summary may be in opposition while not dealing with the same subject
our rule based linguistic post processing targeted tence length reduction and coherency maximization
an example of summary post processing is given in table
the process is composed by the following steps
acronym rewriting rst occurrence of an acronym is replaced by its complete form acronym and denition following ones only by their duced forms
denitions are automatically mined in the corpus by pattern matching
in case of acronym ambiguity the most frequent one is selected

date and number rewriting numbers are reformatted and dates are normalized to the us standard forms mm dd yyyy mm yyyy and mm dd

temporal references rewriting time tags are used to replace fuzzy temporal references
for example


the end of next year


with poral tag is replaced by


the end of




discursive form rewriting ambiguous discursive forms are deleted
for example but it is


is replaced by it is




finally say and parenthesized content are removed and ation cleaned
sentences are ordered within the summary by original document order and temporal order of documents
since the acronym rewriting process is dent to the sentence order and modies sentence s lengths multiple passes are required to generate the nal summary
within summary redundancy is aged by using a simple similarity threshold that prevents duplicate and highly redundant sentences to enter the summary
example ambiguous say clause


he said is removed
last u
s
scientists issued a report saying the rate of ice melting in the arctic is increasing and within a century could lead to summertime free ocean conditions not seen in the area in a million years
the rate of ice melting in the arctic is increasing and a panel of researchers says it sees no natural process that is likely to change that trend
for the white sea ice reects solar radiation back into space but as the ice melts the dark water will absorb some of the light warming and melting more ice
words the rate of ice melting in the arctic is increasing and a panel of researchers says it sees no natural process that is likely to change that trend
the white sea ice reects solar radiation back into space but as the ice melts the dark water will absorb some of the light warming and melting more ice
in us scientists issued a report saying the rate of ice melting in the arctic is increasing and within a century could lead to summertime ice free ocean conditions not seen in the area in a million years
words l a n i g i r o e s s e c o r p table example of post processing treatments applied to the summary duced from cluster b of tac
dates are standardized tences are ordered with temporal constraints ambiguous discursive forms are deleted
experiments the method described in the previous section has been implemented and uated by participating to the text analysis conference tac update summarization conducted by the national institute of standards and technology nist
the following subsections present details of the dierent experiments

the tac update track piloted in document understanding duc the update marization task consists in producing a short word summary of a set of newswire articles under the assumption that the user has already read a given set of earlier articles
the purpose of each update summary is to inform the reader of new information about a particular topic
the test data set in tac comprises topics
each topic has a topic statement examples are given in table and relevant documents which have been divided into two document set a and document set b
each document set has documents where all the documents in set a chronologically precede any of the documents in set b
the documents are coming from the collection of news articles
information about the tac update track is available at
nist
gov
nist
data was consisting of three temporal document sets a b and c
arctic and antarctic ice melt describe the developments and impact of the continuing arctic and antarctic ice melts
paris riots describe the violent riots occurring in the paris suburbs beginning ber
include details of the causes and casualties of the riots and government and police responses
table example of topic statements and
given a duc topic and its two document sets a and b the task is to create two brief uent summaries that contribute to satisfying the information need expressed in the topic statement
the rst one is a topic oriented summary of the document set a while the second one is an update summary of the document set b produced under the assumption that the reader has already read documents in set a

evaluation all summaries produced by our approach were evaluated both automatically and manually by the nist
the manual evaluation comprised three scores an overall responsiveness based on both the linguistic quality of the summary and the amount of information in the summary that helps to satisfy the information need expressed in the topic narrative
a linguistic quality guided by consideration of the following factors grammaticality non redundancy referential clarity focus structure and coherence
a pyramid recall score computed on summary content units scus annotations
human annotators select overlapping content in multiple model summaries to construct a pyramid of scus
most existing automated evaluation methods work by comparing the erated summaries to one or more reference summaries ideally produced by humans
in the tac evaluation four human summaries were written for each document set
to evaluate the quality of our generated summaries several automatic measures were computed is a n gram recall measure calculated between a candidate summary and a set of reference summaries
it is computed as between very poor and very good
is available at
isi
edu srref srref n gramss co grams n gramss grams where n stands for the length of the n gram and co grams is the maximum number of n grams co occurring in a candidate summary and a set of reference summaries
in our experiments and rouge will be computed
basic elements is similar to rouge but uses minimal length ments of sensible meaning as units such as kitchen knife or bank of america
in the tac nist received runs from participants for the update summarization task
each participant submitted up to three runs ranked by priority
all runs were evaluated automatically runs but manual evaluations were provided only for runs with priority and runs
in addition one baseline summarizer was included in the evaluation
it consists in returning all the leading sentences up to words in the most recent document
the duc update data was used to train our system and to estimate the interpolation coecient of the similarity measure and the novelty factor
as the duc update task was consisting of three temporal documents sets we have adapted the data set to match the tac guideline by removing the third cluster
parameters for the relevance function and the novelty factor were tuned using this modied data set
the optimal values we have found are
and h c with c for cluster a no history and for cluster b
n
ocial results table shows the results obtained by our submission at the update rization task of tac
our system has achieved good results for overall responsiveness and linguistic quality respectively ranked and out of submissions but average ones for automatic evaluations ranked between the and place out of submissions
giving more condence to manual evaluation we can say that our system performed quite well
one surprising result is that our system has obtained high marks in linguistic quality despite elements is available at
isi
edu the simplicity of our rule based post processing
evaluation overall responsiveness linguistic quality pyramid rouge basic elements score rank






table results of manual and automatic evaluations at the tac update task
for a comparative evaluation figures and show the results obtained by all the systems participating in the update summarization task at tac
the baseline consisting of word summaries generated by taking the rst sentences in most recent articles is also shown in the two gures
it is worth noting that teams were allowed to submit up to three runs generally consisting of dierent parameter congurations
that way the number of submissions that have obtained better marks than our system may have in fact been produced by a number of systems three times lower
being more balanced between content and linguistic evaluations our system always outperforms the widely used based baseline that have been proved to be very challenging
figure scatter plot of linguistic quality and overall responsiveness for the tac update task
our system red star and the baseline big blue mond are highlighted
figure scatter plot of and rouge average recall scores for the tac update task
our system red star and the baseline big blue diamond are highlighted
results for separated document sets are presented in table
one can say that evaluation scores are signicantly lower for summaries of document sets b but it is worth noting that manual evaluation ranks are signicantly better overall responsiveness going from to and linguistic quality from to
this shows that from the linguistic quality point of view our system is less aected by the increasing diculty of update summarization than other approaches
evaluation overall resp
linguistic quality pyramid rouge docset a rank score




score




docset b rank table automatic and manual evaluation results for document set a and b

additional results in these additional experiments rouge scores have been computed using the conguration described in the ocial guidelines of tac
to observe the behavior of our method on presence of noisy data we have added in each cluster a number of random documents taken from dierent clusters
since each cluster contains relevant documents this means a and noise on the data sets
results on noisy data are given in guidelines are available at
nist
gov tac
table
there is no signicant performance loss on our method proving that information retrieval approaches are robust for query oriented summarization
evaluation rouge











table comparison of rouge average recall scores for our system on and noisy tac data
we also wanted to examine the impact of the novelty factor used in equation on the summaries produced for document sets b
on figure we observe an improvement of the rouge scores for all the values greater than zero obtaining the best results for values comprised between
and

the dierence with the optimal value found on the training data is minimal but handicap our performance
the size of the adapted duc training data was obviously too small topics of documents to avoid problems
n figure plot of rouge average recall scores for docset b summaries in relation for the tac update task
to the novelty factor n discussion the summarizer based on the smmr sentence scoring algorithm succeeds in identifying most relevant but containing new facts sentences from clusters of news articles
the results obtained during the tac evaluation prove that our method can achieve good results for both linguistic and content quality
unlike other approaches our system does not use large linguistic or knowledge resources which makes it lightweight and easily adaptable to any other language tac or any domain
computing the whole tac test data takes less than ve minutes on a dual core with of ram
as applications that are ject to use update summarization algorithms are gathering tremendous amount of data such as news aggregators computational complexity is becoming an important feature to take into consideration
we have observed another interesting result on our submission automatic and manual evaluations are not often correlated
to illustrate this lack of relation the topics that within our submission have received the best manual and automatic scores are compared
results are shown in table
as we can see manual and automatic evaluation scores are in total contradiction
indeed according to manual evaluations our best summaries have been generated for the topic while automatic scores for this topic are poor
inversely according to automatic scores our best topic is while its manual scores are very poor
by scrutinizing the generated summaries shown in the table we have identied the reasons of this issue
redundancy is the main factor for these high rouge scores
units of meaning such as the billed woodpecker are split in an incorrect way wrongly increasing the number of matching tokens used for computing recall scores
this example proves that using only automatic evaluations is somehow risky
evaluation overall responsiveness linguistic quality pyramid rouge basic elements













table results of manual and automatic evaluations for topics et
ranks obtained by the topic within our submission are shown in parenthesis
the topic ranked in rst place contains the summaries that have obtained the best scores in comparison to the other topics of our submission
conclusions in this paper we have explained how we had revisited the classical mmr rithm in order to propose a novel approach to update summarization so called the smmr
an important aspect of our approach is that it does not requires ranking nor linguistic which makes it a simple and ecient method to tackle the issue of update summarization
system only uses minimal linguistic resources for post processing that are easily adaptable to any other language
a d b d a d b d martha stewart in prison describe martha stewart s experiences while in prison
new york it s check in day for martha stewart
larry stewart who is not related to martha stewart was acquitted of the charges
q
what will happen to the company martha stewart living omnimedia stewart spends up to three hours a night writing on a prison typewriter with ribbons purchased at a prison store
bacanovic and stewart were both given the option of staying out of prison while they appealed
martha stewart has been exercising reading and making friends in prison but the food at the minimum security prison camp in west virginia is terrible the domestic diva s daughter said
martha stewart in a christmas message posted on her personal web site called for sentencing reform and took a swipe at the bad food in prison
since entering federal prison in october martha stewart has tried her hand at ceramics learned to crochet and become an expert on vending machine snacks
martha stewart who is about to get out of prison seems to have undergone a makeover on the cover of the latest newsweek
one of the tasks ahead of stewart is to try and spin the goodwill she gained in prison into prots for her martha stewart living omnimedia inc
ivory billed woodpecker describe developments in the rediscovery of the ivory billed woodpecker long thought to be extinct
the ivory billed woodpecker a bird long thought extinct has been sighted in the swamp forests of eastern arkansas for the rst time in more than years nell university scientists said
the ivory billed woodpecker long suspected to be extinct has been rediscovered in the big woods region of eastern arkansas searchers reported in the journal science to be published
the ivory billed pecker is one of six north american bird species thought to have gone extinct since
the ivory billed woodpecker once prized for its plumage and sought by american indians as magical was thought to be extinct for years
recordings of the ivory billed woodpecker s distinctive double rap sounds have convinced doubting researchers that the large bird once thought extinct is still living in an east arkansas swamp
the recordings seem to indicate that there is more than one ivory billed woodpecker in the area
for half a century watchers have longed for a glimpse of the ivory billed woodpecker a bird long given up for extinct but recently rediscovered in arkansas
the ivory billed woodpecker was thought to be extinct until it was spotted in the swamps of southeast arkansas in
the ivory bill was or is the largest north american woodpecker
table examples of our submission for the topics and of tac
the novelty factor characterized in our sentence scoring method by a ear function h turns out to be a very important parameter requiring to be tuned in a more judicious manner
using a linear function that relies on the number of previous clusters instead of the exact amount of text can be ardous
high redundancy within news articles forces us to believe that the reader n can gain knowledge of only a reduced number of concepts
this is the reason why we think computing the novelty factor by using the concept redundancy is worthy of further work
recent work by gives some interesting ideas on how to remove redundancy by constructing novel graph based representations from documents
it was pointed out that question answering and query oriented tion have been converging on a common task the value added by summarization lying in the linguistic quality
we have seen that applying simple ruled based linguistic treatments to candidate sentences allows to signicantly increase the linguistic quality
current research works are predominantly focused on the english language
this is why we are currently developing a bilingual evaluation corpus english and french
among the others this point sounds like a promise for further investigation
acknowledgments this work was supported by the agence nationale de la recherche france project

sinequa
com references j
allan r
gupta and v
khandelwal
temporal summaries of new topics
in proceedings of the annual international acm sigir conference on research and development in information retrieval pages
acm new york ny usa
f
boudin
exploration dapproches statistiques pour resume tique
phd thesis universite davignon pays de vaucluse december
florian boudin marc el and juan manuel torres moreno
a able mmr approach to sentence scoring for multi document update marization
in coling companion volume posters and tions pages manchester uk august
coling organizing committee
florian boudin and juan manuel torres moreno
a cosine minimization approach for user oriented multi document update marization
in recent advances in natural language processing ranlp pages borovets bulgaria september
florian boudin juan manuel torres moreno and patricia morales
an ecient statistical approach for automatic organic istry summarization
in bengt nordstrom and aarne ranta editors international conference on natural language processing gotal volume of lecture notes in computer science pages burg sweden august
springer
r
brandow k
mitze and l
f
rau
automatic condensation of electronic publications by sentence selection
information processing and ment
j
carbonell and j
goldstein
the use of mmr diversity based in annual ing for reordering documents and producing summaries
international acm sigir conference on research and development in formation retrieval pages
acm press new york ny usa
h
iii
practical structured learning for natural language ing
phd thesis university of southern california august
h
p
edmundson
new methods in automatic extracting
journal of the acm jacm
g
erkan and d
r
radev
lexrank graph based lexical centrality as salience in text summarization
journal of articial intelligence research
b
hachey g
murray and d
reitter
the embra system at duc query oriented multi document summarization with a very large latent semantic space
in document understanding conference duc ver canada october
a
hickl k
roberts and f
lacatusu
lcc s gistexter at duc machine reading for update summarization
in document understanding conference duc rochester usa april
erhard hinrichs
temporal anaphora in discourses of english
linguistics and philosophy february
e
hovy c
y
lin l
zhou and j
fukumoto
automated in fifth conference on language tion evaluation with basic elements
resources and evaluation lrec may
j
kupiec j
pedersen and f
chen
a trainable document summarizer
in proceedings of the annual international acm sigir conference on research and development in information retrieval pages
acm new york ny usa
yulia ledeneva
eect of preprocessing on extractive summarization with maximal frequent sequences
in micai advances in articial telligence volume of lecture notes in computer science pages
springer berlin heidelberg
wenjie li furu wei qin lu and yanxiang he
ranking tences with positive and negative reinforcement for query oriented update in proceedings of the international conference on summarization
computational linguistics coling pages manchester uk august
coling organizing committee
chin yew lin
rouge a package for automatic evaluation of maries
in stan szpakowicz marie francine moens editor text rization branches out proceedings of the workshop pages barcelona spain july
association for computational linguistics
z
lin t
s
chua m
y
kan w
s
lee l
qiu and s
ye
nus at duc in document understanding using evolutionary models of text
conference duc rochester usa april
h
p
luhn
the automatic creation of literature abstracts
ibm journal of research and development
i
mani g
klein d
house l
hirschman t
firmin and b
sundheim
summac a text summarization evaluation
natural language ing
i
mani and m
t
maybury
advances in automatic text summarization
mit press
g
murray s
renals and j
carletta
extractive summarization of ing recordings
in ninth european conference on speech communication and technology eurospeech lisboa portugal september
ani nenkova and rebecca passonneau
evaluating content selection in summarization the pyramid method
in daniel marcu susan dumais and salim roukos editors hlt naacl main proceedings pages boston massachusetts usa may
association for tational linguistics
d
r
radev and k
r
mckeown
generating natural language summaries from multiple on line sources
computational linguistics
g
salton a
wong and c
s
yang
a vector space model for automatic indexing
communications of the acm
k
sparck jones
a statistical interpretation of term specicity and its application in retrieval
journal of documentation
s
teufel and m
moens
sentence extraction as a classication task
in acl eacl workshop on intelligent and scalable text summarization pages
kapil thadani and kathleen mckeown
a framework for decreasing textual redundancy
in coling manchester uk august
coling organizing committee
w
e
winkler
the state of record linkage and current research problems
statistics of income division internal revenue service publication r
rene witte ralf krestel and sabine bergler
generating update in document understanding conference duc maries for duc
rochester usa april
s
ye l
qiu t
s
chua and m
y
kan
nus at duc ing documents via concept links
in document understanding conference duc vancouver canada october

