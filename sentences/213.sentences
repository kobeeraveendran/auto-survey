strass a light and effective method for extractive summarization based on sentence embeddings leo bouscarrat antoine bonnefoy thomas peel cecile pereira eura nova marseille france leo
bouscarrat antoine
bonnefoy thomas
peel cecile

eu abstract this paper introduces strass tion by transformation selection and it is an extractive text summarization ing
method which leverages the semantic mation in existing sentence embedding spaces
our method creates an extractive summary by selecting the sentences with the closest dings to the document embedding
the model learns a transformation of the document bedding to minimize the similarity between the extractive summary and the ground truth summary
as the transformation is only posed of a dense layer the training can be done on cpu therefore inexpensive
over inference time is short and linear cording to the number of sentences
as a second contribution we introduce the french cass dataset composed of judgments from the french court of cassation and their sponding summaries
on this dataset our sults show that our method performs similarly to the state of the art extractive methods with effective training and inferring time
introduction summarization remains a eld of interest as numerous industries are faced with a growing amount of textual data that they need to process
creating summary by hand is a costly and demanding task thus automatic methods to erate them are necessary
there are two ways of summarizing a document abstractive and tive summarization
in abstractive summarization the goal is to ate new textual elements to summarize the text
summarization can be modeled as a to sequence problem
for instance rush et al
tried to generate a headline from an article
however when the system generates longer maries redundancy can be a problem
see et al
introduce a pointer generator model pgn that generates summaries by copying words from the text or generating new words
moreover they added a coverage loss as they noticed that other models made repetitions on long summaries
even if it provides state of the art results the pgn is slow to learn and generate
paulus et al
added a layer of reinforcement learning on an encoder decoder architecture but their results can present uency issues
in extractive summarization the goal is to tract part of the text to create a summary
there are two standard ways to do that a sequence beling task where the goal is to select the tences labeled as being part of the summary and a ranking task where the most salient sentences are ranked rst
it is hard to nd datasets for these tasks as most summaries written by humans are abstractive
nallapati et al
introduce a way to train an extractive summarization model without labels by applying a recurrent neural network rnn and using a greedy matching proach based on rouge
recently narayan et al
combined reinforcement learning to tract sentences and an encoder decoder ture to select the sentences
some models combine extractive and tive summarization using an extractor to select sentences and then an abstractor to rewrite them chen and bansal cao et al
hsu et al

they are generally faster than models using only abstractors as they lter the input while maintaining or even improving the quality of the summaries
this paper presents two main contributions
first we propose an inexpensive scalable trainable and efcient method of extractive text summarization based on the use of sentence beddings
our idea is that similar embeddings are semantically similar and so by looking at the l u j l c
s c v
v i x r a figure training of the model
the blocks present steps of the analysis
all the elements above the blocks are inputs document embedding sentences embeddings threshold real summary embedding trade off
proximity of the embeddings it is possible to rank the sentences
secondly we introduce the french cass dataset section
composed of judgments with their corresponding summaries
model related work in our model strass it is possible to use an embedding function trained with state of the art methods
is a classical method used to form a word into a vector mikolov et al

methods like keep information about semantics mikolov et al

pagliardini et al
create embedding of tences
it has state of the art results on datasets for unsupervised sentence similarity evaluation
embedrank bennani smires et al
plies to extract keyphrases from a ment in an unsupervised fashion
it hypothesizes that keyphrases that have an embedding close to the embedding of the entire document should resent this document well
we adapt this idea to select sentences for maries section

we suppose that sentences close to the document share some meaning with the document and are sentences that summarize well the text
we go further by proposing a pervised method where we learn a transformation of the document embedding to an embedding of the same dimension but closer to sentences that summarize the text
this paper embedding function embedding space and embedding will refer to the function that takes a textual element as input and outputs a vector the vector space and the vectors
the aim is to construct an extractive summary
our approach strass uses embeddings to lect a subset of sentences from a document
we apply to the document to the tences of the document and to the summary
we suppose that if we have a document with an d and a set s with all the embeddings of the sentences of the document and a reference summary with an embedding ref sum there is a subset of sentences es s forming the reference summary
our target is to nd an afne function irn irn such that if s es t otherwise where t is a threshold and sim is a similarity function between two embeddings
the training of the model is based on four main steps shown in figure transform the document embedding by applying an afne function learned by a ral network section
extract a subset of sentences to form a summary section
approximate the embedding of the tractive summary formed by the selected tences section
are lowercased vectors embeddings are cased and in bold sets are uppercased and matrices are percased and in bold
document embeddingsentences embeddingsneural network to learn a transformationreal summary
of the sentences of the extracted summaryselectionscoringapproximationtransformationapproximation of the embedding of our extracted summaryscoring the quality of our summarytrade score the embedding of the resulting mary approximation with respect to the bedding of the real summary section

to generate the summary only the rst two steps are used
the selected sentences are the output
approximation and scoring are only essary during the training phase when computing loss function

transformation to learn an afne function in the embedding space the model uses a simple neural network
a single fully connected feed forward layer
irn irn w d with w the weight matrix of the hidden layer and b the bias vector
optimization is only conducted on these two elements

sentence extraction inspired by embedrank bennani smires et al
our proposed approach is based on dings similarities
instead of selecting the top n elements our approach uses a threshold
all the sentences with a score above this threshold are lected
as in pagliardini et al
our larity score is the cosine similarity
selection of sentences is the rst element d s t s t with sigmoid the sigmoid function and a normalized cosine similarity explained in section

a sigmoid function is used instead of a hard threshold as all the functions need to be tiable to make the back propagation
sel outputs a number between and
indicates that a tence should be selected and that it should not
with this function we select a subset of sentences from the text that forms our generated summary

approximation as we want to compare the embedding of our erated extractive summary and the embedding of the reference summary the model approximates the embedding of the proposed summary
as the system uses the approximation is the erage of the sentences weighted by the number of words in each sentence
we have to apply this proximation to the sentences extracted with sel which compose our generated summary
the proximation is s t s nb d s t ss where nb is the number of words in the tence corresponding to the embedding s

scoring the quality of our generated summary is scored by comparing its embedding with the reference mary embedding
here the compression ratio is added to the score in order to force the model to output shorter summaries
the compression ratio is the number of words in the summary divided by the number of words in the document
loss nb sum nb cos sum ref sum with a trade off between the similarity and the compression ratio cos y y irn the cosine similarity and gen sum s t
the user should note that is also useful to change the trade off between the proximity of the maries and the length of the generated one
a higher results in a shorter summary

normalization to use a single selection threshold on all our uments a normalization is applied on the ities to have the same distribution for the ties on all the documents
first we transform the cosine similarity from irn irn to irn irn y cos y then as in mori and sasaki the function is reduced and centered in
y x y y
xkx y where y is an embedding x is a set of dings x and are the mean and standard deviation
a threshold is applied to select the closest tences on this normalized cosine similarity
in der to always select at least one sentence we stricted our similarity measure in where for each document the closest sentence has a ilarity of y x y x y x max xkx experiments
datasets to evaluate our approach two datasets were used with different intrinsic document and summary structures which are presented in this section
more detailed information is available in the pendices table gure and gure
we introduce a new dataset for text rization the cass
this dataset is posed of judgments given by the french court of cassation between and and their summaries one summary by original ment
those summaries are written by lawyers and explain in a short way the main points of the judgments
as multiple lawyers have ten summaries there are different types of mary ranging from purely extractive to purely stractive
this dataset is maintained up to date by the french government and new data are regularly added
our version of the dataset is composed of judgements
the cnn dailymail dataset hermann et al
nallapati et al
is composed of couples containing a news article and its highlights
the highlights show the key points of an article
we use the split created by nallapati et al
and rened by see et al


oracles we introduce two oracles
even if these models do not output the best possible results for extractive summarization they show good results
the rst model called oracle is the same as the baseline but instead of taking the document embedding the model takes the embedding of the summary and then extracts the closest sentences
the second model called oraclesent extracts the closest sentence to each sentence of the mary
this is an adaptation of the idea that ati et al
and chen and bansal used to create their reference extractive summaries

evaluation details rouge lin is a widely used set of metrics to evaluate summaries
the three main metrics in this set are and which pare the grams and grams of the generated and reference summaries and rouge l which sures the longest sub sequence between the two summaries
rouge is the standard measure for summarization especially because more cated ones like meteor denkowski and lavie require resources not available for many languages
our results are compared with the unsupervised system textrank mihalcea and tarau rios et al
and with the supervised systems pointer generator network see et al
and rnn ext chen and bansal
the generator network is an abstractive model and rnn ext is extractive
for all datasets a embedding of mension was trained on the training split
to choose the hyperparameters a grid search was computed on the validation set
then the set of hyperparameters with the highest rouge l were used on the test set
the selected hyperparameters are available in appendix a


baseline results an unsupervised version of our approach is to use the document embedding as an approximation for the position in the embedding space used to select the sentences of the summary
it is the application of embedrank bennani smires et al
on the extractive summarization task
this approach is used as a baseline for our model dataset is available here
com euranova cass dataset tables and present the results for the cass and the cnn dailymail datasets
as expected the pervised model performs better than the vised one
on the three datasets the supervision has improved the score in terms of and rouge l
in the same way our oracles are ways better than the learned models proving that there is still room for improvements
information concerning the length of the generated summaries baseline textrank pgn rnn ext strass oracle oracle sent rl




















table results of different models on the french cass dataset using rouge with condence
the models of the rst block are unsupervised the models of the second block are supervised and the models of the last block are the oracles
is the f measure
and rl stand for and rouge l
baseline textrank pgn rnn ext strass oracle oracle sent pgn rl


























results of different models on the table cnn dailymail
the pgn is the lead score as reported in see et al

the scores with are taken from the corresponding publications
is the f measure
and rl stand for and rouge l
figure processing time of the summarization tion y axis by the number of lines of the text as input axis
results computed on an
and the position of the sentences taken are able in the appendices a


on the french cass dataset our method forms similarly to the rnn ext
the pgn performs a bit better


rouge l compared to the other models which could be linked to the fact that it can select elements smaller than sentences
on the cnn dailymail dataset our supervised model performs poorly
we observe a signicant difference

and
rouge l between the two oracles
it could be explained by the fact that the summaries are multi topic and our models do not handle such case
therefore as our loss does nt look at the diversity strass may miss some topics in the generated summary
a second limitation of our approach is that our model does nt consider the position of the tences in the summary information which presents a high relevance in the cnn dailymail dataset
strass has some advantages
first it is able on cpu and thus light to train and run
deed the neural network in our model is only posed of one dense layer
the most recent vances in text summarization with neural networks are all based on deep neural networks requiring gpu to be learned efciently
second the method is scalable
the processing time is linear with the number of lines of the documents figure
the model is fast at inference time as dings are fast to generate
our model generated the summaries of the cass dataset in less than minutes on an cpu
conclusion and perspectives to conclude we proposed here a simple effective and scalable extractive summarization method
strass creates an extractive summary by selecting the sentences with the closest beddings to the projected document embedding
the model learns a transformation of the ment embedding to maximize the similarity tween the extractive summary and the ground truth summary
we showed that our approach obtains similar results than other extractive methods in an effective way
there are several perspectives to our work
first we would like to use the sentence dings as an input of our model as this should crease the accuracy
additionally we want to vestigate the effect of using other ding spaces especially more generalist ones or other embedding functions like le and mikolov or bert devlin et al

for now we have only worked on sentences but this model can use any embeddings so we could try to build summaries with smaller textual ments than sentences such as key phrases noun phrases


likewise to apply our model on topic texts we could try to create clusters of tences where each cluster is a topic and then tract one sentence by cluster
moreover currently the loss of the system is only composed of the proximity and the sion ratio
other meaningful metrics for document summarization such as diversity and tivity could be added into the loss
especially submodular functions could allow to obtain near optimal results and allow to include ments like diversity lin and bilmes
other information we could add is the position of the sentences in the documents like narayan et al

finally the approach could be extended to query based summarization v
v
muralikrishna et al

one could use the embedding tion on the query and take the sentences that are the closest to the embedding of the query
acknowledgement we thank cecile capponi carlos ramisch laume stempfel jakey blue and our anonymous reviewer for their helpful comments
references federico barrios federico lopez luis argerich and rosa wachenchauzer

variations of the larity function of textrank for automated tion
corr

kamil bennani smires claudiu musat andreea mann michael baeriswyl and martin jaggi

simple unsupervised keyphrase extraction using in proceedings of the sentence embeddings
conference on computational natural language learning pages
association for tational linguistics
ziqiang cao wenjie li sujian li and furu wei

retrieve rerank and rewrite soft template based in proceedings of the neural summarization
annual meeting of the association for tional linguistics volume long papers pages
association for computational tics
yen chun chen and mohit bansal

fast tive summarization with reinforce selected sentence rewriting
in proceedings of the annual ing of the association for computational linguistics volume long papers pages
tion for computational linguistics
michael denkowski and alon lavie

meteor universal language specic translation evaluation for any target language
in proceedings of the ninth workshop on statistical machine translation pages
association for computational tics
jacob devlin ming wei chang kenton lee and kristina toutanova

bert pre training of deep bidirectional transformers for language in proceedings of the conference standing
of the north american chapter of the association for computational linguistics human language technologies volume long and short papers pages minneapolis minnesota
ation for computational linguistics
karl moritz hermann tomas kocisky edward grefenstette lasse espeholt will kay mustafa leyman and phil blunsom

teaching chines to read and comprehend
in proceedings of the international conference on neural mation processing systems volume pages cambridge ma usa
mit press
wan ting hsu chieh kai lin ming ying lee kerui min jing tang and min sun

a unied model for extractive and abstractive summarization using inconsistency loss
in proceedings of the annual meeting of the association for tional linguistics volume long papers pages
association for computational tics
quoc le and tomas mikolov

distributed sentations of sentences and documents
in ings of the international conference on chine learning volume of proceedings of chine learning research pages bejing china
pmlr
chin yew lin

rouge a package for automatic in text summarization evaluation of summaries
branches out proceedings of the shop pages barcelona spain
association for computational linguistics
hui lin and jeff bilmes

a class of submodular functions for document summarization
in ings of the annual meeting of the association for computational linguistics human language technologies volume hlt pages stroudsburg pa usa
association for tional linguistics
rada mihalcea and paul tarau

textrank bringing order into text
in proceedings of emnlp pages barcelona spain
tion for computational linguistics
linguistics volume long papers pages
association for computational linguistics
r v
v
muralikrishna s y
pavan kumar and ch reddy

a hybrid method for query based automatic summarization system
tional journal of computer applications
a appendices a
datasets the composition of the datasets and the splits are available in table
a
preprocessing on the french cass dataset we have deleted all the accents of the texts and we have lower cased all the texts as some of them where entirely cased without any accent
to create the maries all the ana parts of the xml les vided in the original dataset where taken and catenate to form a single summary for each ument
these summaries explain different key points of the judgment
on the cnn dailymail the preprocessing of see et al
was used
as an extra cleaning step we deleted the documents that had an empty story
a
hyperparameters to obtain the embeddings functions for both datasets we trained a model of dimension with unigrams on the train splits
for the cass dataset the baseline model has a threshold at
the oracle at
and strass has a threshold at
and a at

textrank was used with a ratio of

the pgn for the cnn dailymail dataset the baseline model has a threshold at
the oracle at
and strass has a threshold at
and a at

textrank was used with a ratio of

a
results a

rouge score more detailed results are available in tables and
high recall with low precision is generally onym of long summary
tomas mikolov kai chen greg corrado and jeffrey dean

efcient estimation of word tations in vector space
corr

tomas mikolov wen tau yih and geoffrey zweig

linguistic regularities in continuous space in proceedings of the word representations
conference of the north american chapter of the association for computational linguistics human language technologies pages atlanta georgia
association for computational linguistics
tatsunori mori and takuro sasaki

tion gain ratio meets maximal marginal relevance a method of summarization for multiple documents
in ntcir
ramesh nallapati feifei zhai and bowen zhou

summarunner a recurrent neural network based sequence model for extractive summarization of documents
corr

ramesh nallapati bowen zhou cicero dos tos caglar gulcehre and bing xiang

abstractive text summarization using sequence sequence rnns and beyond
in proceedings of the signll conference on computational natural language learning pages berlin many
association for computational linguistics
shashi narayan shay b
cohen and mirella lapata

do nt give me the details just the summary topic aware convolutional neural networks for in proceedings of the treme summarization
conference on empirical methods in natural guage processing pages brussels gium
association for computational linguistics
shashi narayan shay b
cohen and mirella lapata

ranking sentences for extractive in rization with reinforcement learning
ings of the conference of the north american chapter of the association for computational guistics human language technologies volume long papers pages
association for computational linguistics
matteo pagliardini prakhar gupta and martin jaggi

unsupervised learning of sentence dings using compositional n gram features
corr

romain paulus caiming xiong and richard socher

a deep reinforced model for abstractive marization
corr

alexander m
rush sumit chopra and jason weston

a neural attention model for abstractive in proceedings of the tence summarization
conference on empirical methods in natural guage processing pages
association for computational linguistics
abigail see peter j
liu and christopher d
manning

get to the point summarization with generator networks
in proceedings of the nual meeting of the association for computational dataset cass cnn dailymail sd

ss

td ts train val test table size information for the datasets sd and ss are respectively the average number of sentences in the document and in the summary td and tt are respectively the number of tokens in the document and in the summary
train val and test are respectively the number of documents in the train validation and test sets
baseline textrank pgn rnn ext strass oracle oracle sent p r p r rl p rl r rl






























































table full results of different models on the french cass dataset using rouge with condence
the models in the rst part are unsupervised models then supervised models and the last part is the oracle
p is precision r is recall and is the f measure
and rl stand for and rouge l
baseline textrank pgn rnn ext strass oracle oracle sent lead pgn





p r p r rl p rl r rl
























































table full results of different models on the cnn dailymail
the pgn is the lead score as reported in see et al

the scores with are taken from the corresponding publications
p is precision r is recall and is the f measure
and rl stand for and rouge l
model reference strass oracle oracle sent s



w w s



size information for the generated summary on the test split of the cass dataset s w w s are respectively the average number of sentences the average number of words and the average number of words per sentences
percentage of times that a sentence is taken in a erated summary in function of their position in the ment on the cass dataset
density of the number of sentences in the generated summaries for several models and the reference on the cass dataset
density of the number of words in the generated maries for several models and the reference on the cass dataset
figure information about the length of the generated summaries for the cass dataset
ten the fourth sentences than the rst three and still have better results than the which means that the fourth sentences could have some interest
with strass the rst three sentences have a ferent tendency than the rest of the text showing that the rst three sentences may have a different structure than the rest
then the farther a sentence is in the text the lower the probability to take it
a

words and sentences on the french cass dataset the summaries erated by the models are generally close in terms of length number of words number of sentences and number of words per sentences gure
all the tested extractive methods tend to lect sentences at the beginning of the documents
the rst sentence make an exception to that rule gure
we observe that this sentence can have the list of the lawyers and judges that were present at the case
strass tends to generate longer summaries with more sentences
the discrepancy in the average number of sentences between the reference and oraclesent is due to sentences that are extracted multiple times
on the cnn dailymail dataset strass tends to extract less sentences but longer ones ing to the oraclesent gure
on the gure we can see that the three models tend to extract different sentences
oraclesent which is the best performing model tends to tract the rst sentences oracle extracts more






taken in summariesstrassoracleoracle












in the resultsdensity of the number of sentencesstrassoracleoracle





in the resultsdensity of the number of wordsstrassoracleoracle sentreference model reference strass oracle oracle sent s



w w s

size information for the generated summary on the test split of the cnn dm dataset s w w s are tively the average number of sentences the average ber of words and the average number of words per tences
percentage of times that a sentence is taken in a erated summary in function of their position in the ment on the cnn dm dataset
density of the number of sentences in the generated summaries for several models and the reference on the cnn dm dataset
density of the number of words in the generated summaries for several models and the reference on the cnn dm dataset
figure information about the length of the generated summaries for the cnn dm dataset






taken in summariesstrassoracleoracle












in the resultsdensity of the number of sentencesstrassoracleoracle






in the resultsdensity of the number of wordsstrassoracleoracle sentreference
