c e d l c
s c v
v i x r a mudos ng multi document summaries using n gram graphs tech report george giannakopoulos ncsr demokritos greece george vouros university of the aegean greece vangelis karkaletsis ncsr demokritos greece october abstract this report describes the mudos ng summarization system which applies a set of language independent and generic methods for generating extractive summaries
the posed methods are mostly combinations of simple operators on a generic character n gram graph representation of texts
this work denes the set of used operators upon n gram graphs and proposes using these operators within the multi document summarization cess in such subtasks as document analysis salient sentence selection query expansion and redundancy control
furthermore a novel chunking methodology is used together with a novel way to assign concepts to sentences for query expansion
the experimental results of the summarization system performed upon widely used corpora from the document derstanding and the text analysis conferences are promising and provide evidence for the potential of the generic methods introduced
this work aims to designate core methods exploiting the n gram graph representation providing the basis for more advanced rization systems
introduction since the late and luhn luhn the information community has expressed its interest in summarizing texts
the domains of application of such methodologies range from news rization wu and liu barzilay and mckeown radev et al
to scientic cle summarization teufel and moens and meeting summarization niekrasz et al
erol et al

summarization has been dened as a reductive transformation of a given set of texts usually described as a three step process selection of salient portions of text aggregation of the tion for various selected portions and abstraction of this information and nally presentation of the nal summary text mani and bloedorn jones
the summarization community aims to address major problems that arise during the summarization process
how can one detect and select salient information to be included in the summary does the use of a query drive the information selection task and how how can one assure that the nal summary does not contain redundant or repeated formation especially when multiple documents are used as input to the summarization process can one develop methods that will function independently from the language of documents and on what degree can this be achieved up to date many summarization systems have been developed and presented especially within such endeavors as the document understanding conferences duc and text analysis conferences
the summarization community appears to have moved from single text to multi text input and has also reached such domains as opinion summarization and trend summarization as in the case of
however dierent evaluations performed in recent years have proved that the multi summarization task is highly complex and demanding and that automatic summarizers have a long way to go to perform equally well to humans dang dang dang and owczarzak
it was recently shown genest et al
that the extractive approach has an upper limit of performance which is lower when compared to the abstractive approach of humans
however extractive summarization appears to have more room for improvement in order to reach that upper limit of performance which is set by humans performing extractive summarization through simple sentence selection and reordering
even the current methods for the evaluation of summaries are under criticism jones conroy and dang giannakopoulos et al

indeed it has been shown that ing dierent aspects of a summary is far from being a trivial task
nevertheless when it comes to ranking summarization systems i
e
the average performance of a summarization system over a set of summaries the existing evaluation methodologies oer quite good correlation to human judgment dang dang and owczarzak giannakopoulos et al

within this work we tackle the problems of salience detection and redundancy control in extractive multi document summarization using a unied language independent and generic framework based on n gram graphs
the contributed methods oer a basic language neutral easily adaptable set of tools
the basic idea behind this framework is that neighborhood and relative position of characters words and sentences in documents oer more information than that of the bag of words approach
furthermore the methods go beyond the word level of analysis into the sub word character n gram level which oers further exibility and dence from language and acts as a uniform representation for sentences senses documents and document sets
through this study we provide a proof of concept methodology that can be used in more advanced summarization systems
we also experiment using this methodology as a basic summarization system named mudos ng
in the following sections we briey review the current literature section we present the proposed approach section and perform a set of experiments to evaluate its performance and show its potential section
the article concludes with discussion and proposals for further development section
the literature presently the literature of automatic multi document summarization has grown to a level that is very hard to overview in detail jones
however one can identify specic commonalities in the way summarizers extract and reproduce information into output summaries
rizing systems are usually classied as being either extractive or abstractive in their proach mani and bloedorn
extractive approaches focus on the extraction and use of text snippets from the source texts in the nal summary
abstractive text chunks i
e
approaches on the other hand aim to rst represent information using an intermediate sentation for example rst order logic and then use language generation to produce the output
nist
and
nist
gov

nii
ac
jp summary from the representation
even though it has been shown that humans summarize in an abstractive fashion banko and vanderwende endres niggemeyer many current systems continue to use the extractive paradigm to perform summarization
this may be due to the lack of highly robust text to intermediate representation methods as well as natural guage generation methods
on the other hand the description of a summarization system as purely abstractive can be disputed and it appears that summarization systems can dierentiate themselves according to their purpose jones
in the following paragraphs we overview the approaches existing for salience detection and redundancy removal as this is the focus of our proposed work
we also make a brief literature review for graph based methods to introduce the reader to related work from the domain of graphs

salience detection to determine salience of information researchers have used positional and structural properties of the judged sentences with respect to the source texts
these properties can be the sentence tion e

number of sentence from the beginning of the text or from the beginning of the current paragraph in a document or the fact that a sentence is part of the title or of the abstract of a ument edmundson radev et al

also the relation of sentences with respect to a user specic query or to a specied topic conroy et al
varadarajan and hristidis park et al
are features providing evidence towards importance of information
sion proper name anaphora reiteration synonymy and hypernymy and coherence based on rhetorical structure theory mann and thompson relations between sentences were used in mani et al
to dene salience
based on a graph representation where each sentence is a vertex and vertices are connected when there is a cohesion or coherence relation between them e

common anaphora the salience of a sentence is computed as an operation dependent on the graph representation e

spreading activation starting from important nodes
torralbo et al

often following the bag of words assumption a sentence is represented as a word feature vector e

in such cases the sequence of the represented words is ignored
the vector dimensions represent word frequency or the term frequency inverse document frequency tf idf value of a given word in the source texts
in other cases further analysis is performed aiming to reduce dimensionality and produce vectors in a latent topic space steinberger and jezek flores et al

vector representations can be exploited for measuring the semantic similarity between information chunks by using measures such as the cosine distance or euclidean distance between vectors
when the feature vectors for the chunks have been created clustering of vectors can be formed for identifying clusters corresponding to specic topics
a cluster can then be represented by a single vector for example the centroid of the corresponding cluster s vectors radev et al

chunks closest to these representative vectors are considered to be the most salient
we must point out that for the aforementioned vector based approaches one needs to perform ing to avoid pitfalls due to stop words and inection of words that create feature spaces of very high dimension
however the utility of the preprocessing step which usually involves stemming and stop word removal is an issue of dispute ledeneva leite et al

more recent approaches use machine learning techniques and sets of dierent features to determine whether a source text chunk sentence should be considered salient and included in the output summary
in that case the feature vector calculated for every sentence may include information like sentence length sentence absolute position in the text sentence position within its corresponding paragraph number of verbs and so forth e

see teufel and moens
it has been shown that for specic tasks like the news summarization task of duc ple positional features for the determination of summary sentences can prove very promising for summarization systems dang
however this may falsely lead to the expectation that features like the ones corresponding to the rst sentence heuristic i
e
the features that indicate whether a sentence appears to be similar in content and properties to the rst tences of a set of training instances can be used as a universal rule
the example of short stories is an example where a completely dierent approach is taken to perform the rization kazantseva and szpakowicz the summary is expected to describe the setting without giving away the plot
in jatowt and ishizuka we nd an approach where aware summaries take into account the frequency of terms over time in dierent versions of web pages to determine salience
in varadarajan and hristidis the authors create a graph where the nodes sent text chunks and edges indicate relation between the chunks
however in contrast to our work the authors in varadarajan and hristidis consider as optimal summary the maximum spanning tree of the document graph that contains all the keywords the graph in varadarajan and hristidis is not a character n gram graph there is no proposed methodology for the chunking other than a given parsing delimiter parameter not all the edges are kept only those above a given threshold parameter
furthermore in varadarajan and hristidis the tf okapi function is used to assign weights to nodes indicating self importance of a node
in multi document summarization dierent iterative ranking algorithms like pagerank brin and page and hits kleinberg over graph representations of texts have been used to determine the salient terms over a set of source texts mihalcea
salience has also been determined by the use of graphs based on the fact that documents can be represented as small world topology graphs matsuo et al
where important terms appear highly linked to other terms
ing the salient terms one can determine the containing sentences salience and create the nal summary
in another approach hendrickx and bosma content units sentences are assigned a normalized value to based on a set of graphs representing dierent aspects of the content unit
these aspects include query relevance cosine similarity of sentences within the same document termed relatedness cross document relatedness which is considered an aspect of redundancy redundancy with respect to prior texts and coreference based on the number of coreferences between dierent content units
all the above aspects and their corresponding graphs are combined into one model that assigns the nal value of salience using an iterative cess
the process spreads importance over nodes based on the probabilistic centrality method that takes into account the direction of edges to either augment or penalize the salience of nodes based on their neighbors salience
the notion of bayesian expected risk or loss is applied in the summarization domain by kumar et al
where the selection of sentences is viewed as a decision process where the selection of each sentence is considered a decision and the system has to select the sentences that minimize the risk
see conroy et al
the classy system e

conroy et al
extracts quently occurring signature terms from source texts as well as terms from the user query
using these terms the system estimates an oracle score for sentences which relates the terms contained within the candidate sentences to an estimated ideal distribution based on term pearance in the query the signature terms and the topic document cluster
some optimization method mostly integer linear programming is then used to determine the best set of sentences for a given length of summary given sentence weights based on their oracle score
this article proposes a summarization method that uses language independent and generic operators that apply to a generic representation of chunks based on interconnected n grams
the n gram approach has much in common with giannakopoulos et al
for chunk and thus sentence similarity
this method overcomes the need for any kind of preprocessing and oers a basic i
e
core method for extractive summarization
even the chunking process which separates a sentence into sub sentential strings is based upon statistical analysis of a given document set
the method does not use the bag of words approach as the n gram graphs take into account the relevant position of n grams in the text
we do not use any features like sentence position or part of speech information
our method does not deduce salience based on the centrality or connectedness of graph vertices
the method extracts a graph expected to represent the common content of input texts which is in turns considered to indicate salient information
given a user query the approach combines using n gram graph operators the common content graph with the given query into an overall importance indicative graph
then we calculate the salience of source text sentences based on the similarity of their respective n gram graph representation to the overall graph i
e
the more a sentence representation is similar to the representation of content and query the more it is considered appropriate for the nal summary
our methodology is described in depth in section

redundancy and novelty a problem that is somewhat complementary to salience selection is that of redundancy tion
whereas salience which is a desired attribute for the information chunks in the summary can be detected via measuring the similarity of these chunks to a query redundancy indicates the unwanted repetition of information
research on redundancy has given birth to the marginal evance measure carbonell and goldstein and the maximal marginal relevance mmr selection criterion
the basic idea behind mmr is that good summary sentences or ments are sentences or documents that are relevant to a topic without repeating information already in the summary
the mmr measure is a generic linear combination of any two principal functions that can measure relevance and redundancy
another approach to the redundancy problem is that of the cross sentence informational subsumption csis radev et al
where one judges whether the information oered by a sentence is contained in another sentence already in the summary
the informationally sumed sentence can then be omitted from the summary
the main dierence between the two approaches is the fact that csis is a binary decision on information subsumption whereas the mmr criterion oers a graded indication of utility and non redundancy
other approaches overviewed in allan et al
use statistical characteristics of the judged sentences with respect to sentences already included in the summary to avoid tion
such methods are the newword and cosine distance methods larkey et al
that use variations of the bag of words based vector model to detect similarity between all pairs of candidate and summary sentences
other language model based methods create a language model of the summary sentences either as a whole or independently and compare the language model of the candidate sentence to the summary sentences model zhang et al

the didate sentence model with the minimum kl divergence from the summary sentences language model is supposed to be the most redundant
the classy system conroy et al
conroy et al
represents documents in a term vector space and enforces redundancy through the following process given a pre existing set of sentences a corresponding to a sentence term matrix ma and a currently judged set of sentences b corresponding to a matrix mb b is judged using the term sub space that is orthogonal to the eigenvalues of the space dened by a this means that only terms that are not already considered important in a will be taken into account as valuable content
in this work we have used and evaluated two dierent strategies for the detection of tion redundancy
these strategies use a statistical graph based model of sentences by exploiting character n grams
the rst strategy similarly to csis compares all the candidate sentences and determines the redundant ones
the second strategy aiming to detect intra summary elty and more similar to mmr creates an iterative n gram graph model for every snapshot of the summary after a new sentence is added to it
then each new candidate sentence is compared to that graph model to determine redundancy

graph based methods and graph matching graphs have been used to determine salient parts of text mihalcea erkan and radev erkan and radev or query related sentences otterbacher et al
in close relation to the summarization process
lexical relationships mohamed and rajasekaran or ical structure marcu and even non apparent information lamkhede have been represented by graphs
graphs have also been used to detect dierences and similarities tween source texts mani and bloedorn and inter document relations witte et al
as well as relations of various granularity from cross word to cross document as described in cross document structure theory radev
in this work the graphs are used to represent strings of any length or granularity from chunk to sentence to document set
throughout the proposed methodologies we use a set of dierent operators like similarity merging intersection to perform dierent subtasks of the summarization process query expansion content selection redundancy control
graph similarity calculation methods can be classied in two main categories according to the literature
isomorphism based isomorphism is a bijective mapping between the vertex set of two graphs such that all mapped vertices are equivalent and every pair of vertices from in other shares the same state of neighborhood as their corresponding vertices of
words in two isomorphic graphs all the nodes of one graph have their unique equivalent in the other graph and the graphs have identical connections between equivalent nodes
based on the isomorphism a common subgraph between can be dened as a subgraph of having an isomorphic equivalent graph which is a subgraph of
the maximum common subgraph of and is dened as the common subgraph with the maximum number of vertices
for more formal denitions and an excellent introduction to the error tolerant graph matching i
e
fuzzy graph matching see bunke
given the denition of the maximum common subgraph a series of distance measures have been dened using various methods for the calculation of the maximum common subgraph or similar constructs like the maximum common edge subgraph or maximum common induced graph also see raymond et al

edit distance based edit distance has been used in fuzzy string matching for some time now using many variations see navarro for a survey on approximate string matching
the edit distance between two strings corresponds to the minimum number of edit ter operations namely insertion deletion and replacement needed to transform one string to the other
based on this concept a similar distance can be used for graphs bunke
dierent edit operations can be given dierent weights to indicate that some edit tions indicate more important changes than others
the edit operations for graphs nodes are node deletion insertion and substitution
the same three operations can by applied on edges giving edge deletion insertion and substitution
using a transformation from text to graph the aforementioned graph matching methods can be used as a means to indicate text similarity
a graph method for text comparison can be found in tomita et al
where a text is represented by rst determining weights for the text s terms using a tf idf calculation and then by creating graph edges based on the term occurrences
the method proposed in this article does not require term extraction identication and the corresponding representation graph is constructed by exploiting the text in a direct manner i
e
no language dependent preprocessing is required without exploiting further ground supportive information such as a corpus for the calculation of tf idf or any other weighting factor
the main dierence of our method to existing methods is that we enter the sub word level through the use of character n grams
we aim to perform all required tasks towards the marization of a set of documents using a uniform representation of sentences senses documents and document sets regardless of the underlying language
moreover we map seemingly dierent summarization subtasks such as content selection and query expansion to a set of basic graph operators that function as generic purpose nlp operators over a common representation
in order to understand the n gram graph representation we use one should take into account the fact that adjacency between dierent linguistic units within specic contexts seems to be a very important factor of meaning
contextual information has been widely used and several methodologies have been built upon its value e

burgess et al
yarowsky
our methodology described in detail in section can be summarized by the following main steps analysis of source documents content
query expansion
candidate content grading
redundancy removal or novelty detection
composition of the summary
in the following paragraphs we present the framework i
e
the representation and the operators that is used throughout these steps
having said that we need to emphasize on the point that a single framework allows for dierent operators on the natural language processing nlp domain
an early presentation of the concepts and processes described herein can be found in giannakopoulos et al

n gram graphs operators and algorithms we now provide the denition of n gram given a text viewed as a character sequence denition
if n n z and ci is the i th character of an l length character sequence t l


our text then a character n gram sn


sn is a subsequence of length n of t l for i l n and j n sj
we shall indicate the n gram spanning from ci to ck i as si while n grams of length n will be indicated as sn
the meaning of the above formal specication is that n gram sn can be found as a substring of length n of the original text spanning from the i th to the i n character of the original text
the length n of an n gram is called either the length size or the rank of the n gram
the n gram graph is a graph g v g eg l w where v g is the set of vertices eg is the set of edges l is a one to one function assigning a label to each vertex and edge and w is a function assigning a weight to every edge
we consider that l labels edges by concatenating the labels of the corresponding vertices in the direction of the edge if the edge is directed or in lexicographic order if the edge is undirected adding also a special separator character
labels in vertices are n grams vg v g and the edges eg eg the superscript g will be omitted where easily assumed connecting the n grams indicate adjacency of the corresponding vertex n grams in a specic context within distance dwin also see giannakopoulos et al

example
vertex corresponds to n gram abc and to bcd
then abc bcd
if the edge connecting and then abc bcd where is the special separator character
the edges within this work are weighted by applying the number of co occurrences of the vertices n grams within the given window in the original documents
for simplicity when for a vertex v v g we may also write v g the same notation may be used for an edge e eg where we may write e g
given two instances of n gram graph representation there is a number of operators that can be applied on to provide the n gram graph equivalent of union intersection and other such operators of set theory
in our summarization task these operators are useful as primary tools for all the subtasks i
e
salience detection novelty detection redundancy removal query expansion
an example of such an operator is the merging operator of and corresponding to the union operator in set theory
this operator adds all edges from both operand graphs to a third one while making sure no duplicate edges are created
two edges are considered duplicates of each other when they share identical vertices
we note that the denition of identity between vertices can be customized within our applications two vertices are the same if they correspond to the same n gram
the denition of the graph operators is actually non trivial because a number of questions arise such as the handling of weights on common edges after a union operator or the meaning and thus handling of the zero weighted edges after the application of any operator
all the operators that we shall present operate on edges only because we consider single nodes to be of little value
we argue that information is contained within the relation between n grams and not in the n grams themselves
therefore our minimal unit of interest is the edge which is actually a pair of vertices
overall we have dened a number of operators all of which with the exception of similarity are functions from g g to g where g is the set of valid n gram graphs of a specic rank n
thus operators function upon graphs of a given rank and produce a graph of the same rank
the operators are the following
the similarity function sim g g r which returns a value of similarity between two n gram graphs
this function is symmetric in that
there are many variations of the similarity function within this study each tted to a specic task
the common ground of these variations is that the similarity values are normalized in the interval with higher values indicating higher actual similarity between the graphs
the computation of similarity is described in subsection

the containment function contains which indicates what part of a given graph is contained in a second graph
this function is expected to be asymmetric
in other words should the function indicate that a graph is contained in another graph we know nothing about whether the inverse stands
in the implementations of the containment function proposed herein result values are normalized in the interval with higher similarity values indicating that a bigger part of a given graph is contained in a second graph
we consider a graph to be contained within another graph if all its edges appear within the latter
if this is not the case then any common edge contributes to the overall containment function a percentage inversely proportional to the size of so that the function value indicates what part of is contained within
the computation of containment is described in subsection

the merging or union operator returns a graph with all the edges both common and uncommon of the two operand graphs
in our implementation of this operator we have decided that the union operator will set the weight of a common edge equal to the average of the weights into the corresponding graphs
the intersection operator returns a graph with the common edges of the two operand graphs with averaged weights
the averaging over edge weights is based on the idea that the intersection and the union of two graphs should be a graph that includes the edges of both operand graphs with edge weights as close as possible to both the original graphs
the union merge and intersection operators are presented in section

the delta operator also called all not in operator returning the subgraph of a graph that is not common with a graph
this operator is non symmetric i
e
in general
the inverse intersection operator returning all the edges of two graphs that are not common between the graphs
this operator is symmetric i
e

zero weighted edges are treated as all other edges even though zero weight means that the edge does not exist i
e
the vertices are not related
the empty graph is a graph with no nodes and no edges
the size of any graph is its edge count and is indicated as for graph
the algorithm we use giannakopoulos et al
to convert a given string to its character n gram graph representation is quite simple extract all character n grams of rank n from a given text and create graph vertices one for every unique n gram
the vertices are labeled by their corresponding n gram
add edges connecting all n grams that occur at least once within a given distance dwin of each other in the string
in this work the weight of the added edge is the number of co occurrences of the corresponding

similarity and containment to represent a character sequence or text we can use a set of n gram graphs for various n gram ranks instead of a single n gram graph
to compare a sequence of characters in a chunk a sentence a paragraph or a whole document i
e
in any textual chunk we apply variations of a single algorithm that acts upon the n gram graph representation of the character sequences
the algorithm is actually a similarity measure between two n gram graph sets corresponding to two texts and
this similarity can be indicative of the similarity of content of two information chunks in the way any fuzzy string matching technique is
we can therefore apply the application of operators between graphs the edge weights e

of union graphs may represent average co occurrences or other functions of co occurrences thus not being integer numbers
this similarity measurement to determine whether e

a given sentence is related to a user query of a summarization task
the we consider that an n gram graph maps to a given n gram rank in this work i
e
rank r is a parameter of the n gram graph
given that the representation of a text ti is a set of graphs gi containing graphs of various ranks and two texts we use the value similarity vsr to compare graphs of the same rank
so for every n gram rank r of we use the corresponding graph of rank r e

and
the measures how many of the edges contained in graph are contained in graph considering also the weights of the matching edges
in this measure each matching edge e having weight e in graph to the sum while not matching edges do not contribute consider contributes that for an edge e we dene e
the valueratio vr scaling factor is dened as the equation indicates that the valueratio takes values in and is symmetric
thus the full equation for vsr is vsr is a measure converging to for graphs that share edges with similar weights which means that a value of vsr indicates perfect match between the compared graphs
another tant measure is the normalized value similarity nvs which is computed as e e e e gj e wj e egi e wj e vsr the fraction of similarity where the ratio of sizes of the two compared graphs does not play a role
is also called size similarity
the nvs is a measure the overall similarity vsoof the sets is computed as the weighted sum of the vs over all ranks lmax r vsr lmax r where vsr is the vs measure for extracted graphs of rank r in g and lmin lmax are arbitrary chosen minimum and maximum n gram ranks
the function contains realizing the graph containment operator has a small but signicant dierence from the value similarity function it is not commutative
more precisely if we call value containment vcr the containment function using edge weights then vcr is e e e e the denominator is the cause for the asymmetric nature of the function and makes it spond to a graded membership function between two graphs

graph union or merging and intersection the union or merge operator has two important aspects
the rst deals with unweighted edges as pairs of labeled vertices e while the second considers the weights of the edges as well
the merge operator denes the basic operator required to perform updates in the graph model
the intersection operator on the other hand can be used to determine the common subgraphs of dierent graphs
this use has a variety of applications such as common topic detection in a set of documents see section
or the detection of stopword eect edges
the stopword eect edges are edges that are apparent in the graphs of most texts of a given language and have high frequency much like stopwords
the detection of stopword eect edges in n gram graphs can be accomplished by simply applying the intersection operator upon graphs of adequately big texts of various dierent topics
the resulting graph will represent that part of language that has appeared in all the text graphs and can be considered noise
more on the notion of noise in relation to n gram graphs can be found in section


when performing the union of two graphs we create a graph gu v u eu l w u such that eu eg are the edge sets of correspondingly
in out implementation we consider two edges to be equal when they have the same label i
e
which means that the weight is not taken into account when calculating eu
where eg eg eg to calculate the weights in gu there can be various functions depending on the eect the merge should have over weights of common edges
one can follow the fuzzy logic paradigm and keep the maximum of the weights of a given edge in two source graphs w where are the weighting functions of the corresponding graphs and e is a common edge of and
another alternative would be to keep the average of the values so that the weight represents the expected value of the weights of the original weights
given these basic tives we chose the average value as the default union operator eect on edge weights
it should be noted that the merging operator is a specic case of the graph update function presented in section

formally if are the edge sets of correspondingly w u is the result graph edge weighting function and are the weighting functions of the operand graphs with e eu w i then the edge set eu of the merged graph is eu w e the intersection operator returns the common edges between two graphs performing the same averaging operator upon the edges weights
formally the edge set ei of the intersected graph is ei e w e
delta all not in and inverse intersection the delta or all not in operator is a non commutative operator that given two graphs returns the subset of edges from the rst graph that do not exist in the second graph
formally the edge set is e the weight of the remaining edges is not changed when applying the delta operator
obviously the operator is non commutative
a similar operator is the inverse intersection operator which creates a graph that only contains the non common edges of two graphs
the dierence between this operator and the delta operator is that in the inverse intersection the edges of the resulting graph may belong to any of the original graphs
formally the resulting edge set is e consider the labeling function to be the same over all graphs
both the delta and the inverse intersection operators can be applied to determine the ences between graphs
this way one can e

remove a graph representing noisy content from another graph
another application is determining the non common part of two texts that deal with the same subject which may refer to the unique or novel part of each text with respect to the subject

representing document sets updatability the n gram graph representation specication indicates how to map a text to an n gram graph
however in our task it is required to represent a whole document set
the most simplistic way to do this using the n gram graph representation would be to concatenate the documents of the set into a single overall document but this kind of approach would not oer an updatable model i
e
a model that could easily change when a new document enters the set
in our applications we have used an update function u that is similar to the merging operator with the exception that the weights of the resulting graph s edges are calculated in a dierent way
the update function u l takes as input two graphs one that is considered to be the pre existing graph i
e
a graph that may have resulted by a sequence of applications of the update operator on an initial graph and one that is considered to be the new graph
the function also has a parameter called the learning factor l which determines the sensitivity of to the changes brings
focusing on the weighting function of the graph resulting from the application of u l the higher the value of learning factor the higher the impact of the new graph to the existing graph
more precisely the denition of the weighting performed in the graph resulting from u is w w w w l according to this formula the value of l indicates that will completely ignore the new graph
a value of l indicates that the weights of the edges of will be assigned the values of the new graph s edges weights
a value of
gives us the simple merging operator
the u operator allows using the graph as a representation model for a set of documents
this approach is used in our case to represent the common content of source documents
the training step for the creation of the content representation model comprises the initialization of a common content graph with the representation of the rst document and the subsequent update of that initial graph with the graphs of the other documents
especially when one wants the common content graph s edges to hold weights averaging the weights of all the individual graphs that have contributed to the common content graph then the i new graph that updates the common content graph should use a learning factor of l
i i i this methodology creates a common content graph that can function as a representative graph for all the source documents in that we expect it to be as close as can be to the individual graphs of the individual documents in terms of value similarity
when the common content graph is created one can determine whether a new document is similar to the content of the source documents by measuring the similarity of the document graph to the common content graph
this information has been used to determine the salience of information chunks in section

we have further used the update operator to determine noisy information in terms of useless graph edges
this is illustrated in the following paragraphs


determining noise using the n gram graph operators when determining the common content graph one faces the presence of noise within the graph
the noise within the graphs for a classication task would be the set of common subgraphs over all classes of documents as they do not oer distinctive information
similarly in our summarization task we consider that the part of the common content graph that would appear no matter the underlying topic of the sources is noise
in traditional text classication techniques stopword removal is usually used to remove what is considered to be the noisy part of the data
up to this point we have seen that n gram graphs do not need such preprocessing
however based on the task we can usually identify non interesting parts of data that hinder the task
this noise can be removed via the already proposed n gram graph algorithms
in order to see what we should do to remove the noise we will use the related paradigm of a classication task
for the classication task we consider to have a set of training documents for a number of classes
we consider noise the part of information in the graphs that does not help determine the class of a document
if we manage to nd this information as represented in the graphs we will be able to remove the noise from the common content graph
in the case of a classication task we create a merged graph using the update operator for the full set of training documents tc belonging to each class
after creating the classes model graphs one can determine the maximum common subgraph between classes and remove it to improve the distinction between dierent classes
a number of questions arise given this train of thought how can one determine the maximum common subgraph between classes according to our operators it is easy to see that the maximum common subgraph is the intersection of all the class graphs
in other words the same operator that is used to determine the common content within a class of documents becomes useful as noise indicator when doing inter class analysis
is this unique no it is not
even though the intersection operator is associative if edge weights are omitted the averaging of edge weights per intersection causes associativity
in other words due to the calculations of the edge weights
can we approximate the noisy subgraph without iterating through all the classes yes
as can be seen in figure the noisy subgraph can be easily approximated in very few steps
in the gure the horizontal axis indicates the number of consecutive intersections performed between the classes graphs and the vertical axis illustrates the number of edges of the resulting intersected graph
it shows that even from the third iteration there is only insignicant change in the resulting graph size
does the removal of the noisy from each class graph really improve results the answer is yes as we will immediately show
to support our intuition that there is a common part of the n gram graphs whose eect is the same as that of noise i
e
what we have called stopword eect edges we performed a set of experiments on classication which can be easily related to performing topic detection over a set of topics or to performing common content extraction in the analysis step of our summarization methodology
we created a topic graph for each of the tac topics based on the documents contained within each topic
then we tried to classify each of these documents to a corresponding topic
the tac site at
nist
gov tac for more
figure convergence of common graph size over the number of intersections the classication was performed by measuring the similarity of a judged document to a set of topic representative graphs
the topic of the graph with the maximum similarity was selected as the topic of the document
this way we wanted to see if all documents would be found to be maximally similar to their topic graphs since the training instances would be expected to be recognized as belonging to their original topic
if that was not the case then perhaps this would be the eect of common elements between the content of dierent topics i
e
noise
as an indication of the performance of the classication we use the recall value i
e
the value of the number of documents that were correctly indicated as belonging to a given class divided by the numbers of all documents that belong to the class
the histogram of recall values for all classes topics before removing the noise is shown in gure
the recall histogram after removing the alleged noise can be seen in gure
the y axis indicates the number of classes for which the recall value was within a given range axis
thus the axis indicates the recall value ranges
of course ideally all the values should be
to indicate perfect classication
from the gure it is shown that several class results are moved towards higher recall values
to see whether the results illustrate statistically signicant improvement we used a paired wilcoxon ranked sum test wilcoxon because of the non normal distribution of dierences between the recall of each class in the noisy and noise free tests
the test indicated that within the statistical condence level p value
the results when removing the noise were indeed improved
the conclusion of this experiment is that the removal of noise can really help determine relation to a topic using the n gram graph representation
this advances the eectiveness of the content selection process when a noise free topic graph is available
therefore in our content analysis of the original graphs we make use of the noise removal process to keep a noise free graph
including noise without noise figure recall values histogram for all the classes used in the classication task with and without noise
the proposed methodology and system mudos ng in this section we provide an overview of the mudos ng system also see figure as well as an in depth analysis of the proposed individual steps
the analysis of source documents content step gets as input a set of source documents from which it extracts and represents through the n gram graph representation of documents and operators such as the intersection the information that is common
this information is considered to be important due to its presence in all the source documents
the output of the step is the common content graph see more in section
representing the common part of the source documents
the query expansion step which is an optional step aims to annotate a user supplied query sentence indicating the subject of the requested summary with a set of concepts so as to expand it
the input of this step is the query which is expected to be free text and the output is a graph representing the expanded version of the query
the candidate content grading step assigns scores to sentences from the source documents in order to evaluate the salience of each sentence
the grading takes into account the user query as well as the information common to all source documents and outputs an ordered list of sentences
the next step uses either a redundancy removal or a novelty detection approach to avoid repetition in the output summary
this step either eliminates or reranks the already graded and sorted candidate summary sentences in order to avoid or penalize repetition of information
this step is in fact integrated in the summary creation nal step
its input is the set of candidate sentences and its output is a possibly reranked subset of the candidate sentences
given the output of the previous steps the nal step creates the summary as a sequence of selected candidate sentences
we have not used any sentence ordering method to improve the output of the system as our main purpose was to determine whether the n gram graph based tools we devised can be useful throughout the summarization process
this system aims to class











figure mudos ng system overview summarysource texts analysisquery expansionquerycommoncontentexpandedquerycandidatecontentcandidate contentgradingredundancy removalornovelty detection provide core methods exploiting the n gram graph representation providing the basis for more advanced summarization systems
experiments indicate that indeed even without the sentence ordering or any rewrite methodology our system provides promising results

analysis of source documents content to analyze the source documents we need to be able to identify and represent the minimal units of information
in other approaches this minimal unit of information would be the word but we need to remain language independent and also take into account the fact that a word can also be split in sub word parts
to do this we use the next character entropy chunking section


each chunk which may be part of a word or longer than a word e

part of a collocation is then represented by its corresponding n gram graph
to identify whether this splitting of the sentence into smaller parts makes any dierence in judging the salience of a sentence we perform experiments see section taking into account two alternatives creating a graph per sentence versus creating a graph per chunk
in the following subsection we describe the solutions we have applied to identify these chunks


next character entropy chunking to determine the information chunks that will be used in the steps leading to the nal summary we need to determine the appropriate delimiters
to do this in a language neutral way we exploit our document corpus to determine the probability p that a single given character c will follow a given character n gram sn for every character c in the corpus
the probabilities can then be used to calculate the entropy of the next character for a given character n gram sn as follows
if ci i n i m the set of characters that have been found to follow the given n gram sn and fi the frequency count of ci being found after sn then the next character entropy of in the document corpus is given by pi p fi m fi pi pi m the entropy measure indicates uncertainty
we have supposed that substrings of a character sequence where the entropy of p surpassed a statistically computed threshold represent candidate delimiters
the threshold is based on the analysis of entropy values that are illustrated in figure on the left
we noted that one can detect three fuzzy regions in the entropy graph the rst is the region containing delimiters the second is the region containing non delimiters and the third contains symbols that have very low entropy of next character i
e
they are part of common syllables
the regions are dened by non trivial changes in the curve of the entropy measure
to detect the most prominent changes we measured the delta of the entropy values for successive symbols in the graph
the delta dh is the absolute value of the change in entropy between two successive symbols and is illustrated in figure on the right
in both gures the horizontal line parallel to the symbol axes indicates the mean value of entropy and entropy delta in each gure correspondingly
given the ordered set h


hm of values n figure entropy per symbol left and delta of entropy per symbol right ordered scending n n


which the set of n grams in a corpus then the delta value dh sk n of an n gram for sk n with a value hk as dh hk m dh hm where is the absolute value operator
the entropy value corresponding to the local maximum of the entropy delta in the right half of the symbol entropy delta function is selected as the threshold depicted by a dark circle in the gure for determining delimiter characters
this happens because we consider exactly three areas as is depicted in the left part of gure and we expect their split points to be on either side of the middle symbol in the delta ordered list of symbols
formally the threshold is given by argmax m hi where is the oor operator returning the closest integer lower than or equal to
in our application we have only checked for unigrams i
e
simple letters as delimiters even though delimiters of higher rank can be determined
for example in bi gram chunking the sequence comma and space would be considered to be delimiter while in unigrams the space character only would be considered delimiter
given a character sequence sn and a set of delimiters d our chunking algorithm splits the string after every occurrence of a delimiter d
this way a sentence is split into a set of chunks that can be then assigned salience during the content selection process

query expansion summarization systems like the ones presented within the duc and tac communities need to be able to respond to specic queries
in information retrieval for some time now the use of query expansion has been shown to be useful at times
we wanted to try using query expansion in this summarization system as well to determine whether it oers improvement to the system performance
to perform query expansion we use a two step process
first we devise a methodology that can map a sentence to a set of concepts provided external knowledge
once again our analysis of the sentence tries to remain as language independent as possible even though the use of the external resource may dictate a specic language
the second step for the query expansion is the use of the concepts descriptions mapped to the query sentence in order to expand the graph representation of the query
we elaborate on these two steps in the following sections


mapping a sentence to a set of concepts using external knowledge within the scope of our work we tried to map sentences to concepts i
e
to terms with well dened meaning
usually this happens by looking up words in thesauri e

net miller et al

in our approach we have used a decomposition module based on the notion of the symbolic graph
a symbolic graph is a graph where each vertex contains a string and edges are connecting vertices in a way indicative of a substring relation
as an example if we have two strings abc ab labeling two corresponding vertices then since ab is a substring of abc there will be a directed edge connecting ab to
in general the symbolic graph of a given text t contains every string in t and for every string it illustrates the set of substrings that compose it
when a symbolic graph has been constructed then one can run through all the vertices of the graph and look up each vertex in a thesaurus to determine if there is a match with an entry
if the thesaurus contains an entity lexicalized by the vertex string then the vertex is assigned the corresponding term meaning
in cases of polysemous terms then the vertex is annotated with all possible meanings of this term
this annotated graph together with a facility that supports comparing meanings is what we call the semantic index
the semantic index therefore represents links between n grams and their semantic parts implemented as e

wordnet denitions which are textual descriptions of the possible senses
such denitions are used within example

if are the sets of denitions of two terms then to compare the semantics meaning of using the semantic index we actually compare the n gram graph representation i j of each pair of denitions of the given terms
within this section we consider the meaning of a term to map directly to the set of possible senses the term has
the relatedness of meaning relmeaning is considered to be the averaged sum of the similarities over all pairs of denitions of the compared terms when represented as n gram graphs this use of relatedness implies that uncertainty concerning the actual meaning of terms is handled within the measure itself because many alternative senses i
e
high will cause a lower result of relatedness
an alternative version of the relatedness measure that only requires a single pair to be similar to determine high relatedness of the meanings is the following
relmeaning within our examples in this section we have used equation
example
compare smart clever wordnet sense denitions for clever cagey cagy canny clever showing self interest and shrewdness in dealing with others a cagey lawyer too clever to be sound symbolic graph can also be represented more eciently as a trie fredkin
smart smart run run run run hollow hollow pretty stupid jump walk die operate empty relmeaning







table other examples of comparisons in ascending relatedness value using sense overview apt clever mentally quick and resourceful an apt pupil you are a clever man


you reason well and your wit is stoker clever cunning ingenious showing inventiveness and skill a clever gadget the cunning maneuvers leading to his success an ingenious solution to the problem wordnet sense denitions for smart smart showing mental alertness and calculation and resourcefulness chic smart voguish elegant and smart new suit of voguish cut bright smart characterized by quickness and ease in children are brighter in one subject than children talk earlier than the average fresh impertinent impudent overbold smart saucy sassy wise improperly forward or be fresh with of a child to lecture a impudent boy given to insulting get wise with me smart painfully gave the dog a smart blow smart quick and gave him a smart walked at a smart pace smart capable of independent and apparently intelligent weapons relatedness of meaning relmeaning
in table we present some more pairs of terms and their corresponding relatedness values
these preliminary results indicate that even though the measure appears to have higher values for terms with similar meaning it may be biased when two words have similar spelling
this happens because the words themselves appear in the denitions causing a partial match between otherwise dierent denitions
the results further depend heavily on the textual description i
e
denition of any term s individual sense synset in wordnet
the results from the examples when using synonyms as descriptors of individual senses can be seen in table
we notice that e

the words smart and clever are found to have no relation whatsoever because no common synonyms are found within wordnet
furthermore since a given word always appears in its own synonym list word spelling similarity still plays an important role when judging relatedness between senses of two dierent words
for example the words hollow and holler have an important overlap in terms of spelling
this makes some of their senses have high overlap which then biases the comparison process to consider the corresponding senses related
the use of a semantic index is that of a meaning look up engine
we remind the reader that the semantic index is actually an annotated symbolic graph
if there is no matching vertex in the graph to provide a meaning for a given input string then the string is considered to have the meaning of its closest in terms of graph path length substrings that have been given a meaning
this inheritance of meaning from short to longer strings is actually based on the intuition that a text chunk contains the meaning of its individual parts
furthermore a word may be broken down to elementary constituents that oer meaning
if one uses an ontology or even a thesaurus including prexes suxes or elementary morphemes and their meanings to annotate smart run smart run smart run hollow run hollow clever jump stupid walk pretty die empty operate holler relmeaning








table other examples of comparisons in ascending relatedness value using only synonyms from overview the symbolic graph then the resulting index becomes a powerful semantic annotation engine in that it can combine the meaning of sub word items to determine the word meaning
on the other hand since composite words or collocations are not necessarily the simple composition of the meaning of their parts in several cases we expect the resulting meaning annotation to be false
in the context of this work we have combined a symbol graph and wordnet into a semantic index to annotate queries with meanings and perform query expansion knowing that the process will be non optimal


query expansion through graph merging query expansion is based on the assumption that a set of words related to an original query can be used as part of the query itself to improve the set of the returned results
in the literature much work has indicated that query expansion should be carefully applied in order to improve results voorhees qiu and frei
in our approach we have used a semantic index based on wordnet s overview of senses
an example of such overview of senses for the words test ambiguous can be seen in example

example
overview of verb test the verb test has senses first from tagged texts
test prove try try out examine essay put to the test as for its quality or give experimental use to this approach has been tried with good results test this recipe
screen test test or examine for the presence of disease or infection screen the blood for the hiv virus
quiz test examine someone s knowledge of something the teacher tests us every week we got quizzed on french irregular verbs
test show a certain characteristic when tested he tested positive for hiv
test achieve a certain score or rating on a test she tested high on the lsat and was admitted to all the good law schools
test determine the presence or properties of a substance
test undergo a test she does nt test well overview of adj ambiguous the adj ambiguous has senses first from tagged texts
equivocal ambiguous open to two or more interpretations or of uncertain nature or significance or often intended to mislead an equivocal statement the polling had a complex and equivocal or ambiguous message for potential female candidates the officer s equivocal behavior increased the victim s uneasiness popularity is an equivocal crown an equivocal response to an embarrassing question
ambiguous having more than one possible meaning ambiguous words frustrated by ambiguous instructions the parents were unable to assemble the toy
ambiguous having no intrinsic or objective meaning not organized in conventional patterns an ambiguous situation with no frame of reference ambiguous inkblots for a given word w in the original query the overview of senses is returned by the semantic index from these senses si i we only utilize senses sj with graph representations gsj whose graph similarity to the common content graph cu is greater than a given threshold see section
for the denitions of the used functions gsj cu and cu t t
we remind the reader that the content graph is the intersection of all the graph representations of the texts in the set
finally the query is expanded by merging the representation of the original query gq and the representations gsj of all the senses that have been ltered according to the procedure giving we can judge the importance a new query based content denition cu
we refer of sentences simply by comparing the graph representation of each sentence to the cu to the removal of the irrelevant denitions from the overview of senses as sense lter

having calculated cu even though the query expansion process was nally rather successful in our rst attempt for query expansion noise was added due to chunks like an in and o which were directly assigned the wordnet meanings of angstrom inch and oxygen correspondingly
this lowered the evaluation scores of our submitted runs in tac giannakopoulos et al

using the sense lter the deciency has been tackled in the current version of the system signicantly improving overall performance

the content selection process concerning the content matching part of the presented summarization system the following basic assumptions have been made
the content cu of a text set u is considered to be the noise free intersection of all the graph representations of the texts in the set cu tugt where gt is the graph representation of text t over all the arbitrary selected n gram ranges
in other words we consider that the common part of the graph representation of all the texts in a topic indicates the common content of the texts
through the optional query expansion which is process one can nally use instead of cu the query based content denition cu determined as described in the previous section
a sentence s is considered more similar to the content cu of a text set as more of the sentence s chunks sub strings of a sentence have an n gram graph representation similar to the corresponding content representation
every chunk s similarity to the content is added to the overall similarity of a sentence to the content
the chunks of a sentence are extracted using the aforementioned entropy based approach section


as we will see in the experiments in section we test whether the use of chunks dierentiates the performance of the system as opposed to the case that each sentence is considered a single chunk
according to the chunking process each sentence is assigned a score which is actually the sum of the normalized value similarities see eq
of its chunks to the content
this process oers an ordered list of sentences l
a naive selection algorithm would then select the scoring sentences from the list until the summary word count limit is reached
however this would not take redundancy into account and thus this is where redundancy removal comes in

redundancy removal and novelty detection in the composition of the nal summary the step of redundancy control is fully integrated
as we will shortly describe two alternatives were studied concerning the control of redundant information one detects novelty of a new piece of information with respect to the current summary snapshot or to a user model the other detects redundancy among the whole set of source information pieces before the creation of the summary
these approaches just represent two dierent views of the same problem which is the control of redundant information within a multi document summary
the novelty detection process has two aspects the intra summary novelty and the summary or user modeled novelty
the intra summary novelty refers to the novelty of a sentence in a summary given the content of the summary at a specic time point
in order to ensure intra summary novelty one has to make sure that every sentence minimally repeats already existing information
to achieve this goal we use the following process
extract the n gram graph representation of the summary so far indicated as gsum

keep the part of the summary representation that does not contain the common content of the corresponding document set u sum gsum cu

for every candidate sentence in l that has not been already used a extract its n gram graph representation gcs
keep only cs gcs cu because we expect to judge redundancy for the part of the n gram graph that is not contained in the common content cu
c assign the similarity between cs sum as the sentence redundancy score

for all candidate sentences in l set the score of the sentence to be its rank based on the similarity to cu minus the rank based on the redundancy score

select the sentence with the highest score as the best option and add it to the summary

repeat the process until the word limit has been reached or no other sentences remain
the inter summary or user modeled novelty refers to the novelty of information apparent when the summarization process takes into account information already available to the reader as per the tac update task
this information can be contained in a user model keeping track of the most recent summaries provided to that user
in the tac summarization task systems are supposed to take into account the rst of two sets per topic set a as prior user knowledge for the summary of set b of the same topic
in fact set a contains documents cerning a news item e

antarctic ice melting that have been published before the documents in set b
we have used the content of the given set a cua in the redundancy removal process considering it to be the pre existing user model
to do that we always merged the representation of set a to the representation of the current snapshot of the summary
in other words the content of set a appears to always be included in the current version of the summary and thus new sentences avoid redundancy with respect to a
within this work we have also implemented a method of redundancy removal as opposed to novelty detection where redundancy is pinpointed within the original set of candidate sentences we consider a sentence to be redundant if it surpasses an empirically computed threshold in the experiments this threshold had a value of
of overlap to any other candidate sentence
in each iteration within the redundancy removal process each sentence is compared only to sentences not already marked as redundant
as a result of this process only the sentences that are not marked as redundant are used in the output summary
given the whole set of tools we described so far we now provide some experimental results of applying variations of the aforementioned methodology and the conclusions reached
experiments we conducted numerous experiments using the tac corpus
we consider each variation of our system based on a dierent parameter set to be a dierent system with a dierent system id
our main target is to see how our components aect the summarization process as a whole and not to judge individual steps separately
ideally the summaries should be judged by humans but there are automatic methods lin hovy et al
giannakopoulos et al
that correlate well to human judgment
we have used the autosummeng giannakopoulos et al
as our system evaluation method since it consistently correlates well to the duc and tac manually assigned siveness measure
responsiveness rst appeared in the document understanding conference duc of
this extrinsic measure has been used in later ducs as well
in duc the appointed task was the synthesis of a word well organized answer to a complex question where the data of the answer would originate from multiple documents dang
in duc the question the summarizing peers i
e
summarizer systems or humans were supposed to answer consisted of a topic identier a title a narrative question and a granularity indication with values ranging from general to specic
the responsiveness score is an extrinsic measure that was supposed to provide as dang states in dang a coarse ranking of the summaries for each topic according to the amount of information in the summary that helps to satisfy the information need expressed in the topic statement at the level of granularity requested in the user prole
in the automatically evaluating summaries of peers aesop task of tac the autosummeng method was shown to still be one of the top performing methods in terms of correlation to responsiveness dang giannakopoulos and karkaletsis
thus the evaluation using the automatic autosummeng measure is meant to result in a partial ordering indicative of how well a given summary answers a given question taking into account the cations of the question
in the following paragraph we dene the task upon which mudos ng was evaluated using autosummeng
in tac there were two tasks
the main task was to produce a word summary from a set of documents summary a
the update task was to produce a word summary from a set of subsequent documents with the assumption that the information in the rst set is already known to the reader summary
there were topics with documents per topic threshold should be computed via experiments or machine learning to relate with human estimated redundancy of information but this calculation has not been performed in this work

nist
gov tac presentations
pdf for an system id cs ss rr nd qe ne score





table autosummeng summarization performance for dierent settings concerning scoring redundancy and query expansion
legend cs chunk scoring ss sentence scoring rr redundancy removal nd novelty detection qe query expansion ne no expansion
best performance in bold
in chronological order
each summary was to be extracted based on a topic description dened by a title and a narrative query
for every topic model summaries were provided for evaluation purposes
at this point we indicate the pitfalls in using an overall evaluation measure like meng rouge lin or basic elements hovy et al
also see belz for a related discussion small variations in system performance are not indicative of real performance change due to statistical error
when judging a system
summary only
the measure can say little about individual summaries because it correlates really well the measure can not judge performance of intermediate steps because it judges the output the measure can only judge the summary with respect to the given model summaries
given the above restrictions we have performed experiments to judge the change in performance when using chunk salience scoring versus sentence salience scoring
redundancy removal versus novelty detection
query expansion versus no query expansion
in addition to the results of applying the dierent system congurations on the tion task indicated in table we performed an anova analysis of variance test to determine whether the system id i
e
system conguration is an important factor for the meng similarity of the peer text to the model texts
it was shown with a value below that there are topics of various diculty and the topic is an important factor for system formance
selection of dierent components for the summarizer from the range of our proposed components can aect the summaries quality
the nding was in fact that the systemid is an important factor of the performance
overview of the text analysis conference summarization update task of
system tac sysid top peer last peer peer average all peers proposed system autosummeng score


std
dev


table autosummeng performance data for tac
note the top and last peers are based on the autosummeng measure performance of the systems
the systems using chunk scoring have no statistically signicant dierence in performance from the ones that use sentence scoring as the paired t test gave a value of

however the systems using chunk scoring namely systems and had a slightly lower average performance than the others
the systems using redundancy removal appear to have statistically signicant dierence in performance from the ones that use novelty detection nearly at the
condence level one sided t test
system was chosen to not use any redundancy removal method and performs near the average of all other systems thus no conclusion can be drawn
concerning query expansion it was not proved whether query expansion indeed oers improvement as the t test gave a value of

this result is consistent with the slight improvement indicated in conroy et al
where the reverse mapping of a porter stemmer was used to expand the query with other versions of its words e

adding s as verb sux or noun sux to terms in the query
similar non decisive results were found by blake et al
where query expansion was determined to be of little use after experiments were applied
in table information on the average performance of tac participants over all topics is illustrated
more on the performance of tac systems can be found in dang and owczarzak
our system performs slightly below average but quite better than the last successful participant
this is very encouraging for the potential of the proposed summarization method as it is based on generic algorithms performed on a generic representation providing core operators for addressing the diculties in each single summarization step
moreover the language neutrality of the method shows that it may provide a steady basis which can be made more eective when combined with heuristics and machine learning methods exploiting language dependent characteristics
to further examine the performance of our system in other corpora we performed rization using the conguration that performed optimally in the tac corpus on the corpora of duc year
systems in duc were to synthesize from a set of documents a brief well organized uent answer to a non trivially expressed declaration of a need for information
this means that the query could not be answered by just stating a name date quantity or similar singleton
the organizers of duc nist also developed a simple baseline system that returned all the leading sentences of the text eld of the most recent document for each topic up to words dang
in table we illustrate the performance of our proposed system on the duc corpus
it is shown that the system strongly outperforms the baseline system and is less than a dard deviation
below the mean performance
of all the participating systems
from the comparison between the results on the duc and the tac task we can conclude that our proposed system performed better in terms of responsiveness in the generic summarization task of duc than in the update task of tac
to identify the exact defects of the tac summaries is non trivial and requires further investigation across several dimensions what are the problems that aect the performance is it the content selection the ordering system duc sysid baseline top peer last peer peer average all peers proposed system autosummeng score



std
dev


table autosummeng performance data for duc
note the top and last peers are based on the autosummeng measure performance of the systems
of sentences the anaphora problems the lack of coherence or something else can the problem or error be quantied if yes how if not can a qualitative ranking of quality be applied and then approximated by a some evaluation methodology how can we minimize the error or maximize the quality nevertheless it is very important that the proposed summarization components oered petitive results without using machine learning techniques combined with a rich set of sentence features like sentence position or grammatical properties
this indicates the usefulness of n gram graphs as well as the generality of application of the n gram graph operators and functions
ever other components need to be added to reach state of the art performance given the existing means of evaluation
these components should aim to improve the overall coherence of the text and tackle problems of anaphora resolution for examples of such problems see the summary in the appendix section a
discussion and future work we have oered a generic method based on the language neutral representation and algorithms of n gram graphs aiming to tackle a number of automatic summarization problems salience detection we have indicated ways to determine the content of a cluster of documents and judge salience for a given sentence
redundancy removal we have presented two dierent approaches following the csis and mmr paradigms
query expansion we have proposed a scheme to broaden a given query with a slightly proving eect over the summaries
the query expansion module is partially dependent on the language in that it requires a thesaurus in the same language as the original query to perform expansion
from the alternatives we examined within the experiments as far as responsiveness of the summaries is concerned it stands that whole sentence scoring should be preferred to chunk based sentence scoring
query expansion does not oer signicant improvement even though it does not appear to penalize the performance either
redundancy removal performs better that novelty detection
the newspaper international herald tribune reported on friday that production problems at one of airbus s main parts plants in germany was at the root of the problem rather than any safety or quality issues
construction problems have delayed the introduction of the double deck the largest passenger plane in the world
prang said airbus s management had made that announcement after analysing the production timetable for the whole project and that no one factor could be blamed for the delay
we are in the process of reviewing the timetable
airbus superjumbo passes emergency evacuation test
figure sample summary for the a topic of tac on the airbus production and launch news
the experimental results presented judged only one aspect of our system namely its sponsiveness
based on these results we have seen that combining dierent methods for the components of the overall summarization method one can achieve signicantly dierent results
it is very important however that the proposed summarization components oered tive results through simple application of n gram graph theory and methodologies without any optimization on specic corpora
as a side result we have shown that there is a way to detect and remove noisy patterns from within n gram graphs using simple graph operators
furthermore we have illustrated that the removal of these patterns can improve the results of certain tasks
these tasks like classication and topic detection should be investigated through the n gram graph representation prism to determine the potential of this representation as a generic nlp tool
it is obvious from the experiments that individual components are not easy to judge as parts of a summarization system
focused and exhaustive performance evaluations should be carried out to identify the impact of each component to the overall performance
it might also be needed that components are examined outside the summarization context as stand alone methods for chunking semantic annotation redundancy detection
in the future we plan to test the eect of using various n gram ranks within dierent parts of the summarization process
we have so far intuitively concluded that n grams of lower ranks express the grammar model of a given language i
e
the set of allowed sequences of acters while higher rank n grams cross over the word boundaries and oer topic information
in giannakopoulos et al
there exists a methodology for the detection of determined important substrings of a text called symbols within the context of that work
the use of these symbols only within n gram graphs may alleviate the insertion of noise within the dierent summarization steps and diminish the computational cost of the method
furthermore emergent subgraphs and paths within a document set graph may allow for the extraction of non obvious relations between text snippets as well as the detection of discourse phenomena and subtopics within a document set
these phenomena and subtopics can then be used to improve the structure and sentence ordering of the summary as it has been shown that sentence ordering has an important eect on summarization dang barzilay et al

last but denitely not least we need to evaluate the summaries extracted in any of the given corpora under the view of additional textual qualities i
e
regardless of any related score
we should identify in what way the individual extracted summaries see figure for a sample summary and appendix a are worse from gold standard summaries
only then will we be able to improve on our promising current work
we note that the whole mudos ng source code is available and under constant revision and improvement as part of the jinsect open source to facilitate the study of the methods we have presented

net projects for more
acknowledgments the research described within this article was supported by the research and development project which is in turn funded by the greek general secretariat for research and nology
as a sentence splitting module we have used the sentencesplitter module of the javarap toolkit see
comp
nus
edu
nlptools javarap
html
references allan et al
allan j
wade c
and bolivar a

retrieval and novelty detection at the sentence level
in proceedings of the annual international acm sigir conference on research and development in information retrieval pages
acm new york ny usa
banko and vanderwende banko m
and vanderwende l

using n grams to derstand the nature of summaries
in susan dumais d
m
and roukos s
editors naacl short papers pages boston massachusetts usa
association for putational linguistics
barzilay et al
barzilay r
elhadad n
and mckeown k

inferring strategies for sentence ordering in multidocument news summarization
journal of articial intelligence research
barzilay and mckeown barzilay r
and mckeown k
r

sentence fusion for multidocument news summarization
computational linguistics
belz belz a

that s nice


what can you do with it comput
linguist

blake et al
blake c
kampov j
orphanides a
west d
and lown c

ch at duc query expansion lexical simplication and sentence selection strategies for multi document summarization
in proceedings of document understanding conference duc workshop
brin and page brin s
and page l

the anatomy of a large scale hypertextual web search engine
computer networks and isdn systems
bunke bunke h

error tolerant graph matching a formal framework and rithms
advances in pattern recognition lncs
burgess et al
burgess c
livesay k
and lund k

explorations in context space words sentences discourse
discourse processes
carbonell and goldstein carbonell j
and goldstein j

use of mmr based reranking for reordering documents and producing summaries the
in proceedings of the annual international acm sigir conference on research and development in information retrieval pages
acm press new york ny usa
conroy et al
conroy j
schlesinger j
and oleary d

classy at duc
in proceedings of document understanding conference duc workshop
also
ontosum
conroy et al
conroy j
schlesinger j
oleary d
and goldstein j

back to basics classy
in proceedings of document understanding conference duc workshop volume
conroy and dang conroy j
m
and dang h
t

mind the gap dangers of divorcing evaluations of summary content from linguistic quality
in proceedings of the international conference on computational linguistics coling pages ester uk
coling organizing committee
conroy et al
conroy j
m
schlesinger j
d
and oleary d
p

classy summarization and metrics
proceedings of the text analysis conference tac
conroy et al
conroy j
m
schlesinger j
d
and stewart j
g

classy based multi document summarization
in proceedings of the document understanding conf
wksp
duc at the human language technology conf

on empirical ods in natural language processing hlt emnlp
dang dang h
t

overview of duc
in proceedings of the document derstanding conf
wksp
duc at the human language technology conf

on empirical methods in natural language processing hlt emnlp
dang dang h
t

overview of duc
in proceedings of hlt naacl
dang dang h
t

overview of the tac summarization track slides
in proceedings of the text analysis conference tac
dang and owczarzak dang h
t
and owczarzak k

overview of the tac update summarization task
in tac workshop notebook papers and results pages maryland md usa
edmundson edmundson h

new methods in automatic extracting
journal of the acm jacm
endres niggemeyer endres niggemeyer b

human style www summarization
erkan and radev erkan g
and radev d
r

lexrank graph based lexical centrality as salience in text summarization
journal of articial intelligence research
erkan and radev erkan g
and radev d
r

michigan at duc using sentence prestige for document summarization
proceedings of the document understanding conferences boston ma
erol et al
erol b
lee d
hull j
center r
and menlo park c

multimodal summarization of meeting recordings
in multimedia and expo

proceedings
international conference on volume
flores et al
flores j
g
gillard l
ferret o
and de chandelar g

bag of senses versus bag of words comparing semantic and lexical approaches on sentence extraction
in tac workshop notebook papers and results pages maryland md usa
fredkin fredkin e

trie memory
communications of the acm
genest et al
genest p
lapalme g
m
and montreal q

tac the creation of a manual extractive run
notebook gaithersburg maryland usa nov
giannakopoulos and karkaletsis giannakopoulos g
and karkaletsis v

n gram graphs representing documents and document sets in summary system evaluation
in ceedings of text analysis conference to appear
giannakopoulos et al
giannakopoulos g
karkaletsis v
and vouros g

in tac workshop testing the use of n gram graphs in summarization sub tasks
notebook papers and results pages maryland md usa
giannakopoulos et al
giannakopoulos g
karkaletsis v
vouros g
and p

summarization system evaluation revisited n gram graphs
acm trans
speech lang
process

hendrickx and bosma hendrickx i
and bosma w

using coreference links and in proceedings of the text analysis sentence compression in graph based summarization
conference tac
hovy et al
hovy e
lin c
y
zhou l
and fukumoto j

basic elements
jatowt and ishizuka jatowt a
and ishizuka m

temporal multi page rization
web intelligence and agent systems
jones jones k
s

automatic summarising the state of the art
information processing management
text summarization
jones jones s

automatic summarizing factors and directions pages
kazantseva and szpakowicz kazantseva a
and szpakowicz s

summarizing short stories
computational linguistics
kleinberg kleinberg j

authoritative sources in a hyperlinked environment
journal of the acm
kumar et al
kumar c
pingali p
and varma v

estimating risk of picking a sentence for document summarization
in proceedings of the international conference on computational linguistics and intelligent text processing pages
springer
lamkhede lamkhede s

multidocument summarization using concept chain graphs
master s thesis
larkey et al
larkey l
allan j
connell m
bolivar a
and wade c

umass at trec cross language and novelty tracks
pages
national institute of standards technology
ledeneva ledeneva y

eect of preprocessing on extractive summarization with maximal frequent sequences
in in proceedings of micai pages
leite et al
leite d
s
rino l
h
m
pardo t
a
s
and nunes m
v

extractive automatic summarization does more linguistic knowledge make a dierence in proceedings of the second workshop on textgraphs graph based algorithms for natural language processing pages rochester ny usa
association for computational guistics
lin lin c
y

rouge a package for automatic evaluation of summaries
ceedings of the workshop on text summarization branches out was pages
luhn luhn h
p

automatic creation of literature abstracts the
ibm journal of research and development
mani and bloedorn mani i
and bloedorn e

multi document summarization by graph search and matching
in proceedings of pages
aaai
mani et al
mani i
bloedorn e
and gates b

using cohesion and coherence models for text summarization
in intelligent text summarization symposium pages
mani and bloedorn mani i
m
and bloedorn e
m

summarizing similarities and dierences among related documents
information retrieval
mann and thompson mann w
c
and thompson s
a

rhetorical structure theory a theory of text organization
university of southern california information ences institute
marcu marcu d

theory and practice of discourse parsing and summarization the
the mit press
matsuo et al
matsuo y
ohsawa y
and ishizuka m

a document as a small world
in proceedings the world multi conference on systemics cybenetics and infomatics volume pages
mihalcea mihalcea r

graph based ranking algorithms for sentence extraction applied to text summarization
in proceedings of the annual meeting of the association for computational lingusitics acl volume
acl
mihalcea mihalcea r

multi document summarization with iterative graph based algorithms
in proceedings of the first international conference on intelligent analysis ods and tools ia
mclean
miller et al
miller g
a
beckwith r
fellbaum c
gross d
and miller k
j

introduction to wordnet an on line lexical database
international journal of lexicography
mohamed and rajasekaran mohamed a
a
and rajasekaran s

query based summarization based on document graphs
navarro navarro g

a guided tour to approximate string matching
acm computing surveys
niekrasz et al
niekrasz j
purver m
dowding j
and peters s

based discourse understanding for a persistent meeting assistant
in proceedings of the aaai spring symposium persistent assistants living and working with ai
otterbacher et al
otterbacher j
erkan g
and radev d
r

using random walks for question focused sentence retrieval
in hlt proceedings of the conference on human language technology and empirical methods in natural language processing pages morristown nj usa
association for computational linguistics
park et al
park s
lee j
h
ahn c
m
hong j
s
and chun s
j

query based summarization using non negative matrix factorization
proceeding of kes pages
qiu and frei qiu y
and frei h

concept based query expansion
in proceedings of the annual international acm sigir conference on research and development in information retrieval pages
acm press new york ny usa
radev radev d

a common theory of information fusion from multiple text sources step one cross document structure
in proceedings acl sigdial workshop on discourse and dialogue
radev et al
radev d
otterbacher j
winkel a
and blair goldensohn s

newsinessence summarizing online news topics
communications of the acm
radev et al
radev d
r
jing h
and budzikowska m

centroid based marization of multiple documents sentence extraction utility based evaluation and user studies
anlp naacl workshop on summarization
radev et al
radev d
r
jing h
stys m
and tam d

centroid based summarization of multiple documents
information processing management
raymond et al
raymond j
w
gardiner e
j
and willett p

rascal lation of graph similarity using maximum common edge subgraphs
the computer journal
steinberger and jezek steinberger j
and jezek k

using latent semantic ysis in text summarization and summary evaluation
in proc
isim pages
teufel and moens teufel s
and moens m

summarizing scientic articles periments with relevance and rhetorical status
computational linguistics
tomita et al
tomita j
nakawatase h
and ishii m

calculating similarity between texts using graph based text representation model
pages washington d
c
usa
acm
torralbo et al
torralbo r
alfonseca e
guirao j
m
and moreno sandoval a

description of the uam system at
in proceedings of the document standing conf
wksp
duc at the human language technology conf

on empirical methods in natural language processing hlt emnlp
varadarajan and hristidis varadarajan r
and hristidis v

a system for specic document summarization
proceedings of the acm international conference on information and knowledge management pages
voorhees voorhees e

query expansion using lexical semantic relations
in ceedings of the annual international acm sigir conference on research and development in information retrieval pages
springer verlag new york inc
new york ny usa
wilcoxon wilcoxon f

individual comparisons by ranking methods
biometrics bulletin pages
witte et al
witte r
krestel r
and bergler s

context based multi document summarization using fuzzy coreference cluster graphs
in proceedings of document ing workshop duc new york city ny usa
over all the rate of substance abuse among native american adults is over percent nationwide
a retail shopping center is proposed for a nearby section of the reservation
as a result some urban native americans feel driven away
juvenile crime is one strand in the web of social problems facing urban and reservation indian communities the report said
he hopes to move his business to union gap a few miles north and off the reservation
nearly native americans live in the new york city metropolitan area
powless said the onondaga people want to work with the community outside the reservation to improve the economy of the region perhaps creating tourism destinations that might include indian culture or setting up a free trade zone at unused manufacturing sites
the navajo reservation is primarily dependent on federal money
reservations are home to some of the gravest poverty and worst health care in the country
the issue was the tribe s operation of a shoe box casino on its reservation
let s find a way to benefit from it
bowing to a court ruling congress in laid out a statutory framework allowing american indian tribes to offer high stakes bingo games and casino style gambling on their historic reservation lands
the picture is similar on some smaller reservations like the fort peck reservation in eastern montana
but people in neah bay point out that native american communities do not have a single monolithic viewpoint any more than other communities
figure sample summary for the topic of duc wu and liu wu c
w
and liu c
l

ontology based text summarization for business news articles
proceedings of the isca international conference on computers and their applications pages
yarowsky yarowsky d

unsupervised word sense disambiguation rivaling vised methods
in proceedings of the annual meeting on association for computational linguistics pages
association for computational linguistics morristown nj usa
zhang et al
zhang y
callan j
and minka t

novelty and redundancy tection in adaptive ltering
in proceedings of the annual international acm sigir conference on research and development in information retrieval pages
acm new york ny usa
a sample summary and scoring here we provide a sample duc corpus topic example a
and the mudos ng extracted summary in figure
example a
topic denition topic num title native american reservation system pros and cons narr discuss conditions on american indian reservations or among native american communities
include the benefits and drawbacks of the reservation system
include legal privileges and problems
in table we indicate the top candidate sentences from the document set used to extract the summary
the sentences appear in decreasing order of their score
it is important to note that some of the sentences were removed from the redundancy removal step therefore not appearing in the nal summary
sentence the income disparity on the reservation has widened
value

many reservation residents are frustrated

over all the rate of substance abuse among native american adults is over percent nationwide



but not everyone on the reservation is happy about the growth
a retail shopping center is proposed for a nearby section of the reservation
and the migration from the reservations continues
mostly it s economics says joanne dunne a spokeswoman for the boston indian council a nonprot cultural group
the reservation typically does nt provide you with any real opportunities
also many native americans travel between the reservation and urban areas
some tribes traditionally go from one place to another says dunne
the micmacs always crossed the border from canada to come from time immemorial
in other cities city and state governments have created agencies to specically deal with the native american population
as a result some urban native americans feel driven away
from to the urban native american population has more than doubled
but a non indian resident of a reservation has no say in tribal government
smith and thousands like her are seeking help for their substance abuse at the american indian community house the largest of a handful of native american cultural institutions in the new york area
where else can i go for help she asks
any place else they do nt understand you like they do here
native americans around the country are leaving reservations and relocating in urban areas at a dizzying rate




table the top sentences provided from mudos ng and their scores sentence scoring

