n u j l c
s c v
v i x r a automatic text summarization of medical research articles using bert and bowen tan laboratory of molecular genetics rockefeller university new york ny
edu virapat kieuvongngam laboratory of membrane biology and biophysics rockefeller university new york ny
edu yiming niu laboratory of molecular neurobiology and biophysics rockefeller university new york ny
edu abstract with the pandemic there is a growing urgency for medical community to keep up with the accelerating growth in the new coronavirus related literature
as a result the open research dataset challenge has released a corpus of scholarly articles and is calling for machine learning approaches to help bridging the gap between the researchers and the rapidly growing publications
here we take advantage of the recent advances in pre trained nlp models bert and openai to solve this challenge by performing text summarization on this dataset
we evaluate the results using rouge scores and visual inspection
our model provides abstractive and comprehensive information based on keywords extracted from the original articles
our work can help the the medical community by providing succinct summaries of articles for which the abstract are not already available
introduction
open research dataset chanllenge the global health and research community is need of a way to survey the scientic literature to come up with a treatment and measures against the
in response to this challenge the white house and leading research groups have established the open research dataset to bring in the nlp expertise to help nding the answer within the literature or bringing insights to the public at large wang et al

this dataset consists of over scholarly articles including over with full text about the or related diseases

text summarization automatic text summarization is an active area of research focusing on condensing large piece of text to smaller text retaining the relevant information
there are two general approaches
first is the extractive summarization aiming at extracting and concatenating important span of the source text
designed and conducted the pre processing and quantitative assessment
vk proposed the idea designed the experiment implemented a prototype model
yn trained the model conducted the quantitative assessment
preprint
work in progress
this is akin to skimming the text
the second approach focus on generating new summaries that paraphrase the source text
the extractive approach has been shown to maintain reasonable degree of grammaticality and accuracy
on the contrary abstractive approach is much more challenging due to the fact that the model must be able to represent semantic information of the source text and then use this semantic representation to generate a paraphrase
however the model may gain the ability to make a creative use of words or ability to make inference from the source text

existing body of work both approaches of summarization have progressed considerably thanks to the recent advances and the availability of the large pre trained nlp models often made use of the attention mechanism vaswani et al

these models include the bidirectional encoder representations from transformers bert devlin et al
and more recently openai radford et al

they are trained on a very large dataset of text such as the entire corpus of wikipedia and are able to perform well across diverse nlp tasks including machine translation question answering multiple choice question text classication
early text summarization models that uses pre trained bert is bertsum liu and lapata
bertsum is an extractive modied variant of bert model trained on a general news cnn daily news summarization dataset
the model performs binary classication task to predict whether a sentence is to be included in the summary
however as bert is not built to perform language generative task its use for abstractive summarization is limited
in the past few years sequence to sequence models based on the transformer decoder architecture has been widely used for abstractive summarization shi et al

from the architecture point of view the encoder reads the source text and transform it to hidden states and the decoder takes the hidden states and output a summary text
the mapping from hidden representation to output text gives the architecture language generative capability
even more recently a unied text to text framework has been used to train the large language model on multiple nlp tasks all at once raffel et al

the basic idea is to train one single model to map the input text of various tasks to the output text
in this work we take a similar spirit to ne tuned a pre trained to perform mapping from a selected keywords to a summary text hence generating a summary abstractly

low resource challenge an additional challenge to our task is due to the low availability of the domain specic corpus
unlike a more general summarization like the cnn daily mail dataset with document summary pairs the related literature as of april contain approximately full text abstract pairs
moreover the scientic terminology found in the peer reviewed literature can often be esoteric thus are not used in the mainstream text where the pre training was performed
this low resource may present considerable impediment to the ne tuning
however this framework if found useful can be further expand
approach
overall outline the project is subdivided into two parts the unsupervised extractive part is used as a baseline performance and the novel abstractive part
the unsupervised extractive summarization takes the already pre trained bert model to perform a sentence embedding whereby every individual sentence is transformed to high dimensional representation
subsequently k medoid clustering analysis is performed on the high dimensional representation miller
representing semantic centers of text the cluster centers are selected extracted summary
comparing against the extractive summarization the abstractive summarization is trained to generate a summary from a set of keywords
the keywords are extracted from the source text using existing token classication tools such as nltk part of speech tagging packages or ne tuned bert token classier for part of speech tagging
the keywords are tokens classied as three different groups verbs nouns verbs and nouns
following the extraction the keywords are paired with the generated abstract gold summary abstract
this keyword summary pairs are processed and fed to the model as illustrated in gure
after training summary results are generated using stochastic sampling method described in section

the results are compared and qualitative assess by reading inspection
quantitatively the generated results are compared against the gold summary using rouge

model architecture many of the state of the art nlp models are built using transformer architecture vaswani et al
relying on attention mechanism to convert the input sequences to output sequences
two kinds of transformer architectures are widely used the transformer encoder and the transformer decoder
the bert model used here for unsupervised extractive summarization is a pre trained transformer encoder model sanh et al

the model has attention heads and transformer encoder layers
the output is dimensional last hidden state of the model
we use pytorch based distilbert implemented in the huggingface transformer wolf et al

due to the gpu resource constraint the abstractive summarization model is a pre trained distil version of
the can take up to token length
it has attention heads and transformer decoder layers
we use the pytorch version of implemented in the huggingface transformer package wolf et al


training strategy of the abstractive summarization the model is trained on tasks the language modeling lm task and the multiple choice mc prediction task
for the lm task the model predicts a next word token given previous tokens and context
for the mc task given a set of keywords the model choose the correct gold summary from summary choices
each of the tasks has an associated loss
the lm task projects the hidden state to the word embedding ouput layer
cross entropy loss is applied on the target corresponding to the gold summary to get an lm loss
for the training we label the start and the end of text with special tokens
to enable the model to recognize the summarization task a special token is used to separate the keywords and the gold summary
the input are all padded with padding token to tokens and any input longer than tokens are truncated
for the mc task the hidden state of the last token is passed through a linear layer to get a class likelihood score i
e
a classication task
the cross entropy loss is applied to obtain a mc loss
to create the training dataset we randomly select summaries unrelated to the keywords so called distractors and paired the distractors with the keywords in the similar manners as the gold summary forming a batch of input items
the language modeling training labels are the token of summary that are right shifted by token
this is because is auto regressive in nature and the nth token output is generated from all previous token inputs to the left
the multiple choice training label is a tensor of a numeric i indicating the ith item that is the correct keyword gold summary pair
the total loss is a weighted sum of the two losses at ratio of lm loss to mc loss

intuition of the training strategy the intuition behind this training strategy is the following
because the model aims at text generation it is designed to be auto regressive
that is to say the model takes the backward context of previous tokens to predict the nth token
this is achieved using the masked self attention mechanism to block the information from tokens to the right of the current position from being for bert token classication is adapted from
depends on the definition
com named entity recognition with
the training dataset is from
kaggle
entity annotated corpus calculated
the special token is used to signify the context whereby the subsequent tokens are to be the summary of the information before this special token
the model is to learn this context clue from the ne tuning
the multi loss training is used in the hope that we will be able to induce the model to map the local semantic context in the keywords to the gold summary
at the same time the model retains the global contextual information of the keywords so that in the end of the text the model is able to distinguish the gold summary from the distractors
this multi loss training seem to be prominent in recent language model training that aims at general language understanding li et al
raffel et al

figure overview of multi loss training
the example input shown has n items in the input true abstract and distractors
the true abstract is shown as the item
the beginning of sequence end of sequence padding and summarization token are denoted bos eos pad s respectively
the language modeling label contains a masked token masked everywhere except the gold summary tokens
the multiple choice label is a tensor indicates the item as the correct keyword summary pair
experiments and results
model training the training of is carried out on a google colab equiped with nvidia tesla gpu
a total of epochs are performed
the training dataset consists of training samples each sample has multiple choice options
the validation dataset consists of samples each also has multiple choices
the training parameters include the learning rate with batch size and gradient accumulation of steps
the linearly decreasing learning rate scheduler is used for every epoch
the training loss of the rst epoch is shown in gure
for the validation the lm loss is
mc loss is indicating no sign of overtting

visualizing the results the attention mechanism allows us to assess the model performance
the attention can be broadly interpreted as a vector of importance weights i
e
how strongly the tokens in sequences are correlated with other tokens vig
to visualize the attention we input the sequence illustrated in table and plot the attention as matrix of alignment et al

in the gure below we visualize the learned attention from the model by comparing the attention of the pre trained figure training results
the two losses are shown during epoch of training iterations
the language model loss blue is shown in the exponentiated form of the cross entropy loss so called the perplexity score elmloss
the multiple choice loss orange is calculated from the cross entropy loss over all the multiple choices
before the ne tuning and after the ne tuning
because of the multi layer multi attention head architecture the total unique structures are
we only select attentions that seems to exhibit interesting phenomena
first obvious pattern exihibited in layer head both before and after ne tuning is the diagonal pattern
this could be interpreted as representing self correlation
observed more strongly only after ne tuning the second pattern is the left shifted diagonal line shown in layer head and layer head
this could be interpreted as the correlation between the keyword input and the summary
this attention learned during the ne tuning became more strongly after more epochs of ne tuning
this indicates that our training strategy works as expected
thirdly the vertical line pattern observed in the attention of layer head both before and after ne tuning
this could be interpreted as long range correlation within phrases or sentences in the sequence important for maintaining coherence or grammarticality

sequence generation the language modeling output of the is a tensor of size sequence length vocab size
this is a tensor of likelihood distribution over all words before the softmax
to generate a text sequence from this output we sample words from this distribution in the word by word manner
to obtain the ith word we consider the conditional probability of previous i words
p


firstly before the sampling we can apply a scaling factor so called temperature t to the likelihood u to reshape the skews likelihood distribution before the softmax holtzman et al

t l the high temperature tends to skews the distribution in favor of low probability words whereas the low temperature skews to the distribution towards a few high probability words
the result is a tug of war between favoring generation accuracy at the cost of decreasing word diversity
secondly we employs a stochastic sampling method called top p sampling over the probability distribution of words to pick conditional on previous words in the sequence
the rule for top p sampling is that the smallest set of candidate words to consider is such that the cumulative conditional probability is greater than p
xp additionally to prevent the model from sampling too many low probability words we limit the number of candidate words to consider to k words
we empirically tested a few of the sampling parameters and found that temperature k and
yields a reasonable generations
gold summary generated abstractive summary table example of summary result keyword input epoch training inuenza virus is the most quently reported viral cause of rhabdomyolysis
a old child is presented with rhabdomyolysis associated with parainuenza type virus
nine cases of rhabdomyolysis ciated with parainuenza virus have been reported
cations may include electrolyte disturbances acute renal failure and compartment syndrome
rhabdomyolysis associated with parainuenza virusinuenza virus is cause rhabdomyolysis child is domyolysis parainuenza type virus cases rhabdomyolysis parainuenza virus have plications include electrolyte turbances renal failure ment syndrome epoch training rhabdomyolysis associated with parainuenza virusinuenza virus is cause rhabdomyolysis child is domyolysis parainuenza type virus cases rhabdomyolysis parainuenza virus have plications include electrolyte turbances renal failure partment syndrome inuenza virus is the most mon cause of respiratory it is domyolysis in the child
believed that the ysis and parainuenza type virus cases with sis in parainuenza type virus
recent cases with ysis in parainuenza virus have been described
complications include electrolyte disturbances kidney failure and nal compartment syndrome
rhabdomyolysis inuenza virus is a leading cause of in child
however several cases of domyolysis in the parainuenza virus have been reported
plications include electrolyte turbances in renal failure of the normal renal compartment drome
analysis the result in table in noteworthy in that the model learns that inuenza virus is the most common cause of rhabdomyolysis a breakdown of muscle tissue even though this knowledge is not presented in the keyword
in this respect the model gains an ability to infer some knowledge from the training corpus
however this could be problematic because it could wrongly infers falsehood as well
for example the model outputs complication include


gastrointestinal conpartment syndrome
in fact the compartment syndrome is not a gastrointestinal condition
however we should note that this could also be attributed to our sampling method
figure visualizing attention mechanism
the weights of attentions layers mapping the input to the output is shown
the input sequence shown in table is passed through either the pre trained model or our summarization ne tuned model
the axis represents the input sequence
the y axis represents the aligned output
the notation b s e denotes the start summarization and end token respectively
the part of keyword or summary sequences are labeled
the gure compares of the attention before and after the ne tuning
selected attention layers and heads are plotted as matrix heatmaps
quantitative assessment
rouge metric rouge recall oriented understudy for gisting evaluation is a metric used in nlp for evaluating text summarization
the metric compare a model generated summary against a human generated reference summary
rouge n measures overlap of n grams between the two texts
rouge l measures the longest matching sequences of words without predened n gram length and does not require consecutive matches
rouge w measures the longest matching sequences that take consecutive matching into account lin
the recall version of rouge reports the ratio of n grams in the reference that are also present in the generated summary
the precision version of rouge reports the ratio of n grams in the generated summary that are also present in the reference summary
the f score version of rouge is the harmonic mean of the precision rouge and recall rouge
in our report we will use the f score version of rouge

extractive summarization we applied kmeans clustering followed by k nearest neighbour to extract sentences representing comprehensive semantic meanings of abstracts
during the extraction we compared the effects between versus compression ratios on rouge scores
as shown in gure and all of the rouge scores of extraction are higher than extraction irrespective of training epochs
additionally extractive summary produces reuslts with higher rouge scores compared to the abstractive one figure
this is consistent with the assumption that less compression during extraction preserves more information compared to the original abstracts

abstractive summarization in the abstractive summarization stage the effect of training time was rstly investigated
as shown in figure the plotted weights of attentions layers mapping the input to the output i between and epochs suggested a more concentrated distribution
in other words the model would benet signicantly by longer training process and this is reected in the better t of relevance between the input and output
furthermore the output summary shown in table illustrates that longer training could help to generate more abstract summary compared to the gold summary model with epoch training summarize special cases as several cases of mixed infections have been reported rather than explaining the specic case whereas model with epoch training still tries to describe it
interestingly the seemingly more abstract results are not reected in the calculated rouge score
as shown in figure the difference of rouge scores between model with and epochs is insignicant
to investigate the effect of keywords used for sentence generation we then compared all of keywords nouns and verbs yielded from versus extraction
figure and together show that the overall rouge scores from abstractive group are higher than the group
this indicates that using more keywords as input tokens would result in more coverage of information compared to original abstracts after generator
we next compared whether different word classes inuence generated summaries compared to the original ones
as shown in figure the rst observation is that abstraction tend to show lower rouge scores than the group irrespective of word classes
second only using verbs as keywords shows very low rouge scores while only using nouns tend to show almost similar rouge scores compared to the group using both verbs and nouns
this may suggest that nouns are generally weighted more than verbs during summary generation or nouns themselves are representing more accurate information that the original abstracts convey
however this does not exclude the possibility that this advantage of using nouns is due to using larger percentage of keywords since nouns tend to be used more than verbs in sentences
in gure we further evaluate whether using different word sampling methods the greedy search versus the top k sampling would inuence the results
although the rouge scores between the two groups are similar in some cases the greedy search group even shows slightly higher scores the readability and abstractive meanings are signicantly worse in the greedy search group compared to the top k group
figure summary of experiments on hyper parameters
a and b comparison of rouge scores between model with a and b epochs training
c effect of nouns and verbs in keywords
percentages represent the fraction of specic category of words included
d effect of the greedy search approach
top and stands for with and without the greedy search respectively
conclusion and future work abstractive summarization still represents a standing challenge for deep learning nlp
even more so when this task is applied to a domain specic corpus that are different from the pre training are highly technical or contains low amount of training materials
open research dataset challenge exemplify all the abovementioned difculty
nevertheless we have here illustrated that the text to text multi loss training strategy could be used to ne tune a pre trained language model such as to perform abstractive summarization
the result is interpretable and reasonable even though it is not near human level performance
first of all we think that our model could benet from further training as the new coronavirus related research publication are becoming available
this should make the model more accurate in its ability to infer conclusion from the keywords
in retrospect we think that the keyword generation phrase is exible and could also be signicantly improved without too much investment
our keywords are taken from nouns and verbs present in the text
we could investigated how much more delity could be gained from adding more information such as adjective part to the keyword
more data augmentation could be performed by randomly dropping or adding words to the keyword sets
this would in effect create much more summary pairs from the existing ones
finally we think that other strategy for extracting keywords could be experimented
for example one could ne tune a token classication model to selectively extract scientic keywords
the evaluation process of abstractive summaries still requires more exploration
from our data the rouge scores represent more direct information or phrases overlapping rather than the actual meanings and readability of summaries
clearly all of the extractive models yield higher rouge scores than abstractive ones
intuitively extractions of raw sentences could result in higher similarity but this approach would not be favored for better summarization
the fact that the generated abstractive summaries showing good readability and succinct information coverage are not reected by rouge scores strongly suggest that other scores or evaluation systems on these aspects are needed in the future
one possible idea would be combining regularization to penalize local similarity but reward global similarity
finally we think that our approach can be further leveraged if more intensive computation resources is available
overall our implementation is mostly limited by computation power
with tesla gpu the training can only be done on the version with the batch size because of the memory limit of the gpu
it is likely that the result could greatly benet from using more bigger version and with more training if permitted by the available computation power
in the end we hope that a text summarization approach
such as ours can help the medical research community keep up with the rapidly growing literature and that it helps bring new insight to ght the pandemic
code availability all source codes and models of implemented in this study are publicly available on github
com
references dzmitry bahdanau kyunghyun cho and yoshua bengio
neural machine translation by jointly learning to align and translate
jacob devlin ming wei chang kenton lee and kristina toutanova
bert pre training of deep bidirectional transformers for language understanding
ari holtzman jan buys li du maxwell forbes and yejin choi
the curious case of neural text degeneration
jianquan li xiaokang liu wenpeng yin min yang and liqun ma
an empirical evaluation of multi task learning in deep neural networks for natural language processing
chin yew lin
rouge a package for automatic evaluation of summaries
in acl
yang liu and mirella lapata
text summarization with pretrained encoders
derek miller
leveraging bert for extractive text summarization on lectures
alec radford jeffrey wu rewon child david luan dario amodei and ilya sutskever
language models are unsupervised multitask learners

url
cloudfront
net better language models language models
pdf
colin raffel noam shazeer adam roberts katherine lee sharan narang michael matena yanqi zhou wei li and peter j
liu
exploring the limits of transfer learning with a unied text to text transformer
victor sanh lysandre debut julien chaumond and thomas wolf
distilbert a distilled version of bert smaller faster cheaper and lighter
tian shi yaser keneshloo naren ramakrishnan and chandan k
reddy
neural abstractive text summarization with sequence to sequence models a survey
ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan n
gomez lukasz kaiser and illia polosukhin
attention is all you need
jesse vig
a multiscale visualization of attention in the transformer model
arxiv preprint

url
org

lucy lu wang kyle lo yoganand chandrasekhar russell reas jiangjiang yang darrin eide kathryn funk rodney kinney ziyang liu william merrill paul mooney dewey murdick devvret rishi jerry sheehan zhihong shen brandon stilson alex d
wade kuansan wang chris wilhelm boya xie douglas raymond daniel s
weld oren etzioni and sebastian kohlmeier
the open research dataset
thomas wolf lysandre debut victor sanh julien chaumond clement delangue anthony moi pierric cistac tim rault rmi louf morgan funtowicz and jamie brew
huggingface s transformers state of the art natural language processing
appendix
generated sample gold summary publisher summary demyelination is a component of several viral diseases of humans
the best known of these are subacute sclerosing panencephalitis sspe and progressive multifocal leukoencephalopathy pml
there are a number of naturally occurring virus infections of animals that involve demyelination and many of these serve as instructive models for human demyelinating diseases
in addition to the naturally occurring diseases many viruses have been shown to be capable of producing demyelination in experimental situations
in discussing virus associated demyelinating disease the chapter reviews the architecture and functional organization of the cns and considers what is known of the interaction of viruses with cns cells
it also discusses the immunology of the cns that differs in several important aspects from that of the rest of the body
experimental models of viral induced demyelination have also been considered
viruses capable of producing demyelinating disease have no common taxonomic features they include both dna and rna viruses enveloped and nonenveloped viruses
the chapter attempts to summarize the important factors inuencing viral demyelination their common features and possible mechanisms
abstractive summary abstract demyelination is a component of many diseases humans are experiencing
sclerosing panencephalitis sspe and leukoencephalopathy pml are common causes of demyelination in many animal and human diseases
however the virus infections of animals involve demyelination
these situations of demyelinating disease chapter reviews the architecture of the cns and considers its interactions with the viruses in the cns
different aspects of the rest of the body models of demyelination have been explored
viruses for example have been identied as the most important virus infections in humans
abstractive summary abstract demyelination is an important component of the diseases human are suffering from
the sclerosing panencephalitis sspe and leukoencephalopathy pml are common causes of demyelination in various clinical conditions and are considered as a potential cause of demyelination in some clinical conditions
however there are several clinical and epidemiological factors that may inuence demyelination in some clinical situations
demyelination in clinical situations may be a result of a combination of viral and cellular factors such as viral rna viruses and viruses
in this chapter we reviews the architecture and organization of the cns and considers its potential to be a potential cause of demyelination in some clinical situations
we discuss the immunology of the cns
generated sample gold summary rnai pathway is an antiviral defence mechanism employed by insects that result in degradation of viral rna thereby curbing infection
several viruses including aviviruses encode viral suppressors of rnai vsrs to counteract the antiviral rnai pathway
till date no vsr has been reported in alphaviruses
the present study was undertaken to evaluate chikungunya virus chikv proteins for rnai suppressor activity
we systematically analyzed all nine chikv proteins for rnai suppressor activity using rnai sensor cell line based assay
two non structural proteins namely and were found to exhibit rnai suppressor activity
we further validated the ndings in natural hosts namely in aedes and in mammalian cell lines and further through emsa and agrobacterium inltration in gfp silenced transgenic tobacco plants
domains responsible for maximum rnai suppressor activity were also identied within these proteins
rna binding motifs in these domains were identied and their participation in rnai suppression evaluated using site directed mutagenesis
sequence alignment of these motifs across all species of known alphaviruses revealed conservation of these motifs emphasizing on a similar role of action in other species of alphaviruses as well
further validation of rnai suppressor activity of these proteins awaits establishment of specic virus infection models
abstractive summary abstract chikungunya virus chikv proteins are highly conserved and are highly conserved in the host cell line
however their activity against rnai is not well understood
here we evaluated the chikungunya virus chikv proteins as a non structural rnai suppressor activity
we analyzed the chikv proteins as a non structural rnai suppressor activity
we analyzed the chikv proteins as a structural rnai suppressor activity
rnai sensor cell line assay proteins were found to exhibit rnai activity further
ourndings on host aedes cell lines emsa agrobacterium inltration gfp tobacco plants domains rnai suppressor activity were consistent with the observed rnai suppression mutagenesis
sequence alignment motifs of species alphaviruses revealed the conserved conservation motifs in the role of action species
abstractive summary abstract chikungunya virus chikv proteins are a novel and promising rnai mediated antiviral protein
however their antiviral activity against chikungunya virus chikv proteins is not well understood
in this study we was able to evaluate chikungunya virus chikv proteins as a novel rnai mediated antiviral protein
we detected that chikv proteins as a novel rnai mediated antiviral protein could suppress chikungunya virus infection by targeting the rnai sensor cell line assay proteins and
we were able to demonstrate that chikv proteins as a novel rnai mediated antiviral protein could suppress chikungunya virus infection by targeting the rnai sensor cell line assay proteins and
