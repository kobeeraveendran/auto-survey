t c
o l c
s
c
v
v
i x r a a semantic relevance based neural network for text summarization and text simplication shuming ma
peking university xu sun peking university text summarization and text simplication are two major ways to simplify the text for poor ers including children non native speakers and the functionally illiterate
text summarization is to produce a brief summary of the main ideas of the text while text simplication aims to reduce the linguistic complexity of the text and retain the original meaning
recently most approaches for text summarization and text simplication are based on the sequence to sequence model which achieves much success in many text generation tasks
however although the generated simplied texts are similar to source texts literally they have low semantic relevance
in this work our goal is to improve semantic relevance between source texts and simplied texts for text summarization and text simplication
we introduce a semantic relevance based neural model to encourage high semantic similarity between texts and summaries
in our model the source text is represented by a gated attention encoder while the summary representation is produced by a decoder
besides the similarity score between the representations is maximized during training

our experiments show that the proposed model outperforms the state of the art systems on two benchmark

introduction text summarization and text simplication is to make the text easier to read and understand especially for poor readers including children non native speakers and the functionally erate
text summarization is to simplify the texts at the document level
the source texts are often consist of many sentences or paragraphs and the simplied texts are some brief sentences of the main ideas of the source texts
text simplication is to simplify the texts at the sentence level
it aims to simplify the sentences to reduce the lexical and structure complexity
unlike text summarization it does not require the simplied sentences are shorter but requires the words are simple to understand

in some previous work extractive summarization achieves satisfying performance by lecting a few sentences from source texts radev et al

cheng and lapata cao et al

by extracting the sentences the generated texts are grammatical and retain the same meaning with the source texts
however it does not simplify the texts but only shorten the texts

some previous related work regards text simplication as a combination of three operations splitting deletion and paraphrasing which requires some rule based models or heavy tic features zhu bernhard and gurevych woodsend and lapata filippova et al


moe key laboratory of computational linguistics peking university beijing china and school of electronics engineering and computer science peking university beijing china
e
mail
moe key laboratory of computational linguistics peking university beijing china and school of electronics engineering and computer science peking university beijing china
e
mail
our code is available at association for computational linguistics computational linguistics volume number table
an example of the simplied text generated by for summarization
the summary has high similarity to the text literally but has low semantic relevance

text


last night several people were caught to smoke on a ight of china
united airlines from chendu to beijing
later the ight temporarily landed on taiyuan airport
some passengers asked for a security check but were denied by the captain which led to a collision between crew and passengers


china united airlines exploded in the airport leaving several people dead

gold

several people smoked on a ight which led to a collision between crew and passengers
most recent approaches use
sequence to sequence model tion
rush chopra and weston hu chen and zhu
tion nisioi et al cao et al zhang and lapata
sequence to sequence model is a widely used end to end framework for text generation such as machine translation

it compresses the source text information into dense vectors with the neural encoder and the neural decoder generates the target text using the compressed vectors
for text and text for both text summarization and text simplication the simplied texts must have high semantic relevance to the source texts
however current sequence to sequence models tend to produce grammatical and coherent simplied texts regardless of the semantic relevance to source texts
table shows that the summary generated by a lstm sequence to sequence model is similar to the source text literally but it has low semantic relevance

in this work our goal is to improve the semantic relevance between source texts and generated simplied texts for text summarization and text simplication
to achieve this goal we propose a semantic relevance based neural network model srb
in our model we compress the source texts into dense vectors with the encoder and decode the dense vector into simplied texts with the decoder
the encoder produces the representation of source texts and the decoder produces the representation of the generated texts
a similarity evaluation component is introduced to measure the relevance of source texts and generated texts
during training it maximizes the similarity score to encourage high semantic relevance between source texts and simplied texts
in order to better represent a long source text we introduce a self gated attention encoder to memory the input text
we conduct the experiments on three corpus namely lcsts pwkp and ew sew
experiments show that our proposed model has better performance than the state of the art systems on two benchmark corpus

the contributions of this work are as follow
we propose a semantic relevance based neural network model srb to improve the semantic relevance between source texts and generated simplied texts for text summarization and text simplication
a similarity evaluation component is shuming ma et al
a semantic relevance based neural network for summarization and simplication introduced to measure the relevance of source texts and generated texts and the similarity score is maximized to encourage high semantic relevance between source texts and simplied texts

we introduce a self gated encoder to better represent a long redundant text
we perform the experiments on three corpus namely lcsts pwkp and ew sew
experiments show that our proposed model outperforms the state of the art systems on two benchmark corpus

background sequence to sequence model
most recent models for text summarization and text simplication are based on the to sequence model
the sequence to sequence model is able to compress source texts xn into a continuous vector representation with an encoder and then generates the simplied text y ym with a decoder
in the previous work nisioi et al hu chen and zhu the encoder is a two layer long short term memory network lstm
hochreiter and schmidhuber which maps source texts into the hidden vector hn
the decoder is a uni directional lstm producing the hidden output st which is the dense representation of the words at the tth time step
finally the word generator computes the distribution of output words yt with the hidden state st and the parameter matrix w attention mechanism is introduced to better capture context information of source texts bahdanau cho and bengio
attention vector ct is calculated by the weighted sum of encoder hidden states where hi is an attentive score between the decoder hidden state st and the encoder hidden state hi
when predicting an output word the decoder takes account of the attention vector which contains the alignment information between source texts and simplied texts
with the attention mechanism the word generator computes the distribution of output words yt sof st ct tihi n x ti hi
n hj
p
sof
computational linguistics volume number admission is very hard

cos attention
admission is extremely
competitive figure
our semantic relevance based neural model
it consists of decoder above encoder below and cosine similarity function

proposed model
our goal is to improve the semantic relevance between source texts and simplied texts so our proposed model encourages high similarity between their representations
figure shows our proposed model
the model consists of three components encoder decoder and a similarity function
the encoder compresses source texts into semantic vectors and the decoder generates summaries and produces semantic vectors of the generated summaries
finally the similarity function evaluates the relevance between the sematic vectors of source texts and generated summaries
our training objective is to maximize the similarity score so that the generated summaries have high semantic relevance to source texts
self gated encoder the goal of the complex text encoder is to provide a series of dense representation of source texts for the decoder and the semantic relevance component
in the previous work nisioi et al the complex text encoder is a two layer uni directional long short term memory work lstm which produces the dense representation hn from the source text xn
however in text summarization and text simplication source texts are usually very long and noisy
therefore some encoding information in the beginning of the texts will vanish until the end of the texts which leads to bad representations of the texts
bi directional lstm is an alternative to deal with the problem but it needs double time to encoder the source texts and it does not represents the middle of the texts well when the texts are too long
to solve the problem we propose a self gated encoder to better represent a long text

in text summarization and text simplication some words or information in the source texts are unimportant so they need to be simplied or discarded
therefore we introduce a self gated shuming ma et al
a semantic relevance based neural network for summarization and simplication
self gated layer
admission is extremely
competitive figure
the self gated encoder
it measure the importance of each word and decide how much information is reserved as the representation of the texts
encoder which can reduce the unnecessary information and enhance the important information to represent a long text
self gated encoder try to measure the importance of each word and decide how much information is reserved as the representation of the texts
at each time step every upcoming word xt is fed into the lstm cell which outputs the dense vector ht where f is the lstm function and ht is the output vector of the lstm cell
a feed forward neural network is used to measure the importance and decide how much information is reversed where g is the feed forward neural network function and t measures the proportion of the reserved information
finally the reversed information is computed by multiplying t ht
xt
t
ht tht
where ht is the representation at the tth time step and is the input embedding of at the t time step

simplied text decoder the goal of the simplied text decoder is to generate a series of simplied words from the dense representation of source texts
in our model the dense representations of the source texts are fed computational linguistics volume number into an attention layer to generate the context vector ct ct tihi n x ti
n
p sof st st
yt vt

vs vt kvskkvtk where st is the dense representation of generated simplied computed by a two layer lstm
in this way ct and st respectively represent the context information of source texts and the target texts at the tth time step
to predict the tth word the decoder uses ct and st to generate the probability distribution of the candidate words where w and wc are the parameter matrix of the output layer
finally the word with the highest probability is predicted semantic relevance our goal is to compute the semantic relevance of source texts and generated texts given the source semantic vector vt and the generated sementic vector vs
here we use cosine similarity to measure the semantic relevance which is represented with a dot product and magnitude source texts and generated texts share the same language so it is reasonable to assume that their semantic vectors are distributed in the same space
cosine similarity is a good way to measure the distance between two vectors in the same space

with the semantic relevance metric the problem is how to get the semantic vector vs and vt
there are several methods to represent a text or a sentence such as mean pooling of lstm output or reserving the last state of lstm
in our model we select the last state of the encoder as the representation of source texts vs
a natural idea to get the semantic vector of a summary is to feed it into the encoder as well
however this method wastes much time because we encode the same sentence twice
actually the last output of the decoder sm contains information of both source text and generated
shuming ma et al
a semantic relevance based neural network for summarization and simplication summaries
we simply compute the semantic vector of the summary by subtracting hn from sm vs sm
hn previous work has proved that it is effective to represent a span of words without encoding them once more wang and chang
training
given the model parameter and input text the model produces corresponding summary y and semantic vector vs and vt
the objective is to minimize the loss function l vt where is the conditional probability of summaries given source texts and is computed by the encoder decoder model
vt is cosine similarity of semantic vectors vs and vt
this term tries to maximize the semantic relevance between source input and target output
we use adam optimization method to train the model with the default hyper parameters the learning rate and

in this section we present the evaluation of our model and show its performance on three popular corpus
besides we perform a case study to explain the semantic relevance between generated summary and source text

experiments datasets
we introduce a chinese text summarization dataset and two popular text simplication datasets

the simplication datasets are both from the alignments between english wikipedia and simple english wikipedia
the simple english wikipedia is built for the children and adults who are learning the english language and the articles are composed with easy words and short sentences
therefore simple english wikipedia is a natural public simplied text corpus
most of the text simplication benchmark datasets are constructed from simple english wikipedia

large scale chinese short text summarization dataset lcsts
lcsts is constructed by hu chen and zhu
the dataset consists of more than million text summary pairs constructed from a famous chinese social media website called sina
it is split into three parts with pairs in part i pairs in part ii and pairs in part iii
all the text summary pairs in part ii and part iii are manually annotated with relevant scores ranged from to and we only reserve pairs with scores no less than
following the previous work we use part i as training set part ii as development set and part iii as test set
parallel pwkp

pwkp zhu bernhard and gurevych is a widely used benchmark for evaluating text simplication systems
it consists of aligned complex text from english wikipedia as of simplication wikipedia corpus computational linguistics volume number and simple text from simple wikipedia as of
the dataset contains sentence pairs with words on average per complex sentence and words per simple sentence
following the previous work zhang and lapata we remove the duplicate sentence pairs and split the corpus with pairs for training pairs for development and pairs for test
english wikipedia and simple english wikipedia ew sew
ew sew is a publicly available dataset provided by hwang et al

to build the corpus they rst align the complex simple sentence pairs score the semantic similarity between the complex sentence and the simple sentence and classify each sentence pair as a good good partial partial or bad match

following the previous work nisioi et al we discard the unclassied matches and use the good matches and partial matches with a scaled threshold greater than
the corpus contains about k good matches and k good partial matches
we use this corpus as the training set and the dataset provided by xu et al
xu et al as the development set and the test set
the development set consists of sentence pairs and the test set contains sentence pairs
besides each complex sentence is paired with reference simplied sentences provided by amazon mechanical turk workers
settings
we describe the experimental details of text summarization and text simplication respectively
text summarization
to alleviate the risk of word segmentation mistakes xu and sun sun wang and li we use chinese character sequences as both source inputs and target outputs
we limit the model vocabulary size to which covers most of the common characters
each character is represented by a random initialized word embedding
we tune our parameter on the development set
in our model the embedding size is the hidden state size of encoder decoder is and the size of gated attention network is
we use
adam optimizer to learn the model parameters and the batch size is set as
the parameter is
both the encoder and decoder are based on lstm unit
following the ous work hu chen and zhu our evaluation metric is f score of rouge and
rouge l lin and hovy
text simplication
the text simplication datasets contain a lot of named entities which makes the vocabulary too large
to reduce the vocabulary size we follow the setting by
zhang and lapata
we recognize the named entities with the stanford corenlp
ger
manning al and replace the named entities with the anonymous symbols where loc org misc where n represents the n th entity in the sentence
to limit the vocabulary size we prune the vocabulary to top most frequent words and replace the rest words with the unk symbols
at test time we replace the unk symbols with the highest probability score from the attention alignment matrix following jean et al
jean et al



we lter out sentence pairs whose lengths exceed words in the training set
the encoder is implemented on lstm and the decoder is based on lstm with luong style attention luong pham and manning
we tune our hyper parameter on the development set
the model has two lstm layers
the hidden size of lstm is and the embedding size is
we use adam optimizer kingma and ba to learn the parameters and the batch size is set to be
we set the dropout rate srivastava et al to be
all of the gradients are clipped when the norm exceeds
the evaluation metric is bleu score
shuming ma et al
a semantic relevance based neural network for summarization and simplication table
results of our model and baseline systems
our models achieve substantial improvement of all rouge scores over baseline systems
the results are reported on the test
word level c character level
rouge l model w hu chen and zhu c
hu chen and zhu
attention w
hu chen and zhu
attention c
hu chen and zhu
attention c our implementation
srb c our proposal
table
comparison with our model and the recent neural models for text simplication
our models achieve substantial improvement of bleu score over baseline systems
the results are reported on the test sets
pwkp attention nisioi et al
attention nisioi et al
attention our implementation
srb our proposal
ew sew attention nisioi et al
attention nisioi et al
attention our implementation
srb our proposal bleu
bleu baseline systems
rst compare
we sequence to sequence our model with model sutskever vinyals and le
it is a widely used model to generate texts so it is an important baseline
attention
attention bahdanau cho and bengio is a sequence sequence framework with neural attention
attention mechanism helps capture the context information of source texts
this model is a stronger baseline system
basic a results
we compare our model with above baseline systems including and attention

we refer to our proposed semantic relevance based neural model as srb
table shows the results of our models and baseline systems on lcsts
as shown in table the models at the character level achieve better performance than the models at the word level
therefore we implement our model at the character level
for fair comparison we also implement a attention model following the details in the previous work hu chen and zhu

our implementation of attention has better score mainly because we tune the
parameters well on the development set
we can see srb outperforms both and attention with the f score of and rouge computational linguistics volume number table
results of our model and state of the art systems
copynet incorporates copying mechanism to solve out of vocabulary problem so its has higher rouge scores
our model does not incorporate this mechanism currently
in the future work we will implement this technique to further improve the performance
the results are reported on the test sets
word word level char character level f score of f score of r l f score of rouge l model w hu chen and zhu c hu chen and zhu
attention w
hu chen and zhu
attention c
hu chen and zhu

copynet c gu et al
srb c our proposal rouge l table
results of our model and state of the art systems
srb achieves the best bleu scores compared with the related systems on pwkp and ew sew
the results are reported on the test sets

pwkp nts nisioi et al
nts nisioi et al

dress zhang and lapata

dress ls zhang and lapata

srb our proposal ew sew pbmt r wubben al


sbmt sari

xu et al
nts nisioi et al
nts nisioi et al

dress zhang and lapata

dress ls zhang and lapata

srb our proposal bleu bleu table shows the results in two text simplication corpus
we compare our model with attention and attention
attention is a attention with pretrain word embeddings
we also implement a attention model and carefully tune it on the development set
our implementation get bleu score on pwkp and bleu score on ew sew
our srb outperforms all of the baseline systems with the bleu score of on pwkp and the bleu score of on ew sew
table summarizes the results of our model and state of the art systems
copynet has the highest scores because it incorporates copying mechanism to deals with out of vocabulary word problem
in this paper we do not implement this mechanism in our model
our model can also be improved with these additional techniques which however are not the focus of this paper
we also compare srb with other models for text simplication which are not limit to neural models
table summarizes the results of srb and the related systems
on pwkp dataset we compare srb with nts nts dress and dress ls
we run the public release code of shuming ma et al
a semantic relevance based neural network for summarization and simplication table
an example of srb generated summary on lcsts dataset compared with the system output of attention and the reference
text bat ppspptv

with careful calculation there are many successful internet companies in shanghai but few of them becomes giant company like bat
this is also the reason why few internet companies are listed in top hundred companies of paying tax
some of them are merged such as ebay tudou pps pptv
yihaodian and so on
others are satised with segment market for years
reference

why shanghai comes out no giant company
a
shanghai s giant company
srb

shanghai has few giant companies
nts and nts provided by nisioi et al
and get the bleu score of and respectively
as for dress and dress ls we use the scores reported by zhang and lapata
the goal of dress is not to generate the outputs closer to the references so bleu of dress and dress ls are relatively lower than nts and nts
srb achieves a bleu score of outperforming all of the previous systems
on ew sew dataset we compare wean dot with pbmt r sbmt sari and the neural models described above
we do not nd any public release code of pbmt r and sbmt sari
fortunately xu et al
provides the predictions of pbmt r and sbmt sari on ew sew test set so that we can compare our model with these systems
it shows that the neural models have better performance in bleu and wean dot achieves the best bleu score with
case study table is an example to show the semantic relevance between the source text and the summary
it shows that the main idea of the source text is about the reason why shanghai has few giant company
rnn context produces shanghai s giant companies which is literally similar to the source text while srb generates shanghai has few giant companies which is closer to the main idea in semantics
it concludes that srb produces summaries with higher semantic similarity to texts
table shows an examples of different text simplication system outputs on ew sew

nts omits so many words that it lacks a lot of information
pbmt r generates some
evant words like siemens martin and which hurts the uency and adequacy of the generated sentence
sbmt sari is able to generate a uent sentence but the meaning is different from the source text and even more difcult to understand
compared with the statistic model srb generates a more uent sentence
besides srb improves the semantic revelance computational linguistics volume number table
an examples of different text simplication system outputs in ew sew dataset
differences from the source texts are shown in bold

source reference nts nts pbmt r sbmt sari
srb
depending on the context another closely related meaning of constituent is that of a citizen residing in the area governed represented or otherwise served by a politician sometimes this is restricted to citizens who elected the politician

the word constituent can also be used to refer to a citizen who lives in the area that is governed represented or otherwise served by a politician sometimes the word is restricted to citizens who elected the politician

depending on the context another closely related meaning of constituent is that of a citizen living in the area governed represented or otherwise served by a politician sometimes this is restricted to citizens who elected the politician

this is restricted to citizens who elected the politician

depending on the context and meaning of closely related siemens martin is a citizen living in the area or otherwise was governed by a shurba this is restricted to people who elected it

in terms of the context another closely related sense of the component is that of a citizen living in the area covered make up or if not served by a policy sometimes this is limited to the people who elected the policy
depending on the context another closely related meaning of constituent is that of a citizen living in the area governed represented or otherwise served by a politician sometimes the word is restricted to citizens who elected the politician
between the source texts and the generated texts so the generated sentence is semantically correct and very close to the original meaning

related work sutskever vinyals and le summarization has achieved successful performance thanks abstractive text to the and attention sequence to sequence model nism
bahdanau cho and bengio
rush chopra and weston rst used an attention based encoder to compress texts and a neural network language decoder to generate summaries
following this work recurrent encoder was introduced to text summarization and gained better performance lopyrev chopra auli and rush
towards chinese texts hu chen and zhu built a large corpus of chinese short text summarization
to deal with unknown word problem nallapati et al
proposed a generator pointer model so that the decoder is able to generate words in source texts
gu et al
also solved this issue by incorporating copying mechanism
besides ayana et al
proposes a minimum risk training method which optimizes the parameters with the target of rouge scores
zhu bernhard and gurevych constructs a wikipedia dataset and proposes a based simplication model which is the rst statistical simplication model covering splitting dropping reordering and substitution integrally
woodsend and lapata introduces a driven model based on quasi synchronous grammar which captures structural mismatches and complex rewrite operations
wubben van den bosch and krahmer presents a method for text simplication using phrase based machine translation with re ranking the outputs
shuming ma et al
a semantic relevance based neural network for summarization and simplication kauchak proposes a text simplication corpus and evaluates language modeling for text simplication on the proposed corpus
narayan and gardent propose a hybrid approach to sentence simplication which combines deep semantics and monolingual machine translation
hwang et al
introduces a parallel simplication corpus by evaluating the similarity between the source text and the simplied text based on wordnet
glava and tajner propose an unsupervised approach to lexical simplication that makes use of word vectors and require only regular corpora
xu et al
design automatic metrics for text simplication and they introduce a statistic machine translation and tune with the proposed automatic metrics
recently most works focus on the neural sequence to sequence model
nisioi et al

present a sequence to sequence model and re ranks the predictions with bleu and sari

zhang and lapata propose a deep reinforcement learning model to improve the simplicity uency and adequacy of the simplied texts
cao et al
introduce a novel sequence sequence model to join copying and restricted generation for text simplication
like sequence to sequence model has achieved success our work is also related to the encoder decoder framework cho et al and the attention mechanism bahdanau cho and bengio
encoder decoder work in machine

tion sutskever vinyals and le jean et al luong pham and manning text summarization rush chopra and weston chopra auli and rush nallapati et al

cao et al and other natural language processing tasks
neural attention model is rst proposed by bahdanau cho and bengio
there are many other methods to improve neural attention model jean et al luong pham and manning

conclusion in this work our goal is to improve the semantic relevance between source texts and generated simplied texts for text summarization and text simplication
to achieve this goal we propose a semantic relevance based neural network model srb
a similarity evaluation component is introduced to measure the relevance of source texts and generated texts
during training it maximizes the similarity score to encourage high semantic relevance between source texts and simplied texts
in order to better represent a long source text we introduce a self gated attention encoder to memory the input text
we conduct the experiments on three corpus namely lcsts pwkp and ew sew
experiments show that our proposed model has better performance than the state of the art systems on the benchmark corpus
acknowledgements
this work was supported in part by national natural science foundation of china no and an okawa research grant
xu sun is the corresponding author of this paper
this work is a substantial extension of the conference version presented at acl ma et al
references ayana shiqi shen zhiyuan liu and maosong sun

neural headline generation with minimum risk training
corr
bahdanau dzmitry kyunghyun cho and yoshua bengio
neural machine translation by jointly learning to align and translate
corr
cao ziqiang wenjie li sujian li furu wei and yanran li

attsum joint learning of focusing and summarization with neural attention
in coling international conference on computational linguistics proceedings of the conference technical papers december osaka japan pages
computational linguistics volume number cao ziqiang chuwei luo wenjie li and sujian hu baotian qingcai chen and fangze zhu
li

joint copying and restricted generation for paraphrase
in proceedings of the thirty first aaai conference on articial intelligence pages

cao ziqiang furu wei sujian li wenjie li ming zhou and houfeng wang


learning summary prior representation for extractive summarization
in proceedings of the annual meeting of the association for computational linguistics and the international joint conference on natural language processing of the asian federation of natural language processing acl july beijing china volume short papers pages
cheng jianpeng and mirella lapata

neural summarization by extracting sentences and words
in proceedings of the annual meeting of the association for computational
linguistics acl august berlin germany volume long papers

cho kyunghyun bart van merrienboer aglar glehre dzmitry bahdanau fethi bougares holger schwenk and yoshua bengio


learning phrase representations using rnn encoder decoder for statistical machine translation
in proceedings of the conference on empirical methods in natural language processing emnlp pages
chopra sumit michael auli and
alexander rush
abstractive sentence summarization with attentive recurrent neural networks
in naacl hlt the conference of the north american chapter of the association for computational linguistics human language technologies pages

filippova katja enrique alfonseca carlos colmenares lukasz kaiser and oriol vinyals
sentence compression by deletion with lstms
in proceedings of the conference on empirical methods in natural language processing emnlp pages

glava goran and sanja tajner
simplifying lexical simplication do we need simplied corpora
in proceedings of the
annual meeting of the association for computational linguistics acl pages

gu jiatao zhengdong lu hang li and victor li

incorporating copying mechanism in sequence to sequence learning

in proceedings of the annual meeting of the association for computational linguistics acl
hochreiter sepp and jrgen schmidhuber


long short term memory
neural computation

lcsts
a large scale chinese short text summarization dataset
in proceedings of the conference on empirical methods in natural language processing emnlp lisbon portugal september pages
hwang william hannaneh hajishirzi mari ostendorf and wei wu

aligning sentences from standard wikipedia to simple wikipedia
in naacl hlt pages
jean sbastien kyunghyun cho roland memisevic and yoshua bengio
on using very large target vocabulary for neural machine translation
in proceedings of the

annual meeting of the association for computational linguistics acl pages
kauchak david

improving text simplication language modeling using unsimplied text data
in proceedings of the

annual meeting of the association for computational linguistics acl pages
kingma diederik and jimmy ba

adam
a method for stochastic optimization
corr lin chin yew and eduard hovy

automatic evaluation of summaries using n gram co occurrence statistics
in human language technology conference of the north american chapter of the association for computational linguistics hlt naacl
lopyrev konstantin
generating news headlines with recurrent neural networks

corr
luong thang hieu pham and christopher manning

effective approaches to attention based neural machine translation
in proceedings of the conference on empirical methods in natural language processing emnlp pages
ma shuming xu sun jingjing xu houfeng wang wenjie li and qi su
improving semantic relevance for sequence to sequence learning of chinese social media text summarization
in proceedings of the annual meeting of the association for computational linguistics acl vancouver canada july august volume short papers pages
manning christopher mihai surdeanu john bauer jenny rose finkel steven bethard and david mcclosky

the stanford corenlp natural language processing toolkit
in proceedings of the annual meeting of the association for computational linguistics acl pages

shuming ma et al
a semantic relevance based neural network for summarization and simplication nisioi sergiu sanja stajner simone paolo xu jingjing and xu sun

nallapati ramesh bowen zhou ccero nogueira dos santos aglar glehre and bing xiang

abstractive text summarization using sequence to sequence rnns and beyond
in proceedings of the signll conference on computational natural language learning conll berlin germany august pages narayan shashi and claire gardent

hybrid simplication using deep semantics and machine translation
in proceedings of the
annual meeting of the association for computational linguistics acl pages
ponzetto and liviu dinu
exploring neural text simplication models
in proceedings of the annual meeting of the association for computational linguistics acl pages radev dragomir timothy allison sasha blair goldensohn john blitzer arda elebi stanko dimitrov elliott drbek ali hakim
wai lam danyu liu jahna otterbacher hong
qi horacio saggion simone teufel michael topper adam winkel and zhu zhang


mead a platform for multidocument multilingual text summarization
in proceedings of the fourth international conference on language resources and evaluation lrec
rush alexander sumit chopra and jason weston

a neural attention model for abstractive sentence summarization
in proceedings of the conference on empirical methods in natural language processing emnlp lisbon portugal september pages

srivastava nitish geoffrey hinton alex krizhevsky ilya sutskever and ruslan salakhutdinov

dropout a simple way to prevent neural networks from overtting

journal of machine learning research
sun xu houfeng wang and wenjie li


fast online training with frequency adaptive learning rates for chinese word segmentation and new word detection
in proceedings of pages sutskever ilya oriol vinyals and quoc le


sequence to sequence learning with neural networks
in advances in neural information processing systems annual conference on neural information processing systems pages
wang wenhui and baobao chang


graph based dependency parsing with bidirectional lstm
in proceedings of the annual meeting of the association for computational linguistics acl
woodsend kristian and mirella lapata

learning to simplify sentences with quasi synchronous grammar and integer programming
in proceedings of the conference on empirical methods in natural language processing emnlp pages
wubben sander antal van den bosch and emiel krahmer

sentence simplication by monolingual machine translation
in the
annual meeting of the association for computational linguistics proceedings of the conference pages
dependency based gated recursive neural network for chinese word segmentation
in meeting of the association for computational
linguistics pages
xu wei courtney napoles ellie pavlick quanze chen and chris callison burch


optimizing statistical machine translation for text simplication
tacl

zhang xingxing and mirella lapata
sentence simplication with deep reinforcement learning
corr
zhu zhemin delphine bernhard and iryna gurevych

a monolingual tree based translation model for sentence simplication

in coling pages

