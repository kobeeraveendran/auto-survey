pre training for abstractive document summarization by reinstating source text yanyan xingxing wei furu and ming jd
com microsoft research asia beijing china statnlp research group singapore university of technology and design zoe

com xizhang fuwei
com
edu
sg abstract abstractive document summarization is ally modeled as a sequence to sequence learning problem
unfortunately training large based summarization models on limited supervised summarization data is challenging
this paper presents three sequence to sequence pre training in hand step objectives which allow us to pre train a based abstractive marization model on unlabeled text
the main idea is that given an input text cially constructed from a document a model is pre trained to reinstate the original ment
these objectives include sentence ordering next sentence generation and masked document generation which have close tions with the abstractive document rization task
experiments on two benchmark summarization datasets i
e
cnn dailymail and new york times show that all three jectives can improve performance upon lines
compared to models pre trained on large scale data gb our method with only gb text for pre training achieves comparable results which demonstrates its effectiveness
code and models are lic available at

introduction automatic document summarization is the task of condensing a document into its shorter form with important content preserved which requires coverage understandings of the document rather than specic words or phrases
this task can be typically classied into two categories extractive and abstractive document summarization
tive summarization cheng and lapata lapati et al
narayan et al
aims to contribution during internship at microsoft research
the rst author now works in jd
com
extract important sentences from the input ment and concatenates such extracted sentences as the corresponding output summary
thus the ative orders of the selected sentences in the mary is the same as their relative orders in the put document
differently abstractive tion nallapati et al
see et al
paulus et al
rewrites the source text and ates the corresponding summary which may tain novel words and phrases not featured in the put
the output summary is closely related to the input document
also summary sentences phrased from the input by the abstractive rizers might have a different relative order pared to the source text
in other words contents of the original document may be reordered in its summary
such a phenomena is dened as tent reordering see section
for detailed nition
statistically we observed that around instances of the training split of our summarization dataset have this content reordering phenomena
therefore it is necessary to design a model that is capable of reordering content
however as far as we know relatively rare prior work has studied this for abstractive summarization
abstractive summarization is usually framed as a sequence to sequence learning lem nallapati et al
see et al

in this paper we adopt the transformer vaswani et al
which has been strated to be the state of the art for modeling vaswani et al
ott et al

recent studies song et al
dong et al
lewis et al
zhang et al
fel et al
have proven effectiveness of trained transformer models on the ural language generation tasks such as abstractive summarization
based on the above observations with regard to abstractive summarization this work proposes t c o l c
s c v
v i x r a three sequence to sequence pre training in hand step objectives which can be used to train a model on unlabeled text namely sentence reordering sr next sentence eration nsg and masked document tion mdg
all three objectives are designed to reinstate the original source text
sr learns to recover a document with randomly shufed tences
given the rst segment of a document nsg generates the next segment of the original document
mdg learns to recover a masked ument to its original form
after pre training a model with our proposed on unlabeled documents we tune it on supervised summarization datasets i
e
cnn dailymail and new york times
ments show that even pre training on documents from the training split of a summarization dataset our method can improve performance upon a heavily tuned large transformer model which already includes a strong pre trained coder by a large margin
by involving more data gb for pre training the performance is further improved
compared to models pre trained with much more data gb we can still achieve comparable or even higher rouge scores
related work extractive summarization this task aims to nd the informative sentences in a document as its summary
this task is usually viewed as a tence ranking problem kupiec et al
roy and oleary using scores from a nary sequence classication model which dicts whether a sentence is in the summary or not
extractive neural models cheng and ata nallapati et al
narayan et al
zhang et al
employ hierarchical lstms cnns as the feature learning part of the binary sequence classier which largely perform discrete feature based models radev et al
filatova and hatzivassiloglou nenkova et al

very recently the feature learning part was replaced again with pre trained transformer encoders zhang et al
liu and lapata that lead to another huge formance gain
however extractive models have their own limitations
for example the extracted sentences might be too long and redundant
sides manually written summaries in their nature are abstractive
therefore we focus on abstractive summarization in this paper
abstractive summarization this task aims to generate a summary by rewriting a document which is a learning problem
attentive lstms hochreiter and schmidhuber bahdanau et al
are employed in nallapati et al
that have been extended with copy mechanism gu et al
coverage model see et al
and reinforcement ing paulus et al

liu and lapata used a transformer model with only its encoder initialized with a pre trained transformer encoder i
e
bert devlin et al

this work proposes to pre train the decoder together with the encoder and then initialize both the coder and decoder of a summarization model with the pre trained transformer model
there is also a line of work that bridges tive and abstractive models with attention nisms gehrmann et al
hsu et al
and reinforcement learning chen and bansal while our model is simpler
pre training pre training methods draw a lot of attentions recently
peters et al
and ford et al
pre trained lstm and former using language modeling objectives
to leverage the context in both directions bert vlin et al
is trained with the masked guage modeling and next sentence prediction jectives
spanbert joshi et al
applied only the masked language modeling objective that masks contiguous random spans rather than dom tokens
xlnet yang et al
proposed a permutation language modeling objective that moves the independence assumption of masked kens in bert
roberta liu et al
extends bert with more training data and better training strategies
the above models focus on pre training an encoder or a decoder while we propose ods to pre train a model i
e
the coder together with the decoder for abstractive summarization
dong et al
unilm proposed a unied language model that can be used for both ral language understanding and generation tasks which is pre trained using masked unidirectional and language modeling objectives
the encoder and decoder parameters are shared
by contrast we pre train a transformer with separate parameters for the encoder and coder
song et al
mass proposed a method to pre train a transformer by masking a span of text and then predicting the masked tokens
their pre training task is similar to our mdg task but we apply a different ing strategy and predict the original text
song et al
tested their model on sentence level tasks e

machine translation and sentence pression while we aim to solve document level tasks e

abstractive document summarization
lewis et al
bart adopted the tion of text inlling and sentence permutation as a single objective for transformer training
differently we propose three objectives and use them individually
specically mdg placed each selected token with a masked token in the input sequence
raffel et al
ies different pre training objectives model tectures and unlabeled datasets
prophetnet yan et al
predicts the next n tokens ously
zhang et al
pegasus proposed to remove mask sentences from an input document and learn to generate such removed masked tences for pre training while nsg predicts the following sentences of the input sequence and mdg masks randomly selected tokens
proposed method
sequence to sequence learning in this work the task of abstractive document summarization is modeled as a learning problem
we adopt the transformer chitecture vaswani et al

given a ment x


paired with its mary y


we aim to learn the model parameters and estimate the conditional probability p y t x where y t stands for all tokens before position t i
e
y



given the whole training set x y this model can be trained by maximizing the log likelihood of the training document summary pairs log p y x y x y y
pre training objectives automatic abstractive summarization requires comprehensive understanding of the input ment and rewrites the source text into its shorter form where the summary is closely related to the input retaining important contents
also ing the document may result in content reordering
now we dene content reordering as follows
for each document summary pair we rst map each sentence in the summary to its ing sentence in the document by maximizing the rouge score see appendix a more details
if the relative orders of sentences in the summary are different from the relative orders of their mapped sentences in the original document we count this as one content reordering
according to the tics on the training split of our summarization dataset contents of the original documents are reordered in their summaries for of cases proximately
the above observations motivate us to propose sequence to sequence pre training objectives that are capable of pre training a model ing the abstractive summarization task
sentence reordering in sentence reordering sr we rst divide an unlabeled document into multiple sentences based on full stops
let us change the notation of a document slightly in this paragraph
let x


denote a document where si is a sentence m is the number of sentences and refers to sentence tion
the sentence index order in x can be resented as o


m
we then shufe in other words the the document by sentences
items in the order o are rearranged and we obtain a shufed order os


am where ai m aj m and ai aj for any i j m and i j
concatenating tences following os we obtain a shufed ment xs



a model takes as input the shufed document xs and is pre trained to reinstate the original one x as demonstrated in figure
the training tive is calculated as x log p xs xx we rst pre train the transformer model on the unlabeled text using our proposed pre training objectives see section
and then ne tune it on the document summary dataset
there are several reasons why we design this objective
first a summary of a document ally consists of multiple sentences
we expect that the model is pre trained to learn to generate figure assume a document contains three sentences i
e
sent
sent
and sent

a transformer model can be pre trained with our proposed objective
it takes the transformed document i
e
a shufed document the rst segment of a document or a masked document as input and learns to recover the original document or part of the original document by generation
sr sentence reordering nsg next sentence generation mdg masked document generation
long and coherent summaries across sentences
the output of the objective i
e
the original ment also contains multiple sentences
second as we discussed earlier sentence reordering or tent reordering is necessary for summarization
third abstractive summary requires reproducing factual details e

named entities gures from the source document
we also expect the model to learn to copy tokens
note that document is a special case of sentence reordering with a signicant amount of partially ordered sentences which we believe is a simpler objective
in this work we only consider the general case of sentence reordering
next sentence generation next sentence eration nsg uses one span of text in a document to predict its next span of text which leverages the natural order of text as shown in figure
ically we split a document into two segments i
e
and
note that each segment might contain multiple sentences
intuitively in a ument sentences are highly correlated with their document is randomly divided into two fragments x using full stops
the rotated document is xr
document rotation recovers x using xr
preceding sentences due to the context dependent nature of documents or language
our intention is to learn to generate multiple sentences and also learn to focus on input text which ts the ument summarization task since either a ment or its summary usually includes multiple sentences and they are closely related
the ing objective is calculated as x log p xx we do not make constraints that the split point must be the position right after a full stop bol which ensures full sentences for each ment
instead the split point can be at any position within the document which may lead to plete sentences in segments
we intend to force the model to understand input text without complete information
similarly as a common wisdom in abstractive summarization documents as input are truncated to a xed number of tokens which may also contain incomplete sentences
this ting allows to reduce mismatches between the training and ne tuning input
masked document generation the third jective is masked document generation mdg that learns to reinstate a document with a masked span of tokens see figure
a document is noted as x
we randomly sample the length of the span l from a discrete form distribution a and b are distribution parameters and the span starting position from another discrete uniform distribution l
thus m xk is the text span to be masked
let xm denote the ment after the application of our masking strategy
the training objective is calculated as x log p xm xx one straightforward masking strategy is to place each token residing in m with a special mask token
however we refrain from doing so because of the following two reasons
usually mask tokens will not appear in downstream tasks
second similar to sr avoiding replacing every token with mask also helps our model learn the ability of copying tokens from the input while preserving the ability of generating novel kens
thus in the sub sequence m each token is processed with one of the three strategies placed with the mask token replaced with a random token remains unchanged
inspired by bert devlin et al
for of selected tokens we follow strategy
in of cases we employ strategy and we use strategy for the remaining of cases
during pre training we consider two settings
setting one pre training a model with one gle objective i
e
sr nsg or mdg resulting in three different pre trained models
setting two employing all three objectives
for each training batch we randomly choose one objective and each objective is used for of the training time taining one model i
e
all see section
for better reference we name our model as step i
e
sequence to sequence pre training that can be used to denote a model trained using our proposed

fine tuning after a model is pre trained we tune the model on abstractive document rization datasets
in other words we continue to train the model on the document summary pairs
experimental setup
datasets cnndm the cnndm dataset contains news articles and the associated highlights i
e
maries collected from the cnn and daily mail online
articles were collected starting in april for cnn and june for daily mail both until the end of april
the tion data is from march and the test data from april hermann et al

lowing previous work see et al
liu and lapata we use the non anonymized sion of cnndm
specically we preprocessed the dataset with the publicly available vided by see et al
and obtained document summary pairs for training for validation and for test
nyt the nyt dataset sandhaus is a collection of articles along with multi sentence summaries written by library scientists
following the preprocessing procedures described in durrett et al
liu and lapata the test set is constructed by including all articles published on january or later which contains cles
the remaining articles are split into a training set of examples and a validation set of examples
following durrett et al
we also removed articles whose summaries contain less than words from the test set and the resulting test set contains examples
giga cm to pre train our model with the jectives introduced in section
following the procedures in zhang et al
we created the giga cm dataset which contains only unlabeled documents
the training set of giga cm is posed of documents sampled from the english gigaword and the training ments in cnndm resulting in gb text for training
we used the documents in the validation split of cnndm as the validation set
note that the gigaword dataset overlaps with the nyt dataset and we therefore excluded the test set of nyt from the training set of giga cm
table lists the number of document summary pairs for cnndm and nyt and unlabeled uments for giga cm
for cnndm nyt and
cnn
com and
co

com abisee cnn dailymail
ldc
upenn
edu dataset training validation test cnndm nyt giga cm table the number of document summary pairs for cnndm and nyt and unlabeled documents for giga cm
giga cm datasets we segmented and tokenized documents summaries giga cm only contains documents using the stanford corenlp toolkit manning et al

we further applied the based bpe sennrich et al
radford et al
to reduce the vocabulary size
as a common wisdom in abstractive summarization documents and summaries in cnndm and nyt are usually truncated to and tokens spectively
we leverage unlabeled documents differently for different pre training objectives
we rst split each document into token pieces if it contains more than tokens pieces or documents with less than tokens are removed
in sr and mdg we use the piece after transformation to predict its original form
we set the minimum and maximum masked length a and in mdg individually
in nsg each piece is used to predict its next tokens

implementation details as mentioned in section we adopt the transformer model vaswani et al
as our backbone architecture
the purpose of releasing large pre trained models is to reuse so that the community can avoid high computational costs
hence similar to previous work liu and ata our encoder is initialized with a trained model i
e
robertalarge liu et al
and therefore they share the same chitecture
specically the encoder is a layer transformer
each layer has attention heads and its hidden size and feed forward lter size are and respectively
the decoder is lower with layers and is randomly initialized
the number of total trainable model parameters is m
the hidden size and number of attention head of the decoder are identical to those of the encoder but the feed forward lter size is
we use a smaller lter size in the decoder to tried robertabase and obtained inferior results
duce the computational and memory cost
the dropout rates of all layers in the encoder are set to
and all dropout rates in the decoder are set to

our models are optimized using adam kingma and ba with


the other optimization hyper parameters for training and ne tuning are different
in the training stage the encoder is initialized with a pre trained model while the decoder is randomly initialized
therefore similar to liu and lapata we used two separate optimizers for the encoder and decoder
the peak learning rates of the encoder and decoder are set to and with warmup steps respectively
we also adopted the same learning rate schedule strategies as in vaswani et al

we used smaller batch sizes for datasets with less examples i
e
for giga cm for cnndm and for nyt to ensure each epoch has sufcient number of model updates
we trained our models until their convergence of validation perplexities around epochs on giga cm epochs on cnndm and epochs on nyt
one epoch on giga cm takes around hours with nvidia tesla gpus
the time costs for different pre training objectives are close
we highlight the parameters used in the tuning stage that are different from the pre training stage
others remain the same
the learning rates for both the encoder and decoder are set to with warmup steps since both the encoder and decoder are already pre trained
we trained our models for epochs on cnndm and epochs on nyt respectively
we selected the best model with regard to rouge score on the validation set
during decoding similar to liu and lapata dong et al
we applied beam search with beam size of
we also ducted experiments on the validation set of ndm with different beam sizes i
e
to
cording to rouge l is indeed optimal
detailed results with different beam sizes are cluded in the appendix b
following paulus et al
we also blocked repeated trigrams ing beam search and tuned the minimum summary length on the validation set in the range of
the search range of minimum summary length was empirically set according to the summaries of training split of cnndm where the average and medium minimum lengths are both around
we used step size of to get quick feedback
similar to the pre training process the datasets with less instances were ne tuned with smaller batch sizes i
e
for nyt and for cnndm
results
automatic evaluation we used rouge lin to measure the quality of different summarization model puts
we reported full length based and rouge l scores on ndm while we used the limited length call based and l on nyt
the rouge scores are computed using the


pl
following durrett et al
models in comparison is a baseline which simply takes the rst three sentences of a document as its summary
bertext liu and lapata is an extractive model ne tuned on bert devlin et al
that outperforms other extractive systems
ptgen see et al
drm paulus et al
and dca celikyilmaz et al
are ing based models extended with copy and age mechanism reinforcement learning as well as deep communicating agents individually
tomup gehrmann et al
assisted mary generation with a word prediction model
bertabs liu and lapata and unilm dong et al
are both pre training based models and are trained based on bert devlin et al

we also implemented four stractive models as our baselines
is a layer transformer with dom initialization
when we replaced the coder of transformer with robertabase or robertalarge liu et al
we obtain two baselines robertabase and respectively
following liu et al
we further train the robertalarge on the ments of training split of cnndm for epochs same as the number of epochs for our models indicated as in domain
we replaced the coder of transformer with this further trained model resulting in robertacont
results on cnndm the results on the ndm are listed in table
the rst and ond blocks show results of previous extractive and
com bheinzerling pyrouge
git model r l bertext liu and lapata





extractive abstractive ptgen see et al
drm paulus et al
bottomup gehrmann et al
dca celikyilmaz et al
bertabs liu and lapata unilm dong et al
transformer robertabase roberta robertacont ours step in domain step giga cm sr nsg mdg all sr nsg mdg all





















































table results on the test split of cnndm using length based and rouge l r l
indicates signicant improvements p
measured with the rouge script compared to models in the rst two blocks
abstractive models respectively
results of ours are all listed in the third block
outperforms transformer by nearly rouge points
roberta further improves the performance
this shows the effectiveness of the pre trained encoders
then we study the effects of different training objectives see section

we rst train a transformer model the sizes of our model and roberta are identical on unlabeled documents of cnndm training split to get quick denoted as step domain
from the top part of the third block in table we can see that sentence ing sr next sentence generation nsg and masked document generation mdg can all improve robertabase and roberta signicantly measured by the rouge
terestingly even though we merely use the domain training split around gb our method still signicantly outperforms unilm dong et al
that is pre trained on gb data
compared to step in domain e

pre training with sr epoch takes hours on cnndm and
on nyt
to the rouge script
rouge almost always means a signicant difference with p

model r l model corpus size bertext liu and lapata





extractive abstractive ptgen see et al
drm paulus et al
bertabs liu and lapata transformer roberta




ours step in domain step giga cm sr nsg mdg all sr nsg mdg all































table results on the test set of nyt dataset using limited length recall based rouge
indicates nicant improvements p
measured with the rouge script to models in the rst two blocks
with robertacont although the encoders of such two models are pre trained on the same corpus for the same epochs our model achieves better performance
this shows that the mance gains mainly result from our proposed jectives for pre training the decoder together with the encoder
training roberta longer may prove understanding tasks liu et al
but no evidence shows longer training time for roberta may improve generation performance
when we pre train the model on even larger dataset i
e
giga cm in the size of gb indicated as step giga cm the sults are further improved and our method forms all models under comparison as listed in the bottom part of table
results on nyt table presents results on nyt dataset
following the same evaluation tocol as durrett et al
we adopted the limited length recall based rouge where we truncated the predicted summaries to the length of the gold ones
again the rst and second blocks show results of previous extractive and abstractive models respectively
results of our models are listed in the third block
similar to the trends in cnndm our method leads to signicant mance gains with p

comparisons among objectives among all three pre training objectives sr works slightly pegasus pegasus hugenews bart prophetnet gb prophetnet gb unilm step gb gb gb gb gb gb gb gb















r l







table results on the cnndm test split of models pre trained on different corpora
indicates signicant differences from our model
better than the other two objectives i
e
nsg and mdg
we also tried to randomly use all the three objectives during training with ity each indicated as all
interestingly we served that in general all outperforms all three objectives when employing unlabeled documents of training splits of cnndm or nyt which might be due to limited number of unlabeled uments of the training splits
after adding more data i
e
giag cm for pre training sr sistently achieves the highest on both cnndm and nyt
we conclude that sr is the most effective pre training objective for tive summarization since sentence reordering jective ts content reordering and it requires prehensively understanding a document in a wide coverage going beyond individual words and tences which is highly close to the essence of stractive document summarization
we put the performance of our models on the validation splits of cnndm and nyt in the pendix b
comparison to models pre trained with scale corpora it is worth noting that several models have been released recently which are trained using various corpora much larger than ours as listed in table top part
raffel et al
introduced gb as its training corpus
pegasuslarge has two sions that are pre trained on and hugenews gb respectively
both bart lewis et al
and prophetnet gb yan et al
are pre trained on a gb corpus introduced by liu et al

we compare our best ing model step i
e
pre training on the cm dataset using sr objective with such els and focus on the performance on the ndm which is the well known benchmark for stractive summarization
we highlight the est rouge scores in table using bold font and use the symbol to indicate the models that form signicantly different from step
both and pegasus hugenews achieve signicantly higher scores than our model
ever we obtain higher and rouge l scores
on the other hand we also consider els pre trained on the relatively small scale pus
following bert devlin et al
both prophetnet gb yan et al
and unilm dong et al
use the same gb text for training
as listed in table bottom part our model signicantly outperforms such two models
systems mr
bertabs unilm
roberta

step
gold
























table human evaluation results proportions of tem rankings
mr mean rank the lower the better
as input and thus may lose information residing in the following tokens
qualitative analysis with generated examples are illustrated in the appendix c

human evaluation conclusion since summaries generated by abstractive models may produce disuent or ungrammatical outputs we also evaluated abstractive systems by eliciting human judgements
we compared our best forming model i
e
pre training on the cm dataset using sr objective with human ences denoted as gold as well as several strong baselines whose system outputs are available to us including roberta and two pre training based models i
e
bertabs liu and lapata and unilm dong et al

uments are randomly sampled from the test split of cnndm
participants are presented with a document and a list of outputs generated by ent abstractive summarization systems
then they are asked to rank the outputs of these systems from best to worst according to informativeness does the summary capture the informative part of the document uency is the summary cal and succinctness does the summary express the document clearly in a few words we report the proportions of system rankings and mean rank lower is better in table
the output of step is selected as the best for the of cases and we obtained lower mean rank than all systems except for gold which shows the participants preference for our model
we further converted ranking bers into ratings i
e
rank i is converted into and applied the student t test on the ratings
ours is signicantly better than all other systems cept for gold in comparison with p

ever it still lags behind human
one possible son is that our system as well as other systems only takes the rst tokens of a long document we proposed three sequence to sequence training objectives including sentence reordering next sentence generation and masked document generation
all those objectives have relations with abstractive summarization task and are signed based on reinstating the source text
a model for abstractive document marization can be pre trained using such tives and then ne tuned on the summarization dataset
compared to models pre training on the even larger corpora gb our method with only gb for pre training can still achieve parable and even better performance
in the ture we would like to investigate other objectives to pre train models for abstractive marization
acknowledgments we would like to thank the anonymous reviewers for their thoughtful and constructive comments
yanyan zou and wei lu were supported by gapore ministry of education academic research fund acrf tier project
references dzmitry bahdanau kyunghyun cho and yoshua bengio

neural machine translation by jointly learning to align and translate
in proc
of iclr
asli celikyilmaz antoine bosselut xiaodong he and yejin choi

deep communicating agents for abstractive summarization
in proc
of naacl
of bertabs and unilm are publicly able at
com nlpyang presumm and
com microsoft unilm yen chun chen and mohit bansal

fast tive summarization with reinforce selected sentence rewriting
in proc
of acl
jianpeng cheng and mirella lapata

neural summarization by extracting sentences and words
in proc
of acl
john m conroy and dianne p oleary

text marization via hidden markov models
in proc
of sigir
jacob devlin ming wei chang kenton lee and kristina toutanova

bert pre training of deep bidirectional transformers for language ing
in proc
of acl
li dong nan yang wenhui wang furu wei aodong liu yu wang jianfeng gao ming zhou and hsiao wuen hon

unied language model pre training for natural language ing and generation
in proc
of nips
greg durrett taylor berg kirkpatrick and dan klein

learning based single document tion with compression and anaphoricity constraints
in proc
of acl
elena filatova and vasileios hatzivassiloglou

event based extractive summarization
in text marization branches out
sebastian gehrmann yuntian deng and alexander rush

bottom up abstractive summarization
in proc
of emnlp
jiatao gu zhengdong lu hang li and victor o
k
incorporating copying mechanism in li

sequence to sequence learning
in proc
of acl
karl moritz hermann tomas kocisky edward grefenstette lasse espeholt will kay mustafa leyman and phil blunsom

teaching chines to read and comprehend
in proc
of nips
sepp hochreiter and jurgen schmidhuber

neural computation long short term memory

wan ting hsu chieh kai lin ming ying lee kerui min jing tang and min sun

a unied model for extractive and abstractive summarization using inconsistency loss
in proc
of acl
mandar joshi danqi chen yinhan liu daniel s weld luke zettlemoyer and omer levy

spanbert improving pre training by representing and ing spans
transactions of the association for putational linguistics
diederik p kingma and jimmy lei ba

adam in proc
of a method for stochastic optimization
iclr
julian kupiec jan pedersen and francine chen

in proc
of a trainable document summarizer
gir
mike lewis yinhan liu naman goyal jan ghazvininejad abdelrahman mohamed omer levy ves stoyanov and luke zettlemoyer

bart denoising sequence to sequence pre training for natural language generation translation and comprehension
arxiv preprint

chin yew lin

rouge a package for in proc
of matic evaluation of summaries
workshop
yang liu and mirella lapata

text tion with pretrained encoders
in proc
of emnlp
yinhan liu myle ott naman goyal jingfei du dar joshi danqi chen omer levy mike lewis luke zettlemoyer and veselin stoyanov

roberta a robustly optimized bert pretraining proach
in proc
of acl
christopher manning mihai surdeanu john bauer jenny finkel steven bethard and david mcclosky

the stanford corenlp natural language in proc
of acl system cessing toolkit
strations
ramesh nallapati feifei zhai and bowen zhou

summarunner a recurrent neural network based quence model for extractive summarization of ments
in proc
of aaai
ramesh nallapati bowen zhou cicero dos santos caglar gulcehre and bing xiang

tive text summarization using sequence to sequence rnns and beyond
in proc
of signll
shashi narayan shay b cohen and mirella lapata

do nt give me the details just the summary topic aware convolutional neural networks for treme summarization
in proc
of emnlp
shashi narayan shay b cohen and mirella lapata

ranking sentences for extractive in proc
of rization with reinforcement learning
naacl
ani nenkova lucy vanderwende and kathleen eown

a compositional context sensitive multi document summarizer exploring the factors that inuence summarization
in proc
of sigir
myle ott sergey edunov alexei baevski angela fan sam gross nathan ng david grangier and michael auli

fairseq a fast extensible toolkit for sequence modeling
in proc
of naacl demonstrations
romain paulus caiming xiong and richard socher

a deep reinforced model for abstractive marization
in proc
of iclr
matthew peters mark neumann mohit iyyer matt gardner christopher clark kenton lee and luke zettlemoyer

deep contextualized word resentations
in proc
of naacl
dragomir radev timothy allison sasha goldensohn john blitzer arda c elebi stanko dimitrov elliott drabek ali hakim wai lam danyu liu jahna otterbacher hong qi horacio saggion simone teufel michael topper adam winkel and zhu zhang

mead a platform for multidocument multilingual text summarization
in proc
of lrec
alec radford jeffrey wu rewon child david luan dario amodei and ilya sutskever

language models are unsupervised multitask learners
openai blog
colin raffel noam shazeer adam roberts katherine lee sharan narang michael matena yanqi zhou wei li and peter j liu

exploring the limits of transfer learning with a unied text to text former
arxiv preprint

evan sandhaus

the new york times annotated corpus
linguistic data consortium philadelphia
abigail see peter j liu and christopher d manning

get to the point summarization with generator networks
in proc
of acl
rico sennrich barry haddow and alexandra birch
neural machine translation of rare words with word units
in proc
of acl
kaitao song xu tan tao qin jianfeng lu and yan liu

mass masked sequence to sequence in proc
of pre training for language generation
icml
ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan n gomez ukasz kaiser and illia polosukhin

attention is all you need
in proc
of nips
yu yan weizhen qi yeyun gong dayiheng liu nan duan jiusheng chen ruofei zhang and ming prophetnet predicting future zhou

gram for sequence to sequence pre training
arxiv preprint

zhilin yang zihang dai yiming yang jaime bonell ruslan salakhutdinov and quoc v le

xlnet generalized autoregressive arxiv preprint ing for language understanding


jingqing zhang yao zhao mohammad saleh and ter j liu

pegasus pre training with tracted gap sentences for abstractive summarization
arxiv preprint

xingxing zhang mirella lapata furu wei and ming zhou

neural latent extractive document marization
in proc
of acl
xingxing zhang furu wei and ming zhou

hibert document level pre training of cal bidirectional transformers for document rization
in proc
of acl
qingyu zhou nan yang furu wei shaohan huang ming zhou and tiejun zhao

neural ment summarization by jointly learning to score and select sentences
in proc
of acl
a additional setup details statistics for content reordering recall that it is not an unusual case that a human rewrites a document to summarize its most important mation yet does not track the ordering in which how such information is described in the ment
this phenomena is dened as content ordering as follows
for each document summary pair we rst map each sentence in the summary to one sentence in the document by maximizing the score
if the relative orders of tences in the summary are different from the tive orders of their mapped sentences in the inal document we count this as one content ordering
we did statistics of such cases over the training and validation splits of cnndm dataset
to be specic we borrow the sentence annotations from extractive summarization zhang et al
zhou et al
zhang et al
that sider extractive summarization as a sentence sication task
the sentences in a document that maximize score lin against the human references are labeled as true while other sentences are assigned false
like previous extractive summarization systems zhang et al
we also concatenate sentences with label true in a document as its associated mary
for each sentence in a summary we search for its string closet sentence in its associated ument according to the count of overlapped grams
we found that for some instances the relative orders of sentences in the summary is not consistent with the relative orders of their closet sentences appearing in the document
in practice we found that
instances in the training split and
instances in the validation set have this phenomenon
b additional results results on validation set the performance of our proposed models on the validation splits of cnndm and nyt are listed in table and spectively
results on validation set of cnndm with ferent beam sizes table lists the rouge l beam size rouge l









table rouge l results on the validation set of cnndm with different beam sizes
model mnli sst qqp qnli sts rte mrpc cola roberta step encoder















table results on glue model r l step in domain step giga cm
sr nsg
mdg

all
sr nsg
mdg

all















table results on the validation split of cnndm ing full length based and rouge l r l
model r l step in domain step giga cm sr
nsg
mdg

all
sr nsg
mdg

all















table results on the validation set of nyt dataset using limited length recall based rouge
results on the validation set of cnndm with ferent beam sizes for the beam search during coding
beam size of gives the highest l score
thus we use in this work
results on xsum different from cnndm and nyt xsum consists of online news articles extracted from british ing corporation bbc each annotated with a short one sentence news summary ing the question what is the article about
the same split for ing validation testing and preprocessing dures described in the work of narayan et al
are adopted to make direct comparisons
model r l


extractive abstractive ptgen see et al
narayan et al
bertabs liu and lapata transformer roberta














step


ours table results on the test split of xsum using length based and rouge l r l
table lists results on the xsum
here we report our model pre trained using sr objective on the in domain pre training corpus indicated as step
as we can see that after pre training step does not give performance gain
one ble reason is that the summary of xsum contains only one sentence
the sr objective might not be helpful for this dataset
results on glue we also apply the encoder of our best performing model to the glue tasks as listed in table
compared to roberta liu et al
the encoder of our best performing model does not consistently achieve higher results which demonstrates that the improvements of our models on the abstractive summarization task do not come from a better encoder
c examples of system outputs table and demonstrate three output ples of various systems including bertabs liu and lapata unilm dong et al
gold standard summaries human references noted as gold the roberta baseline and our best performing model
table shows an ample that the outputs of systems bertabs and unilm copied a sentence from the input article while our model generates summaries by ing sentences
table lists an instance where the summary generated by the system unilm tains an incomplete sentence
article bertabs unilm cnn in response to reports of big banks threatening to withhold campaign funds from senate democrats sen
elizabeth warren last week offered a deant response bring it on
warren said she is nt going to slack off on her calls for breaking up banks and other measures to rein in wall street
as hillary clinton prepares to ofcially launch her presidential campaign this month she will need to make a choice about how much to highlight issues relating to economic inequality
former maryland gov
martin omalley who is also running for the democratic nomination is trying to steal clinton s thunder by talking about the problems of disproportionate wealth
in other words there are many signs that democrats are planning to take on the big issue of economic inequality
but in other recent news the likelihood that new york s chuck schumer will replace harry reid as leader of the senate democrats means the dreams of a more economically leftward party are crashing into political reality
while schumer has been a very effective democrat and skilled legislative leader he is also a wall street democrat who has spent much of his time courting and protecting powerful nancial interests who run one of the dominant industries in his state
he is not alone
even at his most progressive moments president barack obama relied on wall street donations for both of his campaigns
despite all the talk from conservatives about left wing socialism in the white house the nancial community has been willing to open its coffers to democrats without much concern even in the election
democratic populism ca nt really work within the current campaign nance system
the enormous pressures for parties to raise funds in campaigns has for many decades created pressure on democrats despite their political base to court big donors
during the california democrat tony coelho serving as the chairman of the democratic congressional campaign committee and then as majority whip made a strong appeal to savings and loans executives before the crash of the industry to catch up to republicans who had been outanking them in raising money
the democrats were and have continued to losing their traditional base of campaign support organized labor which had been a central source of campaign muscle since the providing money and campaign assistance during campaigns
without organized labor to serve as their foundation and with the pressure for raising private funds increasing many democrats concluded they needed business by their side
democrats running for president have made the same kind of choices
in obama disappointed many supporters upon becoming the rst president to abandon the post watergate public nance system for campaigns altogether preferring to raise money himself for the general campaign
while small donors were enormously important to his victories so too were business and wall street executives
at the height of the nancial crash when public sentiment had clearly turned against wall street the administration agreed to a nancial regulation bill dodd frank that was structured in such a way as to give powerful interests more than enough opportunity to limit the bite over the coming years
wall street with an army of counsel succeeded in eroding the impact of the legislation
not only does the acceptance of our campaign nance system limit the policy choices democrats can make but it also greatly damages the party s brand name
as the washington post reported the scandal that might bring down new jersey democratic sen
robert menendez is the rst involving large scale super pac donations
at the heart of the story is almost that physician salomon melgen gave to senate majority pac possibly in exchange for favors
this is not simply some sort of accommodation of democrats to the corporate system
they do nt have much of a choice
without these funds they wo nt be able to compete
in this election cycle independent campaign donors are causing a huge stir
in conservative circles the koch brothers and their allies are throwing around enormous amounts of money to candidates who will support their deregulatory agenda
individual donors such as las vegas gambling magnate sheldon adelson are causing ripples every time candidates speak pressuring them to adjust their agenda
democrats have found their own magnates for political support such as tom steyer and george soros
this is why campaign nance reform is so important without congress changing the fundamental dynamics there wo nt be much room for populism to thrive
even if democrats select someone like a elizabeth warren as their candidate or hillary clinton decides to move sharply to the left on economic policy there wo nt be much room for reform when the time of governance actually starts
the democratic party needs wall street more than it needs to take a stand against wall street
those are the facts on the ground
if democrats really want to take on wall street and tackle economic inequality they rst have to bring about reform of the campaign nance system
if campaigns were publicly funded or there were more stringent limits on independent expenditures wall street would have much more trouble achieving disproportionate inuence
reform could level the playing eld
more often than not campaign nance reform is an issue that gets sidetracked with little more than some pro forma words of support
a more populist economic agenda that revolved around progressive taxation and substantial public assistance to strengthen the middle class can only work in a different kind of political system
if things stay the same democrats can only continue to win elections by turning to their corporate and nancial base of support
julian zelizer sen
elizabeth warren said she is nt going to slack off on her call for breaking up banks
he says the likelihood that new york s chuck schumer will replace harry reid as leader of the senate democrats
zelizer democratic populism ca nt really work within the current campaign nance system julian zelizer democratic populism ca nt really work within the current campaign nance system
he says the pressure for parties to raise funds in campaigns has created pressure on democrats to court big donors
he says even at his most progressive moments president barack obama relied on wall street donations for both campaigns
he says obama s decision to abandon the post watergate public nance system step roberta julian zelizer sen
elizabeth warren is nt going to slack off on wall street
zelizer democrats are planning to take on the big issue of economic inequality
he says democrats have lost their traditional base of campaign support organized labor and money in their campaigns
julian zelizer democrats are planning to take on the big issue of economic inequality
zelizer democratic populism ca nt work within the current campaign nance system
he says democrats have lost traditional base of campaign support
julian zelizer elizabeth warren was deant about wall street but hillary clinton likely wo nt be
zelizer the democrats need wall street s campaign donations to be competitive in
gold table an example article sampled from the test splitting of cnndm paired with a list of summaries generated by different systems
we highlight with bold the sentences in the summaries that are copied from the article
article bertabs unilm cnn hillary clinton is nally announcing her candidacy for the presidential election
although she has watched her standing in the polls sag in recent months there is likely to be a boost in the days that follow the announcement
for democrats there is ample reason to be excited about clinton s run for the presidency
she is certainly one of the strongest candidates in many decades
she brings to the table extensive political and policy experience a combination of skills that is often lacking
she has been through some of the roughest partisan wars and emerged stronger than ever before
she has a keen sense about the nature of the modern news media how to use it to her advantage and how to survive scandal frenzies
she is a hardened tough partisan who will not shy away from republican attack
americans have many positive memories of clinton name given the booming economy of the late during bill clinton s presidency
if hillary clinton puts together an effective campaign she could be unbeatable in the democratic primaries as well as in the general election
however during the buildup to her nal decision some of her weaknesses have also been exposed
clinton does nt want to end up like vice president al gore in
although he did relatively well in the nal election with many americans believing that he did actually defeat george w
bush he did nt generate much energy once the campaign started
although he too was touted as a perfect candidate who was the ideal person for the job something seemed stiff and inauthentic when he actually hit the trail
he seemed to freeze when the television cameras were rolling
gore had trouble connecting with voters and he seemed to remake his image constantly
his biggest asset ended up being that he was viewed as the inevitable nominee rather than what he actually stood for
clinton must avoid following gore s path
she suffered this fate in the primaries and ca nt afford to do so again
she needs to do more than rest on the perception that her candidacy is inevitable and on her record of experience
that is not enough
more important is for her to put forth an exciting vision about what she would stand for in the white house
voters thirst for signs of greatness when they pick their presidents even if they are savvy enough to understand that the reality of a polarized washington will probably limit her ability to achieve bold change
a recent story in the washington post suggests that her advisers are aware of this potential liability
after the announcement they are going to avoid big rallies and events and instead concentrate on smaller events where she will meet with voters directly in states such as iowa and new hampshire
clinton also will have to contend with doubts about her authenticity
in his rst day on the campaign trail sen
rand paul immediately tapped into these concerns by raising questions about whether she could be trusted
that question has dogged the clintons ever since they came onto the national political scene in the late
their greatest virtue their immense skills as politicians has often come back to haunt them
bill clinton was attacked as slick willie by members of both parties for the perception that he would say anything to win and hillary clinton has faced similar criticism
when she tried to distance herself from her vote for the use of force in iraq many democrats did nt buy her critique of president george w
bush s foreign policies and went for barack obama instead
when she conducted her listening tour of new york before running for the senate many voters saw it as a manufactured effort to hide the fact she was running for ofce as an outsider
when she explained that there was nothing to the recent stories about her use of a private email server rather than her state department email some felt that even if the story was relatively minor it indicated that she was nt always telling us what she was really about
even if she is nt hiding anything she often gives that appearance
during the next few months clinton will also have to connect with her party s base
the ongoing speculation about sen
elizabeth warren of massachusetts has suggested that the most active part of the democratic party is not that enthused with clinton s candidacy
while they will probably vote for her they are not very motivated and do nt trust that she will stand for democratic values
she will need to address these concerns not through her style but through her agenda
voters will want to hear her talking about issues such as tougher nancial regulation and policies to diminish economic inequality as well as her positions on race and policing
she will also need to make clear that she has heard voters on being too hawkish about going to war and give clear indications about how she would handle a nuclear agreement with iran
clinton will also have to contend with the gender bias that still exists in the electorate at large
without any doubt she will be subject to questions and comments about her appearance for instance that wo nt be aimed at male candidates
part of her candidacy is itself an effort to break down these remaining vestiges of political sexism
but the struggle will be tough
finally and this relates to the last challenge clinton will have to contend with her husband
to be sure he can be an immense force on the campaign trail one of the most compelling democrats of our generation
but he can also be liability
as she learned in bill clinton is not always easy to control
when he speaks his mind as he did in dismissive comments about obama s candidacy it can often work against her
the fund raising records of the clinton foundation will also raise questions about conict of interest and ongoing stories about his personal life as was the case when monica lewinsky returned to the media a few months ago could re emerge on the campaign trail
whether that is fair or not is beside the point everything is fair game on the modern campaign trail
hillary clinton has the potential to be a hugely successful presidential candidate
but she and her campaign team will need to address the multiple questions and weaknesses that have become clear in recent months
julian zelizer hillary clinton is nally announcing her candidacy for the presidential election
zelizer she has been through some of the roughest partisan wars and emerged stronger than ever before
he says she is a hardened tough partisan who will not shy away from republican attack julian zelizer hillary clinton is nally announcing her candidacy for the presidential election
he says she has extensive political and policy experience a combination of skills often lacking
he says clinton does nt want to end up like vice president al gore in he did nt generate much energy once the campaign started
clinton must avoid following gore s path he step roberta julian zelizer for democrats there is plenty of reason to be excited about hillary clinton s run
zelizer if clinton puts together an effective campaign she could easily win the general election
he says clinton needs to put forth on what she would stand for in the white house
julian zelizer for democrats there is ample reason to be excited about hillary clinton s run for president
zelizer clinton needs to put forth an exciting vision about what she would stand for
julian zelizer hillary clinton has immense political and governmental experience
he says she needs to make stronger connection to her party s base
clinton also needs to convince voters of her authenticity zelizer says
gold table an example article sampled from the test splitting of cnndm paired with a list of summaries generated by different systems
the incomplete sentence is highlighted with bold

