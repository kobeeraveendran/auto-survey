multi view sequence to sequence models with conversational structure for abstractive dialogue summarization jiaao chen school of interactive computing georgia institute of technology
edu diyi yang school of interactive computing georgia institute of technology
edu abstract text summarization is one of the most lenging and interesting problems in nlp
although much attention has been paid to summarizing structured text like news ports or encyclopedia articles summarizing conversations an essential part of human machine interaction where most tant pieces of information are scattered across various utterances of different speakers remains relatively under investigated
this work proposes a multi view sequence sequence model by rst extracting tional structures of unstructured daily chats from different views to represent tions and then utilizing a multi view coder to incorporate different views to erate dialogue summaries
experiments on a large scale dialogue summarization corpus demonstrated that our methods signicantly outperformed previous state of the art els via both automatic evaluations and man judgment
we also discussed specic challenges that current approaches faced with this task
we have publicly released our code at
com gt multi view
introduction we live in an information age where cations between human and human machine are increasing exponentially in the form of textual alogues between users and users agents kester
it is challenging and time consuming to review all the content before starting any tions especially when the chatting history becomes very long gao et al

how to process and ganize those interaction activities into concise and structured data i
e
conversation summarization becomes technically and socially important
most existing research efforts on text rization have been focused on single speaker uments like news reports nallapati et al
see et al
scientic publications nikolov et al
or encyclopedia articles liu et al
where structured text is usually used to orate a core idea in the third person point of view and the information ow is very clear through graphs or sections
different from these structured documents conversations are often informal bose and repetitive sprinkled with false starts back channeling reconrmations hesitations speaker interruptions sacks et al
and the salient information is scattered in the whole chat ing current summarization models hard to focus on many informative utterances
take the conversation in table as an example turns informal words breviations and emoticons all introduce new forms of challenges to the task of summarization
this calls for the design and development of new ods for dialogue summarization instead of directly applying current document summarization models
there has been some recent research on sation summarization such as directly deploying existing document summarization models gliwa et al
and exploring multi sentence sion shang et al
however most of them have nt utilized specic conversational structures which refer to the way utterances are organized in order to make the conversation meaningful able and understandable sacks et al
in alogues a key factor that differentiates dialogues from structured documents
as a way of using language socially of doing things with words gether with other persons the conversation has its own dynamic structures that organize utterances in certain orders to make the conversation meaningful enjoyable and understandable sacks et al

although there are a few exceptions such as ing topic segmentation liu et al
li et al
dialogue acts goo and chen or key point sequence liu et al
they either need t c o l c
s c v
v i x r a conversation topic view stage view james hey i have been thinking about you hannah oh that s nice james what are you up to hannah james i m about to sleep i miss u
i was hoping to see you hannah have to get up early for work tomorrow greetings today s plan openings intention james what about tomorrow plan for tomorrow hannah to be honest i have plans for tomorrow evening james oh ok
what about sat then hannah yeah
sure i am available on sat i ll pick you up at james hannah sounds good
see you then
plan for saturday discussion pick up time conclusion summary james misses hannah
they agree for james to pick hannah up on saturday at
table example conversation from samsum gliwa et al
with its topic view and stage view extracted by our methods and the human annotated summary
extensive expert annotations of discourse and chen liu et al
or only code conversations based on their topics liu et al
which fails to capture rich conversation structures in dialogues
even one single conversation can be viewed from different perspectives resulting in multiple conversational or discourse patterns
for instance in table based on what topics were discussed topic view galley et al
liu et al
li et al
it can be segmented into greetings today s plan plan for tomorrow plan for saturday and pick up time from a conversation progression perspective stage view ritter et al
paul althoff et al
the same dialogue can be categorized into openings intention discussion and conclusion
from a coarse perspective global view conversations can be treated as a whole or each utterance can serve as one segment discrete view
models that only utilized a xed topic view of the conversation joty et al
liu et al
may fail to capture its comprehensive and nuanced conversational structures and any amount of information loss introduced by the conversation encoder may lead to larger error cascade in the decoding stage
to ll these gaps we propose to combine those multiple diverse views of tions in order to generate more precise summaries
to sum up our contributions are we propose to utilize rich conversational structures i
e
tured views topic view and stage view and the generic views global view and discrete view for abstractive conversation summarization
we sign a multi view sequence to sequence model that consists of a conversation encoder to encode ferent views and a multi view decoder with view attention to generate dialogue summaries
we perform experiments on a large scale sation summarization dataset samsum gliwa et al
and demonstrate the effectiveness of our proposed methods
we conduct thorough error analyses and discuss specic challenges that current approaches faced with this task
related work document summarization document rization has received extensive research attention especially for abstractive summarization
for instance rush et al
introduced to use sequence to sequence models for abstractive text summarization
see et al
proposed a pointer generator network to allow copying words from the source text to handle the oov issue and avoid generating repeated content
paulus et al
chen and bansal further utilized reinforcement learning to select the correct content needed by summarization
large scale pre trained language models liu and lapata raffel et al
lewis et al
have also been duced to further improve the summarization mance
other line of work explored long document summarization by utilizing discourse structures in text cohan et al
introducing hierarchical models fabbri et al
or modifying tion mechanisms beltagy et al

there are also recent studies looking at the faithfulness in figure model architecture
different views of conversations are rst extracted automatically and then encoded through the conversation encoder a and combined in the multi view decoder to generate summaries b
in the conversation encoder each view consists of blocks is encoded separately and the block s representations si are encoded through lstm to represent the view
in the multi view decoder the model decides attention weights over different views and then attend to each token in different views through the multi view attention
document summarization cao et al
zhu et al
in order to enhance the information consistency between summaries and the input
including topic segments conversational stages alogue overview and utterances to design a view model for dialogue summarization
dialogue summarization when it comes to the summarization of dialogues shang et al
proposed a simple multi sentence compression technique to summarize meetings
zhao et al
zhu et al
introduced turn based hierarchical models that encoded each turn of terance rst and then used the aggregated sentation to generate summaries
a few studies have also paid attention to utilizing conversational analysis for generating dialogue summaries such as leveraging dialogue acts goo and chen key point sequence liu et al
or topics liu et al
li et al

however they either needed a large amount of human annotation for dialogue acts key points or visual focus goo and chen liu et al
li et al
or only utilized topical information in conversations li et al
liu et al

these prior work also largely ignored diverse conversational structures in dialogues for instance reply relations among participants mayeld et al
zhu et al
dialogue acts ritter et al
paul and conversation stages thoff et al

models that only utilized a xed topic view of the conversation galley et al
joty et al
may fail to capture its sive and nuanced conversational structures and any amount of information loss introduced by the versation encoder may lead to larger error cascade in the decoding stage
to ll these gaps we pose to leverage diverse conversational structures method conversations can be interpreted from different views and every single view enables the model to focus a specic aspect of the conversation
to take advantages of those rich conversation views we design a multi view sequence to sequence model see figure that rstly extracts different views of conversations section
and then encodes them to generate summaries section


conversation view extraction conversation summarization models may easily stray among all sorts of information across ous speakers and utterances especially when versations become long
naturally if informative structures in the form of small blocks can be plicitly extracted from long conversations models may be able to understand them better in a more ganized way
thus we rst extract different views of structures from conversations
topic view although conversations are often less structured than documents they are mostly organized around topics in a coarse grained ture honneth et al

for instance a phone chat could possess a pattern of greetings invitation party details rejection from a ical perspective
such explicit view and topic ow could help models interpret conversations more cisely and generate summaries that cover important topics
here we combine the classic topic segment stage interpretation top freq words openings intentions discussions conclusions hey hi good yeah going time need like think get want really will know time come tomorrow meet thanks ok see great thank sure table the top frequent words appearing in each stage and the interpretations for different stages
versation followed by discussions of the details and nally conclude with certain endings
table shows an example of the stage view
global view and discrete view in addition to the aforementioned two structured views tions can also be naturally viewed from a relatively coarse perspective i
e
a global view that nates all utterances into one giant block gliwa et al
and a discrete view that separates each utterance into a distinct block liu and chen gliwa et al


multi view sequence to sequence model we extend generic sequence to sequence models to encode and combine different conversation views
to better utilize semantic information in recent trained models we implement our base encoders and decoders with a transformer based pre trained model bart lewis et al

note that our multi view sequence to sequence model is agnostic to bart with which it is initialized



xk


bk xk i j in a block bk n each token xk conversation encoder given a conversation der a specic view with n blocks ck bk j xk m j is rst encoded through the conversation encoder e e

bart encoder as shown in figure into hidden representations m j m j note that we add special tokens xk at the ning of each block and use these tokens tations to describe each block i
e
sk


hk


xk hk xk hk
to depict different views using hidden vectors we aggregate the information from all blocks in one conversation through lstm layers hochreiter and schmidhuber j sk j n sk figure allowed state transitions for the hmm versation model
si are conversation stages oi are tences encoded representations
conversation stages evolve in an increasing order from to n
algorithm choi that segments sations based on inter sentence similarities with recent advanced sentence representations bert reimers and gurevych to extract the topic view
specically each utterance ui in a conversation c


um is rst encoded into hidden vectors via sentence bert
then the conversation c is divided into blocks ctopic


bn through where bi is one block that contains several consecutive ances such as the topic view described in table
stage view as a way of doing things with words socially together with other people conversation organizes utterances in certain orders to make it meaningful enjoyable and understandable
sacks et al
althoff et al
for example seling conversations are found to follow a common pattern of introductions problem exploration problem solving wrap up althoff et al

such conversation stage view provides high level sketches about the functions or goals of different parts in conversations which could help models focus on the stages with key information
we follow althoff et al
to extract stages through a hidden markov model hmm
we pose a xed ordering on the stages and only allow transitions from the current stage to the next one
the observations in the hmm model are the coded representations hi from sentence bert
we set the number of hidden stages as
similar to the topic view extraction we segment the sations into blocks cstage


bn where si is one block that contains several consecutive utterances
we interpret the inferred stages itatively and further visualize the top frequent words appearing in each stage in table
we found that conversations around daily chats usually start with openings introduce the goals focus of the we use the last hidden state sk current view k denoted as vk
n to represent the experiments
dataset and baselines multi view decoder different views could vide different types of conversational aspects for models to learn and further determine which set of utterances should deserve more attention in der to generate better dialogue summaries
as a result the ability to strategically combine ent views is essential
to this end we propose a transformer based multi view decoder to integrate encoded representations from different views and generate summaries as shown in figure
the input to the decoder contains previously generated tokens



via our multi view decoder d the l th token is predicted via





p l c here wp is a parameter to be learned
different from generic transformer decoder we introduce a multi view attention layer in each former block
multi view attention layer rst cides the importance k of each view vk through uk tanh w vk exp i i exp k where v is a randomly initialized context vector w and b are parameters
to avoid the attention weights being too similar to each other as views are actually encoded from a similar context we utilize a sharpening function over k with a temperature i
when t the attention t k weights will behave like a one hot vector
i t t then the multi head attention is performed over conversation tokens hk i from different views and form ak separately
the attended results are further combined based on the view attention weights k and continue forward passing training we minimize the cross entropy loss ing training l log p l c specically we apply the teacher forcing strategy at training time the inputs are previous tokens from the ground truth at test time the inputs are ous tokens predicted by the decoder
we evaluate our model on a large scale dialogue summary dataset samsum gliwa et al
that has dialogues with human written maries
the data statistics are shown in table
samsum contains messenger like conversations about daily topics such as chit chats arranging meetings discussing events
we compare our multi view sequence to sequence model view bart with several baseline models pointer generator see et al
ing gliwa et al
we added separators between each utterance discrete view and used it as input for pointer generator model
dynamicconv news wu et al
we followed gliwa et al
to use to initialize token embeddings ford et al

we also added news marization corpus cnn dm nallapati et al
as extra training data
fast abs rl enhanced chen and bansal rst selects salient sentences and then rewrites them abstractively via sentence level policy gradient methods
we combined it with the global view gliwa et al

bart generic views lewis et al
utilized bart a denoising autoencoder for pretraining sequence to sequence models gether with generic views global view and discrete view
we used the bart large model with its default settings

model we loaded the pre trained bert base nli stsb for sentence bert to get representations for each utterance
for extracting the topic view via we set the window size and std coefcient
for extracting the stage view we set the number of hidden states in hmm
these hyper parameters were set with a grid search
the bart tured views stage and topic views used the same set of parameters as bart generic views
for
com pytorch fairseq details are shown in section a in the appendix

com sentence transformers conversations train dev test participants std


mean


interval mean


turns std


interval mean


reference length std


interval table samsum dataset statistics
interval denotes the minimum and maximum range
model views pointer generator dynamicconv fast abs rl enhanced dynamicconv news bart bart multi view bart discrete global global discrete discrete global stage topic global stage global topic topic stage p






r






f










p






r






f










rouge l p






r






f










table and rouge l scores for different models on the test set
results are averaged over three runs
meant our methods or utilized views introduced by us
multi view bart we experimented with ent view combinations the best generic view global view was combined with two structured views stage and topic view separately the best two structured views are also combined topic stage
the settings for bart encoder decoder kept identical as baselines
we used a one layer lstm for encoding sections
the learning rate for section encoder and multi view attention was set
the temperature t was

the beam search size during inference for all the models was

results figure relations between rouge scores and the number of participants turns in conversations
quantitative results we evaluated models with the standard metric rouge score with stemming lin and och and reported and rouge
results on the test set for different models were shown in table
pared to pointer generator using reinforcement learning to select important sentences rst fast abs rl enhanced slightly increased f scores
adding pre trained embeddings or extra documents training data to lightweight convolution models dynamicconv news lead to even ter rouge scores
when using pre trained former based model bart with generic views all rouge scores improved signicantly and bart we followed bart and used
com pltrdy rouge
note that different tools may ate different rouge scores
global outperformed bart discrete especially in terms of rouge l f scores
segmenting versations into blocks from structured views stage view and topic view further boosted the mance suggesting that our extracted conversation structures help conversational encoders to capture nuanced and informative aspects of dialogs
we did not see any performance boost when bining the generic global view with either topic or conversational stage views partially due to that the coarse granularity of global view does not ment structured views well
in contrast utilizing both structured views topic view stage view further increased rouge scores consistently cating the effectiveness of synthesizing informative conversation blocks introduced by both views
we visualized the attention weight distributions figure human evaluation results
the mean score for each model is also shown in the box plot
model analysis and discussion the highest human annotation scores signicantly higher via a student t test than either generic crete or global view or structured stage or topic view which further proved the effectiveness of combing different views
so far we have achieved a reasonable tion performance
to further study why dialog marization is challenging and how future research could advance this direction we take a closer look at this dialogue summarization dataset samsum model generation errors as well as certain lenges that existing approaches are struggling with

challenges in dialog summarization we conduct a thorough examination of the lenges in conversation summarization and nized them into categories as below
informal language use many conversations especially in online contexts such as ter reddit jackson and moulinier tain typos word abbreviations slang or cons emojis making it hard to be represented and summarized

multiple participants as shown in figure conversations with more speakers are harder to be summarized since it may require els to accurately differentiate both language styles and content from different speakers similar to the multiple characters issue in story summarization zhang et al


multiple turns similar to long document summarization xiao and carenini conversations with many utterances contain more information to be processed thus harder to be summarized

referral and coreference people usually fer to each other mention others names or use coreference in their messages which troduces extra difculty to dialogue rization also a challenge also exists in reading comprehension chen et al
and ment summarization falke et al


repetition and interruption information is generally scattered through the whole sation and speakers may interrupt each other for the stage view and topic view in our best model see appendix and found contributions of topic views are slightly more prominent compared to stage views
this also communicated that the two different structured views can complement each other well though sharing the same dialogue tent
note that the gains from multi view bart topic stage are mainly from the precision scores while recall scores are kept comparable gesting that our proposed model produced fewer irrelevant tokens while preserving necessary mation in its generated summary
impact of participants and turns we ized the impact of two essential components in conversations the number of participants and turns on rouge scores via our best performing model multi view bart with topic view stage view in figure
as the number of pants turns increases rouge scores decrease dicating that the difculty of conversation rization increased with more participants involved in conversations and more utterances
qualitative human evaluation we also ducted human annotations to evaluate the extracted dialogue summaries in addition to rouge scores
similar to gliwa et al
we asked human annotators on amazon mechanical turk to rate each summary randomly sampled summaries in total on the scale of where means that a summary was poor extracted irrelevant formation or did not make sense at all means it was understandable and gave a concise overview of the text and refers to that the summary only tracted only a part of relevant information or made some mistakes
the score for each summary was averaged among three different annotators
the intra class correlation was
indicating erate agreement koo and li
as shown in figure consistent with rouge scores in table our multi view model achieved
mturk
generic informal language multiple participants multiple turns referral coreference repetition interruption negations rhetorical role language change challenge l























table the breakdown of challenges in dialogue marization based on our analyses of sampled versations and the rouge scores per challenge errors l other missing information redundancy wrong references incorrect reasoning improper gendered pronouns

















table the common error types of our model pared to golden reference on sampled tions and the rouge scores per error type

error reconrm back channeling or repeat selves a unique discourse challenge for logue summarization
we examined summaries generated by our performing model compared to ground truth maries and observed several major error types
negations and rhetorical questions as a long standing problem in nlp eld li et al
negation related issues are even more frequent in conversations as there are more question answer exchanges between speakers

role and language change conversations usually involve more than one speaker and the role of a speaker may shift from a questioner to an answerer requiring the summarization model to dynamically deal with speaker roles and the associated language e

rst sonal pronouns we randomly sampled from our test set and classied them using the above lenge taxonomy
a conversation might have more than one category labels and if it had none of the aforementioned challenges we labeled it as generic
usually the one marked as generic were shorter or had a simple structure
table presents the percentage of each type of challenge and per category performances from our best model multi view bart with topic view stage view
we observed that referral erence and role language change were the two most frequent challenges that logue summarization task faced
as expected generic conversations were relatively easier marize
our best model performed relatively worse when it came to repetition interruption multiple turns and referral coreference ing for more intelligent summarization methods to tackle those challenges

missing information content mentioned in references is missing in generated summaries

redundancy content occurred in generated summaries was not mentioned by references

wrong references generated summaries contain information that is not faithful to the original dialogue and associate one s tions locations with a wrong speaker

incorrect reasoning generated summaries reasoned relations in dialogues incorrectly thus came to wrong conclusions

improper gendered pronouns summaries used improper gendered pronouns e

the misuse of gendered pronouns
we annotated the same set of randomly pled summaries via the above error type taxonomy
a summary might have more than one category labels and we categorized a summary as other if it did not belong to any error types
table presents the breakdown of error types and per category rouge scores
we found that i missing information was the most quent error type indicating that current tion models struggled with identifying key mation
incorrect reasoning had a percentage of with the worst despite of ing a minor type improper gendered pronouns seemed to severely decrease both and
the relatively low rouge scores associated with incorrect reasoning and wrong erences urged better summarization models in ing with faithfulness in dialogue summarization
full analyzed set of examples are shown in appendix
analysis for baselines are displayed in the appendix
tract different views
in the future we plan to tate some of the data explore supervised tation models li et al
and introduce more conversation structures like dialogue acts oya and carenini joty and hoque into tive dialogue summarization
acknowledgment we would like to thank the anonymous reviewers for their helpful comments and the members of georgia tech salt group for their feedback
we acknowledge the support of nvidia corporation with the donation of gpu used for this research
references tim althoff kevin clark and jure leskovec

large scale analysis of counseling conversations an application of natural language processing to mental health
transactions of the association for computational linguistics
iz beltagy matthew e
peters and arman cohan

longformer the long document transformer
ziqiang cao furu wei wenjie li and sujian li

faithful to the original fact aware neural tive summarization
in aaai
danqi chen jason bolton and christopher d
the ning

a thorough examination of cnn daily mail reading comprehension task
in proceedings of the annual meeting of the sociation for computational linguistics volume long papers pages berlin germany
association for computational linguistics
yen chun chen and mohit bansal

fast tive summarization with reinforce selected sentence rewriting
in proceedings of the annual ing of the association for computational tics volume long papers pages bourne australia
association for computational linguistics
freddy y
y
choi

advances in domain pendent linear text segmentation
in meeting of the north american chapter of the association for computational linguistics
arman cohan franck dernoncourt doo soon kim trung bui seokhwan kim walter chang and zli goharian

a discourse aware attention model for abstractive summarization of long in proceedings of the conference of ments
the north american chapter of the association for computational linguistics human language nologies volume short papers pages new orleans louisiana
association for tional linguistics
figure relations between difculties in tions and errors made by our model

relation between challenges and errors to gure out relations between challenges and rors made by our models i
e
how different types of errors correlate with different types of lenges we visualized the co occurrence heat map in figure
we found that our model generated good summary for generic simple conversations
ii all kinds of challenges had high correlations with or could lead to the missing information ror
wrong references were highly associated with referral coreference this was as expected since co references in conversations would rally increase the difculty for models to associate correct speakers with correct actions
iv high relations between role language change referral coreference and incorrect reasoning indicated that interactions between multiple participants with frequent co references might easily lead current summarization models to reason incorrectly
conclusion in this work we proposed a multi view to sequence model that leveraged multiple sational structures topic view and stage view and generic views global view and discrete view to generate summaries for conversations
in order to strategically combine these different views for better summary generations we propose a view sequence to sequence model
experiments conducted demonstrated the effectiveness of our proposed models in terms of both quantitative and qualitative evaluations
via thorough error ses we concluded a set of challenges that current models struggled with which can further tate future research on conversation summarization
due to the lack of annotations we only adopted simple unsupervised segmentation methods to alexander fabbri irene li tianwei she suyi li and dragomir radev

multi news a large scale multi document summarization dataset and tive hierarchical model
proceedings of the nual meeting of the association for computational linguistics
tobias falke christian m meyer and iryna gurevych

concept map based multi document rization using concept coreference resolution and global importance optimization
in proceedings of the eighth international joint conference on ral language processing volume long papers pages
michel galley kathleen r
mckeown eric lussier and hongyan jing

discourse mentation of multi party conversation
in ings of the annual meeting of the association for computational linguistics pages poro japan
association for computational tics
shen gao xiuying chen zhaochun ren dongyan zhao and rui yan

from standard rization to new tasks and beyond summarization with manifold information
bogdan gliwa iwona mochol maciej biesek and aleksander wawer

samsum corpus a human annotated dialogue dataset for abstractive summarization
in proceedings of the workshop on new frontiers in summarization pages hong kong china
association for computational linguistics
chih wen goo and yun nung chen

tive dialogue summarization with sentence gated modeling optimized by dialogue acts
ieee spoken language technology workshop slt
sepp hochreiter and jurgen schmidhuber

neural comput
long short term memory

axel honneth hans joas al

social action and human nature
cup archive
peter jackson and isabelle moulinier

natural language processing for online applications text retrieval extraction and categorization volume
john benjamins publishing
shaq joty giuseppe carenini gabriel murray and raymond t
ng

exploiting conversation structure in unsupervised topic segmentation for emails
in proceedings of the conference on empirical methods in natural language ing pages cambridge ma
association for computational linguistics
grant h kester

conversation pieces nity and communication in modern art
univ of ifornia press
terry k koo and mae y li

a guideline of selecting and reporting intraclass correlation cients for reliability research
journal of tic medicine
mike lewis yinhan liu naman goyal jan ghazvininejad abdelrahman mohamed omer levy ves stoyanov and luke zettlemoyer

bart denoising sequence to sequence pre training for natural language generation translation and comprehension
jing li aixin sun and shaq r joty

segbot a generic neural text segmentation model with pointer network
in ijcai pages
jiwei li xinlei chen eduard hovy and dan jurafsky

visualizing and understanding neural models in nlp
in proceedings of the conference of the north american chapter of the association for computational linguistics human language nologies pages san diego california
sociation for computational linguistics
manling li lingyu zhang heng ji and richard j
radke

keep meeting summaries on topic abstractive multi modal meeting summarization
in proceedings of the association for computational linguistics pages florence italy
association for tational linguistics
the annual meeting of chin yew lin and franz josef och

matic evaluation of machine translation quality ing longest common subsequence and skip bigram statistics
in proceedings of the annual ing on association for computational linguistics page
association for computational tics
chunyi liu peng wang jiang xu zang li and jieping ye

automatic dialogue summary generation for customer service
in proceedings of the acm sigkdd international conference on knowledge discovery data mining page new york ny usa
association for computing machinery
peter j
liu mohammad saleh etienne pot ben goodrich ryan sepassi lukasz kaiser and noam shazeer

generating wikipedia by ing long sequences
in international conference on learning representations
shaq joty and enamul hoque

speech act eling of written asynchronous conversations with task specic embeddings and conditional structured in proceedings of the annual models
ing of the association for computational linguistics volume long papers pages
yang liu and mirella lapata

text tion with pretrained encoders
proceedings of the conference on empirical methods in ral language processing and the international joint conference on natural language processing emnlp ijcnlp
zhengyuan liu and nancy chen

reading turn by turn hierarchical attention architecture for ken dialogue comprehension
in proceedings of the annual meeting of the association for putational linguistics pages florence italy
association for computational linguistics
nils reimers and iryna gurevych

bert sentence embeddings using siamese networks
proceedings of the conference on empirical methods in natural language processing and the international joint conference on ral language processing emnlp ijcnlp
zhengyuan liu angela ng sheldon lee ai ti aw and nancy f
chen

topic aware generator networks for summarizing spoken sations
ieee automatic speech recognition and understanding workshop asru
elijah mayeld david adamson and carolyn stein rose

hierarchical conversation ture prediction in multi party chat
in proceedings of the annual meeting of the special interest group on discourse and dialogue pages seoul south korea
association for computational linguistics
ramesh nallapati bowen zhou cicero dos santos c aglar and bing xiang

tive text summarization using sequence to sequence in proceedings of the rnns and beyond
signll conference on computational natural guage learning pages berlin germany
association for computational linguistics
nikola i
nikolov michael pfeiffer and richard h
r
hahnloser

data driven summarization of entic articles
corr

tatsuro oya and giuseppe carenini

extractive summarization and dialogue act modeling on email threads an integrated probabilistic approach
in proceedings of the annual meeting of the cial interest group on discourse and dialogue dial pages
michael j
paul

mixed membership markov models for unsupervised conversation modeling
in proceedings of the joint conference on ical methods in natural language processing and computational natural language learning pages jeju island korea
association for tational linguistics
romain paulus caiming xiong and richard socher

a deep reinforced model for abstractive marization
in international conference on ing representations
alec radford jeffrey wu rewon child david luan dario amodei and ilya sutskever

language models are unsupervised multitask learners
openai blog
alan ritter colin cherry and bill dolan

pervised modeling of twitter conversations
in man language technologies the annual ference of the north american chapter of the ation for computational linguistics pages los angeles california
association for tional linguistics
alexander m
rush sumit chopra and jason weston

a neural attention model for abstractive in proceedings of the tence summarization
conference on empirical methods in natural guage processing pages lisbon portugal
association for computational linguistics
harvey sacks emanuel a schegloff and gail son

a simplest systematics for the tion of turn taking for conversation
in studies in the organization of conversational interaction pages
elsevier
abigail see peter j
liu and christopher d
manning

get to the point summarization with generator networks
proceedings of the annual meeting of the association for computational guistics volume long papers
guokan shang wensi ding zekun zhang toine tixier polykarpos meladianos michalis giannis and jean pierre

vised abstractive meeting summarization with sentence compression and budgeted submodular the maximization
nual meeting of the association for computational linguistics volume long papers pages melbourne australia
association for tational linguistics
in proceedings of felix wu angela fan alexei baevski yann dauphin and michael auli

pay less attention with in lightweight and dynamic convolutions
tional conference on learning representations
wen xiao and giuseppe carenini

extractive summarization of long documents by combining global and local context
in emnlp ijcnlp
weiwei zhang jackie chi kit cheung and joel oren

generating character descriptions for in proceedings of matic summarization of ction
the aaai conference on articial intelligence ume pages
colin raffel noam shazeer adam roberts katherine lee sharan narang michael matena yanqi zhou wei li and peter j
liu

exploring the limits of transfer learning with a unied text to text former
zhou zhao haojie pan changjie fan yan liu lin li min yang and deng cai

tive meeting summarization via hierarchical tive segmental network learning
in the world wide web conference www page new york ny usa
association for computing ery
chenguang zhu william hinthorn ruochen xu qingkai zeng michael zeng xuedong huang and meng jiang

boosting factual correctness of abstractive summarization
chenguang zhu ruochen xu michael zeng and dong huang

end to end abstractive rization for meetings
henghui zhu feng nan zhiguo wang ramesh pati and bing xiang

who did they respond to conversation structure modeling using masked hierarchical transformer
a model settings we load the pre trained bert base nli stsb for sentence bert to get representations for each utterance
when extracting the topic view we set the window size and std coefcient in
when extracting the stage view we set the number of hidden states in hmm
these parameters were set after a grid search with ing randomly sampled segmented results by human
the bart structured views stage and topic views followed the same parameters as bart generic views
for multi view bart we selected different views to combine generic view structured view best generic view global view was bined with two structured views stage and topic view structured view structured view best two single views are combined topic stage
the settings for bart encoder decoder kept the same as baseline
we used a one layer lstm for ing sections
the learning rate for section encoder and multi view attention was set
the perature t was

the beam search size during inference for all the models was
experiments were performed on two tesla gb memory
b view attention visualization we visualized the attention weights distribution for the stage view and topic view in our best multi view model to explore the importance of stage verses topic in figure
we found that the topic views were more prominent than the stage views tent with the performances of bart topic view and bart stage view
this indicated that having
com sentence transformers figure attention weights distribution for stage view and topic view in the multi view model
table a full index list of our samples
discourse structures about topics might be more portant while both topic and stage could improve the conversation summarization
this also municated that the two different structured views can complement each other well though sharing the same dialogue content
we displayed two examples in table with the golden references each single view s ated summaries and the combined views ated summaries
the combined view could balance the advantages of each single view and generated more precise summaries
and the attention weights the model learned were also consistent with single view s performances
c supplementary examples for model analysis and discussion for the analysis in the model analysis and cussion section in our paper we randomly sampled examples from the test set of the samsum reference stage topic james misses hannah
they agree for james to pick hannah up on saturday at
hannah has to get up early for tomorrow
james will pick her up at on saturday



james and hannah will see each other on saturday at



stage topic attention weight james will pick hannah up on saturday at pm





petra is very sleepy at work today andy nds the day boring and ezgi is working
petra needs to sleep because she s sleepy
ezgi is working



nobody is working at the ofce today
ezgi is working
petra is sleepy and wants to sleep



petra is sleepy and needs to sleep
ezgi is working at the ofce






table some generated summary examples compared to references
rouge l is shown after each summary and stage weight topic weight is displayed in the last row
errors discrete global stage topic multi view other missing information redundancy wrong references incorrect reasoning improper gendered pronouns table common error types of different models compared to golden reference on sampled conversations
dataset which can be downloaded here
table provides a full index list of the samples
table shows the error analysis for discrete bart global bart stage bart topic and bart multi view models
it can be observed that i without any explicit structures view and global view models generated summaries with more redundancies compared to golden ence summaries as models may easily lost focus on massive information once we introduced certain conversation structures such as topic view and stage view models behaved better in terms of redundancy and incorrect reasoning which cated that the structured views could help models to better understand the conversations our view models which combined both stage view and topic view made the least number of errors pared to all single view models suggesting the effectiveness of combining different views for versation summarization

org

