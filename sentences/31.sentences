journal of articial intelligence research submitted published revisiting centrality as relevance support sets and similarity as geometric proximity ricardo ribeiro instituto universitario de lisboa iscte iul av
das forcas armadas lisboa portugal inesc id lisboa rua alves redol lisboa portugal david martins de matos instituto superior tecnico universidade tecnica de lisboa av
rovisco pais lisboa portugal inesc id lisboa rua alves redol lisboa portugal ricardo
id
pt david
id
pt abstract in automatic summarization centrality as relevance means that the most important content of an information source or a collection of information sources corresponds to the most central passages considering a representation where such notion makes sense graph spatial

we assess the main paradigms and introduce a new centrality based relevance model for automatic summarization that relies on the use of support sets to better estimate the relevant content
geometric proximity is used to compute semantic relatedness
centrality relevance is determined by considering the whole input source and not only local information and by taking into account the existence of minor topics or lateral subjects in the information sources to be summarized
the method consists in creating for each passage of the input source a support set consisting only of the most semantically related passages
then the determination of the most relevant content is achieved by selecting the passages that occur in the largest number of support sets
this model produces extractive summaries that are generic and and independent
thorough automatic evaluation shows that the method achieves state of art performance both in written text and automatically transcribed speech summarization including when compared to considerably more complex approaches

introduction a summary conveys to the end user the most relevant content of one or more information sources in a concise and comprehensible manner
several diculties arise when addressing this problem but one of utmost importance is how to assess the signicant content
ally approaches vary in complexity if processing text or speech
while in text tion up to date systems make use of complex information such as syntactic vanderwende suzuki brockett nenkova semantic tucker sparck jones and discourse information harabagiu lacatusu uzeda pardo nunes either to assess relevance or reduce the length of the output common approaches to speech summarization try to cope with speech related issues by using speech specic information for example prosodic features maskey hirschberg or recognition condence scores zechner waibel or by improving the intelligibility of the output of an automatic speech ai access foundation
all rights reserved
ribeiro de matos recognition system by using related information ribeiro matos
in fact spoken language summarization is often considered a much harder task than text rization mckeown hirschberg galley maskey furui problems like speech recognition errors disuencies and the accurate identication of sentence boundaries not only increase the diculty in determining the salient information but also constrain the applicability of text summarization techniques to speech summarization although in the presence of planned speech as it partly happens in the broadcast news domain that bility is more feasible christensen gotoh kolluru renals
nonetheless shallow text summarization approaches such as latent semantic analysis lsa landauer foltz laham gong liu and maximal marginal relevance mmr carbonell goldstein seem to achieve performances comparable to the ones using specic speech related features penn zhu
following the determination of the relevant content the summary must be composed and presented to the user
if the identied content consists of passages found in the input source that are glued together to form the summary that summary is usually designated as extract on the other hand when the important content is devised as a series of concepts that are fused into a smaller set and then used to generate a new concise and tive text we are in the presence of an abstract
in between extraction and concept to text generation especially in text summarization text to text generation methods which rely on text rewriting paraphrasing of which sentence compression is a major tive are becoming an up to date subject cohn lapata
given the hardness of abstraction the bulk of the work in the area consists of extractive summarization
a common family of approaches to the identication of the relevant content is the centrality family
these methods base the detection of the most salient passages on the identication of the central passages of the input
one of the main tives of this family is centroid based summarization
centroid based methods build on the idea of a pseudo passage that represents the central topic of the input source the centroid selecting as passages to be included in the summary the ones that are close to the troid
pioneer work on multi document summarization by radev hatzivassiloglou and mckeown and radev jing and budzikowska creates clusters of documents by representing each document as a tf idf vector the centroid of each cluster is also dened as a tf idf vector with the coordinates corresponding to the weighted average of the tf idf values of the documents of the cluster nally sentences that contain the words of the troids are presumably the best representatives of the topic of the cluster thus being the best candidates to belonging to the summary
centroid another approach to centrality estimation is to compare each candidate passage to every other passage y and select the ones with higher scores the ones that are closer to every other passage
one simple way to do this is to represent passages as vectors using a weighting scheme like the aforementioned tf idf then passage similarity can be assessed using for instance the cosine assigning to each passage a centrality score as dened in eq

y n y revisiting centrality as relevance these scores are then used to create a sentence ranking sentences with highest scores are selected to create the summary
a major problem of this relevance paradigm is that by taking into account the entire input source in this manner either to estimate centroids or average distances of input source passages we may be selecting extracts that being central to the input source are however not the most relevant ones
in cognitive terms the information reduction techniques in the summarization process are quite close to the discourse understanding process niggemeyer which at a certain level works by applying rules that help uncovering the macrostructure of the discourse
one of these rules deletion is used to eliminate from the understanding process propositions that are not relevant to the interpretation of the subsequent ones
this means that it is common to nd in the input sources to be summarized lateral issues or considerations that are not relevant to devise the salient information discourse structure based summarization is based on the relevance of nuclear text segments marcu uzeda et al
and that may aect centrality based summarization methods by inducing inadequate centroids or decreasing the scores of more suitable sentences
as argued by previous work gong liu steyvers griths we also assume that input sources are mixtures of topics and propose to address that aspect using the input source itself as guidance
by associating to each passage of the input source a support set consisting only of the most semantically related passages in the same input source groups of related passages are uncovered each one constituting a latent topic the union of the supports sets whose intersection is not empty
in the creation of these support sets semantic relatedness is assessed by geometric proximity
moreover while similar work usually explores dierent weighting schemes to address specic issues of the task under research orasan pekar hasler murray renals ribeiro matos we explore dierent geometric distances as similarity measures analyzing their performance in context the impact of dierent metrics from both theoretical and empirical perspectives in a clustering setting was shown in aggarwal hinneburg keim
to build the summary we select the sentences that occur in the largest number of support sets hence the most central sentences without the problem that aects previous centrality based summarization
our method produces generic and domain independent summaries with low computational requirements
we test our approach both in speech and text data
in the pirical evaluation of the model over text data we used an experimental setup previously used in published work mihalcea tarau antiqueira oliveira jr
da fontoura costa nunes which enabled an informative comparison to the existing approaches
in what concerns the speech experiments we also used a corpus collected in previous work ribeiro matos as well as the published results
this allowed us to compare our model to state of the art work
the rest of this document is structured as follows in section we analyze tive models of both centrality as relevance approaches passage to centroid similarity based centrality and pair wise passage similarity based centrality section describes the support sets based relevance model the evaluation of the model is presented in section where we compare its performance against other centrality as relevance models and discuss the achieved results nal remarks conclude the document
ribeiro de matos
centrality as relevance there are two main approaches to centrality based summarization passage to centroid similarity and pair wise passage similarity

passage to centroid similarity based centrality in centroid based summarization passage centrality is dened by the similarity between the passage and a pseudo passage that considering a geometrical representation of the input source is the center of the space dened by the passages of the input source the centroid
the work in multi document summarization by radev et al
and radev jing stys and tam and the work developed by lin and hovy are examples of this approach
radev et al
present a centroid based multi document summarizer mead that has as input a cluster of documents
associated to each cluster of documents is a centroid
documents are represented by vectors of tf idf weights and the centroid of each cluster consists of a vector which coordinates are the weighted averages of the tf idf values of the documents of the cluster above a pre dened threshold
thus the centroid of a cluster of documents is in this case a pseudo document composed by the terms that are statistically relevant
given a cluster of documents segmented into sentences is


sn a centroid c and a compression rate summarization is done by selecting the appropriate number according to the compression rate of the sentences with the highest scores assigned by a linear function eq
of the following features centroid value ci position value pi and rst sentence overlap value fi
wcci wppi wf fi i the centroid value dened as ci ct i establishes that the sentences closer to the centroid the ones that contain more terms t from the centroid have higher scores
tion value pi scores sentences according to their position in the encompassing document
finally rst sentence overlap value fi scores sentences according to their similarity to the rst sentence of the document
tsi lin and hovy designate the centroid as topic signature and dene it as a set of related terms t s topic


tt wt where ti represents a term related to the topic topic and wi is an associated weight that represents the degree of correlation of ti to the topic
topic signatures are computed from a corpus of documents previously classied as relevant or non relevant for a given topic using the log likelihood ratio based quantity
this quantity due to its asymptotic relation to the distribution as well as the adequacy of the log likelihood ratio to sparse data is used to rank the terms that will dene the signature and to select a cut o value that will establish the number of terms in the signature
summarization is carried out by ranking the sentences according to the topic signature score and selecting the top ranked ones
the topic signature score tss is computed in a similar manner to mead s centroid value given an input source is


pn where pi


tm the most relevant passages are the ones with more words from the topic eq

revisiting centrality as relevance wj with wj weight of tj as dened in a topic signature m
pair wise passage similarity based centrality in pair wise passage similarity based summarization passage centrality is dened by the similarity between each passage and every other passage
the work presented by erkan and radev as well as the work developed by mihalcea and tarau are examples of this approach
erkan and radev propose three graph based approaches to pair wise passage similarity based summarization with similar performance degree centrality lexrank and continuous lexrank
degree centrality is based on the degree of a vertex
pair wise tence similarity is used to build a graph representation of the input source vertices are sentences and edges connect vertices which corresponding sentences are similar above a given threshold
sentences similar to a large number of other sentences are considered the most central relevant ones
degree centrality is similar to the model we propose
ever in the model we propose we introduce the concept of support set to allow the use of a dierent threshold for each sentence
this improves the representation of each sentence leading to the creation of better summaries
lexrank based on google s pagerank brin page builds on degree centrality degree by making the centrality of a sentence s be inuenced by similar sentences the adjacent ones in the graph representation eq

the ranking model is similar to pagerank except in what concerns the similarity adjacency graph that in this case is undirected eq
d is a damping factor and n the number of sentences
d n continuous lexrank is a weighted version of lexrank it uses eq
instead of eq

t t mihalcea and tarau in addition to google s pagerank also explore the hits algorithm kleinberg to perform graph based extractive text summarization again documents are represented as networks of sentences and these networks are used to globally determine the importance of each sentence
as it happens in the models proposed by erkan and radev sentences are vertices v and edges between vertices are established by passage similarity
the textrank mihalcea tarau the model based on pagerank was designated and the main contribution formalization is similar to continuous lexrank see eq
although mihalcea and tarau also explore directed graphs in the ribeiro de matos representation of the
for summarization the best results were obtained using a backward directed graph the orientation of the edges from a vertex representing a sentence is to vertices representing previous sentences in the input source
t d t vs vu passage similarity is based on content and is dened in eq

given two sets p


pn and q


qn each corresponding to a passage similarity consists in the cardinality of the intersection over the sum of the logarithms of the cardinality of each set
vq q t p t a similar graph based approach is described by antiqueira et al

this work uses complex networks to perform extractive text summarization
documents are also represented as networks where the sentences are the nodes and the connections between nodes are established between sentences sharing common meaningful nouns

beyond automatic summarization apart from summarization and considering that pagerank and hits stem from the area of information retrieval centrality based methods similar to the ones previously described have been successfully applied to re rank sets of documents returned by retrieval methods
kurland and lee present a set of graph based algorithms named inux that are similar to our model to reorder a previously retrieved collection of documents c
the method starts by dening a k neighbor k nn graph over the initial collection based on generation links dened as in eq
kl kullback leibler divergence m le maximum likelihood estimate smoothing parameter of a dirichlet smoothed version of p and s documents
exp kl pm le s centrality is determined as dened in eq

edges can be weighted weight given by s or not weight is
edges corresponding to generation probabilities below the k highest ones are not considered
oc
in a language independent algorithm for single and multiple document summarization mihalcea tarau the weighted pagerank equation has a minor dierence from the one in textrank bringing order into texts mihalcea tarau
the latter presents the correct equation

although both lexrank and textrank are based on pagerank dierent equations are used in their formalization
the equation used in textrank formalization is the same of pagerank original tion however pagerank authors observe that the pageranks form a probability distribution over web pages so the sum of all web pages pageranks will be one
this indicates the need of the normalization factor that is observed in lexrank formalization and currently assumed to be the correct pagerank formalization

the metric proposed by mihalcea and tarau has an unresolved issue the denominator is when comparing two equal sentences with length one something that can happen when processing speech transcriptions
instead the jaccard similarity coecient could be used
revisiting centrality as relevance there are also recursive versions of this centrality model which are similar to ank lexrank and continuous lexrank

support sets and geometric proximity in this work we hypothesize that input sources to be summarized comprehend dierent topics lateral issues beyond the main topic and model this idea by dening a support set based on semantic relatedness for every passage in the input source
semantic relatedness is estimated within the geometric framework where we explore several distance metrics to compute proximity
the most relevant content is determined by computing the most central passages given the collection of support sets
the proposed model estimates the most salient passages of an input source based exclusively on information drawn from the used input source

model the leading concept in our model is the concept of support set the rst step of our method to assess the relevant content is to create a support set for each passage of the input source by computing the similarity between each passage and the remaining ones selecting the closest passages to belong to the support set
the most relevant passages are the ones that occur in the largest number of support sets
given a segmented information source i


pn support sets si associated with each passage pi are dened as indicated in eq
sim is a similarity function and i is a threshold
si i pi i s pi the most relevant segments are given by selecting the passages that satisfy eq

arg max sn a major dierence from previous centrality models and the main reason to introduce the support sets is that by allowing dierent thresholds to each set i we let centrality be inuenced by the latent topics that emerge from the groups of related passages
in the degenerate case where all i are equal we fall into the degree centrality model proposed by erkan and radev
but using for instance a nave approach of having dynamic thresholds i set by limiting the cardinality of the support sets a k nn approach ity is changed because each support set has only the most semantically related passages of each passage
from a graph theory perspective this means that the underlying tion is not undirected and the support set can be interpreted as the passages recommended by the passage associated to the support set
this contrasts with both lexrank models which are based on undirected graphs
on the other hand the models proposed by halcea and tarau are closer to our work in the sense that they explore directed graphs although only in a simple way graphs can only be directed forward or backward
nonetheless semantic relatedness content overlap and centrality assessment performed by the graph ranking algorithms hits and pagerank is quite dierent from our proposal
in what concerns the work of kurland and lee which considering this k nn ribeiro de matos approach to the denition of the support set size is the most similar to our ideias although not addressing automatic summarization the neighborhood denition strategy is ent than ours kurland and lee base neighborhood denition on generation probabilities eq
while we explore geometric proximity
nevertheless from the perspective of our model the nn approach to support set denition is only a possible strategy others can be used our model can be seen as a generalization of both k nn and nn approaches since what we propose is the use of dierentiated thresholds i for each support set eq


semantic space we represent the input source i in a term by passages matrix a where each matrix element aij pj is a function that relates the occurrences of each term ti within each passage pj t is the number of dierent terms n is the number of passages
in what concerns the denition of the weighting function pj several term weighting schemes have been explored in the literature for the analysis of the impact of dierent weighting schemes on either text or speech summarization see the work of orasan et al
and murray and renals or ribeiro and matos respectively
since the exact nature of the weighting function although relevant is not central to our work we opted for normalized frequency for simplicity as dened in eq
where ni j is the number of occurrences of term ti in passage pj
nevertheless this is in line with the work of sahlgren that shows that in several tasks concerning term semantic relatedness one of the most eective weighting schemes for small contexts is the binary term weighting scheme eq
alongside raw or dampened counts that is weighting schemes based on the frequency that do not use global weights note also that in such small contexts most of the words have frequency which normalized or not is similar to the binary weighting scheme
a








at n pj tfi ni j nk j pj if ti pj if ti pj
semantic relatedness as indicated by sahlgren the meanings are locations metaphor is completely ous without the similarity is proximity metaphor
in that sense we explore the prevalent distance measures found in the literature based on the general minkowski distance eq

y n n revisiting centrality as relevance semantic relatedness is computed using the manhattan distance n eq
the euclidean distance n eq
the chebyshev distance n eq
and fractional distance metrics we experimented with n
n
n
and n

note that when n eq
does not represent a metric since the triangle inequality does not hold koosis page
in this case it is common to use the variation dened in eq

moreover we also experiment with the general minkowski equation using the tuple sion as n
distn y n n y y xi n n y lim n n n max i the cosine similarity eq
since it is one of the most used similarity metrics especially when using spatial metaphors for computing semantic relatedness was also part of our experiments
y y xiyi i i grounding semantic relatedness on geometric proximity enables a solid analysis of the various similarity metrics
for instance when using the euclidean distance eq
ferences between tuple coordinate values less than make passages closer while values greater than make passages more distant chebyshev s distance eq
only takes into account one coordinate the one with the greatest dierence between the two passages and the manhattan distance eq
considers all coordinates evenly
in the cosine similarity eq
tuples representing passages are vectors and the angle they form establishes their relatedness
in contrast mihalcea and tarau and antiqueira et al
dene passage similarity as content overlap
figure n ranges from
with an almost perceptible graphical representation to n a square shows how the unit circle is aected by the several geometric distances manhattan n and euclidean n are highlighted
although geometric proximity enables a solid analysis of the eects of using a specic metric it mainly relies on lexical overlap
other metrics could be used although the costs in terms of the required resources would increase
examples are based vector space models of semantics turney pantel like lsa landauer et al
hyperspace analogue to language lund burgess atchley or random indexing kanerva kristoferson holst kanerva sahlgren or similarity metrics based on knowledge rich semantic resources such as wordnet fellbaum
ribeiro de matos figure unit circles using various fractional distance metrics n equals to


and
the manhattan distance n the euclidean distance n and the chebyshev s distance n

threshold estimation as previously mentioned a simple approach to threshold estimation is to dene a xed cardinality for all support sets a nn approach
this means that thresholds although unknown are dierent for each support set
a simple heuristic that allows to automatically set per passage thresholds is to select as members of the support set the passages which distance to the passage associated to the support set under construction is smaller than the average distance
in the next sections we explore several heuristics inspired by the nature of the problem that can be used as possibly better approaches to threshold estimation
however this subject merits further study


heuristics based on distance progression analysis one possible approach is to analyze the progression of the distance values between each passage and the remaining ones in the creation of the respective support set
this type of heuristics uses a sorted permutation n of the distances of the passages sk to the passage pi corresponding to the support set under construction with pi k n and n the number of passages
di











revisiting centrality as relevance we explore three approaches a standard deviation based approach where i is given by eq
with a parameter that controls the width of interval around the average distance in relation to the standard deviation an approach based on the diminishing dierences between consecutive distances such that k is the largest one that j and an approach based on the l average dierence between consecutive distances k k n where i di di di n di k n l n
where i such that k is the largest one that di i i i with i n n k and i k n n

heuristics based on passage order the estimation of specic thresholds aims at dening support sets containing the most important passages to the passage under analysis
in that sense in this set of heuristics we explore the structure of the input source to partition the candidate passages to be in the support set in two subsets the ones closer to the passage associated with the support set under construction and the ones further appart
these heuristics use a permutation n of the distances of the passages sk to the passage pi related to the support set under construction with pi n corresponding to the order of occurrence of passages sk in the input source
algorithm describes the generic procedure


heuristics based on weighted graph creation techniques there are several ways to dene a weighted graph given a dataset
the main ideia is that similar nodes must be connected by an edge with a large weight
in this set of heuristics we explore two weight functions zhu eqs
and considering that if the returned value is above a given threshold the passage sk belongs to the support set of passage pi with di pi
k min k j n n
integrating additional information as argued by wan yang and xiao and ribeiro and matos the use of additional related information helps to build a better understanding of a given subject thus improving summarization performance
wan et al
propose a graph based ranking model that uses several documents about a given topic to summarize a single one of them
ribeiro and matos using the lsa framework present a method that combines the input source ribeiro de matos input two values and each a representative of a subset and the set of the passages sk and corresponding distances under construction to the passage associated with the support set output the support set of the passage under analysis for k to n do if dik dik then dik sik dik sik else end end l arg if sl then return else end return algorithm generic passage order based heuristic
consisting of a spoken document with related textual background information to cope with the diculties of speech to text summarization
the model we propose may be easily expanded to integrate additional information
by using both an information source i


pn and a source for additional relevant information b we may redene eq
as shown in eq
to integrate the additional information
si s i b pi i s pi matrix a from eq
should be redened as indicated in eq
where aidk the weight of term ti i t t is the number terms in passage pdk the number of documents used as additional information with dk dk s of document dk and ainl l s are the elements associated with the input source to be summarized
represents k d d is j j j a at s








at s





s





at dd


at dd s at


at ns given the new denition of support set and a common representation for the additional information the most relevant content is still assessed using eq

the same line of thought can be applied to extend the model to multi document marization
revisiting centrality as relevance
evaluation summary evaluation is a research subject by itself
several evaluation models have been put forward in the last decade beyond the long established precision and recall mostly useful when evaluating extractive summarization using also extractive summaries as models ature is lled with metrics some are automatic others manual like relative utility radev et al
radev tam summaccy hori hori furui rouge lin vert oliveira torrens cidral schossland bittencourt or the mid method nenkova passonneau mckeown
for a more comprehensive analysis of the evaluation eld see the work by nenkova and nenkova et al

despite the number of approaches to summary evaluation the most widely used metric is still rouge and is the one we use in our study
we chose rouge not only owing to its wide adoption but also because one of the data sets used in our evaluation has been used in published studies allowing us to easily compare the performance of our model with other known systems
rouge n summaries summaries gramn gramn s namely we use the score known to correlate well with human judgment lin
rouge n is dened in eq

moreover we estimate condence intervals using non parametric bootstrap with resamplings mooney duval
since we are proposing a generic summarization model we conducted experiments both in text and speech data

experiment text

data in this section we describe the experiments performed and analyze the corresponding results when using as input source written text
the used corpus known as temario consists of newspaper articles in brazilian tuguese pardo rino
although our model is general and language independent this corpus was used in several published studies allowing us to perform an informed parison of our results
the articles in the corpus cover several domains such as world politics and foreign aairs
for each of the newspaper articles there is a reference human produced summary
the text was tokenized and punctuation removed maintaining sentence boundary information
table sumarizes the properties of this data set


evaluation setup to compare the performance of our model when the input is not aected by speech related phenomena we use previously published state of the art results for text summarization
however since there was no information available about any kind of preprocessing for the previous studies we could not guarantee a fair comparison of our results with the previous ones without the denition of an adequate methodology for the comparisons
the following systems were evaluated using the temario dataset ribeiro de matos average minimum maximum words news story ns ns sentence summary s s sentence sentences news story summary table corpus characterization
a set of graph based summarizers presented by mihalcea and tarau namely pagerank backward hitsa backward and hitsh forward supor leite rino pardo nunes a classier based system that uses features like the occurrence of proper nouns lexical chaining and an ontology two modied versions of mihalcea s pagerank undirected called textrank saurus and textrank stem presented by leite et al
and several complex networks summarizers proposed by antiqueira et al

considering the preprocessing step we applied to the corpus and the observed dierences in the published results we found it important to evaluate the systems under the same conditions
thus we implemented the following centrality models uniform inux corresponds to the non recursive unweighted version of the model proposed by kurland and lee for re ranking in document retrieval we experimented with several k in graph denition the sames used for support set dinality in the knn strategy and and present only the best results pagerank proposed by both mihalcea and tarau and erkan and radev passage similarity metrics dier and mihalcea and tarau also explore directed graphs degree centrality as proposed by erkan and radev we experimented with several thresholds ranging from
to
and show only the best results and baseline in which the ranking is dened by the order of the sentences in the news article with relevance decreasing from the begining to the end
table further discriminates pagerank based models
pagerank over a directed forward graph performs consistently worse mihalcea tarau than over undirected and directed backward graphs and it was not included in our trials
degree and continuous lexrank bound the performance of the lexrank model and are the ones we use in this revisiting centrality as relevance proposed model generic designation similarity metric continuous lexrank pagerank undirected cosine textrank undirected pagerank undirected content overlap content overlap textrank backward pagerank backward table models based on pagerank
evaluation
moreover to assess the inuence of the similarity metrics in these based centrality models we tested the best performing metric of our model the manhattan distance with the pagerank model
additionally given that the models proposed by erkan and radev use idf we present some results clearly identied using both weighting schemes using and not using idf
concerning summary size the number of words in the generated summaries directly depends on the number of words of the reference abstracts which consisted in compressing the input sources to of the original size


results table illustrates the comparison between the previously proposed models and our model
in this table our model is identied in boldface by the distance name and the conditions used by that particular instance
every time the best performance is achieved by an instance using supports sets whose cardinality is specied in absolute terms we also present the best performance using support sets whose cardinality is specied in relative terms of the input source
for the fractional metrics we also present the value of n in eq
if n or eq
if n
for the automatically set thresholds we identify which heuristic produced the best results using the following notation means the heuristic based on the average distance means heuristics based on the analysis of the distances progression with
corresponding to the one based on the standard deviation
corresponding to the one based on the diminishing dierences between consecutive distances and
corresponding to the one based on the average dierence between consecutive distances means heuristics based on passage order with
using as the minimum distance and as the average of the distances
using as the minimum distance and as the maximum distance and
using as the distance of the rst passage and the distance of the second passage according to the required permutation dened in section

means heuristics based on weighted graph creation techniques with
based on eq
and
based on eq

the best overall results were obtained by the support sets based centrality model using both the fractional with n
and using idf and the manhattan distance
the next best performing variants of our model were cosine minkowski n dened by the dimension of the semantic space and euclidean all over performing both textrank undirected and the uniform inux model
the best pagerank variant using a backward directed graph and the cosine similarity with idf achieved a performance similar to the cosine ssc idf and the minkowski ssc variants of our model
textrank undirected uniform inux and continuous lexrank idf obtained performances similar to the euclidean ssc systems condence interval ribeiro de matos fractional n
idf
manhattan ssc manhattan manhattan idf
cosine idf ssc pagerank backward cosine idf minkowski ssc minkowski
cosine idf manhattan
euclidean idf ssc textrank undirected uniform inux cosine continuous lexrank idf fractional n

fractional n
ssc pagerank backward cosine degree
idf textrank backward minkowski euclidean
cosine
fractional n
chebyshev
pagerank backward manhattan euclidean chebyshev ssc chebyshev continuous lexrank pagerank undirected manhattan baseline fractional n

degree
fractional n

fractional n

fractional n
fractional n
fractional n





















































































































table scores for the text experiment ssc stands for support set cardinality
revisiting centrality as relevance idf and the cosine variants
notice that although not exhaustively analyzing the eects of term weighting the use of idf clearly benets some metrics see for instance the cosine and fractional n
variants of our model the pagerank variants based on the cosine similarity and degree
it is relevant to note that our model which has low computational requirements achieves results comparable to graph based state of the art systems ceylan mihalcea ozertem lloret palomar antiqueira et al

notice that although the estimated condence intervals overlap the performance of the manhattan variant is signicantly better using the directional wilcoxon signed rank test with continuity correction than the ones of textrank undirected w
uniform inux w p
and also continuous lexrank w
p

the only variants of our model that perform below the baseline are the fractional variants with n
fractional distances with n as can be seen by the eect of the metric on the unit circle figure increase the distance between all passages negatively inuencing the construction of the support sets and consequently the estimation of relevant content
concerning the automatically set per passage thresholds it is possible to observe that the best overall performance was achieved by a metric fractional n
with idf using the heuristic based on the average dierence between consecutive distances
for cosine manhattan euclidean and minkowski variants the heuristic based on the average distance cosine and the heuristics based on passage order achieved results comparable to the best performing knn approaches
for chebyshev and fractional with n variants the best results were obtained using the heuristics based on the analysis of the progression of the distances
figure shows the improvements over the baseline and over the previous best performing system
it is possible to perceive that the greatest performance jumps are introduced by euclidean and euclidean
minkowski and the best performing hattan all instances of the support sets based relevance model
additionally it is important to notice that the improvement of cn voting over the baseline computed in the same ditions of cn voting is of only having a performance worse than the poorest textrank version which had an improvement over the baseline of

in what concerns the tic knowledge based systems and the enriched versions of textrank undirected we can not make an informed assessment of their performance since we can not substantiate the used baseline taken from the work of mihalcea and tarau
nonetheless using that baseline it is clear that linguistic information improves the performance of extractive summarizers beyond what we achieved with our model improvements over the baseline range from to

notice however that it would be possible to enrich our model with linguistic information in the same manner of textrank
regarding the eect of the similarity metric on the pagerank based systems it is ble to observe that pagerank undirected based on content overlap textrank undirected has a better performance than when similarity is based on a geometric metric either hattan or cosine continuous lexrank
however the same does not happen when ering the results obtained by the several variants of pagerank backward
although the use of content overlap in fact leads to a better performance than using a manhattan based
statistical tests were computed using r r development core team
ribeiro de matos figure analysis of the increase in performance of each model
similarity metric the use of the cosine similarity results in a performance comparable to the one of using the content overlap metric
the manhattan based similarity metric is dened in eq

y y
experiment speech in this section we describe the experiments performed and analyze the corresponding results when using as input source automatically transcribed speech









fractional n
idf
manhattan ssc manhattan manhattan idf
cosine ssc idf pagerank backward cosine idf minkowski ssc minkowski
cosine idf manhattan
euclidean ssc idf textrank undirected uniform influx cosine continuous lexrank idf fractional n

fractional n
ssc pagerank backward cosine degree
idf textrank backward minkowski euclidean
cosine
fractional
chebyshev
pagerank backward manhattan euclidean chebyshev ssc chebyshev continuous lexrank pagerank undirected manhattan improvement over the previous system improvement over the baseline revisiting centrality as relevance

data to evaluate our ideas in the speech processing setting we used the same data of ribeiro and matos the automatic transcriptions of broadcast news stories in european portuguese part of a news program
subject areas include society politics sports among others
table details the corpus composition
for each news story there is a human produced reference summary which is an abstract
the average word recognition error rate is
and automatic sentence segmentation attained a slot error rate ser commonly used to evaluate this kind of task of

as it is possible to observe in table it is important to distinguish between the notion of sentence in written text and that of sentence like unit su in speech data
note in particular the dierence in the average number of words per sentence in the summary versus the average number of words per su in the news story
according to liu shriberg stolcke hillard ostendorf and harper the concept of su is dierent from the concept of sentence in written text since although semantically complete sus can be smaller than a sentence
this is corroborated by the fact that it is possible to nd news stories with sus of length this corpus has sus of length
beyond the denition of su note that an ser of
is a high value currently the automatic punctuation module responsible for delimiting sus achieves an ser of
using prosodic information batista moniz trancoso meinedo mata mamede
average minimum maximum words news story ns ns su summary s s sentence sus setences summary news story table corpus characterization


evaluation setup regarding speech summarization even considering the diculties concerning the bility of text summarization methods to spoken documents shallow approaches like lsa or mmr seem to achieve performances comparable to the ones using specic speech related features penn zhu especially in unsupervised approaches
given the implemented models in this experiment we compare the support sets relevance model to the following systems an lsa baseline
the following graph based methods uniform inux kurland lee continuous lexrank and degree centrality erkan radev and textrank halcea tarau
ribeiro de matos the method proposed by ribeiro and matos which explores the use of from online additional related information less prone to speech related errors e

newspapers to improve speech summarization mixed source
two human summarizers extractive using as source the automatic speech tions of the news stories human extractive
before analyzing the results it is important to examine human performance
one of the relevant issues that should be assessed is the level of agreement between the two human summarizers this was accomplished using the kappa coecient carletta for which we obtained a value of
what is considered a fair to moderate good agreement landis kosh fleiss
concerning the selected sentences figure shows that human summarizer consistently selected the rst n sentences and that in choices there is also a noticeable preference for the rst sentences of each news story
figure human sentence selection patterns
to be able to perform a good assessment of the automatic models we conducted two experiments in the rst one the number of sus extracted to compose the automatic summaries was dened in accordance to the number of sentences of the reference human abstracts which consisted in compressing the input source to about of the original size in the second experiment the number of extracted sus of the automatic summaries was determined by the size of the shortest corresponding human extractive summary
notice that mixed source and human summaries are the same in both experiments


results table shows the scores obtained for both speech experiments
in this table it is possible to nd more than one instance of some models since sometimes the performing variant when using as summary size the size of the abstracts was dierent from the one using as summary size the size of the human extracts
remaining sentences number of times selected sentence position revisiting centrality as relevance systems using as summary size reference human abstracts shortest human extracts human extractive human extractive cosine idf
pagerank backward cosine textrank backward pagerank backward cosine idf first sentences pagerank backward manhattan chebyshev
chebyshev cosine
minkowski
cosine euclidean
mixed source cosine idf minkowski fractional n
idf
manhattan
fractional n
cosine fractional n

degree
fractional n
manhattan euclidean euclidean euclidean fractional n
fractional n
textrank undirected degree
idf uniform inux


































































































continuous lexrank idf fractional n
continuous lexrank pagerank undirected manhattan fractional n
fractional n







































































































































lsa baseline





table scores with condence intervals computed using bootstrap tics for the speech experiment ssc stands for support set cardinality sorted using the scores of the human abstracts
ribeiro de matos a rst observation concerns a particular aspect of corpus as it can be seen especially in the experiment using as reference size the size of the shortest human extracts both man and first sentences summarizers attained the same scores this does not happen in the experiment using the abstracts size only due to the fact that first sentences summaries are shorter adapted to the experiment required size than the ones of human which were not changed
in fact the summaries are equal which shows a consistent bias indicating that the most relevant sentences tend to occur in the beginning of the news stories
this bias although not surprising since the corpus is composed of broadcast news stories is also not that common as can be seen in previous work ribeiro matos lin yeh chen
second it is interesting to notice the performance of the pagerank based models while in text there is no observable trend concerning the tionality of the graph and both lexrank versions performed above the baseline in speech only the backward versions achieved a good performance the four undirected versions formed around the baseline with lexrank obtaining results below the lsa baseline with exception for the experiment using the extracts size and idf
from a models perspective and considering the performance of backward versions in both text and speech the use of backward directionality seems the main reason for the good performance in speech where input sources consist of transcriptions of broadcast news stories from a news program
in fact as mentioned before this kind of input source is usually short cf
table and the main information is given in the opening of the news story
this suggests that ality introduces position information in the model which is only relevant for specic types of input source this is also discussed in mihalcea tarau
moreover note that continuous lexrank performance was close to the lsa baseline which implies that the model is quite susceptible to the referred bias to the noisy input or to both
taking into consideration that the model is based on pair wise passage similarity and that one of the best performing support sets based instance was cosine the same similarity metric used by lexrank it seems that the model was not able to account for the structure of the put sources of this data set
in fact degree centrality also based on the cosine similarity performed better than all pagerank undirected models
the inux model performed close to degree centrality far from the best performing approaches which in this case suggests that the method for generating the graph the generation probabilities is aected by the noisy input especially when considering small contexts like passages
approaches based on generation probabilities seem more adequate to larger contexts such as documents kurland lee erkan
erkan mentions that results in query based marization using generation probabilities were worse than the ones obtained by lexrank in generic summarization
concerning the overall results performance varies according to the size of the maries
when using the abstracts size the best performing instance is cosine with idf using an heuristic based on the passage order when using the reference extracts size the best performance was achieved by the backward pagerank model followed by the shev variant also using an heuristic based on passage order and the same cosine variant
both variants achieved better results than textrank backward
given the success of the heuristic
in these experiments it seems that this heuristic may also be introducing position information in the model
although not achieving the best performance in the experiment using the extracts size there is no signicant dierence between the best revisiting centrality as relevance port sets based relevance model instance the chebyshev variant using an heuristic based on passage order and the ones achieved by human summarizers applying the directional wilcoxon signed rank test with continuity correction the test values when using the est human extracts size are w p

this means a state of the art performance in the experiment using the abstracts size and comparable to a human results similar to first sentence which is similar to human extractive when using the shortest human extracts size
in fact chebyshev to avoid the inuence of possible position information is also not signicantly dierent than human extractive w p

cosine with idf and using
has a better performance with statistical signicance than degree with
w
p
when using the abstracts size w p
when using the shortest human extracts size textrank undirected w
p
when using the abstracts size w p
when using the shortest human extracts size and uniform inux w p
when using the abstracts size w p
when using the shortest human extracts size using the same statistical test
the obtained results in both speech transcriptions and written text suggest that the model is robust being able to detect the most relevant content without specic information of where it should be found and performing well in the presence of noisy input
moreover cosine similarity seems to be a good metric to use in the proposed model performing among the top ranking variants in both written and spoken language
fractional variants with n were again the worst performing approaches we did not include values for the automatically set per passage thresholds in table since they were worse than the simple knn approach because their eect on the similarity assessment boosts the inuence of the recognition errors
on the other hand chebyshev seems more imune to that inuence the single use of the maximal dierence through all the dimensions makes it less prone to noise recognition errors
the same happens with the variant using the generic minkowski distance with n equal to the number of dimensions of the semantic space
figures and shows the performance variation introduced by the dierent approaches
notice that in the speech experiments performance increments are a magnitude higher when compared to the ones in written text
overall the chebyshev variant of the support sets based relevance model introduces the highest relative gains close to in the iment using the abstracts size close to in the experiment using the extracts size
in the experiment using the extracts size textrank undirected also achieves relative gains of near over the previous best performing system the lsa baseline
similar relative improvements are introduced by the human summarizers in the experiment using the stracts size
as expected increasing the size of the summaries increases the coverage of the human abstracts bottom of figure
further comparing our model to more complex not centrality based state of the art models like the one presented by lin et al
suggests that at least similar performance is attained the relative performance increment of our model over lexrank is of
and
both speech experiments whereas the relative gain of the best variant of the model proposed by lin et al
over lexrank is of

note that this can only be taken as indicative since an accurate comparison is not possible because data sets dier lin et al
do not explicit which variant of lexrank is used and do not address statistical signicance
ribeiro de matos figure analysis of the increase in performance of each model experiment using the abstracts size

inuence of the size of the support sets on the assessment of relevance we do not propose a method for determining an optimum size for the support sets
less we analyze the inuence of the support set size on the assessment of the relevant content both in text and speech
figure depicts the behavior of the model variants with a performance above the line over written text while figure illustrates the variants under the same conditions over human extractive human extractive cosine idf
pagerank backward cosine textrank backward pagerank backward cosine idf first sentences pagerank backward manhattan chebyshev
chebyshev cosine
minkowski
cosine euclidean
mixed source cosine idf minkowski fractional
idf
manhattan
fractional
cosine fractional

degree
fractional
manhattan euclidean euclidean euclidean fractional
fractional
textrank undirected degree
idf uniform influx lsa baseline continuous lexrank idf summary size determined by human abstracts improvement over the previous system improvement over the baseline revisiting centrality as relevance figure analysis of the increase in performance of each model experiment using the abstracts size
automatic speech transcriptions in this case error bars were omitted for clarity
we lyze the general performance of those variants considering as support set size the number of passages of the input source in increments
given the average size of an input source both in written text table and speech transcriptions table absolute cardinalities ssc ranging from to passages broadly cover possible sizes in the interval
a rst observation concerns the fact that varying the cardinality of the support sets when the input sources consist of written text has a smooth eect over the performance
in contrast when processing automatic this allows the analysis of generic tendencies
human extractive pagerank backward cosine idf human extractive first sentences pagerank backward manhattan pagerank backward cosine chebyshev
cosine idf
textrank backward chebyshev cosine idf fractional
idf
minkowski
cosine
cosine cosine euclidean
minkowski manhattan
fractional

manhattan fractional
euclidean fractional
euclidean fractional
mixed source degree
uniform influx fractional
euclidean degree
idf continuous lexrank idf textrank undirected summary size determined by shortest human extracts improvement over the previous system improvement over the baseline ribeiro de matos figure analysis of the impact of the cardinality of the support sets over text rization
y axes are scores and x axes are support sets cardinalities absolute and relative to the length of the input source in terms of passages
speech transcriptions it is possible to perceive several irregularities
these irregularities can have two dierent causes the intrinsic characteristics of speech transcriptions such as recognition errors sentence boundary detection errors and the type of discourse or the specicities of the data set in particular the global size of the corpus and the specic news story structure
however considering the performance of the metrics over both text and speech the irregularities seem to be mainly caused by the intrinsic properties of speech transcriptions and the specic structure of the news story








cosine







euclidean







chebyshev n







fractional








manhattan







minkowski variable n revisiting centrality as relevance figure analysis of the impact of the cardinality of the support sets over speech to text summarization
y axes are scores and x axes are support sets ities absolute and relative to the length of the input source in terms of passages
lines with square marks correspond to the experiment using as summary size the size of human abstracts lines with circle marks correspond to the experiment using as summary size the size of the shortest human extracts
horizontal lines correspond to baselines







chebyshev n






cosine






euclidean






fractional







manhattan






minkowski variable n ribeiro de matos concerning performance itself in written text the best performances are achieved using low cardinalities absolute cardinalities of or passages or of about of the passages of the input source
moreover an increase in the size of the support sets leads to a decay of the results except when using the cosine similarity
when processing automatic speech transcriptions it is dicult to nd a clear denition of when the best results are achieved
considering absolute cardinalities with the exception of the manhattan distance every variant has a peak when using support sets with passages
however it is not possible to extend such line of thought to relative sizes due to the previously referred irregularities
nonetheless higher cardinalities lead to worse results what is expected given the nature of the model again with exception of when using the cosine similarity
in addition note that increasing the size of the summaries improves the distinction from the baseline summaries based on the size of the shortest human extracts are longer than the ones based on the size of the human abstracts
this means that the model is robust to needs regarding summary size continuing to select good content even for larger summaries

conclusions the number of up to date examples of work on automatic summarization using based relevance models is signicant garg favre reidhammer hakkani tur antiqueira et al
ceylan et al
wan li xiao
in our work we assessed the main approaches of the centrality as relevance paradigm and introduced a new centrality based relevance model for automatic summarization
our model uses support sets to better characterize the information sources to be summarized leading to a better estimation of the relevant content
in fact we assume that input sources comprehend several topics that are uncovered by associating to each passage a support set composed by the most semantically related passages
building on the ideas of ruge


the model of semantic space in which the relative position of two terms determines the semantic similarity better ts the imagination of human intuition about semantic similarity


semantic relatedness was computed by geometric proximity
we explore several metrics and analyze their impact on the proposed model as well as to a certain extent on the related work
centrality relevance is determined by taking into account the whole input source and not only local information using the support sets based representation
moreover although not formally analyzed notice that the proposed model has low computational requirements
we conducted a thorough automatic evaluation experimenting our model both on ten text and transcribed speech summarization
the obtained results suggest that the model is robust being able to detect the most relevant content without specic information of where it should be found and performing well in the presence of noisy input such as automatic speech transcriptions
however it must be taken into consideration that the use of rouge in summary evaluation although generalized allowing to easily compare results and replicate experiments is not an ideal scenario and consequently results should be corroborated by a perceptual evaluation
the outcome of the performed trials show that the proposed model achieves state of the art performance in both text and speech rization including when compared to considerably more complex approaches
nonetheless we identied some limitations
first although grounding semantic similarity on geometric revisiting centrality as relevance proximity in the current experiments we rely mainly on lexical overlap
while maintaining the semantic approach the use of more complex methods turney pantel may prove the assessment of semantic similarity
second we did not address a specic procedure for estimating optimum thresholds leaving it for future research
nonetheless we explored several heuristics that achieved top ranking performance
moreover we carried out in this document an analysis that provides some clues for the adequate dimension of the support sets but a more analytical analysis should be performed
we would like to thank the anonymous reviewers for their insightful comments
this work was supported by fct inesc id multiannual funding through the piddac program funds
acknowledgments references aggarwal c
c
hinneburg a
keim d
a

on the surprising behavior of distance metrics in high dimensional space
in den bussche j
v
vianu v
eds
database theory icdt international conference london uk january proceedings vol
of lecture notes in computer science pp

springer
antiqueira l
oliveira jr
o
n
da fontoura costa l
nunes m
g
v

a complex network approach to text summarization
information sciences
batista f
moniz h
trancoso i
meinedo h
mata a
i
mamede n
j

extending the punctuation module for european portuguese
in proceedings of the annual conference of the international speech communication association terspeech pp

isca
brin s
page l

the anatomy of a large scale hypertextual web search engine
computer networks and isdn systems
carbonell j
goldstein j

the use of mmr diversity based reranking for reordering documents and producing summaries
in sigir proceedings of the annual international acm sigir conference on research and development in information retrieval pp

acm
carletta j

assessing agreement on classication tasks the kappa statistic
putational linguistics
ceylan h
mihalcea r
ozertem u
lloret e
palomar m

quantifying the limits and success of extractive summarization systems across domains
in human language technologies the annual conference of the north american chapter of the acl pp

association for computational linguistics
christensen h
gotoh y
kolluru b
renals s

are extractive text sation techniques portable to broadcast news
in proceedings of the ieee ribeiro de matos shop on automatic speech recognition and understanding asru pp

ieee
cohn t
lapata m

sentence compression as tree transduction
journal of articial intelligence research
oliveira p
c
f
torrens e
w
cidral a
schossland s
bittencourt e

evaluating summaries automatically a system proposal
in proceedings of the sixth international language resources and evaluation pp

elra
endres niggemeyer b

summarizing information
springer
erkan g

language model based document clustering using random walks
in proceedings of the human language technology conference of the north american chapter of the acl pp

association for computational linguistics
erkan g

using biased random walks for focused summarization
in proceedings of the document understanding conference
erkan g
radev d
r

lexrank graph based centrality as salience in text summarization
journal of articial intelligence research
fellbaum c
ed


wordnet an electronic lexical database
mit press
fleiss j
l

statistical methods for rates and proportions edition
john wiley
furui s

recent advances in automatic speech summarization
in proceedings of the conference on recherche dinformation assistee par ordinateur riao
centre hautes etudes internationales dinformatique documentaire
garg n
favre b
reidhammer k
hakkani tur d

clusterrank a graph based method for meeting summarization
in proceedings of the annual ence of the international speech communication association interspeech pp

isca
gong y
liu x

generic text summarization using relevance measure and tent semantic analysis
in sigir proceedings of the annual international acm sigir conference on research and development in information retrieval pp

acm
harabagiu s
lacatusu f

topic themes for multi document summarization
in sigir proceedings of the annual international acm sigir conference on research and development in information retrieval pp

acm
hori c
hori t
furui s

evaluation method for automatic speech marization
in proceedings of the eurospeech interspeech pp

isca
jaccard p

etude comparative la distribution orale dans une portion des alpes jura
bulletin del la societe vaudoise des sciences naturelles
kanerva p
kristoferson j
holst a

random indexing of text samples for latent semantic analysis
in gleitman l
r
joshi a
k
eds
proceedings of the annual conference of the cognitive science society p

psychology press
revisiting centrality as relevance kanerva p
sahlgren m

foundations of real world intelligence chap
from words to understanding pp

no

center for the study of language and information
kleinberg j
m

authoritative sources in a hyperlinked environment
journal of the acm
koosis p

introduction to hp spaces
cambridge universisty press
kurland o
lee l

pagerank without hyperlinks structural re ranking using links induced by language models
in sigir proceedings of the annual international acm sigir conference on research and development in information retrieval pp

acm
kurland o
lee l

pagerank without hyperlinks structural reranking using links induced by language models
acm transactions on information systems
landauer t
k
foltz p
w
laham d

an introduction to latent semantic analysis
discourse processes
landis j
r
kosh g
g

the measurement of observer agreement for gorical data
biometrics
leite d
s
rino l
h
m
pardo t
a
s
nunes m
g
v

extractive tomatic summarization does more linguitic knowledge make a dierence
in ceedings of the second workshop on textgraphs graph based algorithms for natural language processing pp

association for computational linguistics
lin c


rouge a package for automatic evaluation of summaries
in moens m

szpakowicz s
eds
text summarization branches out proceedings of the workshop pp

association for computational linguistics
lin c

hovy e

the automated acquisition of topic signatures for text summarization
in coling the international conference on computational linguistics vol
pp

association for computational linguistics
lin s

yeh y

chen b

extractive speech summarization from the view of decision theory
in proceedings of the annual conference of the international speech communication association interspeech pp

isca
liu y
shriberg e
stolcke a
hillard d
ostendorf m
harper m

riching speech recognition with automatic detection of sentence boundaries and disuencies
ieee transactions on speech and audio processing
lund k
burgess c
atchley r
a

semantic and associative priming in dimensional semantic space
in moore j
d
lehman j
f
eds
proceedings of the annual conference of the cognitive science society pp

psychology press
marcu d

the theory and practice of discourse parsing and summarization
the mit press
ribeiro de matos maskey s
r
hirschberg j

comparing lexical acoustic prosodic strucural in proceedings of the and discourse features for speech summarization
rospeech interspeech
mckeown k
r
hirschberg j
galley m
maskey s
r

from text to speech summarization
in ieee international conference on acoustics speech and signal processing
proceedings vol
v pp

ieee
mihalcea r
tarau p

textrank bringing order into texts
in proceedings of the conference on empirical methods in natural language processing pp

association for computational linguistics
mihalcea r
tarau p

a language independent algorithm for single and multiple document summarization
in proceedings of the second international joint conference on natural language processing companion volume to the proceedings of conference including posters demos and tutorial abstracts pp

asian eration of natural language processing
mooney c
z
duval r
d

bootstrapping a nonparametric approach to tical inference
sage publications
murray g
renals s

term weighting for summarization of multi party spoken dialogues
in popescu belis a
renals s
bourlard h
eds
machine learning for multimodal interaction iv vol
of lecture notes in computer science pp

springer
nenkova a

summarization evaluation for text and speech issues and approaches
in proceedings of interspeech icslp pp

isca
nenkova a
passonneau r
mckeown k

the pyramid method incorporating human content selection variation in summarization evaluation
acm transactions on speech and language processing
orasan c
pekar v
hasler l

a comparison of summarisation methods based on term specicity estimation
in proceedings of the fourth international language resources and evaluation pp

elra
pardo t
a
s
rino l
h
m

temario a corpus for automatic text tion
tech
rep
nilc nucleo interinstitucional de lingustica cional nilc sao carlos brazil
penn g
zhu x

a critical reassessment of evaluation baselines for speech summarization
in proceeding of hlt pp

association for putational linguistics
r development core team
r a language and environment for statistical ing
r foundation for statistical computing vienna austria
isbn
radev d
r
hatzivassiloglou v
mckeown k
r

a description of the cidr system as used for
in proceedings of the darpa broadcast news workshop
radev d
r
jing h
budzikowska m

centroid based summarization of tiple documents sentence extraction utility based evaluation and user studies
in revisiting centrality as relevance naacl anlp workshop automatic summarization pp

association for computational linguistics
radev d
r
jing h
stys m
tam d

centroid based summarization of multiple documents
information processing and management
radev d
r
tam d

summarization evaluation using relative utility
in proceedings of the international conference on information and knowledge agement pp

acm
ribeiro r
matos d
m

extractive summarization of broadcast news paring strategies for european portuguese
in matousek v
mautner p
eds
text speech and dialogue international conference tsd pilsen czech republic september
proceedings vol
of lecture notes in computer science subseries lnai pp

springer
ribeiro r
matos d
m

mixed source multi document speech to text summarization
in coling proceedings of the workshop on multi source multilingual information extraction and summarization pp

coling ganizing committee
ribeiro r
matos d
m

using prior knowledge to assess relevance in in ieee workshop on spoken language technology speech summarization
pp

ieee
ruge g

experiments on linguistically based term associations
information cessing and management
sahlgren m

the word space model using distributional analysis to represent tagmatic and paradigmatic relations between words in high dimensional vector spaces
ph
d
thesis stockholm university
steyvers m
griths t

handbook of latent semantic analysis chap
bilistic topic models pp

lawrence erlbaum associates
tucker r
i
sparck jones k

between shallow and deep an experiment in tomatic summarising
tech
rep
university of cambridge computer laboratory
turney p
d
pantel p

from frequency to meaning vector space models of semantics
journal of articial intelligence research
uzeda v
r
pardo t
a
s
nunes m
g
v

a comprehensive comparative evaluation of rst based summarization methods
acm transactions on speech and language processing
vanderwende l
suzuki h
brockett c
nenkova a

beyond sumbasic task focused summarization and lexical expansion
information processing and agement
wan x
li h
xiao j

eusum extracting easy to understand english maries for non native readers
in sigir proceedings of the annual ternational acm sigir conference on research and development in information retrieval pp

acm
ribeiro de matos wan x
yang j
xiao j

collabsum exploiting multiple document clustering for collaborative single document summarizations
in sigir proceedings of the annual international acm sigir conference on research and development in information retrieval pp

acm
zechner k
waibel a

minimizing word error rate in textual summaries of spoken language
in proceedings of the conference of the north american chapter of the acl pp

morgan kaufmann
zhu x

semi supervised learning with graphs
ph
d
thesis language technologies institute school of computer science carnegie mellon university

