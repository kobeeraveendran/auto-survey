readonce transformers reusable representations of text for transformers shih ting lin ashish sabharwal tushar khot university of texas austin u
s
a
allen institute for ai seattle u
s
a

edu ashishs
org t c o l c
s c v
v i x r a abstract while large scale language models are tremely effective when directly ne tuned on many end tasks such models learn to extract information and solve the task simultaneously from end task supervision
this is wasteful as the general problem of gathering information from a document is mostly task independent and need not be re learned from scratch each time
moreover once the information has been captured in a computable representation it can now be re used across examples ing to faster training and evaluation of els
we present a transformer based approach readonce transformers that is trained to build such information capturing tions of text
our model compresses the ument into a variable length task independent representation that can now be re used in ferent examples and tasks thereby requiring a document to only be read once
ally we extend standard text to text models to consume our readonce representations along with text to solve multiple downstream tasks
we show our task independent sentations can be used for multi hop qa stractive qa and summarization
we observe speedups compared to standard text text models while also being able to handle long documents that would normally exceed the length limit of current models
introduction transformer based large scale language models lms radford et al
devlin et al
are task independent models that are surprisingly effective when directly ne tuned on many ent end tasks rajpurkar et al
wang et al
a
this has reduced the need for ing task specic system architectures
however the author s work was primarily done during an ship at the allen institute for ai
figure readonce transformers rather than learning to extract information specic to each task we use transformer based encoders to build independent reusable document representations once and feed them into various former models trained for end tasks
this approach relies heavily on using end task pervision to learn to solve two sub problems multaneously extract information from an input document d and solve the end task e

answer a question about d
this incentivizes lm based models to learn to extract only task specic and even example specic information when tuned on the end task
for example a ment classication model may learn to only tract sentiments from d while a question swering qa model may learn to only extract from d the information needed for the specic question at hand
this strategy while effective on many datasets is also inefcient
first it requires models to be trained from scratch for each end task even though the sub problem of gathering the text to textmodeltext to textmodeltext to


task independentrepresentationstask specicmodelsreadoncetransformersreadingcomprehensionsummarizationabstractive qarepr
textmodelrepr
textmodel tion content of the input document d is shared across tasks
second each d must be re read from scratch in the context of each example e

once for each question even when many ples share d
not only is this computational dundancy undesirable slow inference can quickly become a bottleneck in deployed real time tems if models with billions of parameters must re read d for every input query about d
further the longer the document the worse the problem
inspired by humans ability to read a ment and extract key information from it without having to know the use case in advance we ask the following question can we use based lms to build compressed representations of text that are and task independent and hence reusable further can we extend to text transformer architectures to consume such representations in conjunction with text prior representation learning approaches tempt to capture the meaning of sentences into a continuous vector conneau et al
kiros et al

while they have been effective on downstream classication tasks it is unclear whether they can capture the information content of entire paragraphs
more importantly these proaches were designed based on hidden tations from recurrent networks and then fed as is to task specic classiers
in contrast our goal is to a build such document representations ing transformer based lms and combine them with example specic text inputs in a way that can be consumed by a transformer architecture
to this end we propose an approach to vert any encoder decoder based transformer lm such as bart lewis et al
into a new chitecture termed readonce transformer with it reads documents only two key properties once to create compressed information capturing reusable representations that we refer to as once representations and it then consumes these document representations together with and example specic plain text e

a question
compressing information in a document to a xed length vector can be lossy for long uments and wasteful for short documents
we therefore use a variable length representation that scales with the length of the input document
a key challenge is to nd a compressed tation that contains just the right level of mation
we choose to capture enough tion to answer factoid questions about d but nore syntax and low level semantics
ingly we use supervision from two factoid qa datasets squad rajpurkar et al
and supervisedqa lewis et al
when ing readonce representations
finally to solve an end task we modify a text to text transformer architecture to consume readonce tions and example specic text together in the coder
our experiments with a qa model trained to use readonce representations demonstrate that these representations are more effective at ing information compared to baseline approaches
our representations also generalize to other tasks such as multi hop qa yang et al
tive qa kocisky et al
and tion narayan et al

since readonce representations are puted only once and can be cached we can train and infer with models faster than standard approaches with only a marginal drop in accuracy about points on qa and rouge l points on summarization for a speedup
the sion ratio parameter of our representations vides an easy way to trade off this computation time with accuracy
further our experiments simulating a long document setting demonstrate that readonce transformers by virtue of their capturing yet compressed representations stantially outperform a standard truncation based usage of transformer models when the document text is too long to t in them
our approach thus provides a novel way to use existing transformer models to handle large documents ing a recent line of research on designing novel transformer architectures specically for long uments beltagy et al
zaheer et al

related work representation learning approaches are monly used to extract xed length sentence beddings conneau et al
kiros et al
from variable length text inputs
such xed length representations have enabled the development of simpler downstream models that do not have to deal with the variable lengths of textual inputs
however these representations have mainly been used for simple classication tasks on short input texts bowman et al
wang et al

recent work reimers and gurevych he et al
artetxe and schwenk karpukhin et al
have tried building document embedding using large scale language models as well
however these xed length resentations have mostly been built to identify similar documents reimers and gurevych karpukhin et al

these representations have never been used to directly answer questions over them leaving it unclear whether they capture the key pieces of information in the document
one such approach quase he et al
also used question answering supervision to learn reusable sentence embeddings
however their goal was to transfer the supervision of qa datasets to other tasks via sentence embeddings learned on the source dataset
we on the other hand focus on compressing the input document while ing the information content of the document
artetxe and schwenk learned lingual sentence embeddings based on parallel corpora such that just the embeddings of any tence could be used to translate the sentence
these representations would be able to capture the knowledge present in a sentence but they were signed for bilstms
kiros extend this idea to transformer based language models but are still limited to only sentences
large scale have been especially designed to handle long ments yang et al
beltagy et al
zaheer et al

these models propose novel architectures that need to be pre trained on large corpora
we present an orthogonal approach to scale existing language models to longer uments without requiring any expensive model pre training
language models some deformer cao et al
proposed an nate approach to scale language models by forming local attentions in the lower layers and then aggregating them in the higher layers
ever it is a task specic model without any usable representations
readonce transformers our goal in this work is to identify the optimal architecture to extract information capturing usable representations
at the same time we also need to nd the optimal architecture to use such representation in conjunction with text inputs
so at a high level as shown in fig
we need to develop two systems a model to compute the representation document encoder and a general model for tasks that can consume tor representations and text model
given the recent success and generality of encoder decoder models radford et al
raffel et al
lewis et al
we focus on developing models for such an architecture
we present the potential choices for each model with the nal model used in our system indicated by a

document encoder given an encoder decoder model there are ent ways to compute representations for a ment d with tokens


tn
we focus on ing the output representation generated by the coder represented with hi for each token ti


fixed length aggregation the most common approach is to extract a single vector representation from a sequence of tations kiros et al
conneau et al

while this can be a very compact representation of a document it tends to be very lossy cially when dealing with large documents
as a result these representations are mainly used for classication conneau et al
reimers and gurevych or retrieval karpukhin et al
and have not been shown to capture the content of the document
e
g infersent conneau et al
presented a self attentive approach to extract sentence embedding using these hidden representations r i where u is a function that computes a scalar tention over each hi
to reduce information loss we extend these models to produce a m representation vectors by learning m sets of parameters j for j


m i
e
rj uj i i ej hi
where uj hi ej

special token representations with the advent of transformer models other common approach is adding a special radford et al
devlin et al
or s liu et al
token to the context
the output representation of this special token can then be used as inputs to classiers and other stream models
again a single representation can be lossy so we generate m representations by serting multiple special tokens
since there are no parameters to be learned that depend on the length m of the output tion we can dynamically adjust the number of cial tokens based on the input length
to achieve a compression ratio of k special tokens and use their representations
we insert n we consider three different variations of ing these special tokens into the context add them at the beginning of the context sufx add them at the end of the context interleave add them after every k tokens
while the rst two approaches ensure that the text does not have any loss in continuity the last interleaving based approach might more directly incentivize the model to capture the local context around each special token


sliding window aggregation finally we apply the idea of aggregating vector representations to generate a length representation
we apply an aggregation function f over sliding windows of size w tokens to capture the local context of the window
for a stride length of s this would result in tion vectors rj f hsj where f corresponds to pooling linear weighting as described in eqn
and max pooling respectively
figure shows how we would compute these representations using a window size of with no overlap i
e
and the linear weighting function
the resulting readonce tions would have m vectors where n is the number of tokens in the input

model next we present our modication to downstream task models to use both text and our generated readonce representations
since most nlp tasks can be re formulated as a text to text lem radford et al
raffel et al
we focus on extending text to text encoder decoder models to a text model
figure sliding window aggregation approach to tract meaning representations from a transformer based encoder
linear weighted sum is used to aggregate the vectors from the nal output layer into a single vector resulting in the readonce representations with vectors
figure appending the readonce representations to the lth layer of the encoder to extend standard encoder decoder models to handle inputs


append to encoder since the transformer block in an encoder can dle any input length in each layer one possible approach is to append the representations to the lth layer of the encoder
this allows the model to focus on parsing the input example text e

question in the layers followed by focusing on answering the question in the remaining layers
we show this model in figure where the encoder only processes the q tokens of the question for the rst l layers
once the m readonce tations are added to the lth layer all the quent layers produce m q vectors by attending over both the representations and text
finally an unmodied decoder produces the output answer


modify transformer block attention rather than just modifying the input we sider an alternate approach of modifying the transformer block itself
similar to chines rashkin et al
we view the sentation as a memory that the self attention block can attend over in addition to the input text
the self attention block uses two separate attention modules for both of these input types and averages
representationlinear weightinglinear weightinglinear weightinglinear the vectors
specically let hl enc be the matrix of hidden states generated from the lth layer of a standard transformer hl enc enc enc enc where k v is the attention module used in the transformer that takes q k v as the query key and value matrix
to take extra once representations as an input we instead compute hl enc as hl enc attn enc enc r enc enc where attn is a separate attention module to include the readonce representations r in our model whose weights are initialized by the corresponding weights in attn to speed up the training
for the decoder in the model we also pute the hidden states of each layer as per eqn
so that the model can attend over the extracted document information during the decoding cess too

training readonce via qa given the overall architecture of such a system shown in fig
we next focus on training this model to produce readonce representations that capture the information present in the ment
while prior representation learning models have often focused on classication tasks we stead use the reading comprehension qa task to if a ensure this information capturing property
model is able to use just the readonce sentations to answer the questions grounded in the document the representations would contain the information needed to answer such questions
the key question here is which qa datasets are most suitable for training a compact yet information capturing document tion low level semantic qa datasets such as qamr michael et al
he et al
do nt allow for any compression as the questions require the representation to capture ery word in the input sentence
more complex multi hop qa datasets such as hotpotqa yang et al
are also not appropriate as they cus on learning to reason rather than learning to capture information
factoid qa tasks provide a sweet spot between these two extremes where figure the nal architecture of the readonce transformers model
we use the aggregated sliding window representations as shown in fig
as our ument encoder to compute the readonce tations
we append these representations to the lth layer of the encoder in our model as shown in fig

we ne tune this end to end model on qa tasks to train the document encoder to extract information capturing representations
we freeze this encoder and further ne tune the model on various downstream tasks
extracting the key information is sufcient to swer the questions
we use two such datasets squad rajpurkar et al
and unsupervised qa lewis et al
datasets to train our els
once these representations are trained to tract the information in the document we can use them to solve varied tasks by learning the based reasoning in the downstream models

downstream usage of readonce to verify the generality of the readonce sentations we train models to perform multi hop reasoning abstractive qa and summarization ing our learned representations
specically we freeze the document encoder model and use it to generate the representations for documents
we further ne tune the model on the downstream task to produce the output label given the readonce representations and any example specic input
representation learning experiments we rst evaluate the different potential tural choices for extracting and using document representations discussed in
and
tively
while our main interest is in learning fective representations we need to train the model model for each candidate representation in order to nd the optimal tecture that can consume the representation

training setup we train the entire model on the factoid qa task to ensure that the document representations do capture factual knowledge
we primarily use the squad reading comprehension dataset jpurkar et al
containing more than crowd sourced factoid questions
we further augment this dataset with about based questions from the dataset lewis et al

this allows increasing the size of the training dataset while also ing question diversity
to avoid these cally generated questions overwhelming training we ensure that the same number of questions are selected from both the datasets in each batch by duplicating squad questions
in the same vein we evaluate each model based on their mance on the squad task
unless otherwise mentioned we use the large model in all our experiments and optimize the model with cross entropy loss
we set the learning rate to for the weights initialized from the bart model and to for randomly initialized newly added weights which is shown benecial in peters et al

for other parameters we follow lewis et al


architecture evaluation to be able to evaluate the representations we need to rst decide the architecture of the model suming these representations


model we explore the different choices for the model model discussed in
suming the representation is generated by a ple document encoder model mean aggregation over a sliding window with both window size and stride being tokens
the results are shown in ble
architecture append append append modifyatt design parameters squad em







table a comparison of different bart based tectures for jointly operating over continuous tations and text
we see that appending readonce tions

too early or too late in the encoder stack is not as effective as appending about half way
we suspect that ing too early does not allow the model to focus on understanding the question whereas ing too late does not leave room for enough attention between the question and document resentations
modifying the transformer block to attend over these representations

results in a able score on squad but it is still formed by our simple append architecture
hence for the rest of this work we stick to the simpler chitecture of appending the representation at the layer denoted


document encoder given the model model chitecture chosen above we now explore potential document encoder architectures to extract once representations
for a fair comparison we ensure that all our evaluated representations use on average across a dataset the same number of vectors to represent documents
table presents em and scores on squad for the various chitectural choices discussed in

architecture design parameters squad slidingwindow slidingwindow f f slidingwindow f slidingwindow m sufx m interleave fixedlength em













table a comparison of different architectures for extracting continuous representations using bart coder
each approach extracts representations of the same length namely of the document length ther for each document or on average across the dataset
the top rows explore the sliding window architecture

with both window size and stride length of i
e
no overlap between dows with the three different aggregation tions mentioned earlier
we see that both the mean and the learned weighted sum have ble performance on this task and outperform the max pooling function
we also evaluate the pact of increasing the overlap between windows scores on uqa correlate well with the scores on also experimented with and and did nt nd squad with close to for most models
any signicant gains
by increasing the window size not changing the stride length keeps the average number of vectors constant
for the learned weighted sum function this results in a point drop in the score bly due to the aggregation function having to erate over a larger window
we next evaluate the approaches inspired by prior work where we add special tokens

and use the representations of these tokens
for the bart model we use a newly added cls token as our special token
we see from table that neither appending these tokens at the end nor interleaving them in the input results in tations comparable to the sliding window based approaches
finally if we x the representation length

to vectors computed based on the average token length of squad
the learned representations are still not as effective
final readonce architecture based on this set of experiments we use the sliding window chitecture for the document encoder with learned weighted sum as the aggregation function and pend these representations to the layer in the nal task dependent model
downstream task experiments next we evaluate the quality of our tions by using them on three downstream tasks different from the tasks readonce transformers are trained on
the results demonstrate the tage of using such representations for faster ing and inference
finally we show the benet of using our representation in a scenario where uments are much longer than the maximum token limit of the underlying lm

experimental setup tasks we consider three end tasks extractive qa summarization and abstractive qa and uate our system using the following datasets
hotpotqa yang et al
is a multi hop reasoning dataset that requires models to gate information from two paragraphs to produce the answer a span from the input paragraphs
we focus on their distractor setting where they ditionally provide models with distractor graphs
for efciency we use the output of the quark system groeneveld et al
which lects up to tokens including the question the special tokens as prex had similar scores to the sufx model
from the input paragraphs
we use the answer em and scores as the metrics
xsum narayan et al
is an stractive news summarization dataset that requires models to generate summaries that go beyond ply extracting key sentences
we use the l summ
commonly used for tion datasets which computes the union lcs of the longest common subsequences lcs between each pair of reference and hypothesis sentences
in contrast the standard rouge l score computes lcs between the reference and hypothesis ing both of them as one sentence
kocisky et al
is an abstractive qa dataset where answers may not be extractive spans in the input document
models would need to understand the content of the ment to generate such answers
we use the same rouge l summ
score as for the summarization task
baselines we compare readonce formers to bart based qa models that use the document text directly to answer the given tion
since these models use text directly out any lossy compression their score is best viewed as an upper bound for any based bart model including ours
we train the bart model to generate the answer given the tire document and question the question is the word summary in the case of xsum
since our representations were trained on the squad and uqa dataset we also use as a baseline bartsquad uqa which is the bart model but rst ne tuned on these two datasets
readonce models we freeze the parameters of the document encoder to generate the tations for all the documents in the datasets
we then use these representations with our model which is further ne tuned on each end task
to evaluate the impact of our training on qa datasets we compare our model to the readonce architecture initialized with the bart model weights readonce
the document encoder is trained using a dow size of and stride length of which results in representations that are of the input length

com google google research tree master rouge our experiments we did not notice any tial difference between the simple rouge l metric and this summarization based metric
since our model can dle a variable number of representation vectors we can change this compression ratio on without having to re train our models
cally we can use a stride length of k to generate representations that are kth of the input length and then feed them to a downstream model
by reducing the value of k we can reduce the pression ratio and improve the model accuracy at the cost of increased runtime
interestingly we discovered that we do nt even need to train document encoder for each value of k
we can achieve a performance comparable to encoders trained individually for each value of k by using the document encoder trained on k and only varying the value of k during the tuning step

representation quality first we assess the ability of readonce resentations to capture document information as compared to using the original document text
as shown in table our framework at is only and rouge l points behind the bart model which uses the text directly
this demonstrates that readonce representations do capture most of the relevant information in the document
hotpotqa narr
qa xsum architecture readonce readonce upper bounds bart bartsquad uqa




r l




r l




table performance of readonce transformers on three datasets vs
a standard text to text transformer with full access to document text
competitive mance at indicates our document representation is effective at capturing information
setting drops the score but as we will show shortly improves i
e
reduces the training and inference time of these models
for fairness we also ne tuned the bart model on the same qa datasets squad uqa and found no ticeable difference to the baseline bart model
lastly we note that the readonce system which simply uses the bart model parameters is about and rouge l points behind our model with learned representations
this shows that our model does rely on the factoid questions to learn to extract meaningful representations

model efciency one key advantage of readonce tions is that the model needs to read the document only once and can reuse pre computed tions for multiple examples or even multiple tasks
these pre computed compressed representations can be used directly when training a downstream task model making the training more efcient by reducing the document level self attention cost to the representation level cost
we can similarly reduce the cost of inference abling faster question answering over a static pus a very common application scenario
in general the cost to compute readonce representations is moderate since we only use the encoder stack of the model
for example it takes the model about seconds to compute the resentations for documents in hotpotqa
even in narr
qa where documents are longer on average computing the representations still only requires about seconds per documents
training time as shown in figure our model exhibits a speedup of in training time compared to the standard bart model
note that this includes the cost of reading readonce resentations from disk
by caching these tations in memory this speedup could be further increased
figure training time seconds per batch
for readonce models document representations are pre computed and cached resulting in a speedup
also observe drops in score if we use the bart model parameters only in the document encoder or the model are generally much shorter so we only focus on the document level cost
further we only pay this cost on half the layers since we set for a layer transformer
since certain computation costs are not affected by the reduction in the self attention cost we found our speedup relative to the baseline model plateaus out to about when
inference time similarly we observe a speedup in the inference time in figure which again plateaus out at
figure accuracy of models under different mum window length assumptions
readonce formers stay substantially more accurate as the mum window length decreases
line bart model drops signicantly from
to

with our model consistently performs the baseline model but exhibits a lar drop in score as the maximum token limit of this model with is still only tokens as per the equation above
on the other hand with and we can handle documents up to tokens thereby requiring no truncation on of the examples even in this extreme nario
as a result we only see a
point drop in score as t is decreased from to
these simulations provide strong evidence of the ability of readonce transformers to handle long uments more effectively than standard transformer based models
conclusion this work introduced readonce transformers a novel approach for using large scale based language models to both build and sume reusable document representations
akin to humans ability to read a document and tract useful information without having to know in advance what the end use might be once representations are compact capturing document representations that can be pre computed once in a and independent fashion
our results on extractive qa summarization and abstractive qa tasks demonstrate that using readonce representations in lieu of re reading document text in the context of every example results in substantially faster training and ence at a modest cost in accuracy
the figure inference time seconds per batch
for readonce models document representations are pre computed and cached resulting in a speedup

handling long documents compressing document representations used in the downstream model also enables the model to reason over longer documents that would normally not t within its maximum token length limit
for example if a model has a max limit of t tokens we would have to truncate all inputs to this length or nd intelligent ways to sub select text
on the other hand when using compression ratio k we can compute readonce tations for k such length t chunks of the input document
we can then concatenate these t length representations together to produce a sentation that would still t within the max token limit
if each chunk has an overlap of o tokens with the previous chunk our model would thus be able to handle inputs of lengths
in these experiments we set to ensure that any partial sentence on the border of one chunk would appear whole in the next chunk
to evaluate the impact of the compression tor k on the max token limit t we simulate ent token limits for the narrativeqa datasets
the average number of tokens in the documents in this dataset is
with of the documents ing under tokens
the results are depicted in figure
as we reduce t for the underlying transformer model from to the score of the token








once framework also offers an easy way to trol the trade off between speed and accuracy via the compression ratio parameter and enables the use of standard transformer architectures on long documents that would normally be beyond the model s token limit
using these representations for other tasks such as better document retrieval and summarizing long scientic documents han et al
are future next steps
acknowledgements we thank dirk groeneveld for providing the put of the quark system for training our model on hotpotqa
computations on beaker
org were ported in part by credits from google cloud
references m
artetxe and holger schwenk

massively multilingual sentence embeddings for zero shot cross lingual transfer and beyond
tacl
iz beltagy matthew e
peters and arman cohan
longformer the long document
former


samuel r
bowman gabor angeli christopher potts and christopher d
manning

a large tated corpus for learning natural language inference
in emnlp
qingqing cao h
trivedi aruna balasubramanian and niranjan balasubramanian

deformer decomposing pre trained transformers for faster question answering
in acl
arman cohan franck dernoncourt doo soon kim trung bui seokhwan kim w
chang and nazli goharian

a discourse aware attention model for abstractive summarization of long documents
in naacl hlt
alexis conneau douwe kiela holger schwenk loc barrault and antoine bordes

supervised learning of universal sentence representations from natural language inference data
in emnlp
jacob devlin ming wei chang kenton lee and kristina toutanova

bert pre training of deep bidirectional transformers for language standing
in naacl
dirk groeneveld tushar khot ashish sabharwal et al

a simple yet strong pipeline for potqa
in emnlp
hangfeng he qiang ning and dan roth

quase question answer driven sentence encoding
in acl
luheng he m
lewis and luke zettlemoyer

question answer driven semantic role labeling ing natural language to annotate natural language
in emnlp
v
karpukhin barlas oguz sewon min patrick lewis ledell yu wu sergey edunov danqi chen and wen tau yih

dense passage retrieval for open domain question answering
in emnlp
jamnie kiros

contextual lensing of universal sentence representations
arxiv

ryan kiros yukun zhu russ r salakhutdinov richard zemel raquel urtasun antonio torralba in and sanja fidler

skip thought vectors

tomas kocisky jonathan schwarz p
blunsom chris dyer k
hermann gabor melis and edward grefenstette

the narrativeqa reading prehension challenge
tacl
mike lewis yinhan liu naman goyal jan ghazvininejad abdelrahman mohamed omer levy ves stoyanov and luke zettlemoyer

bart denoising sequence to sequence training for natural language generation translation and comprehension
in acl
patrick lewis ludovic denoyer and sebastian riedel

unsupervised question answering by cloze translation
in acl
yinhan liu myle ott naman goyal jingfei du dar joshi danqi chen omer levy mike lewis luke zettlemoyer and veselin stoyanov

roberta a robustly optimized bert pretraining proach
arxiv preprint

julian michael gabriel stanovsky luheng he i
gan and luke zettlemoyer

ing question answer meaning representations
in naacl
s
narayan shay b
cohen and mirella lapata

do nt give me the details just the summary aware convolutional neural networks for extreme summarization
in emnlp
matthew e
peters mark neumann robert l gan roy schwartz vidur joshi sameer singh and noah a
smith

knowledge enhanced tual word representations
in emnlp
alec radford karthik narasimhan tim salimans and ilya sutskever

improving language standing with unsupervised learning
technical port openai
colin raffel noam shazeer adam roberts katherine lee sharan narang michael matena yanqi zhou wei li and peter j liu

exploring the limits of transfer learning with a unied text to text former
jmlr
pranav rajpurkar jian zhang konstantin lopyrev and percy liang

squad questions for machine comprehension of text
in emnlp
hannah rashkin a
c elikyilmaz yejin choi and feng gao

plotmachines outline conditioned in generation with dynamic plot state tracking
emnlp
nils reimers and iryna gurevych

bert sentence embeddings using siamese networks
in emnlp ijcnlp
alex wang yada pruksachatkun nikita nangia amanpreet singh julian michael felix hill omer levy and samuel bowman

superglue a stickier benchmark for general purpose language understanding systems
in neurips
alex wang amanpreet singh julian michael felix hill omer levy and samuel r bowman

glue a multi task benchmark and analysis form for natural language understanding
in iclr
zhilin yang zihang dai yiming yang jaime bonell russ r salakhutdinov and quoc v le

xlnet generalized autoregressive pretraining for language understanding
in neurips
zhilin yang peng qi saizheng zhang yoshua gio william w
cohen ruslan salakhutdinov and christopher d
manning

hotpotqa a dataset for diverse explainable multi hop question answering
in emnlp
manzil zaheer guru guruganesh kumar avinava dubey joshua ainslie c
alberti s
ontanon philip pham anirudh ravula qifan wang l
yang and a
ahmed

big bird transformers for longer sequences
arxiv


