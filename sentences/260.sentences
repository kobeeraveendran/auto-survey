a m l c
s c v
v i x r a attend to the beginning a study on using bidirectional attention for extractive summarization ahmed cezary university of pittsburgh pitssburgh pa usa microsoft research fuse lab ccp bellevue wa usa
edu
com abstract forum discussion data differ in both structure and ties from generic form of textual data such as news
forth summarization techniques should in turn make use of such differences and craft models that can benet from the structural nature of discussion data
in this work we propose attending to the beginning of a document to improve the performance of extractive summarization models when plied to forum discussion data
evaluations demonstrated that with the help of bidirectional attention mechanism ing to the beginning of a document initial comment post in a discussion thread can introduce a consistent boost in rouge scores as well as introducing a new state of the art sota rouge scores on the forum discussions dataset
additionally we explored whether this hypothesis is able to other generic forms of textual data
we make use of the tendency of introducing important information early in the text by attending to the rst few sentences in generic tual data
evaluations demonstrated that attending to ductory sentences using bidirectional attention improves the performance of extractive summarization models when even applied to more generic form of textual data
introduction recently automatic text summarization models either tractive or abstractive witnessed fast performance strides due to the emergence of deep learning models specially models
a large number of recent neural abstractive rization models employ the encoder decoder structure see liu and manning gehrmann deng and rush paulus xiong and socher to convert the input quence into a relatively shorter sequence
most of the recent extractive models on the other hand employ only an coder part to convert the input sequence into a xed feature vector followed by a classication part nallapati zhai and zhou liu and lapata
text summarization has been applied to different natural language domains news academic papers emails meeting notes forum discussions

while some models can be transferable from one main to the other it might be more benecial to craft this work was done during internship copyright association for the advancement of articial intelligence www
aaai
org
all rights reserved
ditional modications in those models to account for ences between domains
forum discussion data tarnpradab liu and hua for example is different in both ture and properties when compared to generic textual data such as news
discussion threads usually start with an initial post comment i
e
seeking knowledge or help


the following comments tend to target the initial post comment providing additional information or opinions
with that said a question arises
can we enhance existing summarization models to benet from such properties inspired by seo et al
we propose integrating rectional attention mechanism in extractive summarization models to help to attend to early pieces of text initial ment
the main objective is to benet from the dependency between the initial comment and the following comments and try to distinguish between important and irrelevant or supercial replies
moreover recent research by jung et al
showed that in some domains humans tend to duce relatively important information early at the beginning of textual articles
unlike discussion threads we explore the benet of attending to the beginning in a more generic textual setting
simply by integrating bidirectional attention mechanism and attending to the rst few sentences in a ument
we conducted some experiments to evaluate this pothesis using a dataset of generic non discussion based documents
thus our contributions in this work are fold
first we introduce integrating bidirectional attention mechanism into extractive summarization models to help to attend to earlier pieces of text
second we achieved a new sota on the forum discussion dataset through the proposed attending to the beginning mechanism
third to further ify the transferability of our hypothesis i
e
attending to the beginning we perform evaluations to show that attending to earlier sentence in a more generic text can also benet summarization models
on different domains other than cussions
related work automatic text summarization has seen increasing est and improved performance due to the emergence of models sutskever vinyals and le and tention mechanisms bahdanau cho and bengio
this is true for both automatically generating coherent mary abstractive summarization and extracting salient pieces of text extractive summarization
the majority of recent research has been directed towards the news domain see liu and manning paulus xiong and socher i
e
due to the existence of huge annotated datasets cnn dailmail gigawords newyork times
unlike news other domains such as emails discussions meeting notes students feedback and opinions can still be considered derexplored
recent efforts to tackle such domains started to emerge luo liu and litman targeted student feedback marization by extracting a set of representative phrases
li et al
proposed doing abstractive summarization for meeting notes by employing textual and visual tion into a multi model setting
li li and zong yang et al
tackled the problem of opinion and review summarization
a work targeting similar domain as ours is done by tarnpradab liu and hua
in which they proposed doing hierarchical attention to perform extractive summarization over a dataset of forum discussions collected from trip advisor
another work which shares a similar sign concept as ours was done by wang quan and wang
they also integrated bidirectional attention nism in their model however their model and ours are ferent in both intuition and application
the major motive to integrate bidirectional attention in their design is to attend to an external template during the summarization process while in ours we propose attending to early pieces of input text using the bidirectional attention mechanism
another major difference is the intended application
they developed their model for the task of abstractive summarization over news data while ours is intended for extractive tion task
dataset in this work we employ two extractive summarization datasets
first we used the discussion dataset proposed by tarnpradab liu and hua
the discussion dataset is extracted from trip advisor forum discussions
the data consists of threads
in their work tarnpradab liu and hua used threads for training and for idation
we did nt use the same data distribution reported by the authors however we kept the same testing data size for comparability reasons
we used our own split to verify the utility of our proposed techniques
we used threads for training for validation and for testing
over we conducted additional experiments using msw et al

msw dataset is a generic textual dataset that is used to verify the transferability of our pothesis to more generic textual domains
we verify whether we can benet from documents structure and human s dency to present important information earlier by attending to early sentences
msw dataset consists of a collection of generic documents of different domains
we split the dataset trip advisor msw part train val test train val test documents total sentences table model datasets data into training validation and testing of and documents respectively
table summarizes the bution of datasets used
baselines in order to validate our hypothesis and show the utility of our proposed enhancements we implemented baselines
the following sections provide additional details regarding each of the baselines implemented
we used the python off the shelf package for text rization sumy
summarization is done hierarchically where each comment from the discussion thread is passed rately to sumy
the resultant summaries are then combined and passed to sumy as one nal document to get the thread summary
lsa clustering we implemented a simple baseline for extractive rization
the baseline uses latent semantic analysis lsa to embed sentences into vector space
sentences are then clustered using the k means clustering algorithm
we use a number of clusters n where n is the number of tences in the input document
lastly for each cluster a ter head is picked
the cluster head is the sentence closest to the mean point of the cluster
summarunner summarunner is an auto regressive extractive tion model proposed by nallapati zhai and zhou
siatl siatl is sentence classication model developed by chronopoulou baziotis and potamianos
the model employs multi task learning by integrating language ing auxiliary loss during the training process
siatl model was developed originally as a sentence classication model
however we decided to deal with extractive summarization as a pure sentence classication problem and use siatl model as extractive summarizer
unlike summarunner which is an auto regressive model i
e
previous decisions made by the model affect its future decisions siatl performs classication independently for can be downloaded from
dropbox
com s threaddataset
dataset is not publicly available
org project to paper for original model design each sentence
while it may always seem that sive models would perform better xiao and carenini showed that non auto regressive models can sometimes be more efcient
thus we decided to use the siatl model as a baseline and compare the performance of auto regressive and non auto regressive models within the extent of our study
attend to the beginning i i hb hd throughout this work we hypothesize that attending to the initial part of a text during extractive summarization would help in selecting more salient sentences
the intuition is that in some

discussion threads the initial part of a text holds important topical information
forth it renders an important factor in selecting salient tences for summarization objective
thus we validate this hypothesis by calculating the importance of a sentence with respect to the initial part of the text in the form of tention
inuenced by seo et al
wang quan and wang the same interaction approach is employed here to produce beginning aware sentence representations for each sentence in the document
first a sentence resentation is produced for each sentence of the document as well as each sentence of the beginning part of the ument i
e
initial post comment in the case of discussion rmn is then computed for dataset
similarity matrix s each pair of document and beginning part sentences sij hip j
where n is the number of sentences in the beginning part m is the number of sentences in the input document is the concatenation operator hd i is the sentence representation of the i s sentence in the document and hb j is sentence representation of the j s sentence in the beginning part
each row and column of s is then ized by softmax which produces two new matrices s and s
hb b bidirectional attention is then calculated as a s hb where a represents document to beginning s tention and b represents beginning to document attention
finally we obtain the beginning aware sentence tions for each sentence in the document gd m where ai hd gd hd bi the underlying mechanism to integrate bidirectional tention in siatl and is very much the same except for the level of granularity in which attention operates on
summarunner operates on the level of ment so the bidirectional attention mechanism is calculated on the level of sentences between all document sentences and the beginning sentences i
e
hd is the sentence i resentation of the i s sentence in the document and hb j is sentence representation of the j s sentence in the beginning part
on the other hand siatl operates on the level of sentence thus bidirectional attention is calculated between words of the input sentence and words of the beginning part of the document i
e
hd i is the word representation of the i s word in the input sentence and hb j is word representation of the j s word in the beginning part
i ai hd i i s i t is available at github
com amagooda summarunner coattention additional proposed modications bert embedding recently released et al
embeddings showed the ability to outperform simply using shallow word embeddings
moreover it helped pushing the state of the art for numerous tasks within the nlp community
in this work we integrate bert embeddings within the summarunner model
instead of initializing word embeddings randomly bert embeddings are used
the model embedding layer is initialized with bert embedding and froze during the ing phase
keyword extraction attention mechanisms aim to weight tokens differently based on their importance
another modication we duce in this work is directed towards feeding the model with an extra signal
the extra signal in this case is keywords
the intuition behind feeding the model with keywords is pushing the model to give more attention to some specic words
the way we integrate keywords in summarunner model is by extracting keywords from each sentence si
then separately encode the keywords into hidden states ing bilstm hkwi nkwi where nkwi is the ber of extracted keywords from sentence si
the last den state is then used to represent all the keywords hkw
a new sentence embedding hdkw is then formed by directly concatenating the original document aware sentence sentation and the keywords representation
j j i i hd hdkw i hkw i experiments to verify our hypotheses and validate the utility of our proposed modications we conducted a number of iments
our experimental designs address the following hypotheses hypothesis attending to the beginning of a discussion thread would help extractive summarization models to select more salient sentences
hypothesis non auto regressive models such as siatl might be more suitable for thread discussion summarization compared to auto regressive models such as summarunner
hypothesis adding additional features such as contextual embeddings e

bert and keywords can give summarization models a boost in performance
hypothesis attend to the beginning is transferable to different forms of text other than discussion threads
lsa kmeans
as part of two lsa vector spaces were used first a vector space trained on a part of wikipedia
second a vector space trained using the forum discussion dataset
scikitlearn python package was used to produce lsa vector spaces of dimensions
the lsa baseline summarunner
we summarunner model following nallapati zhai and zhou
to operate on forum discussion data comments are split into implemented

for j si for si in sentences using stanford sentence parser
all sentences are then concatenated into a single document


n si for si in cj d where si is the i s sentence is the concatenation operator cj is the j s comment and n is the number of comments
moreover to operate on msw dataset each document is also split into sentences using stanford sentence parser
summarunner used randomly initialized embeddings of size
the hidden state size of the lstm is
input sentences are truncated to tokens while shorter sentences are padded
the model is trained with batch size for epoch
we calculate rouge score over the development set on each epoch
later on the checkpoint with maximum rouge is used for testing
siatl
we used the implementation of siatl released by the
the model used embeddings of size dimensions
the hidden state size of the shared lstm is
the task lstm is of size
input tences are truncated to tokens while shorter sentences are padded
the model is trained with batch size for epoch
similarly we calculate the rouge score over the development set
later on the checkpoint with the maximum rouge score is used for testing
summarunner bidirectional att
and
the bidirectional attention mechanism integrated in marunner operates on the level of document
to conduct experiments on forum discussion data the beginning part is the rst comment which is the initial post in the thread
on the other hand during experiments on the msw dataset the beginning part is the rst n sentences in each document
in this work we used n
summarunner bert embedding
to tialize summarunner with bert word embeddings bert base uncased embeddings were
each word is represented by the concatenation of bert s last two layers which leads to a word representation of size
we tried combining different number of and for each word representation
we found that combining or layers performs better than using only the last layer
we decided to use only layers to reduce the number of model parameters
summarunner keyword extraction
to extract keywords we use rapid automatic keyword extraction rake rose et al
to identify keywords
for each sentence in the document keywords extracted and concatenated
each pair of sentence and corresponding concatenated keywords are then passed to summarunner as separate inputs
siatl bidirectional att
and
like summarunner siatl operates on the level of
com alexandra chron siatl
googleapis
com bert uncased
zip individual sentences
thus the bidirectional attention mechanism integrated operates on the level of words
the to conduct experiments on forum discussion data beginning part is all the words from the initial comment in the thread
on the other hand during experiments on the msw dataset the beginning part is all the words from the rst n sentences in each document
in this work we used n
results on forum dataset table presents summarization performance results for the non neural extractive baselines for the original and proposed variants of the two summarization models marunner and siatl and nally for the highest score reported by tarnpradab liu and hua
following tarnpradab liu and hua and other recent work performance is evaluated using and rouge l r l lin on
summarization model r l baselines summarunner tarnpradab best sumy lsa kmeans discussions lsa kmeans wikipedia summarunner basic siatl self attention bidir
att
bert keywords kws bidir
att
kws bert kws bert bidir
att
bert bidir
att
kws bidir
att
self att
bidir
att














siatl





























table rouge results
italics indicates outperforms all baselines
boldface indicates best result over all models
underlining indicates best result within model group the motivation for using bidirectional attention nism is our hypothesis
table supports this esis
all rouge scores for summarunner and siatl that involves attending to the beginning by using tional attention mechanism rows and form their corresponding counterpart without using tional attention rows and respectively
our ond hypothesis is non auto regressive models might be more suitable than auto regressive ones for discussion marization
table shows that using non auto regressive model siatl indeed improve rouge scores compared to the auto regressive model summarunner
in rows and we see that siatl improved scores from
to

similarly and r l are also improved from
to
and from
to
respectively
additionally siatl introduced a new sota with a huge improvement in rouge scores compared to the previous work using chical attention rows and in which improved by

improved by
and nally r l proved by

we also see the same benets of ing to the beginning for siatl model compared to using only the self attention mechanism i
e
original model ing only bidirectional attention or combining both attention mechanisms self and bidirectional boost rouge scores rows and our next hypothesis is that enriching models with additional features such as contextual embeddings or words would boost the performance
for these experiments we only used summarunner model since it still has a room for improvement to catch up with the siatl model
table shows that our third hypothesis is a valid one but not for all cases
it shows that while adding contextual embeddings by itself or adding keywords by itself helps the model
bining contextual embeddings with keywords tends to harm the model
we can see that adding keywords to both variants of summarunner original and with bidirectional tion introduces a slight improvement over rouge scores rows and
where improved from
and
to
and
respectively
improved from
and
to
and
and r l proved from
and
to
and

larly adding bert contextual embedding introduces a good improvement over rouge scores for both variants of marunner rows and
where improved from
and
to
and
respectively
improved from
and
to
and
and r l improved from
and
to
and

surprisingly adding both features bert and keywords tends to be harmful to the model rows and
further analysis is still needed to reach a solid conclusion for such behavior
results on msw dataset table presents summarization performance results for the original and proposed variants of summarunner for the best performing variant of siatl
the motivation behind conducting experiments on the msw dataset is to validate our last hypothesis
we can see that table clearly shows that our hypothesis is a valid one
it shows that tending to the beginning of a document helps selecting more salient sentences not just for discussion threads but even for generic textual documents
similar to the results on the cussions dataset we can see that attending to the beginning through a bidirectional attention mechanism boosts rouge scores rows and
additionally we can see that ing bidirectional attention with bert embeddings further improves the performance rows and
discussion analysis unlike its promising performance on discussions table siatl performed poorly on msw dataset ble
surprisingly it was only able to outperform the baseline
through analyzing different criteria of the generated output for summarunner and siatl over the model summarunner bidir
att
bert bidir
att
bert siatl self att
bidir
att










r l




table rouge results over msw dataset
two used datasets
we observed that siatl tends to erate longer summaries compared to summarunner and this most likely due to its non auto regressive nature
marunner on the other hand tends to generate shorter summaries
table shows the average and standard viation of the number of sentences generated using marunner and siatl model compared to the human notation
it shows that for the forum discussion dataset the expected summary length is sentences
for the same dataset the siatl model generates summaries of length sentences while summarunner generates summaries of length sentences
this can justify the superior mance of siatl compared to summarunner on the rum discussion dataset
on the other hand we can notice that the expected summary length for the msw dataset is sentences
for the same dataset summarunner tently generates shorter summaries compared to siatl of lengths
compared to respectively
it is clear that the huge difference in the length of the summary between the human and siatl generated is the reason siatl performs on the msw dataset
a potential solution for the siatl model would be by adding a nal post processing step e

clustering redundancy reduction


the rule of the post processing step would be slightly ltering the generated summary and help to pick a number of sentences close to the average number humans select
model human summarunner siatl forum discussions avg

std


msw avg


std


table average and standard deviation of the number of sentences generated from each model and the human lected sentences conclusion future work we explored improving the performance of neural extractive summarizers when applied to discussion threads by ing to the beginning of the text i
e
initial comment post through a bidirectional attention mechanism
we showed that attending to the beginning of the text improved rouge scores of different models and different variants of these models
we also showed the applicability of using a recent sentence classication model siatl for extractive marization and introduced a new sota rouge score on the trip advisor forum discussion dataset
additionally we luo w
liu f
and litman d

an improved phrase based approach to annotating and summarizing in proc
of coling the dent course responses
international conference on computational tics technical papers
nallapati r
zhai f
and zhou b

summarunner a recurrent neural network based sequence model for tractive summarization of documents
in thirty first aaai conference on articial intelligence
paulus r
xiong c
and socher r

a deep inforced model for abstractive summarization
in tional conference on learning representations
rose s
engel d
cramer n
and cowley w

tomatic keyword extraction from individual documents
text mining applications and theory
see a
liu p
j
and manning c
d

get to the point summarization with pointer generator networks
in proc
of the annual meeting of the acl volume long papers volume
seo m
kembhavi a
farhadi a
and hajishirzi h

bidirectional attention ow for machine comprehension
in proc
of the international conference on learning sentations iclr
sutskever i
vinyals o
and le q
v

sequence to sequence learning with neural networks
in advances in neural information processing systems
tarnpradab s
liu f
and hua k
a

toward tractive summarization of online forum discussions via archical attention networks
in the thirtieth international flairs conference
wang k
quan x
and wang r

biset directional selective encoding with template for abstractive summarization
arxiv preprint

xiao w
and carenini g

extractive summarization of long documents by combining global and local context
arxiv preprint

yang m
qu q
shen y
liu q
zhao w
and zhu j

aspect and sentiment aware abstractive review summarization
in proc
of the international conference on computational linguistics
showed that attending to the beginning of the text is not ited to datasets in the form of discussion threads
we showed that it is transferable to more generic forms of text in which we can attend to the rst n sentences of the text similar to attending to the initial post comment in discussion threads
lastly we showed that the utility of attending to the ning is constant regardless of the used model or dataset
integrating bidirectional attention always introduces an provement in rouge scores
future plans include trying more generic datasets such as news to further verify the ity of attending to the beginning
further experimenting with the siatl model on other datasets as it showed promising results when used as extractive summarizer
we also plan to try extending the siatl model with a post processing step to enforce more control over the output length
we also plan to try different values for n the number of sentences as tial part from generic documents
references bahdanau d
cho k
and bengio y

neural chine translation by jointly learning to align and translate
arxiv preprint

chronopoulou a
baziotis c
and potamianos a

an embarrassingly simple approach for transfer learning from pretrained language models
in proc
of the ference of naacl hlt volume long and short papers
devlin j
chang m

lee k
and toutanova k

bert pre training of deep bidirectional transformers for guage understanding
arxiv preprint

gehrmann s
deng y
and rush a

bottom up abstractive summarization
in proc
of the conference on emnlp
jha r
bi k
li y
pakdaman m
celikyilmaz a
bodev i
and mcdonald k

artemis a novel tation methodology for indicative single document rization
arxiv preprint

jung t
kang d
mentch l
and hovy e

earlier is nt always better sub aspect analysis on corpus and system biases in summarization
in proc
of the conference on emnlp and the ijcnlp
li m
zhang l
ji h
and radke r
j

keep ing summaries on topic abstractive multi modal meeting summarization
in proc
of the annual meeting of acl
florence italy acl
li j
li h
and zong c

towards personalized review summarization via user aware sequence network
in proc
of the aaai conference on articial intelligence ume
lin c


rouge a package for automatic evaluation of summaries
text summarization branches out
liu y
and lapata m

text summarization with trained encoders
in proc
of the conference on ical methods in natural language processing and the international joint conference on natural language cessing emnlp ijcnlp

