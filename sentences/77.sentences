g u a l c
s c v
v i x r a tsinghuauniversity beijing chinaabstractneuralmodelshaverecentlybeenusedintextsummarizationincludingheadlinegener ation
themodelcanbetrainedusingasetofdocument headlinepairs
however themodeldoesnotexplicitlyconsidertopicalsimilaritiesanddifferencesofdocuments
wesuggesttocategorizingdocumentsintovarioustopicssothatdocumentswithinthesametopicaresim ilarincontentandsharesimilarsummariza tionpatterns
takingadvantageoftopicinfor mationofdocuments weproposetopicsen sitiveneuralheadlinegenerationmodel
ourmodelcangeneratemoreaccuratesummariesguidedbydocumenttopics
wetestourmodelonlcstsdataset andexperimentsshowthatourmethodoutperformsotherbaselinesoneachtopicandachievesthestate of artperfor mance
includingheadlinegeneration isanimportanttaskinnaturallanguageprocessing
itistypicallychallengingtocapturethecoreinfor mationofadocumentandcreateaninformativebutbriefsummaryofthedocument
mostexistingtextsummarizationapproachescanbedividedintotwocategories extractiveandgener ative
umentandreorderthemintoacompactsummary
duetothelimitationofvocabularyandsentenceindicatesequalcontribution

edu

edu
cnstructure itisextremelydifcultforextractivemod elstogeneratecoherentandconcisesummaries
generativemodels ontheotherhand aimatcom prehendingadocumentandgeneratingthesummarynotnecessarilyhavingappearedintheoriginaldoc ument
recentyearshavewitnessedthedevelopmentofsequence to

andthengenerateanoutputsequenceaccordingly
theadvantageofneu ralmodelsisthatthesemodelslearnasemanticmappingdirectlyaccordingtopairsofdocument headlinesequenceswithoutdesigninghand craftedfeatures

becausethesemodelscanexiblymodeldocumentsemanticsfrominternalwordsequenceswithinthedocument
nevertheless muchexternalinformationaboutdocumentsmayalsoplayimpor tantrolesfortextsummarization
forexample doc umentsusuallygroupintovarioustopics andthedocumentswithinacertaintopicmayexhibitspe cicsummarizationpatterns
forexample adocu mentabouteconomyisusuallysummarizedinclud locationandplaceoftheevent
inthispaper weproposetoincorporatetopicin formationofdocumentsintoneuralmodelsfortextsummarizationandproposetopic
morespecically
andintroducethetopiclabelsinneuralmodelstobuilduniqueencodersanddecodersforeachtopicrespec tively
inthisway topicnhgcaneffectivelyiden tifythecorrespondingcrucialpartsinadocumentguidedbyitstopicinformation andareexpectedtogeneratewell focusedheadlines
inthispaper weevaluateourmodelonalarge
experimentresultsshowthatourmodelsignicantlyoutperformsotherbaselinesystems
moreover itconsistentlyperformsthebestoneachindividualtopic whichprovesthestatisticalsignicanceandrobustnessoftopicnhg

basedonwhichagrudecodergeneratesaconciseheadline











thet
whencalculatinght itusesupdategateztandresetgaterttoimprovetheperformanceonlongsequences
thegatesarecom putedaszt
thenitcomputescandidateoutputhtandnaloutputhtasht

decodernhgincludesanencodertoencodeinputtextxintoafeaturevectorvandadecodertogeneratehead lineybasedonv
theattentionmechanismcanim

itassignsdifferentfeaturevectorsvtfordifferentstepsofthedecoder
fig

themodelgeneratestheoutputsequencefollowingamarkovprocess



nhgwithattentionmechanism





t
natedtogethasht
thefeaturevectorvistheaverageofoutputh
for mally

fort thunit

attentionthefeaturevectorvremainsidenti
theattentionmechanismdeter minesdifferentfeaturevtfort

forshorttextx summaryyandtopiclabell l


tionfeatureforadocument



arethedirichletpriorsontheper documenttopicdistributionandper topicworddis tributionrespectively

forsimplicity weassigneachinputsequencewithexactlyonetopicasl d



inourmodel thetopiclabeloftheinputshorttextwillaf fectweightmatricesinencoder decoderandatten tionlayer
forktopics
kdifferentencoders decodersandattentionlay ers
withkdifferentencoder wegeneratekrepre



thenweselectarepresentationaccordingtothetopiclabelloftheinputsequence


k





thetrainingofourmodelistime consuming
foreachtopic
soweusetheparameterstrainedintheconven tionalnhgmodeltoinitializeourmodel
specic
usinginitializationalsomakesourmodelmoregeneralthateachpartofourmodelwillrstlybetrainedbyalargesetofgeneraltextsum mariesandlaterbetrainedtogeneratetopic specicones
icnhg
dia

anteethequalityofdata

iiandpart iiihasahuman labeledscore reectingtherelevanceofthesummary

datainpart anddatainpart
inourexperiment weusepart iasourtrainingdata andweusepart
notethatwetakechinesecharactersasinputtoavoiderrorscausedbywordsegmentation



wemanuallymarkthesetopicsasjob economy accident politicsandtechnology

topickeywordsjobundergraduate graduate pho tographer researchereconomyrmb usd realty investor company manager ipoaccidentsuspect police court idcard bus taxi highwaypoliticsstatedepartment authority civilservants urbanizationtechnologyinternet consumer smartphone e business topicandkeywords
topicpart ipart iipart samplesintopic


ta fevaluationonthesemod els
byintroducingtopics
oneachtopic wecompareourmodelagainstthebaseline










rouge






























rouge topicnhg


fig

inthisexample topic nhgperformsmuchbetterthanthebaseline
thebaselineconsiderstherstsentenceoftheinputasitsmainpoint
inmostcases thisassumptionisprobablytrue
sothebaselinelearnsageneralreg ularityinsummarizationthattherstsentenceisimportant
however therstsentenceofaweiboconcerningpoliticsusuallytalksaboutaconference whilethecontentoftheconferenceinthefollowingsentencesismoreimportant

weibo text this afternoon the meeting of beijing s session of the national people s congress held the first press conference focusing on people s house problem
beijing s economical departments with an average of per square meter will become history
after solving all the waiting families no new economical departments will be built
human notation economical departments will become history this year
baseline the meeting of beijing s session of the national people s congress held a press conference
topicnhg the economical department in beijing will become history
comparingtopicnhgwithbaseline
weproposetopic sensitiveneuralheadlinegeneration
theexperimentsprovethattopicisanimportantfeatureinheadlinegenerationtasks
however ourmodelisrelativelysimple
itshighcostintrainingpreventsitfromhandlingmoretopics
inthefuture dleplentyoftopics


referencesdzmitrybahdanau kyunghyuncho andyoshuaben gio

neuralmachinetranslationbyjointlylearningtoalignandtranslate


fangzezhubaotianhu qingcaichen

lcsts alargescalechineseshorttextsummarizationdataset


davidmblei andrewyng andmichaelijordan

latentdirichletallocation
thejournalofma
junyoungchung caglargulcehre kyunghyuncho andyoshuabengio

empiricalevaluationofgatedrecurrentneuralnetworksonsequencemodel ing




newmethodsinautomaticextracting

jiataogu zhengdonglu hangli andvictorokli

incorporatingcopyingmechanisminsequence to sequencelearning


quocvleandtomasmikolov

distributedrepre sentationsofsentencesanddocuments


tomasmikolov martinkaraat lukasburget jancer andsanjeevkhudanpur

recurrentneuralnetworkbasedlanguagemodel
ininter speech
alexandermrush sumitchopra andjasonwe ston

aneuralattentionmodelforab stractivesentencesummarization


ilyasutskever oriolvinyals andquocvle

sequencetosequencelearningwithneuralnetworks
inadvancesinneuralinformationprocessingsystems

