a hybrid word character model for abstractive summarization chieh teng chang chi chia huang chih yuan yang and jane yung jen hsu department of computer science and information engineering national taiwan university taipei taiwan
com yangchihyuan
ntu
edu
tw abstract automatic abstractive text tion is an important and challenging search topic of natural language ing
among many widely used languages the chinese language has a special erty that a chinese character contains rich information comparable to a word
isting chinese text summarization ods either adopt totally character based or word based representations fail to fully exploit the information carried by both representations
to accurately capture the essence of articles we propose a hybrid word character approach hwc which preserves the advantages of both based and character based representations
we evaluate the advantage of the proposed hwc approach by applying it to two ing methods and discover that it generates state of the art performance with a gin of rouge points on a widely used dataset lcsts
in addition we nd an sue contained in the lcsts dataset and offer a script to remove overlapping pairs a summary and a short text to create a clean dataset for the community
the posed hwc approach also generates the best performance on the new clean sts dataset
introduction text summarization aims to create a short uent and effective summary from a long text document
since the rapid growth of information stored in the textual form in digital documents summaries greatly help address the amount of text data able online for searching useful information and consuming relevant articles
text summarization covers various types
general summarization proves the effectiveness of indexing and reduces the bias caused by humans
query focused marization takes preferences into consideration to satisfy individual needs of information
document summarization aims to generate maries across multiple documents about the same topic
extractive summarization combines a group of informative pieces of text from the source out changing them and abstractive summarization generates entirely new sentences by absorbing the information contained in the source text moreno gambhir and gupta
the recently rapid development of neural works brings signicant advances into text marization especially from the family of tional sequence to sequence tures sutskever et al
bahdanau et al
which generates promising performance for abstractive summarization rush et al
nallapati et al

they learn an internal guage representation from a large number of amples to generate summaries similar to the ones made by humans
since the used language is a critical factor of text summarization many studies have been done for chinese due to its large number of users tinuous use in the long history and widespread inuence in east asia
chinese is signicantly different from any european languages especially in its character representation and word tation
as the old chinese characters were oped thousands of years ago with a monosyllabic structure and used as words the derived modern chinese inherits a bank of characters numbering tens of thousands and composing words from ther single or multiple characters
although tuations are widely used in modern chinese to arate sentences there is still no delimiter within a sentence to isolate words
word segmentation is an error prone process and it largely affects the result of automatic summarization ayana et al
ma et al

since chinese p e s l c
s c v
v i x r a ters own semantic meanings although polysemous many existing studies use character based sentation to simplify the effort and prevent the certainty of segmentation hu et al
chen et al
li et al
ayana et al
ma et al
li et al

although a few existing methods test based representation hu et al
gu et al
their performances are only slightly proved or even worse
they use word based resentation on both source and target text but it is doubtful since the approach relies on a site of sufcient training samples in terms of curred words
however the lengths of source and target text of a text summarization sample are initely asymmetric
summaries as the targets are surely shorter than their source articles and thus it is questionable whether the used dataset is large enough to provide a satisfactory size of target text
another issue is the memory limitation
since a chinese word is composed of either a single or multiple chinese characters representing a given chinese text dataset using words instead of acters means signicantly increasing the lary size
as a text summarization algorithm plemented in an encoder decoder framework and running on a gpu for fast execution the size of its vocabulary will be restricted by a gpu s ory capacity
to the best of our knowledge gpu platforms to expand overall memory ity for training text summarization models are still being developed
existing methods which use word based representation for the decoder have to use a selected subset of the complete vocabulary bank extracted from the target text usually the high frequency words
however once the size of the vocabulary subset is not large enough many low frequency words in the text will be replaced by the unknown token and lose their messages which results in incomplete summaries and low rouge scores
to address the problem we propose a hybrid word character hwc approach which uses brid embedding units for an encoder and a decoder to preserve the advantages of both word based and character based representations
since an encoder does not contain a softmax layer its computational load and memory requirement are far less than a decoder jean et al

thus it is feasible to apply a word based representation on the coder and use a large vocabulary bank
mental results show this approach works well on two encoder decoder summarization methods and generates state of the art performance on a widely used chinese text summarization dataset
related work extractive and abstractive summarization
merous automatic summarization methods have been proposed in the literature and the formats of the generated summaries categorize existing ods into two classes extractive and abstractive
while extractive summarization selects keywords or sentences from the original text and arranges them to form a summary luhn erkan and radev mihalcea and tarau cheng and lapata abstractive summarization erates a brief version of the original text to serve its information content ansd overall ing
extractive summarization is developed lier since it highly simplies text summarization into text partition and selection and abstractive summarization is heavily studied recently due to its challenge and practicability
early studies on abstractive approach include statistical machine translation techniques banko et al
knight and marcu and deletion and compression methods cohn and lapata filippova et al

with the rapid spread of neural works many recent studies build their models in an encoder decoder framework especially the tentional model rush et al
lapati et al
for its promising performance
chinese text summarization datasaet
in tion to english which is the rst language studied for text summarization the large number of nese users motivates the studies to explore its own language features
the rst compiled chinese text dataset available for text summarization is chinese gigaword graff and chen which contains a comprehensive archive of newswire text data quired from chinese news sources such as tral news agency of taiwan and xinhua news agency of beijing over several years
although the corpus is impressive for its richness it is ther free of charge and nor thoroughly categorized
there is a lack of human evaluation on the quality of the summaries titles of news reports
on the contrary the lcsts large scale chinese short text summarization dataset hu et al
is created for academic research
its text sources are still news reports and titles collected from a nese microblogging website
since they are croblogging articles their length is restricted der a short text s limit and make the collected ticles consistent
the dataset provides predened training and test subsets manually labeled ity indexes on its test summaries free assess and long term maintenance
open source implementation
the rapid opment and signicant advances of many puter science elds have motivated the trend of publicly available algorithm libraries such as opencv bradski for computer vision and openmnt klein et al
for neural machine translation
such platforms provide great nience to test new ideas reproduce results and optimize performance
since those libraries are well maintained new methods are soon available and easily called through a unied interface
searchers benet from them by reducing the fort of re implementation preventing the errors of misunderstanding and saving the time of ing experiments
due to the considerable merits we validate the proposed hwc approach using the openmnt library
language translation
text summarization and language translation are two distinct problems in the family of natural language processing
ever they share certain similar properties such as a representation of sequential data and a sion from text to tokens
since the two problems are highly close it is possible to apply models veloped for one problem to the other i
e
ing the source and target languages in a tion problem by the source and target text in a inspired by the summarization problem
ca nt breakthrough in the translation problem wu et al
gehring et al
gehring et al
vaswani et al
we apply the posed hwc approach to a state of the art lation model and nd that the integrated method generates leading edge performance
proposed method as illustrated in figure the proposed hwc proach is used in an encoder decoder framework that the input articles are represented by words and the output summaries by characters
in this per we verify the effectiveness of the proposed hwc approach using a baseline method tional and a state of the art one former vaswani et al

figure the input and output formats of the proposed hwc approach on an encoder decoder summarization method
different from existing methods which use a xed type of the embedding units for both coder and decoder as shown in table the posed method uses word embedding to represent input articles and character embedding for output summaries
such a design is motivated by the observation in the chinese language words are more precise to provide information than ters because chinese characters are highly mous but characters are shorter and more exible than words
on the one hand for input data it is less ambiguous to represent articles using words than characters
in the lcsts
dataset we use for experimental validation there are words made up by mere characters
using word bedding units expands the limit of a vocabulary bank in terms of size to train an effective encoder to capture the meaning contained in the word lationship
on the other hand for output targets since summarized sentences are highly trated and it is common in chinese to shorten long phrases for convenience e

the international olympic committee is abbreviated by where the term olympic game is reduced to and committee to characters are more exible than words to resent the output sentences because of their nitely shorter length
experimental setup all of our experiments are conducted on a machine equipped with a
core cpu g ory and a high performance gpu nvidia ti
we use an open source implementation and the code released by the original authors to duce results of the and transformer vocabulary size word based copynet character based copynet dgrd ac abs distraction wean encoder usage decoder usage





table vocabulary sizes available from the sts
dataset in different representation units and the used by existing methods
the low portion of the used vocabulary in a word based tion tested by existing methods is caused by the limitation of gpu memory
the words are mented from documents using the jeiba utilities
methods respectively
we get the rouge scores of rnn and rnn context methods on the lcsts
dataset from the original authors
to train the and transformer methods we low the existing methods rnn and copynet to split the part i set into two distinct training and idation sets
since their original authors only port random splitting rather than an explicit ting mechanism we use a long standing random number generator with ve seeds to to select articles as the validation sets and use the remaining as training sets and report the mean rouge scores

dataset on conduct experiments we the lcsts dataset hu et al
to evaluate the posed method
this dataset contains a large number of short chinese news articles with made headlines as the short summaries collected from sina a chinese microblogging site
this dataset is composed of three parts as shown in table
part i contains a large number of pairs of articles and headlines but no annotation
parts ii and iii contain not only text data but also human labeled scores measuring the quality of summaries in terms of their relevance to the source articles
the difference between parts ii and iii are the numbers of annotators to create the scores which is for part ii but for part iii
the relevance scores range from to the larger the more relevant
for a fair comparison we follow the same split setting of existing methods hu et al
gu et al
chen et al
li et al
ayana et al
ma et al
li et al
to
weibo
part i ii iii version
and

clean

and
clean articles scores n a table the statistics of the lcsts datasets in three different versions
the version
is updated from the version
by replacing articles in part i which re appear in part iii with newly collected articles
thus the number of part i articles does not change
the version
clean is further rened from
using a strict criterion to remove articles in part i which are highly similar to any articles in part iii and sharing the same summaries
use part i as the training set and part iii s relevance subset scores equivalent to or greater than as the testing set
lcsts

by examining the rst released sion
of the lcsts dataset we found that its part iii contains a high ratio of articles repeated in its part i
we reported this problem to the authors and received the response that they released an correct dataset which failed to lter out common articles in parts i and iii
to deal with the problem they re released the dataset they actually used for their experiments which replaced overlapping articles in part i with newly collected ones and assigned it a new version

lcsts
clean
after scrutinizing lcsts
we nd the cleanup is not complete
many items in lcsts
s part i are almost the same to items in part iii in terms of exactly the same summaries and highly similar articles only differing on a few characters at the end of the articles to show the name of source newspaper
as an example shown in table the name of source newspaper shanghai morning post does not contribute to the message carried in the article
since the sue is likely to weaken the dataset we remove the highly repeated items from part i and name the amended dataset lcsts
clean
in order to evaluate the proposed method on a well made dataset we remove all highly ping items from the part i split of lcsts
and name it lcsts
clean
the script to ate lcsts
clean is in github repository for reproducing our experimental results by other searchers
summary article table an example of highly overlapping items in the lcsts
dataset
an item in the part i split differs from another item in the part iii split only on the presence of the term shanghai morning post at the end of the cle

evaluation metrics we adopt rouge lin metrics for uation which has been widely used for tive summarization
they measure the quality of summaries by computing the overlap of generated and reference ones
for a fair ison we report gram bigrams and rouge l longest common sequence scores for all compared methods

compared methods rnn and rnn context hu et al
are two similar rnn based methods except for a context generator included
in the simpler architecture without a context generator its decoder uses the rnn encoder s last state as the input data in the complexer architecture a context generator is nected with all gated recurrent units hidden states and uses generated context to generate summaries
copynet gu et al
integrates the copying mechanism into the attentional model in order to combine selected subsequences from the input sequence to generate an output sequence
distraction chen et al
is a framework which distracts a document into ent regions by their content in order to better grasp the overall meaning of the input document
drgn li et al
is an attentional model equipped with a latent structure modeling component
mrt ayana et al
employs the minimum risk training strategy on an attentional model
wean ma et al
is based on an tional model which generates summaries by querying distributed word representations with an attention mechanism in the decoder
ac abs li et al
employs an actor critic approach originally developed for reinforcement learning on an attentional model
is the method using the simplest tentional model without any additional component
we use an implementation available from the openmnt system klein et al
and evaluate it as a baseline
transformer vaswani et al
is a newly veloped encoder decoder method which uses tention mechanisms rather than complex recurrent or convolutional neural networks
we adopt its model for chinese abstractive summarization and rst report its performance

preprocessing and hyperparameters we adopt the same approach as rnn context and copynet to segment input articles into words ing the jieba segmentation
about the hyperparameter of vocabulary size we do iments on several numbers as shown in table in the same ranges used by rnn context and net and some large numbers to assess the extent
the numbers and are the amount of words in the training sets whose occurrence quency is greater than one
about the eters used the method we cally set the numbers of embedding dimension and hidden layers both as
we use adagrad duchi et al
as the optimizer and set the initial learning rate to
and dropout rate to

we set the beam size to for all decoders in our periments
for the transformer method we use the default parameters of an implementation able in the opennmt py
results and discussion by evaluating the proposed model on the sts dataset we nd that the dataset s original version
contains overlapping items and rm the issue by its authors
therefore we do experiments for fair comparisons with ing methods not only on its original version but also on two new versions
and
clean to fairly evaluate the performance
the rouge scores on the three datasets are shown in tables and respectively
on the original version the method generates the best score with a signicant margin over existing ods
using the same method on the lcsts

python
org pypi
cc encoder decoder measure vocab size rouge l method copynet gu et al
hu et al
base char char word char distraction chen et al
char dgrd li et al
char mrt ayana et al
char wean ma et al
char ac abs li et al
char char max vocab size word word max vocab size word char transformer char transformer max vocab size word vocab size base char char word char char char char char char char char char char char char char














n a














in the cases of maximal table scores of three rouge measures on the lcsts
dataset
vocabulary size we report the numbers of vocabularies available in training splits rather than the overall dataset training plus validation so that the numbers may be slightly smaller than the ones reported in table
the score of the method is given by the authors but the other two rouge scores are unavailable
encoder decoder measure vocab size rouge l method rnn hu et al
max vocab size base char word char word char char word vocab size base char word char word char char char













table scores of three rouge measures on the lcsts
dataset
we report the scores of the rnn and methods here rather than in table due to the authors addendum as explained in section

encoder decoder measure vocab size rouge l method base char char max vocab size word max vocab size word char transformer char transformer max vocab size word vocab size base char char char char char char char













table scores of three rouge measures on the lcsts
clean dataset
n a



























clean dataset the rouge scores decline ably and it shows the importance to evaluate ods on a well made dataset
vocabulary size
as shown in tables and larger vocabulary banks lead to higher rouge scores
on the lctst
dataset the char based method generates higher rouge scores over existing methods merely by ing its vocabulary bank
on the lctst
and
clean datasets the char based method also benets from large vocabulary banks
adopting the hwc approach the method with a word based encoder uses larger vocabulary banks and generates better rouge scores
on both lcsts
and
datasets the transformer method generates the top mance by using the hwc approach with a large vocabulary bank
the effects of the hwc approach
on all of the three datasets lcsts

and
clean the method generates higher rouge scores over the method
such improvements are also observed about the transformer method on the lcsts
and
clean dataset
table outlines the enhancement and shows that overlapping data result in larger creases of the rouge scores
it indicates that the model is an aggressive learner which effectively adapts its hidden states to its training data
thus it generates very high rouge scores as the test data overlap the training ones
rouge l lcsts
transformer lcsts
clean transformer











table the rouge scores improved by ing the hwc approach to two encoder decoder methods
analysis of generated summaries
in order to show the reason of the success caused by the hwc approach we show four example summaries erated by the top two methods transformer and in table including their source articles and man made references
the four examples show that the method better catches the major messages tained in the source articles which are lars in article private trusts in article executive meetings and li keqiang in article and xian ge qing xiaomi and can not compare with us in article
in contrast the character based transformer method tends to copy a full sentence from its source article but miss the point
for ample it copies i love football but my family nancial status is supported by me
in article but misses the key term dollars what is the highest level of the rich in article but misses the key term private trust and combining ternet boxes such as xiaomi and routers can not compare with us
but misses the key term xian ge qing
since the proposed hwc approach helps detect key terms it leads to higher rouge scores
training time
the proposed hwc approach has the advantages of not only improving formance but also increasing the training ciency
we show the speedup as a chart in ure from the experiments shown in table ducted on the lctst
clean dataset
their cabulary sizes are all of the maxima except for the method due to a memory limitation
both of the and former methods use fewer epoches to generate high scores by applying the hwc proach
please note the unit of the horizontal axis is a minute rather than an epoch which means the actual execution time has been taken into ation
for the method the hwc proach increases the mean training time per epoch from to minutes but for the transformer method it reduces the time from to utes
such result is caused by the tension between two factors that a word based representation ens the length of input sequences and thus reduces the level of training difculty but it also increases the load of updating vocabulary embedding in the encoder due to a larger vocabulary size
figure training time and performance ison of two evaluated methods with and without the proposed hwc approach
conclusion and future study in this paper we propose a hybrid representation approach to improve the performance of text marization methods in an encoder decoder work
experimental results demonstrate that the kick the ball when opening the eyes fans will be ecstatic about standing out in the asian cup when blindfolded the eyes china has won the place in the world after brazil but the situation is very different
the blind footed veteran lin jinbiao did not go to the training camp this time although it covers the accommodation during the training time the salary is only dollars per day
i love football but my family nancial status is supported by me
reference china blind footed player who has won the place in the world the salary is only dollars per day
transformer lin jinbiao i love football but my family nancial status is supported by me
the football veteran in china will not go to the training camp because the salary is only dollars
what is the highest level of the rich money is not in their share but it can be controlled by themselves
in fact the biggest characteristic of private trusts is that they can cross life cycles and help customers manage personal wealth for life inherit and distribute wealth across generations
their unique privacy secrecy and stability are among many wealthy individuals and family business pursuit in china
reference private trusts create a new bridge for wealth inheritance
transformer what is the highest level of the rich private trusts cross life cycles
besides visiting or participating important events premier li keqiang hosts the state council executive meeting on wednesday
there are executive meetings times on wednesday in months
if the subject of these meetings are strung together with a red line it shows not only the trajectory of changes in major policies and policies but also the governance and policy of the current government
reference analyze li keqiang s way of governance by executive meetings
transformer state council executive meetings are strung together with a red line
analyze li keqiang s executive meetings in months
for xian ge qing which is about to rename to zhongke cloud network meng kai is lled with expectation
others say that we work on big data and its concept but i believe that the upgrading of the radio and tv network will be equivalent to combining radio and tv with nuclear weapons
combining internet boxes such as xiaomi and routers can not compare with us
reference xian ge qing works on cable tv combining xiaomi s products can not compare with us
transformer meng kai combining internet and routers can not compare with us
chairman of xian ge qing talks about upgrading radio and tv networks xiaomi and routers can not compare with us
table example summaries generated from the
clean dataset
proposed approach clearly generates state of in addition we nd a few art performance
rors in a widely used dataset and provide a script to polish it
beyond the improved performance it may be a better representation by ing part of speech tagging into the proposed proach
we are also interested in the applicability of the proposed approach to other natural language processing problems such as dialogue generation and machine translation
references ayana et al
ayana shiqi shen zhiyuan liu and maosong sun

neural headline ation with minimum risk training
arxiv preprint

bahdanau et al
dzmitry bahdanau kyunghyun cho and yoshua bengio

neural machine translation by jointly learning to align and translate
in iclr
banko et al
michele banko vibhu o
mittal and michael j
witbrock

headline tion based on statistical translation
in acl pages
gary bradski

the opencv brary
dr
dobb s journal of software tools
chen et al
qian chen xiaodan zhu zhenhua ling si wei and hui jiang

based neural networks for modeling documents
in ijcai pages
cheng and jianpeng cheng and mirella lapata

neural summarization by extracting sentences and words
in acl pages
cohn and trevor cohn and mirella ata

sentence compression beyond word tion
in coling pages
duchi et al
john duchi elad hazan and yoram singer

adaptive subgradient methods for line learning and stochastic optimization
journal of machine learning research
erkan and gunes erkan and dragomir r
radev

lexrank graph based lexical trality as salience in text summarization
journal of articial intelligence research
filippova et al
katja filippova enrique seca carlos a colmenares lukasz kaiser and oriol vinyals

sentence compression by tion with lstms
in emnlp pages
gambhir and mahak gambhir and vishal gupta

recent automatic text summarization techniques a survey
articial intelligence review
gehring et al
jonas gehring michael auli david grangier and yann n dauphin

a convolutional encoder model for neural machine translation
acl
gehring et al
jonas gehring michael auli david grangier denis yarats and yann n
dauphin

convolutional sequence to sequence ing
icml
graff and david graff and ke chen

chinese gigaword
linguistic data consortium
gu et al
jiatao gu zhengdong lu hang li and victor o
k
li

incorporating copying in acl anism in sequence to sequence learning
pages
et al
baotian hu qingcai chen and fangze zhu

lcsts a large scale chinese short text in emnlp pages summarization dataset

jean et al
sebastien jean kyunghyun cho roland memisevic and yoshua bengio

on using very large target vocabulary for neural chine translation
arxiv preprint

klein et al
guillaume klein yoon kim tian deng jean senellart and alexander m
rush

open source toolkit for neural machine translation
in acl pages
knight and kevin knight and daniel marcu

statistics based summarization step in aaai iaai pages one sentence compression

et al
piji li wai lam lidong bing and hao wang

deep recurrent generative decoder in emnlp for abstractive text summarization
pages
et al
piji li lidong bing and wai lam
actor critic based training framework arxiv preprint
for abstractive summarization


chin yew lin

rouge a package for automatic evaluation of summaries
in ings of the workshop pages
hans peter luhn

the automatic creation of literature abstracts
ibm journal of search and development
et al
shuming ma xu sun wei li sujian li wenjie li and xuancheng ren

word embedding attention network generating words by querying distributed word representations for phrase generation
in naacl hlt
mihalcea and rada mihalcea and paul rau

textrank bringing order into text
in emnlp
nallapati et al
ramesh nallapati bowen zhou caglar gulcehre bing xiang al

tive text summarization using sequence to sequence rnns and beyond
in signll pages
rush et al
alexander m
rush sumit chopra and jason weston

a neural attention model for abstractive sentence summarization
in emnlp pages
sutskever et al
ilya sutskever oriol vinyals and quoc v
le

sequence to sequence in nips pages ing with neural networks

torres juan manuel torres moreno

automatic text summarization
john wiley sons inc
vaswani et al
ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan n gomez ukasz kaiser and illia polosukhin

in nips pages attention is all you need

wu et al
yonghui wu mike schuster zhifeng chen quoc v
le mohammad norouzi wolfgang macherey maxim krikun yuan cao qin gao klaus macherey et al

google s neural chine translation system bridging the gap between arxiv preprint human and machine translation



