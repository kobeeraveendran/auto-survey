query controllable video summarization jia hong huang university of amsterdam amsterdam netherlands j

nl marcel worring university of amsterdam amsterdam netherlands m

nl r a r i
s c v
v i x r a abstract when video collections become huge how to explore both within and across videos efficiently is challenging
video summarization is one of the ways to tackle this issue
traditional summarization approaches limit the effectiveness of video exploration because they only generate one fixed video summary for a given input video independent of the information need of the user
in this work we introduce a method which takes a text based query as input and erates a video summary corresponding to it
we do so by modeling video summarization as a supervised learning problem and propose an end to end deep learning based method for query controllable video summarization to generate a query dependent video mary
our proposed method consists of a video summary controller video summary generator and video summary output module
to foster the research of query controllable video summarization and conduct our experiments we introduce a dataset that contains frame based relevance score labels
based on our experimental sult it shows that the text based query helps control the video mary
it also shows the text based query improves our model formance

com jhhuangkay query controllable video summarization
ccs concepts computing methodologies artificial intelligence video summarization
acm reference format jia hong huang and marcel worring

query controllable video marization
in proceedings of the international conference on multimedia retrieval icmr june dublin ireland
acm new york ny usa pages



introduction video data is now ubiquitous in our daily life
most of the raw videos are too long and are containing redundant content
as a consequence the amount of video data people have to watch is overwhelming
this raises new challenges in efficiently exploring both within and across videos
video summarization helps people explore a video efficiently by capturing the essence of the video
learning what is essential depends on the information need of the user
yet traditional video summarization methods such as generate one fixed video summary for a given input video
hence they either create a video capturing all permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page
copyrights for components of this work owned by others than the must be honored
abstracting with credit is permitted
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission a fee
request permissions from
org
icmr june dublin ireland copyright held by the owner
publication rights licensed to acm
acm isbn






figure this figure shows the main idea of the controllable video summarization
a user gives a text based query which represents the desired video summary to trol a video summary generator to create a video summary based on the query
note that the input of the proposed method is both query and video
if two different queries are given with the same video input two different dependent video summaries will be generated
it is different from the traditional video summarization which only has a video input
for details of the video summary controller and video summary generator please refer to figure and
possible information needs and therefore yield limited reduction in time or they lose essential information for specific needs
having a fixed summary limits the effectiveness of video exploration
to make the video exploration more effective and efficient we will need a new specialized method that steered by the information need of the user is capable of generating various video summaries for a given video
we call this query controllable video rization
there are two main features of query controllable video summarization which complicate this task when compared to the well studied domain of conventional video summarization
one example is that query controllable video summarization has a based query input and a video input where the conventional video summarization only has a video input
so in query controllable video summarization we need to model the implicit relations or teractions between the input query and video
evaluating the ated video summaries is another challenge
previously researchers usually conduct human expert evaluation based on predefined rules or showing human experts two different video summaries and ing them to select the better one
human expert based evaluation methods for this task are problematic as these methods are expensive and time consuming because they rely on the ments of humans for each evaluation
in this paper we prefer to conduct automatic evaluation which is more efficient
there are different solutions to model the video summarization problem both supervised and pervised
for the automatic manvideosummarycontrollervideosummarygeneratorvideo summaryoutput module evaluation in this paper we model video summarization as a pervised learning problem
then we propose an end to end deep learning based model illustrated in figure to generate video maries depending on the text based input queries
to the best of our knowledge we are the first proposing an end to end deep model for query controllable video summarization
note that a non to end method for query controllable video summarization needs many preprocessing steps and in practice it will reduce the ciency of video exploration
the model consists of a video summary controller a video summary generator and a video summary put module
the controller uses text such as words phrases or sentences to describe the desired video summary
then the erator creates a video summary based on the implicit relationship between the text based description and the input video
the video summary output module then outputs a video summary based on a relevance score prediction vector
to build a query controllable system for videos using multiple information sources requires dedicated datasets and evaluation methods
we undertake such a study in this work
starting from an existing dataset we establish a new video dataset on which to build our models
with such a dataset in place it is the right time to research how to exploit deep learning based models with textual query inputs which are to steer the output
as mentioned above our proposed model takes a text based query and video as input so how to effectively fuse the multi modal features with minimum loss of information is one of the technical problems
it has been shown that in similar multi modal contexts the performance of models can decrease if feature fusion methods are improper how to solve this issue in general remains an open question
so we not only conduct experiments to see how the input query affects our model performance but also conduct experiments to see how the commonly used feature fusion methods affect our model performance
to foster the research community of video summarization we intend to publish the dataset and code
we demonstrate experimental results of the proposed end to end deep model and show that our proposed method is capable of ating query dependent and controllable video summaries for given videos
our experimental result shows that the text based input query helps control the video summary
it also shows that the based input query improves the performance of our summarization model
in the sense of accuracy
contributions
we propose a new end to end deep learning based model for a text visual embedding space for query controllable video summarization
we conduct detailed experiments to show that the text based query not only helps control the video summary but also improves the performance of the proposed end to end model
we introduce a query video pair based dataset based on the dataset proposed in for a query controllable video summarization task
the dataset contains videos and corresponding frame based relevance score annotations
related work in this section we discuss related work in terms of different ods and different datasets
we first discuss the two main types of methods of video summarization i
e
supervised and unsupervised
then we review the commonly used video summarization datasets

unsupervised video summarization unsupervised approaches for video summarization usually exploit hand crafted heuristics to satisfy some properties such as interestingness ness and diversity
in the authors propose a method based on color feature extraction from video frames and k means clustering
the authors of observe that the key visual concepts usually pear repeatedly across videos with the same topic
so they propose to create a summarized video by finding shots that co occur most frequently across videos
they develop a maximal biclique finding mbf algorithm to find sparsely co occurring patterns
in the authors introduce a space time video summarization method to extract the visually informative space time portions of input videos and analyze the distribution of the spatial and temporal tion in a video simultaneously
the authors of present a video summarization method for egocentric camera data
they develop region cues e

the nearness to hands gaze and frequency of occurrence in egocentric video and learn a model to predict the relative importance of a new region based on these cues
the thors of present a video summarization method to discover the story of a given egocentric video
their proposed method is capable of selecting a short chain of sub shots of video which depicting the essential events
in based on the modeling of the ers attention the authors propose a generic framework of video summarization
the framework without the fully semantic content understanding of a given video eliminates the needs of complicated heuristic rules in video summarization task and takes advantage of computational attention models
in the authors introduce a unified method for video summarization based on the analysis of structures and highlights of the video
their approach emphasizes the content balance and perceptual quality of a video summary at the same time
they also incorporate a normalized cut algorithm to partition a video into clusters and a motion attention model based on human perception to compute the perceptual quality of clusters and shots
the authors of develop a new method to extract a video summary that captures important particularities arising in a given video and generalities identified from a set of given videos simultaneously
the authors of introduce a method to learn a dictionary from the given video using group sparse coding
a video summary is then generated by combining segments that can not be reconstructed sparsely using the dictionary
in the authors propose a new formulation to perform video summarization from unpaired data
their model aims to learn mapping from a set of raw videos to a set of video summaries such that the distribution of the generated video summary is similar to the distribution of the set of video summaries with the help of an adversarial objective
also they enforce a diversity constraint on the mapping to ensure the generated video summaries are diverse enough visually
in the authors introduce an end to end reinforcement learning based framework to train their video summarization model
the work incorporates a new reward function to jointly account for diversity and representativeness of generated video summaries
the design of the reward function makes it not rely on user interactions or labels at all
the authors of propose a more based method with a specific objective for query adaptive video summarization
since the method is not end to end it will need many preprocessing steps before generating a video summary
so in practice it is inconvenient and not efficient for video exploration
this motivates us to propose an end to end deep learning based method
although many existing works model video tion task as an unsupervised problem and propose their methods to tackle it in general the performance of the unsupervised approach is worse than the supervised one

supervised video summarization the other type of approach for video summarization task is supervised
the learning of these ods is supervised by human expert labeled data i
e
ground truth video summaries
the authors of treat video summarization as a supervised subset selection task
they propose a probabilistic model for selecting a diverse sequential subset called the tial determinantal point process seqdpp
note that the standard dpp treats video frames as randomly permutable elements
the seqdpp heeds the inherent sequential structures in video data so it not only overcomes the deficiency of the standard dpp but also retains the power of modeling diverse subsets which is essential for video summarization
the authors of introduce a new video summarization method and focus on user videos containing a set of interesting events
the method starts by segmenting a given video based on a superframe segmentation tailored to raw videos
then according to the estimation score of visual interestingness per frame by using a set of low level mid level and high level features the method picks an optimal subset of superframes to generate a video summary
in the authors propose a new model to learn the importance score of global characteristics of a video summary
the models jointly optimized for multiple objectives is capable of generating high quality video summaries
in the authors duce a new probabilistic model built upon seqdpp to tackle video summarization problem
the period of a video segment where the local diversity is imposed can be dynamically controlled by the model
to get a well trained summarization model the authors develop a reinforcement learning algorithm to train the proposed model
the authors of formulate video summarization as a sequence labeling problem
they propose fully convolutional quence models to tackle the video summarization task
first they establish a novel connection between video summarization and semantic segmentation
second the adapted popular semantic mentation networks are used to generate video summaries
in the authors propose an improved sequential determinantal point process seqdpp model
in terms of modeling a new probabilistic distribution is designed to make when it is integrated into seqdpp the resulting model accepts the input of a user about the intended length of the video summary
in terms of learning a large margin algorithm is proposed to address the problem of exposure bias in seqdpp
in the authors propose a subset selection method that leverages supervision in the form of human created video maries to perform keyframe based video summarization
the main idea of this method is nonparametrically transferring structures of summaries from annotated videos to unseen testing videos
also the authors generalize the proposed method to sub shot based video summarization
the authors of cast a video summarization task as a structured prediction problem on sequential data
then they propose a new supervised learning technique incorporating long short term memory lstm to model the variable range dependencies entailed in the task
also they exploit domain tation techniques based on the auxiliary annotated video datasets to improve the quality of the video summary
in the authors propose a sequence to sequence learning model to tackle the video summarization problem
to complement the discriminative losses with another loss such as measuring whether the generated video summary preserves the same information as the original video they propose to augment standard sequence learning models with a retrospective encoder that embeds the predicted video summary into an abstract semantic space
then the embedding is compared to the original video s embedding in the same space
the authors of introduce a novel dilated temporal relational generative adversarial network dtr gan to tackle the frame based video summarization
dtr gan exploits an adversarial manner with a three player loss to learn a dilated temporal relational generator and discriminator
the authors introduce a new dilated temporal relational unit to enhance the capturing of temporal representation and then the generator creates keyframes based on the unit
the supervised methods are capable of learning useful cues which are hard to capture with hand crafted heuristics from ground truth video summaries
so they usually outperform the unsupervised models
that is the reason why we prefer to model video rization as a supervised learning problem in this paper

video summarization dataset comparison in this section we shortly introduce a commonly used video summarization dataset and do some comparison with the dataset used in this paper
to tackle the video summarization task the authors of propose a dataset named tvsum
it contains videos with categories and the corresponding shot level importance scores obtained via crowdsourcing
the categories are selected from the trecvid multimedia event detection med task and the videos five per category are collected from youtube by using the names of categories as search queries
from the search results videos are chosen based on the following criteria i the selected video should contain more than a single shot the title of video is descriptive of the visual topic in the video iii under the creative commons license iv the duration of video is around and minutes
the authors exploit amazon mechanical turk amt to collect responses per video and these responses are treated as gold standard labels
a participant from amt is asked to read the title of video first simulating a typical scenario of online video browsing watch the whole video in a single take provide an importance score to each of uniform length shots for the whole video denoting from not important to very important
the audio is muted to ensure the important scores are only based on visual stimuli
according to the authors experience a two second shot length is ate for capturing local context with good visual coherence
in the authors introduce another video summarization benchmark called summe consisting of videos covering holidays events and sports
the length of video ranges from to minutes and each video is summarized by to different people
the thors asked males and females to participate in making the dataset
given a video participants are asked to produce a video summary containing most of the important content in the video
they are allowed to watch cut and edit a video by using a simple table summary of commonly used video summarization datasets
based on this table we find that the proposed dataset is much larger than the other datasets and it contains two types of input modalities including video and text
the other two video summarization dataset only contains video data and the dataset size is not large
so the proposed dataset is unique
relevance scores in this work and important scores from tvsum are different referring to crowd sourced annotation subsection
name of dataset summe tvsum ours based on annotation type interval based shot and frame level scores frame based important scores frame based relevance scores content user videos number of videos input modality video youtube videos youtube videos video video text interface
the length of a video summary is required to range from to of the original video length
that is to ensure the input video is indeed summarized rather than being shortened slightly
the videos are shown randomly and the audio is muted to ensure the generated video summaries are only based on visual stimuli
regarding the evaluation of video summarization approaches viously researchers conduct the human expert evaluation in one of the following ways based on a set of predefined criteria such as the degree of redundancy counting the inclusion of predefined important content summary duration and so on
ii showing human experts two different video summaries and asking them to select the better one
the authors of claim that the above human expert evaluation methods are problematic as these methods are expensive and time consuming because they rely on judges of human for each evaluation
for example in the uation of the method requires one full week of human labor
both of the human expert evaluation methods help to tell which video summary is better than another but fail to show what a good video summary should look like
so the authors of do not exploit the above approaches
instead they let a set of participants create their video summaries and collect multiple video summaries for each video
the reason is that there is no true answer for correct video summarization but rather multiple possible ways
with these man expert video summaries they can compare any summarization method which creates an automatic video summary in a repeatable and efficient way
in such automatic versus human ison has already been used successfully for keyframes
also the authors of show that comparing automatic keyframe based summaries to human keyframe based selections yields ratings that are comparable to letting humans directly judge the automatic video summaries
both tvsum and summe datasets allow the automatic evaluation of video summarization approaches
in this paper we also establish a dataset based on with automatic evaluation for our query controllable video summarization task
the proposed dataset contains videos with frame level relevance score tations
for convenience we summarize the above existing datasets and comparison with our dataset in table
dataset introduction and analysis in this section we start to describe and analyze our proposed dataset for query controllable video summarization in terms of types of videos video labels and some statistics of the dataset
note that although the dataset from partially matches our research pose and is publicly available we discover that the specification figure this figure shows the original number of frames of each video
the axis denotes the video index and the y axis indicates the number of frames
note that for venience we make all the videos have the same number of frames when we develop our proposed method
such as annotations and amount of videos of the published dataset are different from the one mentioned in
also some parts of the published dataset are not available anymore
we base our uation on the dataset published in
so we will first describe the process they have used to create the dataset and from there indicate what changes needed to make it suitable for our purpose

setup since our dataset is based on the rules from for the dataset collection are similar
the proposed dataset consists of videos and each video is retrieved based on a given text based query
then according to the authors use amazon mechanical turk amt to annotate the video frames with the labels of text based query relevance scores
the labels in this dataset are used similarly in the mediaeval diverse social images challenge
the purpose of human expert annotated labels in the proposed dataset is to matically evaluate the methods for generating relevant and diverse video summaries
in the proposed dataset based on the sentative samples of queries and videos are collected based on the following procedure the seed queries with different categories are selected from the top youtube queries between and
typically since these queries are generic concepts and short the youtube auto complete function is exploited to obtain more tic and longer queries e

ariana grande focus instrumental and ark survival evolved dragon
for each query the top video result of frame label for each query video pair by merging the corresponding evance score annotations from amt workers
then we base on the majority vote rule to evaluate the model performance for a relevance score prediction i
e
a predicted relevance score is correct if the majority of human annotators provided that exact score
note that we map annotations very good to good to not good to and bad to
note that referring to the relevance score in this work and the importance score from tvsum are different
the relevance score in this work is to capture the relation between a given text based query and a video frame
the important score is to capture the importance between a video frame and a final video summary of the video
method overview
in this section we start to describe the proposed controllable video summarization method
the proposed method is composed of a video summary controller video summary generator and video summary output module
the summary controller takes a text based query as input and outputs the vector representation of the query
the summary generator takes the embedded query and a video as inputs and outputs the frame based relevance score prediction
finally the video summary output module will use the score prediction to generate a video summary
figure explains the above procedure

video summary controller text based queries are meant to represent the expected video summary content while subtly alludes its semantic relationship
therefore we use the following way to encode input queries and add their contribution to our proposed method
in our paper we exploit the vector representation of the text based input query to control the generated video summary
the main idea of our video summary controller is to generate a vector representation of an input query based on a dictionary
in the beginning we form a dictionary based on a bag of words which are collected from all the unique words of the training queries
then we encode an input query by exploiting the dictionary
after the encoding we have a vector representation of the input query to represent the expected video summary content
to make the procedure clearer we make a flowchart to explain the above referring to figure

video summary generator the main idea of the video summary generator is to take a vector representation of an input text based query and a video to generate a frame based relevance score vector
the summary generator is composed of a convolutional neural network cnn structure and a multi modality features fusion module
note that the cnn structure will be trained on our training set
before an input video goes to the cnn structure it is sampled at fps
then in our case we use to extract the frame based features for each input video
note that the feature used is the visual layer one layer below the classification layer
after the features are extracted we exploit a feature fusion module to fuse the frames based features and the input text based query feature
the fused feature vector will be sent to a fully connected layer for the frame based relevance score prediction
the feature fusion module will be depicted in the following subsection
please refer to figure for the flowchart of the above procedure
note that we take cross entropy loss figure this figure conceptually depicts our video mary controller
in the video summary controller we take all the queries from our training set to form a bag of words and create a dictionary
then we base on this dictionary to embed an input query
with a duration of to minutes is collected
the following task is set up on amt by for video annotations
all of the videos are sampled at one frame per second fps
then an amt worker is asked to annotate every frame with its relevance with respect to the given text based query
the answer candidates are very good good not good and bad where bad denotes the frame is not relevant and low quality such as bad contrast blurred and so on
to reduce the subjectivity of labels every video in the proposed dataset is annotated by at least different amt workers
additionally a qualification task is defined to ensure high quality annotations
the results are manually reviewed to make sure the workers provide annotations with good quality
after workers pass this task then they are allowed to take further assignments
since our maximum number of frames of video is to make all the videos have the same number of frames we repeat frames starting from the first frame until reaching frames in total for each video similar to
figure shows the original number of frames of each video

crowd sourced annotation in this subsection we analyze the frame based relevance score annotations obtained through the above procedure
also we explain how we merge these relevance score annotations for each video into one set of ground truth labels
label distributions of relevance scores
the distribution of evance score annotations is very good
good
not good
and bad

ground truth
as mentioned in the video summarization dataset comparison subsection human based evaluation is problematic and time consuming
so in this work based on the ing approach are used for evaluation
for evaluation of the testing videos one way is to ask human experts to watch the full video instead of just video summaries and access the relevance of every single part of the video
then their responses are considered as gold standard annotations
the advantage of this approach is that once the annotations are obtained experiments can be carried out indefinitely
this is desirable especially for a computer vision system involving multiple iterations and testing
note that in the proposed dataset we create a single ground truth relevance score all of thetraining queriesword embeddingmodulequeryvectorrepresentation of the input querydictionarybag of words figure this figure conceptually depicts our video summary generator
in the generator we take an input video and sample it at fps
then we input the sampled frames to a cnn based structure for extracting frame based features
the feature will be fused with a text based input query feature
finally the fused feature will be pass to a prediction layer to generate the frame based relevance scores
finally we base on the predicted relevance scores to output a query dependent video summary
as our loss function referring to equation and adam as our optimizer
for the optimizer parameters coefficients used for computing moving averages of gradient and its square are
and
respectively
the term added to the denominator to improve numerical stability is and the learning rate
class ln j where class denotes the ground truth class and indicates the prediction
multi modality features fusion module
one of the technical problems of our proposed method is fusing the query and based features with minimum loss of information
it has been shown that in similar multi modal contexts the performance of models can decrease if models are poorly designed how to solve this issue in general remains an open question
in this work we exploit difference commonly used approaches summation concatenation and element wise multiplication to fuse query and frame based features

video summary output module after we get the frame based relevance score prediction vector from the video summary generator we pass this vector to our video summary output module
the main idea of this module is to output a video summary based on the relevance score prediction vector
in our case we map labels very good to good to good to andbad to
if a predicted relevance score greater than or equal to then we consider the corresponding frame is relevant
if a predicted relevance score is less than then we consider the corresponding frame is irrelevant
finally we collect k relevant frames in time order as our video summary
note that k is a user defined parameter for the length of a video summary
experiments and analysis in this section we will evaluate our proposed end to end method for the query controllable video summarization task based on the setup of the proposed dataset
we will also analyze the effectiveness of the query and methods of multi modal features fusion

dataset preparation to validate our proposed query controllable video tion method we base on the following dataset setup to conduct our experiments
we separate the whole dataset into i
e
for training validation testing respectively
one video has one corresponding query
the maximum number of words of the query is
regarding the frame size input of cnn is by with channels i
e
red green and blue
note that we malize each image channel by mean


and std



the maximum number of frames of video is
similar to the video preprocessing method in we make all the videos have the same number of frames i
e

we show the original number of frames of each video in figure

effectiveness analysis of query in this experiment we want to know whether the text based query will help generate a better video summary or not
so we conduct the experiment based on two types of models query driven and non query driven
according to figure we discover that the query driven model with the testing accuracy
is better than the non query driven one with the testing accuracy

also based on the validation accuracy versus the number of epochs we can see that the textual query is capable of guiding the query driven model to perform better the non query driven one
however when














vector representation of the input querycandidatefeature fusionmoduleframe basedfeaturesprediction summary figure the figure shows the model performance in two cases video only and
in this figure the axis denotes the number of epochs
the y axis denotes the model accuracy
we test models after the epochs of ing
note that the purple point denotes the video only testing accuracy
and the black point indicates the testing accuracy

we compare the worst model fusing features by summation in figure to the non query driven model we discover that the query driven model performs better
this motivates us to conduct the other experiment about the comparison of multi modal feature fusion methods referring to the next subsection

effectiveness analysis of different fusion methods in general multi modal features fusion is still an open question
so we base on the commonly used methods summation concatenation and the element wise multiplication to conduct our experiment
according to figure we can see that the model with element wise multiplication fusion method has the best formance
if we compare the performance of the other feature fusion methods to the non query driven model we discover that the model with the concatenation fusion method is still better than the non query driven model
however the model with the summation fusion method is worse than the non query driven model
interaction between query and video
based on the result of figure and figure it implies that using a proper multi modal feature fusion method is important
the reason is that the fused feature embeds the implicit interaction between video i
e
frames and query
if we use an improper fusion method such as summation the query will confuse the network in some sense
this situation also happens in

qualitative results and analysis in this subsection we show some qualitative results illustrated in figure
note that because of the limited space we are only able to show some frames to represent the original video and the corresponding generated video summary in time order
in figure the input is a video with the query civil war spiderman
based on our video summary output module subsection we use green to indicate the relevant frame and black to indicate evant frames
the second row in a represents the video frames with ground truth labels
the third row in a represents the video frames with predicted labels
the correct number of relevance score figure the figure shows the model performance under three different cases i
e
multi modal features fusion by summation concatenation and the element wise plication
the model with element wise multiplication sion method has the best performance
in this figure the axis denotes the case index
the y axis denotes the model accuracy
note that each model is well trained with a ent number of epochs
prediction is out of
note that the number of frames of the original input video is so we only show the video summary frames selected from to in order
in figure the input is another video with the query movies
we also use the same color to indicate the relevant and irrelevant frames
the second row of b indicates the video frames with ground truth labels
the third row in b represents the video frames with predicted labels
the correct number of relevance score prediction is out of
similar to since the number of frames of the original input video is so we only show the video summary frames selected from to in order
based on figure it shows that our proposed method is capable of generating video summaries with content relevant to the input query
conclusion and future work to sum up we treat a query controllable video summarization task as a supervised learning problem in this work
to tackle this lem we propose an end to end deep learning based approach to generate a query dependent video summary
the proposed method contains a video summary controller video summary generator and video summary output module
to foster the query controllable video summarization research and conduct our experiments we propose a new dataset
each video in the proposed dataset is notated by frame based relevance score labels
our experimental results show that the text based query not only helps control video summary but also improves the model performance with
in the sense of accuracy
based on our experiment we know that the multi modal feature fusion method is crucial so developing a new fusion approach will be interesting future work
of





video queryvalidation video querytraining videovalidation








query civil war spiderman
correct number of relevance score prediction total number of frames
query movies
correct number of relevance score prediction total number of frames
figure in this figure based on a similar qualitative result visualization method from we show two generated video summaries with the corresponding queries
in a color coded by red indicates the original total number of frames of the input video
in the first row we show some frames from the original video with frames to represent the input video
the second row represents the original input video
the third row represents the prediction
in the fourth row we show e

k frames to represent our generated video summary
the numbers at the bottom indicate the frame index in the original video
note that the video frame index is starting from
in we show the second generated video summary result and the corresponding notations are similar to
acknowledgments this project has received funding from the european unions horizon research and innovation programme under the marie skodowska curie grant agreement no
references stanislaw antol aishwarya agrawal jiasen lu margaret mitchell dhruv batra c lawrence zitnick and devi parikh

vqa visual question answering
in proceedings of the ieee international conference on computer vision

hedi ben younes rmi cadene matthieu cord and nicolas thome

mutan multimodal tucker fusion for visual question answering
in proceedings of the ieee international conference on computer vision

wen sheng chu yale song and alejandro jaimes

video co summarization video summarization by visual co occurrence
in proceedings of the ieee ence on computer vision and pattern recognition

sandra eliza fontes de avila ana paula brando lopes antonio da luz jr and arnaldo de albuquerque arajo

vsumm a mechanism designed to produce static video summaries and a novel evaluation method
pattern recognition letters
akira fukui dong huk park daylen yang anna rohrbach trevor darrell and marcus rohrbach

multimodal compact bilinear pooling for visual question answering and visual grounding
arxiv preprint

boqing gong wei lun chao kristen grauman and fei sha

diverse sequential subset selection for supervised video summarization
in advances in neural information processing systems

michael gygli helmut grabner hayko riemenschneider and luc van gool

creating summaries from user videos
in european conference on computer vision
springer
michael gygli helmut grabner and luc van gool

video summarization by learning submodular mixtures of objectives
in proceedings of the ieee conference on computer vision and pattern recognition

kaiming he xiangyu zhang shaoqing ren and jian sun

deep residual learning for image recognition
in proceedings of the ieee conference on computer vision and pattern recognition

tao hu pascal mettes jia hong huang and cees gm snoek

silco show a few images localize the common object
in proceedings of the ieee international conference on computer vision

jia hong huang

robustness analysis of visual question answering models by basic questions
king abdullah university of science and technology ms thesis
jia hong huang modar alfadly and bernard ghanem

vqabq visual question answering by basic questions
cvpr vqa challenge workshop
jia hong huang modar alfadly bernard ghanem and marcel worring

arxiv preprint assessing the robustness of visual question answering


jia hong huang cuong duc dao modar alfadly and bernard ghanem

a novel framework for robustness analysis of visual qa models
in proceedings of the aaai conference on artificial intelligence vol


jia hong huang cuong duc dao modar alfadly c huck yang and bernard ghanem

robustness analysis of visual qa models by basic questions
cvpr vqa challenge and visual dialog workshop
bogdan ionescu alexandru lucian gnsca bogdan boteanu adrian popescu mihai lupu and henning mller

retrieving diverse social images at mediaeval challenge dataset and evaluation
mediaeval
hong wen kang yasuyuki matsushita xiaoou tang and xue quan chen

space time video montage
in ieee computer society conference on computer vision and pattern recognition vol

ieee
aditya khosla raffay hamid chih jen lin and neel sundaresan

scale video summarization using web image priors
in proceedings of the ieee conference on computer vision and pattern recognition

diederik p kingma and jimmy ba

adam a method for stochastic mization
arxiv preprint

yong jae lee joydeep ghosh and kristen grauman

discovering important people and objects for egocentric video summarization
in ieee conference on computer vision and pattern recognition
ieee
yandong li liqiang wang tianbao yang and boqing gong

how local is the local diversity reinforcing sequential determinantal point processes with dynamic ground sets for supervised video summarization
in proceedings of the european conference on computer vision eccv

tiecheng liu and john r kender

optimization algorithms for the selection of key frame sequences of variable length
in european conference on computer vision
springer
yi chieh liu yung an hsieh min hung chen chao han huck yang jesper tegner and yi chang james tsai

interpretable self attention temporal reasoning for driving behavior understanding
arxiv preprint

yi chieh liu hao hsiang yang c h huck yang jia hong huang meng tian hiromasa morikawa yi chang james tsai and jesper tegner

synthesizing new retinal symptom images by multiple generative models
in asian conference on computer vision
springer
zheng lu and kristen grauman

story driven summarization for egocentric video
in proceedings of the ieee conference on computer vision and pattern recognition

yu fei ma lie lu hong jiang zhang and mingjing li

a user attention model for video summarization
in proceedings of the tenth acm international conference on multimedia
acm
chong wah ngo yu fei ma and hong jiang zhang

automatic video marization by graph modeling
in proceedings ninth ieee international conference on computer vision
ieee
paul over alan f smeaton and george awad

the trecvid bbc rushes summarization evaluation
in proceedings of the acm trecvid video summarization workshop
acm
rameswar panda and amit k roy chowdhury

collaborative tion of topic related videos
in proceedings of the ieee conference on computer vision and pattern recognition

danila potapov matthijs douze zaid harchaoui and cordelia schmid

category specific video summarization
in european conference on computer vision
springer
mrigank rochan and yang wang

video summarization by learning from unpaired data
in proceedings of the ieee conference on computer vision and pattern recognition

mrigank rochan linwei ye and yang wang

video summarization using fully convolutional sequence networks
in proceedings of the european conference on computer vision eccv

aidean sharghi ali borji chengtao li tianbao yang and boqing gong

improving sequential determinantal point processes for supervised video marization
in proceedings of the european conference on computer vision eccv

gunnar a sigurdsson santosh divvala ali farhadi and abhinav gupta

asynchronous temporal fields for action recognition
in proceedings of the ieee conference on computer vision and pattern recognition

alan f smeaton paul over and wessel kraaij

evaluation campaigns and trecvid
in proceedings of the acm international workshop on multimedia information retrieval
acm
yale song jordi vallmitjana amanda stent and alejandro jaimes

tvsum summarizing web videos using titles
in proceedings of the ieee conference on computer vision and pattern recognition

arun balajee vasudevan michael gygli anna volokitin and luc van gool

query adaptive video summarization via quality aware relevance estimation
in proceedings of the acm international conference on multimedia
acm
c h huck yang jia hong huang fangyu liu fang yi chiu mengya gao weifeng lyu jesper tegner al

a novel hybrid machine learning model for auto classification of retinal diseases
joint icml and ijcai workshop on computational biology
c h huck yang fangyu liu jia hong huang meng tian md i hung lin yi chieh liu hiromasa morikawa hao hsiang yang and jesper tegner

auto classification of retinal diseases in the limit of sparse data using a streams machine learning model
in asian conference on computer vision
springer
chao han huck yang yi chieh liu pin yu chen xiaoli ma and yi chang james tsai

when causal intervention meets adversarial examples and image masking for deep neural networks
in ieee international conference on image processing icip
ieee
ke zhang wei lun chao fei sha and kristen grauman

summary transfer exemplar based subset selection for video summarization
in proceedings of the ieee conference on computer vision and pattern recognition

ke zhang wei lun chao fei sha and kristen grauman

video rization with long short term memory
in european conference on computer vision
springer
ke zhang kristen grauman and fei sha

retrospective encoders for video summarization
in proceedings of the european conference on computer vision eccv

yujia zhang michael kampffmeyer xiaodan liang min tan and eric p xing

query conditioned three player adversarial network for video tion
arxiv preprint

yujia zhang michael kampffmeyer xiaoguang zhao and min tan

gan dilated temporal relational adversarial network for video summarization
in proceedings of the acm turing celebration conference china
acm
bin zhao and eric p xing

quasi real time summarization for consumer videos
in proceedings of the ieee conference on computer vision and pattern recognition

kaiyang zhou yu qiao and tao xiang

deep reinforcement learning for unsupervised video summarization with diversity representativeness reward
in thirty second aaai conference on artificial intelligence

