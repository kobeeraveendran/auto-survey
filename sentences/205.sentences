bigpatent a large scale dataset for abstractive and coherent summarization eva chen and lu college of computer sciences northeastern university ai lab
neu
edu
neu
edu
com n u j l c
s c v
v i x r a abstract most existing text summarization datasets are compiled from the news domain where maries have a attened discourse structure
in such datasets summary worthy content ten appears in the beginning of input cles
moreover large segments from input ticles are present verbatim in their respective summaries
these issues impede the ing and evaluation of systems that can stand an article s global content structure as well as produce abstractive summaries with in this work we high compression ratio
present a novel dataset bigpatent ing of
million records of u
s
patent uments along with human written abstractive summaries
compared to existing rization datasets bigpatent has the ing properties summaries contain a richer discourse structure with more recurring ties ii salient content is evenly distributed in the input and lesser and shorter extractive fragments are present in the summaries
nally we train and evaluate baselines and ular learning models on bigpatent to shed light on new challenges and motivate future rections for summarization research
introduction there has been a growing interest in building neural abstractive summarization systems see et al
paulus et al
gehrmann et al
which requires large scale datasets with high quality summaries
a number of tion datasets have been explored so far sandhaus napoles et al
hermann et al
grusky et al

however as most of them are acquired from news articles they share cic characteristics that limit current state of art models by making them more extractive rather than allowing them to understand input content and generate well formed informative summaries
sample cnn daily mail news summary an explosion rocks a chemical plant in china s eastern fujian province for the second time in two years
six were injured after the explosion and are ing hospitalized
the explosion was triggered by an oil leak though local media has not reported any toxic chemical spills
sample bigpatent summary a shoelace cover incorporating an interchangeable fashion panel for covering the shoelaces of a gym shoe
the shoelace cover is secured to the shoe by a number of straps threaded through slots in the shoelace cover
these straps secured to each side of the gym shoe clude a loop and hook material such that the straps can be disengaged and the shoelace cover can be drawn back to expose the shoelaces


figure sample summaries from cnn daily mail and bigpatent
extractive fragments reused from input are underlined
repeated entities indicating course structure are highlighted in respective colors
specically in these datasets the summaries are attened narratives with a simpler discourse ture e

entities are rarely repeated as illustrated by the news summary in fig

moreover these summaries usually contain long fragments of text directly extracted from the input
finally the summary worthy salient content is mostly present in the beginning of the input articles
we introduce a new large scale summarization dataset consisting of
million patent documents with human written abstractive summaries
bigpatent addresses the mentioned issues thus guiding summarization search to better understand the input s global structure and generate summaries with a more complex and coherent discourse structure
the key features of bigpatent are summaries hibit a richer discourse structure with entities dataset is available to download online at evasharma
github
io bigpatent
curring in multiple subsequent sentences as shown in fig
ii salient content is evenly distributed in the document and iii summaries are considerably more abstractive while reusing fewer and shorter phrases from the input
to further illustrate the challenges in text marization we benchmark bigpatent with lines and popular summarization models and compare with the results on existing large scale news datasets
we nd that many models yield noticeably lower rouge scores on bigpatent than on the news datasets suggesting a need for developing more advanced models to address the new challenges presented by bigpatent
over while existing neural abstractive models duce more abstractive summaries on bigpatent they tend to repeat irrelevant discourse entities cessively and often fabricate information
these observations demonstrate the importance of bigpatent in steering future research in text summarization towards global content modeling semantic understanding of entities and relations and discourse aware text planning to build stractive and coherent summarization systems
related work recent advances in abstractive summarization show promising results in generating uent and informative summaries rush et al
pati et al
tan et al
paulus et al

however these summaries often contain fabricated and repeated content cao et al

fan et al
show that for content selection existing models rely on positional information and can be easily fooled by adversarial content present in the input
this underpins the need for global content modeling and semantic understanding of the input along with discourse aware text ning to yield a well formed summary mckeown barzilay and lapata
several datasets have been used to aid the velopment of text summarization models
these datasets are predominantly from the news domain and have several drawbacks such as limited ing data document understanding shorter summaries gigaword napoles et al
xsum narayan et al
and room grusky et al
and near extractive summaries cnn daily mail dataset hermann et al

moreover due to the nature of
nist
dataset doc comp
dens
summary doc ratio word sent word
cnn dm nyt
newsroom

xsum
arxiv pubmed
bigpatent




























table statistics of bigpatent and other rization datasets
doc raw number of documents in each dataset
for all other columns mean values are reported over all documents
bigpatent has a lower extractive fragment density dens
and a higher pression ratio comp
ratio
news reporting summary worthy content is uniformly distributed within each article
arxiv and pubmed datasets cohan et al
which are collected from scientic repositories are ited in size and have longer yet extractive maries
thus existing datasets either lack cial structural properties or are limited in size for learning robust deep learning methods
to dress these issues we present a new dataset patent which guides research towards ing more abstractive summarization systems with global content understanding
bigpatent dataset we present bigpatent a dataset consisting of
million u
s
patent documents collected from google patents public datasets using query google
it contains patents led after across nine different technological eas
we use each patent s abstract as the standard summary and its description as the put
additional details for the dataset including the preprocessing steps are in appendix a

table lists statistics including compression ratio and extractive fragment density for patent and some commonly used summarization corpora
compression ratio is the ratio of the number of words in a document and its summary whereas density is the average length of the and maintained by ifi claims patent vices and google and licensed under creative commons tribution
international license
summarization task studied using bigpatent is notably different from traditional patent summarization task where patent claims are summarized into a more readable mat cinciruk
figure of salient unigrams present in the n ments of the input
figure of novel n grams in the summaries
tractive to which each word in the mary belongs grusky et al

among isting datasets cnn dm hermann et al
nyt napoles et al
newsroom leased grusky et al
and xsum narayan et al
are news datasets while arxiv and pubmed cohan et al
contain scientic articles
notably bigpatent is signicantly larger with longer inputs and summaries
dataset characterization
salient content distribution inferring the distribution of salient content in the input is critical to content selection of tion models
while prior work uses probabilistic topic models barzilay and lee haghighi and vanderwende or relies on classiers trained with sophisticated features yang et al
we focus on salient words and their rences in the input
we consider all unigrams except stopwords in a summary as salient words for the respective ument
we divide each document into four equal segments and measure the percentage of unique salient words in each segment
formally let u be a function that returns all unique unigrams except stopwords for a given text
then u di denotes the unique unigrams in the ith segment of a ment d and u y denotes the unique unigrams in the corresponding summary y
the percentage of salient unigrams in the ith segment of a document is calculated as u fig
shows that bigpatent has a fairly even distribution of salient words in all segments of the fragments are the set of shared sequences of tokens in the document and summary
input
only more salient words are observed in the segment than in other segments
in trast for cnn dm nyt and newsroom imately of the salient words are present in the segment and the proportion drops cally to in the segment
this indicates that most salient content is present in the ning of news articles in these datasets
for xsum another news dataset although the trend in the rst three segments is similar to bigpatent the percentage of novel unigrams in the last segment drops by compared to
for bigpatent
for scientic articles arxiv and pubmed where content is organized into sections there is a clear drop in the segment where related work is often discussed with most salient information being present in the rst introduction and last conclusion sections
whereas in bigpatent since each embodiment of a patent invention is sequentially described in its document it has a more uniform distribution of salient content
next we probe how far one needs to read from the input s start to cover the salient words only those present in input from the summary
about of the sentences from the input are required to construct full summaries for cnn dm for xsum for nyt and for newsroom
whereas in the case of bigpatent of the input is required
the aforementioned tions signify the need of global content modeling to achieve good performance on bigpatent

summary abstractiveness and coherence summary n gram novelty
following prior work see et al
chen and bansal we compute abstractiveness as the fraction of novel n grams in the summaries that are absent from the input
as shown in fig
xsum prises of notably shorter but more abstractive maries
besides that bigpatent reports the cnn n grams cnn dmnytnewsroomxsumarxivpubmedbigpatent t t t t



cnn dm



nyt newsroom







arxiv



pubmed



bigpatent table of entities occurring t times in summaries
ent
chain length in ent
recurrence at datasets l l l l t t t
cnn dm nyt
newsroom

arxiv
pubmed
bigpatent



































table left of entities of chain length l
right avg
number of entities that appear at the tth summary sentence and recur in a later sentence
ond highest percentage of novel n grams for n
signicantly higher novelty scores for trigram and gram indicate that bigpatent has fewer and shorter extractive fragments compared to others except for xsum a smaller dataset
this further corroborates the fact that bigpatent has the lowest extractive fragment density as shown in table and contains longer summaries
coherence analysis via entity distribution
to study the discourse structure of summaries we analyze the distribution of entities that are dicative of coherence grosz et al
strube and hahn
to identify these entities we extract non recursive noun phrases regex np using nltk loper and bird
finally we use the entity grid tion by barzilay and lapata and their erence resolution rules to capture the entity bution across summary sentences
in this work we do not distinguish entities grammar roles and leave that for future study
on average there are


and
unique entities in the summaries for newsroom nyt cnn dm and bigpatent
pubmed and arxiv reported higher number of unique entities in summaries
and
spectively since their summaries are considerably longer table
table shows that
of entities recur in bigpatent summaries which is higher than that on other datasets indicating more complex discourse structures in its summaries
to understand local coherence in summaries we measure the longest chain formed across sentences by each entity denoted as l
table shows that
of the entities in bigpatent appear in two consecutive sentences which is again higher than that of any other dataset
the presence of longer entity chains in the bigpatent summaries gests its higher sentence to sentence relatedness than the news summaries
finally we examine the entity recurrence tern which captures how many entities rst ring in the tth sentence are repeated in subsequent t ith sentences
table right shows that on average
entities in bigpatent summaries cur in later sentences summing up the numbers for and after
the corresponding recurring frequency for news dataset such as cnn dm is only

though pubmed and arxiv report higher number of recurrence their patterns are different i
e
entities often recur after three tences
these observations imply a good tion of local and global coherence in bigpatent
experiments and analyses we evaluate bigpatent with popular rization systems and compare with well known datasets such as cnn dm and nyt
for line we use which selects the rst three sentences from the input as the summary
we consider two oracles i oraclefrag builds summary using all the longest fragments reused from input in the gold summary grusky et al
and oracleext selects globally mal combination of three sentences from the input that gets the highest score
next we consider three unsupervised extractive tems textrank mihalcea and tarau lexrank erkan and radev and basic nenkova and vanderwende
we also adopt rnn ext rl chen and bansal a model that selects three salient sentences to construct the summary using forcement learning
finally we train four tive systems with attention generator pointgen and a version with erage mechanism pointgen cov see et al
and sentrewriting chen and bansal
experimental setups and model parameters are described in appendix a

exclude xsum as its summaries are all one sentence
table reports scores of cnn dm nyt bigpatent models r l r l r l oraclefrag grusky et al
oracleext


























textrank mihalcea and tarau lexrank erkan and radev sumbasic nenkova and vanderwende
rnn ext rl chen and bansal










sutskever et al
pointgen see et al
see et al
sentrewriting chen and bansal



























































table rouge scores on three large datasets
the best results for non baseline systems are in bold
except for sentrewriting on cnn dm and nyt for all abstractive models we truncate input and summaries at and
models novel n grams entities occurring m times n m m m m n

gold

pointgen cov

sentrewriting

















table of novel n grams highest are lighted and of entities occurring m times in ated summaries of bigpatent
peats entities less often than humans do
and l lin and hovy for all models
for bigpatent almost all models outperform the baseline due to the more uniform bution of salient content in bigpatent s input articles
among extractive models textrank and lexrank outperform rnn ext rl which was trained on only the rst words of the put again suggesting the need for neural models to efciently handle longer input
finally trewriting a reinforcement learning model with rouge as reward achieves the best mance on bigpatent
table presents the percentage of novel grams in the generated summaries
although the novel content in the generated summaries for both unigrams and bigrams is comparable to that of gold we observe repeated instances of cated or irrelevant information
for example the upper portion is congured to receive the upper portion of the sole portion part of generated summary has irrelevant repetitions pared to the human summary as in fig

this suggests the lack of semantic understanding and control for generation in existing neural models
table also shows the entity distribution
in the generated summaries for bigpatent
we nd that neural abstractive models except tend to repeat entities more often than humans do
for gold only
and
of entities are mentioned thrice or more pared to
and
for
which employs coverage mechanism to explicitly penalize repetition generates cantly fewer entity repetitions
these ndings dicate that current models failt to learn the entity distribution pattern suggesting a lack of standing of entity roles e

their importance and discourse level text planning
conclusion we present the bigpatent dataset with written abstractive summaries containing fewer and shorter extractive phrases and a richer course structure compared to existing datasets
salient content from the bigpatent summaries is more evenly distributed in the input
bigpatent can enable future research to build robust systems that generate abstractive and coherent summaries
acknowledgements this research is supported in part by national ence foundation through grants and and by the ofce of the director of national intelligence odni intelligence vanced research projects activity iarpa via contract
the views and conclusions contained herein are those of the thors and should not be interpreted as necessarily representing the ofcial policies either expressed or implied of odni iarpa or the u
s
ernment
the u
s
government is authorized to reproduce and distribute reprints for tal purposes notwithstanding any copyright tation therein
we also thank the anonymous viewers for their constructive suggestions
references dzmitry bahdanau kyunghyun cho and yoshua gio

neural machine translation by jointly arxiv preprint learning to align and translate


federico barrios federico lopez luis argerich and rosa wachenchauzer

variations of the larity function of textrank for automated tion
arxiv preprint

regina barzilay and mirella lapata

modeling local coherence an entity based approach
tational linguistics
regina barzilay and lillian lee

catching the drift probabilistic content models with applications in proceedings to generation and summarization
of the human language technology conference of the north american chapter of the association for computational linguistics hlt naacl
steven bird ewan klein and edward loper

natural language processing with python ing text with the natural language toolkit
oreilly media inc

ziqiang cao furu wei wenjie li and sujian li

faithful to the original fact aware neural in proceedings of the tive summarization
ation for the advancement of articial intelligence aaai
yen chun chen and mohit bansal

fast tive summarization with reinforce selected sentence rewriting
in proceedings of the annual ing of the association for computational linguistics volume long papers pages
tion for computational linguistics
david cinciruk

patent summarization and
ece
drexel
phrasing
walsh
pdf
arman cohan franck dernoncourt doo soon kim trung bui seokhwan kim walter chang and nazli goharian

a discourse aware attention model for abstractive summarization of long documents
in proceedings of the conference of the north american chapter of the association for tional linguistics human language technologies volume short papers pages
tion for computational linguistics
john duchi elad hazan and yoram singer

adaptive subgradient methods for online learning journal of machine and stochastic optimization
learning research
gunes erkan and dragomir r radev

lexrank graph based lexical centrality as salience in text summarization
journal of articial intelligence search
lisa fan dong yu and lu wang

robust neural abstractive summarization systems and evaluation against adversarial information
in workshop on terpretability and robustness in audio speech and language irasl
neural information processing systems
sebastian gehrmann yuntian deng and alexander rush

bottom up abstractive summarization
in proceedings of the conference on cal methods in natural language processing pages
association for computational tics
sebastian gehrmann yuntian deng and alexander rush

bottom up abstractive in proceedings of the conference on tion
empirical methods in natural language ing pages
google

paid google patents public datasets and private patent
cloud
connecting public data
google
com marketplace google patents public



accessed
barbara j
grosz scott weinstein and aravind k
joshi

centering a framework for ing the local coherence of discourse
computational linguistics
max grusky mor naaman and yoav artzi

newsroom a dataset of
million summaries with diverse extractive strategies
in proceedings of the conference of the north american chapter of the association for computational linguistics man language technologies volume long pers pages
association for tional linguistics
aria haghighi and lucy vanderwende

ing content models for multi document in proceedings of human language tion
nologies the annual conference of the north american chapter of the association for tional linguistics pages
association for computational linguistics
karl moritz hermann tomas kocisky edward grefenstette lasse espeholt will kay mustafa leyman and phil blunsom

teaching chines to read and comprehend
in advances in ral information processing systems pages
chin yew lin and eduard hovy

matic evaluation of summaries using n gram in proceedings of the occurrence statistics
human language technology conference of the north american chapter of the association for computational linguistics
edward loper and steven bird

nltk the ral language toolkit
in proceedings of the workshop on effective tools and methodologies for teaching natural language processing and tational linguistics
kathleen r mckeown

discourse strategies for generating natural language text
articial gence
rada mihalcea and paul tarau

textrank ing order into text
in proceedings of the ference on empirical methods in natural language processing
ramesh nallapati bowen zhou cicero dos santos caglar gulcehre and bing xiang

stractive text summarization using sequence sequence rnns and beyond
in proceedings of the signll conference on computational natural language learning pages
association for computational linguistics
courtney napoles matthew gormley and benjamin in van durme

annotated gigaword
ceedings of the joint workshop on automatic edge base construction and web scale knowledge extraction akbc wekex pages
ation for computational linguistics
shashi narayan shay b
cohen and mirella lapata

do nt give me the details just the summary topic aware convolutional neural networks for in proceedings of the treme summarization
conference on empirical methods in natural guage processing pages
association for computational linguistics
ani nenkova and lucy vanderwende

the pact of frequency on summarization
microsoft search redmond washington tech
rep
msr
romain paulus caiming xiong and richard socher

a deep reinforced model for abstractive marization
arxiv preprint

alexander m rush sumit chopra and jason weston

a neural attention model for abstractive in proceedings of the tence summarization
conference on empirical methods in natural guage processing pages
evan sandhaus

the new york times annotated corpus
linguistic data consortium philadelphia
abigail see peter j
liu and christopher d
manning

get to the point summarization with generator networks
in proceedings of the nual meeting of the association for computational linguistics volume long papers pages
association for computational linguistics
michael strube and udo hahn

functional tering grounding referential coherence in tion structure
computational linguistics
ilya sutskever oriol vinyals and quoc v le

sequence to sequence learning with neural works
in advances in neural information ing systems pages
jiwei tan xiaojun wan and jianguo xiao

abstractive document summarization with a in proceedings of based attentional neural model
the annual meeting of the association for putational linguistics volume long papers pages
association for computational linguistics
cooperative patent classication
uspto
gov uspto

scheme
patents classification cpc cpc
html
accessed
yonghui wu mike schuster zhifeng chen quoc v le mohammad norouzi wolfgang macherey maxim krikun yuan cao qin gao klaus macherey et al

google s neural chine translation system bridging the gap between arxiv preprint human and machine translation


yinfei yang forrest bao and ani nenkova

tecting content for single document in proceedings of the news summarization
conference of the european chapter of the tion for computational linguistics volume short papers pages
association for tional linguistics
a appendices a
dataset details bigpatent a novel large scale summarization dataset of
million us patent documents is lected from google patents public datasets using bigquery google
google has indexed more than million patents with full text from different patent ofces so far
we only consider patent documents from united states patent and trademark ofce uspto led in english guage after in order to get considerably more consistent writing and formatting style to facilitate easier parsing of the text
each us patent application is led der a cooperative patent classication cpc code uspto that provides a hierarchical system of language independent symbols for the classication of patents according to the different areas of technology to which they pertain
there are nine such classication categories a human necessities b performing operations porting c chemistry metallurgy d textiles paper e fixed constructions f mechanical cpc code doc comp
dens
summary ratio word sent word doc a b c d e f g h y












































table statistics for cpc codes in bigpatent
engineering lightning heating weapons ing g physics h electricity and y eral tagging of new or cross sectional technology
table summarizes the statistics for bigpatent across all nine categories
from the full public dataset for each patent record we retained its title authors abstract claims of the invention and the description text
abstract of the patent which is generally written by the inventors after the patent application is proved was considered as the gold standard mary of the patent
description text of the patent contains several other elds such as background of the invention covering previously published lated inventions description of gures and tailed description of the current invention
for the summarization task we considered the detailed description of each patent as the input
we tokenized the articles and summaries ing natural language toolkit nltk bird et al
since there was a large variation in
size of summary and input texts we removed patent records with compression ratio less than and higher than
further we only kept records with summary length between and words and input length of at least and at most
next to focus on the tive summary input pairs we removed the records whose percentage of summary worthy unigrams absent from the input novel unigrams was less than
finally we removed references of ure from summaries and input along with full bles from the input
salient content distribution bigrams and longest common subsequences
as also shown in the main paper i
e
figure and figure patent demonstrates a relatively uniform bution of the salient content from the summary figure of salient bigrams present in n th segment of input
figure of salient longest common subsequences present in n th segment of input
in all parts of the input
here the salient content is considered as all bigrams and longest common sub sequences from the summary
a
experiment details for all experiments we randomly split patent into training pairs validation pairs and test pairs
for cnn dm we followed preprocessing steps from see et al
using training validation and test pairs
for nyt lowing preprocessing steps from paulus et al
we used training tion and test pairs
extract based systems
for textrank we used the barrios al
to ate summary with three sentences based on trank algorithm mihalcea and tarau
for lexrank and sumbasic we used
for rnn ext rl from chen and bansal we used the implementation provided by the
abstract based systems
for all the neural stractive summarization models except for trewriting we truncated the input to words and output to words
except for trewriting all other models were trained
org project
python
org pypi sumy
com chenrocks fast abs rl cnn cnn dmnytnewsroomxsumarxivpubmedbigpatent ing opennmt py python based on the instructions provided by the authors gehrmann et al

we provide further details for each model below
with attention sutskever et al
was trained using a dimensional embedding and dimensional layer lstm
we used a bidirectional lstm for the encoder and attention mechanism from bahdanau et al

the model was trained using adagrad duchi et al
with learning rate
and an tial accumulator value of

at inference time we used the beam size
we used the same tings for training pointgen and pointgen cov see et al
adding the copy attention mechanism that allows the model to copy words from the source
at inference time for gen cov we used coverage penalty with beta set to and length penalty wu et al
with alpha as

for sentrewriting from chen and bansal we again used the implementation by the to train their full rl based model using their default parameters
a
summaries for sample input document from bigpatent for the sample summary presented in tion of the main paper in table we list complete gold standard summary along with the summaries generated by pointgen cov and sentrewriting
for the respective input we also list the rst words for brevity

net opennmt py summarization
html
com chenrocks fast abs rl gold standard summary a shoelace cover incorporating an interchangeable fashion panel for covering the shoelaces of a gym shoe
the shoelace cover is secured to the shoe by a number of straps threaded through slots in the shoelace cover
a strap secured to each side of the gym shoe includes a loop and hook material such that the straps can be disengaged and the shoelace cover can be drawn back to expose the shoelaces of the shoe
the fashion panel is attached to the shoelace cover by a loop and hook material such that at the whim of the shoe wearer the fashion panel can be replaced by other fashion panels to convey a fashion statement
generated summary a shoe having a sole portion and an upper portion
the sole portion includes an upper portion and a lower portion
the upper portion is congured to receive the upper portion of the sole portion
the lower portion of the upper portion is congured to receive the upper portion of the sole portion
pointgen cov generated summary a gym shoe and associated shoelace shoe is disclosed
the shoe includes a sole portion a shoelace cover and an upper portion
the upper portion has a toe area that extends from the toe area to the opening
the shoelace cover is curved to the shoelace
sentrewriting generated summary a gym shoe and associated shoelace cover and associated shoelace cover and fashion panel are disclosed
the shoe includes a sole portion and an upper portion
the shoelace cover is a semi rigid panel that is curved to conform to the shoelace area of the shoelace area
the shoelace area is generally split into a shoelace area and a shoelace area
a shoe for use in a shoe such as a shoe is disclosed
a tongue extends from the toe area to the shoelace
input rst words the following discussion of the preferred embodiment concerning a gym shoe and associated shoelace cover and fashion panel is merely exemplary in nature and is in no way intended to limit the invention or its applications or uses
the shoe includes a sole portion generally comprised of a rugged rubber material and an upper portion generally comprised of a durable and pliable leather or canvas material
at a back location of the upper portion is an opening for accepting a wearer s foot
a cushion is visible through the opening on which the wearer s foot is supported
at a front end of the upper portion is a toe area
extending from the toe area to the opening is a shoelace area
the shoelace area is generally split such that a shoelace is threaded through eyelets associated with the shoelace area in order to bind together the shoelace area and secure the shoe to the wearer s foot
a tongue also extending from the toe area to the opening is positioned beneath the shoelace such that the tongue contacts the wearer s foot and thus provides comfort against the shoelace to the wearer
the basic components and operation of a gym shoe is well understood to a person of normal sensibilities and thus a detailed discussion of the parts of the shoe and their specic operation need not be elaborated on here
secured to the upper portion of the shoe covering the shoelace area is a shoelace cover
in a preferred embodiment the shoelace cover is a semi rigid panel that is curved to be shaped to conform to the shoelace area such that an upper portion of the shoelace cover extends a certain distance along the sides of the upper portion adjacent the opening
the shoelace cover narrows slightly as it extends towards the toe area
the specics concerning the shape dimensions material rigidity
of the shoelace cover will be discussed in greater detail below
additionally the preferred method of securing the shoelace cover to the shoe will also be discussed below
in a preferred embodiment afxed to a top surface of the shoelace cover is a fashion panel
the fashion panel is secured to the shoelace cover by an applicable securing mechanism such as a loop and hook velcro type fastener device so that the fashion panel can be readily removed from the shoelace cover and replaced with an alternate fashion panel having a different design
table gold standard and system generated summaries for bigpatent
input pre processed is truncated to words for brevity

