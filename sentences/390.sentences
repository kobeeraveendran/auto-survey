extractive summarization of call transcripts pratik k
biswas and aleksandr iakubovich ai data science global network and technology verizon communications new jersey usa abstract text summarization is the process of extracting the most important information from the text and presenting it concisely in fewer sentences
call transcript is a text that involves textual description of a phone conversation between a customer caller and customer representatives
this paper presents an indigenously developed method that combines topic modeling and sentence selection with punctuation restoration in condensing ill punctuated or un punctuated call transcripts to produce summaries that are more readable
extensive testing evaluation and comparisons have demonstrated the efficacy of this summarizer for call transcript summarization
index terms extractive summarization topic models transformers embedding punctuation restoration
i
introduction in recent years there is an abundance of multi sourced information available for public consumption fueled by the growth of the internet
in many cases this volume of readily available text needs effective summarization to be useful for different purposes
it is very difficult for human beings to summarize large quantities of text manually
hence automatic text summarization has become a very desirable tool in today s information age
it has the task of producing concise fluent and readable summaries from larger bodies of text while still preserving the original information content and meaning
such summarization can be very useful when applied to various domains e

news articles emails call transcripts medical history mobile text messages
many such summarizers are available online on the internet like microsoft for news articles mead swesum for biomedical information wikisummarizer for wikipedia articles
extractive numerous approaches have been developed for automatic text summarization and can be broadly classified into two groups abstractive summarization
extractive summarization extracts important sentences from the original text and reproduce them verbatim in the summary while abstractive summarization generates new sentences
summarization and call transcripts are written texts originally presented in a different medium and so call transcription is defined as the process of converting a voice or video call audio track into written words through speech to text conversion to be stored as plain text in a conversational language
in this paper however we will be confining ourselves to textual descriptions of audio recordings of voice calls between customer caller and customer representatives of a phone company
transcripts summarization of call automatic in our consideration pose certain unique challenges as follows they are not continuous texts but include conversation between customers and agents they are often very long and are embedded with small talks and can include a large number of sentences that are irrelevant and even meaningless they include several ill formed grammatically incorrect sentences they are either un punctuated or are improperly punctuated based on pauses in the conversation as perceived by annotators and so are often unreadable and existing open source too well with call summarization transcripts
tools do nt perform into customer and agent in this paper we have presented a novel extractive summarization technique that combines channel separation topic separation modeling sentence selection with punctuation restoration to produce properly punctuated fixed length and readable customer and agent summaries from the original call transcripts that can adequately summarize customer concerns and agent resolutions
transcripts ii
related work related research can be broadly grouped into two categories extractive summarization and abstractive summarization
research under the first category is most relevant to our work
radef et al
defined summary as a text that is produced from one or more texts that conveys important information in the original and that is no longer than half of the original and usually significantly less than that
automatic text summarization gained attraction as early as the
different methods text summarization have been provided in
surveys of automatic extensive and luhn al
introduced a method to extract salient sentences from the text using features such as word and phrase frequency
they proposed to weight the sentences of a document as a function of high frequency words ignoring very high frequency common words
edmundson al
described a based on key phrases where they used four different methods to determine the sentence weight
the trainable document kupiec al
developed summarizer which performed the sentence extracting task based on a number of weighting heuristics
bookstein et al
built clusters of index terms phrases and other sub parts of documents for extractive text summarization
brandow al
that automatically condensed domain independent electronic news data
conroy al
mittendorf al
used hidden models for text summarization
chen al
text extraction system the anes launched copyright verizon
all rights reserved
information contained herein is provided as is and subject to change without notice
all trademarks used herein are property of their respective owners
this work has now been submitted to the ieee for possible publication
copyright may be transferred without notice after which this version may no longer be accessible
et to al
discussed a sentence selection based approach text summarization while gong al
and wang al
described how multiple documents could be summarized using topic models
neto al
introduced machine learning approaches to automatic text summarization and kaikhah discussed how neural networks could be useful for summarizing news articles
suanmali et al
proposed fuzzy logic based extractive text summarization to improve the quality of the summaries created by the general statistical method
nallapati al
presented a recurrent neural network rnn based sequence model for extractive summarization of documents
narayan conceptualized extractive summarization as a sentence ranking task and proposed a novel training algorithm for optimizing the rouge evaluation metric through a reinforcement learning objective
xu et al
constructed a neural model for single document summarization based jointly on extraction and syntactic compression
verma al
applied a restricted boltzmann machine to enhance the summaries of factual reports created through extractive summarization
miller used bidirectional encoder representations from transformers bert for summarization of lecture notes
liu described bertsum a simple variant of bert for extractive summarization
liu et al
displayed how bert could be generally applied in text summarization and proposed a general framework for both extractive and abstractive models
zhong et al
formulated the extractive summarization task as a semantic text matching problem while wu et al
instituted a new text to graph task of predicting summarized knowledge graphs from long documents
lemberger et al
recounted the applicability of deep learning models for automatic text summarization
lin al
surveyed the state of the art in abstractive summarization while khan al
reviewed various abstractive summarization methods
nallapati al
paulus et al
and see et al
employed recurrent neural network deep reinforcement learning and pointer generator network for abstractive summarization
iii
major contributions in this paper we have proposed an innovative extractive summarization technique for call transcript summarization
our main contributions and advantages can be summarized as follows we have integrated topic modeling and embedding based sentence selection with transformer based punctuation restoration for extractive summarization through a novel step sequential procedure method
our method splits the original call transcript into customer and agent transcripts by the associated channel identifiers and then summarizes each transcript separately for more coherent results
our method restores full punctuation in the summaries of un punctuated or ill punctuated call transcripts
we have uniquely modified and retrained the bert transformer model architecture for punctuation restoration by adding a classification layer above bert s layers
our method creates compares and evaluates the performances of multiple different types of topic models for the transcripts before selecting the most optimal one for extractive summarization it also provides the option to specify the topic model type to be used for extractive summarization and allows the summarizer to use different to generate customer and agent topic model summaries
types we have introduced a new metric for measuring the effectiveness of punctuation restoration in the punctuated summaries
iv
preliminaries concepts and terminologies in this section we clarify key concepts and terminologies and explain certain technologies which provide the foundation for our work
the automatic summarization of text is a well defined task in the field of natural language processing nlp
automatic text summarization attempts to convert a larger document into a shorter version preserving its information content and overall meaning
a good summary should reflect the diverse topics of the document while keeping redundancy to a minimum
in general there are two different approaches to automatic text summarization namely extractive and abstractive
extractive summarization methods identify the relevant sections in the original text extract the most important paragraphs sentences phrases
from there and concatenate them into shorter form
in contrast abstractive summarization methods attempt to convey the most important information from the original text by generating new sentences
in other words they interpret examine and analyze the original text using advanced natural language techniques to get a better understanding of the content and then describe it through shorter and more focused text comprising of new sentences
purely extractive summaries often give better results than automatic abstractive summaries
this is because of the that abstractive summarization methods cope with fact problems like semantic representation inference and natural language generation which are relatively harder than data driven approaches such as sentence extraction
most abstractive summarization techniques specifically the ones extractive using deep summarization to extract the summaries for the training samples from which they train to generate new text
in this paper we focus only on extractive summarization as it is relevant to our work
also depend upon learning a
extractive summarization extractive summarization techniques generate summaries by selecting the most important sentences paragraphs
from the original text
the importance of sentences is decided based on statistical and linguistic features of sentences
input can be either single or multiple documents or sources of text
extractive summarization consists of three main steps namely intermediate representation of input text scoring of sentences based on the intermediate representation selection of sentences for summary generation
there are two approaches namely topic based and indicator based which are used for intermediate representation copyright verizon
all rights reserved
information contained herein is provided as is and subject to change without notice
all trademarks used herein are property of their respective owners
this work has now been submitted to the ieee for possible publication
copyright may be transferred without notice after which this version may no longer be accessible
of original text
topic representation based approaches transform input text into constituent topics
they are further subdivided into frequency driven topic word based cluster based latent semantic analysis dependent and bayesian topic model based methods
indicator representation based approaches characterize sentences in the input text through features such as sentence length position in the document having certain phrases
they can be further subdivided into graph theoretic fuzzy logic driven machine learning based and neural network based methods
b
latent semantic analysis latent semantic analysis lsa also known as latent semantic indexing lsi is an unsupervised method for extracting a representation of text semantics based on observed words
it tries to bring out latent relationships within a collection of documents on to a lower dimensional space
lsa is based on the principle that words that are close in meaning will occur in similar pieces of text the distributional hypothesis
it uses a value mathematical decomposition svd to identify patterns in the relationships between in unstructured texts
it was introduced by deerwester et al
in
gong et al
proposed a lsa based method to select highly ranked sentences for single and multi document news summarization
the terms and concepts contained technique singular called c
bayesian topic models topic modeling can be described as a type of a statistical method for finding a group of words i
e
topic from a collection of documents that best represents the information in the collection
bayesian topic models are unsupervised probabilistic models that uncover and represent the topics of documents or source texts
they have gained huge popularity in recent years
their advantage in describing and representing topics in detail enables the development of summarizer systems which can use them to determine the similarities and differences between documents to be used in summarization
there are many techniques which are used to obtain probabilistic topic models
latent dirichlet allocation lda is one such widely used topic modelling technique that represents the documents as a random mixture of latent topics where each topic is a probability distribution of words
it has been used in recent times for multi document summarization
wang et al
introduced a bayesian sentence based topic model for summarization which using both term document and term sentence associations achieved significant performance improvement and outperformed many other summarization methods
hierarchical dirichlet process hdp is another topic modeling technique which is an extension of lda
it is a a nonparametric bayesian mixed membership model for unsupervised analysis of grouped data
unlike lda its s finite counterpart hdp infers the number of topics from the data
approach which uses d
transformers transformers in nlp provide general purpose architectures for natural language understanding nlu and natural language generation nlg with over pre trained models
the earlier deep they were first introduced in
transformers are deep learning models that transform sequential inputs to sequential outputs
however they are based solely on attention recurrence and mechanisms dispensing entirely with learning architectures
convolutions of transformers do not require that the sequential data be processed in order which allows for much more parallelization than recurrent neural networks rnns and therefore reduced training times
since their introduction transformers have become the model of choice for tackling many problems in nlp replacing older recurrent neural network models such as the long short term memory lstm
transformer models can train on much larger datasets than before as they can support more parallelization during training
this has resulted in the as bidirectional development of pre trained encoder representations from transformers bert which have been trained with huge general language datasets and can be fine tuned to specific linguistic tasks
bert is a bidirectional transformer pre trained using a combination of language modeling objective and next sentence masked prediction on a large comprising the toronto book corpus and wikipedia by jointly conditioning on both left and right contexts in all layers
consequently a pre trained bert model can be fine tuned with just one additional output layer to create state of the art models for a wide range of nlp tasks
systems such the input transformers employ a layered encoder decoder architecture that comprises a stack of encoding layers that layer after another iteratively one processes and another stack of decoding layers that does the same thing to the output of the encoder
the encoders are all identical in structure
each one is broken down into two sub layers namely self attention and feed forward neural network
the decoder has one more layer between them which is an attention layer that helps it to focus on relevant parts of the input sentence similar to what attention does in models
therefore when we pass a sentence into a transformer it is embedded and passed into a stack of encoders
the output from the final encoder is passed into each decoder block in the decoder stack which then generates the output
e
embeddings embeddings are mathematical functions that map entities to a latent space with complex and meaningful dimensions
words or sentences or paragraphs can be mapped into a shared latent space such that the meaning of the word sentence paragraph can be represented geometrically
machine learning approaches towards nlp require words to be expressed in vector form
word embeddings proposed in is a feature engineering technique in which words are mapped into a vector of real numbers in a pre defined vector space
it is a learned representation for text where words that have the same meaning have a similar representation
the idea of using a dense distributed representation for each word is a key to the approach
glove
provide pre trained word embedding models in a type of transfer learning
embedding techniques initially focused on words but the attention soon started to shift to other types of copyright verizon
all rights reserved
information contained herein is provided as is and subject to change without notice
all trademarks used herein are property of their respective owners
this work has now been submitted to the ieee for possible publication
copyright may be transferred without notice after which this version may no longer be accessible
textual content such as n grams sentences and documents
the universal sentence encoder use encodes text into high dimensional vectors that can be used for text classification semantic similarity clustering and other natural language tasks
the model is trained and optimized for sentences phrases or short paragraphs from a variety of data sources with the aim of dynamically accommodating a wide variety of natural language understanding tasks
the model maps variable length input english text into an output of a dimensional vector
v
extractive summarization of call transcripts restoration this section provides a description of an extractive summarization technique that we are proposing for the summarization of call transcripts
this extractive summarization technique uniquely integrates channel speaker separation topic modeling and similarity based sentence selection with punctuation sequential through procedure method
the procedure is highly parameterized
the following are its ten self contained steps each with its brief description

call transcript channel speaker separation separate each call transcript into customer and agent transcripts based on its channel or speaker identifier by iterating through all transcripts
a step
partial punctuation restoration preprocess transcripts customer agent to remove existing punctuations and use a transformer based model to restore punctuations partially i
e
restore only periods as delimiters so that sentences can be separated in each transcript by iterating through all transcripts

document preparation preprocess transcripts and generate documents from customer and agent transcripts by iterating through all transcripts i
e
one document from each transcript where each document is a list of words from that transcript obtained through nlp pipeline based preprocessing

topic modeling build and optimize different types of topic models using the vocabularies corpus and documents from all of customer and agent transcripts and then pick the best customer and agent topic models based on their coherence scores
i
build different types of customer agent topic models e

lda lsi hdp by varying their topic number values hyper parameter e

within pre specified ranges and evaluate the models using their coherence scores

select the most optimal or near about topic models for customer and agent transcripts
the topic model type is a parameter of this procedure and so if it is provided during invocation then model optimization is confined to only that topic model type in step i for best model selection
ii

dominant topic identification get the most dominant from the aforesaid topic models with the associated keywords for each of customer and agent documents in every pair by iterating through all transcripts

significant term selection get the most relevant keywords terms from each pair of customer and agent transcripts by doing term based similarity analysis between the keywords of the corresponding dominant topics using one of the following two approaches by iterating through all transcripts
i
ii
global extraction extract terms from the keywords associated with each pair of dominant topics which need not be necessarily present in the transcripts customer agent themselves
local extraction extract terms from local to the customer and agent transcripts that are similar to the corresponding dominant topic keywords and are also similar to themselves
the choice for the term extraction method has also been parameterized for the procedure

summary generation fixed length user specified number customer and agent transcript summaries by iterating through all pairs of customer and agent transcripts
generate ii
i
identify the most unique sentences in each of customer and agent transcripts in every pair if necessary based on similarity analysis among all sentences of the corresponding transcript using embeddings
extract a fixed number user specified of most relevant sentences from each of customer and agent transcripts through sentence based similarity analysis between the every corresponding transcript and the string document created out of the most significant terms for that pair of transcripts step using embeddings
the desired length of the summary number of sentences sentence of is a parameter of the procedure

punctuation restoration remove existing periods from each pair of customer and agent summaries restore partial and full punctuation using a transformer based model and post process to make them more readable by iterating through all of them

summary tabulation save summaries of all transcripts in a table for future use

summarization efficacy determination evaluate summaries on content information and readability punctuation restoration by iterating through every pair of transcripts their corresponding summaries
i
ii
transcripts against original summary evaluation evaluate the goodness of summarization by comparing customer and agent summaries or manually generated summaries to generate average rouge scores
punctuation restoration evaluation evaluate the correctness of punctuation restoration by matching the number of punctuation symbols periods their partially between the extracted and copyright verizon
all rights reserved
information contained herein is provided as is and subject to change without notice
all trademarks used herein are property of their respective owners
this work has now been submitted to the ieee for possible publication
copyright may be transferred without notice after which this version may no longer be accessible
punctuated summaries for both customer and agent to generate the average accuracy scores
the full punctuation restored summaries from step are the outputs from this procedure
next we take a deeper look at some key steps of this procedure discuss their implementations in detail and provide algorithms where necessary
a
channel separation call transcripts include conversations dialogs between customer and one or more agents and so the resultant summaries can often get mixed up
separation of a transcript into customer and agent transcripts can make each summary more coherent
customer summaries can give better ideas of the problems while the agent summaries can give a better understanding of the causes or the solutions
the call transcripts are generally available to us as json formatted objects
hence channel separation involves extraction of the transcript string from json formatted object channel identification and then decomposition of the transcript into customer and agent transcripts using the associated channel identifiers
if the channel identifiers do not clearly identify the speakers then we can use a pre trained bert transformer model with a linear classifier from pytorch nn module as an additional layer on top of bert s layers to classify each dialog of the transcript into one of the classes i
e
customer and agent and then combine each type of dialogs to create customer and agent transcripts
b
document preparation a document is a list of keywords extracted from each transcript and is used as input to the topic model
for document preparation we have built a custom nlp preprocessing pipeline comprising of tokenization punctuation extended stop words small words length removal regular expression matching lowercasing contraction mapping bigrams and trigrams creation lemmatization parts of speech tagging allowable tag selection
this has been implemented by combining modules functionalities available from python packages namely re spacy nltk and genism
c
topic model optimization optimal model selection if the topic model type is specified at the invocation of the procedure then we create multiple topic models of the desired type for both customer and agent using the documents and vocabulary from the corresponding call transcripts by varying the hyper parameter e

topic number values within the pre defined ranges by the pre defined steps compute their coherence scores and identify the topic models and associated hyper parameter values that produce the best scores
otherwise by default we perform the above mentioned activity for all different topic model types namely lda lsi and hdp in parallel and identify the topic models and associated hyper parameter values that produce the best scores amongst topic models of all types
fig
displays the steps of this algorithm
for topic modeling we have exclusively used the python based genism package
fig
topic model optimization selection d
punctuation restoration here we describe the punctuation restoration algorithm used in steps and of the previously mentioned proposed procedure in detail
we have used the bertformaskedlm class of the pytorch bert model for punctuation restoration and added an additional linear layer pytorch nn module above the bert layers
the output of original bert layers is a vector with the size of all vocabulary
the additional linear layer takes this as input and gives as output one of four classes i
e
o other comma period and question for each encoded word
we retrained this modified bert model using ted transcripts consisting of two million words
different variations of punctuation restoration with bert model have been presented earlier but the retraining with the proposed architecture is a unique approach for punctuation restoration
the steps of this algorithm are as follows preprocess either transcript to remove duplicate words phrases and expressions or punctuations inserted as delimiters based on annotator s perceptions of the pauses in the conversation or a summary to remove periods
the output from this step is a continuous string representing the cleaned unpunctuated text of the transcript summary
instantiate the pre trained bert punctuation model and initialize it on gpus to classify each encoded word in text to one of classes namely other comma period and question
tokenize numeric berttokenizer
transcript summary and encode format identifier id token tokens using to the copyright verizon
all rights reserved
information contained herein is provided as is and subject to change without notice
all trademarks used herein are property of their respective owners
this work has now been submitted to the ieee for possible publication
copyright may be transferred without notice after which this version may no longer be accessible
create segments of surrounding token ids for each encoded word token id from the text and insert as a placeholder halfway through each segment
the segment size is a parameter of the algorithm
use the placeholders from the above step to predict punctuation class identifiers ids for all token ids in the the modified bert model segments classifications
through to sets restore map class ids to words symbols and merge words if needed punctuated transcripts summaries one with just periods partial punctuation restoration and the other with all punctuations full punctuation restoration
partial punctuation restoration is used in step while both partial and full punctuation restorations are used in step of the main procedure
of fig
displays the algorithm through block diagrams
we have found that the bert model for punctuation restoration gives more accurate results than the lstm based model
we have implemented the punctuation restoration algorithm using bert transformer bertpunc and nn modules available from pytorch
the algorithm are a pair of customer and agent transcripts and the corresponding optimized topic models documents from all transcripts
use selected topic models customer and agent to identify from each of customer agent documents for every pair and produce two lists of associated keywords one for each of customer agent transcripts
the number of dominant topics to be identified per transcript is a parameter of the procedure
use the keywords terms associated with customer and agent dominant topics to extract the most significant inter related terms for each pair of transcripts
this has been achieved using word based similarity analysis
as mentioned before in the main procedure we provide alternatives to keyword term extraction where the choice is parameterized
if a global extraction is desired then use the two lists of keywords associated with the corresponding customer and agent dominant topics in every pair and identify terms that are most similar to each other i
e
where the degree of similarity is above a certain parameterized threshold
the otherwise if a local extraction is desired then first find a set of terms from each of customer and agent documents in every pair that is most similar above parameterized the to threshold corresponding dominant transcript secondly identify terms from these two sets of terms extracted locally from customer and agent documents that are most inter related i
e
where degree of similarity is above the parameterized threshold
associated with that keywords for construct a string or document with the extracted significant terms for each pair of customer and agent transcripts
identify the most unique sentences in each of the customer and agent transcripts in every pair using embeddings and eliminate the redundant sentences to condense the original transcripts
this is achieved by generating a correlation matrix with the embeddings for all the sentences in the original transcript and removing those sentences whose correlations are above a certain pre specified threshold
the uniqueness threshold for sentence elimination is a parameter to the procedure
select a certain specified number parameter to the procedure of most important sentences from each of the condensed customer and agent transcripts in every pair that are most similar to the string constructed step of the current procedure using sentence based embeddings list and concatenate them in order subsequently present the results as the summaries for the corresponding transcripts
fig
illustrates the crux of this algorithm for any given pair of term based through block diagrams
for similarity analysis we have used algorithm based spacy s while sentence based similarity analysis and summary generation we have used the universal sentence encoder use from tensorflow hub along with the python based pandas and numpy packages
transcripts for fig
punctuation restoration e
summary generation through sentence selection next we present the algorithm for generating separate customer and agent summaries from each pair of transcripts through sentence selection starting with their corresponding topic models
in other words the following algorithm implements steps to of the proposed procedure
the inputs to copyright verizon
all rights reserved
information contained herein is provided as is and subject to change without notice
all trademarks used herein are property of their respective owners
this work has now been submitted to the ieee for possible publication
copyright may be transferred without notice after which this version may no longer be accessible
pre processing punctuations removalbert punctuation model instantiation token id based segment creations placeholder insertions punctuation class id predictions merging restorationinput transcript summarypunctuated output transcript summarytokenization encoding produce lists one each for extracted and partially punctuated summaries for each of customer and agent
count the number of matches between the two extracted lists of periods from the summaries of both customer agent and compute the above defined accuracy scores for both
in each case the number reflects the of periods from the partially punctuated periods only summary found in the extracted summary
we have computed the customer and agent rouge punctuation restoration accuracy scores for every pair of customer agent summaries by iterating through all transcripts calculated their averages from their respective individual scores
vi
performance evaluation user satisfaction effectiveness quality of summaries correctness of punctuations efficiency summarization time flexibility and performance comparison with another open source off the shelf extractive summarizer are some of the considerations that helped us evaluate the performance of our summarizer for call transcript summarization
a
experimental setup we set up a spark cluster consisting of a driver node and dynamically allocated multiple executor nodes for data collection preprocessing and summarization
the nvidia cuda deep neural network
accelerated our training process for punctuation restoration
we re trained and tested the modified bert transformer model on nvidia tesla gb gpu based nodes
the driver node used anywhere between to gpus
we have tested our extractive summarizer on separate samples from different use cases with of these samples consisting of around call transcripts and another larger one consisting of call transcripts
the evaluation of results have been both manual and automated
b
manual evaluation the summaries generated by the proposed method have been manually verified and validated for content and readability by different user groups for the four different use cases spread across multiple business units
the goal was to see if the summaries were deemed generally useful for the purposes they were used in the specific use cases
the process was informal in nature and the evaluation was subjective
we relied on user our customer feedback and let them manage and control their own evaluation process and satisfaction levels
four different user groups customers three internal and one external have now manually evaluated close to pairs of customer and agent summaries for the four different use cases
for user evaluations we were mainly looking for answers to questions which were both generic and specific to the use cases
the following are some examples of the two kinds
generic did the customer and agent summaries in general give a fair description of the main problems complaints and the resolutions based on the original unseparated call fig
summary generation through sentence selection f
summarization evaluation we have determined the effectiveness of the summarizer by measuring both the goodness quality of summarization and the correctness accuracy of the punctuation restoration reflecting the readability of the summaries
for the quality of the information content of the generated summaries we have used the metric rouge or bleu scores as a measurement of their goodness
we have compared the customer and agent summaries against the corresponding transcripts or manually generated summaries if available and computed their individual rouge l or bleu scores using the python packages rouge rouge and nltk nltk
translate

for the correctness of the punctuation restoration we have defined as punctuation restoration accuracy score to measure the accuracy of the algorithm
following named metric the definition

punctuation restoration accuracy score represents the number of matches of punctuation symbols text periods transcript summary text transcript summary expressed as a percentage
original extracted punctuated the and between the we have used the function from python s sklearn
metrics package to implement this metric
we have evaluated the effectiveness of the punctuation restoration algorithm through the following two steps
extract periods from both the extracted customer and agent summaries step of the main procedure together with their period only summaries step of the same procedure and copyright verizon
all rights reserved
information contained herein is provided as is and subject to change without notice
all trademarks used herein are property of their respective owners
this work has now been submitted to the ieee for possible publication
copyright may be transferred without notice after which this version may no longer be accessible
dominant identification with associated keywordssignificant terms selection unique sentences identificationimportant sentences selectioncustomer agent transcripts anddocuments corpus optimized topic modelsfixed length summariesstring document construction results and summarizer comparisons fig
and fig
a show short sentences summaries from two call transcripts after channel separations describing customer complaints about internet and phone service disruptions and agents confirmations about impending technicians visits
table i compares the effectiveness efficiency of the proposed summarizer for shorter summaries with those from the open source bert extractive summarizer using the four different samples on two different evaluation metrics
bert extractive summarizer generated the summaries using the period restored customer and agent transcripts from step of the proposed procedure
it s ratio parameter was automatically adjusted using the number of words in the transcript to ensure that its summaries were of comparable shorter lengths
this is important as we found that longer the summary the more similar it is to the original transcript and so higher the rouge score
for the larger sample sizes where manual summaries were not available we compared the generated summaries with the period restored original transcripts from step of our procedure for computing their corresponding rouge scores
this was done for the summaries from both the proposed method as well as the bert extractive summarizer to ensure that there was consistency and similarity in the comparisons
the results in table i establish that the proposed summarizer is more effective and efficient than bert extractive summarizer for call transcripts
the punctuation restoration accuracy scores for customer and agent summaries have also varied between in all cases
it may also be noted here that the proposed summarizer is highly parameterized and provides more options than the bert extractive summarizer
transcripts if so then what percentage of customer and agent summaries accurately summarized the content did the summaries help users better comprehend the information content of the transcripts than the original transcripts themselves did the summaries capture other secondary issues topics besides the main issue topic if so then how many did the punctuations help in making the summaries more readable for understanding the content of the transcripts have the punctuations been generally restored correctly how did our summaries compare with manually generated summaries if available note a manually generated summary would consist of a fixed number same as that for the automated of ordered sentences extracted manually from the period only customer and agent transcripts generated at step of the proposed procedure that the user would deem as most important from those transcripts
use case specific would the user be able to send the agent summaries as short text messages to the customers to prevent them from making repeat calls would the summaries indicate the possibility of churning for callers which are classified as churners what percentage of customer summaries included negative snippets for churn classification how did the summaries generated by the proposed step approach compare with summaries generated by several other open source off the shelf summarizers from transcripts which were originally ill punctuated with periods and where transcripts for external summarizers did nt go through an accurate period restoration step i
e
step of the proposed procedure
we have used the feedbacks from each use case to improve our method and the results
we are happy to report that different business units are now using our summaries for very different business purposes on an ongoing basis
these c
automated evaluation for automated evaluation we have looked at effectiveness and efficiency
for measuring the effectiveness of our summarization and for comparing performances with another popular open source extractive summarizer we have used the metric rouge l score
we have determined the efficacy of our punctuation own using punctuation restoration accuracy score metric
restoration algorithm our the efficiency of a summarizer is important to real world applications
we have measured the efficiency of our summarizer by recording the time taken by each of the steps of our proposed procedure method
we have also compared the efficiency of our summary generation through sentence extraction algorithm step with that of the same open source extractive summarizer by recording the time taken by each to summarize each of the four different samples
fig
internet service disruption confirmation of technician s visit copyright verizon
all rights reserved
information contained herein is provided as is and subject to change without notice
all trademarks used herein are property of their respective owners
this work has now been submitted to the ieee for possible publication
copyright may be transferred without notice after which this version may no longer be accessible
fig
customer complaint about phone internet disruption vii
conclusion in this paper we have presented an extractive summarization technique to address some of the challenges associated in general with call transcript summarization
we have combined channel separation topic modeling and sentence selection with punctuation restoration to generate more readable call transcript summaries in order to provide a better understanding of the customer complaints and the agent recommended solutions
this is perhaps the first summarizer that creates and evaluates multiple different types of topic models before selecting the most optimal one for summarization
we have provided a finer grained similarity analysis by using both term based similarities for significant term extraction and sentence based similarities for extractive summarization
this similarity analysis leverages both and use based embeddings to exploit the semantic contents of words and sentences to determine their significance uniqueness and relevance
the proposed extractive summarizer is the only one that restores full punctuation to the summaries generated from either ill punctuated or unpunctuated original call transcripts using a novel bert transformer based model
we have introduced a new metric to evaluate the accuracy of the punctuation restoration in the resultant summaries
finally we have established the efficacy of the proposed summarizer through extensive evaluations and performance comparisons
conflict of interest statement the authors state that they are all employees of verizon and this paper addresses work performed in course of authors employment
references mehdi allahyari seyedamin pouriyeh mehdi assefi saeid safaei elizabeth d
trippe juan b
gutierrez and krys kochut

text summarization techniques a brief survey
computing research repository arxiv

david m blei andrew y ng and michael i jordan

latent dirichlet allocation
the journal of machine learning research
a
bookstein s
t
klein and t
raita

detecting content bearing words by serial clustering
in proceedings of the acm sigir conference pages
ronald brandow karl mitze and lisa f
rau

automatic condensation of electronic publications by sentence selection
information processing and management
daniel cer yinfei yang sheng yi kong nan hua nicole limtiaco rhomni constant mario john noah guajardo cespedes steve yuan chris tar yun hsuan sung brian strope ray kurzweil

universal sentence encoder
computing research repository arxiv

st
fang chen kesong han and guilin chen

an approach to sentence selection based text summarization
proceedings of ieee
john m
conroy and dianne p


text summarization via hidden models
proceedings of the annual international acm sigir conference on research and development in information retrieval
pp

scott c
deerwester susan t dumais thomas k
landauer george w
furnas and richard a
harshman

indexing by latent semantic analysis
jasis
jacob devlin ming wei chang kenton lee kristina toutanova

bert pre training of deep bi directional transformers for language understanding
computing research repository arxiv

fig
agent s confirmation of technician s visit summarizer comparisons table i evaluation metric scores for extractive summarizers from samples of call transcripts copyright verizon
all rights reserved
information contained herein is provided as is and subject to change without notice
all trademarks used herein are property of their respective owners
this work has now been submitted to the ieee for possible publication
copyright may be transferred without notice after which this version may no longer be accessible
harold p edmundson

new methods in automatic extracting
journal of the acm jacm
book advances in artificial intelligence lecture notes in computer science springer berlin heidelberg vol
gunes erkan and dragomir r radev

lexrank graph based lexical centrality as salience in text summarization
j
artif
intell
res
jair
mohamed abdel fattah and fuji ren

automatic text summarization
proceedings of world academy of science engineering and technology vol feb
deepali k
gaikwad and c
namrata mahender

a review paper on text summarization
international journal of advanced research in computer and communication engineering vol
issue
pp

yihong gong and xin liu

generic text summarization using relevance measure and latent semantic analysis
in proceedings of the annual international acm sigir conference on research and development in information retrieval
acm
vishal gupta and gurpreet singh lehal

a survey of text summarization extractive techniques
journal of emerging technologies in web intelligence
karel jezek and josef steinberger

automatic text summarization
vaclav snasel ed
znalosti pp
isbn fiit stu brarislava ustav informatiky a softveroveho inzinierstva
karen sparck jones

automatic summarizing the state of the art
information processing management
ani nenkova and kathleen mckeown

a survey of text summarization techniques
in mining text data
springer
romain paulus caiming xiong and richard socher

a deep reinforced model for abstractive summarization
corr

dragomir r radev eduard hovy and kathleen mckeown

introduction to the special issue on summarization
computational linguistics
lawrence h
reeve hyoil han saya v
nagori jonathan c
yang tamara a
schwimmer ari d
brooks

concept frequency distribution in biomedical text summarization
acm conference on information and knowledge management cikm arlington va usa
david e rumelhart geoffrey e hintont and ronald j williams

representations by back propagating errors
nature learning
horacio saggion and thierry poibeau

automatic text summarization past present and future
in multi source multilingual information extraction and summarization
springer
c
s
saranyamol l
sindhu

a survey on automatic text international journal of computer science and summarization
information technologies vol
issue
atif khan and naomie salim

a review on abstractive summarization methods
journal of theoretical and applied information technology vol
no

abigail see peter j
liu christopher d
manning

get to the point summarization with pointer generator networks
computing research repository arxiv

mark steyvers and tom griffiths

probabilistic topic models
handbook of latent semantic analysis
ladda suanmali naomie salim mohammed salem binwahlan

fuzzy logic based method for improved text summarization
computing research repository arxiv

ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan n
gomez lukasz kaiser illia polosukhin

attention is all you need
computing research repository arxiv

sukriti verma and vagisha nidhi

extractive summarization using deep learning
computing research repository arxiv

dingding wang shenghuo zhu tao li and yihong gong

multidocument summarization using sentence based topic models
in proceedings of the acl ijcnlp conference short papers
zeqiu wu rik koncel kedziorski mari ostendorf and hannaneh hajishirzi

extracting summary knowledge graphs from long documents
computing research repository arxiv
jiacheng xu and greg durrett

neural extractive summarization with syntactic compression
in proceedings of emnlp ijcnlp pp

klaus zechner

a literature survey on information extraction and text summarization
computational linguistics program carnegie mellon university april
ming zhong pengfei liu yiran chen danqing wang xipeng qiu xuanjing huang

extractive summarization as text matching
in proceedings of annual meeting of acl pp

bert extractive summarizer khosrow kaikhah

automatic text summarization with neural networks
in proceedings of second international conference on intelligent systems ieee texas usa
j
kupiec j
pedersen and f
chen

a trainable document summarizer
in proceedings of the acmsigir conference pages
pirmin lemberger

deep for automatic summarization
computing research repository arxiv

chin yew lin

rouge a package for automatic evaluation of summaries
in text summarization branches out proceedings of the workshop

learning models hui lin and vincent ng

abstractive summarization a survey of the state of the art
in the proceedings of the aaai conference on artificial intelligence
yang liu

fine tune bert for extractive summarization
computing research repository arxiv

yang liu mirella lapata

text summarization with pre trained encoders
in proceedings of emnlp ijcnlp pp

hans peter luhn

the automatic creation of literature abstracts
ibm journal of research and development
inderjeet mani

automatic summarization
natural language processing
john benjamins publishing company
derek miller

leveraging bert for text summarization on lectures
computing research repository arxiv

e
mittendorf and p
schauble

document and passage retrieval based on hidden markov models
in proceedings of the acm sigir conference pages
ramesh nallapati feifei zhai and bowen zhou

summarunner a recurrent neural network based sequence model for extractive summarization of documents
in proceedings of the aaai conference on artificial intelligence
san francisco california usa pages
ramesh nallapati bowen zhou ccero nogueira dos santos c aglar gulcehre and bing xiang

abstractive text summarization using sequence to sequence rnns and beyond
in proceedings of the signll conference on computational natural language learning
berlin germany pages
shashi narayan shay b
cohen mirella lapata

ranking sentences for extractive summarization with reinforcement learning
in proceedings of naacl hlt pp

joel larocca neto alex a
freitas and celso a
a
kaestner

automatic text summarization using a machine learning approach
copyright verizon
all rights reserved
information contained herein is provided as is and subject to change without notice
all trademarks used herein are property of their respective owners
this work has now been submitted to the ieee for possible publication
copyright may be transferred without notice after which this version may no longer be accessible

