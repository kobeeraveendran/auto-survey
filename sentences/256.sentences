two level transformer and auxiliary coherence modeling for improved text segmentation goran and swapna and web science research group university of mannheim
uni mannheim
de testing service ets
org n a j l c
s c v
v i x r a abstract breaking down the structure of long texts into semantically coherent segments makes the texts more readable and supports downstream applications like summarization and retrieval
starting from an apparent link between text coherence and segmentation we introduce a novel supervised model for text segmentation with simple but explicit coherence modeling
our model a neural architecture consisting of two chically connected transformer networks is a multi task learning model that couples the sentence level segmentation objective with the coherence objective that differentiates rect sequences of sentences from corrupt ones
the proposed model dubbed coherence aware text segmentation cats yields state of the art segmentation performance on a tion of benchmark datasets
furthermore by coupling cats with cross lingual word embeddings we demonstrate its fectiveness in zero shot language transfer it can successfully segment texts in languages unseen in training
introduction natural language texts are more often than not a result of a deliberate cognitive effort of an author and as such consist of semantically coherent segments
text tion deals with automatically breaking down the structure of text into such topically contiguous segments i
e
it aims to identify the points of topic shift hearst choi brants chen and tsochantaridis riedl and biemann du buntine and johnson glavas nanni and ponzetto koshorek et al

reliable tion results with texts that are more readable for humans but also facilitates downstream tasks like automated text summarization angheluta de busser and moens bokaei sameti and liu passage retrieval huang et al
shtekh et al
topical classication zirn et al
or dialog modeling manuvinakurike et al
zhao and kawahara
text coherence is inherently tied to text segmentation intuitively the text within a segment is expected to be more coherent than the text spanning different segments
consider e

the text in figure with two topical segments
snippets and are more coherent than and all sentences relate to amsterdam s history and all sentences to terdam s geography in contrast and contain sentences figure snippet illustrating the relation i
e
dependency between text coherence and segmentation
from both topics
and being more coherent than and signals that the fourth sentence starts a new segment
given this duality between text segmentation and ence it is surprising that the methods for text segmentation capture coherence only implicitly
unsupervised tation models rely either on probabilistic topic modeling brants chen and tsochantaridis riedl and biemann du buntine and johnson or semantic similarity between sentences glavas nanni and ponzetto both of which only indirectly relate to text coherence
similarly a recently proposed state of the art supervised neural mentation model koshorek et al
directly learns to predict binary sentence level segmentation decisions and has no explicit mechanism for modeling coherence
in this work in contrast we propose a supervised neural model for text segmentation that explicitly takes coherence into account we augment the segmentation prediction tive with an auxiliary coherence modeling objective
our posed model dubbed coherence aware text segmentation cats encodes a sentence sequence using two cally connected transformer networks vaswani et al
devlin et al

similar to koshorek et al
cats main learning objective is a binary sentence level tation prediction
however cats augments the tation objective with an auxiliary coherence based amsterdam is younger than dutch cities such as nijmegen rotterdam and utrecht
amsterdam was granted city rights in either or
in the century amsterdam flourished because of trade with the hanseatic league
amsterdam is located in the western netherlands
the river amstel ends in the city centre and connects to numerous canals
amsterdam is about metres
feet below sea level
tive which pushes the model to predict higher coherence for original text snippets than for corrupt i
e
fake sentence sequences
we empirically show that even without the auxiliary coherence objective the two level transformer model for text segmentation tlt ts yields state of the art performance across multiple benchmarks that the full cats model with the auxiliary coherence modeling ther signicantly improves the segmentation and that both tlt ts and cats are robust in domain transfer
thermore we demonstrate models effectiveness in zero shot language transfer
coupled with a cross lingual word ding our models trained on english wikipedia cessfully segment texts from unseen languages ing the best performing unsupervised segmentation model glavas nanni and ponzetto by a wide margin
cats coherence aware two level transformer for text segmentation figure illustrates the high level architecture of the cats model
a snippet of text a sequence of sentences of xed length is an input to the model
token encodings are a catenation of a pretrained word embedding and a positional embedding
sentences are rst encoded from their tokens with a token level transformer vaswani et al

next we feed the sequence of obtained sentence representations to the second sentence level transformer
transformed i
e
contextualized sentence representations are next fed to the feed forward segmentation classier which makes a binary segmentation prediction for each sentence
we additionally feed the encoding of the whole snippet i
e
the sentence sequence to the coherence regressor a feed forward net which predicts a coherence score
in what follows we scribe each component in more detail
transformer based segmentation the segmentation decision for a sentence clearly does not depend only on its content but also on its context i
e
formation from neighboring sentences
in this work we employ the encoding stack of the attention based former architecture vaswani et al
to ize both token representations in a sentence and more portantly sentence representations within the snippet
we choose transfomer encoders because they have recently been reported to outperform recurrent encoders on a range of nlp tasks devlin et al
radford et al
shaw uszkoreit and vaswani and they are faster to train than recurrent nets
sentence encoding
let s


sk denote a single training instance a snippet consisting of k sentences and let each sentence si ti t be a xed size sequence of t tokens
following devlin et al
we prepend each sentence si with a special sentence start token


ti ti ruder sgaard and vulic glavas et al
for a comprehensive overview of methods for inducing cross lingual word embeddings
trim pad sentences longer shorter than t tokens
figure high level depiction of the coherence aware text segmentation cats model
input snippet sentence was granted city rights


in the century


is located in the





sk is about metres


word embedding lookuppositional embeddingtoken encoding layertoken level transformermulti head attentionadd normalizefeed forward netadd normalizenttxsentence representations sss


sk multi head attentionadd normalizefeed forward netadd normalizentsxsentence level transformerfeed forward netsoftmax


sk transformed sentence representationssegmentation classifierfeed forward netcoherence regressorcoherencescoresegmentation each sentence ti aiming to use the transformed representation of that token as the sentence encoding
we encode each j i


k j


t with a vector ti ken ti j which is the concatenation of a de dimensional word ding and a dp dimensional embedding of the position j
we use pretrained word embeddings and x them in training we learn positional embeddings as model s parameters
let transform t denote the encoder stack of the transformer model vaswani et al
consisting of nt t layers each coupling a multi head attention net with a feed forward net
we then apply transform t to the token sequence of each snippet sentence tti transform t the sentence encoding is then the transformed vector of the sentence start token tti
sentence contextualization
sentence encodings produced with transform t only capture the content of the sentence itself but not its context
we thus employ a second sentence level transformer transform s with nt s layers to produce context informed sentence representations
we prepend each sequence of non contextualized sentence beddings with a xed embedding denoting the snippet start token in order to capture the encoding of the whole snippet i
e
sequence of k sentences as the transformed embedding of the token transform s with the transformed vector being the encoding of the whole snippet s
segmentation classication
finally contextualized tence vectors ssi go into the segmentation classier a layer feed forward net coupled with softmax function yi softmax ssiwseg bseg with wseg and bseg as classier s parameters
let yi be the true segmentation label of the i th sentence
the segmentation loss jseg is then the simple negative log likelihood over all sentences of all n snippets in the training batch jseg ln yn i i
n k auxiliary coherence modeling given the obvious dependency between segmentation and coherence we pair the segmentation task with an auxiliary task of predicting snippet coherence
to this effect we couple each true snippet s from the original text with a corrupt i
e
incoherent snippet s created by randomly shufing the eliminates the need for an additional self attention layer for aggregating transformed token vectors into a sentence encoding
more details on the encoding stack of the transformer model see the original publication vaswani et al

order of sentences in s and randomly replacing sentences from s with other document sentences
let s s be a pair of a true snippet and its corrupt terpart and their respective encodings obtained with the two level transformer
the encodings of the rect snippet and the scrambled snippet are then presented to the coherence regressor which independently generates a coherence score for each of them
the scalar output of the coherence regressor is ys ys with wc and bc r as regressor s parameters
we then jointly softmax normalize the scores for s and s softmax
we want to force the model to produce higher coherence score for the correct snippet s than for its corrupt counterpart s
we thus dene the following contrastive margin based coherence objective jcoh max coh where coh is the margin by which we would like to be larger than
creating training instances our presumed training corpus contains documents that are generally longer than the snippet size k and annotated for segmentation at the sentence level
we create training stances by sliding a sentence window of size k over uments sentences with a stride of
for the sake of auxiliary coherence modeling for each original snippet s we create its corrupt counterpart s with the following ruption procedure we rst randomly shufe the order of sentences in s for percent of snippets random selection we additionally replace sentences of the shufed snippet with the probability with randomly chosen tences from other non overlapping document snippets
inference at inference time given a long document we need to make a binary segmentation decision for each sentence
our model however does not take individual sentences as input but rather sequences of k sentences i
e
snippets and makes in context segmentation prediction for each sentence
since we can create multiple different sequences of k tive sentences that contain some sentence our model can obtain multiple segmentation predictions for the same sentence
as we do not know apriori which of the snippets containing the sentence s is the most reliable with respect to the segmentation prediction for s we consider all ble snippets containing s
in other words at inference time unlike in training we create snippets by sliding the window of k sentences over the document with the stride of
let the sentence window with the stride of the m th sentence will in the general case be found in k different snippets m k m m k m


m k
s


sk be the set of at most k different snippets containing a sentence s
we then average the mentation probabilities predicted for the sentence s over all snippets in pseg s ys sk k sks finally we predict that s starts a new segment if pseg s where is the condence threshold tuned as a rameter of the model
cross lingual zero shot transfer models that do not require any language specic features other than pretrained word embeddings as input can at least conceptually be easily transferred to another guage by means of a cross lingual word embedding space ruder sgaard and vulic glavas et al

let be the monolingual embedding space of the source language most often english which we use in training and let be the independently trained embedding space of the target language to which we want to transfer the segmentation model
to transfer the model we need to project target language vectors from to the language space
there is a plethora of recently posed methods for inducing projection based cross lingual embeddings faruqui and dyer smith et al
artetxe labaka and agirre vulic et al
inter alia
we opt for the supervised alignment model based on solving the procrustes problem smith et al
due to its simplicity and competitive performance in zero shot guage transfer of nlp models glavas et al

given a limited size word translation training dictionary d we tain the linear projection matrix between and as follows with xs and xt as subsets of lingual spaces that align vectors from training translations pairs from d
once we obtain the language fer of the segmentation model is straightforward we put the embeddings of words from the projected space
experimental setup we rst describe datasets used for training and evaluation and then provide the details on the comparative evaluation setup and model optimization
data k corpus
koshorek et al
leveraged the manual structuring of wikipedia pages into sections to tomatically create a large segmentation annotated corpus
k consists of documents created from english en wikipedia pages divided into training development and test portions
we train mize and evaluate our models on respective portions of the k dataset
rst element i
e
index of the predicted vector y denotes the positive segmentation probability
standard test corpora
koshorek et al
ally created a small evaluation set to allow for comparative evaluation against unsupervised segmentation models e

the graphseg model of glavas nanni and ponzetto for which evaluation on large datasets is prohibitively slow
for years the synthetic dataset of choi was used as a standard becnhmark for text tation models
choi dataset contains documents each of which is a concatenation of paragraphs randomly pled from the brown corpus
choi dataset is divided into subsets containing only documents with specic variability of segment lengths e

segments with or with tences
finally we evaluate the performance of our models on two small datasets cities and elements created by chen et al
from wikipedia pages dedicated to the cities of the world and chemical elements respectively
other languages
in order to test the performance of our transformer based models in zero shot language fer setup we prepared small evaluation datasets in other languages
analogous to the dataset created by koshorek et al
from english en wikipedia we ated cs fi and tr datasets consisting of randomly selected pages from czech cs finnish fi and turkish tr wikipedia respectively
comparative evaluation evaluation metric
following previous work riedl and biemann glavas nanni and ponzetto koshorek et al
we also adopt the standard text segmentation measure pk beeferman berger and lafferty as our evaluation metric
pk score is the probability that a model makes a wrong prediction as to whether the rst and last tence of a randomly sampled snippet of k sentences belong to the same segment i
e
the probability of the model ing the same segment for the sentences from different ment or different segments for the sentences from the same segment
following glavas nanni and ponzetto koshorek et al
we set k to the half of the average ground truth segment size of the dataset
baseline models
we compare cats against the state the art neural segmentation model of koshorek et al
and against graphseg glavas nanni and ponzetto the state of the art unsupervised text segmentation model
additionally as a sanity check we evaluate the random baseline it assigns a positive segmentation label to a tence with the probability that corresponds to the ratio of the total number of segments according to the gold tion and total number of sentences in the dataset
koshorek et al
we evaluate our models on the whole choi corpus and not on specic subsets
our language transfer experiments we selected target guages from different families and linguistic typologies w

t english as our source language czech is like english an indo european language but as a slavic language it is unlike english fusional by type finnish is an uralic language fusionally agglutinative by type whereas turkish is a turkic language agglutinative by type
model conguration model variants
we evaluate two variants of our two level transformer text segmentation model with and without the auxiliary coherence modeling
the rst model tlt ts imizes only the segmentation objective jseg
cats our ond model is a multi task learning model that alternately minimizes the segmentation objective jseg and the ence objective jcoh
we adopt a balanced alternate training regime for cats in which a single parameter update based on the minimization of jseg is followed by a single parameter update based on the optimization of jcoh
word embeddings
in all our experiments we use dimensional monolingual fasttext word embeddings trained on the common crawl corpora of respective guages en cs fi and tr
we induce a cross lingual word embedding space needed for the zero shot language fer experiments by projecting cs fi and tr monolingual embedding spaces to the en embedding space
following smith et al
glavas et al
we create training dictionaries d for learning projection matrices by machine translating most frequent en words to cs fi and tr
model optimization
we optimize all hyperparameters including the data preparation parameters like the snippet size k via cross validation on the development portion of the k dataset
we found the following uration to lead to performance for both tlt ts and cats training instance preparation snippet size of k sentences with t tokens scrambling bilities
conguration of transformers nt t nt s layers and with attention heads per layer in both other model hyperparameters sitional embedding size of dp coherence objective contrastive margin of coh
we found different optimal inference thresholds
for the segmentation only ts model and
for the coherence aware cats model
we trained both tlt ts and cats in batches of n snippets each with k sentences using the adam timization algorithm kingma and ba with the initial learning rate set to
results and discussion we rst present and discuss the results that our models ts and cats yield on the previously introduced en uation datasets
we then report and analyze models mance in the cross lingual zero shot transfer experiments

com the large hyperparameter space and large training set we only searched over a limited size grid of hyperparameter rations
it is thus likely that a better performing conguration than the one reported can be found with a more extensive grid search
do not tune other transformer hyperparameters but rather adopt the recommended values from vaswani et al
lter size of and dropout probabilities of
for both attention layers and feed forward relu layers
base evaluation table shows models performance on ve en evaluation datasets
both our transformer based models tlt ts and cats outperform the competing supervised model of koshorek et al
a hierarchical encoder based on current components across the board
the improved formance that tlt ts has with respect to the model of koshorek et al
is consistent with improvements that transformer based architectures yield in comparison with models based on recurrent components in other nlp tasks vaswani et al
devlin et al

the gap in formance is particularly wide pk points for the ements dataset
evaluation on the elements test set is arguably closest to a true domain transfer while the train portion of the k set contains pages similar in type to those found in and cities test sets it does not contain any wikipedia pages about chemical elements all such pages are in the elements test set
this would suggest that tlt ts and cats offer more robust domain transfer than the recurrent model of koshorek et al

cats and consistently outperforms ts
this empirically conrms the usefulness of explicit herence modeling for text segmentation
moreover koshorek et al
report human performance on the dataset of
which is a mere one pk point better than the performance of our coherence aware cats model
the unsupervised graphseg model of glavas nanni and ponzetto seems to outperform all supervised els on the synthetic choi dataset
we believe that this is primarily because by being synthetic the choi dataset can be accurately segmented based on simple lexical overlaps and word embedding similarities and graphseg relies on similarities between averaged word embeddings and because by being trained on a much more challenging real world k on which lexical overlap is insufcient for accurate segmentation supervised models learn to ment based on deeper natural language understanding and learn not to encode lexical overlap as reliable segmentation signal
additionally graphseg is evaluated separately on each subset of the choi dataset for each of which it is provided the gold minimal segment size which further facilitates and improves its predicted segmentations
zero shot cross lingual transfer in table we show the results of our zero shot cross lingual transfer experiments
in this setting we use our based models trained on the english k dataset to segment texts from the x cs fi tr datasets in other languages
as a baseline we additionally evaluate graphseg glavas nanni and ponzetto as a language agnostic model requiring only pretrained word embeddings of the test language as input
choi dataset albeit from a different domain is thetic which impedes direct performance comparisons with other evaluation datasets
to the non parametric random shufing test yeh
for k choi and cities p
for and elements
model model type k choi cities elements random graphseg koshorek et al
tlt ts cats unsupervised unsupervised supervised supervised supervised
























table performance of text segmentation models on ve english evaluation datasets
graphseg model glavas nanni and ponzetto was evaluated independently on different subcorpora of the choi dataset indicated with an asterisk
model random graphseg tlt ts cats cs



fi



tr



table performance of text segmentation models in shot language transfer setting on the x cs fi tr datasets
both our transformer based models tlt ts and cats outperform the unsupervised graphseg model which seems to be only marginally better than the random line by a wide margin
the coherence aware cats model is again signicantly better p
for fi and p
for cs and tr than the tlt ts model which was trained to optimize only the segmentation objective
while the results on the fi tr datasets are not directly parable to the results reported on the en see table because the datasets in different languages do not contain mutually comparable wikipedia pages results in table still suggest that the drop in performance due to the cross lingual transfer is not big
this is quite encouraging as it suggests that it is possible to via the zero shot language transfer rather reliably segment texts from under resourced languages ing sufciently large gold segmented data needed to directly train language specic segmentation models that is robust neural segmentation models in particular
related work in this work we address the task of text segmentation we thus provide a detailed account of existing segmentation models
because our cats model has an auxiliary based objective we additionally provide a brief overview of research on modeling text coherence
text segmentation text segmentation tasks come in two main avors linear i
e
sequential text segmentation and chical segmentation in which top level segments are ther broken down into sub segments
while the cal segmentation received a non negligible research tion yaari eisenstein du buntine and son the vast majority of the proposed models cluding this work focus on linear segmentation hearst beeferman berger and lafferty choi brants chen and tsochantaridis misra et al
riedl and biemann glavas nanni and ponzetto koshorek et al
inter alia
in one of the pioneering segmentation efforts hearst proposed an unsupervised texttiling algorithm based on the lexical overlap between adjacent sentences and graphs
choi computes the similarities between tences in a similar fashion but renormalizes them within the local context the segments are then obtained through divisive clustering
utiyama and isahara and fragkou petridis and kehagias minimize the segmentation cost via exhaustive search with dynamic programming
following the assumption that topical cohesion guides the segmentation of the text a number of segmentation proaches based on topic models have been proposed
brants chen and tsochantaridis induce latent tions of text snippets using probabilistic latent semantic ysis hofmann and segment based on similarities tween latent representations of adjacent snippets
misra et al
and riedl and biemann leverage topic tors of snippets obtained with the latent dirichlet allocation model blei ng and jordan
while misra et al
nds a globally optimal segmentation based on the ties of snippets topic vectors using dynamic programming riedl and biemann adjust the texttiling model of hearst to use topic vectors instead of sparse ized representations of snippets
malioutov and barzilay proposed a rst based model for text segmentation
they segment lecture transcripts by rst inducing a fully connected sentence graph with edge weights corresponding to cosine similarities tween sparse bag of word sentence vectors and then running a minimum normalized multiway cut algorithm to obtain the segments
glavas nanni and ponzetto propose graphseg a graph based segmentation algorithm similar in nature to malioutov and barzilay which uses dense sentence vectors obtained by aggregating word embeddings to compute intra sentence similarities and performs tation based on the cliques of the similarity graph
finally koshorek et al
identify wikipedia as a free large scale source of manually segmented texts that can be used to train a supervised segmentation model
they train a neural model that hierarchically combines two bidirectional lstm networks and report massive improvements over pervised segmentation on a range of evaluation datasets
the model we presented in this work has a similar hierarchical chitecture but uses transfomer networks instead of recurrent encoders
crucially cats additionally denes an auxiliary coherence objective which is coupled with the primary segmentation objective in a multi task learning model
text coherence measuring text coherence amounts to predicting a score that indicates how meaningful the order of the information in the text is
the majority of the proposed text coherence models are grounded in formal theories of text coherence among which the entity grid model barzilay and lapata based on the centering theory of grosz weinstein and joshi is arguably the most popular
the entity grid model represent texts as matrices encoding the grammatical roles that the same entities have in different sentences
the tity grid model as well as its extensions elsner and niak feng and hirst feng lin and hirst nguyen and joty require text to be preprocessed entities extracted and grammatical roles assigned to them which prohibits an end to end model training
in contrast li and hovy train a neural model that couples recurrent and recursive sentence encoders with a convolutional encoder of sentence sequences in an end to end fashion on limited size datasets with gold coherence scores
our models architecture is conceptually similar but we use transformer networks to both encode sentences and sentence sequences
with the goal of supporting text segmentation and not aiming to predict exact coherence scores our model does not require gold coherence labels instead we devise a coherence objective that contrasts original text snippets against corrupted sentence sequences
conclusion though the segmentation of text depends on its local herence existing segmentation models capture coherence only implicitly via lexical or semantic overlap of adjacent sentences
in this work we presented cats a novel vised model for text segmentation that couples segmentation prediction with explicit auxiliary coherence modeling
cats is a neural architecture consisting of two hierarchically nected transformer networks the lower level sentence coder generates input for the higher level encoder of sentence sequences
we train the model in a multi task learning setup by learning to predict sentence segmentation labels and that original text snippets are more coherent than corrupt sentence sequences
we show that cats yields state of art performance on several text segmentation benchmarks and that it can in a zero shot language transfer setting coupled with a cross lingual word embedding space successfully segment texts from target languages unseen in training
although effective for text segmentation our coherence modeling is still rather simple we use only fully randomly shufed sequences as examples of highly incoherent text
in subsequent work we will investigate negative instances of different degree of incoherence as well as more elaborate objectives for auxiliary modeling of text coherence
references angheluta r
de busser r
and moens m


the use of topic segmentation for automatic summarization
in proc
of the workshop on automatic tion
artetxe m
labaka g
and agirre e

a robust self learning method for fully unsupervised cross lingual mappings of word embeddings
in proc
of acl
barzilay r
and lapata m

modeling local ence an entity based approach
computational linguistics
beeferman d
berger a
and lafferty j

statistical models for text segmentation
machine learning
blei d
m
ng a
y
and jordan m
i

latent dirichlet allocation
journal of machine learning research
bokaei m
h
sameti h
and liu y

extractive summarization of multi party meetings through discourse segmentation
natural language engineering
brants t
chen f
and tsochantaridis i

topic based document segmentation with probabilistic latent semantic analysis
in proc
of cikm
acm
chen h
branavan s
barzilay r
and karger d
r

global models of document structure using latent tions
in proc
of human language technologies the annual conference of the north american chapter of the association for computational linguistics
ation for computational linguistics
choi f
y

advances in domain independent linear text segmentation
in meeting of the north american chapter of the association for computational linguistics
devlin j
chang m

lee k
and toutanova k

bert pre training of deep bidirectional transformers for guage understanding
arxiv preprint

du l
buntine w
and johnson m

topic in proc
of the mentation with a structured topic model
conference of the north american chapter of the sociation for computational linguistics human language technologies
eisenstein j

hierarchical text segmentation from multi scale lexical cohesion
in proc
of hlt naacl
association for computational linguistics
elsner m
and charniak e

extending the entity grid with entity specic features
in proc
of the annual meeting of the association for computational linguistics human language technologies
faruqui m
and dyer c

improving vector space word representations using multilingual correlation
in proc
of eacl
feng v
w
and hirst g

extending the based coherence model with multiple ranks
in proc
of the conference of the european chapter of the association for computational linguistics
association for computational linguistics
tion for computational linguistics volume long papers
radford a
narasimhan k
salimans t
and sutskever i

improving language understanding by generative pre training
technical report
preprint
riedl m
and biemann c

topictiling a text tation algorithm based on lda
in proc
of acl student research workshop
association for computational linguistics
ruder s
sgaard a
and vulic i

a survey of lingual embedding models
arxiv preprint

shaw p
uszkoreit j
and vaswani a

self attention with relative position representations
in proc
of the conference of the north american chapter of the association for computational linguistics human language gies volume short papers
shtekh g
kazakova p
nikitinsky n
and skachkov n

exploring inuence of topic segmentation on tion retrieval quality
in international conference on internet science
springer
smith s
l
turban d
h
hamblin s
and hammerla n
y

ofine bilingual word vectors orthogonal transformations and the inverted softmax
in proc
of iclr
utiyama m
and isahara h

a statistical model for domain independent text segmentation
in proc
of acl
vaswani a
shazeer n
parmar n
uszkoreit j
jones l
gomez a
n
kaiser
and polosukhin i

attention is all you need
in advances in neural information processing systems
vulic i
glavas g
reichart r
and korhonen a

do we really need fully unsupervised cross lingual dings in proceedings of the conference on empirical methods in natural language processing and the national joint conference on natural language processing emnlp ijcnlp
yaari y

segmentation of expository texts by chical agglomerative clustering
in proc
of ranlp
yeh a

more accurate tests for the statistical cance of result differences
in proc
of coling
zhao t
and kawahara t

joint learning of dialog act segmentation and recognition in spoken dialog using neural networks
in proc
of ijcnlp
zirn c
glavas g
nanni f
eichorts j
and schmidt h

classifying topics and detecting topic shifts in political manifestos
in proceedings of the tional conference on the advances in computational sis of political text
university of zagreb
feng v
w
lin z
and hirst g

the impact of deep hierarchical discourse structures in the evaluation of text coherence
in proc
of coling the international conference on computational linguistics technical papers
fragkou p
petridis v
and kehagias a

a dynamic programming algorithm for linear text segmentation
journal of intelligent information systems
glavas g
litschko r
ruder s
and vulic i

how to properly evaluate cross lingual word embeddings on strong baselines comparative analyses and some ceptions
in proceedings of the annual meeting of the association for computational linguistics
rence italy association for computational linguistics
glavas g
nanni f
and ponzetto s
p

unsupervised text segmentation using semantic relatedness graphs
in proc
of the fifth joint conference on lexical and computational semantics
grosz b
j
weinstein s
and joshi a
k

centering a framework for modeling the local coherence of discourse
computational linguistics
hearst m
a

multi paragraph segmentation of tory text
in proc
of the annual meeting on association for computational linguistics
association for putational linguistics
hofmann t

probabilistic latent semantic analysis
in proc
of the fifteenth conference on uncertainty in articial intelligence
morgan kaufmann publishers inc
huang x
peng f
schuurmans d
cercone n
and robertson s
e

applying machine learning to text segmentation for information retrieval
information retrieval
kingma d
p
and ba j

adam a method for tic optimization
arxiv preprint

koshorek o
cohen a
mor n
rotman m
and berant j

text segmentation as a supervised learning task
in proc
of the conference of the north american chapter of the association for computational linguistics human language technologies volume short papers
li j
and hovy e

a model of coherence based on distributed sentence representation
in proc
of the conference on empirical methods in natural language cessing emnlp
malioutov i
and barzilay r

minimum cut model for spoken lecture segmentation
in proc
of coling acl
association for computational linguistics
manuvinakurike r
paetzel m
qu c
schlangen d
and devault d

toward incremental dialogue act segmentation in fast paced interactive dialogue systems
in proc
of the annual meeting of the special interest group on discourse and dialogue
misra h
yvon f
jose j
m
and cappe o

text segmentation via topic modeling an analytical study
in proc
of cikm
acm
nguyen d
t
and joty s

a neural local coherence model
in proc
of the annual meeting of the
