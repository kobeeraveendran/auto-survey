few shot learning of an interleaved text summarization model by pretraining with synthetic data sanjeev kumar francine chen yan ying ulli and hinrich sch for information and language processing cis lmu munich intelligence siemens healthineers princeton research institute los altos california intelligence siemens ag munich
lmu
de
org yan ying

global ulli

com
org r a m l c
s c v
v i x r a abstract interleaved texts where posts belonging to ferent threads occur in a sequence commonly occur in online chat posts so that it can be time consuming to quickly obtain an overview of the discussions
existing systems entangle the posts by threads and then extract summaries from those threads
a major issue with such systems is error propagation from the disentanglement component
while to end trainable summarization system could obviate explicit disentanglement such systems require a large amount of labeled data
to address this we propose to pretrain an to end trainable hierarchical encoder decoder system using synthetic interleaved texts
we show that by ne tuning on a real world ing dataset ami such a system out performs a traditional two step system by
we also compare against transformer models and observed that pretraining with synthetic data both the encoder and decoder outperforms the transformer model which trains only the encoder on a large dataset
introduction interleaved texts are increasingly common ring in social media conversations such as slack and stack exchange where posts belonging to ferent threads may be intermixed in the post quence see a meeting transcript from the ami corpus mccowan et al
in table
due to the mixing getting a quick sense of the different conversational threads is often difcult
in conversation disentanglement interleaved posts are grouped by the thread
however a reader still has to read all posts in each cluster of threads to get the gist
shang et al
proposed a two step system that takes an interleaved text as input and rst disentangles the posts thread wise by clustering and then compresses the thread wise posts to single sentence summaries
however entanglement e

wang and oard gates error to the downstream summarization task
an end to end supervised summarization system that implicitly identies the conversations would eliminate error propagation
however labeling of interleaved texts is a difcult and expensive task barker et al
aker et al
verberne et al

ami utteracnes


who is gon na do a powerpoint presentation think we all huh
you will





and uh the sender will send to the telly itself an red signal to tell it to switch on or switch





y so it s so it s so you got so that s something we should have a look into then i when desi when designing the ergonomics of see have a look





the little tiny weeny batteries all like special lasting batteries





summary the project manager had the team members re introduce


the industrial designer discussed the interior workings of a remote and the team discussed options for batteries and infra red signals



the marketing expert presented research on consumer preferences on remotes in general and on voice recognition and the team discussed the option to have an ergonomically designed remote



table the top section shows ami asr scripts and the bottom section shows human written summaries
and utterances
a refer to the ath sentence in a multi sentence mary
we propose a pretraining approach to tackle these issues
we synthesized a corpus of leaved text summary pairs out of a corpus of ular document summary pairs and train an end end trainable encoder decoder system
to generate the summary the model learns to infer gle the major topics in several threads
we show on synthetic and real world data that the decoder system not only obviates a ment component but also enhances performance
thus the summarization task acts as an auxiliary task for the disentanglement
additionally we show that ne tuning of the encoder decode system with the learned disentanglement representations on a real world ami dataset achieves a tial increment in evaluation metrics despite a small number of labels
we also propose using a hierarchical attention in the encoder decoder system with three levels of formation from the interleaved text posts phrases and words rather than traditional two levels post and word nallapati et al
tan et al
cheng and lapata
the remaining paper is structured as follows
in section we discuss related work
in section we provide a detailed description of our in section we provide a cal model
detailed description on the synthetic data creation algorithm
in section we describe and discuss the experiments
and in section we present our conclusions
related work ma et al
aker et al
shang et al
each designed a system that summarizes posts in multi party conversations in order to vide readers with overview on the discussed ters
they broadly follow the same two step proach cluster the posts and then extract a mary from each cluster
there are two kinds of summarization tive and extractive
in abstractive summarization the model utilizes a level vocabulary and generates novel sentences as the summary while extractive models extract or rearrange the source words as the summary
abstractive models based on neural sequence to sequence rush et al
proved to generate summaries with higher rouge scores than the feature based stractive models
li et al
proposed an encoder decoder auto encoder model that utilizes a hierarchy of networks word to word followed by sentence sentence
their model is better at capturing the derlying structure than a vanilla sequential decoder model
krause et al
and jing et al
showed multi sentence ing of an image through a hierarchical recurrent neural network rnn topic to topic followed by word to word is better than
these works suggest a hierarchical decoder thread thread followed by word to word may intrinsically disentangle the posts and therefore generate more appropriate summaries
integration of attention into a model bahdanau et al
led to further advancement of abstractive summarization nallapati et al
chopra et al

nallapati et al
devised a hierarchical attention mechanism for a model where two levels of attention distributions over the source i
e
sentence and word are puted at every step of the word decoding
based on the sentence attentions the word attentions are rescaled
our hierarchical attention is more itive computes and level attentions for every new summary sentence and is trained end to end
semi supervised learning has recently gained popularity as it helps training parameters of large models without any training data
researchers have pre trained masked language models in which only an encoder is used to reconstruct the text e

bert devlin et al

liu and lapata used bert as encoder and showed proved performance on several abstractive marization tasks
similarly researchers have lished pre trained models using a ent semi supervised learning technique where a model is learned to reconstruct the original text e

bart lewis et al
and mass song et al

in this work we rely on transfer learning and demonstrate that by pretraining with appropriate interleaved text data a model readily transfers to a new domain with just a few examples
model our hierarchical encoder see figure left hand section is based on nallapati et al
where word to word and post to post encoders are directional lstms
the word to word bilstm encoder runs over word embeddings of post pi and generates a set of hidden tions of d dimensions
the average pooled value of the word to word resentations of post pi is p put to the post to post bilstm encoder t which then generates a set of representations corresponding to the posts






i p i j n figure hierarchical encoder decoder architecture
on the left interleaved posts are encoded hierarchically i
e
word to word followed by post to post
on the right summaries are generated hierarchically thread to thread t followed by word to word t
overall for a given channel c output tions of word to word w and post to post p has and dimensions respectively
the hierarchical decoder has two uni directional lstm decoders thread to thread and word to word see right hand side in figure



k at step of thread decoder t we pute elements of post level attention as k i t pi i


n where attn aligns the current thread decoder state vector t to vectors of matrix pi
a phrase is a short quences of words in a sentence post
phrases in interleaved texts are equivalent to visual patterns in images and therefore attending phrases are more relevant for thread recognition than attending posts
thus we have phrase level attentions focusing on words in a channel and with a responsibility of entangling threads
at step of thread decoder we also compute a sequence of attention weights k n corresponding to the set of


encoded word representations n p as ai j where ai j j pi i


n j


p
add aligns a post representation to its word resentations and does element wise addition and attn maps the current thread decoder state t and vector ai j to a scalar value
then we use the post level attention k to rescale the sequence of attention weights k to obtain phrase level tions k as k i j k i
a weighted representation of the words crossed blue circle k i jwij is used as an input to compute the next state of the thread thread decoder t
additionally we also use the last hidden state of the word to word i j t i j decoder lstm of the previously ated summary sentence as the second input to t
the motivation is to provide information about the previous sentence
the current state t is passed through a gle layer feedforward network and a distribution over and is computed pst op where g is a feedforward k network
in figure the process is depicted by a yellow circle
the thread to thread decoder keeps decoding until pst op is greater than

t k additionally the new state t and inputs to t at that step are passed through a two layer feedforward network r followed by a dropout layer to compute the thread representation sk
given a thread representation sk the word word decoder a unidirectional attentional lstm generates a summary for the thread see the right hand side of figure
our word to word decoder is based on bahdanau et al

at step l of word to word decoding of summary of thread k we compute elements of word level attention i
e
l i we refer to bahdanau et al
for further details on it
however we use phrase level word attentions for rescaling the word level attention as k l i j norm k ij where norm softmax renormalizes the values
thus contrary to popular two level hierarchical attention nallapati et al
cheng and lapata tan et al
we have three levels of cal attention and each with its responsibility and is coordinated through the rescaling operation
i j k l we train minimize to our the model following end to end notations c a sequence of pairs of single thread texts and sentence summaries a minimum number of threads a maximum number of threads a minimum number of posts a maximum number of posts a b m n window returns an iterator object that traverse a given sequence w t o e elements in windowed manner window size step size an iterator object that returns a window size sequence of pairs of single thread texts and single sentence maries a window size sequence of pairs of single thread texts and single sentence summaries a sequence of sentences a sequence of single sentence summaries a uniform sampling function a single thread text as a sequence of sentences a single sentence summary as a sequence of words a multi thread interleaved text a multi sentence summary of an interleaved text a set in which variable a is repeated times a sequence of positive integers removes the last element from an array a sentence post as a sequence of words array indexing operation a set of elements that belong to x but not to y a sequence of pairs of multi thread interleaved texts and multi sentence summaries u a t i m s reverse reverses an array pop p x y z table we use lowercase italics for variables percase italics for sets and sequences math symbols for mathematical operations and uppercase words for methods
m op tive log p yst op m and


y are words in a ground truth summary and generation respectively
where


w k synthetic dataset obtaining labeled training data for interleaved versation summarization is challenging
the able ones are either extractive verberne et al
or too small barker et al
anguera et al
to train a neural model and thoroughly ify the architecture
to get around this issue we synthesized a dataset by utilizing a corpus of ventional texts for which summaries are available
we created a corpus of interleaved texts from the abstracts and titles of articles from the pubmed corpus dernoncourt and lee
we chose pubmed abstracts as it has in contrast to other pora such as news articles or stackoverow posts a single sentence summary that can only be prehended out of a whole abstract
further the number of sentences more closely resembles that of a conversationalist in a conversation
selection e s o
next array array r for j to r do a t q n

s s algorithm interleaving algorithm procedure a m n o z w t array while o do k p
pop i
t if t m then m
i m l array array for to l do s s z
m return z interleaving random interleaving of the sentences from a small number of pubmed abstracts roughly bles interleaved texts and correspondingly leaving of titles resembles its multi sentence mary
we devised an algorithm for creating thetic interleaved texts based on this idea
interleave algorithm the interleave rithm generates interleaved texts each containing randomly interleaved sentences from a small ber of abstracts where the number is a random value within a specied range
the number of tences used per abstract is also a random value within a specied range
abstracts to be included in an interleaved text are rst selected then the selected abstracts are interleaved and nally the interleaved texts together with a concatenation of the titles of the selected abstracts are returned
we rst refer to table for terms and tations used in algorithm

interleave takes a corpus of abstract title pairs c


an tn and returns a quence of pairs of multi thread interleaved texts and multi sentence summaries z
each leaved text in the generated sequence will contain a number of threads ranging between a to b where the number is randomly selected
each thread in turn will contain a number of posts or sentences ranging between m and n where this number is also randomly selected
the window function is given c a desired window size w and step size t and it returns an iterator object o of size t
window helps to enlarge the interleaved corpus without redundancy as abstracts are randomly pled out of an iterator element e and also new abstracts are always included in the next element through sliding
similarly sets of sentences are domly sampled out of the selected abstracts
thus interleaved text summary pairs in the corpus are different
the two parts of the interleave rithm selection and interleaving will be described next
selection u in step determines the number of threads
then thread candidates for an leaved text are chosen out of an iterator element e a window size sequence of pairs of single thread texts and single sentence summaries
next post candidates for each selected thread a are sen
u in step determines the number of posts q
thread indices are repeated as many times as its posts and stored in a set s
interleaving in every step in a loop of a size equivalent to the length of indices s u randomly selects a thread index
reverse and pop in step help in selecting a post p in the selected thread in a fifo manner
the single sentence summary t of the thread is added to the multi sentence mary sequence m if it did nt exist previously
as an interleaved text summary pair in the pus has a thread size between a and b and post size per thread between m and n the larger the ference between a and b and m and in a corpus the harder the disentangling and summarization task
so we vary these parameters and create ferent synthetic corpora of varying difculty for the experiments
table shows an example of a data instance from a interleaved pubmed corpus compiled using and
experiments parameters for the word to word encoder the steps are limited to while the steps in the to word decoder are limited to
the steps in the post to post encoder and thread to thread decoder depend on the corpus type e

a hard corpus piled using and has steps in post to post encoder i
e
bn the maximum sible size of posts in an item in the corpus and steps in thread to thread decoder i
e
the mum possible threads in an item in the corpus
we initialized all weights including word embeddings with a random normal distribution with mean and standard deviation

the embedding vectors and interleaved text


conducted to evaluate the inuence of excessive ing during long distance running on the urinary concentration of caffeine





to assess the effect of a program of supervised tness walking and patient education on functional status








patients with a documented diagnosis of primary osteoarthritis of one or both knees participated





examined the effects of intensity of training on ratings of perceived exertion





summary caffeine in sport
inuence of endurance exercise on the urinary caffeine concentration
supervised tness walking in patients with osteoarthritis of the knee
a randomized controlled trial
the effect of training intensity on ratings of perceived tion
table an example of a synthetic interleaved text and summary pair compiled using pubmed corpus and gorithm

it includes three threads abstracts able through superscribed symbols and
model input text dis upper bd shang et al
ent rouge l








table synthetic interleaved text summarization performance rouge recall scores comparing models when the threads are disentangled top section upper bound and when the threads are entangled bottom tion real world on an interleaved pubmed corpus
dis disentangled ground truth and ent entangled
hidden states of the encoder and decoder in the models are set to dimension
texts are cased
the vocabulary size is limited to
we pad short sequences with a special token
we use adam kingma and ba with an tial learning rate of
and batch size of for training
the training evaluation and test sets in a hard interleaved pubmed corpus and are of sizes of and tively
we report and l as the quantitative evaluation of the models
upper bound in upper bound experiments we check the impact of disentanglement on the tive summarization models
in order to do this we rst evaluate the performance of a model when provided the ground truth disentanglement thread indices information
we also evaluate the mance of models for either end to end or two step summarization
ground truth disentangled the ground truth disentanglement information is used and posts of threads are disentangled and concatenated posts are thread wise sequentially arranged i
e
interleaved
the rst row in table shows formance of the summarization model
clearly the model can easily detect a thread ary in concatenated threads and perform very well and therefore sets an upper bound for the task
no disentanglement in real world ios i
e
with no disentanglement shang et al
s unsupervised two step system entangles clusters the posts thread wise and then compresses clusters to single sentence summaries
while is trained end to end and therefore generates multi sentence summaries for a given terleaved text
table shows shang et al
performs worse than compare rows and indicating that a model trained on a sufciently large dataset is better at tion than the unsupervised sentence compression method especially in uency as indicated by an approximately point increase in
tionally the model trained on entangled texts achieves slightly lower performance to when it is trained on disentangled texts compare rows and indicating that the disentanglement ponent can be avoided if summaries are available
the bottom section in table show an example of the model generations shown in color
the top indexes of the phrase level attention is directly visualized in the table through the color coding matching the generation
this shows phrase level attention actually supports in learning to gle the interleaved texts
transfer learning we utilize our interleaving algorithm and pubmed data to compile an leaved corpus with a similar thread distribution as a corpus of real meetings the ami meeting corpus
ami is a very small size corpus so we have a train eval and test split of and respectively
our analysis of the ami show that of meetings have summary sentences while of meetings have summary sentences so we used and as the min a and max b number of threads respectively in the algorithm and ate a synthetic corpus
we pretrain the model for several iterations on the synthetic corpus and then transfer and ne tune the model on the ami corpus with all parameters xed except for the word to word decoder and hierarchical tion parameters
as pubmed and ami are from interleaved text


conducted to evaluate the inuence of excessive ing during long distance running on the urinary tion of caffeine





to assess the effect of a program of supervised tness walking and patient education on functional status








patients with a documented diagnosis of primary teoarthritis of one or both knees participated





examined the effects of intensity of training on ratings of perceived exertion





generation effect of excessive during running on the urinary concentration of caffeine
effect of a physical tness walking on functional status pain and pain effects of intensity of training on perceived in athletes
table an example of generated summary sentences of a three thread interleaved text
summaries are coloured differently and colors of attended phrases in the text are identical to those of the generations
the table is best viewed in color
figure rouge and bi gram precision green and recall blue of ami ne tuned models with different numbers of pretraining iterations
mum words in a summary is
as a reference solid horizontal lines show the scores of a model trained only on ami
different domains we use the byte pair encoding bpe sennrich et al
based subword tionary
as shown in table readily transfers its disentangling knowledge and fore obtains a boost in recall while maintaining its precision
the li et al
system has the best scores however their model is not directly comparable as unlike shang et al
and our text based model it uses audio and video in addition to text
additionally we also performed transfer ing experiments with models pre trained for a ferent number of iterations and as seen in figure readily transfers its disentangling edge and therefore obtains a boost in recall while maintaining its precision
however longer ing drives the model to generate shorter summaries similar to pubmed abstracts and thereby results in increasing precision and decreasing recall
we also experimented with state of the art transformer based models e

sumextabs liu and lapata and bart lewis et al

requires tuning of the encoder and a novo training of decoder while both encoder and decoder of bart are only ne tuned
we use only ami data for the novo training and ne tuning purpose and the bottom two rows in table show the results from these models
although our t learn also only requires ne tuning of the decoder and hierarchical attention a highly sophisticated supervised training of both the encoder and decoder of bart and larger model size yields better performance
however for applications that have limited memory as on some mobile devices our model may be more desirable
furthermore spite a pre trained encoder of a novo training of a large size decoder with a tiny ami data lead to and therefore lower scores
human evaluation we also performed a itative evaluation of our system using human ments
following chen and bansal we performed a comparative evaluation where we vided six human judges graduate students uent in english with meetings words and maries from three sources i
e
human reference two step baseline and t learn here after referred to as the our model and asked them to rate on a scale of to the two questions is the summary concise uent and grammatical uency and does the summary retain key information from the meeting relevancy we sampled six meetings each with three maries corresponding to three sources duplicated them and then randomly sampled two dissimilar meetings and assigned them to each judge to tate
for reference an annotation sample would be an asr transcript and human written summaries model two step shang et al
t learn li et al
p r















p r















rouge scores for summary size table words on the ami corpus
t learn transfer leaning
bart with encoder and decoder ers and m parameters
li et al
uses in addition to text and the transformer models the bottom two rows have lots of extra data for pre training
t learn summaries the project manager opened the meeting and went over the minutes of the previous





the industrial designer discussed the interior workings of a remote and the team





the group discussed the shape of the device and decided to make the device easier





two step shang et al
summaries marketing report uh we observed remote control users in a usability lab


majority except for the forty ve to fty ve year olds for some reason did nt want a voice act speech





headed towards like a b a big yellow and black remote as far as maybe that s our next meeting that we discuss that


table the top and bottom sections show our archical and the shang et al
system summaries respectively for asr transcripts in table
a refer to the ath sentence in a multi sentence summary
as in table and our model and shang et al
summaries as in table
the judges were not shown the source of the summaries
the twelve ratings that we received are converted into two nary comparisons and are summarized in table
our model summaries were often judged to be ter than the shang et al
system summaries in both uency and relevancy
gwet s and brennan s and prediger s kappa inter rater ment statistics show strong agreement for uency
however compared to human summaries our model summaries were similar in terms of uency but were lower in terms of relevancy with rater statistics indicating fair strength of agreement
to the small ami data size batch size and initial learning rate of bertsumext are set to and tively batch size in bertsumextabs is and initial learning rates of bert and transformer decoder in bertsumextabs are
and
respectively
s and brennan and prediger s kappa just the impact of the empirical distributions over the chance agreement and therefore are better suited for cases where the proportion of agreements on one class differs from that of another
metric win tie lose bp gwet our model vs
shang et al


our model vs
human reference

fluency relevancy fluency relevancy



table comparative ratings by human judges of maries on uency and relevancy metrics
gwet and bp refer to gwet s and brennan prediger kappa coefcients respectively
we also compared statistics of reference maries against our and shang et al
model generated summaries of maximum words
we observe our model generates approximately words outputs which is close to ground truth man written summaries of size approximately words
however the shang et al
system generates summaries of average words
ther the median number of threads number of maries of our model human written summaries and shang et al
are
respectively
this indicates our model is learning to generate human like summaries while shang et al
aims to distill words up to the permissible limit and therefore has high recall and very low precision see table
additionally our model has twice the shang et al
values which cates high readability and was supported by human judges
further the difference in number of threads summaries between our model and reference are and for and of cases respectively
this clearly indicates the strength of our hierarchical model in disentangling threads
conclusion we investigated the use of an end to end cal encoder decoder model with three levels of hierarchical attention for jointly rizing and implicitly disentangling interleaved text
to train this model we examined the use of training using synthesized data and ne tuning for adaptation to a new domain with limited labeled real world data
on real world ami data our ne tuned end to end system outperforms a step system by
experiments were also conducted against the transformer based sumextabs and bart systems which indicate that these transformer models can also rize interleaved texts
specically our model also outperformed the transformer based but not bart which suggests that use of pretraining of both the decoder as well as the encoder is important and also indicates the utility of our synthetic data
references ahmet aker monica paramita emina kurtic adam funk emma barker mark hepple and rob gaizauskas

automatic label generation for news comment clusters
in proceedings of the international natural language generation ence pages
xavier anguera simon bozonnet nicholas evans corinne fredouille gerald friedland and oriol vinyals

speaker diarization a review of cent research
ieee transactions on audio speech and language processing
dzmitry bahdanau kyunghyun cho and yoshua neural machine translation by corr bengio

jointly learning to align and translate


emma barker monica lestari paramita ahmet aker emina kurtic mark hepple and robert gaizauskas

the sensei annotated corpus human maries of reader comment conversations in on line news
in proceedings of the annual meeting of the special interest group on discourse and dialogue pages
yen chun chen and mohit bansal

fast tive summarization with reinforce selected sentence rewriting
in proceedings of the annual ing of the association for computational tics volume long papers pages bourne australia
association for computational linguistics
jianpeng cheng and mirella lapata

neural marization by extracting sentences and words
in proceedings of the annual meeting of the sociation for computational linguistics volume long papers pages
sumit chopra michael auli and alexander m
rush

abstractive sentence summarization with in naacl hlt tentive recurrent neural networks
the conference of the north american chapter of the association for computational guistics human language technologies san diego california usa june pages
the association for computational linguistics
franck dernoncourt and ji young lee

pubmed rct a dataset for sequential sentence in proceedings of cation in medical abstracts
the eighth international joint conference on ral language processing volume short papers pages
asian federation of natural guage processing
jacob devlin ming wei chang kenton lee and kristina toutanova

bert pre training of deep bidirectional transformers for language ing
arxiv preprint

baoyu jing pengtao xie and eric xing

on the automatic generation of medical imaging reports
in proceedings of the annual meeting of the association for computational linguistics volume long papers pages
association for computational linguistics
diederik p
kingma and jimmy ba

adam corr a method for stochastic optimization


jonathan krause justin johnson ranjay krishna and li fei fei

a hierarchical approach for erating descriptive image paragraphs
in computer vision and patterm recognition cvpr
mike lewis yinhan liu naman goyal jan ghazvininejad abdelrahman mohamed omer levy veselin stoyanov and luke zettlemoyer

bart denoising sequence to sequence training for natural arxiv preprint lation

language generation and comprehension
jiwei li thang luong and dan jurafsky

a hierarchical neural autoencoder for paragraphs and documents
in proceedings of the annual ing of the association for computational tics and the international joint conference on natural language processing volume long pers pages
association for tional linguistics
manling li lingyu zhang heng ji and richard j
radke

keep meeting summaries on topic abstractive multi modal meeting summarization
in proceedings of the association for computational linguistics pages florence italy
association for tational linguistics
the annual meeting of yang liu and mirella lapata

text in proceedings of tion with pretrained encoders
the conference on empirical methods in ural language processing and the international joint conference on natural language processing emnlp ijcnlp pages hong kong china
association for computational linguistics
zongyang ma aixin sun quan yuan and gao cong

topic driven reader comments tion
in proceedings of the acm international conference on information and knowledge ment pages
acm
i
mccowan j
carletta w
kraaij s
ashby s
ban m
flynn m
guillemot t
hain j
kadlec v
karaiskos m
kronenthal g
lathoud m
coln a
lisowska w
post dennis reidsma and p
wellner

the ami meeting corpus
in ceedings of measuring behavior tional conference on methods and techniques in havioral research pages
noldus tion technology
ramesh nallapati feifei zhai and bowen zhou

summarunner a recurrent neural network based quence model for extractive summarization of in aaai conference on articial uments
gence
ramesh nallapati bowen zhou ccero nogueira dos santos c aglar gulcehre and bing xiang

abstractive text summarization using sequence in proceedings of the sequence rnns and beyond
signll conference on computational natural language learning conll berlin germany august pages
acl
alexander m
rush sumit chopra and jason weston

a neural attention model for abstractive in proceedings of the tence summarization
conference on empirical methods in natural guage processing emnlp lisbon portugal september pages
the ciation for computational linguistics
rico sennrich barry haddow and alexandra birch

neural machine translation of rare words with subword units
in proceedings of the nual meeting of the association for computational linguistics volume long papers pages berlin germany
association for tional linguistics
guokan shang wensi ding zekun zhang toine tixier polykarpos meladianos michalis giannis and jean pierre

vised abstractive meeting summarization with sentence compression and budgeted submodular in proceedings of the annual maximization
meeting of the association for computational guistics volume long papers pages
association for computational linguistics
kaitao song xu tan tao qin jianfeng lu and yan liu

mass masked sequence to sequence pre training for language generation
arxiv preprint

jiwei tan xiaojun wan and jianguo xiao

from neural sentence summarization to headline in ijcai generation a coarse approach
pages
suzan verberne emiel krahmer iris hendrickx sander wubben and antal van den bosch

creating a reference data set for the summarization of discussion forum threads
language resources and evaluation pages
lidan wang and douglas w oard

based message expansion for disentanglement of terleaved text conversations
in proceedings of man language technologies the annual ference of the north american chapter of the ation for computational linguistics pages
association for computational linguistics

