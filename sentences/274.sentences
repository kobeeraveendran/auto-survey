r a m l c
s c v
v i x r a published as a conference paper at iclr amharic abstractive text summarization amr m
zaki department of computer engineering ain shams university
com mahmoud i
khalil department of computer engineering ain shams university mahmoud

asu
edu
eg hazem m
abbas department of computer engineering ain shams university hazem

asu
edu
eg abstract text summarization is the task of condensing long text into just a handful of tences
many approaches have been proposed for this task some of the very rst were building statistical models extractive methods paice et al
capable of selecting important words and copying them to the output ever these models lacked the ability to paraphrase sentences as they simply select important words without actually understanding their contexts nor understanding their meaning here comes the use of deep learning based architectures tive methods chopra et al
et al
which effectively tries to understand the meaning of sentences to build meaningful summaries
in this work we discuss one of these new novel approaches which combines curriculum learning with deep learning this model is called scheduled sampling bengio et al

we apply this work to one of the most widely spoken african languages which is the amharic language as we try to enrich the african nlp community with top notch deep learning architectures
dataset and word embedding for the amharic language working with the amharic language turned to be quite challenging as african languages are typically known to be low resource languages
data for our task was nt easily collected as there is nt an available dataset for our task this is why we had to collect and build our own dataset from scratch
data for text summarization is found in form of long text articles and their summaries titles for the english case researchers work on data scrapped from cnn dailynews hermann et al
so we used their same approach and scrapped data from well known amharic news websites

goolgule


ethiopianregistrar
com amharic

ethsat
com

com amharic

zehabesha
com

ethiopianregistrar
com

ethiopianreporter
com we scrapped over articles and only used those with long titles about articles
word embedding has proved itself as one of the best methods to represent text for deep models
one of the most widely used english word embedding models is et al
it represents each word with a list of vectors to be easily used in the deep models however no such models were trained for the amharic language this is why we trained our own model for this task


com published as a conference paper at iclr in this work we provide both the scrapped news dataset and the trained word embedding as open source to help enrich the african nlp research community
text summarization deep learning building blocks text summarization is considered as a time series problem as we are trying to generate the next word given the past words
novel deep models rely on some basic blocks in this section we go through these building blocks

using lstm with attention since our task is a time series problem rnn models were rst used to address this task however given the long sentence dependencies in natural languages lstm based architectures were used given its memory structure hochreiter schmidhuber
our task can actually be seen as mapping between input and output however since they differ in length long input short output based architectures are used nallapati et al

to give our models even more human like abilities in summarization bahdanau et al
suggested building a deep model on top of the architecture which helped it attend to important words in the input

pointer generator model this previously discussed model has a well known problem which is working with unknown of vocab words as it can only be trained on a xed sized vocabulary
a solution was proposed by nallapati et al
et al
which builds a deep model on top of the architecture capable of learning when to copy words and when to generate new ones
scheduled sampling one of the problems that the above based architecture suffers from comes from the way it is trained as the model is trained by supplying it both an input long text and a reference short summary while when we test the model we only supply it with the input long text and no reference is given
this forms an inconsistency between the training phase and the testing phase as the model has never been trained to depend on its own this problem is called exposure bias ranzato et al

a solution proposed by bengio et al
helped in solving this problem which included combining curriculum learning with our deep model
we start the training normally by supplying both the long training text and the reference summary but when the model becomes mature enough we gradually introduce the model to its own mistakes while training decreasing its dependency on the reference sentence teaching the model to depend on itself in the training phase in other words making the learning problem more difcult while the model matures hence curriculum learning

com theamrzaki text summurization abstractive methods tree master amharic figure scheduled sampling architecture published as a conference paper at iclr experiments we have applied the scheduled sampling model on the amharic dataset that we have built we have used google colab as our training framework as it provides us with free gpu and up to gb of ram
our model is built over the library keneshloo et al
we have modied it to work on and to work with the amharic dataset
we evaluate our experiments using well known metrics used for evaluating text summarization these metrics are and et al
which measure the amount of n grams that overlap between the reference summary and our generated one as the measure increases the amount of overlap increases indicating a better output
we ran our evaluation on test sentences scores were
rouge
rouge
rouge

for comparison running scheduled sampling on english well known datasets cnn dailymail dataset achieves of
and
this discrepancy of the results from the english counterpart comes from the fact that the english dataset is huge articles with long summary compared to our scrapped amharic dataset of articles with short summaries this comes from the fact that collecting english dataset is comparatively much easier than collecting an african one due to the huge amount of available english resources
conclusion by building a custom word embedding model for a specic african language we are able to apply any deep model that works on english on that selected african language like what we have proven by our work
in our coming work we are willing to experience with other advanced architectures that have recently proven extremely efcient in addressing problems
one of these architectures is bert devlin et al
which stands for bidirectional encoder representations from transformers it uses a similar encoder decoder architecture but instead of using recurrent based cells it only uses attention self attention
one efcient way to use bert is by using an already pre trained model that has been trained on the english dataset cnn dailymail dataset and then apply cross lingual transfer to the amharic dataset we believe that by this we believe that this may actually result in better summaries in spite of the relatively small amharic dataset
we hope that by this work we have helped pave the way in applying novel deep learning techniques for african languages we also hope that we have contributed a guideline in applying deep models that can be further used in other nlp tasks
published as a conference paper at iclr references dzmitry bahdanau kyunghyun cho and yoshua bengio
neural machine translation by jointly learning to align and translate
arxiv preprint

samy bengio oriol vinyals navdeep jaitly and noam shazeer
scheduled sampling for sequence prediction with recurrent neural networks
in proceedings of the international conference on neural information processing systems volume pp
cambridge ma usa
mit press
sumit chopra michael auli and alexander m
rush
abstractive sentence summarization with attentive recurrent neural networks
in proceedings of the conference of the north american chapter of the association for computational linguistics human language technologies pp
san diego california june
association for computational linguistics


url
aclweb
org anthology
jacob devlin ming wei chang kenton lee and kristina toutanova
bert pre training of deep bidirectional transformers for language understanding
in proceedings of the conference of the north american chapter of the association for computational linguistics human language technologies volume long and short papers pp
minneapolis minnesota june
association for computational linguistics


url https
aclweb
org anthology
karl moritz hermann tomas kocisky edward grefenstette lasse espeholt will kay mustafa suleyman and phil blunsom
teaching machines to read and comprehend
in c
cortes n
d
lawrence d
d
lee m
sugiyama and r
garnett eds
advances in neural information processing systems pp

curran associates inc

url
nips
cc teaching machines to read and comprehend
pdf
sepp hochreiter and jurgen schmidhuber
long short term memory
neural comput
november
issn

neco




url

neco




yaser keneshloo tian shi naren ramakrishnan and chandan reddy
deep reinforcement learning for sequence to sequence models
ieee transactions on neural networks and learning systems

tnnls


julian kupiec jan o
pedersen and francine chen
a trainable document summarizer
in sigir
chin yew lin
rouge a package for automatic evaluation of summaries
in text summarization branches out pp
barcelona spain july
association for computational linguistics
url
aclweb
org anthology
tomas mikolov g
s corrado kai chen and jeffrey dean
efcient estimation of word tions in vector space
pp

ramesh nallapati bowen zhou cicero dos santos c aglar gulcehre and bing xiang
abstractive text summarization using sequence to sequence rnns and beyond
in proceedings of the signll conference on computational natural language learning pp
berlin germany august
association for computational linguistics


url
aclweb
org anthology
chris d paice
constructing literature abstracts by computer techniques and prospects
information processing management
kishore papineni salim roukos todd ward and wei jing zhu
bleu a method for automatic evaluation of machine translation
in proceedings of the annual meeting of the association for computational linguistics pp
philadelphia pennsylvania usa july
association for computational linguistics



url
aclweb
org anthology
published as a conference paper at iclr marcaurelio ranzato sumit chopra michael auli and wojciech zaremba
sequence level training with recurrent neural networks
in international conference on learning representations iclr san juan puerto rico may conference track proceedings
url
org

abigail see peter liu and christoper manning
get to the point summarization with generator networks
pp




