searching for effective neural extractive summarization what works and what s next ming zhong pengfei liu danqing wang xipeng qiu xuanjing huang shanghai key laboratory of intelligent information processing fudan university school of computer science fudan university zhangheng road shanghai china
edu
abstract the recent years have seen remarkable cess in the use of deep neural networks on text summarization
however there is no clear derstanding of why they perform so well or how they might be improved
in this paper we seek to better understand how neural extractive summarization systems could benet from ferent types of model architectures able knowledge and learning schemas
tionally we nd an effective way to improve current frameworks and achieve the state the art result on cnn dailymail by a large margin based on our observations and ses
hopefully our work could provide more clues for future research on extractive marization
source code will be available on and our project
introduction recent years has seen remarkable success in the use of deep neural networks for text tion see et al
celikyilmaz et al
jadhav and rajan
so far most research utilizing the neural network for text tion has revolved around architecture ing zhou et al
chen and bansal gehrmann et al

despite their success it remains poorly stood why they perform well and what their comings are which limits our ability to design ter architectures
the rapid development of neural architectures calls for a detailed empirical study of analyzing and understanding existing models
in this paper we primarily focus on extractive summarization since they are computationally cient and can generate grammatically and ent summaries nallapati et al

and seek to these two authors contributed equally
corresponding author

com fastnlp fastnlp
com better understand how neural network based proaches to this task could benet from different types of model architectures transferable edge and learning schemas and how they might be improved
architectures architecturally the better mance usually comes at the cost of our standing of the system
to date we know little about the functionality of each neural component and the differences between them peters et al
which raises the following typical tions how does the choice of different ral architectures cnn rnn transformer ence the performance of the summarization tem which part of components matters for specic dataset do current models suffer from the over engineering problem understanding the above questions can not only help us to choose suitable architectures in different application scenarios but motivate us to move ward to more powerful frameworks
external transferable knowledge and ing schemas clearly the improvement in racy and performance is not merely because of the shift from feature engineering to structure neering but the exible ways to incorporate nal knowledge mikolov et al
peters et al
devlin et al
and learning schemas to introduce extra instructive constraints paulus et al
arumae and liu
for this part we make some rst steps toward answers to the following questions which type of pre trained models supervised or unsupervised pre training is more friendly to the summarization task when architectures are explored exhaustively can we push the state of the art results to a new level by introducing external transferable knowledge or changing another learning schema to make a comprehensive study of above l u j l c
s c v
v i x r a perspective content sec
id learning schemas sup
reinforce
structure dec
enc
pointer seqlab
lstm transformer knowledge exter
glove bert news
inter
random






table outline of our experimental design
dec
and enc
represent decoder and encoder respectively
sup
denotes supervised learning and news
means vised pre training knowledge
alytical perspectives we rst build a testbed for summarization system in which training and ing environment will be constructed
in the ing environment we design different tion models to analyze how they inuence the performance
specically these models differ in the types of architectures encoders cnn lstm transformer vaswani et al
coders auto non auto regressive ternal transferable knowledge glove ton et al
bert devlin et al
newsroom grusky et al
and different learning schemas supervised learning and forcement learning
to peer into the internal working mechanism of above testing cases we provide sufcient ation scenarios in the testing environment
cretely we present a multi domain test sentence shufing test and analyze models by different metrics repetition sentence length and position bias which we additionally developed to provide a better understanding of the characteristics of ferent datasets
empirically our main observations are rized as architecturally speaking models with regressive decoder are prone to achieving ter performance against non auto regressive coder
besides lstm is more likely to suffer from the architecture overtting problem while transformer is more robust
the success of extractive summarization tem on the cnn dailymail heavily relies on the ability to learn positional information of the sentence
unsupervised transferable knowledge is more useful than supervised transferable regressive indicates that the decoder can make rent prediction with knowledge of previous predictions
edge since the latter one is easily inuenced by the domain shift problem
we nd an effective way to improve the rent system and achieving the state of the art sult on cnn dailymail by a large margin with the help of unsupervised transferable knowledge
score
and this result can be further enhanced by introducing reinforcement learning
score
hopefully this detailed empirical study can vide more hints for the follow up researchers to design better architectures and explore new of the art results along a right direction
related work the work is connected to the following threads of work of nlp research
task oriented neural networks interpreting without knowing the internal working mechanism of the neural network it is easy for us to get into a hobble when the performance of a task has reached the bottleneck
more recently peters et al
investigate how different learning works inuence the properties of learned tualized representations
different from this work in this paper we focus on dissecting the neural models for text summarization
a similar work to us is kedzie et al
which studies how deep learning models perform context selection in terms of several typical marization architectures and domains
compared with this work we make a more comprehensive study and give more different analytic aspects
for example we additionally investigate how able knowledge inuence extractive tion and a more popular neural architecture former
besides we come to inconsistent clusions when analyzing the auto regressive coder
more importantly our paper also shows how existing systems can be improved and we have achieved a state of the art performance on cnn dailymail
extractive summarization most of recent work attempt to explore different neural nents or their combinations to build an end to end learning model
specically these work tiate their encoder decoder framework by ing recurrent neural networks cheng and lapata nallapati et al
zhou et al
as encoder auto regressive decoder chen and bansal jadhav and rajan zhou et al
or non auto regressive decoder isonuma et al
narayan et al
arumae and liu as decoder based on pre trained word representations mikolov et al
pennington et al

however how to use transformer in extractive summarization is still a missing issue
in addition some work uses reinforcement ing technique narayan et al
wu and hu chen and bansal which can provide more direct optimization goals
although above work improves the performance of summarization system from different perspectives yet a hensive study remains missing
a testbed for text summarization to analyze neural summarization system we pose to build a training testing environment in which different text cases models are rstly erated under different training settings and they are further evaluated under different testing tings
before the introduction of our train testing testbed we rst give a description of text rization

task description existing methods of extractive summarization rectly choose and output the salient sentences or phrases in the original document
formally given a document d dn consisting of n tences the objective is to extract a subset of tences r rm from d m is tic during training while is a hyper parameter in testing phase
additionally each sentence tains words
generally most of existing extractive rization systems can be abstracted into the ing framework consisting of three major modules sentence encoder document encoder and coder
at rst a sentence encoder will be utilized to convert each sentence into a sentential sentation
then these sentence representations will be contextualized by a document encoder to
finally a decoder will extract a subset of tences based on these contextualized sentence resentations
schemas


sentence encoder we instantiate our sentence encoder with cnn layer kim
we do nt explore other options as sentence encoder since strong evidence of vious work kedzie et al
shows that the ferences of existing sentence encoder do nt matter too much for nal performance


document encoder given a sequence of sentential representation dn the duty of document encoder is to contextualize each sentence therefore obtaining the contextualized representations sn
to achieve this goal we investigate the lstm based structure and the transformer structure both of which have proven to be effective and achieved the state of the art results in many other nlp tasks
notably to let the model make the best of its tural bias stacking deep layers is allowed
lstm layer long short term memory work lstm was proposed by hochreiter and schmidhuber to specically address this sue of learning long term dependencies which has proven to be effective in a wide range of nlp tasks such as text classication liu et al
semantic matching rocktaschel et al
liu et al
text summarization rush et al
and machine translation sutskever et al

transformer layer transformer vaswani et al
is essentially a feed forward attention architecture which achieves pairwise interaction by attention mechanism
recently transformer has achieved great success in many other nlp tasks vaswani et al
dai et al
and it is appealing to know how this neural module performs on text summarization task


decoder decoder is used to extract a subset of sentences from the original document based on alized representations sn
most ing architecture of decoders can divide into regressive and non auto regressive versions both of which are investigated in this paper

setup for training environment the objective of this step is to provide typical and diverse testing cases models in terms of model architectures transferable knowledge and learning sequence labeling seqlab the models which formulate extractive summarization task as a sequence labeling problem are equipped with non auto regressive decoder
formally given a document d consisting of n sentences dn the summaries are extracted by predicting a quence of label yi for the document where yi represents the i tence in the document should be included in the summaries
pointer network pointer as a representative of auto regressive decoder pointer network based decoder has shown superior performance for tractive summarization chen and bansal jadhav and rajan
pointer network lects the sentence by attention mechanism using glimpse operation vinyals et al

when it extracts a sentence pointer network is aware of previous predictions


external transferable knowledge the success of neural network based models on nlp tasks can not only be attributed to the shift from feature engineering to structural ing but the exible ways to incorporate external knowledge mikolov et al
peters et al
devlin et al

the most common form of external transferable knowledge is the rameters pre trained on other corpora
to investigate how different pre trained models inuence the summarization system we take the following pre trained knowledge into tion
unsupervised transferable knowledge two typical unsupervised transferable knowledge are explored in this paper context independent word embeddings mikolov et al
pennington et al
and contextualized word embeddings peters et al
devlin et al
have put the state of the art results to new level on a large number of nlp taks recently
supervised pre trained knowledge besides unsupervised pre trained knowledge we also can utilize parameters of networks pre trained on other summarization datasets
the value of this tigation is to know transferability between ent dataset
to achieve this we rst pre train our model on the newsroom dataset grusky et al
which is one of the largest datasets and tains samples from different domains
then we ne tune our model on target domains that we vestigate


learning schemas utilizing external knowledge provides a way to seek new state of the art results from the tive of introducing extra data
additionally an ternative way is resorting to change the learning schema of the model
in this paper we also plore how different learning schemas inuence tractive summarization system by comparing pervised learning and reinforcement learning

setup for testing environment in the testing environment we provide sufcient evaluation scenarios to get the internal working mechanism of testing models
next we will make a detailed deception
rouge following previous work in text marization we evaluate the performance of ferent architectures with the standard and rouge l scores lin by using pyrouge
cross domain evaluation we present a domain evaluation in which each testing model will be evaluated on multi domain datasets based on cnn dailymail and newsroom
detail of the multi domain datasets is descried in tab

repetition we design repetition score to test how different architectures behave diversely on avoiding generating unnecessary lengthy and peated information
we use the percentage of peated n grams in extracted summary to measure the word level repetition which can be calculated as repn where count is used to count the number of grams and uniq is used to eliminate n gram cation
the closer the word based repetition score is to the lower the repeatability of the words in summary
it is meaningful the ground truth distribution of positional bias to study the whether datasets is different and how it affects different chitectures
to achieve this we design a positional bias to describe the uniformity of ground truth tribution in different datasets which can be
python
org pypi

lated as posbias we divide each article into k parts we choose because articles from cnn dailymail and newsroom have sentences by average and denotes the probability that the rst golden label is in part i of the articles
sentence length sentence length will affect different metrics to some extent
we count the erage length of the k th sentence extracted from different decoders to explore whether the decoder could perceive the length information of tences
sentence shufing we attempt to explore the impact of sentence position information on ent structures
therefore we shufe the orders of sentences and observe the robustness of different architectures to out of order sentences
experiment
datasets instead of evaluating model solely on a single dataset we care more about how our testing els perform on different types of data which lows us to know if current models suffer from the over engineering problem
domains train valid test cnn dailymail nytimes washingtonpost foxnews theguardian nydailynews wsj usatoday table statistics of multi domain datasets based on cnn dailymail and newsroom
cnn dailymail the cnn dailymail question answering dataset hermann et al
ed by nallapati et al
is commonly used for summarization
the dataset consists of line news articles with paired human generated summaries
sentences on average
for the data prepossessing we use the data with anonymized version as see et al
which does nt replace named entities
newsroom recently newsroom is structed by grusky et al
which contains
million articles and summaries extracted from major news publications across years
we regard this diversity of sources as a diversity of summarization styles and select seven tions with the largest number of data as different domains to do the cross domain evaluation
due to the large scale data in newsroom we also choose this dataset to do transfer experiment

training settings for different learning schemas we utilize cross entropy loss function and reinforcement learning method close to chen and bansal with a small difference we use the precision of as a reward for every extracted sentence instead of the value of rouge l
hird columns show the scope and methods of interactions for different words wi in a sentence
for context independent word representations glove we directly utilize them to initialize our words of each sentence which can be ne tuned during the training phase
for bert we truncate the article to kens and feed it to a feature based bert without gradient concatenate the last four layers and get a dimensional token embedding after passing through a mlp

experimental observations and analysis next we will show our ndings and analyses in terms of architectures and external transferable knowledge


analysis of decoders we understand the differences between decoder pointer and seqlab by probing their behaviours in different testing environments
domains from tab
we can observe that models with pointer based decoder are prone to achieving better performance against based decoder
specically among these eight datasets models with pointer based decoder perform seqlab on six domains and achieves comparable results on the other two domains
for example in nytimes washingtonpost model r l r l r l r l dec
enc
cnn dm nytimes washingtonpost foxnews seqlab pointer lstm transformer lstm transformer lead oracle lead oracle seqlab pointer lstm transformer lstm transformer















































































































































dec
enc
theguardian nydailynews wsj usatoday table results of different architectures over different domains where enc
and dec
represent document coder and decoder respectively
lead means to extract the rst k sentences as the summary usually as a competitive lower bound
oracle represents the ground truth extracted by the greedy algorithm nallapati et al
usually as the upper bound
the number k in parentheses denotes k sentences are extracted during testing and choose lead k as a lower bound for this domain
all the experiments use to obtain word representations
and theguardian domains pointer passes seqlab by at least
improvment
we attempt to explain this difference from the lowing three perspectives
repetition for domains that need to extract multiple sentences as the summary rst two mains in tab
pointer is aware of the ous prediction which makes it to reduce the plication of n grams compared to seqlab
as shown in fig
models with pointer always get higher repetition scores than models with qlab when extracting six sentences which cates that pointer does capture word level mation from previous selected sentences and has positive effects on subsequent decisions
positional bias for domains that only need to extract one sentence as the summary last six mains in tab
pointer still performs better than seqlab
as shown in fig
the mance gap between these two decoders grows as the positional bias of different datasets increases
for example from the tab
we can see in the domains with low value positional bias such as

seqlab achieves closed performance against pointer
the performance gap grows when processing these domains with value positional bias


consequently seqlab is more sensitive to positional bias which by contrast impairs its performance on some datasets
sentence length we nd pointer shows the ity to capture sentence length information based on previous predictions while seqlab does nt
we can see from the fig
that models with pointer tend to choose longer sentences as the rst sentence and greatly reduce the length of the tence in the subsequent extractions
in ison it seems that models with seqlab tend to extract sentences with similar length
the ability allows pointer to adaptively change the length of the extracted sentences thereby achieving better performance regardless of whether one sentence or multiple sentences are required


analysis of encoders in this section we make the analysis of two coders lstm and transformer in different testing environments
domains from tab
we get the following servations transformer can outperform lstm on some datasets nydailynews by a relatively large margin while lstm beats transformer on some domains with closed improvements
besides ing different training phases of these eight mains the hyper parameters of transformer keep while for lstm many sets of layers dimensions for pointer and layers dimensions for seqlab repetition score positional bias average length figure different behaviours of two decoders seqlab and pointer under different testing environment
a shows repetition scores of different architectures when extracting six sentences on cnn dailymail
shows the relationship between r and positional bias
the abscissa denotes the positional bias of six different datasets and r denotes the average rouge difference between the two decoders under different encoders
c shows average length of k th sentence extracted from different architectures
d d nallapati et al
narayan et al
r l




















table results of transformer with seqlab using different proportions of sentence embedding and sitional embedding on cnn dailymail
the input of transformer is sentence embedding plus tional
the bottom half of the table tains models that have similar performance with former that only know positional information
models
however transformer obtains lower crease against lstm suggesting that transformer are more robust
disentangling testing transformer provides us an effective way to disentangle position and tent information which enables us to design a cic experiment investigating what role positional information plays
as shown in tab
we dynamically regulate the ratio between sentence embedding and tional embedding by two coefcients and
surprisingly we nd even only utilizing sitional embedding the model is only told how many sentences the document contains vaswani et al
the input of transformer is word embedding plus positional embedding so we design the above different proportions to carry out the disentangling test
figure results of different document encoders with pointer on normal and shufed cnn dailymail
r denotes the decrease of performance when the tences in document are shufed
parameters are
above phenomena suggest that lstm easily suffers from the architecture overtting problem compared with transformer
additionally in our experimental setting transformer is more cient to train since it is two or three times faster than lstm
when equipped with seqlab decoder former always obtains a better performance pared with lstm the reason we think is due to the non local bias wang et al
of transformer
shufed testing in this settings we shufe the orders of sentences in training set while test set keeps unchanged
we compare two models with different encoders lstm transformer and the results can be seen in fig

generally there is signicant drop of performance about these two number of layers searches in and sion searches in











model r l r l r l r l dec
enc
baseline glove bert newsroom seqlab pointer lstm transformer lstm transformer















































table results of different architectures with different pre trained knowledge on cnn dailymail where enc
and dec
represent document encoder and decoder respectively
our model can achieve
on which is comparable to many existing models
by contrast once the positional information is moved the performance dropped by a large gin
this experiment shows that the success of such extractive summarization heavily relies on the ability of learning the positional information on cnn dailymail which has been a benchmark dataset for most of current work
models r l chen and bansal dong et al
zhou et al
jadhav and rajan lstm pn lstm pn rl lstm pn bert lstm pn bert rl

























analysis of transferable knowledge next we show how different types of transferable knowledge inuences our summarization models
table evaluation on cnn dailymail
the top half of the table is currently state of the art models and the lower half is our models
unsupervised pre training here as a line is used to obtain word sentations solely based on the training set of cnn dailymail
as shown in tab
we can nd that independent word representations can not tribute much to current models
however when the models are equipped with bert we are cited to observe that the performances of all types of architectures are improved by a large margin
the model cnn lstm pointer specically has achieved a new state of the art with
on surpassing existing models dramatically
supervised pre training in most cases our models can benet from the pre trained ters learned from the newsroom dataset
ever the model cnn lstm pointer fails and the performance are decreased
we understand this phenomenon by the following explanations the transferring process from cnn dailymail to newsroom suffers from the domain shift lem in which the distribution of golden labels sitions are changed
and the observation from fig
shows that cnn lstm pointer is more sitive to the ordering change therefore obtaining a lower performance
why does bert work we investigate two ferent ways of using bert to gure out from where bert has brought improvement for tive summarization system
in the rst usage we feed each individual tence to bert to obtain sentence representation which does not contain contextualized tion and the model gets a high score of

however when we feed the entire article to bert to obtain token representations and get the tence representation through mean pooling model performance soared to
score
the experiment indicates that though bert can provide a powerful sentence embedding the key factor for extractive summarization is textualized information and this type of tion bears the positional relationship between tences which has been proven to be critical to tractive summarization task as above

learning schema and complementarity besides supervised learning in text tion reinforcement learning has been recently used to introduce more constraints
in this paper we also explore if several advanced techniques be and evaluated on the anonymized version
complementary with each other
the we and rst choose based model lstm pointer lstm pointer bert then the reinforcement learning are duced aiming to further optimize our models
as shown in tab
we observe that even though the performance of has been largely improved by bert when applying ment learning the performance can be improved further which indicates that there is indeed a plementarity between architecture transferable knowledge and reinforcement learning
conclusion from different in this paper we seek to better understand how neural extractive summarization systems could benet types of model tectures transferable knowledge and learning schemas
our detailed observations can provide more hints for the follow up researchers to design more powerful learning frameworks
acknowledgment we thank jackie chi kit cheung peng qian for useful comments and discussions
we would like to thank the anonymous reviewers for their valuable comments
the research work is ported by national natural science foundation of china no
and hai municipal science and technology sion and hai municipal science and technology major
zjlab
references kristjan arumae and fei liu

reinforced tractive summarization with question focused in proceedings of acl student wards
search workshop
pages
asli celikyilmaz antoine bosselut xiaodong he and yejin choi

deep communicating agents for in proceedings of the abstractive summarization
conference of the north american chapter of the association for computational linguistics man language technologies volume long pers
volume pages
jianpeng cheng and mirella lapata

neural summarization by extracting sentences and words
in proceedings of the annual meeting of the association for computational linguistics volume long papers
volume pages
zihang dai zhilin yang yiming yang william w cohen jaime carbonell quoc v le and ruslan salakhutdinov

transformer xl language modeling with longer term dependency
jacob devlin ming wei chang kenton lee and kristina toutanova

bert pre training of deep bidirectional transformers for language ing
arxiv preprint

yue dong yikang shen eric crawford herke van hoof and jackie chi kit cheung

sum extractive summarization as a contextual dit
in proceedings of the conference on pirical methods in natural language processing
pages
sebastian gehrmann yuntian deng and alexander rush

bottom up abstractive summarization
in proceedings of the conference on cal methods in natural language processing
pages
max grusky mor naaman and yoav artzi

newsroom a dataset of
million summaries with diverse extractive strategies
in proceedings of the conference of the north american chapter of the association for computational linguistics man language technologies volume long pers
volume pages
karl moritz hermann tomas kocisky edward grefenstette lasse espeholt will kay mustafa leyman and phil blunsom

teaching chines to read and comprehend
in advances in ral information processing systems
pages
sepp hochreiter and jurgen schmidhuber

neural computation long short term memory

masaru isonuma toru fujino junichiro mori yutaka matsuo and ichiro sakata

extractive marization using multi task learning with document in proceedings of the classication
ence on empirical methods in natural language processing
pages
aishwarya jadhav and vaibhav rajan

tive summarization with swap net sentences and in words from alternating pointer networks
ceedings of the annual meeting of the tion for computational linguistics volume long papers
volume pages
yen chun chen and mohit bansal

fast tive summarization with reinforce selected sentence rewriting
in proceedings of the annual ing of the association for computational linguistics volume long papers
volume pages
chris kedzie kathleen mckeown and hal daume iii

content selection in deep learning models of in proceedings of the summarization
ference on empirical methods in natural language processing
pages
yoon kim

works for sentence classication


convolutional neural arxiv preprint technologies volume long papers
volume pages
chin yew lin

rouge a package for matic evaluation of summaries
text summarization branches out
pengfei liu xipeng qiu jifan chen and xuanjing huang

deep fusion lstms for text in proceedings of the annual tic matching
meeting of the association for computational guistics volume long papers
volume pages
pengfei liu xipeng qiu and xuanjing huang

recurrent neural network for text classication with multi task learning
in proceedings of ijcai
pages
pengfei liu xipeng qiu and xuanjing huang

adversarial multi task learning for text tion
in proceedings of the annual meeting of the association for computational linguistics ume long papers
volume pages
tomas mikolov kai chen greg corrado and efcient estimation of word arxiv preprint frey dean

representations in vector space


ramesh nallapati feifei zhai and bowen zhou

summarunner a recurrent neural network based quence model for extractive summarization of ments
in thirty first aaai conference on articial intelligence
ramesh nallapati bowen zhou cicero dos santos c a glar and bing xiang

tive text summarization using sequence to sequence rnns and beyond
conll page
shashi narayan shay b cohen and mirella lapata

ranking sentences for extractive tion with reinforcement learning
in proceedings of the conference of the north american ter of the association for computational linguistics human language technologies volume long pers
volume pages
romain paulus caiming xiong and richard socher

a deep reinforced model for abstractive marization
arxiv preprint

jeffrey pennington richard socher and christopher manning

glove global vectors for word representation
in proceedings of the ence on empirical methods in natural language cessing emnlp
pages
matthew peters mark neumann mohit iyyer matt gardner christopher clark kenton lee and luke zettlemoyer

deep contextualized word resentations
in proceedings of the conference of the north american chapter of the association for computational linguistics human language matthew peters mark neumann luke zettlemoyer and wen tau yih

dissecting contextual word embeddings architecture and representation
in proceedings of the conference on cal methods in natural language processing
pages
tim rocktaschel edward grefenstette karl moritz hermann tomas and phil blunsom

reasoning about entailment with neural attention
arxiv preprint

alexander m rush sumit chopra and jason weston

a neural attention model for abstractive in proceedings of the tence summarization
conference on empirical methods in natural guage processing
pages
abigail see peter j liu and christopher d manning

get to the point summarization with generator networks
in proceedings of the nual meeting of the association for computational linguistics volume long papers
volume pages
ilya sutskever oriol vinyals and quoc vv le

sequence to sequence learning with neural works
in advances in neural information ing systems
pages
ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan n gomez ukasz kaiser and illia polosukhin

attention is all you need
in advances in neural information cessing systems
pages
oriol vinyals samy bengio and manjunath kudlur

order matters sequence to sequence for sets
arxiv preprint

xiaolong wang ross girshick abhinav gupta and kaiming he

non local neural networks
in proceedings of the ieee conference on computer vision and pattern recognition
pages
yuxiang wu and baotian hu

learning to extract coherent summary via deep reinforcement learning
in thirty second aaai conference on articial telligence
qingyu zhou nan yang furu wei shaohan huang ming zhou and tiejun zhao

neural ment summarization by jointly learning to score and select sentences
in proceedings of the annual meeting of the association for computational guistics volume long papers
volume pages

