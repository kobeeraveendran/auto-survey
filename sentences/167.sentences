learning to encode text as human readable summaries using generative adversarial networks yau shian wang national taiwan university
com hung yi lee national taiwan university
com t c o l c
s c v
v i x r a abstract auto encoders compress input data into a latent space representation and reconstruct the original data from the representation
this tent representation is not easily interpreted by humans
in this paper we propose training an auto encoder that encodes input text into human readable sentences and unpaired stractive summarization is thereby achieved
the auto encoder is composed of a generator and a reconstructor
the generator encodes the input text into a shorter word sequence and the reconstructor recovers the generator input from the generator output
to make the erator output human readable a discriminator restricts the output of the generator to ble human written sentences
by taking the generator output as the summary of the put text abstractive summarization is achieved without document summary pairs as training data
promising results are shown on both glish and chinese corpora
introduction when it comes to learning data representations a popular approach involves the auto encoder chitecture which compresses the data into a tent representation without supervision
in this paper we focus on learning text representations
because text is a sequence of words to encode a sequence a sequence to sequence encoder li et al
kiros et al
is ally used in which a rnn is used to encode the input sequence into a xed length representation after which another rnn is used to decode the original input sequence given this representation
although the latent representation learned by the auto encoder can be used in stream applications is usually not readable
a human readable representation should comply the rule of human grammar and can be comprehended by human
therefore in this work it we use comprehensible natural language as a tent representation of the input source text in an auto encoder architecture
this human readable latent representation is shorter than the source text in order to reconstruct the source text it must ect the core idea of the source text
intuitively the latent representation can be considered a mary of the text so unpaired abstractive rization is thereby achieved
the idea that using human comprehensible guage as a latent representation has been plored on text summarization but only in a supervised scenario
previous work miao and blunsom uses a prior distribution from a pre trained language model to constrain the erated sequence to natural language
however to teach the compressor network to generate text summaries the model is trained using labeled data
in contrast in this work we need no labeled data to learn the representation
as shown in fig
the proposed model is posed of three components a generator a inator and a reconstructor
together the generator and reconstructor form a text auto encoder
the generator acts as an encoder in generating the tent representation from the input text
instead of using a vector as latent representation however the generator generates a word sequence much shorter than the input text
from the shorter text the reconstructor reconstructs the original input of the generator
by minimizing the tion loss the generator learns to generate short text segments that contain the main information in the original input
we use the model in modeling the generator and reconstructor because both have input and output sequences with ent lengths
however it is very possible that the ator s output word sequence can only be cessed and recognized by the reconstructor but is not readable by humans
here instead of ularizing the generator output with a pre trained language model miao and blunsom we borrow from adversarial auto encoders makhzani et al
and cycle gan zhu et al
and introduce a third component the tor to regularize the generator s output word quence
the discriminator and the generator form a generative adversarial network gan fellow et al

the discriminator inates between the generator output and written sentences and the generator produces put as similar as possible to human written tences to confuse the discriminator
with the gan framework the discriminator teaches the tor how to create human like summary sentences as a latent representation
however due to the non differential property of discrete distributions generating discrete distributions by gan is lenging
to tackle this problem in this work we proposed a new kind of method on language eration by gan
by achieving unpaired abstractive text rization machine is able to unsupervisedly extract the core idea of the documents
this approach has many potential applications
for example the output of the generator can be used for the stream tasks like document classication and timent classication
in this study we evaluate the results on an abstractive text summarization task
the output word sequence of the generator is garded as the summaries of the input text
the model is learned from a set of documents out summaries
as most documents are not paired with summaries for example the movie reviews or lecture recordings this technique makes it ble to learn summarizer to generate summaries for these documents
the results show that the ator generates summaries with reasonable quality on both english and chinese corpora
related work abstractive text summarization recent model architectures for abstractive text summarization basically use the sequence sequence sutskever et al
framework in combination with various novel mechanisms
one popular mechanism is attention bahdanau et al
which has been shown helpful for rization nallapati et al
rush et al
chopra et al

it is also possible to directly optimize evaluation metrics such as rouge lin figure proposed model
given long text the generator produces a shorter text as a summary
the generator is learned by minimizing the struction loss together with the reconstructor and making discriminator regard its output as written text
with reinforcement learning ranzato et al
paulus et al
bahdanau et al

the hybrid pointer generator network see et al
selects words from the original text with a pointer vinyals et al
or from the whole vocabulary with a trained weight
in order to inate repetition a coverage vector tu et al
can be used to keep track of attended words and coverage loss see et al
can be used to encourage model focus on diverse words
while most papers focus on supervised learning with novel mechanisms in this paper we explore supervised training models
gan for language generation in this paper we borrow the idea of gan to make the generator output human readable
the major challenge in applying gan to sentence tion is the discrete nature of natural language
to generate a word sequence the generator usually has non differential parts such as argmax or other sample functions which cause the original gan to fail
in gulrajani et al
instead of feeding a discrete word sequence the authors directly feed the generator output layer to the discriminator
this method works because they use the earth mover s distance on gan as proposed in jovsky et al
which is able to evaluate the distance between a discrete and a continuous tribution
seqgan yu et al
tackles the sequence generation problem with reinforcement learning
here we refer to this approach as versarial reinforce
however the tor only measures the quality of whole sequence and thus the rewards are extremely sparse and the rewards assigned to all the generation steps are all the same
mc search yu et al
is proposed to evaluate the approximate reward at each time step but this method suffers from high time plexity
following this idea li et al
poses partial evaluation approach to evaluate the expected reward at each time step
in this per we propose the self critical adversarial inforce algorithm as another way to evaluate the expected reward at each time step
the formance between original wgan and proposed adversarial reinforce is compared in ment
proposed method the overview of the proposed model is shown in fig

the model is composed of three ponents generator g discriminator d and constructor r
both g and r are brid pointer generator networks see et al
which can decide to copy words from encoder put text via pointing or generate from vocabulary
they both take a word sequence as input and put a sequence of word distributions
tor d on the other hand takes a sequence as input and outputs a scalar
the model is learned from a set of documents and human written sentences yreal
to train the model a training document


xt


xt where xt is fed to g which outputs resents a word a sequence of word distributions





yn where yn is a distribution over all words in the lexicon
then we randomly sample a word ys n from each distribution and a word sequence ys ys


ys n is obtained according to
we feed the sampled word quence ys to reconstructor r which outputs other sequence of word distributions x
the constructor r reconstructs the original text from ys
that is we seek an output of reconstructor x that is as close to the original text as possible hence the loss for training the reconstructor rloss is dened as ys rloss x k where the reconstruction loss x is the entropy loss computed between the reconstructor output sequence x and the source text or the negative conditional log likelihood of source text given word sequence ys sampled from
the reconstructor output sequence x is forced by source text
the subscript s in x indicates that x is reconstructed from ys
k is the number of training documents and is the mation of the cross entropy loss over all the ing documents
in the proposed model the generator g and constructor r form an auto encoder
however the reconstructor r does not directly take the tor output distribution as input
instead the reconstructor takes a sampled discrete sequence ys as input
due to the non differentiable property of discrete sequences we apply the reinforce gorithm which is described in section
in addition to reconstruction we need the criminator d to discriminate between the real quence yreal and the generated sequence ys to ularize the generated sequence satisfying the mary distribution
d learns to give yreal higher scores while giving ys lower scores
the loss for training the discriminator d is denoted as dloss this is further described in section
g learns to minimize the reconstruction loss rloss while maximizing the loss of the nator d by generating a summary sequence ys that can not be differentiated by d from the real thing
the loss for the generator gloss is gloss rloss loss where loss is highly related to dloss but not necessary the and is a hyper parameter
after obtaining the optimal generator by ing we use it to generate summaries
generator g and discriminator d together form a gan
we use two different adversarial training methods to train d and g as shown in fig
these two methods have their own discriminators and
discriminator takes the generator put layer as input whereas discriminator takes the sampled discrete word sequence ys as input
the two methods are described respectively in sections
and

minimizing reconstruction loss because discrete sequences are non differentiable we use the reinforce algorithm
the ator is seen as an agent whose reward given the source text is x
maximizing the ward is equivalent to minimizing the tion loss rloss in
however the reconstruction found that if the reconstructor r directly takes as input the generator g learns to put the information about the input text in the distribution of making it difcult to sample meaningful sentences from
loss has different formulations in different approaches
this will be clear in sections
and

figure architecture of proposed model
the generator network and reconstructor network are a hybrid pointer generator network but for simplicity we omit the pointer and the attention parts
from the discriminator for use as a reward signal loss varies widely from sample to sample and thus to the generator
the rewards to the generator are not stable either
hence we add a baseline to reduce their difference
we apply self critical sequence training rennie et al
the modied reward x from reconstructor r with the baseline for the tor is
method wasserstein gan in the lower left of fig
the discriminator model of this method is shown as
is a deep cnn with residual blocks which takes a sequence of word distributions as input and outputs a score
the discriminator loss dloss is x x x b ya x where x is the baseline
is also the same cross entropy reconstruction loss as x except that x is obtained from ya is a word sequence ya instead of ys
n where ya n


ya


ya ya n is selected using the argmax function from the output distribution of generator
as in the early training stage the sequence ys barely yields higher reward than sequence ya to encourage exploration we duce the second baseline score b which ally decreases to zero
then the generator is dated using the reinforce algorithm with ward x to minimize rloss
gan training with adversarial training the generator learns to produce sentences as similar to the human written sentences as possible
here we conduct ments on two kinds of methods of language eration with gan
in section
we directly feed the generator output probability distributions to the discriminator and use a wasserstein gan wgan with a gradient penalty
in section
we explore adversarial reinforce which feeds sampled discrete word sequences to the inator and evaluates the quality of the sequence dloss k k k k k k where k denotes the number of training ples in a batch and k denotes the k ple
the last term is the gradient penalty et al

we interpolate the tor output layer and the real sample yreal and apply the gradient penalty to the interpolated sequence yi
determines the gradient penalty scale
in equation for wgan the generator maximizes loss loss
k k
method self critic adversarial reinforce in this section we describe in detail the posed adversarial reinforce method
the core idea is we use the lstm discriminator to ate the current quality of the generated sequence ys


ys ys i at each time step i
the generator knows that compared to the last time step as the generated sentence either improves or worsens it can easily nd the problematic generation step in a long sequence and thus x the problem easily


discriminator as shown in fig
the is a unidirectional lstm network which takes a crete word sequence as input
at time step i given input word ys i it predicts the current score based on the sequence


yi
the score is viewed as the quality of the current sequence
an example of discriminator regularized by weight et al
is shown in fig

figure when the second arrested appears as the sentence becomes ungrammatical the tor determines that this example comes from the generator
hence after this time step it outputs low scores
in order to compute the discriminator loss dloss we sum the scores


sn of the whole sequence ys to yield n n sn
k k k k where n denotes the generated sequence length
then the loss of discriminator is dloss k k similar to previous section the last term is dient penalty term
with the loss mentioned above the discriminator attempts to quickly mine whether the current sequence is real or fake
the earlier the timestep discriminator determines whether the current sequence is real or fake the lower its loss


self critical generator since we feed a discrete sequence ys to the the gradient from the discriminator criminator can not directly back propagate to the generator
here we use the policy gradient method
at timestep i we use the i timestep score from the discriminator as its self critical baseline
the reward rd i evaluates whether the quality of quence in timestep i is better or worse than that in timestep i
the generator reward rd from i is rd i si if i otherwise
however some sentences may be judged as bad sentences at the previous timestep but at later timesteps judged as good sentences and vice
hence we use the discounted expected ward d with discount factor to calculate the counted reward di at time step i as di jird j
n j i to maximize the expected discounted reward the loss of generator is loss eys i we use the likelihood ratio trick to approximate the gradient to minimize



ys i
experiment our model was evaluated on the english chinese gigaword datasets and cnn daily mail dataset
in section

and
the experiments were conducted on english gigaword while the ments were conducted on cnn daily mail dataset and chinese gigaword dataset respectively in tions
and

we used as our evaluation metric
during testing when ing the generator to generate summaries we used beam search with beam and we eliminated repetition
we provide the details of the mentation and re processing respectively in appendix a and b
before jointly training the whole model we pre trained the three major components ator discriminator and reconstructor separately
first we pre trained the generator in an vised manner so that the generator would be able to somewhat grasp the semantic meaning of the source text
the details of the pre training are in appendix c
we pre trained the discriminator and reconstructor respectively with the pre trained generator s output to ensure that these two critic networks provide good feedback to the generator
used pyrouge package with option
to compute rouge score for all experiments
task labeled
m b trivial baseline c unpaired d semi supervised e transfer learning methods training on generator rush et al
chopra et al
zhou et al
pre trained generator wgan adversarial reinforce wgan adversarial reinforce and blunsom wgan adversarial reinforce and blunsom wgan adversarial reinforce pre trained generator wgan adversarial reinforce





































r l


















k k m table average rouge scores on english gigaword
and r l refers to rouge rouge and rouge l respectively
results marked with are obtained from corresponding papers
in part a the model was trained supervisedly
in row we select the article s rst eight words as its summary
part c are the results obtained without paired data
in part d we trained our model with few labeled data
in part e we pre trained generator on cnn diary and used the summaries from cnn diary as real data for the discriminator

english gigaword the english gigaword is a sentence tion dataset which contains the rst sentence of each article and its corresponding headlines
the preprocessed corpus contains
m training pairs and k validation pairs
we trained our model on part of or fully unparalleled data on
m ing set
to have fair comparison with previous works the following experiments were evaluated on the k testing set same as rush et al
miao and blunsom
we used the sentences in article headlines as real data for
as shown in the following experiments the lines can even come from another set of ments not related to the training documents
the results on english gigaword are shown in table
wgan and adversarial reinforce refer to the adversarial training methods tioned in sections
and
respectively
sults trained by full labeled data are in part a
in row we trained our generator by of using general sentences as real data for criminator we chose sentences from headlines because they have their own unique distribution
pervised training
compared with the previous work zhou et al
we used simpler model and smaller vocabulary size
we did not try to achieve the state of the art results because the cus of this work is unsupervised learning and the proposed approach is independent to the rization models used
in row we simply took the rst eight words in a document as its mary
the results for the pre trained generator with method mentioned in appendix
c is shown in row
in part c we directly took the sentences in the summaries of gigaword as the training data of discriminator
compared with the pre trained generator and the trivial baseline the proposed approach rows and showed good provement
in fig
we provide a real example
more examples can be found in the appendix
d

semi supervised learning in semi supervised training generator was trained with few available labeled data
during training we conducted teacher forcing with beled data on generator after several updates out labeled data
with k k and m beled data the teacher forcing was conducted ery and updates without paired data spectively
in teacher forcing given source text as input the generator was teacher forced to dict the human written summary of source text
teacher forcing can be regarded as regularization of unpaired training that prevents generator from producing unreasonable summaries of source text
we found that if we teacher forced generator too frequently generator would overt on training data since we only used very few labeled data on semi supervised training
the performance of semi supervised model in english gigaword regarding available labeled data is shown in table part d
we compared our results with miao and blunsom which was the previous state of the art method on supervised summarization task under the same amount of labeled data
with both k and m labeled data our method performed better
thermore with only m labeled data using versarial reinforce even outperformed vised training in table with the whole
m labeled data
figure real examples with methods referred in table
the proposed methods generated maries that grasped the core idea of the articles

cnn daily mail dataset the cnn daily mail dataset is a long text marization dataset which is composed of news ticles paired with summaries
we evaluated our model on this dataset because it s a popular mark dataset and we want to know whether the proposed model works on long input and long output sequences
the details of processing can be found in appendix
b
in paired training to prevent the model from directly matching the input articles to its corresponding summaries we split the training pairs into two equal sets one set only supplied articles and the other set only supplied summaries
the results are shown in table
for vised approaches in part a although our model was similar to see et al
due to the smaller vocabulary size we did nt tackle of vocabulary words simpler model architecture shorter output length of generated summaries there was a performance gap between our model and the scores reported in see et al

pared to the baseline in part b which took the rst three sentences of articles as summaries the models fell behind
that was cause news writers often put the most important information in the rst few sentences and thus even the best abstractive summarization model only slightly beat the baseline on rouge scores
however during pre training or training we did nt make assumption that the most tant sentences are in rst few sentences
we observed that our unpaired model yielded it yielded lower decent score but and rouge l score
that was bly because the length of our generated sequence was shorter than ground truth and our lary size was small
another reason was that the generator was good at selecting the most tant words from the articles but sometimes failed to combine them into reasonable sentences cause it s still difcult for gan to generate long sequence
in addition since the reconstructor only evaluated the reconstruction loss of whole quence as the generated sequence became long the reconstruction reward for generator became extremely sparse
however compared to trained generator rows v
s
our model still enhanced the rouge score
an real example of generated summary can be found at appendix
d fig


transfer learning the experiments conducted up to this point quired headlines unpaired to the documents but in in the same domain to train discriminator
this subsection we generated the summaries from english gigaword target domain but the maries for discriminator were from cnn daily mail dataset source domain
the results of transfer learning are shown in ble
part e
table is the result of methods training on our generator see et al
baseline see et al
c unpaired pre trained generator wgan adversarial reinforce











r l





table rouge scores on cnn diary mail dataset
in row b the rst three sentences were taken as summaries
part c are the results obtained without paired data
the results with symbol are directly obtained from corresponding papers
a training with paired data supervised baseline methods


pre trained generator
wgan adversarial reinforce





r l




c unpaired table rouge scores on chinese gigaword
in row b we selected the article s rst fteen words as its summary
part c are the results obtained without paired data
trained generator and the poor pre training result indicates that the data distributions of two datasets are quite different
we nd that using sentences from another dataset yields lower rouge scores on the target testing set parts e v
s
c due to the mismatch word distributions between the maries of the source and target domains
ever the discriminator still regularizes the ated word sequence
after unpaired training the model enhanced the rouge scores of the trained model rows v
s
and it also surpassed the trivial baselines in part b

gan training in this section we discuss the performance of two gan training methods
as shown in the table in english gigaword our proposed versarial reinforce method performed better than wgan
however in table our proposed method slightly outperformed by wgan
in tion we nd that when training with wgan vergence is faster
because wgan directly uates the distance between the continuous bution from generator and the discrete distribution from real data the distribution was sharpened at an early stage in training
this caused generator to converge to a relatively poor place
on the other hand when training with reinforce tor keeps seeking the network parameters that can better fool discriminator
we believe that training gan on language generation with this method is worth exploring

chinese gigaword the chinese gigaword is a long text tion dataset composed of paired headlines and news
unlike the input news in english gigaword the news in chinese gigaword consists of eral sentences
the results are shown in table
row a lists the results using
m summary pairs to directly train the generator out the reconstructor and discriminator this is the upper bound of the proposed approach
in row b we simply took the rst fteen words in a ment as its summary
the number of words was chosen to optimize the evaluation metrics
part c are the results obtained in the scenario out paired data
the discriminator took the maries in the training set as real data
we show the results of the pre trained generator in row rows and are the results for the two gan training methods respectively
we nd that despite the performance gap between the paired and supervised methods rows v
s
a the proposed method yielded much better performance than the trivial baselines rows v
s
b
conclusion and future work using gan we propose a model that encodes text as a human readable summary learned out document summary pairs
in future work we hope to use extra discriminators to control the style and sentiment of the generated summaries
steven j
rennie etienne marcheret youssef mroueh jarret ross and vaibhava goel

self critical sequence training for image captioning
cvpr
alexander m
rush sumit chopra and jason weston

a neural attention model for abstractive tence summarization
emnlp
abigail see peter j
liu and christopher d
manning

get to the point summarization with generator networks
acl
ilya sutskever oriol vinyals and quoc v
le

sequence to sequence learning with neural works
nips
zhaopeng tu zhengdong lu yang liu xiaohua liu and hang li

modeling coverage for neural machine translation
acl
oriol vinyals meire fortunato and navdeep jaitly

pointer networks
nips
lantao yu weinan zhang jun wang and yong yu

seqgan sequence generative adversarial nets with policy gradient
aaai
qingyu zhou nan yang furu wei and ming zhou

selective encoding for abstractive sentence summarization
acl
jun yan zhu taesung park phillip isola and alexei a
efros

unpaired image to image translation using cycle consistent adversarial works
arxiv preprint

references martin arjovsky soumith chintala and lon arxiv preprint tou

wasserstein gan


dzmitry bahdanau philemon brakel kelvin xu anirudh goyal ryan lowe joelle pineau aaron courville and yoshua bengio

an actor critic algorithm for sequence prediction
arxiv preprint

dzmitry bahdanau kyunghyun cho and yoshua gio

neural machine translation by jointly learning to align and translate
iclr
sumit chopra michael auli and alexander m
rush

abstractive sentence summarization with tentive recurrent neural networks
hlt naac
ian j
goodfellow jean pouget abadie mehdi mirza bing xu david warde farley sherjil ozair aaron courville and yoshua bengio

generative versarial networks
arxiv preprint

ishaan gulrajani faruk ahmed martin arjovsky cent dumoulin and aaron courville

proved training of wasserstein gans
arxiv preprint

ryan kiros yukun zhu ruslan salakhutdinov richard s
zemel antonio torralba raquel sun and sanja fidler

skip thought vectors
nips
jiwei li minh thang luong and dan jurafsky

a hierarchical neural autoencoder for paragraphs and documents
acl
jiwei li will monroe tianlin shi sbastien jean alan ritter and dan jurafsky

adversarial ing for neural dialogue generation
arxiv preprint

chin yew lin

rouge a package for matic evaluation of summaries
in text tion branches out acl workshop
alireza makhzani jonathon shlens navdeep jaitly ian goodfellow and brendan frey

ial autoencoders
arxiv preprint

yishu miao and phil blunsom

language as a latent variable discrete generative models for tence compression
emnlp
ramesh nallapati bowen zhou cicero nogueira dos santos caglar gulcehre and bing xiang

abstractive text summarization using sequence sequence rnns and beyond
emnlp
romain paulus caiming xiong and richard socher

a deep reinforced model for abstractive marization
arxiv preprint

marcaurelio ranzato sumit chopra michael auli and wojciech zaremba

sequence level ing with recurrent neural networks
iclr
a implementation network architecture
the model architecture of generator and reconstructor is almost the same except the length of input and output sequence
we adapt model architecture for our generator and reconstructor from see et al
who used hybrid pointer network for text summarization
the hybrid pointer networks of generator and constructor are all composed of two one layer directional lstms as its encoder and decoder spectively with a hidden layer size of
since we use two kinds of methods on adversarial ing there are two discriminators with different model architecture
in the section
the criminator is composed of four residual blocks with hidden dimensions
while in section
we use only one layer unidirectional lstm with a hidden size of as our discriminator
details of training
in all experiments except in section
we set the weight in ling rloss to
in section
to prevent tor from overtting to sentences from cnn daily mail summary we set the weight to which was larger than other experiments
we nd that the if the value of is too large generator will start to generate output unlike human written tences
on the other hand if the value of is too small the sentences generated by generator will sometimes become unrelated to input text of erator
for all the experiments the baseline b in gradually decreases from
to zero within updates on generator
we set the weight of the gradient penalty in section
to and used mizer with a learning rate of
and
on the generator and discriminator respectively
in section

the weight of gradient penalty terms was
and used rmspropoptimizer with a learning rate of
and
on the ator and discriminator respectively
it s also ble to apply weight clipping in discriminator ing but the performance of gradient penalty trick was better
b corpus pre processing english gigaword we used the script of rush et al
to construct our training and testing datasets
the vocabulary size was set to k in all experiments
cnn diary mail we obtained ing pairs validation pairs and testing pairs identical to see et al
by using the scripts provided by see et al

to make our model easier to train during training and testing time we truncated input articles to tokens original articles has tokens on average and restricted the length of generator output summaries nal summaries has tokens on average to tokens
the vocabulary size was set to
chinese gigaword the chinese gigaword is a long text summarization dataset which is composed of
m paired data of headlines and news
we preprocessed the raw data as following
first we selected the k most quent chinese characters to form our lary
we ltered out headline news pairs with excessively long or short news segments or that contained too many out of vocabulary chinese characters yielding
m news pairs from which we randomly selected k headline news pairs as our testing set k headline news pairs as our validation set and the remaining pairs as our training set
ing training and testing the generator took the rst chinese characters of the source text as input
c model pre training as we found that the different pre training ods for the generator inuenced nal performance dramatically in all of the experiments we felt it was important to nd a proper unsupervised training method to help the machine grasp mantic meaning
the summarization tasks on two datasets is different one is sentence rization while the other is long text tion
therefore we used the different pre training strategies on two datasets described below
cnn diary mail the cnn diary mail is a long text summarization dataset in which the source text consists of several tences
given the previous i sentences


from the source text the generator predicted the next four sentences senti

in the source text as its pre training target
if more than of the words in target sentences senti


did not appear in in each cnn diary mail article as tor input and generator randomly predicted one of the sentences of the article s summary
in addition we used the one sentence from cnn diary mail summaries as real data to discriminator instead full summaries
the given text we ltered out this pre training sample pair
this pre training method lowed the generator to capture the tant semantic meanings of the source text
although the rst few sentences of articles in cnn diary mail contains the main mation of articles we hope we can provide a more general pre training method which do nt have any assumption of dataset and can be easily applied to other datasets
the chinese gigaword pre training method of chinese gigaword was similar to cnn diary mail except that generator predicted the next sentence instead of next consecutive four sentences
english chinese gigaword as the source text of english gigaword is made up of only one sentence it is not feasible to split the last sentence from the source text hence the vious pre training method on chinese word is not appropriate for this dataset
to properly initialize the set we randomly lected to consecutive words in the source text after which we randomly swapped of the words in the source text
given text with incorrect word arrangements the ator predicted the selected words in the rect arrangement
we pre trained in this way because we expect the generator to initialize in chinese with a rough language model
gigaword we also conducted experiments on pre training in this manner but the results were not as good as those shown in the part c of table
in addition we also used the retrieved paired data in row in table to pre train the generator in english word
however pre training generator with this method does nt yield results better than those in table
transfer learning before unsupervised training the generator was pre trained with paralleled data on cnn daily mail dataset
however the characteristics for two datasets in english gigaword the are different
ticles were short and the summaries consist of only one sentence while in cnn daily mail dataset the articles were extremely long and summaries consist of several sentences
to overcome these differences during training time we took the rst words d examples figure figure figure figure figure figure figure an example of generated summary of cnn diary mail

