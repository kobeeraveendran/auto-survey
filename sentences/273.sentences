abstractive summarization with combination of pre trained sequence to sequence and saliency models itsumi saito kyosuke nishida kosuke nishida junji tomita ntt media intelligence laboratory ntt corporation itsumi
saito

ntt
co
jp r a m l c
s c v
v i x r a abstract pre trained sequence to sequence seq to seq models have signicantly improved the racy of several language generation tasks cluding abstractive summarization
although the uency of abstractive summarization has been greatly improved by ne tuning these models it is not clear whether they can also identify the important parts of the source text to be included in the summary
in this study we investigated the effectiveness of ing saliency models that identify the tant parts of the source text with the trained seq to seq models through extensive experiments
we also proposed a new bination model consisting of a saliency model that extracts a token sequence from a source text and a seq to seq model that takes the quence as an additional input text
tal results showed that most of the tion models outperformed a simple ne tuned seq to seq model on both the cnn dm and xsum datasets even if the seq to seq model is pre trained on large scale corpora
moreover for the cnn dm dataset the proposed bination model exceeded the previous performed model by
points on l
introduction pre trained language models such as bert vlin et al
have signicantly improved the accuracy of various language processing tasks
however we can not apply bert to language eration tasks as is because its model structure is not suitable for language generation
several pre trained seq to seq models for language eration lewis et al
raffel et al
based on an encoder decoder transformer model which is a standard model for language tion have recently been proposed
these work in progress
els have achieved state of the art results in ous language generation tasks including tive summarization
however when generating a summary it is sential to correctly predict which part of the source text should be included in the summary
some previous studies without pre training have ined combining extractive summarization with stractive summarization gehrmann et al
hsu et al

although pre trained seq to seq models have achieved higher accuracy compared to previous models it is not clear whether ing which part of the source text is important can be learned through pre training
the purpose of this study is to clarify the tiveness of combining saliency models that tify the important part of the source text with a trained seq to seq model in the abstractive marization task
our main contributions are as follows we investigated nine combinations of trained seq to seq and token level saliency models where the saliency models share the parameters with the encoder of the seq to seq model or extract important tokens dently of the encoder
we proposed a new combination model the conditional summarization model with portant tokens cit in which a token quence extracted by a saliency model is plicitly given to a seq to seq model as an ditional input text
we evaluated the combination models on the cnn dm hermann et al
and xsum narayan et al
datasets
our cit model outperformed a simple ne tuned model in terms of rouge scores on both datasets
task denition our study focuses on two tasks abstractive marization and saliency detection
the main task is abstractive summarization and the sub task is saliency detection which is the prediction of portant parts of the source text
the problem mulations of each task are described below
task abstractive summarization given the source text x the output is an abstractive mary y


yt
task saliency detection given the source text x with l words


xl the output is the saliency score s


sl
in this study we investigate several tions of models for these two tasks
pre trained seq to seq model there are several pre trained seq to seq models applied for abstractive summarization song et al
dong et al
raffel et al

the models use a simple transformer based decoder model vaswani et al
in which the encoder decoder model is pre trained on large labeled data

transformer based encoder decoder in this work we dene the transformer based encoder decoder model as follows
encoder the encoder consists of m layer coder blocks
the input of the encoder is x xi


xl
the output through the m layer encoder blocks is dened as e hm el rld



hm hm h m the encoder block consists of a self attention module and a two layer feed forward network
decoder the decoder consists of m layer coder blocks
the inputs of the decoder are the output of the encoder h m and the output of the e previous step of the decoder



the output through the m layer transformer decoder blocks is dened as h m dt rtd



hm hm in each step t the hm dt is projected to the ulary space and the decoder outputs the highest probability token as the next token
the former decoder block consists of a self attention module a context attention module and a layer feed forward network
multi head attention the encoder and coder blocks use multi head attention which sists of a combination of k attention heads and is denoted as k v


o where each head is headi q v w v i kw k i
the weight matrix a in each attention head attention k a v is dened as a softmax rij where dk d k rid k v rjd
in the m th layer of self attention the same is given to q k and v
in the to k resentation h m context attention we give h m and v
to q and h m e
summary loss function to ne tune the seq to seq model for abstractive summarization we use cross entropy loss as lsum log p t n t n t where n is the number of training samples
saliency models several studies have proposed the combination of a token level saliency model and a seq to seq model which is not pre trained and reported its effectiveness gehrmann et al
zhou et al

we also use a simple token level saliency model as a basic model in this study

basic saliency model a basic saliency model consists of m transformer encoder blocks encodersal and a single layer feed forward network
we dene the saliency score of the l token l l in the source text as sl where encodersal represents the output of the last layer of encodersal rd and are learnable parameters and represents a sigmoid function

two types of saliency model for combination in this study we use two types of saliency model for combination a shared encoder and an tor
each model structure is based on the basic saliency model
we describe them below
shared encoder the shared encoder shares the parameters of encodersal and the encoder of the seq to seq model
this model is jointly trained with the seq to seq model and the saliency score is used to bias the representations of the seq to seq model
extractor the extractor extracts the important tokens or sentences from the source text on the basis of the saliency score
the extractor is arated with the seq to seq model and each model is trained independently

pseudo reference label the saliency model predicts the saliency score sl for each token xl
if there is a reference label rl for each xl we can train the saliency model in a supervised manner
however the reference label for each token is typically not given since the training data for the summarization consists of only the source text and its reference summary
although there are no reference saliency labels we can make pseudo reference labels by aligning both source and summary token sequences and tracting common tokens gehrmann et al

we used pseudo labels when we train the saliency model in a supervised manner

saliency loss function to train the saliency model in a supervised way with pseudo reference labels we use binary cross entropy loss as lsal n l n l l log sn rn l l sn l combination types we roughly categorize the combinations into three types
figure shows an image of each combination
the rst type uses the shared encoder

these models consist of the shared encoder and the decoder where the shared encoder module plays two roles saliency detection and the ing of the seq to seq model
the saliency scores are used to bias the representation of the seq seq model for several models in this type
the second type uses the extractor


these models consist of the extractor encoder and decoder and follow two steps rst the tractor extracts the important tokens or sentences from the source text and second the encoder uses them as an input of the seq to seq models
our proposed model cit belongs to this type
the third type uses both the shared encoder and the extractor

these models consist of the extractor shared encoder and decoder and also follow two steps rst the extractor extracts the important tokens from the source text and second the shared encoder uses them as an input of the seq to seq model
loss function from the viewpoint of the loss there are two major types of model function those that use the saliency loss
and those that do not
we also denote the loss function for the seq to seq model as labs and the loss function for the extractor as lext
lext is trained with lsal and labs is trained with lsum or lsum lsal

using shared encoder to combine the saliency model and the seq to seq model multi task mt this model trains the shared encoder and the decoder by minimizing both the summary and saliency losses
the loss function of this model is labs lsum lsal
selective encoding se this model uses the saliency score to weight the shared encoder put
specically the nal output hm el of the shared encoder is weighted as where rn the n th sample
l is a pseudo reference label of token xl in hm el hm el sl
combined models this section describes nine combinations of the pre trained seq to seq model and saliency models
then we replace the input of the decoder hm el with hm el
although zhou et al
used bigru we use transformer for fair comparison
the loss function of this model is labs lsum
figure combinations of seq to seq and saliency models
purple encoder
blue decoder
red shared encoder which is a shared model for saliency detection and encoding used in a and
yellow extractor which is an independent saliency model to extract important c sentences xs or e tokens c from the source text x
each of these colored blocks represents m transformer blocks
gray linear transformation
green context attention
pink output trained in a supervised manner where s is the saliency score and y is the summary
combination of se and mt this model has the same structure as the se
the loss function of this model is labs lsum lsal
selective attention sa this model weights the attention scores of the decoder side unlike the se model
specically the attention score i rl in each step t is weighted by sl
at at i is a t th row of ai rt l which is a weight matrix of the i th attention head in the context attention eq

at il at ilsl l at ilsl
gehrmann et al
took a similar approach in that their model weights the copy probability of a pointer generator model
however as the pre trained seq to seq model does not have a copy mechanism we weight the context attention for all transformer decoder blocks
the loss function of this model is labs lsum
combination of sa and mt this model has the same structure as the sa
the loss function of this model is labs lsum lsal

using the extractor to rene the input text sentence extraction then generation seg this model rst extracts the saliency sentences on the basis of a sentence level saliency score sj
sj is calculated by using the token level saliency score of the extractor sl as sj nj sl l xlxj where nj and xj are the number of tokens and the set of tokens within the j th sentence
top p sentences are extracted according to the level saliency score and then concatenated as one text xs
these extracted sentences are then used as the input of the seq to seq model
in the training we extracted xs which imizes the rouge l scores with the reference summary text
in the test we used the average number of sentences in xs in the training set as p
the loss function of the extractor is lext lsal and that of the seq to seq model is labs lsum

proposed using extractor to extract an additional input text conditional summarization model with portant tokens we propose a new combination of the extractor and the seq to seq model cit which can consider important tokens explicitly
although the se and sa models softly weight the representations of the source text or attention scores they can not select salient tokens explicitly
seg explicitly extracts the salient sentences from the source text but it can not give token level mation to the seq to seq model and it sometimes drops important information when extracting tences
in contrast cit uses the tokens extracted according to saliency scores as an additional input of the seq to seq model
by adding token level formation cit can effectively guide the tive summary without dropping any important formation
specically k tokens c


ck are tracted in descending order of the saliency score s
s is obtained by inputting x to the extractor
the order of c retains the order of the source text x
a combined text x is given to the seq to seq model as the input text
the loss function of the extractor is lext lsal and that of the seq to seq model is labs lsum

proposed combination of extractor and shared encoder combination of cit and se this model bines the cit and se so cit uses an extractor for extracting important tokens and se is trained by using a shared encoder in the seq to seq model
the se model is trained in an unsupervised way
e of the shared encoder the output h m is weighted by saliency score s with eq
where s is estimated by using the output of the shared encoder with eq

the loss tion of the extractor is lext lsal and that of the seq to seq model is labs lsum
combination of cit and sa this model bines the cit and sa so we also train two saliency models
the sa model is trained in the same as the cit an unsupervised way i is se model
the attention score at weighted by s with eq

the loss function of the extractor is lext lsal and that of the seq to seq model is labs lsum
experiments
dataset we used the cnn dm dataset hermann et al
and the xsum dataset narayan et al
which are both standard datasets for news summarization
the details of the two datasets are listed in table
the cnn dm is a highly tractive summarization dataset and the xsum is a highly abstractive summarization dataset


model congurations we used bartlarge lewis et al
which is one of the state of the art models as the trained seq to seq model and robertabase liu et al
as the initial model of the extractor
in the extractor of cit stop words and duplicate tokens are ignored for the xsum dataset
we used for the implementation of the seq to seq model
for ne tuning of bartlarge and the combination models we used the same rameters as the ofcial code
for ne tuning of set train dev eval cnn dm xsum avg
length

table details of the datasets used in this paper
models bart lewis et al
bart our ne tuning mt se se mt sa sa mt seg cit cit se cit sa





















rl










table results of bart and combined models on cnn dm dataset
five row groups are the models scribed in


and
in order from top to bottom
robertabase we used
we set the learning rate to
and the batch size to

evaluation metrics we used rouge scores including and rouge l r l as the evaluation metrics lin
rouge scores were calculated using the

results do saliency models improve summarization curacy in highly extractive datasets rouge scores of the combined models on the cnn dm dataset are shown in table
we can see that all combined models outperformed the simple tuned bart
this indicates that the saliency tection is effective in highly extractive datasets
one of the proposed models cit se achieved the highest accuracy
the cit model alone also outperformed other saliency models
this cates that the cit model effectively guides the stractive summarization by combining explicitly extracted tokens
do saliency models improve summarization curacy in highly abstractive datasets rouge scores of the combined models on the xsum
com transformers
com pytorch fairseq
com pltrdy models bart lewis et al
bart our ne tuning mt se se mt sa sa mt seg cit cit se cit sa





















r l










table results of bart and combined models on xsum dataset
the underlined result represents the best result among the models that outperformed our simple ne tuning result
cnn dm models




cit top k toks





cit sents





xsum r l r l



table results of saliency models on cnn dm and xsum datasets
cit extracted the top k tokens or sentences from the source text
and lapata
the summaries in xsum are highly tive so the result of bertsumext for xsum was not reported
dataset are shown in table
the cit model performed the best although its improvement was smaller than on the cnn dm dataset
moreover the accuracy of the mt se mt and seg els decreased on the xsum dataset
these results were very different from those on the cnn dm dataset
one reason for the difference can be traced to the quality of the pseudo saliency labels
cnn dm is a highly extractive dataset so it is relatively easy to create token alignments for erating pseudo saliency labels while in contrast a summary in xsum is highly abstractive and short which makes it difcult to create pseudo labels with high quality by simple token ment
to improve the accuracy of summarization in this dataset we have to improve the quality of the pseudo saliency labels and the accuracy of the saliency model
how accurate are the outputs of the extractors we analyzed the quality of the tokens extracted by the extractor in cit
the results are rized in table
on the cnn dm dataset the and scores of our tor top k tokens were higher than other data params r l models g m


massbase g m


g m


g


g m


large g m


hugenews
t m


g m


ernie large g m


g m


g m


cit table results of state of the art models and the posed model on cnn dm dataset
we also report the size of pre training data and parameters utilized for each model
et al
and ata et al
et al
et al
et al
et al
al
et al
models data params r l massbase g m


g m


g m


large g m


hugenews
t m


g m


g m


cit table results of state of the art models and the posed model on xsum dataset
et al
and lapata et al
et al
et al
els while the rouge l score was lower than the other sentence based extraction method
this is because that our token level extractor nds the important tokens whereas the seq to seq model learns how to generate a uent summary rating these important tokens
on the other hand the extractive result on the xsum dataset was lower
for highly abstractive datasets there is little overlap between the tokens
we need to consider how to make the high quality pseudo saliency labels and how to evaluate the similarity of these two sequences
does the cit model outperform other tuned models our study focuses on the binations of saliency models and the pre trained there are several seq to seq model
however studies that focus more on the pre training egy
we compared the cit model with those models
their rouge scores are shown in bles and
from table we can see that our model outperformed the recent pre trained models on the cnn dm dataset
even though pegasushugenews was pre trained on the largest corpus comprised of news like articles the curacy of abstractive summarization was not proved much
our model improved the accuracy without any additional pre training
this result dicates that it is more effective to combine saliency models with the seq to seq model for generating a highly extractive summary
on the other hand on the xsum dataset pegasushugenews improved the rouge scores and achieved the best results
in the xsum dataset summaries often include the expressions that are not written in the source text
therefore ing the pre training data and learning more terns were effective
however by improving the quality of the pseudo saliency labels we should be able to improve the accuracy of the cit model
related work and discussion pre trained language models for abstractive summarization liu used bert for their sentence level extractive summarization model
zhang et al
proposed a new pre trained model that considers document level tion for sentence level extractive summarization
several researchers have published pre trained encoder decoder models very recently wang et al
lewis et al
raffel et al

wang et al
pre trained a based pointer generator model
lewis et al
pre trained a standard transformer based encoder decoder model using large unlabeled data and achieved state of the art results
dong et al
and xiao et al
extended the bert structure to handle seq to seq tasks
all the studies above focused on how to learn a universal pre trained model they did not consider the combination of pre trained and saliency els for an abstractive summarization model
abstractive summarization with saliency models hsu et al
gehrmann et al
and you et al
incorporated a and word level extractive model in the pointer generator model
their models weight the copy probability for the source text by using an extractive model and guide the pointer generator model to copy important words
li et al
proposed a keyword guided abstractive chen and bansal marization model
proposed a sentence extraction and re writing model that is trained in an end to end manner learning
by using reinforcement cao et al
proposed a search and rewrite model
mendes et al
proposed a combination of sentence level extraction and compression
none of these models are based on a pre trained model
in contrast our purpose is to clarify whether combined models are effective or not and we are the rst to investigate the combination of pre trained seq to seq and saliency models
we compared a variety of combinations and claried which combination is the most effective
conclusion this is the rst study that has conducted extensive experiments to investigate the effectiveness of corporating saliency models into the pre trained seq to seq model
from the results we found that saliency models were effective in nding portant parts of the source text even if the to seq model is pre trained on large scale pora especially for generating an highly tive summary
we also proposed a new nation model cit that outperformed simple tuning and other combination models
our nation model improved the summarization racy without any additional pre training data and can be applied to any pre trained model
while cent studies have been conducted to improve marization accuracy by increasing the amount of pre training data and developing new pre training strategies this study sheds light on the importance of saliency models in abstractive summarization
references hangbo bao li dong furu wei wenhui wang nan yang xiaodong liu yu wang songhao piao jianfeng gao ming zhou and hsiao wuen hon

pseudo masked language els for unied language model pre training
corr

ziqiang cao wenjie li sujian li and furu wei

retrieve rerank and rewrite soft template based neural summarization
in acl pages
yen chun chen and mohit bansal

fast tive summarization with reinforce selected sentence rewriting
in acl pages
jacob devlin ming wei chang kenton lee and kristina toutanova

bert pre training of deep bidirectional transformers for language standing
in naacl hlt pages
of transfer learning with a unied text to text former
corr

kaitao song xu tan tao qin jianfeng lu and yan liu

mass masked sequence to quence pre training for language generation
in icml pages
ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan n gomez ukasz kaiser and illia polosukhin

attention is all you need
in nips pages
liang wang wei zhao ruoyu jia sujian li and jingming liu

denoising based in to sequence pre training for text generation
emnlp ijcnlp pages
dongling xiao han zhang yu kun li yu sun hao tian hua wu and haifeng wang

gen an enhanced pre training and tuning framework for natural language generation
corr

yu yan weizhen qi yeyun gong dayiheng liu nan duan jiusheng chen ruofei zhang and ming zhou

prophetnet predicting future gram for sequence to sequence pre training
corr

yongjian you weijia jia tianyi liu and wenmian yang

improving abstractive document marization with salient information modeling
in acl pages
jingqing zhang yao zhao mohammad saleh and ter j
liu

pegasus pre training with tracted gap sentences for abstractive summarization
corr

xingxing zhang furu wei and ming zhou

hibert document level pre training of cal bidirectional transformers for document rization
in acl pages
qingyu zhou nan yang furu wei and ming zhou

selective encoding for abstractive sentence summarization
in acl pages
li dong nan yang wenhui wang furu wei aodong liu yu wang jianfeng gao ming zhou and hsiao wuen hon

unied language model pre training for natural language in neurips pages ing and generation

sebastian gehrmann yuntian deng and alexander rush

bottom up abstractive summarization
in emnlp pages
karl moritz hermann tomas kocisky edward grefenstette lasse espeholt will kay mustafa leyman and phil blunsom

teaching in nips pages chines to read and comprehend

wan ting hsu chieh kai lin ming ying lee kerui min jing tang and min sun

a unied model for extractive and abstractive summarization using inconsistency loss
in acl pages
mike lewis yinhan liu naman goyal jan ghazvininejad abdelrahman mohamed omer levy veselin stoyanov and luke zettlemoyer

bart denoising sequence to sequence training for natural language generation translation and comprehension
corr

chenliang li weiran xu si li and sheng gao

guiding generation for abstractive text tion based on key information guide network
in acl pages
chin yew lin

rouge a package for matic evaluation of summaries
in acl pages
yang liu

fine tune bert for extractive marization
corr

yang liu and mirella lapata

text tion with pretrained encoders
in emnlp ijcnlp pages
yinhan liu myle ott naman goyal jingfei du dar joshi danqi chen omer levy mike lewis luke zettlemoyer and veselin stoyanov

roberta a robustly optimized bert pretraining approach
corr

afonso mendes shashi narayan sebastiao miranda zita marinho andre f
t
martins and shay b
jointly extracting and compressing cohen

documents with summary state representations
in naacl pages
shashi narayan shay b
cohen and mirella lapata

do nt give me the details just the summary topic aware convolutional neural networks for in emnlp pages treme summarization

colin raffel noam shazeer adam roberts katherine lee sharan narang michael matena yanqi zhou wei li and peter j
liu

exploring the limits
