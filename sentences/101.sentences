international journal on digital libraries manuscript no
will be inserted by the editor scientic document summarization via citation contextualization and scientic discourse arman cohan nazli goharian n u j l c
s c v
v i x r a received date accepted date abstract the rapid growth of scientic literature has made it dicult for the researchers to quickly learn about the developments in their respective elds
tic document summarization addresses this challenge by providing summaries of the important contributions of scientic papers
we present a framework for tic summarization which takes advantage of the tations and the scientic discourse structure
citation texts often lack the evidence and context to support the content of the cited paper and are even sometimes accurate
we rst address the problem of inaccuracy of the citation texts by nding the relevant context from the cited paper
we propose three approaches for textualizing citations which are based on query mulation word embeddings and supervised learning
we then train a model to identify the discourse facets for each citation
we nally propose a method for marizing scientic papers by leveraging the faceted tations and their corresponding contexts
we evaluate our proposed method on two scientic summarization datasets in the biomedical and computational tics domains
extensive evaluation results show that our methods can improve over the state of the art by large margins
this is a pre print of an article published on ijdl
the nal publication is available at springer via


arman cohan e mail
cs
georgetown
edu nazli goharian e mail
cs
georgetown
edu information retrieval lab department of computer ence georgetown university washington dc usa introduction the rapid growth of scientic literature in recent decades has created a challenge for researchers in ious elds to keep up with the newest developments
according to a recent study by bibliometric analysts the global scientic output doubles approximately every nine years further signifying this challenge
tence of surveys in dierent elds shows that nding an overview of key developments in scientic areas is sirable however procuring such surveys requires sive human eorts
scientic summarization aims at dressing this problem by providing a concise tion of important ndings and contributions of scientic papers reducing the time required to overview the tire paper to understand important contributions
ticle abstracts are a basic form of scientic summaries
while abstracts provide an overview of the paper they do not necessarily convey all the important tions and impacts of the paper the authors might ascribe contributions to their papers that are not existent
some important contributions might not be included in the abstract
the contributions stated in the abstract do not convey the article s impact over time
iv abstracts usually provide a very broad view of the papers and they may not be detailed enough for people seeking detailed contributions
v the tent distribution in the abstracts are not evenly drawn from dierent sections of the papers
these lems have inspired another type of scientic summaries which are obtained by utilizing a set of citations encing the original paper
each citation is ten accompanied by a short description explaining the ideas methods results or ndings of the cited work
this short description is called citation text or citance
therefore a set of citation texts by dierent arman cohan nazli goharian reference article voorhoeve et al
these mirnas could neutralize mediated cdk inhibition possibly through direct inhibition of the expression of the tumor suppressor
citing articles kloosterman and plasterk in a genetic screen and were found to allow proliferation of primary human cells voorhoeve et al

okada et al
two oncogenic mirnas and directly inhibit the expression of thereby allowing tumorigenic growth in the presence of voorhoeve et al

fig
example of epistemic value drift
the claims that voorhoeve et al
state as possibilities becomes fact in later citations okada et al
kloosterman and plasterk
pers can provide an overview of the main ideas methods and contributions of the cited paper and thus can form a summary of the referenced paper
these community based summaries capture the important contributions of the paper view the article from multiple aspects and reect the impact of the article to the community
at the same time there are multiple problems sociated with citation texts
they are written by ferent authors so they may be biased toward another work
the citation texts lack the context in terms of the details of the methods the data assumptions and results
more importantly the points and claims by the original paper might be misunderstood by the citing authors certain contributions might be ascribed to the cited work that are not on par with the original thor s intent
another serious problem is the tion of the epistemic value of claims which states that many claims by the original author might be stated as facts in the future citations
an example of this is shown in figure
as illustrated while the original authors write on some possibilities later the citing thors state them as known facts
these problems are even more serious in biomedical domain where slight misrepresentations of the specic ndings about ments diagnosis and medications could directly aect human lives
one way to address such problems is to consider the citations in their context from the reference article
therefore citation texts should be linked to the cic parts in the reference paper that correctly reect them
we call this citation contextualization
tion contextualization is a challenging task due to the terminology variations between the citing and cited thor s language usage
scientic papers have the unique characteristic of following a specic discourse structure
for example a typical scientic discourse structure follows this form problem and motivation methods experiments results and implications
the rhetorical status of a citation vides additional useful information that can be used in applications such as information extraction retrieval and summarization
each citation text could refer to specic discourse facets of the referenced paper
for example one citation could be about the main method of the referenced paper while the other one could tion their results
identifying these discourse facets has distinct values for scientic summarization it allows creating more coherent summaries and diversifying the points included in the generated scientic summaries
scientic summarization is recently further vated by summarization track and the computation linguistics summarization shared task
following these works and motivated by the challenges mentioned above we propose a framework for scientic summarization based on citations
our approach sists of the following steps contextualizing citation texts we propose several approaches for contextualizing citations
finding the exact reference context for the citations is lenging due to discourse variation and terminology dierences between the citing and the referenced thors
therefore traditional information retrieval ir methods are inadequate for nding the vant contexts
we propose to address this challenge by query reformulations utilizing word embeddings and domain specic knowledge
our main proach is a retrieval model for nding the ate context of the citations and is designed to handle terminology variations between the citing and cited authors
discourse structure after extracting the context of the citation texts we classify them into dierent course facets
we use a linear classier with variety of features for classifying the citations
summarization we propose two approaches for summarizing the papers
both approaches are based on summarization through the scientic community where the main points of a paper are captured by a set of given citations
our approach extends the vious works on citation based summarization by including the reference context to address the inaccuracy problem associated with the citation texts
after extracting the citation contexts from the reference paper we group them into dierent text analysis conference
nist
scientic document summarization via citation contextualization and scientic discourse discourse facets
then using the most central tences in each group we generate the nal summary
in particular our contributions are summarized as follows an approach for extracting the context of the citation texts from the reference article
fying the discourse facets of the citation contexts
a scientic summarization framework utilizing citation contexts and the scientic discourse structure
tensive evaluation on two scientic domains
related work
citation text analysis citations play an integral role in the scientic opment
they help disseminate the new ndings and they allow new works to be grounded on previous forts
while there is a large body of related work on analysis of citation networks instead of link ysis we focus on textual aspects of the citations
to better utilize the citations researchers have explored ways to extract citation texts which are short textual parts describing some aspects of the cited work
amples of the proposed approaches for extracting the citation texts include jointly modeling the link mation and the citation texts supervised markov random fields classiers and sequence labeling with segment classication
these approaches focus on nding the sentences or textual spans in the citing article that explain some aspects of the cited work
in this work we assume that citation texts are already tained either manually or by using one of these works
given the citation texts we instead focus on alizing these citation texts using the reference we nd the text spans in the reference article that most closely reect the citation text
there exists some related work on further ing the citations for nding their function or rhetorical status
in these works the authors tried to identify the reasons behind citations which can be a statement of weakness contrast or comparison age or compatibility or a neutral category
they posed a classication framework based on lexically and linguistically inspired features for classifying citation functions
the distribution of citations within the ture of scientic papers have been also studied
the authors of have investigated the problem of suring the intensity of the citations in scientic papers and in the authors proposed using the discourse facets for scientic article recommendation
recently a framework for understanding citation function has been proposed which unies all the previous forts in terms of denition of citation functions
while citation function can provide additional information for summarization in this work we do not utilize these formation
instead we utilize the discourse facet of the citation contexts in a reference paper

citation contextualization more recently there has been some eorts in alizing citations from the reference
in particular tac summarization and the cl scisumm shared task on computational linguistic summarization have released datasets to promote research for tion contextualization
the former is more domain cic focusing on biomedical scientic literature while the latter is in a more general domain consisting of lications in computational linguistics
to our edge there is no overview paper on tac
we briey discuss the successful approaches in cl scisumm
the authors of used an svm rank approach with features such as tf cosine similarity position of the reference sentence section position and named entity features
in another approach the authors used an svm classier with sentence similarity and lexicon based features
the authors of proposed a hybrid model based on tf idf similarity and a single layer neural network that scores the relevant reference texts above the irrelevant ones
finally in the work by the thors proposed the use of textsentencerank algorithm which is an enhanced version of the textrank rithm for ranking keywords in the documents
here we specically focus on the problem of terminology tion between the citing and cited authors
we propose approaches that address this problem
our proposed approaches are based on query reformulations word embeddings and domain specic knowledge

text summarization document summarization has been an active research area in nlp in recent decades there is a rich literature on text summarization
approaches towards rization can be divided into the following categories i topic modeling based in these proaches the content or topical distribution of the nal summary is estimated using a probabilistic work
solving an optimization problem these approaches cast the summarization problem as
nist
term frequency inverted document frequency
arman cohan nazli goharian an optimization problem where an objective function needs to be optimized with respect to some constraints
iii supervised models where selection of sentences in the summary are learned using a vised framework
iv graph based these proaches seek to nd the most central sentences in a document s graph where sentences are nodes and edges are similarities
v heuristic based these works approach the summarization problem by greedy selection of the content
vi neural networks more cently there has been some eorts on utilizing neural networks and sequence to sequence models for erating summaries of short texts and sentences
most of these works have focused on general domain summarization and news articles
scientic articles are much dierent than news articles in elements such as length language complexity and structure
one of the rst works in scientic article rization is done by where the authors trained a supervised naive bayes classier to select informative content for the summary
later the impact of citations to generate scientic summaries was realized
in the work by the authors proposed an approach for citation based summarization based on a clustering proach while in and the focused on producing coherent scientic summaries
we argue that citation texts by themselves are not always accurate and they lack the context of the cited paper
therefore if we only use the citation texts for scientic summarization the resulting summary would potentially suer from the same problems and it might not accurately reect the claims made in the original paper
we address this problem by leveraging the citation contexts from the reference paper
we also utilize the inherent discourse structure of the scientic documents to capture the portant content from all sections of the paper
we present a comprehensive framework for scientic summarization which utilizes and builds upon our lier eorts
we propose new approaches for citation contextualization
we further extend our periments on an additional dataset cl scisum and evaluate our approaches on both tac and scisum datasets providing detailed analysis
methodology our proposed method is a pipeline for summarizing entic papers
it consists of the following steps
citation contextualization extracting the relevant context from the reference paper
summarization we rst explain our proposed methods for ization we then describe our approach for identifying discourse facets of the citation contexts and nally we outline our summarization approach

citation contextualization citation contextualization refers to extracting the vant context from the reference article for a given tion text
we propose the following three approaches for this problem query reformulation word beddings and domain knowledge and supervised classication


query reformulation qr we cast the contextualization problem as an mation retrieval ir task
we rst extract textual spans from the reference article and index them ing an ir model
the textual spans are of ity of sentences
in order to capture longer contexts those consisting of multiple consecutive sentences we also index sentence n grams
that is we index each n consecutive sentences as a separate text span
after constructing the index we consider the citation text as the query and we seek to nd the relevant context from the indexed spans
since the citation texts are ten longer than usual queries in standard ir tasks we apply query reformulation methods on the citation to better retrieve the related context
we utilize both eral and domain specic query reformulations for this purpose
we rst remove the citation markers author names and year and numbered citations from the tations as they do not appear in the reference text and hence are not helpful
we design several regular pressions to capture these names
the proposed query reformulation qr methods are described below query reduction since the citation texts are usually more verbose than standard queries there might be many uninformative terms in them that do not tribute in nding the correct context
hence we apply query reduction methods to only retain the important concepts in the citation
after removing the stop words from the citation we further experiment with the lowing three query reduction methods
noun phrases qr np
citation texts are ally linguistically well formed as they are extracted
identifying the discourse facet of the extracted we indexed up to consecutive sentences in our text ments
scientic document summarization via citation contextualization and scientic discourse from scientic papers
this allows us to apply a riety of linguistic tagging and chunking methods to the query to capture the informative phrases
vious works have shown that noun phrases are good representation of informative concepts in the query
we thus extract noun phrases from the citation text and omit all other terms

key concepts qr kw
key concepts or keywords are single or multi word expressions that are mative in nding the relevant context
we use the inverted document frequency idf measure to nd the key concepts
the terms that are lent throughout all the text spans do not provide much information in retrieval
idf values help turing the terms and concepts that are more specic
for key concept extraction we limit the idf values between some threshold that can be tuned ing to the dataset
we consider phrases of up to three terms

ontology qr domain
domain specic ontologies are expert curated lexicons that contain specic concepts
in this reformulation method we use an ontology to only keep important specic concepts in the query
since the tac dataset is in the biomedical domain we use the umls thesaurus which is a comprehensive tology of biomedical concepts
we specically use the snomed ct subset of umls
as explained in section

the indexing approach also contains consecutive sentences
therefore our trieval approach can nd text spans that have laps with each other
furthermore retrieving multiple spans from around the same location in the text nals the importance of that specic location
we apply a reranking and merging method to the retrieved spans to remove shared spans and better rank the more evant context
we merge the two overlapping spans if the retrieval score of the larger span is higher than the smaller span
we also evaluated other query tion methods such as pseudo relevance feedback however they performed worse than the baseline and thus we do not discuss them further


contextualization using word embeddings and domain knowledge to explicitly account for terminology variations and paraphrasing between the citing and the cited authors we propose another model for citation contextualization utilizing word embeddings and domain specic edge
we empirically set this threshold to
and
for the tac and cl scisum datasets respectively
embeddings
word embeddings or distributed sentations of words are mapping of words to dense tors according to a distributional space with the goal that similar words will be located close to each other
we extend the language modeling lm for tion retrieval model by utilizing word embeddings to account for terminology variations
given a citation text query q and a reference span document d the lm scores based on the probability that has ated q
using standard simplifying assumptions of term independence and uniform document prior we have n where qi i


n are the terms in the query
in lm with dirichlet smoothing is calculated using a smoothed maximum likelihood estimate d wv w d where f is the frequency function shows the background probability of term qi in collection c v is the entire vocabulary and is the dirichlet parameter
our model extends the above formulation eq
by using word embeddings
in particular we estimate the probability according to the following equation djd dj dj d dj wv where dj are terms in the document d and s is a tion that captures the similarity between the terms and is dened as dj if
otherwise where shows the unit vector corresponding to the embedding of word qi is a threshold and is a formation function
below we explain the role of eter and the transformation function
word embeddings can capture the similarity values of words according to some distance function
most bedding methods represent the distance in the tional semantics space
therefore similarities between two words qi and can be captured using the dot uct of their corresponding embeddings i
e


while high values of this product suggest syntactic and semantic relatedness between the two terms arman cohan nazli goharian word word similarity marker notebook capture blue produce mint sky promotion sky make




table example of similarity values between terms ing to the dot product of their corresponding embeddings
using the pre trained model on google news pus
the top part of the table shows pairs of random words while the bottom part shows similarity values for pairs of related words
many unrelated words have non zero dot products an example is shown in table
therefore considering them in the retrieval model introduces noise and hurts the performance
we address this issue by rst ering a threshold below which all similarity values are squashed to zero
this ensures that only highly relevant terms contribute to the retrieval model
to identify an appropriate value for we select a random set of words from the embedding model and calculate the average and standard deviation of point wise absolute values of similarities between the pairs of terms from these ples
we then set to be two standard deviations larger than the average similarities to only consider very high similarity values
we also observe that for high ity values between the terms the values are not criminative enough between more or less related words
this is illustrated in figure where we can see that the most similar terms to the given term are not too discriminative
in other words the similarity values cline slowly as moving away from top similar words
we instead want only very top similar words to contribute to the retrieval score
therefore we transform the ilarity values according to a logit function equation to dampen the eect of less similar words see figure log while any approach for training the word dings could be used we use the method which has proven eective in several word ity tasks
we train on the recent dump of wikipedia
since the tac dataset is in biomedical domain we also train embeddings on a domain specic collection we use the trec genomics collections and which together consist of
billion kens

wikimedia
org fig
normalized similarity values between a random word in the embedding model and the top similar words to it
the axis is the word indexes and the y axis is the similarity values
the orange line with markers shows the original similarity values while the green line with triangle markers shows the transformed values using the logit function
the logit function dampens the similarity values of less similar words
incorporating domain knowledge word embedding models learn the relationship between terms by being trained on a large corpus
they are based on the butional hypothesis which states that similar words appear in similar contexts
while these models have been very successful in capturing semantic relatedness recent related works have shown that domain ontologies and expert curated lexicons may contain information that are not captured by embeddings hence we account for the domain knowledge according to the following
retrotting embeddings in this method we apply a post processing step called retrotting to the word embeddings used in the model
retrotting optimizes an objective function that is based on lationships between words in a lexicon it intuitively pulls closer the words that are related to each other and pushes farther the words that are not related to each other according to a given ontology
for the tology since tac data is in biomedical domain we use two domain specic ontologies and protein ontology pro
for the cl scisum data since it is less domain specic we use the wordnet lexicon
interpolating in the lm in this method instead of modifying the word vectors we incorporate the main knowledge directly in the retrieval model
we do so by interpolation of two following probability estimates medical subject headings
georgetown
edu





dot productnormalized logit scientic document summarization via citation contextualization and scientic discourse where is estimated using eq
and is a ilar model that counts in the is synonym relations is syn in calculating similarities
its formulation is exactly like eq
except it replaces the function s with the following function if if qi is syn o

this function is essentially partially counting the synonyms in calculation of the probability estimate by the amount of
we empirically set the value of
word embedding based methods are shown by we in short in the results


supervised classication the two previous context retrieval models are pervised and as such do not take advantage of the ready labeled data
cl scisum dataset includes rate training and testing sets which allow us to also vestigate supervised approaches
we propose a rich classier to nd the correct context for each given citation
our approach aims to capture the semantic latedness between a given citation text and a candidate context sentence
we specically utilize the following features to capture this relatedness word match counts the number of identical words between the source citation text and the candidate reference context normalized by length
fuzzy word match same as above with the ence that we use character n grams to capture tial matches between the words
embedding based alignment measures the ity between the source and target sentences using word embedding alignment
specically for the two sentences and the following function f scores the sentences based on their similarity v where s is a similarity function according to the equation
intuitively captures the similarity tween the two sentences without only relying on ical overlaps it takes into account the similarity ues between the terms
distance between average of embeddings measure the similarity between the two sentences by dot product of the average of their constituent word tors
feature name citation text extracted reference context verb features ralative section position table features for identifying discourse facets
similarity score between the citation text and the candidate reference
tf idf and count vectorized similarities dot product between the sparse tf idf weighted or count weighted vectors associated with the source citation and get reference context
character n gram tf idf and count vectorized larities same as above except that we used gram characters to allow partial word matches
we train a standard linear classier e

logistic regression using these features to identify the correct context for a given citation text

identifying discourse facets the organization of scientic papers usually follows a standardized discourse pattern where the authors rst describe the problem or motivation then they talk about their methods then the results and nally cussion and implications
our goal is to capture the important content from all sections of the paper fore after extracting the citation contexts we identify the associated discourse facet for each of the citation contexts retrieved from the previous step
each citation context refers to some specic discourse facets of the reference document
to identify the correct discourse facets we train a simple supervised model with tures listed in table
essentially we use the citation text and the extracted reference context represented by character n grams the verbs in the context sentence and the relative position of the retrieved context in the paper as features for the classier
while the textual features citation and it s context were the most ful we empirically observed slight improvements by corporating the verb and section position features
we train the model using an svm classier
for the textual features we transform them using character grams to allow fuzzy matching between the terms

generating the summary after extracting reference contexts for the citations as described in section
and identifying their discourse arman cohan nazli goharian facet section
we generate a summary of the erence paper
our goal is to create a summary that contains information from dierent discourse facets of the paper
this helps not only in diversifying the tent in the summary but also in creating a more ent summary
to generate a summary we rst identify the most representative sentences in each group
itively we only need a few top representative sentences from each discourse facet to include in the summary
in order to nd the most representative sentences we consider sentences in each facet as nodes and their ilarities as weighted edges in a graph
we then apply the power method which is an algorithm similar to the pagerank random walk ranking model that nds the most central nodes in a graph
it works by eratively updating the score of each sentence according to its centrality total weight of incoming edges and the centrality of its neighbors
after ranking the tences in each group according to their centrality score we select sentences for the nal summary
we use the following methods for creating the nal summary iterative
this method simply iterates over the course facets and selects the top representative tence from each group until the summary length threshold is met
greedy
the iterative approach could result in lar sentences ending up in the summary this results in redundant information and potential exclusion of other important aspects of the paper from the mary
to address this potential problem we use a heuristic that accounts for both the informativeness of candidate sentence and their novelty with respect to what is already included in the summary
imal marginal relevance is one such heuristic that has these properties
it is based on the linear interpolation of the informativeness and the novelty of the sentences
experiments
data we conducted our experiments on two scientic marization datasets
the rst dataset is the tac scientic summarization dataset
the tac benchmark is in biomedical domain and is publicly available upon request from nist
the second dataset is the cl scisumm dataset which is available on a lic and contains scientic articles from the
nist
national institute of standards and technology
com wing nus scisumm corpus characteristic tac cl scisum documents reference documents
avg
citing docs for each ref total citation texts avg
gold summary length words
stdev
gold summary length words
no separate train test sets


yes table characteristics of the datasets
number of avg average and stdev standard deviation
computational linguistics domain
to our knowledge these two are the only datasets on scientic rization
the tac dataset only has one training set sisting of topics
there is one reference article in each topic and another set of articles citing the erence
for each topic annotators have identied the relevant contexts the correct discourse facet and they have written a summary
the documents are vided as plain text les and there is no predened tence boundaries and sections
on the other hand the cl scisumm data contain separate train development and test sets with topics in total
similar to tac each topic consists of reference and a set of citing cles but in the computational linguistics domain
the articles are in xml format with known sentence aries and sections
another distinction is that topics in the cl scisumm data are annotated by one annotator at a time
the full statistics of the datasets is illustrated in table
the distribution of the discourse facets in the two datasets is also shown in figure
since the two datasets are in dierent domains the dierence between the distribution of the facets is expected

citation contextualization evaluation evaluation of the retrieved contexts is based on the overlap of the position of the retrieved contexts and the gold standard contexts
per tac evaluation of the tac benchmark was performed using character oset overlaps weighted by human annotators
more formally for a set of tem retrieved contexts s and gold standard context r



by m annotators the weighted character based precision pchar and recall rchar are dened as follows pchar i m rchar i i
nist
biomedsumm guidelines
html scientic document summarization via citation contextualization and scientic discourse a tac dataset cl scisum dataset fig
distribution of discourse facets in each dataset
method baselines vsm lmd lmd lda this work qr domain qr np qr kw wewiki webio it webio domain character oset overlap rouge pchar rchar fchar






















































table results of citation contextualization on tac dataset
the reported results are based on top retrieved contexts
the top part shows the baselines and the bottom part shows our proposed model
values are percentages
domain query reformulation by domain ontology umls qr np query reformulation by noun phrases qr kw query reformulation by key words wewiki word embedding model with wikipedia embeddings webio word embedding model with biomedical embeddings it incorporating domain knowledge in biomedical embeddings by retrotting webio domain interpolated language model
the ocial metric for the cl scisum challenge was sentence level overlaps of the retrieved contexts with the gold standard
this was possible because unlike the articles in tac which were in plaintext format the tence boundaries in cl scisum were pre specied
we also report character level metrics for the cl scisum corpus as we will see the character level and sentence level metrics are more or less comparable
one problem with position based evaluation metrics character or sentence is that a system might retrieve a context that is in a dierent position than gold dard but similar to the content of the gold standard
in such cases the system is not rewarded at all
this is possible because authors might talk about a similar concept in dierent sections of the paper
to consider textual similarities of the retrieved context with the gold standard we also compute rouge n scores
comparison to our knowledge no review paper about the tac challenge was released
hence for the tac dataset we compare our method against the following baselines vsm
ranking by vector space model vsm with tf idf weighting of the citations and the target erence contexts

scoring model which is a abilistic framework for ranking the relevant ments based on the query terms appearing in each document regardless of their relative proximity
lmd
language modeling with dirichlet smoothing lmd is a probabilistic framework that models the probability of documents generating the given query
lmd lda
an extension of the lmd retrieval model using latent dirichlet allocation lda which is recently proposed
this model ers latent topics in ranking the relevant documents for the cl scisum data we also compare against the top best performing system
for brief description about these approaches refer to section
results
the results on the tac dataset are presented in table
we observe that our proposed methods arman cohan nazli goharian method psent rsent fsent pchar rchar fchar sentence overlap rouge character oset overlap other methods vsm lm tsr tf idf neural net svm rank jaccard fusion tf this work qr np qr kw wewiki it supervised














































































table results of citation contextualization on cl scisum dataset
the reported values are percentages
the top part shows the baselines and state of the art models while the bottom part shows our methods
p precision r recall f score
sent subscript shows overlap by sentences and char subscript shows character oset overlaps
qr np query reformulation by noun phrases qr kw query reformulation by key words wewiki word embedding model with wikipedia embeddings it incorporating domain knowledge in embeddings by retrotting prove over all the baselines
query reformulation ods np and kw respectively obtain character oset scores of
and
which improve the best line by and
they also obtain higher rouge scores
this shows that noun phrases and key words can capture informative concepts in the citation that help better retrieving the related reference context
our models based on word embeddings are also ing the baselines in virtually all metrics
general main embeddings trained on wikipedia wewiki and domain specic embeddings trained on genomics data webio achieve scores of
and
with and improvement over the best baseline tively
higher performance of the biomedical dings in comparison with general embeddings is pected because the words are captured in their correct context
an example is shown in table where the top similar words to the word expression are shown
the word expression in the biomedical context is dened as the process by which genetic instructions are used to synthesize gene products
as we can see using general domain embeddings we might fail to capture this tion
incorporating domain knowledge in the model sults in further improvement as shown in last two rows of table
the model using retrotting it improves the best baseline by while the lated model achieves the highest provement by
these results show the eectiveness of domain knowledge in the model
general wiki interpretation sense emotion function show domain specic bio upregulation mrna protein induction cell table the words with highest similarity values to sion according to trained on wikipedia general domain and genomics collections biomedical domain
table shows the results for the cl scisum dataset
the rst rows are baselines that also are reported in tac evaluation in addition to those lines we also consider top performing state of the art systems of cl scisum lines as additional baselines to compare with
for the cl scisum ipating systems we report the ocial sentence based evaluation metrics the rouge scores and character based metrics were not reported in the ocial ation of the task
some of our methods are specic to the biomedical domain such as webio therefore we do not evaluate those on the cl scisum dataset which is in a completely dierent domain
as shown in table our methods outperform the state of the art on this dataset as well
the embedding based model with wikipedia trained scientic document summarization via citation contextualization and scientic discourse eect of the parameter on the interpolated alization model tuned on the tac dataset
eect of the parameter on the interpolated alization model tuned on the tac dataset
the eect cut o of point in returning the top results the wewiki model tuned on cl scisum dataset
for fig
parameters of the model for contextualization
feature character n gram tf idf similarity tf idf similarity embedding based alignment distance average embeddings similarity score character n gram count similarity fuzzy word match count based similarity word match weight








table the weights normalized corresponding to the top features in the supervised method for citation ization cl scisum dataset
tf idf similarity based features and embedding based features are the most helpful while the count based similarity and word matching features are among the least helpful features
beddings wewiki achieves the best results with
score of sentence overlaps which is slightly higher than the score of
achieved by the best ous work tf in the table
interestingly we observe that retrotting it does not improve over the standard embedding based approach
this is likely due to the choice of the wordnet lexicon for retrotting
while wordnet contains general domain terms it does not necessarily capture relationships of words in the context of computational linguistics
in contrast to tac where we had a domain specic icon suitable for the dataset for the cl scisum data we did not nd any lexicon capturing the term tionships in the computational linguistics domain
we believe that retrotting with such lexicon could result in further improvements
while query based approaches improve over most of the baselines their performance fall below the best baseline system
on the other hand our supervised method also proves the best baseline achieving the highest overall prevision
and
and number of citations number of annotators with at least partial agreement no agreement table the table shows the number of citations grouped by the number of annotators that agree at least partially on the context
scores

it is encouraging that our based models method names starting with we in the table which are unsupervised models achieve the best results on this task and surpass the mance of the feature rich supervised models
table shows the importance of each feature for our vised method explained in


while the most important features are n gram and character n gram based tf idf similarity embedding based alignment and distance of average embeddings are also important in nding the correct context
as evident from tables and the absolute tem performances are not high which further shows that this task is challenging
since the tac data are annotated by people we investigate the diculty of this task for the human annotators
to do so we culate the agreement of the annotators with respect to the relevant context for the citations
table shows the number of citations grouped by the number of tors that agree at least partially on the correct context
as illustrated there are citations out of that all annotators have partial agreement on the context span
this shows that the contextualization task is not trivial even for the human expert annotators
we do not report results of supervised model on tac dataset because the tac data do not have separate train and test sets














arman cohan nazli goharian method other methods smo decision tree fusion method jaccard cascade jaccard focused method this work qr np qr kw wewiki supervised p r f





























table results for identifying the discourse facets for the retrieved contexts
the metrics are precision p recall r and score f of the identied discourse facets contingent on the correct retrieved span
parameters our interpolated model of embeddings and domain knowledge has two main rameters and
figure shows the sensitivity of our model to dierent parameters
we observe that the best performance is achieved when
and

our models retrieve a ranked list of contexts for the tions we choose a cut o point for returning the nal results
figure shows the eect of the cut o point on one of our models
we observe that the optimal cut o point for best sentence score is
discourse facet p r f aim hypothesis implication method results














average total


table the classier s intrinsic performance for ing the discourse facets on the cl scisum dataset

identifying discourse facets evaluation the ocial metric for evaluation of course facet identication is the precision recall and scores of the discourse facets conditioned on the correctness of the retrieved reference context
therefore we report the results for the cl scisum data based on this metric
for the tac dataset the cial metric is the classication accuracy weighted by the annotator agreements
the accuracy for a tem returned discourse facet is the number of tors agreeing with that discourse facet divided by total number of annotators
results table shows the results of our methods pared with the top performing ocial submitted runs to the cl scisum
we do not report the results of low performing systems
the classication algorithm for identifying the discourse facets is the method scribed in section
across all our methods
however since only the correct retrieved contexts are rewarded the cut o point has similar eect on all the models

nist
biomedsumm the performance of each model diers based on the curacy of retrieving the correct contexts
we observe that most of our methods except for the qr np prove over all the baselines in terms of all metrics
we obtain substantial improvements especially in terms of precision
the best method for identifying the discourse facets is the supervised method indicated with vised in the table which obtains
score proving the best baseline jaccard focused method by
embedding methods also perform well by taining scores of
for the wikipedia dings and
for the retrotted embeddings
these results further show the eectiveness of our ization methods along with the proposed classier for identifying the facets
we also demonstrate the intrinsic performance of our classier for identifying the discourse facets in ble
as illustrated the weighed average mance over all discourse facets is

one challenge in identifying the discourse facets is the unbalanced dataset and the limited number of training examples for some specic facets
as also reected in the table we observe that for categories with smaller number of scientic document summarization via citation contextualization and scientic discourse svm rf lr oracle tac
cl scisum





table eect of learning algorithms in identifying the discourse facets
svm support vector machine with linear kernel rf random forest lr logistic regression cle highest achievable score
numbers are weighted accuracy scores by annotators
instances the performance is generally lower
we fore believe that having more training samples in the rare categories could further increase the performance
table shows the results of facet identication in the tac dataset as well as the eect of learning rithms
since for the tac dataset there are tors and the ocial metric is weighted accuracy scores we also calculate the oracle score by always predicting what the majority of the annotators agree on
the acle achieves
percent suggesting that identifying discourse facets is not trivial for humans
we can see that the svm classier achieves the highest results with relative accuracy to the oracle
for the cl scisum dataset there is only one annotator per discourse facet and therefore the weighted accuracy metrics translates to simple accuracy scores

summarization we evaluate our summarization approach against the gold standard summaries written by human annotators
we set the summary length threshold to the average length of summary by words in each dataset see table
table shows the results for the summarization task
the rst lines show the baselines which are ing summarization approaches including the sic algorithm and the original citation based marization approach
the next four lines are the top state of the art systems on the cl scisum dataset
for the cl scisum systems the ocial reported sults only included and rouge scores
as illustrated in the table virtually all our methods improve over the state of the art showing the ness of our proposed summarization approach
our best method qr np greedy is based on the noun phrases query reformulation using the greedy strategy of tence selection
it achieves score of
which improves over the best baseline by

in general we can see that the greedy sentence selection strategy works better than the iterative approach
this is because the greedy strategy takes into account both the informativeness and the redundancy of the selected sentences
table shows the results of summarization ing on the tac dataset
the reported approaches all use the greedy sentence selection strategy as it sistently outperforms the iterative approach
in eral while all our approaches outperform the baseline query reformulation based approaches achieve the est rouge scores query reformulation method using noun phrases qr np achieves
and
and scores respectively which is the est scores
the interpolated word embedding based model achieves the highest score

comparing tables and we notice that the scores for the tac dataset are lower than that of cl scisum
this is due to the length of the generated summaries
as shown in table the average human summary length in the tac data is almost words more than the cl scisum summaries
an interesting observation in these two tables is regarding the relative poor performance of the citation based summarization baseline clexrank that only uses citation texts in comparison with our methods that also take advantage of the citation context and the discourse structure of the articles
this observation further conrms our tial hypothesis that relying only on the citation texts could result in summaries that do not accurately ect the content of the original paper and that adding citation contexts can help produce better summaries
to better analyze the eect of identifying discourse facets on the overall quality of the summary we pare the rouge scores of the summary generated by our approach with and without this step
table shows the overall summarization results based on our qr np approach when we only use contextualized tations compared with when we use faceted tualized citations
we observe that grouping citation contexts by their corresponding discourse facet has a positive eect on the quality of the summary on both datasets and improvements over tac and cl scisum datasets in terms of tively
this is because identifying facets and grouping the contextualized citations by facets results in a mary that captures the content from all sections of the paper
we observe similar trends for other variants of our approaches for brevity we only show the results for qr np as an illustrative analysis on the eect of tifying discourse facets on the quality of the generated summary
finally an example of the generated summaries by our system qr np greedy that uses citation contexts and discourse facets is illustrated in figure
we serve that compared with the human summary the summary generated by our system can capture the nicant points of the paper
arman cohan nazli goharian rouge lexrank clexrank sumbasic summa lmkl lmeq cist qr kw iter qr kw greedy qr np iter qr np greedy wewiki iter wewiki greedy supervised iter supervised greedy lexrank clexrank sumbasic qr np qr domain qr kw wewiki webio it






































































rouge table summarization results on the cl scisum dataset
metrics are rouge f scores
the top part shows the baselines and the state of the art systems
bottom systems show our method variants based on dierent contextualization approaches and sentence selection strategy from the discourse facets
iter iterative and greedy refer to the sentence selection approach for the nal summary
table summarization results on the tac dataset
metrics are rouge f scores
the top part shows the baselines and the state of the art systems
bottom systems show our method variants based on dierent contextualization approaches and the greedy sentence selection strategy
r discussion tac qr np no facet tac qr np faceted cl scisum qr np no facet cl scisum qr np faceted











table the eect of discourse facets on the tion results on the tac and cl scisum dataset based on qr np approach by greedy sentence selection strategy on the identied facets
other approaches show similar positive trends
metrics are rouge f scores
citations are a signicant part of scientic papers and analysis of citation texts can provide valuable mation for various scholary applications
our work provides new approaches for contextualizing citations which is a sub task for enriching citation texts and thus can benet various bibliometric enhanced nlp cations such as information extraction information trieval article recommendation and article tion
our work provides a comprehensive new work for summarizing scientic papers that helps erating better scientic summaries
we note that our evaluation was based on the rouge automatic summarization evaluation scientic document summarization via citation contextualization and scientic discourse example summary human summary our system clexrank the limited coverage of lexical semantic resources is a signicant problem for nlp systems which can be alleviated by automatically classifying the unknown words
supersense tagging assigns unknown nouns one of broad semantic gories used by lexicographers to organise their manual insertion into wordnet
lexical semantic resources have been applied successful to a wide range of natural language processing nlp problems ranging from collocation extraction and class based smoothing to text classication and question answering
some specialist topics are better covered in wordnet than others
a considerable amount of research addresses structurally and statistically manipulating the hierarchy of wordnet and the construction of new wordnet using the concept structure from english
ciaramita and johnson implement a supersense tagger based on the multi class preceptor classier which uses the standard collocation spelling and syntactic features common in wsd and named entity recognition systems
the authors demonstrate the use of a very ecient shallow nlp pipeline to process a massive corpus
such a corpus is needed to acquire reliable contextual information for the often very rare nouns they are attempting to supersense tag
the limited coverage of lexical semantic resources is a signicant problem for nlp systems which can be alleviated by automatically classifying the unknown words
ciaramita and johnson present a tagger which uses synonym set glosses as annotated training examples
our approach uses voting across the known supersenses of automatically extracted synonyms to select a sense for the unknown nouns
the extracted synonyms are ltered before contributing to the vote with their
our development experiments are performed on the wordnet
test set with one nal run on the net

test set
in particular wordnet fellbaum has signicantly inuenced research in nlp
these results also support ciaramita and johnsons view that abstract concepts like communication cognition and state are much harder
lexicographers can not possibly keep pace with language evolution sense distinctions are continually made and merged words are coined or become obsolete and technical terms migrate into the vernacular
another related task is supersense tagging ciaramita and johnson curran ciaramita and altun
supersense tagging ciaramita and johnson curran evaluates a model s ability to cluster words by their semantics
in contrast some research have been focused on using predened sets of sense groupings for learning based classiers for wsd
although we could adapt our method for use with an automatically induced inventory our method which uses wordnet might also be combined with one that can automatically nd new senses from text and then relate these to wordnet synsets as ciaramita and johnson and curran do with unknown nouns

an additional potential is to integrate automatically acquired relationships with the information found in wordnet which seems to suer from several serious limitations curran and typically overlaps to a rather limited extent with the output of automatic acquisition methods
previous work on prediction at the supersense level ciaramita and johnson curran has focused on lexical acquisition nouns exclusively thus aiming at word type classication rather than tagging
fig
example summary generated by our system qr np greedy on one of the papers from the cl scisum dataset compared with a human written summary and the output generated by clexrank
work
automatic evaluation metrics have their own itations and can not fully characterize the eectiveness of the systems
manual or semi manual evaluation of summarization e

through pyramid framework are alternative evaluation approaches that can provide ditional insights into the performance of the systems
yet due to expense and reproduction issues most of the standard evaluation benchmarks including tac and cl scisum have been evaluated through rouge
as it is standard in the eld and to be able to compare our results with the related work we used the rouge framework for evaluation
we also note that our focus has been on the content quality of the summaries and other criteria such as coherence and linguistic cohesion have not been the focus of our approach
future work can investigate approaches for improving coherence and linguistic properties of the generated summaries
word embeddings and domain knowledge in our ods to capture the terminology variations between the citing and cited authors
we furthermore took tage of the scientic discourse structure of the articles
we demonstrated the eectiveness of our approach on two scientic summarization benchmarks each in a ferent domain
we improved over the state of the art by large margins in most of the tasks
while the results are encouraging the absolute values of some metrics pecially in the contextualization task suggest that this problem is worth further exploration
contextualizing citations is a new task and not only it helps improving scientic summarization but also it can benet other bibliometric enhanced end to end applications such as keyword extraction information retrieval and article recommendation
references conclusions we presented a unied framework for scientic marization our framework consists of three main parts nding the context for the citations in the reference per identifying the discourse facet of each citation text and generating the summary from the faceted tion contexts
we utilized query reformulation methods
abu jbara a
ezra j
radev d
r
purpose and in larity of citation towards nlp based bibliometrics
naacl hlt pp

abu jbara a
radev d
coherent citation based marization of scientic papers
in proceedings of the annual meeting of the association for putational linguistics human language volume pp

association for computational linguistics arman cohan nazli goharian
abu jbara a
radev d
reference scope identication in naacl hlt pp

acl in citing sentences

atanassova i
bertin m
v
bawden d
on the composition of scientic abstracts
journal of mentation
bendersky m
croft w
b
discovering key concepts in verbose queries
in proceedings of the annual ternational acm sigir conference on research and velopment in information retrieval pp

acm
bengio y
courville a
vincent p
representation learning a review and new perspectives
ieee tions on pattern analysis and machine intelligence
bengio y
ducharme r
vincent p
janvin c
a ral probabilistic language model
the journal of machine learning research
berg kirkpatrick t
gillick d
klein d
jointly ing to extract and compress
in proceedings of the annual meeting of the association for computational linguistics human language technologies volume pp

association for computational linguistics
bertin m
atanassova i
gingras y
v
the invariant distribution of references in scientic ticles
journal of the association for information science and technology
doi
asi

url
wiley

asi

bodenreider o
the unied medical language system umls integrating biomedical terminology
nucleic acids research
bornmann l
mutz r
growth rates of modern science a bibliometric analysis based on the number of tions and cited references
journal of the association for information science and technology
cao g
nie j
y
gao j
robertson s
selecting good expansion terms for pseudo relevance feedback
in ceedings of the annual international acm sigir conference on research and development in information retrieval pp

acm
cao z
li w
wu d
polyu at cl scisumm
in birndl joint workshop on bibliometric enhanced information retrieval and nlp for digital libraries
carbonell j
goldstein j
the use of mmr based reranking for reordering documents and producing summaries
in sigir pp

acm
celikyilmaz a
hakkani tur d
a hybrid cal model for multi document summarization
in acl pp

association for computational linguistics
chakraborty t
krishna a
singh m
ganguly n
goyal p
mukherjee a
ferosa a faceted dation system for scientic articles
in pacic asia ference on knowledge discovery and data mining pp

springer
chakraborty t
narayanam r
all ngers are not in equal intensity of references in scientic articles
proceedings of the conference on empirical ods in natural language processing pp

sociation for computational linguistics austin texas
url
org anthology
chali y
hasan s
a
query focused multi document summarization automatic data annotations and vised learning approaches
nat
lang
eng

doi

url



chopra s
auli m
rush a
m
abstractive tence summarization with attentive recurrent neural in proceedings of the conference of the works
north american chapter of the association for tational linguistics human language technologies pp

association for computational linguistics san diego california
url
aclweb
anthology
clarke j
lapata m
global inference for sentence compression an integer linear programming approach
j
artif
int
res

url http
acm
org citation


cohan a
goharian n
scientic article tion using citation context and article s discourse ture
in proceedings of the conference on empirical methods in natural language processing pp

association for computational linguistics lisbon tugal
url
org anthology
cohan a
goharian n
contextualizing citations for scientic summarization using word embeddings and main knowledge
in proceedings of the tional acm sigir conference on research and opment in information retrieval sigir
doi


url
acm



cohan a
soldaini l
goharian n
matching citation text and cited spans in biomedical literature a oriented approach
in proceedings of the hlt pp

association for computational guistics
url
org
conroy j
m
davis s
t
vector space and language models for scientic document summarization
in ceedings of naacl hlt pp

conroy j
m
schlesinger j
d
kubina j
rankel p
a
oleary d
p
classy at tac guided and lingual summaries and evaluation metrics
in ings of the text analysis conference
de waard a
maat h
p
epistemic modality and knowledge attribution in scientic discourse a omy of types and overview of features
in proceedings of the workshop on detecting structure in scholarly course pp

association for computational guistics
durrett g
berg kirkpatrick t
klein d
based single document summarization with compression in proceedings of the and anaphoricity constraints
annual meeting of the association for tional linguistics volume long papers
association for computational linguistics berlin germany
elkiss a
shen s
fader a
erkan g
states d
radev d
blind men and elephants what do citation summaries tell us about a research article journal of the american society for information science and nology
erkan g
radev d
r
lexrank graph based lexical centrality as salience in text summarization
j
artif
intell
res
jair
erkan g
radev d
r
lexrank graph based lexical centrality as salience in text summarization
journal of articial intelligence research
faruqui m
dodge j
jauhar k
s
dyer c
hovy e
smith a
n
retrotting word vectors to in naacl hlt pp

tic lexicons
scientic document summarization via citation contextualization and scientic discourse ation for computational linguistics
url http
org anthology
garzone m
mercer r
e
towards an automated tion classier
in conference of the canadian society for computational studies of intelligence pp

springer
gong y
liu x
generic text summarization using evance measure and latent semantic analysis
in ceedings of the annual international acm sigir conference on research and development in information retrieval pp

acm
guo s
sanner s
probabilistic latent maximal marginal relevance
in sigir pp

acm
harris z
s
distributional structure
word
hernandez alvarez m
gomez j
m
survey about tion context analysis tasks techniques and resources
natural language engineering
hersh w
voorhees e
trec genomics special issue overview
information retrieval
doi

hill f
reichart r
korhonen a
uating semantic models with genuine similarity tion
comput
linguist

doi

url



hulth a
improved automatic keyword extraction given more linguistic knowledge
in proceedings of the conference on empirical methods in natural language processing pp

association for computational linguistics
huston s
croft w
b
evaluating verbose query cessing techniques
in proceedings of the tional acm sigir conference on research and ment in information retrieval pp

acm
jaidka k
chandrasekaran m
k
rustagi s
kan m
y
overview of the computational linguistics entic document summarization shared task cl scisumm in proceedings of the joint workshop on
bibliometric enhanced information retrieval and ral language processing for digital libraries birndl
jha r
coke r
radev d
surveyor a system for generating coherent survey articles for scientic topics
ann arbor
jian f
huang j
x
zhao j
he t
hu p
a simple enhancement for ad hoc information retrieval via topic modelling
in sigir pp

acm
jones k
s
walker s
robertson s
e
a tic model of information retrieval development and parative experiments part
information processing management
jurgens d
kumar s
hoover r
mcfarland d
rafsky d
citation classication for behavioral analysis of a scientic eld
corr
kataria s
mitra p
bhatia s
utilizing context in generative bayesian models for linked corpus
in aaai vol
p

klamp s
rexha a
kern r
identifying referenced text in scientic publications by summarisation and sication techniques
in jcdl pp

le q
mikolov t
distributed representations of tences and documents
in icml pp

li l
mao l
zhang y
chi j
huang t
cong x
peng h
cist system for cl scisumm shared task
in birndl joint workshop on enhanced information retrieval and nlp for digital braries
lin c
y
rouge a package for automatic evaluation in text summarization branches out of summaries
proceedings of the workshop pp

lin j
madnani n
dorr b
j
putting the user in the loop interactive maximal marginal relevance for focused summarization
in naacl hlt pp

association for computational linguistics
lipscomb c
e
medical subject headings mesh
letin of the medical library association
mihalcea r
tarau p
textrank bringing order into texts
association for computational linguistics
mikolov t
sutskever i
chen k
corrado g
s
dean j
distributed representations of words and phrases and their compositionality
in nips pp

miller g
a
wordnet a lexical database for english
communications of the acm
moraes l
baki s
verma r
lee d
university of houston at cl scisumm svms with tree kernels and sentence similarity
in jcdl pp

mrksic n
seaghdha d
o
thomson b
gasic m
rojas barahona l
su p
h
vandyke d
wen t
h
young s
word vectors to linguistic straints
in naacl hlt
nakov p
i
schwartz a
s
hearst m
citances tion sentences for semantic analysis of bioscience text
in proceedings of the workshop on search and discovery in bioinformatics pp

nomoto t
neal a neurally enhanced approach to ing citation and reference
in birndl joint shop on bibliometric enhanced information retrieval and nlp for digital libraries
osborne m
using maximum entropy for sentence traction
in proceedings of the workshop on automatic summarization volume pp

tion for computational linguistics
page l
brin s
motwani r
winograd t
the ank citation ranking bringing order to the web

paul m
zhai c
girju r
summarizing contrastive viewpoints in opinionated text
in emnlp pp

association for computational linguistics
url
org anthology
pennington j
socher r
manning c
d
glove global vectors for word representation
emnlp
ponte j
m
croft w
b
a language modeling approach to information retrieval
in proceedings of the nual international acm sigir conference on research and development in information retrieval pp

acm
qazvinian v
radev d
mohammad s
generating extractive summaries of scientic paradigms
j
artif
intell
res




qazvinian v
radev d
r
scientic paper tion using citation summary networks
in proceedings of the international conference on computational linguistics volume pp

association for putational linguistics
qazvinian v
radev d
r
identifying non explicit ing sentences for citation based summarization
in ceedings of the annual meeting of the association for computational linguistics pp

association for computational linguistics arman cohan nazli goharian
qazvinian v
radev d
r
mohammad s
m
dorr b
zajic d
whidby m
moon t
generating tractive summaries of scientic paradigms
j
artif
int
res

url
acm
citation


robertson s
zaragoza h
the probabilistic relevance framework and beyond
now publishers inc
rush a
m
chopra s
weston j
a neural attention model for abstractive sentence summarization
in ceedings of the conference on empirical methods in natural language processing pp

association for computational linguistics lisbon portugal
url
org anthology
saggion h
aburaed a
ronzano f
trainable citation enhanced summarization of scientic articles
in cabanac g chandrasekaran mk frommholz i jaidka k kan m mayr p wolfram d editors
proceedings of the joint workshop on bibliometric enhanced tion retrieval and natural language processing for ital libraries birndl june newark united states
ceur workshop
p

ceur workshop proceedings
snomed c
systematized nomenclature of international health terminology clinical terms
dards development organisation
sparck jones k
a statistical interpretation of term specicity and its application in retrieval
journal of umentation
steinberger j
jezek k
using latent semantic analysis in text summarization and summary evaluation
in proc
pp

sutskever i
vinyals o
le q
v
sequence to sequence in advances in neural learning with neural networks
information processing systems pp

teufel s
moens m
summarizing scientic cles experiments with relevance and rhetorical status
comput
linguist

doi

url



teufel s
siddharthan a
tidhar d
automatic sication of citation function
emnlp p

vanderwende l
suzuki h
brockett c
nenkova a
beyond sumbasic task focused summarization with tence simplication and lexical expansion
information processing management
wang s
manning c
d
baselines and bigrams ple good sentiment and topic classication
in ings of the annual meeting of the association for computational linguistics short papers volume pp

association for computational linguistics
zhai c
laerty j
a study of smoothing methods for language models applied to information retrieval
acm transactions on information systems tois
