query based abstractive summarization using neural networks johan hasselqvist niklas helmertz
mikael kgebck
department of computer science and engineering chalmers university of technology abstract
in this paper we present a model for erating summaries of text documents with respect to a query
this is known as based summarization
we adapt an isting dataset of news article summaries for the task and train a pointer generator model using this dataset

the ated summaries are evaluated by ing similarity to reference summaries
our results show that a neural network marization model similar to existing ral network models for abstractive rization can be constructed to make use of queries to produce targeted summaries
introduction
creating short summaries of documents with spect to a query has applications in for example search engines where it may help inform users of the most relevant results
however ing such a summary automatically is a difcult
in this paper a problem yet to be fully solved
neural network model for this task is presented

more specically the model is designed for brief commonly single sentence summaries
a tion where this may be useful is when a user has performed a search in a search engine and a set of documents have been returned
concise maries could then be displayed along with the search results giving a quick overview of how the document is related to the search query
what is commonly done in search engines today is that text surrounding an occurrence of a search query in the document is displayed as a summary
this is an example of extractive summarization which produces a summary that only contains parts of the original document
a signicant difference in the model we present is that it generates an abstractive summary
this type of summary lows for rephrasing and using words not ily present in the original document comparable to a human written summary
this has the tial of summarizing documents in a more concise way than what is possible with an extractive mary making it easier for a reader to stand the relationship between a document and a query
automatic text summarization has been a search topic for many years
in general the goal is to concisely represent the most important formation in documents
much previous work in summarization has been using extractive methods
nenkova and mckeown mogren et al
commonly individual sentences are tracted and composed together to form a summary
this gives sentences that are as grammatically rect as the source document
they are however inherently limited and can not reproduce written summaries in general
abstractive marization in particular is closely related to ural language generation and it would be able to reach human level performance in
it may however require ing summaries
level understanding of the context of documents to produce results comparable to human written ones
an important progress in using neural work models for generating text is sequence sequence used by sutskever et al
for machine translation

it is a way of mapping a varying length input text to a varying length put text and it is applicable to machine tion as well as summarization

in recent years progress has been made on using neural network models for text summarization and similar lems
some examples are sequence to sequence models for non query based abstractive rization by rush al
and
nallapati et al


neural network models have additionally been used for generating image captions thy and fei fei which is a form of c e d l c
s
c
v
v
i x r a mary and for question answering problems such as by hermann et al
and tan et al


inspired by this progress we designed a model for query based summarization using neural works
the main contributions of this work includes a model for query based abstractive rization presented in section
a dataset for query based abstractive summarization ated by adapting an existing dataset originally used for question answering described further in section
a quantitative evaluation of the formance of the proposed model compared with an extractive baseline and an uninformed abstractive model presented in section
a qualitative analysis of the generated summaries
related work an early work evaluating several methods for tractive query based summarization is presented by goldstein et al

besides full queries they use short queries which on average are words
these are similar in length to the types of queries used in the experiments of this sis work
besides the work by otterbacher et al
recent work in query based summarization has been done by wang et al
using parse trees and sentence compression
it is described as not pure extractive summarization
during the later stages of this thesis work nema et al

propose a neural network model for query based abstractive summarization which has some ilarities to the model we present
however the dataset they use is smaller in both average ument length and number of documents

tionally the types of queries used are different in that they use complete questions as opposed to our single entity queries
the task of question answering is to produce an answer to a question posed in natural language

the task is very general and many other problems can be expressed as a question answering lem
summarizing with respect to a query may for instance be expressed as what is a summary of the document with respect to the query x for the query
if the answer to a question is a gle complete sentence then it is especially close to the types of query based summaries considered in this thesis
otterbacher et al

present a model biased lexrank which they use for a form of question answering as well as extractive query based summarization
the answers they generate are full sentences which makes it similar to our task of query based summarization

mann et al
present neural network els for question answering
for training these they create a large dataset from cnn daily mail news articles
we adapt this dataset for based summarization as detailed in chapter

kumar et al
introduce dynamic memory networks which they show reached state of art performance in a variety of nlp tasks
we draw inspiration from their use of a question
ule when we incorporate query information in our model
general abstractive summarization differs from query based summarization in that a document is summarized without respect to a query

pati et al
build upon a machine tion model by bahdanau et al
and
erate general abstractive summaries on multiple datasets including the cnn daily mail dataset by hermann et al

additions they make for their model include a pointer generator anism glehre et al that allows the model to copy words from the source document
see et al
propose a similar model using a lar pointer generator mechanism that outperforms nallapati al
on a slightly different sion of the cnn daily mail dataset making the result not strictly comparable
they also porate what they call coverage for avoiding titions in the output
background
in the following sections various terms and cepts used throughout the paper are explained
named entity recognition
information extraction is a class of tasks that
volve extracting structured information from uments
an example of such a task is named entity recognition which is the classication of parts of text into different categories such as persons or locations or no category
an example from the sentence the mathematician jeff paris visited the city of paris is that jeff paris should be tated as a person and the last paris as a location
gated recurrent units
the gated recurrent unit gru is a type of current neural network rnn that is designed to alleviate the vanishing exploding gradient lem
hochreiter bengio et al which hinders the original rnn from capturing long term dependencies
gru is similar to the lar long short term memory lstm model but is simpler and less computationally intensive while still achieving comparable results on many tasks chung al kumar et al
the tire gru architecture can be described by the mulas rt br zt bz t rt bh
ht zt
zt
t the vectors xt is the input at time step t and ht is the output while rt and zt are scaling vectors tended to regulate what information is let through

these can be described as gates
they have
ments in
the vector t is rather intended to carry data
its elements are in generated from a network with a tanh activation function

we denote an entire gru update step as ht xt
word embeddings given a vocabulary v we can encode each word uniquely using a one hot encoding
this gives a vector of length where every word in the cabulary is mapped uniquely to some dimension which a value of while the other dimensions are
this vector can be transformed to an ding for the word by multiplying it by an ding matrix wemb of dimensionality demb where demb is the word embedding ity commonly a hyperparameter in neural network models
the intention is that the embeddings ture some characteristics of words giving useful vector representations
for instance two related words such as football and soccer may be
pected to be close to each other in the vector space

two methods for generating word embeddings are mikolov et al and glove et al
attention
for many problems it has been found to be cial to use more of the rnn states than the nal xed size hidden state
attention is a mechanism for allowing the model to access more information in the decoding process by letting it identify vant parts of the input and use the encoder hidden state at these locations
this technique has been used successfully for machine translation danau et al and image captioning xu al
model
we propose a sequence to sequence model with attention and a pointer mechanism making it a pointer generator model
the input for the problem is a document and a query
these are sequences of words passed to a document coder and a query encoder respectively
the coders outputs are then passed to the attentive decoder which generates a summary
both coders as well as the decoder use rnns with grus
each occurrence of gru with a subscript in the formulas in the following sections has arate weights and biases
the entire model is picted in figure
the different components and variables in the gure will be explained in detail throughout the section
document encoder
the document encoder processes an input ment generating a state for each input word
to get a representation of the context around a word we use a bidirectional rnn schuster and paliwal encoder so both the context before and
ter contribute to the representation
this is used by bahdanau et al
amongst others
ing good results on a similar task related to text comprehension
h i and the combined rnn hidden state at time step i hi and the intermediate states h
i from the forward reader and backward reader tively are computed as
h i gru doc
h gru doc
h i h


h e wi
h
i hi
where wi v for the vocabulary v is word i in wi is word i in the reversed
the input document input and is the word embedding of wi


the initial states are zero vectors
due to the concatenation the combined state hi has twice the dimensionality of the state of each directional encoder
the document encoder state and figure overview of our model
it illustrates connections between parts of the model at a xed decoder time step the bottom part containing labeled boxes correspond to the different rnns
the top part is intended to visualize the two ways the output word yt can be selected through the pointer and generator mechanism to the left and right respectively
dimensionality is denoted ddoc and the word bedding dimensionality demb
query encoder
the query encoder is responsible for creating a xed size internal representation of the input query
unlike the document encoder the query encoder is a unidirectional rnn encoder since queries are relatively short compared to
ments and we only use the nal state to resent the whole query
the rnn state hq
i at is updated according to hq
i query word i
q hq where wq is nq the input query and nq is the length of the query

the initial state hq is the zero vector
the query encoder state dimensionality is denoted dque
decoder
the decoder is a unidirectional rnn for ing a summary of the input document by
ing on the nal state of the input encoder the
it utilizes soft attention in combination query

with a pointer mechanism as well as a generator part similar to bahdanau et al

the query embedding q is fed as input at each decoder time step
this is similar to the answering module in a question answering model presented by kumar et al
who use an rnn encoded question representation as input at each decoder time step

in our model the rnn state is updated ing to st ct q where hnd the nal document encoder state nd being the number of input words corresponds to a special go token used at the initial time step when no previous word has been predicted ct is the context vector at time step t from the attention mechanism dened subsequently and v is the predicted output word at time step t
this is either from the generator nism or the pointer mechanism also dened sequently
the word embeddings are the same as are used in the encoder
the intention of the inclusion of q to the input of grudec is to give the decoder the ability to tune the structure of the output sequence to eventually output something concerning the query
for

ample if the query is a location the decoder can output words leading up to an appropriate sion of the location

the generator outputs a word from a subset of the vocabulary vgen v at each time step
the selection of the output words is done through a distribution of words in vgen computed through a softmax as pgen for j an index uniquely mapped to a word w vgen and ztj as dened subsequently
dening this tj as the probability p gen word ygen arg max wvgen t with the highest probability by ygen we then select output t w
the softmax probability p gen t t gen
st gen
and
pends on ztj the output from two linear mations on the decoder state and context vector gen

dened as zt w
gen w gen where w gen
gen rdgen w
gen are trainable hyperparameters in which dgen is the
ality of the hidden layer
the main function of this layer is to reduce the dimensionality of the input for reducing computation time for the nal layer with size

the model has a soft attention mechanism based on one used by bahdanau et al
for machine translation
the result of the attention mechanism is a context vector ct produced at each time step t computed as ct tihi i ti
eti q where hi is the document encoder hidden state at index
the score function is dened as
s q v att s batt where watt is a weight matrix
vatt rdatt is a vector and batt is a bias vector all of which are trained together with the rest of the network
the query q is included for the model to focus attention around query words when appropriate
pointer mechanism
a general issue is that with a generator mechanism limited to frequent words infrequent words not be generated
further if the model needs to learn to output names and there are many different ones and few occurrences of each in the training data training a model to generate them correctly is problematic
a way to solve these issues is to allow the model to directly copy a word in the put document to the output summary or point to it

this may additionally be viewed as using the input text as a secondary output vocabulary in addition to vgen
the pointer mechanism adds a switch pptr
at each decoder time step t to the model

it is computed as the output of a linear t mation fed through a sigmoid activation function as pptr bptr where vptr and bptr are vectors all of which are trained together with the rest of the work
if pptr t a word is copied from the input otherwise the generator output is used
what is copied from the input for the tth decoder word is determined by the attention distribution
cally at time step t we select the word at index
t arg max ti in the document where the tention is highest as yptr word can then be dened as t
the nal output t
i yt yptr t ygen t t if pptr otherwise training loss t log pptr the model is trained in when to use the pointer mechanism in a supervised manner
we ne an additional training input xptr
that is t ther if the pointer mechanism is set to be used for the tth word in the summary or
otherwise
for training this we dene a loss function lptr
t t pptr xptr t

for training the generator mechanism we ne a loss over the generator softmax layer as lgen w where ns is the length of the target summary
w vgen is the the tth word in the target summary

plying by xptr t excludes any addition to the loss when the pointer mechanism is set to be used

we introduce a form of supervised attention for when the pointer mechanism is set to be used for an output word by introducing a loss function
latt t log ti where i is the dex in the input document to point to
t log p gen xptr xptr t
the nal loss function is the sum of the different losses normalized by the length computed as l ns lgen latt lptr
generating summaries summaries are considered complete when a cial eos token has been generated or after a maximum output length is reached
potential summaries are explored using beam search

ever for time steps where the pointer mechanism is used the partial summaries are prioritized by probabilities as if the generator had been used stead so k partial summaries with different
abilities are created for the word chosen by the table highlights of a cnn article titled airline quality report sorts out the duds from the dynamos in

hawaiian airlines again lands at no in on time performance
the airline quality rankings report looks at the largest airlines
expressjet and american airlines had the worst on time performance
virgin america had the best baggage handling southwest had lowest complaint rate table statistics of the dataset

val training
doc doc query pairs doc query sum avg
words doc avg words query avg words sum test pointer mechanism
this is difcult to justify but we hope that this should give a reasonable bility at time steps when the pointer mechanism is used preventing summaries using the pointer mechanism more to be prioritized
a slight deviation from what is presented in section is that when the pointer mechanism is used and the attended word was not in v we do not output unk which it is otherwise preted as in the model but rather
the actual put word before it being converted to an index in the vocabulary
this may be viewed as a processing step
dataset
the dataset constructed for this paper is based hermann et al
and consist of

ment query answer triples from cnn and daily mail news articles
included with each published news article there are a number of human written highlights which summarize different aspects of the article
table shows some example lights for a single article
they construct a ument query answer by considering a named tity in a highlight to be unknown making the light into a cloze style question taylor whose answer is the entity made unknown
an example document and a cloze style question and its answer can be seen in table
we propose using the cnn daily mail dataset for query based abstractive summarization by regarding each light as a summary of its document and entities in the highlight as queries
for every occurrence of an entity in a highlight we construct a query summary triple for query based tion
table shows for a sample document a cloze style question compared and the sponding query summary pair constructed by us

if an entity is mentioned in multiple highlights we consider there being multiple target references for the document query pair
in contrast to hermann et al
we do not translate entities into tiers but use only minimal preprocessing in the form of tokenization and lowercasing
further we mix articles from dnn and daily mail while
mann et al
keeps them separate
we cided to train our model on a mix of cnn and daily mail articles with a proportion of them
ing reserved for validation and test sets
which articles are included for the validation and test set is determined randomly with equal probability for every article

some statistics of the resulting dataset can be seen in table
the dataset can be reproduced using a script made available on
experiments two experiments were conducted
the rst to measure if the model uses the information in the query section and the second compares the model to an extractive baseline section
a beam width of k and a maximum output length of was used
query dependence to determine whether incorporating a query
ets our model we compare our proposed model to one where the query is corrupted

instead of evaluating the generated summary for a document and a query with id n against
the reference
maries for that query we evaluate it against the reference summaries for query the query id has been offset
for the query with the highest id the reference summaries for the rst query are used
the idea is that if the score is lower than for the normal evaluation then the model has made use of the additional information in the query

querysum data table
reference summaries used during normal evaluation compared to with offset queries

query id normal


offset queries

ble shows for an example document what the generated summaries are evaluated against during the query dependence evaluation

it is worth to mention that two reference summaries for ent queries may be the same as the same original highlight may be used as a reference summary for multiple queries
in these cases the query will be appropriate for the summary and the model may have beneted from the query even in the offset evaluation
extractive baseline
as a baseline we compare the results to a simple extractive summary designed specically for the dataset used in this thesis work
the baseline mary is constructed by selecting the rst sentence in the document containing the query without
if no such stricting the length of the document
sentence is found the document does not tain the query the rst sentence of the document is used instead
this does occur in the dataset but not frequently
we additionally observe that the average length of baseline sentences using the cnn daily mail dataset is commonly greater than for the reference summaries
the average number of words is for the baseline summaries while it is for the reference summaries

it may be possible to gain a higher rouge score if a fewer number of words around the query occurrence is selected but it might not form a complete sentence
evaluation metric
our results are evaluated using four different rics provided by rouge recall oriented

derstudy for gisting evaluation
lin the defacto standard evaluation method for automatic summarization l and rouge
and are the scores for grams and grams tively
rouge l and rouge are more complex metrics detailed by lin
training details the vocabulary v used for the input text contains the most frequent words in the training set while the generator vocabulary vgen consist of the most frequent words
the smaller lary of the generator is due to the pointer nism
word embeddings for the vocabulary words are initialized with dimensional glove trained on wikipedia gigaword

if the word does not have a glove ding we initialize the word embedding by pling the per dimension univariate normal butions with means and standard deviations of the entire collection of glove embeddings
both during training and test time we limit the document length to the rst words to reduce computation time
the loss l is minimized using the sgd based adam optimizer kingma and ba
we used mini batches of samples with an averaged loss over all the samples in the batch
the mini batches remained the same over epochs but the order in which they were trained on was randomized tween every epoch
experiments have been run on a single nvidia tesla with gb of memory and took about hours to train
the model is implemented using
tensorflow
abadi et al and the complete source has been made available
the hyperparameters used for the experiments is reported in table
no extensive rameter tuning has been performed but instead examined hyperparameters used for similar els such as nallapati et al
and see et al


table hyperparameter conguration used

hyperparameter
word embedding size document encoder size
query encoder size
decoder size attention hidden size generator hidden size value demb ddoc dque ddec datt dgen as at
table example document query pair

query netix table rouge scores of the evaluated models

model first query sentence
our model
offset queries

l document cnn the united states have named former germany captain jurgen klinsmann as their new national coach just a day after sacking bob bradley
bradley who took over as coach in january was relieved of his duties on thursday and soccer federation president sunil gulati conrmed in a statement on friday that his replacement has already been appointed
query united states reference jurgen klinsmann is named as coach of the united states national side output klinsmann appointed as the new coach of united states results the results from our experiments are summarised in table
from the result of the query dence evaluation offset queries described in section we can see that the rouge scores goes down with statistical signicance according to the rouge reported condence intervals when the queries are offset
this indicates that the model benets from the information provided by queries

further we observe that our model score lower than the baseline model which we denote the rst query sentence described in section
however it should be noted that this baseline is expected to be strong given the nature of this dataset
further analysis
we observe that the attention at a time step appears to often be highly focused on only a few words in the document
an example of an output summary can be seen in table and figure shows the attention distribution over time for the same erated summary
another observation we make is that the attention often is focused at the beginning of the documents
however there are certainly stances when entities are selected from far back in documents
this bias may partly be due to our decision to point out the rst occurrences of tities
although it has been noted by goldstein et al
that the beginning of news articles
table example document query pair

document president barack obama sided with internet activists on monday urging the federal nications commission to draft new rules that would
classify the broadband net to regulate it more like a
lic utility
the end result would tie the hands of internet service providers that want to cut special deals with vices like netix youtube hulu and amazon to push their streaming content along a fast lane that ordinary icans ca nt access
reference obama s vision would bar providers like izon and comcast from cutting deals with hulu netix and amazon so their streaming content could be delivered along online fast lanes output obama s chief executive of netix has refused to allow users to access the service table example document query pair

document february a breakthrough in belarus a verdict in italy and an expected veto in the
all headline cnn student news this friday
query cnn student news roll call reference at the bottom of the page comment for a chance to be mentioned on cnn student news
you must be a teacher or a student age or older to request a tion on the cnn student news roll call
output at the bottom of the page comment for a chance to be mentioned on cnn student news
you must be a teacher or a student age or older to ten summarizes the article quite well
from examining some of the output summaries from our model we see that they often strongly match the topic of the input documents but they rarely succeed in generating summaries ing something actually stated in the article
table shows an example output that is fairly ically correct but not truthful with respect to the article
we observe that the model manages to learn some of the dataset samples which are not actual summaries described in section such as notices repeated over several articles
the generated mary shown in table is an example of this

terestingly the model manages to literally repeat the reference summary up to the maximum output length limit
we can frequently see repetitions of the same phrases an extreme example can be seen in figure
the model appears to get stuck ing to begin a summary
additionally we observe that the repetition can be observed in the attention distribution as well
the same problem has been seen by nallapati et al
who make an tion temporal attention sankaran et al to their model for alleviating the issue of repetitions

see et al
propose using coverage to solve the same issue

before running experiments we suspected that it may be difcult for the pointer mechanism to sequentially point out words that make up longer entities
however we see that this is done fully quite often
for an example summary the certainty of selecting a sequence of entity words can be seen in figure

compared to the reference summaries the put is generally shorter
the average number of words in output summaries is while the dataset average is
as is noted by wu et al

beam search commonly favors shorter summaries
they propose an addition of length normalization for reducing this tendency
menting such a measure may improve the results of our model as well

in comparison to nallapati et al
and see et al
our rouge scores are low
they use a different version of the dataset where all lights are combined to form a single often sentence summary
with similar models they get results of around on the general summarization task
however while they always train the model to output the same summary for the same document we often have completely ent target summaries for different queries where the queries make up a much smaller part of the put
conclusion
we have designed a model for query based stractive summarization and evaluated it on an adapted qa dataset redesigned for query based summarization
while the overall performance of the model is not enough to outperform our tive baseline we have shown that it can rate a query and utilize the information to create more focused summaries
references martn abadi ashish agarwal paul barham eugene brevdo zhifeng chen craig citro greg corrado andy davis jeffrey dean matthieu devin sanjay ghemawat ian goodfellow andrew harp geoffrey
irving michael isard yangqing jia rafal

icz lukasz kaiser manjunath kudlur josh
enberg dan man rajat monga sherry moore derek murray chris olah mike schuster jonathon shlens benoit steiner ilya sutskever kunal war paul tucker vincent vanhoucke vijay van fernanda vigas oriol vinyals pete warden martin wattenberg martin wicke yuan yu and aoqiang zheng

tensorflow
large scale chine learning on heterogeneous systems
software available from
dzmitry bahdanau kyunghyun cho and yoshua gio

neural machine translation by jointly
international learning to align and translate
ference on learning representations iclr

yoshua bengio patrice simard and paolo frasconi


learning long term dependencies with ent descent is difcult
ieee transactions on neural networks
junyoung chung aglar glehre kyunghyun cho and yoshua bengio

empirical evaluation of gated recurrent neural networks on sequence ing
arxiv e prints
jade goldstein mark kantrowitz vibhu mittal and jaime carbonell
summarizing text
ments sentence selection and evaluation metrics

in proceedings of the annual international acm sigir conference on research and opment in information retrieval
acm new york ny usa sigir pages
aglar glehre sungjin ahn ramesh nallapati bowen zhou and yoshua bengio

pointing
in proceedings of the
the unknown words

annual meeting of the association for tional linguistics volume long papers
ciation for computational linguistics berlin
many pages
karl moritz hermann toms kocisk edward grefenstette lasse espeholt will kay mustafa leyman and phil blunsom

teaching chines to read and comprehend
in advances in
ral information processing systems
pages
sepp hochreiter

untersuchungen zu chen neuronalen netzen
thesis diploma

sis institut fr informatik lehrstuhl prof
brauer technische universitt mnchen
andrej karpathy and li fei fei

deep semantic alignments for generating image tions
in the ieee conference on computer vision and pattern recognition cvpr diederik kingma and jimmy ba
adam a international method for stochastic optimization

conference on learning representations iclr

ankit kumar ozan irsoy peter ondruska mohit
iyyer james bradbury ishaan gulrajani victor zhong romain paulus and richard socher


ask me anything dynamic memory networks for natural language processing
in maria florina can and kilian weinberger editors proceedings of the international conference on machine learning
pmlr new york new york usa ume of proceedings of machine learning search pages

chin yew lin

rouge
a package for matic evaluation of summaries
in stan szpakowicz marie francine moens editor text summarization
branches out proceedings of the shop
association for computational linguistics barcelona spain pages

tomas mikolov ilya sutskever kai chen greg rado and jeffrey dean

distributed tations of words and phrases and their ality
in proceedings of the international ference on neural information processing systems

curran associates usa pages
olof mogren mikael kgebck and devdatt p hashi

extractive summarization by
ing multiple similarities
in ranlp
pages
ramesh nallapati bowen zhou ccero nogueira dos santos aglar glehre and bing xiang


abstractive text summarization using sequence
in proceedings of the sequence rnns and beyond

signll conference on computational natural language learning conll berlin germany august
pages
preksha nema mitesh khapra anirban laha and balaraman ravindran

diversity driven tention model for query based abstractive rization
arxiv e prints ani nenkova and kathleen mckeown

a vey of text summarization techniques springer us boston ma pages
jahna otterbacher gunes erkan and dragomir radev

biased lexrank passage retrieval ing random walks with question based priors
mation processing and management
jeffrey pennington richard socher and pher manning

glove
global vectors for word representation
in empirical methods in
ural language processing emnlp
pages
alexander rush sumit chopra and jason weston


a neural attention model for abstractive
in proceedings of the tence summarization

conference on empirical methods in natural guage processing
association for computational linguistics lisbon portugal pages
baskaran sankaran haitao mi yaser al onaizan and abe ittycheriah

temporal attention model
arxiv e prints for neural machine translation


mike schuster and kuldip k paliwal

tional recurrent neural networks
ieee transactions on signal processing
abigail see peter liu and
christopher
ning
get to the point summarization with pointer generator networks
arxiv e prints
ilya sutskever oriol vinyals and quoc le

sequence to sequence learning with neural
in proceedings of the international works

conference on neural information processing tems
mit press cambridge ma usa pages

ming tan bing xiang and bowen zhou


based deep learning models for non factoid answer selection
arxiv e prints
wilson l taylor

cloze procedure a new tool for measuring readability
journalism bulletin
lu wang hema raghavan vittorio castelli
radu rian and claire cardie

a sentence pression based framework to query focused document summarization
in acl
yonghui wu mike schuster zhifeng chen quoc
le mohammad norouzi wolfgang macherey maxim krikun yuan cao qin gao klaus macherey jeff klingner apurva shah melvin
johnson xiaobing liu lukasz kaiser stephan
gouws yoshikiyo kato taku kudo hideto kazawa keith stevens george kurian nishant
patil wei wang cliff young jason smith jason riesa alex rudnick oriol vinyals greg corrado macduff hughes and jeffrey dean

google s neural machine translation system bridging the gap between human and machine translation
arxiv e prints
kelvin xu jimmy ba ryan kiros kyunghyun cho aaron courville ruslan salakhudinov rich zemel and yoshua bengio

show attend and tell neural image caption generation with visual
in international conference on machine tention

learning
pages

a supplemental material dataset

an example of a record in the dataset is shown in table
we organize the dataset triples hierarchically
by document then query then reference
the documents and queries are numbered numerically starting with while the references are numbered alphabetically starting with document may have queries and and reference summaries and
the order is shufed amongst document query and reference ids
for matching the format expected by pyrouge attention visualisations figure visualization of the attention distribution as the summary in table is generated
the words of the document are shown on the horizontal axis from left to right
only a limited number of document words are shown
the vertical axis shows the output words from top to bottom after the go token

the darker a cell is the higher the attention on that position
figure visualization of the attention distribution ti as an output summary for a document query pair is generated
the query is australia
the format is the same as in figure
figure visualization of the attention distribution ti as an output summary for a document query pair in the test set is generated
the query is only fools and horses
the format is the same as in figure
the ellipsis signies that parts of the attention distribution has been skipped
go klinsmannappointedasthenewcoachofunitedstatesasanychildoftheeightieswilltellyou neonisastapleofanyafter darkcelebration stevefelice stevefelice glennstorey stevefelice glennstorey stevefelice thewheeler hiscockneyaccentwouldbeasbro go hewasinspiredbyarealeastendofonlyfoolsandhorses table
example of dataset samples ated from a document query pair using our method compared to hermann et al

in the style questions the entity corresponding to the swer has been replaced by document cnn former vice president walter mondale was released from the mayo clinic on saturday after being admitted with inuenza hospital spokeswoman kelley luckstein said
he s doing well
we treated him for u and cold symptoms and he was released today she said
mondale was diagnosed after he went to the hospital for a routine checkup following a fever former president jimmy carter said friday
he is in the bed right this moment but looking forward to come back home carter said during a speech at a nobel peace prize forum in minneapolis
he said tell everybody
he is doing well mondale underwent treatment at the mayo clinic in rochester minnesota
the vice dent served under carter between and and later ran for president but lost to ronald reagan
but not before he made history by naming a woman rep
geraldine ferraro of new york as his running mate
before that the former lawyer was a
senator from minnesota
his wife joan mondale died last year

highlight walter mondale was released from the mayo clinic on day hospital spokeswoman said
cloze style question walter mondale was released from the x on saturday pital spokeswoman said
cloze style answer mayo clinic our query mayo clinic
our target summary walter mondale was released from the mayo clinic on day hospital spokeswoman said
