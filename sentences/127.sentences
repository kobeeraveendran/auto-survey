query based abstractive summarization using neural networks johan hasselqvist
ist niklas helmertz
mikael kgebck
department of computer science and engineering chalmers university of technology abstract in this paper we present a model for erating summaries of text documents with respect to a query
this is known as based summarization
we adapt an isting dataset of news article summaries for the task and train a pointer generator model using this dataset
the ated summaries are evaluated by ing similarity to reference summaries
our results show that a neural network marization model similar to existing ral network models for abstractive rization can be constructed to make use of queries to produce targeted summaries
introduction creating short summaries of documents with spect to a query has applications in for example search engines where it may help inform users of the most relevant results
however ing such a summary automatically is a difcult in this paper a problem yet to be fully solved
neural network model for this task is presented
more specically the model is designed for brief commonly single sentence summaries
a tion where this may be useful is when a user has performed a search in a search engine and a set of documents have been returned
concise maries could then be displayed along with the search results giving a quick overview of how the document is related to the search query
what is commonly done in search engines today is that text surrounding an occurrence of a search query in the document is displayed as a summary
this is an example of extractive summarization which produces a summary that only contains parts of the original document
a signicant difference in the model we present is that it generates an abstractive summary
this type of summary lows for rephrasing and using words not ily present in the original document comparable to a human written summary
this has the tial of summarizing documents in a more concise way than what is possible with an extractive mary i
e
making it easier for a reader to stand the relationship between a document and a query
automatic text summarization has been a search topic for many years
in general the goal is to concisely represent the most important formation in documents
much previous work in summarization has been using extractive methods nenkova and mckeown mogren et al

commonly individual sentences are tracted and composed together to form a summary
this gives sentences that are as grammatically rect as the source document
they are however inherently limited and can not reproduce written summaries in general
abstractive marization in particular is closely related to ural language generation and it would be able to reach human level performance in it may however require ing summaries
level understanding of the context of documents to produce results comparable to human written ones
an important progress in using neural work models for generating text is sequence sequence used by sutskever et al
for machine translation
it is a way of mapping a varying length input text to a varying length put text and it is applicable to machine tion as well as summarization
in recent years progress has been made on using neural network models for text summarization and similar lems
some examples are sequence to sequence models for non query based abstractive rization by rush et al
and nallapati et al

neural network models have additionally been used for generating image captions thy and fei fei which is a form of c e d l c
s c v
v i x r a mary and for question answering problems such as by hermann et al
and tan et al

inspired by this progress we designed a model for query based summarization using neural works
the main contributions of this work includes a model for query based abstractive rization presented in section
a dataset for query based abstractive summarization ated by adapting an existing dataset originally used for question answering described further in section
a quantitative evaluation of the formance of the proposed model compared with an extractive baseline and an uninformed abstractive model presented in section
a qualitative analysis of the generated summaries

related work an early work evaluating several methods for tractive query based summarization is presented by goldstein et al

besides full queries they use short queries which on average are
words
these are similar in length to the types of queries used in the experiments of this sis work
besides the work by otterbacher et al
recent work in query based summarization has been done by wang et al
using parse trees and sentence compression
it is described as not pure extractive summarization
during the later stages of this thesis work nema et al
propose a neural network model for query based abstractive summarization which has some ilarities to the model we present
however the dataset they use is smaller in both average ument length and number of documents
tionally the types of queries used are different in that they use complete questions as opposed to our single entity queries
the task of question answering is to produce an answer to a question posed in natural language
the task is very general and many other problems can be expressed as a question answering lem
summarizing with respect to a query may for instance be expressed as what is a summary of the document with respect to the query x for the query x
if the answer to a question is a gle complete sentence then it is especially close to the types of query based summaries considered in this thesis
otterbacher et al
present a model biased lexrank which they use for a form of question answering as well as extractive query based summarization
the answers they generate are full sentences which makes it similar to our task of query based summarization
mann et al
present neural network els for question answering
for training these they create a large dataset from cnn daily mail news articles
we adapt this dataset for based summarization as detailed in chapter
kumar et al
introduce dynamic memory networks which they show reached state of art performance in a variety of nlp tasks
we draw inspiration from their use of a question ule when we incorporate query information in our model
general abstractive summarization differs from query based summarization in that a document is summarized without respect to a query
pati et al
build upon a machine tion model by bahdanau et al
and erate general abstractive summaries on multiple datasets including the cnn daily mail dataset by hermann et al

additions they make for their model include a pointer generator anism glehre et al
that allows the model to copy words from the source document
see et al
propose a similar model using a lar pointer generator mechanism that outperforms nallapati et al
on a slightly different sion of the cnn daily mail dataset making the result not strictly comparable
they also porate what they call coverage for avoiding titions in the output
background in the following sections various terms and cepts used throughout the paper are explained

named entity recognition information extraction is a class of tasks that volve extracting structured information from uments
an example of such a task is named entity recognition which is the classication of parts of text into different categories such as persons or locations or no category
an example from the sentence the mathematician jeff paris visited the city of paris
is that jeff paris should be tated as a person and the last paris as a location

gated recurrent units the gated recurrent unit gru is a type of current neural network rnn that is designed to alleviate the vanishing exploding gradient lem hochreiter bengio et al
which hinders the original rnn from capturing long term dependencies
gru is similar to the lar long short term memory lstm model but is simpler and less computationally intensive while still achieving comparable results on many tasks chung et al
kumar et al

the tire gru architecture can be described by the mulas rt br zt bz t rt bh ht zt zt t the vectors xt is the input at time step t and ht is the output while rt and zt are scaling vectors tended to regulate what information is let through
these can be described as gates
they have ments in
the vector t is rather intended to carry data
its elements are in generated from a network with a tanh activation function
we denote an entire gru update step as ht xt

word embeddings given a vocabulary v we can encode each word uniquely using a one hot encoding
this gives a vector of length where every word in the cabulary is mapped uniquely to some dimension which a value of while the other dimensions are
this vector can be transformed to an ding for the word by multiplying it by an ding matrix wemb of dimensionality demb where demb is the word embedding ity commonly a hyperparameter in neural network models
the intention is that the embeddings ture some characteristics of words giving useful vector representations
for instance two related words such as football and soccer may be pected to be close to each other in the vector space
two methods for generating word embeddings are mikolov et al
and glove et al


attention for many problems it has been found to be cial to use more of the rnn states than the nal xed size hidden state
attention is a mechanism for allowing the model to access more information in the decoding process by letting it identify vant parts of the input and use the encoder hidden state at these locations
this technique has been used successfully for machine translation danau et al
and image captioning xu et al

model we propose a sequence to sequence model with attention and a pointer mechanism making it a pointer generator model
the input for the problem is a document and a query
these are sequences of words passed to a document coder and a query encoder respectively
the coders outputs are then passed to the attentive decoder which generates a summary
both coders as well as the decoder use rnns with grus
each occurrence of gru with a subscript in the formulas in the following sections has arate weights and biases
the entire model is picted in figure
the different components and variables in the gure will be explained in detail throughout the section

document encoder the document encoder processes an input ment generating a state for each input word
to get a representation of the context around a word we use a bidirectional rnn schuster and paliwal encoder so both the context before and ter contribute to the representation
this is used by bahdanau et al
amongst others ing good results on a similar task related to text comprehension
h i and the combined rnn hidden state at time step i hi and the intermediate states h i from the forward reader and backward reader tively are computed as h i gru doc h gru doc h i h h e wi h i hi where wi v for the vocabulary v is word i in wi is word i in the reversed the input document input and is the word embedding of wi
the initial states are zero vectors
due to the concatenation the combined state hi has twice the dimensionality of the state of each directional encoder
the document encoder state and figure overview of our model
it illustrates connections between parts of the model at a xed decoder time step t
the bottom part containing labeled boxes correspond to the different rnns
the top part is intended to visualize the two ways the output word yt can be selected through the pointer and generator mechanism to the left and right respectively
dimensionality is denoted ddoc and the word bedding dimensionality demb

query encoder the query encoder is responsible for creating a xed size internal representation of the input query
unlike the document encoder the query encoder is a unidirectional rnn encoder since queries are relatively short compared to ments and we only use the nal state to resent the whole query
the rnn state hq i at is updated according to hq i query word i q hq where wq is nq the input query and nq is the length of the query
the initial state hq is the zero vector
the query encoder state dimensionality is denoted dque

decoder the decoder is a unidirectional rnn for ing a summary of the input document by ing on the nal state of the input encoder the it utilizes soft attention in combination query
with a pointer mechanism as well as a generator part similar to bahdanau et al

the query embedding q is fed as input at each decoder time step
this is similar to the answering module in a question answering model presented by kumar et al
who use an rnn encoded question representation as input at each decoder time step
in our model the rnn state is updated ing to st ct q where hnd the nal document encoder state nd being the number of input words corresponds to a special go token used at the initial time step when no previous word has been predicted ct is the context vector at time step t from the attention mechanism dened subsequently and v is the predicted output word at time step t
this is either from the generator nism or the pointer mechanism also dened sequently
the word embeddings are the same as are used in the encoder
the intention of the inclusion of q to the input of grudec is to give the decoder the ability to tune the structure of the output sequence to eventually output something concerning the query
for ample if the query is a location the decoder can output words leading up to an appropriate sion of the location
the generator outputs a word from a subset of the vocabulary vgen v at each time step
the selection of the output words is done through a distribution of words in vgen computed through a softmax as pgen for j an index uniquely mapped to a word w vgen and ztj as dened subsequently
dening this tj as the probability p gen word ygen arg max wvgen t with the highest probability by ygen we then select output t
the softmax probability p gen t t gen st gen and pends on ztj the output from two linear mations on the decoder state and context vector gen dened as zt w gen w gen where w gen gen rdgen w gen are trainable hyperparameters in which dgen is the ality of the hidden layer
the main function of this layer is to reduce the dimensionality of the input for reducing computation time for the nal layer with size
the model has a soft attention mechanism based on one used by bahdanau et al
for machine translation
the result of the attention mechanism is a context vector ct produced at each time step t computed as ct tihi i ti eti q where hi is the document encoder hidden state at index i
the score function is dened as s q v att s batt where watt is a weight matrix vatt rdatt is a vector and batt is a bias vector all of which are trained together with the rest of the network
the query q is included for the model to focus attention around query words when appropriate

pointer mechanism a general issue is that with a generator mechanism limited to frequent words infrequent words not be generated
further if the model needs to learn to output names and there are many different ones and few occurrences of each in the training data training a model to generate them correctly is problematic
a way to solve these issues is to allow the model to directly copy a word in the put document to the output summary or point to it
this may additionally be viewed as using the input text as a secondary output vocabulary in addition to vgen
the pointer mechanism adds a switch pptr at each decoder time step t to the model
it is computed as the output of a linear t mation fed through a sigmoid activation function as pptr bptr where vptr and bptr are vectors all of which are trained together with the rest of the work
if pptr t
a word is copied from the input otherwise the generator output is used
what is copied from the input for the tth decoder word is determined by the attention distribution
cally at time step t we select the word at index t arg max ti in the document where the tention is highest as yptr word can then be dened as t
the nal output t i yt yptr t ygen t t
if pptr otherwise
training loss t log pptr the model is trained in when to use the pointer mechanism in a supervised manner
we ne an additional training input xptr that is t ther if the pointer mechanism is set to be used for the tth word in the summary or otherwise
for training this we dene a loss function lptr t t pptr xptr t
for training the generator mechanism we ne a loss over the generator softmax layer as lgen w where ns is the length of the target summary w vgen is the the tth word in the target summary
plying by xptr t excludes any addition to the loss when the pointer mechanism is set to be used
we introduce a form of supervised attention for when the pointer mechanism is set to be used for an output word by introducing a loss function latt t log ti where i is the dex in the input document to point to
t log p gen xptr xptr t the nal loss function is the sum of the different losses normalized by the length computed as l ns lgen latt lptr

generating summaries summaries are considered complete when a cial eos token has been generated or after a maximum output length is reached
potential summaries are explored using beam search
ever for time steps where the pointer mechanism is used the partial summaries are prioritized by probabilities as if the generator had been used stead so k partial summaries with different abilities are created for the word chosen by the table highlights of a cnn article titled airline quality report sorts out the duds from the dynamos in

hawaiian airlines again lands at no
in on time performance
the airline quality rankings report looks at the largest u
s
airlines
expressjet and american airlines had the worst on time performance
virgin america had the best baggage handling southwest had lowest complaint rate table statistics of the dataset
val



training


doc doc query pairs doc query sum avg words doc avg words query avg words sum test


pointer mechanism
this is difcult to justify but we hope that this should give a reasonable bility at time steps when the pointer mechanism is used preventing summaries using the pointer mechanism more to be prioritized
a slight deviation from what is presented in section
is that when the pointer mechanism is used and the attended word was not in v we do not output unk which it is otherwise preted as in the model but rather the actual put word before it being converted to an index in the vocabulary
this may be viewed as a processing step
dataset the dataset constructed for this paper is based hermann et al
and consist of ment query answer triples from cnn and daily mail news articles
included with each published news article there are a number of human written highlights which summarize different aspects of the article
table shows some example lights for a single article
they construct a ument query answer by considering a named tity in a highlight to be unknown making the light into a cloze style question taylor whose answer is the entity made unknown
an example document and a cloze style question and its answer can be seen in table a

we propose using the cnn daily mail dataset for query based abstractive summarization by regarding each light as a summary of its document and entities in the highlight as queries
for every occurrence of an entity in a highlight we construct a query summary triple for query based tion
table a
shows for a sample document a cloze style question compared and the sponding query summary pair constructed by us
if an entity is mentioned in multiple highlights we consider there being multiple target references for the document query pair
in contrast to hermann et al
we do not translate entities into tiers but use only minimal preprocessing in the form of tokenization and lowercasing
further we mix articles from dnn and daily mail while mann et al
keeps them separate
we cided to train our model on a mix of cnn and daily mail articles with a proportion of them ing reserved for validation and test sets
which articles are included for the validation and test set is determined randomly with equal probability for every article
some statistics of the resulting dataset can be seen in table
the dataset can be reproduced using a script made available on
experiments two experiments were conducted
the rst to measure if the model uses the information in the query section
and the second compares the model to an extractive baseline section

a beam width of k and a maximum output length of was used

query dependence to determine whether incorporating a query ets our model we compare our proposed model to one where the query is corrupted
instead of evaluating the generated summary for a document and a query with id n against the reference maries for that query we evaluate it against the reference summaries for query i
e
the query id has been offset
for the query with the highest id the reference summaries for the rst query are used
the idea is that if the score is lower than for the normal evaluation then the model has made use of the additional information in the query

com querysum data table reference summaries used during normal evaluation compared to with offset queries
query id


normal a

b

a

a

b

offset queries a

a

b

a

b

ble shows for an example document what the generated summaries are evaluated against during the query dependence evaluation
it is worth to mention that two reference summaries for ent queries may be the same as the same original highlight may be used as a reference summary for multiple queries
in these cases the query will be appropriate for the summary and the model may have beneted from the query even in the offset evaluation

extractive baseline as a baseline we compare the results to a simple extractive summary designed specically for the dataset used in this thesis work
the baseline mary is constructed by selecting the rst sentence in the document containing the query without if no such stricting the length of the document
sentence is found i
e
the document does not tain the query the rst sentence of the document is used instead
this does occur in the dataset but not frequently
we additionally observe that the average length of baseline sentences using the cnn daily mail dataset is commonly greater than for the reference summaries
the average number of words is
for the baseline summaries while it is
for the reference summaries
it may be possible to gain a higher rouge score if a fewer number of words around the query occurrence is selected but it might not form a complete sentence

evaluation metric our results are evaluated using four different rics provided by rouge recall oriented derstudy for gisting evaluation lin the defacto standard evaluation method for automatic summarization
l and rouge
and are the scores for grams and grams tively
rouge l and rouge are more complex metrics detailed by lin

training details the vocabulary v used for the input text contains the most frequent words in the training set while the generator vocabulary vgen consist of the most frequent words
the smaller lary of the generator is due to the pointer nism
word embeddings for the vocabulary words are initialized with dimensional glove trained on wikipedia gigaword
if the word does not have a glove ding we initialize the word embedding by pling the per dimension univariate normal butions with means and standard deviations of the entire collection of glove embeddings
both during training and test time we limit the document length to the rst words to reduce computation time
the loss l is minimized using the sgd based adam optimizer kingma and ba
we used mini batches of samples with an averaged loss over all the samples in the batch
the mini batches remained the same over epochs but the order in which they were trained on was randomized tween every epoch
experiments have been run on a single nvidia tesla with gb of memory and took about hours to train
the model is implemented using tensorflow abadi et al
and the complete source has been made available
the hyperparameters used for the experiments is reported in table
no extensive rameter tuning has been performed but instead examined hyperparameters used for similar els such as nallapati et al
and see et al

table hyperparameter conguration used
hyperparameter word embedding size document encoder size query encoder size decoder size attention hidden size generator hidden size value demb ddoc dque ddec datt dgen as glove

zip at
stanford
edu projects
com helmertz querysum table example document query pair
query netix table rouge scores of the evaluated models
model first query sentence our model offset queries











l document cnn the united states have named former germany captain jurgen klinsmann as their new national coach just a day after sacking bob bradley
bradley who took over as coach in january was relieved of his duties on thursday and u
s
soccer federation president sunil gulati conrmed in a statement on friday that his replacement has already been appointed



query united states reference jurgen klinsmann is named as coach of the united states national side output klinsmann appointed as the new coach of united states results the results from our experiments are summarised in table
from the result of the query dence evaluation offset queries described in section
we can see that the rouge scores goes down with statistical signicance according to the rouge reported condence intervals when the queries are offset
this indicates that the model benets from the information provided by queries
further we observe that our model score lower than the baseline model which we denote the rst query sentence described in section

however it should be noted that this baseline is expected to be strong given the nature of this dataset

further analysis we observe that the attention at a time step appears to often be highly focused on only a few words in the document
an example of an output summary can be seen in table and figure a
shows the attention distribution over time for the same erated summary
another observation we make is that the attention often is focused at the beginning of the documents
however there are certainly stances when entities are selected from far back in documents
this bias may partly be due to our decision to point out the rst occurrences of tities
although it has been noted by goldstein et al
that the beginning of news articles table example document query pair
document president barack obama sided with internet activists on monday urging the federal nications commission to draft new rules that would classify the broadband net to regulate it more like a lic utility
the end result would tie the hands of internet service providers that want to cut special deals with vices like netix youtube hulu and amazon to push their streaming content along a fast lane that ordinary icans ca nt access



reference obama s vision would bar providers like izon and comcast from cutting deals with hulu netix and amazon so their streaming content could be delivered along online fast lanes output obama s chief executive of netix has refused to allow users to access the service table example document query pair
document february a breakthrough in belarus a verdict in italy and an expected veto in the u
s
all headline cnn student news this friday



query cnn student news roll call reference at the bottom of the page comment for a chance to be mentioned on cnn student news
you must be a teacher or a student age or older to request a tion on the cnn student news roll call
output at the bottom of the page comment for a chance to be mentioned on cnn student news
you must be a teacher or a student age or older to ten summarizes the article quite well
from examining some of the output summaries from our model we see that they often strongly match the topic of the input documents but they rarely succeed in generating summaries ing something actually stated in the article
table shows an example output that is fairly ically correct but not truthful with respect to the article
we observe that the model manages to learn some of the dataset samples which are not actual summaries described in section such as notices repeated over several articles
the generated mary shown in table is an example of this
terestingly the model manages to literally repeat the reference summary up to the maximum output length limit
we can frequently see repetitions of the same phrases an extreme example can be seen in figure a

the model appears to get stuck ing to begin a summary
additionally we observe that the repetition can be observed in the attention distribution as well
the same problem has been seen by nallapati et al
who make an tion temporal attention sankaran et al
to their model for alleviating the issue of repetitions
see et al
propose using coverage to solve the same issue
before running experiments we suspected that it may be difcult for the pointer mechanism to sequentially point out words that make up longer entities
however we see that this is done fully quite often
for an example summary the certainty of selecting a sequence of entity words can be seen in figure a

compared to the reference summaries the put is generally shorter
the average number of words in output summaries is
while the dataset average is

as is noted by wu et al
beam search commonly favors shorter summaries
they propose an addition of length normalization for reducing this tendency
menting such a measure may improve the results of our model as well
in comparison to nallapati et al
and see et al
our rouge scores are low
they use a different version of the dataset where all lights are combined to form a single often sentence summary
with similar models they get results of around on the general summarization task
however while they always train the model to output the same summary for the same document we often have completely ent target summaries for different queries where the queries make up a much smaller part of the put
conclusion we have designed a model for query based stractive summarization and evaluated it on an adapted qa dataset redesigned for query based summarization
while the overall performance of the model is not enough to outperform our tive baseline we have shown that it can rate a query and utilize the information to create more focused summaries
references martn abadi ashish agarwal paul barham eugene brevdo zhifeng chen craig citro greg corrado andy davis jeffrey dean matthieu devin sanjay ghemawat ian goodfellow andrew harp geoffrey irving michael isard yangqing jia rafal icz lukasz kaiser manjunath kudlur josh enberg dan man rajat monga sherry moore derek murray chris olah mike schuster jonathon shlens benoit steiner ilya sutskever kunal war paul tucker vincent vanhoucke vijay van fernanda vigas oriol vinyals pete warden martin wattenberg martin wicke yuan yu and aoqiang zheng

tensorflow large scale chine learning on heterogeneous systems
software available from tensorflow
org
dzmitry bahdanau kyunghyun cho and yoshua gio

neural machine translation by jointly international learning to align and translate
ference on learning representations iclr

yoshua bengio patrice simard and paolo frasconi

learning long term dependencies with ent descent is difcult
ieee transactions on neural networks
junyoung chung aglar glehre kyunghyun cho and yoshua bengio

empirical evaluation of gated recurrent neural networks on sequence ing
arxiv e prints

jade goldstein mark kantrowitz vibhu mittal and jaime carbonell

summarizing text ments sentence selection and evaluation metrics
in proceedings of the annual international acm sigir conference on research and opment in information retrieval
acm new york ny usa sigir pages
aglar glehre sungjin ahn ramesh nallapati bowen zhou and yoshua bengio

pointing in proceedings of the the unknown words
annual meeting of the association for tional linguistics volume long papers
ciation for computational linguistics berlin many pages
karl moritz hermann toms kocisk edward grefenstette lasse espeholt will kay mustafa leyman and phil blunsom

teaching chines to read and comprehend
in advances in ral information processing systems
pages
sepp hochreiter

untersuchungen zu chen neuronalen netzen
ph
d
thesis diploma sis institut fr informatik lehrstuhl prof
brauer technische universitt mnchen
andrej karpathy and li fei fei

deep semantic alignments for generating image tions
in the ieee conference on computer vision and pattern recognition cvpr
diederik p
kingma and jimmy ba

adam a international method for stochastic optimization
conference on learning representations iclr

ankit kumar ozan irsoy peter ondruska mohit iyyer james bradbury ishaan gulrajani victor zhong romain paulus and richard socher

ask me anything dynamic memory networks for natural language processing
in maria florina can and kilian q
weinberger editors proceedings of the international conference on machine learning
pmlr new york new york usa ume of proceedings of machine learning search pages
chin yew lin

rouge a package for matic evaluation of summaries
in stan szpakowicz marie francine moens editor text summarization branches out proceedings of the shop
association for computational linguistics barcelona spain pages
tomas mikolov ilya sutskever kai chen greg rado and jeffrey dean

distributed tations of words and phrases and their ality
in proceedings of the international ference on neural information processing systems
curran associates inc
usa pages
olof mogren mikael kgebck and devdatt p hashi

extractive summarization by ing multiple similarities
in ranlp
pages
ramesh nallapati bowen zhou ccero nogueira dos santos aglar glehre and bing xiang

abstractive text summarization using sequence in proceedings of the sequence rnns and beyond
signll conference on computational natural language learning conll berlin germany august
pages
preksha nema mitesh khapra anirban laha and balaraman ravindran

diversity driven tention model for query based abstractive rization
arxiv e prints

ani nenkova and kathleen mckeown

a vey of text summarization techniques springer us boston ma pages
jahna otterbacher gunes erkan and dragomir r
radev

biased lexrank passage retrieval ing random walks with question based priors
mation processing and management
jeffrey pennington richard socher and pher d
manning

glove global vectors for word representation
in empirical methods in ural language processing emnlp
pages
alexander m
rush sumit chopra and jason weston

a neural attention model for abstractive in proceedings of the tence summarization
conference on empirical methods in natural guage processing
association for computational linguistics lisbon portugal pages
baskaran sankaran haitao mi yaser al onaizan and abe ittycheriah

temporal attention model arxiv e prints for neural machine translation


mike schuster and kuldip k paliwal

tional recurrent neural networks
ieee transactions on signal processing
abigail see peter j
liu and christopher d
ning

get to the point summarization with pointer generator networks
arxiv e prints

ilya sutskever oriol vinyals and quoc v
le

sequence to sequence learning with neural in proceedings of the international works
conference on neural information processing tems
mit press cambridge ma usa pages
ming tan bing xiang and bowen zhou

based deep learning models for non factoid answer selection
arxiv e prints

wilson l taylor

cloze procedure a new tool for measuring readability
journalism bulletin
lu wang hema raghavan vittorio castelli radu rian and claire cardie

a sentence pression based framework to query focused document summarization
in acl
yonghui wu mike schuster zhifeng chen quoc v
le mohammad norouzi wolfgang macherey maxim krikun yuan cao qin gao klaus macherey jeff klingner apurva shah melvin johnson xiaobing liu lukasz kaiser stephan gouws yoshikiyo kato taku kudo hideto kazawa keith stevens george kurian nishant patil wei wang cliff young jason smith jason riesa alex rudnick oriol vinyals greg corrado macduff hughes and jeffrey dean

google s neural machine translation system bridging the gap between human and machine translation
arxiv e prints

kelvin xu jimmy ba ryan kiros kyunghyun cho aaron courville ruslan salakhudinov rich zemel and yoshua bengio

show attend and tell neural image caption generation with visual in international conference on machine tention
learning
pages
a supplemental material a
dataset an example of a record in the dataset is shown in table a

we organize the dataset triples hierarchically by document then query then reference
the documents and queries are numbered numerically starting with while the references are numbered alphabetically starting with a
document may have queries
and
and reference summaries a

b

and a


the order is shufed amongst document query and reference ids
for matching the format expected by pyrouge a
attention visualisations figure a
visualization of the attention distribution as the summary in table is generated
the words of the document are shown on the horizontal axis from left to right
only a limited number of document words are shown
the vertical axis shows the output words from top to bottom after the go token
the darker a cell is the higher the attention on that position
figure a
visualization of the attention distribution ti as an output summary for a document query pair is generated
the query is australia
the format is the same as in figure a

figure a
visualization of the attention distribution ti as an output summary for a document query pair in the test set is generated
the query is only fools and horses
the format is the same as in figure a

the ellipsis signies that parts of the attention distribution has been skipped
justadayaftersackingbobbradley
bradley go klinsmannappointedasthenewcoachofunitedstatesasanychildoftheeightieswilltellyou neonisastapleofanyafter darkcelebration buttwofoodiesfrommelbournehavetakentheirloveofitonestepfurthertocreateasweettreatthatlightsupthenight
stevefelice stevefelice glennstorey stevefelice glennstorey stevefelice e
andifyouheardhimspeak hiscockneyaccentwouldbeasbroadastheriverthames
thisisderekhockley thewheeler dealerwhodavidjasonhasrevealedhetookashisinspirationwhenplayingdelboyinonlyfoolsandhorses

andifyouheardhimspeak hiscockneyaccentwouldbeasbro go hewasinspiredbyarealeastendofonlyfoolsandhorses table a
example of dataset samples ated from a document query pair using our method compared to hermann et al

in the style questions the entity corresponding to the swer has been replaced by x
document cnn former vice president walter mondale was released from the mayo clinic on saturday after being admitted with inuenza hospital spokeswoman kelley luckstein said
he s doing well
we treated him for u and cold symptoms and he was released today she said
mondale was diagnosed after he went to the hospital for a routine checkup following a fever former president jimmy carter said friday
he is in the bed right this moment but looking forward to come back home carter said during a speech at a nobel peace prize forum in minneapolis
he said tell everybody he is doing well
mondale underwent treatment at the mayo clinic in rochester minnesota
the vice dent served under carter between and and later ran for president but lost to ronald reagan
but not before he made history by naming a woman u
s
rep
geraldine a
ferraro of new york as his running mate
before that the former lawyer was a u
s
senator from minnesota
his wife joan mondale died last year
highlight walter mondale was released from the mayo clinic on day hospital spokeswoman said cloze style question walter mondale was released from the x on saturday pital spokeswoman said cloze style answer mayo clinic our query mayo clinic our target summary walter mondale was released from the mayo clinic on day hospital spokeswoman said
