text summarization using deep learning and ridge regression karthik bangalore mani illinois institute of technology abstract we develop models and extract relevant features for automatic text summarization and investigate the performance of different models on the duc dataset
t wo different models were developed one being a ridge regressor and the other one was a multi layer perceptron
t he hyperparameters were varied and their performance were noted
we segregated the summarization task into main steps the first being sentence ranking and the second step being sentence selection
in the first step given a document we sort the sentences based on their importance and in the second step in order to obtain non redundant sentences we weed out the sentences that are have high similarity with the previously selected sentences
introduction t he process of text summarization is to condense the information as much as possible without losing the gist of the document
in this project we develop an extractive summarizer which extracts the most important sentences in a document which are also salient
t here are main steps in a summarization task namely sentence ranking and sentence selection
t he first step is done to get an importance score for every sentence in the document and the second step is done to avoid redundancy in the summary which weeds out sentences that convey the same meaning as the earlier selected sentences
se ntence ranking we use the predicted scores of the models and sort them in the descending order
t he ones with high predictions are considered to be important
se ntence selection we use a greedy approach li and li to stitch together multiple sentences for the summary
in each step of selection the sentence with maximal salience is added into the summary unless its similarity with a sentence already in the summary exceeds a threshold
here we use tf idf cosine similarity et
al set the threshold t sim

t he process of summarization was converted to a regression task wherein the had features for every sentence and y value was the score between the sentence and the real summary in the duc dataset
different models such as deep mlp and ridge were trained and cross validated on this and y
t heir hyperparameters were varied and accuracies were plotted
due to the limited size of dataset and hand crafted features we found that the simple ridge regressor beat all the deep models
since ridge was the best model sentences were ranked and selected using ridge regressor

approach
data collection document understanding conference duc is a standard dataset to experiment with and evaluate summarization models
hence we collected the duc dataset to build the models
t his dataset has documents with complete texts and summaries written by a human

feature extraction a total of features were extracted for every sentence across every documents
t he features are listed below
position t he position of the sentence
suppose there are m sentences in the document then for the ith sentence the position is computed as
le ngth no of words in the sentence
average d tf t he mean term frequency of all words in the sentence divided by the sentence length

average d idf t he mean inverse document frequency of all words in the sentence divided by the sentence length

average d cf t he mean cluster frequency of all words in the sentence divided by the sentence length

po s ratio t he number of nouns verbs adverbs adjectives in the sentence divided by the length of the sentence
name d entity ratio t he number of named entities in the sentence divided by the length of the sentence
numbe r ratio t he number of digits in the sentence divided by the length of the sentence
stopword ratio t he number of stopwords in the sentence divided by the length of the sentence
we use the stopword list in the nltk package
after extracting the above features the train matrix was constructed with the where where c is the no of clusters is the no of docs in clusteri xij is the no of sentences in of the clusteri and m which is the number of features for every sentence

first sentence baseline model usually it has been argued that the first sentence of the document captures the most important information of the document
hence a dummy model which blindly predicts the first sentence as the predicted summary was built
t he mean score between the first sentence and the actual summary across all documents was computed and it s performance was noted

ridge regression model ridge regression t ibshirani is like least squares but shrinks the estimated coefficients towards zero
given a response vector y rn and a predictor matrix x rnp the ridge regression coefficients are defined as utilizes a supervised learning called backpropagation for training the network
t o put it in simple words training an mlp has main passes namely the forward pass and the backward pass
in the forward pass we compute the output of the activation functions and in the backward pass we find the error of the activation functions and finally we make weight updates
t he weight updates are generally done as follows swingler here is a tuning parameter which controls the strength of the penalty term
note that when we get the linear regression estimate when we get ridge for in between we are balancing two ideas fitting a linear model of y on x and shrinking the coefficients

ridge validation error during the validation phase we used fold cross validation to identify the best parameters for the regressor
upon cross validating for various polynomial features such as and we found that the validation error is minimum when the polynomial order is as shown in the below plot an mlp will have input layer output layer and varying hidden layers
for experiments in our project we had an mlp with the below architecture input nodes a total of input nodes hidden nodes a total of hidden nodes output node linear output node to get the predicted rouge score t he hidden layers were varied and their performance were noted
below is an example of a mlp with input hidden and output layers
hence the polynomial order was chosen and the was raised to this order during the testing phase

multi layer perceptron a multilayer wikipedia is a feedforward artificial neural network model that maps sets of input data onto a set of appropriate outputs
an mlp consists of multiple layers of nodes in a directed graph with each layer fully connected to the next one
except for the input nodes each node is a neuron or processing element with a nonlinear activation function
mlp

mlp validation error we use the same fold cross validation for deep mlp to tune the hyperparameters
t o figure out the best hyperparameters we validate the model with various settings as show below epochs optimizers sgd adam lbfgs activation functions logistic and t anh hidden layers t he number of units at every hidden layer was fixed at
so we validated mlp with a total of different settings
the validation errors for the settings were as shown below from the above plots we find the that the validation error is minimum for the optimizers with logistic as the activation function with hidden layers hence for all the optimizers we chose logistic as the activation function with the number of hidden layers to be during the testing phase

results by using the best settings identified during the validation phase we fit different models with below settings first sentence model ridge with order polynomial features mlp with hidden layers with adam optimizer and mlp with hidden layers with lbfgs optimizer and mlp with hidden layers with sgd optimizer and logistic activation logistic activation
logistic activation
future work t he reason ridge regression beats other models is due to limited size of the dataset we obtained documents which is actually very low for any deep learning standard
another reason is that the features were hand crafted and fed to the models
hence in the future we intend to do the following t rain on all the duc datasets
learn the features from text instead of handcrafting them fine tune the hyperparameters by using dropout generate an abstract summary instead of extracting it work on query oriented summarization fit other models such as recurrent neural nets
t est accuracy is the score between the predicted summary and actual gold summary in the duc dataset
we plot the accuracies as shown below and find that the simple ridge regression model beats other deep models

references kevin swingler
multi layer perceptrons

cs
stir
ac
uk courses it lectures mlp
pdf multilayer perceptron

wikipedia
org wiki neural network models supervised
learn
org stable modules
html ryan t
modern regression

stat
cmu
datamining
pdf ziqiang cao furu wei li dong sujian li ming zhou
ranking with recursive neural networks and its application to multi document summarization
www
aaai
org ocs index
php aaai paper download
