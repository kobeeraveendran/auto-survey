a cascade approach to neural abstractive summarization with content selection and fusion logan lebanoff franck dernoncourt doo soon kim walter chang fei liu computer science department university of central florida orlando fl adobe research san jose ca
ucf
edu dernonco dkim
com
ucf
edu abstract we present an empirical study in favor of a cade architecture to neural text summarization
summarization practices vary widely but few other than news summarization can provide a sufcient amount of training data enough to meet the requirement of end to end neural stractive systems which perform content tion and surface realization jointly to generate abstracts
such systems also pose a challenge to summarization evaluation as they force tent selection to be evaluated along with text generation yet evaluation of the latter remains an unsolved problem
in this paper we present empirical results showing that the performance of a cascaded pipeline that separately identies important content pieces and stitches them gether into a coherent text is comparable to or outranks that of end to end systems whereas a pipeline architecture allows for exible tent selection
we nally discuss how we can take advantage of a cascaded pipeline in ral text summarization and shed light on portant directions for future research
introduction there is a variety of successful summarization plications but few can afford to have a large number of annotated examples that are sufcient to meet the requirement of end to end neural abstractive summarization
examples range from ing radiology reports jing et al
zhang et al
to congressional bills kornilova and man and meeting conversations mehdad et al
li et al
koay et al

the lack of annotated resources suggests that end end systems may not be a one all lution to neural text summarization
there is an increasing need to develop cascaded architectures to allow for customized content selectors to be bined with general purpose neural text generators to realize the full potential of neural abstractive summarization
we advocate for explicit content selection as it lows for a rigorous evaluation and visualization of intermediate results of such a module rather than associating it with text generation
existing ral abstractive systems can perform content tion implicitly using end to end models see et al
celikyilmaz et al
raffel et al
lewis et al
or more explicitly with an nal module to select important sentences or words to aid generation tan et al
gehrmann et al
chen and bansal kryscinski et al
hsu et al
lebanoff et al
liu and lapata
however content selection concerns not only the selection of important ments from a document but also the cohesiveness of selected segments and the amount of text to be selected in order for a neural text generator to duce a summary
in this paper we aim to investigate the feasibility of a cascade approach to neural text tion
we explore a constrained summarization task where an abstract is created one sentence at a time through a cascaded pipeline
our pipeline ture chooses one or two sentences from the source document then highlights their summary worthy segments and uses those as a basis for composing a summary sentence
when a pair of sentences are selected it is important to ensure that they are fusible there exists cohesive devices that tie the two sentences together into a coherent text to avoid generating nonsensical outputs geva et al
lebanoff et al

highlighting sentence segments allows us to perform ne grained content selection that guides the neural text generator to stitch selected segments into a coherent sentence
the contributions of this work are summarized as follows
t c o l c
s c v
v i x r a figure model architecture
we divide the task between two main components the rst component performs sentence selection and ne grained content selection which are posed as a classication problem and a tagging problem respectively
the second component receives the rst component s outputs as supplementary information to generate the summary
a cascade architecture provides the necessary exibility to separate content selection from surface realization in abstractive summarization
we present an empirical study in favor of a cascade architecture for neural text marization
our cascaded pipeline chooses one or two sentences from the document and highlights their important segments these ments are passed to a neural generator to duce a summary sentence
our quantitative results show that the mance of a cascaded pipeline is comparable to or outranks that of end to end systems with added benet of exible content selection
we discuss how we can take advantage of a cade architecture and shed light on important directions for future research
a cascade approach our cascaded summarization approach focuses on shallow abstraction
it makes use of text mations such as sentence shortening paraphrasing and fusion jing and mckeown and is in contrast to deep abstraction where a full tic analysis of the document is often required
a shallow approach helps produce abstracts that vey important information while crucially ing faithful to the original
in what follows we describe our approach to select single sentences and sentence pairs from the document highlight summary worthy segments and perform summary generation conditioned on highlights
selection of singletons and pairs our approach iteratively selects one or two sentences from the input document they serve as the basis for ing a single summary sentence
previous research suggests that of human written summary code is publicly available at
com ucfnlp cascaded summ sentences are created by shortening a single tence or merging a pair of sentences lebanoff et al

we adopt this setting and present a coarse strategy for content selection
our strategy begins with selecting sentence singletons and pairs followed by highlighting important ments of the sentences
importantly the strategy allows us to control which segments will be bined into a summary sentencecompatible ments come from either a single document sentence or a pair of fusible sentences
in contrast when all important segments of the document are provided to a neural generator all at once gehrmann et al
it can happen that the generator ily stitches together text segments from unrelated sentences yielding a summary that contains cinated content and fails to retain the meaning of the original document falke et al
lebanoff et al
kryscinski et al

we expect a sentence singleton or pair to be lected from the document if it contains salient tent
moreover a pair of sentences should contain content that is compatible with each other
given a sentence or pair of sentences from the document our model predicts whether it is a valid instance to be compressed or merged to form a summary sentence
we follow lebanoff et al
to use bert devlin et al
to perform the cation
bert is a natural choice since it takes one or two sentences and generates a classication prediction
it treats an input singleton or pair of sentences as a sequence of tokens
the tokens are fed to a series of transformer block layers sisting of multi head self attention modules
the rst transformer layer creates a contextual sentation for each token and each successive layer further renes those representations
an additional sentpredhighlightnon grainedcontent selectionhighlightstudentnon highlight











start dukestudentstudenthas


encoderdecoderwordembeddinghighlightembeddinggeneration figure comparison of various highlighting strategies
thresholding obtains the best performance
cls token is added to contain the sentence resentation
bert is ne tuned for our task by adding an output layer on top of the nal layer resentation hl for sequence s as seen in eq

where u is a vector of weights and is the sigmoid function
the model predicts psent whether the sentence singleton or pair is an appropriate one based on the cls token representation
we scribe the training data for this task in
fine grained content selection it is ing to note that the previous architecture can be naturally extended to perform ne grained content selection by highlighting important words of tences
when two sentences are selected to erate a fusion sentence it is desirable to identify segments of text from these sentences that are tentially compatible with each other
the coarse ne method allows us to examine the intermediate results and compare them with ground truth
cretely we add a classication layer to the nal layer representation hl i for each token wi eq

the per target word loss is then interpolated with instance prediction one or two sentences loss ing a coefcient
such a multi task learning jective has been shown to improve performance on a number of tasks guo et al

i where v is a vector of weights and is the sigmoid function
the model predicts phighlight for each ken whether the token should be included in the output fusion calculated based on the given token s representation
information fusion given one or two sentences taken from a document and their ne grained lights we proceed by describing a fusion process that generates a summary sentence from the lected content
our model employs an decoder architecture based on pointer generator networks that has shown strong performance on its own and with adaptations see et al
gehrmann et al

we feed the sentence gleton or pair to the encoder along with highlights derived by the ne grained content selector the latter come in the form of binary tags
the tags are transformed to a highlight on embedding for each token if it is chosen by the content selector and a highlight off embedding for each token not chosen
the highlight on off embeddings are added to token embeddings in an element wise manner both highlight and token embeddings are learned
an illustration is shown in figure
highlights provide a valuable intermediate resentation suitable for shallow abstraction
our approach thus provides an alternative to methods that use more sophisticated representations such as syntactic semantic graphs filippova and strube banarescu et al
liu et al

it is more straightforward to incorporate highlights into an encoder decoder fusion model and obtaining highlights through sequence tagging can be tially adapted to new domains
experimental results data and annotation to enable direct ison with end to end systems we conduct ments on the widely used cnn dm dataset see et al
to report results of our cascade approach
we use the procedure described in lebanoff et al
to create training instances for the sentence selector and ne grained content selector
our training data contains stances every instance contains one or two date sentences
it is a positive instance if a truth summary sentence can be formed by pressing or merging sentences of the instance tive otherwise
for positive instances we highlight all lemmatized unigrams appearing in the summary excluding punctuation
we further add smoothing to the labels by highlighting single words that r l f score f score f score probability thresholdingproportional to input all to input valuethreshold valuethreshold valuepercentage of words percentage of words percentage of words system r l


sumbasic vanderwende et al



lexrank erkan and radev pointer generator see et al



chen and bansal





bert extr lebanoff et al



bottomup gehrmann et al
bert abs lebanoff et al
cascade fusion ours cascade tag ours








gt sent sys tag gt sent sys tag fusion gt sent gt tag gt sent gt tag fusion











system sents a duke student has admitted to hanging a noose made of rope from a tree near a student union university ofcials said thursday
the student was identied during an investigation by pus police and the ofce of student affairs and admitted to placing the noose on the tree early wednesday the university said
cascade fusion a duke student was identied during an tion by campus police and the ofce of student affairs and admitted to placing the noose on the tree early wednesday
gt sents in a news release it said the student was no longer on pus and will face student conduct review
duke university is a private college with about students in durham north carolina
gt sents fusion duke university student was no longer on pus and will face student conduct review
reference student is no longer on duke university campus and will face disciplinary review
table left summarization results on cnn dm test set
our cascade approach performs comparable to strong extractive and abstractive baselines oracle models using ground truth sentences and segment highlights perform the best
right example source sentences and their fusions
dark highlighting is content taken from the rst sentence and light highlighting comes from the second
our cascade fusion approach effectively performs entity replacement by replacing student in the second sentence with a duke student from the rst sentence
nect two highlighted phrases and by ing isolated stopwords
at test time four scored instances are selected per document their important segments are highlighted by content lector then passed to the fusion step to produce a summary sentence each
the hyperparameter for weighing the per target word loss is set to
and highlighting threshold value is

the model hyperparameters are tuned on the validation split
summarization results we show experimental results on the standard test set and evaluated by rouge metrics lin in table
the mance of our cascade approaches cascade fusion and cascade tag is comparable to or outranks a number of extractive and abstractive baselines
particularly cascade tag does not use a fusion step and is the output of ne grained content selection
cascade fusion provides a direct parison against bert abs lebanoff et al
that uses sentence selection and fusion but lacks a ne grained content selector
our results suggest that a coarse content selection strategy remains necessary to guide the fusion model to produce informative sentences
we observe that the addition of the fusion model has only a moderate impact on rouge scores but the fusion process can reorder text segments to create true and grammatical sentences as shown in ble
we analyze the performance of a number of oracle models that use ground truth sentence selection gt sent and tagging gt tag
when given ground truth sentences as input our cascade models achieve points of improvement in all rouge metrics
when the models are also given ground truth highlights they achieve an additional points of improvement
in a preliminary amination we observe that not all highlights are included in the summary during fusion indicating there is space for improvement
these results show that cascade architectures have great potential to generate shallow abstracts and future emphasis may be placed on accurate content selection
how much should we highlight it is important to quantify the amount of highlighting required for generating a summary sentence
highlighting too much or too little can be unhelpful
we experiment with three methods to determine the appropriate amount of words to highlight
probability olding chooses a set threshold whereby all words that have a probability higher than the threshold are highlighted
when proportional to input is used the highest probability words are iteratively lighted until a target rate is reached
the amount of highlighting can be proportional to the total ber of words per instance one or two sentences or per document containing all sentences selected for the document
we investigate the effect of varying the amount of highlighting in figure
among the three ods probability thresholding performs the best as it gives more freedom to content selection
if the model scores all of the words in sentences highly then we should correspondingly highlight all of the words
if only very few words score highly then we should only pick those few
highlighting a certain percentage of words tend to perform less well
on our dataset a old value of

produces the best rouge scores
interestingly these thresholds end up lighting of the words of each sentence
compared to what the generator was trained on which had a median of of each sentence lighted the system s rate of highlighting is higher
if the model s highlighting rate is set to be similar to that of the ground truth it yields much lower rouge scores
threshold value of
in ure
this observation suggests that the amount of highlighting can be related to the effectiveness of content selector and it may be better to highlight more than less
conclusion we present a cascade approach to neural tive summarization that separates content selection from surface realization
importantly our approach makes use of text highlights as intermediate resentation they are derived from one or two tences using a coarse content selection egy then passed to a neural text generator to pose a summary sentence
a successful cascade approach is expected to accurately select sentences and highlight an appropriate amount of text both can be customized for domain specic tasks
acknowledgments we are grateful to the anonymous reviewers for their comments and suggestions
this research was supported in part by the national science tion grant
references laura banarescu claire bonial shu cai madalina georgescu kira griftt ulf hermjakob kevin knight philipp koehn martha palmer and nathan schneider

abstract meaning representation for sembanking
in proceedings of the tic annotation workshop and interoperability with discourse pages soa bulgaria
tion for computational linguistics
yen chun chen and mohit bansal

fast tive summarization with reinforce selected sentence rewriting
in proceedings of the annual ing of the association for computational tics volume long papers pages bourne australia
association for computational linguistics
jacob devlin ming wei chang kenton lee and kristina toutanova

bert pre training of deep bidirectional transformers for language derstanding
in proceedings of the conference of the north american chapter of the association for computational linguistics human language technologies volume long and short papers pages minneapolis minnesota
ation for computational linguistics
gunes erkan and dragomir r
radev

lexrank graph based lexical centrality as salience in text journal of articial intelligence summarization
research
tobias falke leonardo f
r
ribeiro prasetya ajie ido dagan and iryna gurevych

utama ranking generated summaries by correctness an teresting but challenging application for natural guage inference
in proceedings of the annual meeting of the association for computational guistics pages florence italy
tion for computational linguistics
katja filippova and michael strube

sentence fusion via dependency graph compression
in ceedings of the conference on empirical ods in natural language processing pages honolulu hawaii
association for tional linguistics
sebastian gehrmann yuntian deng and alexander rush

bottom up abstractive summarization
in proceedings of the conference on pirical methods in natural language processing pages brussels belgium
association for computational linguistics
mor geva eric malmi idan szpektor and jonathan berant

discofuse a large scale dataset in for discourse based sentence fusion
ings of the conference of the north american chapter of the association for computational guistics human language technologies volume long and short papers pages neapolis minnesota
association for computational linguistics
asli celikyilmaz antoine bosselut xiaodong he and yejin choi

deep communicating agents for in proceedings of the abstractive summarization
conference of the north american chapter of the association for computational linguistics man language technologies volume long pers pages new orleans louisiana
association for computational linguistics
han guo ramakanth pasunuru and mohit bansal

autosem automatic task selection and in proceedings of the ing in multi task learning
conference of the north american chapter of the association for computational linguistics man language technologies volume long and short papers pages minneapolis nesota
association for computational linguistics
wan ting hsu chieh kai lin ming ying lee kerui min jing tang and min sun

a unied model for extractive and abstractive summarization in proceedings of the using inconsistency loss
annual meeting of the association for putational linguistics volume long papers pages melbourne australia
association for computational linguistics
baoyu jing zeya wang and eric xing

show describe and conclude on exploiting the structure information of chest ray reports
in proceedings of the annual meeting of the association for computational linguistics pages rence italy
association for computational tics
hongyan jing and kathleen r
mckeown

cut and paste based text summarization
in meeting of the north american chapter of the association for computational linguistics
jia jin koay alexander roustai xiaojin dai alec dillon and fei liu

how domain ogy affects meeting summarization performance
in proceedings of the international conference on computational linguistics coling
anastassia kornilova and vladimir eidelman

billsum a corpus for automatic summarization of us legislation
in proceedings of the workshop on new frontiers in summarization pages hong kong china
association for computational linguistics
wojciech kryscinski nitish shirish keskar bryan cann caiming xiong and richard socher

neural text summarization a critical evaluation
in proceedings of the conference on empirical methods in natural language processing and the international joint conference on natural guage processing emnlp ijcnlp pages hong kong china
association for tional linguistics
wojciech kryscinski romain paulus caiming xiong and richard socher

improving abstraction in text summarization
in proceedings of the conference on empirical methods in natural guage processing pages brussels gium
association for computational linguistics
logan lebanoff john muchovej franck dernoncourt doo soon kim seokhwan kim walter chang and fei liu

analyzing sentence fusion in in proceedings of the stractive summarization
workshop on new frontiers in summarization pages hong kong china
association for computational linguistics
logan lebanoff john muchovej franck dernoncourt doo soon kim lidan wang walter chang and fei liu

understanding points of dence between sentences for abstractive tion
in proceedings of the annual meeting of the association for computational linguistics dent research workshop seattle united states
sociation for computational linguistics
logan lebanoff kaiqiang song franck dernoncourt doo soon kim seokhwan kim walter chang and fei liu

scoring sentence singletons and pairs for abstractive summarization
in proceedings of the annual meeting of the association for computational linguistics pages rence italy
association for computational tics
logan lebanoff kaiqiang song and fei liu

adapting the neural encoder decoder framework in from single to multi document summarization
proceedings of the conference on empirical methods in natural language processing pages brussels belgium
association for computational linguistics
mike lewis yinhan liu naman goyal jan ghazvininejad abdelrahman mohamed omer levy veselin stoyanov and luke zettlemoyer

bart denoising sequence to sequence training for natural language generation translation and comprehension
in proceedings of the nual meeting of the association for computational linguistics pages online
association for computational linguistics
manling li lingyu zhang heng ji and richard j
radke

keep meeting summaries on topic abstractive multi modal meeting summarization
in the proceedings of association for computational linguistics pages florence italy
association for tational linguistics
the annual meeting of chin yew lin

rouge a package for matic evaluation of summaries
in text tion branches out pages barcelona spain
association for computational linguistics
fei liu jeffrey flanigan sam thomson norman sadeh and noah a
smith

toward tive summarization using semantic representations
in proceedings of the conference of the north american chapter of the association for tional linguistics human language technologies pages denver colorado
association for computational linguistics
yang liu and mirella lapata

hierarchical formers for multi document summarization
in ceedings of the annual meeting of the ciation for computational linguistics pages florence italy
association for tional linguistics
yashar mehdad giuseppe carenini frank tompa and raymond t
ng

abstractive meeting marization with entailment and fusion
in ings of the european workshop on natural guage generation pages soa bulgaria
association for computational linguistics
colin raffel noam shazeer adam roberts katherine lee sharan narang michael matena yanqi zhou wei li and peter j
liu

exploring the limits of transfer learning with a unied text to text former


abigail see peter j
liu and christopher d
ning

get to the point summarization in proceedings with pointer generator networks
of the annual meeting of the association for computational linguistics volume long papers pages vancouver canada
association for computational linguistics
jiwei tan xiaojun wan and jianguo xiao

abstractive document summarization with a in proceedings based attentional neural model
of the annual meeting of the association for computational linguistics volume long papers pages vancouver canada
association for computational linguistics
lucy vanderwende hisami suzuki chris brockett and ani nenkova

beyond sumbasic focused summarization with sentence tion and lexical expansion
information processing and management
yuhao zhang derek merck emily bao tsai pher d
manning and curtis p
langlotz

timizing the factual correctness of a summary a study of summarizing radiology reports
in ings of the annual conference of the tion for computational linguistics acl

