conceptual text summarizer a new model in continuous vector space mohammad ebrahim khademi mohammad fakhredanesh seyed mojtaba hoseini faculty of electrical and computer engineering malek ashtar university of technology iran abstract traditional methods of summarization are not cost effective and possible today
extractive summarization is a process that helps to extract the most important sentences from a text automatically and generates a short informative summary
in this work we propose an unsupervised method to summarize persian texts
this method is a novel hybrid approach that clusters the concepts of the text using deep learning and traditional statistical methods
first we produce a word embedding based on corpus and a dictionary of word frequencies
then the proposed algorithm extracts the keywords of the document clusters its concepts and finally ranks the sentences to produce the summary
we evaluated the proposed method on pasokh single document corpus using the rouge evaluation measure
without using any hand crafted features our proposed method achieves state the art results
we compared our unsupervised method with the best supervised persian methods and we achieved an overall improvement of recall score of

keywords extractive text summarization unsupervised learning language independent summarization continuous vector space word embedding introduction automatic text summarization of a large corpus has been a source of concern over the years from two areas of information retrieval and natural language processing
the primary studies in this field began in
baxendale edmundson and luhn have done research in those years
automatic generation of summaries provides a short version of documents to help users in capturing the important contents of the original documents in a tolerable time
now humans produce summaries of documents in the best way
today with the growth of data especially in the big data domain it is not possible to generate all of these summaries manually because it s neither economical nor feasible
there are two types of text summarization based on considering all or a specific part of a document generic summarization provides an overall summary of all information contained in a document
it answers the question what is this document about a document is expected to contain several topics
main topics are discussed extensively by many sentences
minor topics have less sentence support and exist to support the main topics
the specific goals of generic summarization are to choose k number of sentences as specified by the user from a given document that best describe the main topics of the document
to minimize redundancy of the chosen sentences
query relevant summarization is specific to information retrieval applications
it attempts to summarize the information a document contains pertaining to a specific search term
the summary indicates what this document says about the query
the snippets bellow each result returned by a search engine is a common example for this type
in addition there are two approaches to text summarization based on the chosen process of generating the summary extractive summarization this approach of summarization selects a subset of existing words phrases or sentences in the original text to form the summary
there are of course limitations on choosing these pieces
one of these limitations which is common in summarization is output summary length
abstractive summarization this approach builds an internal semantic representation and then uses natural language generation techniques to create a summary that is expected to be closer to what the text want to express
based on the current limitations of natural language processing methods extractive approach is the dominant approach in this field
almost all extractive summarization methods encounter two key problems in assigning scores to text pieces choosing a subset of the scored pieces hitherto text summarization has traveled a very unpaved path
in the beginning frequency based approaches were utilized for text summarization
then lexical chain based approaches came to succeed with the blessing of using large lexical databases such as wordnet and farsnet
hence valid methods such as latent semantic analysis lsa based approaches that do not use dedicated static sources requires trained human forces for producing became more prominent
word embedding models learn the continuous representation of words in a low dimensional space
in lexical semantics linear dimension reduction methods such as latent semantic analysis have been widely used
non linear models can be used to train word embedding models
word embedding models not only have a better performance but also lacks many problems of linear dimension reduction methods such as latent semantic analysis
in this paper a novel method of extractive generic document summarization based on perceiving the concepts present in sentences is proposed
therefore after unsupervised learning of the target language word embedding input document concepts are clustered based on the learned word feature vectors hence the proposed method is language independent
after allocating scores to each conceptual cluster sentences are ranked and selected based on the significance of the concepts present in each sentence
ultimately we achieved promising results on pasokh benchmark corpus
the structure of the paper is as follows
section two describes some related works
section three presents the summary generation process
section four outlines evaluation measures and experimental results
section five concludes the paper and discusses the avenues for future research
related works hitherto text summarization has traveled a very unpaved path
in the beginning frequency based approaches were utilized for text summarization
then lexical chain based approaches came to succeed with the blessing of using large lexical databases such as wordnet and farsnet
since the most common subject in the text has an important role in summarization and lexical chain is a better criterion than word frequency for identifying the subject of text as a result a more discriminating diagnosis of the subject of text was made possible which was a further improvement in summarization
however the great reliance of these methods on lexical databases such as wordnet or farsnet is the main weakness of these methods
for the success of these methods depends on enriching and keeping up to date the vocabulary of these databases that is very costly and time consuming removing this weakness is not feasible
hence valid methods such as latent semantic analysis lsa based approaches that do not use dedicated static sources requires trained human forces for producing became more prominent
latent semantic analysis is a valid unsupervised method for an implicit representation of the meaning of the text based on the co occurence of words in the input document
this method is unsupervised and it is considered an advantage
but this method has many other problems the dimensions of the matrix changes very often new words are added very frequently and corpus changes in size
the matrix is extremely sparse since most words do not co occur
the matrix is very high dimensional in general quadratic cost to train i
e
to perform svd many of natural language processing nlp systems and methods consider words as separate units
in such systems the similarity between words is not defined and words are considered as indexes of a dictionary
this approach is generally adopted for the following reasons simplicity reliability advantage of training large data volume over using complex models as for the third reason according to past observations and experiences in general simple models that are trained on a vast amount of data are more effective than complex models that are trained on quantitative data
today we can assume that n gram models can be trained on all existing data billions of words
with the advent of machine learning methods in recent years training more complex models on much larger datasets has become possible
lately the advancement in computing power of gpus and new processors have made it possible for hardwares to implement these more advanced models
one of the most successful of these cases in recent years is the use of the distributed representation of vocabularies
word embedding model was developed by bengio et al
more than a decade ago
the word embedding model w is a function that maps the words of a language into vectors with about to dimensions
to initialize w random vectors are assigned to words
this model learns meaningful vectors for doing some tasks
in lexical semantics linear dimension reduction methods such as latent semantic analysis have been widely used
non linear models can be used to train word embedding models
word embedding models not only have a better performance but also lacks many problems of linear dimension reduction methods such as latent semantic analysis
distributed representation of vocabularies word embedding is one of the important research topics in the field of natural language processing
this method which in fact is one of the deep learning branches has been widely used in various fields of natural language processing in recent years
among these we can mention the following neural language model sequence tagging machine translation contrasting meaning bengio et al
mikolov et al
and schwenk have shown that neural network based language models have produced much better results than n gram models
although many text summarization methods are available for languages such as english little work is done in devising methods of summarizing persian texts
in general these methods can be categorized as supervised and unsupervised while most of the proposed methods so far have been of the former type
supervised summarization methods presented for persian documents are divided into four categories of heuristic lexical chain based graph based and machine learning or mathematical based methods heuristic method hassel and mazdak proposed farsisum as a heuristic method
it is one of the first attempts to create an automatic text summarization system for persian
the system is implemented as a http client server application written in perl
it has used modules implemented in swesum dalianis a persian stop list in unicode format and a small set of heuristic rules
lexical chain based methods zamanifar et al
proposed a new hybrid summarization technique that combined term co occurrence property and conceptually related feature of farsi language
they consider the relationship between words and use a synonym dataset to eliminate similar sentences
their results show better performance in comparison with farsisum
shamsfard et al
proposed parsumist
they presented single document and document summarization methods using lexical chains and graphs
to rank and determine the most important sentence they consider the highest similarity with other sentences the title and keywords
they achieved better performance than farsisum
zamanifar and kashefi proposed azom a summarization approach that combines statistical and conceptual text properties and in regards of document structure extracts the summary of text
azom performes better than three common structured text summarizers fractal yang flat summary and co occurrence
shafiee and shamsfard proposed a single multi document summarizer using a novel clustering method to generate text summaries
it consists of three phases first a feature selection phase is employed
then farsnet a persian wordnet is utilized to extract the semantic information of words
finally the input sentences are clustered
their proposed method is compared with three known available text summarization systems and techniques for persian language
their method obtains better results than farsisum parsumist and ijaz
graph based method shakeri et al
proposed an algorithm based on the graph theory to select the most important sentences of the document
they explain their objective as the aim of this method is to consider the importance of sentences independently and at the same time the importance of the relationship between them
thus the sentences are selected to attend in the final summary contains more important subjects and also have more contact with other sentences
evaluation results indicate that the output of proposed method improves precision recall and metrics in comparison with farsisum
machine learning and mathematical based methods kiyomarsi and rahimi proposed a new method for summarizing persian texts based on features available in persian language and the use of fuzzy logic
their method obtains better results as compared with four previous methods
tofighy et al
proposed a new method for persian text summarization based on fractal theory whose main goal is using hierarchical structure of document to improve the summarization quality of persian texts
their method achieved a better performance than farsisum but weaker than azom
bazghandi et al
proposed a textual summarization system based on sentence clustering
collective intelligence algorithms are used for optimizing the methods
these methods rely on semantic aspect of words based on their relations in the text
their results is comparable to traditional clustering approaches
tofighi et al
proposed an analytical hierarchy process ahp technique for persian text summarization
the proposed model uses the analytical hierarchy as a base factor for an evaluation algorithm
their results show better performance in comparison with farsisum
pourmasoumi et al
proposed a persian single document summarization system called ijaz
it is based on weighted least squares method
their results proved a better performance as compared with farsisum
they also proposed pasokh a popular corpus for evaluation of persian text summarizers
as an unsupervised method honarpisheh et al
proposed a new multi document multi lingual text summarization method based on singular value decomposition svd and hierarchical clustering
success of lexical chain based methods and supervised machine learning methods depends on enriching and keeping up to date lexical databases and training labeled datasets respectively that is very costly and time consuming
these methods often use language dependent features and can not be generalized to other languages
on the other hand unsupervised methods such as svd based methods have many problems that are mentioned in the beginning of this section
the proposed generic extractive method is a novel method that not only is unsupervised but also does not have many problems of svd based methods and without using any hand crafted features achieves much better performance compared to supervised methods
proposed algorithm in this section we propose a novel method of extractive generic document summarization based on perceiving the concepts present in sentences
it is an unsupervised and language independent method that does not have many problems of svd based methods
for this purpose firstly the necessary preprocesses are performed on the corpus texts
subsequently the persian word embedding is created by unsupervised learning of corpus
then the input document keywords are extracted
afterward the input document concepts are clustered based on the learned word feature vectors hence the proposed method can be generalized to other languages and the score of each of these conceptual clusters are calculated
finally the sentences are ranked and selected based on the significance of the concepts present in each sentence
the chart of this method is presented in figure conceptual text summarizer
the following sections will be described based on this chart
figure conceptual text summarizer
text pre processing to learn a persian language model we use
we need to produce a dictionary of vocabularies of
to do this we tokenize the words of each text file of the corpus using hazm library
hazm is an applicable open source natural language processing library in persian
then we compose a dictionary out of these words by counting the frequency of each word throughout the corpus
this dictionary will be used in succeeding steps
we constitute a complete list of persian stopwords out of frequent words in the prepared dictionary along with stopword lists in other open source projects

unsupervised learning of persian word embedding the corpus has text files in unlabeled text sections
each of these files is a concatenation of hundreds of news and articles
these news and articles are from different fields of cultural political social
to construct a suitable persian word embedding set we use cbow model
this model is a neural network with one hidden layer
to learn the model a small window moves across the corpus texts and the network tries to predict the central word of the window using the words around it
we assume a window with nine words length and it goes across the unlabeled texts of corpus to learn the weights of the network as persian word embedding vectors
the first and the last four words of each window is assumed to be the input of the network
the central word of the window is assumed to be the label of the output
thus we have a rich labeled dataset
completing the learning process of network weights on all windows of we will have a suitable persian word embedding set whose words dimension is equal to the size of the hidden layer of the network
the hidden layer size is assumed to be in this work
the persian word embedding generated at this stage maps every words of the to a vector in a dimensional vector space
the generated persian word embedding set contains words
the t sne method for visualization can be used to better understand the word embedding environment
figure a persian word embedding visualization using t sne method
part of the words of one of the texts of the pasokh corpus visualized in this figure in the mapping of the words in figure a persian word embedding visualization using t sne method
part of the words of one of the texts of the pasokh corpus visualized in this figure similare words are closer to each other
this issue can also be examined from other dimensions as another example in figure the closest vocabulary to the header terms is given using the proposed persian word embedding generated in this work the closest vocabularies to the header terms is given using the proposed persian word embedding generated in this work
figure the closest vocabulary to the header terms is given using the proposed persian word embedding generated in this work in the proposed method using the relationship between words the concepts of the input document are represented
in this method the importance of sentences is determined using semantic and syntactic similarities between words
and instead of using single words to express concepts multiple similar words are used
for example the occurrence of words computer keyboard display mouse and printer even though they are not frequently repeated singly in the input document express a certain concept
as stated in the introduction the great reliance of lexical chain based methods on lexical databases is the main weakness of these methods
at this stage to remove this weakness an appropriate word embedding for summarization is created that encompasses the semantic and syntactic communication of the words in a broader and more up to date lexical range than that of lexical databases
the word embedding presented in this work is able to discover relationships present in the outside world that do not exist in common vocabulary databases
for example this word embedding can detect the relation between the words of mashhad neyshabur and khorasan
mashhad is the capital of khorasan province and neyshabur is one of the cities of this province figure the closest vocabulary to the header terms is given using the proposed persian word embedding generated in this work
the common vocabulary databases that can not discover such relationships are comprehensive lexical databases that carry different meanings for each word along with relationships between them such as synonyms antonyms part of containing or more general more specific relationships
but their construction is manual costly and time consuming

extracting the keywords of the document for extracting the keywords of the input document we first tokenized the words of the document using hazm tokenizer
the words of the document tokenized using then we excluded stopwords from input document tokens
the score of each word of the input document calculated using equation where w is the intended word tf calculated from equation point ij idf i ij max where ij is frequency of the i th word in the j th document and max ij of the words in the input document
the tf is normalized using this division
is maximum frequency finally idf in equation was calculated from equation where n is the number of documents of the corpus and ni documents in the corpus that the i th word has been observed there
idfi n is the number of if a word is not in the there will not be a score for it
also due to the absence of a vector in the continuous vector space for this word it is deleted from the decision making cycle
therefore learning word embedding on a richer persian corpus will cause to increase the accuracy of the method

clustering concepts in this phase the concepts present in the input document are constructed using the persian word embedding obtained in section

for this purpose
first we sort the keywords of the previous phase according to their calculated scores

then we map all input document terms into a dimensional space using the prepared persian word embedding
we cluster the concepts of this document into ten different clusters using k means algorithm we consider the ten preferred keywords selected in section
as the primary cluster centers and cluster the entire words of the input document
each obtained cluster can be considered as a concept
thus ten key concepts of the document are constructed
cluster or concept
finally we consider the nearest word to each cluster center as the criterion word for that the total score of each concept is calculated using the equation point w c point w where w is the word c is the concept and is the total score of each word that was calculated based on equation
the indicates the closeness of each word in the intended concept to the concept s criterion word
therefore the words nearer to the concept s criterion word will have larger linear coefficients and the words farther to that criterion word will have smaller linear coefficients
thus the nearness of each word to its concept s criterion word affects the final score of the concept
hence repetition of more closely situated words in the input document will result in a higher score than repetition of farther words

sentence ranking for ranking sentences the following steps are taken first the input document is read line by line and the sentences of each line are separated using hazm sentence tokenizer
for scoring extracted sentences equation is used score point c w s n where s is a sentence n is its number of words and is the score of the the intended word s concept
by dividing the sentence score into its number of words we normalized the obtained score so that shorter and longer sentences would have equal chance of selection
sentences are sorted according to their normalized scores
according to the desired summary length some sentences with the highest score are selected and are displayed in the order they appear in the document
experimental results in this section using rouge criterion our system generated summaries on single document pasokh corpus is evaluated and the obtained results are compared with other available persian summarizers

evaluation measures rouge n is a measure for evaluation of summarizations
this recall based measure is very close to human evaluation of summaries
this measure calculates the number of common n grams between the system generated summaries and the reference human made summaries
it s therefore a suitable measure for automatically evaluating summaries produced in all languages
for this work two public rouge evaluation tools are studied
rouge is a perl implementation of rouge measure that was developed by mr
c
lin et al
at the university of southern california
this implementation dose not support unicode and it generates unrealistic results for the persian summary evaluation
after obtaining the exaggerated results of this tool for persian summaries we realized this great weakness

rouge is a java implementation of rouge n measure developed by rxnlp team and is publicly accessible
this tool supports unicode and the obtained results are accurate but it has only implemented rouge n and not any other variations of rouge measure
in this work a python implementation of rouge n was developed based on mr
c
lin s paper
this tool supports unicode and verifies the results of the implementation
according to the above descriptions the is used for summary evaluation in this study

pasokh corpus pasokh is a popular corpus for the evaluation of persian text summarizers
this dataset consists of a large number of persian news documents on various topics
it contains human written summaries of the documents in the forms of single document multi document extractive and abstractive summaries
the single document dataset of pasokh contains persian news texts that five extractive and five abstractive summaries for each of these news are generated by different human agents
one hundred news texts of the single document pasokh dataset were summarized using the proposed algorithm in this work
the compression ratio of our system summaries was percent
then we needed to calculate rouge n between each of our system generated summaries and the related pasokh extractive reference summaries human made summaries
for this purpose rouge
java implementation tool was used which is mentioned in evaluation tool section earlier
the average of the rouge n is considered as the evaluation of each of our system summaries
finally the average of system summary evaluations was calculated as the final evaluation result
it should be noted that the news headlines of pasokh corpus has not been used in summarization process and the results are obtained without taking advantage of headlines
pourmasoumi et al
presented ijaz as an extractive single document summarizer of persian news in which is available online
in this experiment one hundred news texts of the pasokh corpus were summarized using ijaz summarizer
the compression ratio was percent and the results were obtained without using headlines
the results are reported in table scores percent on pasokh single document dataset table scores percent on pasokh single document dataset and table scores percent on pasokh single document dataset
score shafiee and shamsfard method our proposed method systems ijaz systems ijaz shafiee and shamsfard method our proposed method

















table scores percent on pasokh single document dataset score table scores percent on pasokh single document dataset score systems ijaz shafiee and shamsfard method our proposed method








table scores percent on pasokh single document dataset thus our proposed method in this work has the following advantages over pourmasoumi et al
method our proposed method achieves much better results than the proposed method of pourmasoumi et al
in all and measures
the method proposed by pourmasoumi et al
has taken a supervised learning approach while our learning approach is unsupervised
as defined by authorities supervised learning requires that the algorithm s possible outputs are already known and that the data used to train the algorithm is already labeled with correct answers
while unsupervised machine learning is more closely aligned with what some call true artificial intelligence the idea that a computer can learn to identify complex processes and patterns without a human to provide guidance along the way
although unsupervised learning is prohibitively complex for some simpler enterprise use cases it opens the doors to solving problems that humans normally would not tackle
their proposed method is a persian specific method while our proposed method can be generalized to other languages
shafiee and shamsfard proposed an approach in extractive single document persian summarization in
unfortunately neither their summarizer nor summaries generated by their proposed algorithm are available for comparison therefore the algorithm has been implemented
in this experiment one hundred news texts of the pasokh corpus were summarized using developed summarizer
the compression ratio was percent and the results were obtained using headlines
the results are reported in table scores percent on pasokh single document dataset table scores percent on pasokh single document dataset and table scores percent on pasokh single document dataset
our approach has the following advantages over shafiee and shamsfard s approach our proposed method achieves much better results than the number of similar and related sentences method of shafiee and shamsfard in all and measures
shafiee and shamsfard s method is supervised while ours is unsupervised
in order to calculate a feature s weight they utilize one third of the pasokh single document corpus
to compute a feature s weight the mean of f measure scores is calculated to be considered as the final weight of the selected feature for single document summarization
their proposed method depends on enriching and keeping up to date the farsnet lexical database that is very costly and time consuming while our method depends on unsupervised learning of the target language word embedding
their proposed method is a persian specific method while our proposed method can be generalized to other languages
their method has used the news headlines in the summarization process while our method has obtained the results without using headlines
hassel and mazdak created farsisum in as one of the first persian text summarizers reported in related literature
the available version of farsisum summarizer in their website has a number of bugs
for example the length of the summary farsisum produces has a significant difference with the requested compression ratio percentage
according to previous studies the results of our proposed method on pasokh corpus are much higher than the results obtained by farsisum summarizer
conclusion in this paper a novel method of extractive generic document summarization based on perceiving the concepts present in sentences is proposed
therefore after unsupervised learning of the target language word embedding input document concepts are clustered based on the learned word feature vectors hence the proposed method can be generalized to other languages
after allocating scores to each conceptual cluster sentences are ranked and selected based on the significance of the concepts present in each sentence
one of the most important challenges in recent researches in the field of summarizing persian texts is the lack of a rich lexical database in persian language that can be used to measure semantic similarities
in this research by constructing a persian word embedding using we were able to correctly answer this shortage and provide a new method for summarizing the texts according to the semantic and syntactic relations learned
using the relationship between words the concepts discussed in the input document are represented
in this method the importance of sentences is determined using semantic and syntactic similarities between words
instead of using single words to express concepts different related words are used
we evaluated the proposed method on pasokh single document dataset using the rouge evaluation mesure
without using any hand crafted features our proposed method achieves state of the art results
for system summaries generated with percent compression ratio on pasokh single document corpus and recall scores were and percent respectively
evaluation of our proposed method for summarization of other languages is suggested for future works
learning word embedding on richer persian corpuses may be effective in increasing the accuracy of our method
using pagerank algorithm to produce the concept similarity graph and to find more significant concepts may also increase the accuracy of our concept selection algorithm
using exploited mmr maximum marginal relevance greedy algorithm in sentence selection process may decrease the redundancy of the selected sentences in our proposed method
references p
b
baxendale machine made index for technical literature an experiment ibm j
res
dev
vol
no
pp

h
p
edmundson new methods in automatic extracting j
acm jacm vol
no
pp
h
p
luhn the automatic creation of literature abstracts ibm j
res
dev
vol
no
pp
h
khanpour sentence extraction for summarization and notetaking university of malaya


a
berger and v
o
mittal query relevant summarization using faqs in proceedings of the annual meeting on association for computational linguistics stroudsburg pa usa pp

j
tang l
yao and d
chen multi topic based query oriented summarization in proceedings of the siam international conference on data mining pp

w
song l
c
choi s
c
park and x
f
ding fuzzy evolutionary optimization modeling and its applications to unsupervised categorization and extractive summarization expert syst
appl
vol
no
pp

f
jin m
huang and x
zhu a comparative study on ranking and selection strategies for document summarization in proceedings of the international conference on computational linguistics posters pp

g
a
miller wordnet a lexical database for english commun
acm vol
no
pp
m
shamsfard developing farsnet a lexical ontology for persian in global wordnet
conference szeged hungary
m
shamsfard et al
semi automatic development of farsnet the persian wordnet in proceedings of global wordnet conference mumbai india vol

t
brants a
c
popat p
xu f
j
och and j
dean large language models in machine translation in in proceedings of the joint conference on empirical methods in natural language processing and computational natural language learning
g
e
hinton j
l
mcclelland and d
e
rumelhart distributed representations parallel distributed processing explorations in the microstructure of cognition vol
foundations
mit press cambridge ma
y
bengio r
ducharme p
vincent and c
jauvin a neural probabilistic language model j
mach
learn
res
vol
no
feb pp

p
d
turney and p
pantel from frequency to meaning vector space models of semantics j
artif
intell
res
vol
pp

s
t
roweis and l
k
saul nonlinear dimensionality reduction by locally linear embedding science vol
no
pp

j
b
tenenbaum v
de silva and j
c
langford a global geometric framework for nonlinear dimensionality reduction science vol
no
pp

h
schwenk continuous space language models comput
speech lang
vol
no
pp

r
collobert j
weston l
bottou m
karlen k
kavukcuoglu and p
kuksa natural language processing almost from scratch j
mach
learn
res
vol
no
aug pp

r
collobert and j
weston a unified architecture for natural language processing deep neural networks with multitask learning in proceedings of the international conference on machine learning pp

j
devlin r
zbib z
huang t
lamar r
m
schwartz and j
makhoul fast and robust neural network joint models for statistical machine translation
in acl pp

i
sutskever o
vinyals and q
v
le sequence to sequence learning with neural networks in advances in neural information processing systems pp

z
chen et al
revisiting word embedding for contrasting meaning
in acl pp

t
mikolov a
deoras s
kombrink l
burget and j
ernock empirical evaluation and combination of advanced language modeling techniques in twelfth annual conference of the international speech communication association
m
hassel and n
mazdak farsisum a persian text summarizer in proceedings of the workshop on computational approaches to arabic script based languages pp

a
zamanifar b
minaei bidgoli and m
sharifi a new hybrid farsi text summarization technique based on term co occurrence and conceptual property of the text in software engineering artificial intelligence networking and parallel distributed computing

ninth acis international conference on pp

m
shamsfard t
akhavan and m
e
joorabchi persian document summarization by parsumist world appl
sci
j
vol
pp

a
zamanifar and o
kashefi azom a persian structured text summarizer nat
lang
process
inf
syst
pp

f
shafiee and m
shamsfard similarity versus relatedness a novel approach in extractive persian document summarisation j
inf
sci


h
shakeri s
gholamrezazadeh m
a
salehi and f
ghadamyari a new graph based algorithm for persian text summarization in computer science and convergence springer pp

f
kiyomarsi and f
r
esfahani optimizing persian text summarization based on fuzzy logic approach in international conference on intelligent building and management
m
tofighy o
kashefi a
zamanifar and h
h
s
javadi persian text summarization using fractal theory in international conference on informatics engineering and information science pp

m
bazghandi g
t
tabrizi m
v
jahan and i
mashahd extractive summarization of farsi documents based on pso clustering jia vol
p

s
m
tofighy r
g
raj and h
h
s
javad ahp techniques for persian text summarization malays
j
comput
sci
vol
no
pp

p
asef k
mohsen t
s
ahmad e
ahmad and q
hadi ijaz an operational system for single document summarization of persian news texts vol
no
pp
jan

t
strutz data fitting and uncertainty a practical introduction to weighted least squares and beyond
vieweg and teubner
b
b
moghaddas m
kahani s
a
toosi a
pourmasoumi and a
estiri pasokh a standard corpus for the evaluation of persian text summarizers in computer and knowledge engineering iccke international econference on pp

m
a
honarpisheh g
ghassem sani and s
a
mirroshandel a multi document lingual automatic summarization system
in ijcnlp pp

hamshahri a standard persian text collection sciencedirect

available
sciencedirect
com science article pii
accessed
hazm python library for digesting persian text
sobhe
t
mikolov i
sutskever k
chen g
s
corrado and j
dean distributed representations of words and phrases and their compositionality in advances in neural information processing systems c
j
c
burges l
bottou m
welling z
ghahramani and k
q
weinberger eds
curran associates inc
pp

j
leskovec a
rajaraman and j
d
ullman mining of massive datasets
cambridge university press
c
lin rouge a package for automatic evaluation of summaries
rxnlp
java implementation of rouge for evaluation of summarization tasks
stemming stopwords and unicode support


