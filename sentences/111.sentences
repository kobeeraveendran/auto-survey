p e s l c
s c v
v i x r a deconvolutional paragraph representation learning yizhe zhang dinghan shen guoyin wang zhe gan ricardo henao lawrence carin department of electrical computer engineering duke university abstract learning latent representations from long text sequences is an important rst step in many natural language processing applications
recurrent neural networks rnns have become a cornerstone for this challenging task
however the ity of sentences during rnn based decoding reconstruction decreases with the length of the text
we propose a sequence to sequence purely convolutional and deconvolutional autoencoding framework that is free of the above issue while also being computationally efcient
the proposed method is simple easy to implement and can be leveraged as a building block for many applications
we show empirically that compared to rnns our framework is better at ing and correcting long paragraphs
quantitative evaluation on semi supervised text classication and summarization tasks demonstrate the potential for better utilization of long unlabeled text data
introduction a central task in natural language processing is to learn representations features for sentences or multi sentence paragraphs
these representations are typically a required rst step toward more applied tasks such as sentiment analysis machine translation dialogue systems and text summarization
an approach for learning sentence representations from data is to leverage an encoder decoder framework
in a standard autoencoding setup a vector representation is rst encoded from an embedding of an input sequence then decoded to the original domain to reconstruct the input sequence
recent advances in recurrent neural networks rnns especially long short term memory lstm and variants have achieved great success in numerous tasks that heavily rely on sentence representation learning
rnn based methods typically model sentences recursively as a generative markov process with hidden units where the one step ahead word from an input sentence is generated by conditioning on previous words and hidden units via emission and transition operators modeled as neural networks
in principle the neural representations of input sequences aim to encapsulate sufcient information about their structure to subsequently recover the original sentences via decoding
however due to the recursive nature of the rnn challenges exist for rnn based strategies to fully encode a sentence into a vector representation
typically during training the rnn generates words in sequence conditioning on previous ground truth words i
e
teacher forcing training rather than decoding the whole sentence solely from the encoded representation vector
this teacher forcing strategy has proven important because it forces the output sequence of the rnn to stay close to the ground truth sequence
however allowing the decoder to access ground truth information when reconstructing the sequence weakens the encoder s ability to produce self contained representations that carry enough information to steer the decoder through the decoding process without additional guidance
aiming to solve this problem proposed a scheduled sampling approach during training which gradually shifts from learning via both latent representation and ground truth signals to solely use the encoded latent representation
unfortunately showed that scheduled sampling is a fundamentally inconsistent training strategy in that it produces largely unstable results in practice
as a result training may fail to converge on occasion
during inference for which ground truth sentences are not available words ahead can only be ated by conditioning on previously generated words through the representation vector
consequently decoding error compounds proportional to the length of the sequence
this means that generated sentences quickly deviate from the ground truth once an error has been made and as the sentence progresses
this phenomenon was coined exposure bias in
we propose a simple yet powerful purely convolutional framework for learning sentence tions
conveniently without rnns in our framework issues connected to teacher forcing training and exposure bias are not relevant
the proposed approach uses a convolutional neural network cnn as encoder and a deconvolutional i
e
transposed convolutional neural network as decoder
to the best of our knowledge the proposed framework is the rst to force the encoded latent representation to capture information from the entire sentence via a multi layer cnn specication to achieve high reconstruction quality without leveraging rnn based decoders
our multi layer cnn allows representation vectors to abstract information from the entire sentence irrespective of order or length making it an appealing choice for tasks involving long sentences or paragraphs
further since our framework does not involve recursive encoding or decoding it can be very efciently parallelized using convolution specic graphical process unit gpu primitives yielding signicant computational savings compared to rnn based models
convolutional auto encoding for text modeling
convolutional encoder let wt denote the t th word in a given sentence
each word wt is embedded into a k dimensional word vector xt where we is a learned word embedding matrix v is the vocabulary size and denotes the v th column of we
all columns of we are normalized to have unit norm i
e
v by dividing each column with its norm
after embedding a sentence of length t padded where necessary is represented as x rkt by concatenating its word embeddings i
e
is the t th column of x
for sentence encoding we use a cnn architecture similar to though originally proposed for image data
the cnn consists of l layers l convolutional and the lth fully connected that ultimately summarize an input sentence into a xed length latent representation vector h
layer l


l consists of pl lters learned from data
for the i lter in layer a convolutional operation with stride length applies lter rkh to x where h is the convolution lter size
this yields latent feature map c where is a nonlinear activation function and denotes the convolutional operator
in our experiments is represented by a rectied linear unit relu
note that the original embedding dimension k changes after the rst convolutional layer as for i



concatenating the results from lters for layer results in feature map



after this rst convolutional layer we apply the convolution operation to the feature map using the same lter size h with this repeated in sequence for l layers
each time the length along the spatial coordinate is reduced to t l where is the stride length t l is the spatial length l denotes the l th layer and is the oor function
for the nal layer l the feature map is fed into a fully connected layer to produce the latent representation h
implementation wise we use a convolutional layer with lter size equals to t regardless of h which is equivalent to a fully connected layer this implementation trick has been also utilized in
this last layer summarizes all remaining spatial coordinates t into scalar features that encapsulate sentence sub structures throughout the entire sentence characterized by lters l for i


and l


l where l denotes lter i for layer l
this also implies that the extracted feature is of xed dimensionality independent of the length of the input sentence
c figure convolutional auto encoding architecture
encoder the input sequence is rst expanded to an embedding matrix x then fully compressed to a representation vector h through a multi layer convolutional encoder with stride
in the last layer the spatial dimension is collapsed to remove the spatial dependency
decoder the latent vector h is fed through a multi layer deconvolutional decoder with stride to reconstruct x as x via cosine similarity cross entropy loss
having pl lters on the last layer results in pl dimensional representation vector h for the input sentence
for example in figure the encoder consists of l layers which for a sentence of length t embedding dimension stride lengths lter sizes h and number of lters results in intermediate feature maps and of sizes respectively
the last feature map of size corresponds to latent representation vector h
conceptually lters from the lower layers capture primitive sentence information h grams gous to edges in images while higher level lters capture more sophisticated linguistic features such as semantic and syntactic structures analogous to image elements
such a bottom up architecture models sentences by hierarchically stacking text segments h grams as building blocks for sentation vector h
this is similar in spirit to modeling linguistic grammar formalisms via concrete syntax trees however we do not pre specify a tree structure based on some syntactic structure i
e
english language but rather abstract it from data via a multi layer convolutional network

deconvolutional decoder we apply the deconvolution with stride i
e
convolutional transpose as the conjugate operation of convolution to decode the latent representation h back to the source discrete text domain
as the deconvolution operation proceeds the spatial resolution gradually increases by mirroring the convolutional steps described above as illustrated in figure
the spatial dimension is rst expanded to match the spatial dimension of the l layer of convolution then progressively expanded as t t l h for l up to l deconvolutional layer which corresponds to the input layer of the convolutional encoder
the output of the l layer deconvolution operation aims to reconstruct the word embedding matrix which we denote as x
in line with word embedding matrix we columns of x are normalized to have unit norm
denoting wt as the t th word in reconstructed sentence s the probability of wt to be word v is specied as p wt v where y is the cosine similarity dened as is the v th column of we xt is the t th column of x is a positive number we denote as temperature parameter
this parameter is akin to the concentration parameter of a dirichlet distribution in that it controls the spread of probability vector wt


p wt v thus a large encourages uniformly distributed probabilities whereas a small encourages sparse concentrated probability values
in the experiments we set

note that in our setting the cosine similarity can be obtained as an inner product provided that columns of we and x have unit norm by specication
layersconvolution h h
model learning the objective of the convolutional autoencoder described above can be written as the word wise log likelihood for all sentences s d i
e
lae t log p wt dd where d denotes the set of observed sentences
the simple maximum likelihood objective in is optimized via stochastic gradient descent
details of the implementation are provided in the experiments
note that differs from prior related work in two ways use pooling and un pooling operators while we use convolution deconvolution with stride and more importantly do not use a cosine similarity reconstruction as in but a rnn based decoder
a further discussion of related work is provided in section
we could use pooling and un pooling instead of striding a particular case of deterministic pooling un pooling however in early experiments not shown we did not observe signicant performance gains while convolution deconvolution operations with stride are considerably more efcient in terms of memory footprint
compared to a standard lstm based rnn sequence autoencoders with roughly the same number of parameters computations in our case are considerably faster see experiments using single nvidia titan x gpu
this is due to the high parallelization efciency of cnns via cudnn primitives
comparison between deconvolutional and rnn decoders the proposed framework can be seen as a complementary building block for natural language modeling
contrary to the standard based decoder the deconvolutional decoder imposes in general a less strict sequence dependency compared to rnn architectures
specically generating a word from an rnn requires a vector of hidden units that recursively accumulate information from the entire sentence in an order preserving manner long term dependencies are heavily down weighted while for a deconvolutional decoder the generation only depends on a representation vector that encapsulates information from throughout the sentence without a pre specied ordering structure
as a result for language generation tasks a rnn decoder will usually generate more coherent text when compared to a deconvolutional decoder
on the contrary a deconvolutional decoder is better at accounting for distant dependencies in long sentences which can be very benecial in feature extraction for classication and text summarization tasks

semi supervised classication and summarization identifying related topics or sentiments and abstracting short summaries from user generated content such as blogs or product reviews has recently received signicant interest
in many practical scenarios unlabeled data are abundant however there are not many practical cases where the potential of such unlabeled data is fully realized
motivated by this opportunity here we seek to complement scarcer but more valuable labeled data to improve the generalization ability of supervised models
by ingesting unlabeled data the model can learn to abstract latent representations that capture the semantic meaning of all available sentences irrespective of whether or not they are labeled
this can be done prior to the supervised model training as a two step process
recently rnn based methods exploiting this idea have been widely utilized and have achieved state of the art performance in many tasks
alternatively one can learn the autoencoder and classier jointly by specifying a classication model whose input is the latent representation see for instance
in the case of product reviews for example each review may contain hundreds of words
this poses challenges when training rnn based sequence encoders in the sense that the rnn has to abstract information on as it moves through the sentence which often leads to loss of information particularly in long sentences
furthermore the decoding process uses ground truth information during training thus the learned representation may not necessarily keep all information from the input text that is necessary for proper reconstruction summarization or classication
we consider applying our convolutional autoencoding framework to semi supervised learning from long sentences and paragraphs
instead of pre training a fully unsupervised model as in we cast the semi supervised task as a multi task learning problem similar to i
e
we simultaneously train a sequence autoencoder and a supervised model
in principle by using this joint training strategy the learned paragraph embedding vector will preserve both reconstruction and classication ability
specically we consider the following objective t log p wt lsemi ddl hd yd where is an annealing parameter balancing the relative importance of supervised and pervised loss dl and du denote the set of labeled and unlabeled data respectively
the rst term in is the sequence autoencoder loss in for the th sequence
lsup is the supervision loss for the th sequence labeled only
the classier function that attempts to reconstruct yd from hd can be either a multi layer perceptron mlp in classication tasks or a cnn rnn in text summarization tasks
for the latter we are interested in a purely convolutional specication however we also consider an rnn for comparison
for classication we use a standard cross entropy loss and for text summarization we use either for the cnn or the standard lstm loss for the rnn
in practice we adopt a scheduled annealing strategy for as in rather than xing it a priori as in
during training gradually transits from focusing solely on the unsupervised sequence autoencoder to the supervised task by annealing from to a small positive value min
we set min
in the experiments
the motivation for this annealing strategy is to rst focus on abstracting paragraph features then to selectively rene learned features that are most informative to the supervised task
related work previous work has considered leveraging cnns as encoders for various natural language processing tasks
typically cnn based encoder architectures apply a single convolution layer followed by a pooling layer which essentially acts as a detector of specic classes of h grams given a convolution lter window of size h
the deep architecture in our framework will in principle enable the high level layers to capture more sophisticated language features
we use convolutions with stride rather than pooling operators e

max pooling for spatial downsampling following where it is argued that fully convolutional architectures are able to learn their own spatial downsampling
further uses a layer cnn for text classication
our cnn encoder is considerably simpler in structure convolutions with stride and no more than layers while still achieving good performance
language decoders other than rnns are less well studied
recently proposed a hybrid model by coupling a convolutional deconvolutional network with an rnn where the rnn acts as decoder and the deconvolutional model as a bridge between the encoder convolutional network and decoder
additionally considered cnn variants such as pixelcnn for text generation
nevertheless to achieve good empirical results these methods still require the sentences to be generated sequentially conditioning on the ground truth historical information akin to rnn based decoders thus still suffering from the exposure bias
other efforts have been made to improve embeddings from long paragraphs using unsupervised approaches
the paragraph vector learns a xed length vector by concatenating it with a embedding of history sequence to predict future words
the hierarchical neural autoencoder builds a hierarchical attentive rnn then it uses paragraph level hidden units of that rnn as embedding
our work differs from these approaches in that we force the sequence to be fully restored from the latent representation without aid from any history information
previous methods have considered leveraging unlabeled data for semi supervised sequence tion tasks
typically rnn based methods consider either i training a sequence to sequence rnn autoencoder or a rnn classier that is robust to adversarial perturbation as initialization for the coder in the supervised model or learning latent representation via a sequence to sequence rnn autoencoder and then using them as inputs to a classier that also takes features extracted from a cnn as inputs
for summarization tasks has considered a semi supervised approach based on support vector machines however so far research on semi supervised text summarization using deep models is scarce
experiments experimental setup for all the experiments we use a layer convolutional encoder followed by a layer deconvolutional decoder recall implementation details for the top layer
filter size stride ground truth hier
lstm on every visit to nyc the hotel beacon is the place we love to stay
so conveniently located to central park lincoln center and great local restaurants
the rooms are lovely
beds so comfortable a great little kitchen and new wizz bang coffee maker
the staff are so accommodating and just love walking across the street to the fairway supermarket with every imaginable goodies to eat
every time in new york lighthouse hotel is our favorite place to stay
very convenient central park lincoln center and great restaurants
the room is wonderful very comfortable bed a kitchenette and a large explosion of coffee maker
the staff is so inclusive just across the street to walk to the supermarket channel love with all kinds of what to eat
our lstm lstm on every visit to nyc the hotel beacon is the place to relax and wanting to become conveniently located
hotel in the evenings out good budget accommodations
the views are great and we were more than two couples
manny the doorman has a great big guy come and will denitly want to leave during my stay and enjoy a wonderfully relaxing wind break in having for hour early rick s cafe
oh perfect easy easy walking distance to everything imaginable groceries
if you may want to watch yours on every visit to nyc the hotel beacon is the place we love to stay
so closely located to central park lincoln center and great local restaurants
biggest rooms are lovely
beds so comfortable a great little kitchen and new unk suggestion coffee maker
the staff turned so accommodating and just love walking across the street to former fairway supermarket with every food taxes to eat
our cnn dcnn table reconstructed paragraph of the hotel reviews example used in
and word embedding are set to h rl for l


and k respectively
the dimension of the latent representation vector varies for each experiment thus is reported separately
for notational convenience we denote our convolutional deconvolutional autoencoder as dcnn
in most comparisons we also considered two standard autoencoders as baselines lstm cnn encoder coupled with lstm decoder and lstm lstm lstm encoder with lstm decoder
an lstm dcnn conguration is not included because it yields similar performance to cnn dcnn while being more computationally expensive
the complete experimental setup and baseline details is provided in the supplementary material sm
cnn dcnn has the least number of parameters
for example using as the dimension of h results in about million total trainable parameters for cnn dcnn cnn lstm and lstm lstm respectively
model lstm lstm hier
lstm lstm hier
att
lstm lstm cnn lstm cnn dcnn bleu














table reconstruction evaluation results on the hotel reviews dataset
figure bleu score vs
sentence length for hotel review data
paragraph reconstruction we rst investigate the performance of the proposed autoencoder in terms of learning representations that can preserve paragraph information
we adopt evaluation criteria from i
e
rouge score and bleu score to measure the closeness of the reconstructed paragraph model output to the input paragraph
briey rouge and bleu scores measures the n gram recall and precision between the model outputs and the ground truth references
we use in our evaluation in alignment with
in addition to the lstm and lstm lstm autoencoder we also compared with the hierarchical lstm autoencoder
the comparison is performed on the hotel reviews datasets following the experimental setup from i
e
we only keep reviews with sentence length ranging from to words resulting in training data samples and testing data samples
for all comparisons we set the dimension of the latent representation to h
from table we see that for long paragraphs the lstm decoder in cnn lstm and lstm lstm suffers from heavy exposure bias issues
we further evaluate the performance of each model with different paragraph lengths
as shown in figure and table on this task cnn dcnn demonstrates a clear advantage meanwhile as the length of the sentence increases the comparative advantage becomes more substantial
for lstm based methods the quality of the reconstruction deteriorates quickly as sequences get longer
in constrast the reconstruction quality of cnn dcnn is stable and consistent regardless of sentence length
furthermore the computational cost evaluated as wall clock is signicantly lower in cnn dcnn
roughly cnn lstm is times slower than cnn dcnn and lstm lstm is times slower on a single gpu
details are reported in the sm
character level and word level correction this task seeks to evaluate whether the tional decoder can overcome exposure bias which severely limits lstm based decoders
we consider scorecnn dcnncnn lstmlstm lstm a denoising autoencoder where the input is tweaked slightly with certain modications while the model attempts to denoise correct the unknown modication thus recover the original sentence
for character level correction we consider the yahoo answer dataset
the dataset description and setup for word level correction is provided in the sm
we follow the experimental setup in for word level and character level spelling correction see details in the sm
we considered substituting each word character with a different one at random with probability with

for character level analysis we rst map all characters into a dimensional embedding vector with the network structure for and character level models kept the same
model actor lstm lstm cnn lstm cnn dcnn model lstm lstm cnn lstm cnn dcnn






figure cer comparison
black triangles indicate the end of an epoch
figure spelling error denoising ison
darker colors indicate higher tainty
trained on modied sentences
table cer and wer parison on yahoo and arxiv data
we employ character error rate cer and word error rate wer for evaluation
the wer cer measure the ratio of levenshtein distance a

a
edit distance between model predictions and the ground truth and the total length of sequence
conceptually lower wer cer indicates better performance
we use lstm lstm and cnn lstm denoising autoencoders for comparison
the architecture for the word level baseline models is the same as in the previous experiment
for character level correction we set dimension of h to
we also compare to actor critic training following their experimental guidelines see details in the sm
as shown in figure and table we observed cnn dcnn achieves both lower cer and faster convergence
further cnn dcnn delivers stable denoising performance irrespective of the noise location within the sentence as seen in figure
for cnn dcnn even when an error is detected but not exactly corrected darker colors in figure indicate higher uncertainty denoising with future words is not effected while for cnn lstm and lstm lstm the error gradually accumulates with longer sequences as expected
for word level correction we consider word substitutions only and mixed perturbations from three kinds substitution deletion and insertion
generally cnn dcnn outperforms cnn lstm and lstm lstm and is faster
we provide experimental details and comparative results in the sm
semi supervised sequence classication summarization we investigate whether our dcnn framework can improve upon supervised natural language tasks that leverage features learned from paragraphs
in principle a good unsupervised feature extractor will improve the ization ability in a semi supervised learning setting
we evaluate our approach on three popular natural language tasks sentiment analysis paragraph topic prediction and text summarization
the rst two tasks are essentially sequence classication while summarization involves both language comprehension and language generation
we consider three large scale document classication datasets dbpedia yahoo answers and yelp review polarity
the partition of training validation and test sets for all datasets follows the settings from
the detailed summary statistics of all datasets are shown in the sm
to demonstrate the advantage of incorporating the reconstruction objective into the training of text classiers we further evaluate our model with different amounts of labeled data


and respectively and the whole training set as unlabeled data
for our purely supervised baseline model supervised cnn we use the same convolutional encoder architecture described above with a dimensional latent representation dimension followed by a mlp classier with one hidden layer of hidden units
the dropout rate is set to
word embeddings are initialized at random
as shown in table the joint training strategy consistently and signicantly outperforms the purely supervised strategy across datasets even when all labels are available
we hypothesize that during the early phase of training when reconstruction is emphasized features from text fragments can be readily





error rate dcnncnn lstmlstm lstmoriginalcoriginalaoriginalnoriginal originalaoriginalnoriginalyoriginalooriginalnoriginaleoriginal originalsoriginaluoriginalgoriginalgoriginaleoriginalsoriginaltoriginal originalsoriginalooriginalmoriginaleoriginal originalgoriginalooriginalooriginaldoriginal originalboriginalooriginalooriginalkoriginalsoriginal modifiedamodifiednmodifiedymodifiedomodifiednmodifiedkmodified modifiedwmodifiedumodifiedgmodifiedgmodifiedemodifiedsmodifiedtmodified modifiedxmodifiedomodifiedhmodifiedemodified modifiedimodifiedomodifiedrmodifieddmodified modifiedymodifiedomodifiedomodifiedkmodifiedumodified actorcriticaactorcriticnactorcriticyactorcriticoactorcriticnactorcriticeactorcritic actorcriticwactorcriticiactorcritictactorcritichactorcriticeactorcriticsactorcritictactorcritic actorcritictactorcriticoactorcritic actorcriticeactorcritic actorcriticfactorcriticoactorcriticractorcriticdactorcritic actorcriticyactorcriticoactorcriticuactorcritic actorcriticuactorcritic lstmclstm lstmalstm lstmnlstm lstm lstm lstmalstm lstmnlstm lstmylstm lstmolstm lstmnlstm lstmelstm lstm lstm lstmslstm lstmulstm lstmglstm lstmglstm lstmelstm lstmslstm lstmtlstm lstm lstm lstmjlstm lstmolstm lstmklstm lstmelstm lstm lstm lstmflstm lstmolstm lstmolstm lstmdlstm lstm lstm lstmylstm lstmolstm lstmulstm lstmnlstm lstmglstm lstm lstm lstmccnn lstmacnn lstmncnn lstm cnn lstmacnn lstmncnn lstmycnn lstmocnn lstmncnn lstmecnn lstm cnn lstmgcnn lstmucnn lstmicnn lstmtcnn lstmecnn lstmscnn lstm cnn lstmscnn lstmocnn lstmmcnn lstmecnn lstm cnn lstmocnn lstmwcnn lstmecnn lstm cnn lstmpcnn lstmocnn lstmocnn lstmkcnn lstmscnn lstm cnn lstm cnn lstm cnn dcnnccnn dcnnacnn dcnnncnn dcnn cnn dcnnacnn dcnnncnn dcnnycnn dcnnocnn dcnnncnn dcnnecnn dcnn cnn dcnnscnn dcnnucnn dcnngcnn dcnngcnn dcnnecnn dcnnscnn dcnntcnn dcnn cnn dcnnscnn dcnnocnn dcnnmcnn dcnnecnn dcnn cnn dcnnwcnn dcnnocnn dcnnocnn dcnndcnn dcnn cnn dcnnbcnn dcnnocnn dcnnocnn dcnnkcnn dcnnscnn dcnn cnn originalsoriginal originalyoriginalooriginaluoriginalroriginal originalioriginaldoriginaleoriginalaoriginal originalooriginalforiginal originalaoriginal originalsoriginaltoriginaleoriginalporiginalporiginalioriginalnoriginalgoriginal originalsoriginaltoriginalooriginalnoriginaleoriginal originaltoriginalooriginal originalboriginaleoriginaltoriginaltoriginaleoriginalroriginal originaltoriginalhoriginalioriginalnoriginalgoriginalsoriginal originaltoriginalooriginal originalcoriginalooriginalmoriginaleoriginal modifiedsmodified modifiedymodifiedomodifiedgmodifiedrmodified modifiedimodifieddmodifiedemodifiedmmodified modifiedomodifiedfmodified modifiedtmodified modifiedsmodifiedtmodifiedemodifiedpmodifiedumodifiedkmodifiednmodifiedgmodified modifiedjmodifiedtmodifiedzmodifiednmodifiedemodified modifiedtmodifiedimodified modifiedbmodifiedemodifiedtmodifiedtmodifiedemodifiedrmodified modifiedtmodifiedhmodifiedimodifiednmodifiedgmodifiedzmodified modifiedtmodifiedtmodified modifiedcmodifiedomodifiedemodifiedemodified actorcriticsactorcritic actorcriticyactorcriticoactorcriticuactorcriticractorcritic actorcriticiactorcriticdactorcriticeactorcriticmactorcritic actorcriticoactorcriticfactorcritic actorcritictactorcritic actorcriticsactorcritictactorcriticeactorcriticpactorcriticuactorcriticaactorcriticnactorcriticgactorcritic actorcriticjactorcriticoactorcritickactorcriticnactorcriticeactorcritic actorcritictactorcriticiactorcritic actorcriticbactorcriticeactorcritictactorcritictactorcriticeactorcriticractorcritic actorcritictactorcritichactorcriticiactorcriticnactorcriticgactorcritic actorcriticiactorcritictactorcritictactorcritic actorcriticcactorcriticoactorcriticmactorcriticeactorcritic lstmwlstm lstmhlstm lstmalstm lstmtlstm lstm lstm lstmslstm lstm lstm lstmylstm lstmolstm lstmulstm lstmrlstm lstm lstm lstmilstm lstmdlstm lstmelstm lstmalstm lstm lstm lstmolstm lstmflstm lstm lstm lstmalstm lstm lstm lstmslstm lstmplstm lstmelstm lstmalstm lstmklstm lstmilstm lstmnlstm lstmglstm lstm lstm lstmslstm lstmtlstm lstmalstm lstmnlstm lstmdlstm lstm lstm lstmtlstm lstmolstm lstm lstm lstmblstm lstmelstm lstmtlstm lstmtlstm lstmelstm lstmrlstm lstm lstm lstmtlstm lstmhlstm lstmilstm lstmnlstm lstmglstm lstmslstm lstm lstm lstmtlstm lstmolstm lstm lstm lstmclstm lstmolstm lstmmlstm lstmelstm lstm lstm lstmwcnn lstmhcnn lstmacnn lstmtcnn lstm cnn lstmscnn lstm cnn lstmycnn lstmocnn lstmucnn lstmrcnn lstm cnn lstmicnn lstmdcnn lstmecnn lstmmcnn lstm cnn lstmocnn lstmfcnn lstm cnn lstmacnn lstm cnn lstmscnn lstmtcnn lstmecnn lstmpcnn lstmpcnn lstmicnn lstmncnn lstmgcnn lstm cnn lstmscnn lstmtcnn lstmacnn lstmrcnn lstmtcnn lstm cnn lstmtcnn lstmocnn lstm cnn lstmbcnn lstmecnn lstmtcnn lstmtcnn lstmecnn lstmrcnn lstm cnn lstmtcnn lstmhcnn lstmicnn lstmncnn lstmgcnn lstm cnn lstmtcnn lstmocnn lstm cnn lstmccnn lstmocnn lstmmcnn lstmecnn lstm cnn lstm cnn dcnnwcnn dcnnhcnn dcnnacnn dcnntcnn dcnn cnn dcnnscnn dcnn cnn dcnnycnn dcnnocnn dcnnucnn dcnnrcnn dcnn cnn dcnnicnn dcnndcnn dcnnecnn dcnnacnn dcnn cnn dcnnocnn dcnnfcnn dcnn cnn dcnnacnn dcnn cnn dcnnscnn dcnntcnn dcnnecnn dcnnpcnn dcnnpcnn dcnnicnn dcnnncnn dcnngcnn dcnn cnn dcnnscnn dcnntcnn dcnnocnn dcnnncnn dcnnecnn dcnn cnn dcnntcnn dcnnocnn dcnn cnn dcnnbcnn dcnnecnn dcnntcnn dcnntcnn dcnnecnn dcnnrcnn dcnn cnn dcnntcnn dcnnhcnn dcnnicnn dcnnncnn dcnngcnn dcnnscnn dcnn cnn dcnntcnn dcnnocnn dcnn cnn dcnnccnn dcnnocnn dcnnmcnn dcnnecnn dcnn cnn dcnn learned
as the training proceeds the most discriminative text fragment features are selected
further the subset of features that are responsible for both reconstruction and discrimination presumably encapsulate longer dependency structure compared to the features using a purely supervised strategy
figure demonstrates the behavior of our model in a semi supervised setting on yelp review dataset
the results for yahoo answer and dbpedia are provided in the sm
model ngrams tfidf large word convnet small word convnet large char convnet small char convnet sa lstm word level deep convnet ours purely supervised ours joint training with cnn lstm ours joint training with cnn dcnn dbpedia









yelp p









yahoo








table test error rates of document classication
results from other methods were obtained from
figure semi supervised tion accuracy on yelp review data
for summarization we used a dataset composed of abstract title pairs from arxiv
title pairs are selected if the length of the title and abstract do not exceed and words respectively
we partitioned the training validation and test sets into pairs each
we train a sequence to sequence model to generate the title given the abstract using a randomly selected subset of paired data with proportion
for every value of we considered both purely supervised summarization using just abstract title pairs and supervised summarization by leveraging additional abstracts without titles
we compared lstm and deconvolutional network as the decoder for generating titles for








obs
proportion supervised semi sup
dcnn dec


table summarization task on arxiv data using rouge l metric
first columns are for the lstm decoder and the last column is for the deconvolutional decoder observed
table summarizes quantitative results using rouge l longest common sequence
in general the additional abstracts without titles improve the eralization ability on the test set
estingly even when all titles are observed the joint training objective still yields a better performance than using lsup alone
presumably since the joint training objective requires the latent representation to be capable of reconstructing the input paragraph in addition to generating a title the learned representation may better capture the entire structure meaning of the paragraph
we also empirically observed that titles generated under the joint training objective are more likely to use the words appearing in the corresponding paragraph i
e
more extractive while the the titles generated using the purely supervised objective lsup tend to use wording more freely thus more abstractive
one possible explanation is that for the joint training strategy since the reconstructed paragraph and title are all generated from latent representation h the text fragments that are used for reconstructing the input paragraph are more likely to be leveraged when building the title thus the title bears more resemblance to the input paragraph
as expected the titles produced by a deconvolutional decoder are less coherent than an lstm decoder
presumably since each paragraph can be summarized with multiple plausible titles the deconvolutional decoder may have trouble when positioning text segments
we provide discussions and titles generated under different setups in the sm
designing a framework which takes the best of these two worlds lstm for generation and cnn for decoding will be an interesting future direction
conclusion we proposed a general framework for text modeling using purely convolutional and deconvolutional operations
the proposed method is free of sequential conditional generation avoiding issues associated with exposure bias and teacher forcing training
our approach enables the model to fully encapsulate a paragraph into a latent representation vector which can be decompressed to reconstruct the original input sequence
empirically the proposed approach achieved excellent long paragraph reconstruction quality and outperforms existing algorithms on spelling correction and semi supervised sequence classication and summarization with largely reduced computational cost

of labeled supervisedsemi cnn cnn lstm references andrew m dai and quoc v le
semi supervised sequence learning
in nips
quoc le and tomas mikolov
distributed representations of sentences and documents
in icml
rie johnson and tong zhang
supervised and semi supervised text categorization using lstm for region embeddings
arxiv february
takeru miyato andrew m dai and ian goodfellow
adversarial training methods for semi supervised dzmitry bahdanau kyunghyun cho and yoshua bengio
neural machine translation by jointly learning text classication
in iclr may
to align and translate
in iclr
kyunghyun cho bart van merrinboer caglar gulcehre dzmitry bahdanau fethi bougares holger schwenk and yoshua bengio
learning phrase representations using rnn encoder decoder for statistical machine translation
in emnlp
fandong meng zhengdong lu mingxuan wang hang li wenbin jiang and qun liu
encoding source language with convolutional neural network for machine translation
in acl
tsung hsien wen milica gasic nikola mrksic pei hao su david vandyke and steve young
mantically conditioned lstm based natural language generation for spoken dialogue systems
arxiv
jiwei li will monroe alan ritter michel galley jianfeng gao and dan jurafsky
deep reinforcement learning for dialogue generation
arxiv
jiwei li will monroe tianlin shi alan ritter and dan jurafsky
adversarial learning for neural dialogue generation


ramesh nallapati bowen zhou cicero nogueira dos santos caglar gulcehre and bing xiang
abstractive text summarization using sequence to sequence rnns and beyond
in conll
shashi narayan nikos papasarantopoulos mirella lapata and shay b cohen
neural extractive rization with side information
arxiv april
alexander m rush sumit chopra and jason weston
a neural attention model for abstractive sentence ilya sutskever oriol vinyals and quoc v le
sequence to sequence learning with neural networks
in summarization
in emnlp
nips
tomas mikolov martin karat lukas burget jan and sanjeev khudanpur
recurrent neural network based language model
in interspeech
sepp hochreiter and jrgen schmidhuber
long short term memory
in neural computation
junyoung chung caglar gulcehre kyunghyun cho and yoshua bengio
empirical evaluation of gated recurrent neural networks on sequence modeling
arxiv
ronald j williams and david zipser
a learning algorithm for continually running fully recurrent neural networks
neural computation
samy bengio oriol vinyals navdeep jaitly and noam shazeer
scheduled sampling for sequence prediction with recurrent neural networks
in nips
ferenc huszr
how not to train your generative model scheduled sampling likelihood adversary arxiv
sentences
in acl
nal kalchbrenner edward grefenstette and phil blunsom
a convolutional neural network for modelling yoon kim
convolutional neural networks for sentence classication
in emnlp
ishaan gulrajani kundan kumar faruk ahmed adrien ali taiga francesco visin david vazquez and aaron courville
pixelvae a latent variable model for natural images
arxiv
alec radford luke metz and soumith chintala
unsupervised representation learning with deep convolutional generative adversarial networks
arxiv
vinod nair and geoffrey e hinton
rectied linear units improve restricted boltzmann machines
in icml ian chiswell and wilfrid hodges
mathematical logic volume
oup oxford
emil julius gumbel and julius lieblein
statistical theory of extreme values and some practical applications pages
a series of lectures

ronan collobert jason weston lon bottou michael karlen koray kavukcuoglu and pavel kuksa
natural language processing almost from scratch
in jmlr
sharan chetlur cliff woolley philippe vandermersch jonathan cohen john tran bryan catanzaro and evan shelhamer
cudnn efcient primitives for deep learning
arxiv
zichao yang diyi yang chris dyer xiaodong he alex smola and eduard hovy
hierarchical attention networks for document classication
in naacl
adji b dieng chong wang jianfeng gao and john paisley
topicrnn a recurrent neural network with long range semantic dependency
in iclr
diederik p kingma shakir mohamed danilo jimenez rezende and max welling
semi supervised learning with deep generative models
in nips
yunchen pu zhe gan ricardo henao xin yuan chunyuan li andrew stevens and lawrence carin
variational autoencoder for deep learning of images labels and captions
in nips
sepp hochreiter yoshua bengio paolo frasconi and jrgen schmidhuber
gradient ow in recurrent nets the difculty of learning long term dependencies
richard socher jeffrey pennington eric h huang andrew y ng and christopher d manning
supervised recursive autoencoders for predicting sentiment distributions
in emnlp
association for computational linguistics
samuel r bowman luke vilnis oriol vinyals andrew m dai rafal jozefowicz and samy bengio
generating sentences from a continuous space
arxiv
zichao yang zhiting hu ruslan salakhutdinov and taylor berg kirkpatrick
improved variational autoencoders for text modeling using dilated convolutions
arxiv february
baotian hu zhengdong lu hang li and qingcai chen
convolutional neural network architectures for matching natural language sentences
in nips
rie johnson and tong zhang
effective use of word order for text categorization with convolutional neural networks
in naacl hlt
the all convolutional net
arxiv
tion
in iclr
jost tobias springenberg alexey dosovitskiy thomas brox and martin riedmiller
striving for simplicity karen simonyan and andrew zisserman
very deep convolutional networks for large scale image stanislau semeniuta aliaksei severyn and erhardt barth
a hybrid convolutional variational coder for text generation
arxiv february
nal kalchbrenner lasse espeholt karen simonyan aaron van den oord alex graves and koray kavukcuoglu
neural machine translation in linear time
arxiv
yann n dauphin angela fan michael auli and david grangier
language modeling with gated convolutional networks
arxiv december
j
gehring m
auli d
grangier d
yarats and y
n
dauphin
convolutional sequence to sequence learning
arxiv may
aaron van den oord nal kalchbrenner lasse espeholt oriol vinyals alex graves al
conditional image generation with pixelcnn decoders
in nips pages
jiwei li minh thang luong and dan jurafsky
a hierarchical neural autoencoder for paragraphs and documents
in acl
tomas mikolov ilya sutskever kai chen greg s corrado and jeff dean
distributed representations of words and phrases and their compositionality
in nips
kam fai wong mingli wu and wenjie li
extractive summarization using supervised and semi supervised learning
in iccl
association for computational linguistics
chin yew lin
rouge a package for automatic evaluation of summaries
in acl workshop
kishore papineni salim roukos todd ward and wei jing zhu
bleu a method for automatic evaluation of machine translation
in acl
association for computational linguistics
xiang zhang junbo zhao and yann lecun
character level convolutional networks for text classication
in nips pages
dzmitry bahdanau philemon brakel kelvin xu anirudh goyal ryan lowe joelle pineau aaron courville and yoshua bengio
an actor critic algorithm for sequence prediction
arxiv
jp woodard and jt nelson
an information theoretic measure of speech recognition performance
in workshop on standardisation for speech i o
diederik kingma and jimmy ba
adam a method for stochastic optimization
in iclr
xavier glorot and yoshua bengio
understanding the difculty of training deep feedforward neural networks
in aistats

