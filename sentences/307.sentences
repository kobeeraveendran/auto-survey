n u j l c
s c v
v i x r a combination of abstractive and extractive approaches for summarization of long scientic texts tretyak vladislav
ru and stepanov denis denis

com itmo university kronverksky pr
st
peterburg abstract
in this research work we present a method to generate maries of long scientic documents that uses the advantages of both tractive and abstractive approaches
before producing a summary in an abstractive manner we perform the extractive step which then is used for conditioning the abstractor module
we used pre trained based language models for both extractor and abstractor
our ments showed that using extractive and abstractive models jointly nicantly improves summarization results and rouge scores
introduction the language modeling task in the general case is a process of learning the joint probability function of sequences of words in a natural language
statistical language modeling or language modeling is the development of probabilistic models that are able to predict the next word in the sequence given the words that precede it also known as context
language models could operate on ferent sequence levels
small language models work with sequences of chars and words
while the big language model works with sentences
but most common language model operates with sequences of words
the language model could be used standalone to predict words according to context but usually the language model is used to solve more challenging tasks
for example it helps to solve a large range of natural language tasks such as machine translation natural language understanding speech recognition mation retrieval text summarization
in other words language models are used in most real world natural language processing applications and the quality of such applications mostly depends on the language models performance
that s why language modeling task plays a major role in natural language processing and articial intelligence research
the rst neural language model used feed forward architecture
one of the main features of using neural language models is getting representation vector of words sequences
these word vectors usually called embedding vector v
tretyak et al
and embeddings for similar words located closer to each other in dimension space also having similar representations
after successful usage of feed forward networks recurrent neural networks achieved better results in language eling tasks because of its ability to take into account the position of words in sentences and producing contextual word embeddings
long short term memory networks allows the language model to learn the relevant context of longer sequences than feed forward or rnn s because of its more sophisticated memory mechanism
next the attention mechanism made improvement in language modeling tasks with a combination of sequence to sequence framework
the attention mechanism improves memory mechanism of recurrent ral networks by giving the ability for the decoder network to look at the whole context
the next big step in the language modeling task was developing architecture with novel self attention mechanism that helps the model to use the context of the sentence more eciently
such models could take into account both left and right context of the sequence as it is implemented in bert model and only left context like in gpt model
transformer based lm have own disadvantages one of them is a limited receptive eld
it means that the transformer could only proceed sequences that have limited length while rent neural networks could work with unlimited sequences
this issue partially solved by the transformer xl model which could work with continuous quences of texts like recurrent neural networks
despite of this disadvantage models like gpt with large receptive eld and trained on large amount of data are capable of capturing long range dependencies
for this work we propose method that uses both extractive and abstractive approaches for summarization task
our work improves previous approach by using pre trained lm instead of training it from scratch
in this research work we used arxiv dataset as an great example of long scientic documents and in order to compare our work with previous approaches
we split training process in two steps
first we train extractive model as a classication task that simply selects which sentences should be selected into summary
second we use extracted summary together with dierent article sections as conditioning for generating abstractive summary
adding extractive summary into conditioning part is crucial for generating target summary
also we made experiments with dierent variants of conditioning and found the best combination for it
ing to our experiments extracted summary introduction and conclusion of the paper performs the best
the contributions of this work are we show that combination of extractive and abstractive approaches improves quality of produced summary while using pre trained transformer based guage models as a result of applying proposed model improved rouge metric on arxiv dataset using abstractive and extractive summarization related work automatic summarization is the process of shortening a set of data tionally to create a summary that represents the most important or relevant information within the original content
summarization is one of the main tasks in natural language understanding
there are two main approaches for creating a summary
extractive summarization aims at identifying the information that is then extracted and grouped together to form an extractive summary
it means that while we train a machine learning model it solves the classication task
which information should be included in the summary
abstractive summary generation rewrites the entire document by building internal semantic tation and then a summary is created using natural language processing
as an example of the ml model it receives a source document and generates a summary word by word
without copying text from source document
the rst summarization systems were using extractive techniques as it is much easier than abstractive
hsu et al
combined both extractive and tive approaches for creating summaries
authors proposed a unied model that combines sentence level and word level attention to take advantage of two proaches
also the authors created a novel inconsistency loss function in order to be sure that the proposed model is mutually benecial to both extractive and abstractive summarization
another attempt of using both extractive and abstractive models together was proposed in paper
the authors proposed a model that uses bert to perform extractive summarization with the lstm pointer network that selects which sentences should be included in the summary
then the extracted summary is fed into attention based with the ing mechanism for handling out of vocabulary oov words
this part of the model is responsible for paraphrasing of the extracted summary and producing an abstractive summary
they used reinforcement learning technique to mize the rouge metric directly instead of maximizing log likelihood for the next word given the previous ground truth words
both extractive and tive models are trained
in work authors took pre trained bert model and rst trained it on extractive task using cnn daily mail datasets
then they continue training the same model but doing abstractive rization
subramanian et al
used arxiv dataset for summarizing long scientic texts and uses the similar approach to ours
their model consists of two parts rst is a hierarchical representation of the document that points out to sentences in the document in order to construct an extractive summary second is a transformer language model that conditions on the extractive summary and some part of the paper text as well
proposed model our model consist of two components
extractive model classier that choose which sentences from source text should be included in summary
tive model that uses condition text to produce abstractive summary
during v
tretyak et al
research work we propose a model that combines advantages of extractive and abstractive approaches for creating summaries
the proposed model consists of two parts the rst part is the extractive summarization model second is the stractive model
a similar approach was used in paper
the dierence is that the extractive and abstractive models are replaced by pre trained based lms
the motivation for this is that pre trained language model in most cases shows a signicant increase in performance
in the previous approach the transformer model was used as the abstractive model
instead of it sive pre trained lm was used
during research work a list of experiments for a conditional generation was performed using an extractive summary to ate abstractive summaries
the training is performed in two steps
first the extractive model is trained
second training of the abstractive model that uses extractive summary to produce better results
dataset description as it was already mentioned the dataset from arxiv
org was used for training
this dataset contains long sequences of texts from the scientic domain with ground truth summaries
the structure of the dataset contains such information as article i d abstract text article text labels section name sections
the eld article text holds full text of a paper section name is a list of papers sections sections contains full text of paper divided into sections
that means we could identify which text is from the introduction section which is from the abstract section or conclusion
this advantage was used in experiments
the dataset statistics are described in table
table
arxiv dataset statistics
num
of docs
avg
length words doc
length avg
mary words both for validation and test parts we took percent of documents
during the preprocessing step too long and too short papers were removed also papers without abstracts and text of paper were removed
also we replaced some latex markup with special tokens such as math graph table equation in order to help model recognize special tokens other latex source code we cleared
also we removed all irrelevant chars and exclude all not latin letters
this preprocessing pipeline was applied for both extractive and abstractive tasks
we used common approach for creating dataset for training extractive model
first we create list of sentence pairs abstract sentence sentence from paper every sentence from abstract are matched with every sentence from using abstractive and extractive summarization paper text
between every pair we compute the rouge metric
in particular we compute rouge l and take the average value of score
after that we got a scoring value for each pair of sentences the higher the better
we choose two pairs with the highest scores this is our positive examples
then we randomly sample two sentences from the paper text and mark such pairs as negative examples
after completing these steps we got a dataset that contains a list of sentence pairs and labels
then we save the dataset for extractive summarization in a separate le
for the abstractive summarization task we took our best model that solves extractive summarization tasks and infer it on the dataset in other words we generate summaries for each paper in the dataset with our best extractive model
then we make all necessary preprocessing and save the model generated maries with corresponding papers and abstracts
these are all steps that were done with the dataset

extractive model first we make experiments with extractive models
we used three architectures for extractive summarization models bert roberta and electra
electra has berts architecture but the pre training phase is dierent
in the original paper the authors proposed a new scheme with two models the generator and language model that are trained jointly
generator process tences and corrupts some of the tokens in it
corruption denotes the process of replacing tokens to some other tokens that could t into the context of the sentence
in turn the language model predicts which tokens were replaced by the generator
this scheme of pre training improves the performance of the guage model and reduces the number of training iterations computing resources
all used extractive models were pre trained on a large corpus of texts and we took base variants we did not check large variants because of long training and hardware constraints
it is highly probable that large variants could show even better results
for each of the extractive models we construct special input of tokens equation
inputext seqgt tokencls seqcandidate where xext denotes input to extractive model concat denotes tion function tokencls denotes special classication token that was originaly used in bert model seqgt denotes sentence from ground truth summary seqcandidate corresponds to candidate sentence
we used word piece tokenizer for both bert and electra models as in original papers with vocabulary size
roberta uses bpe tokenizer in original implementation and does not have cls token in its vocabulary that s why we manually add it before training
the key benet of using word tokenizers is a small dataset size and decreasing out of vocabulary cases
v
tretyak et al
also special tokens were added math graph table equation
that were extracted by regular expressions while pre processing
all extractive models were trained using cross entropy loss as an optimization function
we used the learning rate of model input batch size
also we applied gradient clipping technique to make the training process more stable
we applied distributed training using two gpus
the training process took approximately days
the architecture of the proposed models is scribed in figure below
the scheme remains the same only input to model changes and pre trained lm replaced by one of bert electra
fig

architecture of extractive model classication head in the gure above denotes a block that contains a stack of linear layers with an output of size with sigmoid activation
we used rouge metrics for evaluation quality of summaries
to evaluate the summarization model we rstly inference it on the test set
we make a list of pairs sentence from an abstract sentence from paper and score every pair with the model
then we use only candidates that have the highest score
after this process we got the extractive summary that we use with ground truth abstract to calculate rouge scores
all scores are averaged between papers
we used and rouge l as our main evaluation metric for summaries
all proposed models used the same set of training hyperparameters and other settings were the same
from the table above we could conclude that the bert model achieves the highest result among all other models
with equal set of using abstractive and extractive summarization hyperparameters bert model achieves better results
in the table above oracle denotes the rouge score between the ground truth abstract and extracted summary that includes the most relevant sentences from the text of a paper
the oracle scores in the table indicate the limit for extractive models to get the best summary according to rouge metric
in order to get more coherent text we perform paraphrasing of extracted summaries
to get paraphrased sentences we apply the back translation technique
for this we used pre trained transformer lm that was trained to translate sentences from english to german and backward
first we translate the extractive summary into the german language using a pre trained transformer lm and then back to the english language
those paraphrased summaries are used later during experiments with condition model
the results of extractive models are presented in table
table
extractrive model results on arxiv dataset
ext denotes extractive
model bert roberta electra type rouge l ext ext ext








abstractive model after nishing experiments with extractive summarization we made ments with abstractive approaches
for abstractive summarization we used trained autoregressive language models and bart
we also made experiments with conditional generation for both models
during conditional generation we give to the model some context according to which it generates text
we used a condition summary that was extracted by the best extractive model table
also we made conditioning on paraphrased summaries
we used an input size of gpt base variant
the input is dened by equation
inputabs masksegment where we concatenate tc conditioning text with target summary ts and apply a segment mask masksegment that identies which part of the input is condition and which is target text
the experiments with the bart model were performed in a similar way
but there are some dierences in model input
bart consists of two parts the rst one is an encoder which uses bert like architecture
the second part is a decoder that consists of stacked transformer decoders similar architecture to gpt
these two parts are connected so that encoder output is fed into the decoder
in such architecture decoder uses hidden states that were produced by encoder
encoder and decoder are trained end to end
it means that during backward pass we update weights of decoder and encoder
in such a scenario we v
tretyak et al
fed the conditioning part into barts encoder and target text into the decoder during the training process
we propose dierent conditioning scenarios
we made conditioning on the extractive summary on the introduction of paper introduction concatenated with conclusion and introduction concatenated with the extractive summary and with the conclusion
we made the assumption that both the introduction and conclusion concentrate the most valuable information for generating summaries
because usually in introdiction author describes the problem itself some details about the proposed method novelty
in conclusion authors usually make some recap of what was done conclusion of results
we did not apply conditioning on long texts to because of restrictions in the input size
the condition part plus target text in most cases will not t into input size that s why we only made conditioning on summaries extracted by models
the experiments results with and bart are presented in table below
table
proposed model results on arxiv dataset model bert roberta electra bart conditioned on extractive summary bart conditioned on introduction bart conditioned on introduction with clusion bart conditioned on introduction tive summary clusion with condition on extractive summary with tion extractive paraphrased summary ing on subramanian s
al
cohan a
al
et type rouge l our approaches ext ext ext







mix


mix


mix


mix


mix


mix


abs


previous approaches mix


mix


gold extractive gold extractive oracle

using abstractive and extractive summarization from the table above we could conclude that extractive summary plays a cial role in generating abstractive summaries
removing the extractive summary from the condition part leads to a decreasing rouge score
also the tion that both introduction and conclusion holds the most relevant information conrmed
its obvious that extractive summary has a bigger impact than duction with conclusion because extractive summary already holds a lot of vant according to rouge score sentences
also we investigate that and rouge l scores outperforms the best model and outperforms oracle cause during the abstractive summarization model could produce words that could not be presented in the source document
the best model that uses bert as extractor and bart as abstractor is presented in fig

first bert performs extractive summarization of the article extracted summary concatenates with the introduction and conclusion of the paper
this setup shows the best mance according to the rouge metric and outperforms the previous approach that was applied to arxiv dataset
fig

proposed method for generating summaries of long texts conclusion the novel improvements was proposed that uses both extractive and tive approaches as an extracted model bert model was used as an abstractive model bart model was used
during research work comparison analysis of dierent architectures for extractive and abstractive summarization approaches extractive modelabstractive modelextracted summaryrest of the texttext forsummarizationabstractive summary v
tretyak et al
were done
also we make experiments with conditioning on dierent parts of the source document to produce an abstractive summary
conditioning on tractive summary introduction and conclusion of the paper shows the best rouge score
the assumption about conditioning on paraphrased summary failed during paraphrasing source sentence is changed so that it becomes evant to ground truth summary
evaluation results that were obtained during the research outperforms previously applied algorithms for arxiv dataset in all and rouge l metrics

and
ingly
also experiments showed that using the advantages of extractive and stractive approaches improves the quality of the produced summary extractive summarization plays a crucial part in generating abstractive summaries
as a future improvement of the proposed architecture end to end learning could be applied using both extractor and abstractor
this feature potentially could improve the quality of abstractive summaries
also the proposed architecture could be tested for other summarization datasets
references
bae s
kim t
kim j
lee s

summary level training of sentence rewriting for abstractive summarization
arxiv preprint

clark k
luong m
t
le q
v
manning c
d
electra pre training text coders as discriminators rather than generators
arxiv preprint

dai z
yang z
yang y
carbonell j
le q
v
salakhutdinov r
transformer xl attentive language models beyond a xed length context
arxiv preprint

devlin j
chang m
w
lee k
toutanova k
bert pre training of deep tional transformers for language understanding
arxiv preprint

gal y
ghahramani z
a theoretically grounded application of dropout in current neural networks
in advances in neural information processing systems
pp

hochreiter s
schmidhuber j
long short term memory
neural computation
hsu w
t
lin c
k
lee m
y
min k
tang j
sun m
a unied model for extractive and abstractive summarization using inconsistency loss
arxiv preprint

lewis m
liu y
goyal n
ghazvininejad m
mohamed a
levy o
stoyanov v
zettlemoyer l
bart denoising sequence to sequence pre training for natural language generation translation and comprehension
arxiv preprint

liu y
lapata m
text summarization with pretrained encoders
arxiv preprint

liu y
ott m
goyal n
du j
joshi m
chen d
levy o
lewis m
zettlemoyer l
stoyanov v
roberta a robustly optimized bert pretraining approach
arxiv preprint
using abstractive and extractive summarization
mikolov t
sutskever i
chen k
corrado g
s
dean j
distributed sentations of words and phrases and their compositionality
in advances in neural information processing systems
pp

pascanu r
mikolov t
bengio y
on the diculty of training recurrent neural networks
in international conference on machine learning
pp

peters m
e
neumann m
iyyer m
gardner m
clark c
lee k
zettlemoyer l
deep contextualized word representations
arxiv preprint

radford a
wu j
child r
luan d
amodei d
sutskever i
language models are unsupervised multitask learners
openai blog
schuster m
nakajima k
japanese and korean voice search
in ieee international conference on acoustics speech and signal processing icassp
pp

ieee
sennrich r
haddow b
birch a
neural machine translation of rare words with subword units
arxiv preprint

subramanian s
li r
pilault j
pal c
on extractive and abstractive ral document summarization with transformer language models
arxiv preprint

sutskever i
vinyals o
le q
v
sequence to sequence learning with neural networks
in advances in neural information processing systems
pp

vaswani a
shazeer n
parmar n
uszkoreit j
jones l
gomez a
n
kaiser
polosukhin i
attention is all you need
in advances in neural information processing systems
pp

