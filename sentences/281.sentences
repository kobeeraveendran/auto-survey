asking and answering questions to evaluate the factual consistency of summaries alex wang new york university
edu kyunghyun cho facebook ai new york university mike lewis facebook ai r a l c
s c v
v i x r a abstract practical applications of abstractive rization models are limited by frequent tual inconsistencies with respect to their put
existing automatic evaluation metrics for summarization are largely insensitive to such errors
we propose an automatic ation protocol called that is designed to identify factual inconsistencies in a ated summary
qags is based on the ition that if we ask questions about a mary and its source we will receive lar answers if the summary is factually sistent with the source
to evaluate qags we collect human judgments of factual tency on model generated summaries for the cnn dailymail hermann et al
and xsum narayan et al
summarization datasets
qags has substantially higher relations with these judgments than other tomatic evaluation metrics
also qags fers a natural form of interpretability the swers and questions generated while ing qags indicate which tokens of a summary are inconsistent and why
we believe qags is a promising tool in automatically generating usable and factually consistent text
introduction automatic summarization aims to produce maries that are succinct coherent relevant and crucially factually correct
recent progress in conditional text generation has led to models that can generate uent topical summaries lewis et al

however model generated summaries quently contain factual inconsistencies limiting their applicability kryscinski et al

the problem of factual inconsistency is due in part to the lack of automatic evaluation metrics that can detect such errors
standard metrics for evaluating generated text are predominantly based kags
on counting n grams which weigh all n grams equally and are insensitive to semantic errors
this inadequacy leaves human evaluation as the primary method for evaluating the factual consistencies which has been noted to be challenging even for humans daume iii and marcu kryscinski et al
in addition to being slow and costly
we argue that evaluation metrics that are able to capture subtle semantic errors are required to build better models
in this work we introduce a general framework for evaluating conditional text generation that is designed to detect factual sistencies in generated text with respect to some input
our framework consists of three steps given a generated text a question generation qg model generates a set of questions about the text
we then use question answering qa models to answer these questions given both the input and the generated text
a quality score is computed based on the similarity of corresponding answers
this approach leverages recent progress in qa and qg to ask and answer human readable topic questions devlin et al
song et al

it only assumes access to a question ing dataset to train the qg and qa models and is applicable to any modality where a qa model is available e

text images or knowledge graphs
we use this framework to develop qags tion answering and generation for tion a metric for evaluating the factual tency of abstractive document summaries
pared to commonly used automatic metrics such as rouge lin qags shows dramatically higher correlations with human judgements of tuality for example achieving a pearson correlation coefcient of
on the cnn dailymail marization task compared to
for
qags also achieves new state of the art results on evaluating the factuality of summaries forming recently proposed nli models for this task kryscinski et al

finally we analyse the robustness of qags through an ablation study
qags shows ness to the quality of the underlying qg and qa models the domain of the models and the number of questions asked
even under the worst ablation settings qags still has stronger correlation with human judgments than other automatic metrics
overall we contribute the following we introduce qags an automatic model based ation metric for measuring the factual consistency of model generated text
we collect a new set of human judgments of factual consistency of model generated summaries for two tion datasets
we demonstrate that qags lates with these judgments signicantly better than other automatic metrics
we show via tions that qags is robust to a number of factors including underlying model quality and domain mismatch
we analyze the questions and swers produced in computing qags to illustrate which parts of summaries are inconsistent
we will release models and code to compute qags
we identify two key deciencies when using these n gram based evaluation metrics to detect factual inconsistencies in generated text
first these metrics require one or more reference texts to compare against
obtaining references can be expensive and challenging and as such many text generation datasets contain only a single erence
this problem is exacerbated with entropy generation tasks such as summarization or dialogue where there is a very large number of acceptable outputs
in these settings comparing against a single reference is woefully inadequate
second given a reference to compare against n gram based approach weigh all portions of the text equally even when only a small fraction of the n grams carry most of the semantic content
factual inconsistencies caused by minor changes may be drowned out by otherwise high n gram overlap making these metrics insensitive to these errors
for example the sentences i am writing my paper in vancouver
and i am not writing my paper in vancouver
share nearly all unigrams and bigrams despite having the opposite meaning
background automatically evaluating machine generated text a framework for automatically evaluating factual consistency standard approaches to evaluating generated text are primarily based on counting n gram overlap
these methods assume access to one or more ence texts and score a generated summary based on the precision and recall of all reference n grams in the generated summary
we briey describe the most common metrics in this family and refer readers to liu et al
for further discussion
rouge lin was developed specically for evaluating automatic summarization and its variants are the standard for such
the most common variant is rouge n typically n which computes the score for all ence n grams in the generated summary
l another commonly used variant is the length of the longest common subsequence possibly consecutive between a summary and references
bleu papineni et al
is closely related to rouge but was developed for machine translation
bleu computes the precision of the reference grams in the generated summary
meteor lavie and agarwal extends bleu by using an alignment between the generated text and a erence as well as using stemming and synonym replacement for more exible n gram matching
we introduce a framework for automatically tecting factual inconsistencies in generated text while also addressing the deciencies of current approaches
let x and y be sequences of tokens coming from a vocabulary v where x is a source text and y is a summary of x
we dene as a distribution over all possible questions q given summary y and x and y as tributions over all possible answers a to a ular question q given either the source x or the summary y
we constrain the questions q and answers a to also be sequences of tokens from v
then the factual consistency of the summary y is x y where d is some function measuring the ity of the two answer distributions
this expression is maximized when y contains a subset of the mation in x such that it produces the same answer for any question from
this happens ially when y x e

we take x as its own summary but we usually have other desiderata of y such that this solution is undesirable
figure overview of qags
a set of questions is generated based on the summary
the questions are then answered using both the source article and the summary
corresponding answers are compared using a similarity function and averaged across questions to produce the nal qags score
this framework addresses the two issues with gram based approaches
instead of requiring a ence to compare against our framework asks tions based on the generation itself and compares answers with the provided source text
also the use of questions focuses the metric on the tically relevant parts of the generated text rather than weighting all parts of the text equally
in practice exactly computing the expectation in equation is intractable due to the large space of possible questions
one potential workaround is to randomly sample questions from but this suffers from high variance and requires many ples to obtain a good estimate
instead we focus on producing highly probable questions e

as duced by beam search which may be biased in the limit but will require fewer questions to estimate because of the higher quality of the questions
qags using this framework requires specifying the tion distribution the answer distribution y or x and the answer similarity tion d
we apply this framework to summarization to develop qags and describe our instantiations of these components
question generation to instantiate we draw on recent work on automatic question generation qg which models this distribution using neural models du et al
ishna and iyyer
we over sample questions and then lter out low quality questions as follows
first we train and generate from conditional qg models the model receives both the answer and the source article and is trained to maximize the likelihood of the paired question
at test time we extract named entities and noun phrases as answers candidates using spacy
second we lter out low quality questions using a number of heuristics such as duplicates and tions less than three tokens long
we also found it useful to run the qa model see next section on all of the candidate questions and lter out questions for which the qa model predicted no answer
question answering we instantiate the answer distributions as extractive qa models
io api entityrecognizer summarizationkevin sineld scored his rst try of the season against castleford
leeds rhino scored unbeaten run against tigers to six matches
ryan hall was sent to leeds rhino for rst time in his career
leeds showed they are in good shape to cope with kevin sineld s retirement as they claimed a derby victory over castleford in front of a sell out crowd at the mend a hose jungle



ryan hall was sent to the sin bin for the rst time in his career joel moon scored his rst try of the season leeds extended their unbeaten run against the tigers to six matchesgenerated questionswho scored their rst try of the moonkevin sineldwho was sent to leeds rhino for the rst ryan hallhow many matches did they matchessix matchessummary answerssourceanswerssourcesummary for simplicity
we use extractive qa because we assume the facts are represented as text spans in the article and summary
future work should explore using abstractive qa models which could match paraphrases of the same answer
answer similarity we use token level to compare answers which is standard for extractive qa and equivalent to dening d as f max x arg max y the qags score given these components we obtain the qags score of a generation by erating k questions conditioned on the summary answering the questions using both the source article and the summary to get two sets of answers comparing corresponding answers using the answer similarity metric and averaging the swer similarity metric over all questions
we depict this process in figure
experiments
human evaluation we test whether qags accurately measures the factual consistency of a summary with respect to a source article by computing correlations with human judgments of factual consistency
datasets we evaluate on two abstractive marization datasets cnn daily mail cnndm hermann et al
nallapati et al
and xsum narayan et al

abstractive marization is particularly interesting because tual consistency with the original text is crucial to usability and a lack of such consistency has plagued abstractive neural summarization models cao et al
falke et al
kryscinski et al
i
a

cnn dm is a standard dataset for tion that consists of cnn and dailymail articles
each reference summary consists of the nation of three editor written bullet point lights
for summaries we use test outputs from gehrmann et al

xsum was created by taking the rst sentence of a news article as the summary and using the rest of the article as the source
consequently xsum summaries are signicantly more abstractive than those of cnn dm and extractive summarization models perform poorly on this dataset
we found that while the xsum summaries are more abstractive frequently there are facts e

metric cnn dm xsum rouge l meteor bertscore qags



















table summary level pearson correlation cients between various automatic metrics and human judgments of correctness for summarization datasets
qags obtains substantially higher correlations than all other automatic metrics
rst names in the summary that are not available in the article
this quirk made it especially difcult for humans and qags to tell when factual errors were being made by the summarization model
to remedy this for human evaluation and qags we prepend the summary back to the article
we use a subset of test outputs from bart ne tuned on xsum lewis et al

annotation protocol we collect human ments on amazon mechanical via parlai miller et al

we present summaries one sentence at a time along with the entire article
for each summary sentence the annotator makes a nary decision as to whether the sentence is factually consistent with the article
workers are instructed to mark non grammatical sentences as not tent and copies of article sentences as consistent
workers are paid per full summary annotated
see appendix a for further details
we collect annotations per summary
to obtain a single correctness score per summary we rst take the majority vote for each sentence then age the binary scores across summary sentences
inter annotator agreement as measured by pendorff s is
and
for cnn dm and xsum respectively indicating moderate and fair agreement ageeva et al

while not perfect these agreement numbers are in line with similar gures from previous work on tion evaluation daume iii and marcu

mturk

experimental details question generation we use fairseq ott et al
to ne tune a pretrained bart guage model on newsqa trischler et al
a dataset consisting of cnn articles and sourced questions
for each summary we use answer candidates and generate questions using beam search with width for a total of tion candidates
after ltering we use the k most probable questions
if a summary has too few ltered questions we randomly sample tions to reach the required number
for details see appendix b
question answering we train qa models by ne tuning bert devlin et al
on
rajpurkar et al

we use the large uncased bert variant via the transformers library wolf et al

baselines we compare against a number of tomatic evaluation metrics rouge lin meteor lavie and agarwal bleu pineni et al
and bertscore zhang et al

the latter uses bert representations to compute an alignment between generation and erence tokens and which is then used to pute a soft version of unigram
we use the large uncased bert variant

results we present results in table
qags strongly outperforms other automatic evaluation metrics in terms of correlation with human judgments of tual consistency
bleu and rouge perform parably and lower order n gram metrics work ter
bertscore matches the best n gram metrics on cnn dm but the worst overall on xsum
on cnn dm qags obtains nearly twice the correlation of the next best automatic metric
we speculate that this large increase is due to the sensitivity of the qa model to the sentence fusing behavior exhibited in many marization models trained on cnn dm lebanoff et al

when two sentences are fused to produce an incorrect summary statement the qa model produces different answers than when using the source article versus when using the summary
on xsum all metrics correlate worse with man judgments than on cnn dm which reects the fact that xsum is more abstractive
qags still outperforms the next best automatic metric
qa model squad cnn dm xsum pear
pear
bert base bert large bert large wwm








table pearson correlations between human ments of factual consistency and qags using qa els of different qualities as measured by performance on the
development set
the tions are stable across qa model quality
newsqa cnn dm xsum pear
pear
ppl









table pearson correlations between human ments of factual consistency and qags with qg els of varying quality as measured by perplexity on the newsqa development set
we see some decrease in correlation on cnn dm as qg perplexity increases though we do not see a similar trend for xsum

ablations a potential issue with model based evaluation is that the quality of the evaluation metric may pend heavily on specic hyperparameter settings
we explore whether this is true with qags by forming ablations on several factors
model quality we rst consider the degree to which the quality of the underlying models impacts their evaluation capabilities
for qa quality we answer this question by training qa models of varying quality by tuning different versions of bert on squad
we present results in table
the qa els perform similarly despite substantially ferent performances on the squad ment set
surprisingly using the best qa model bert large wwm does not lead to the best correlations with human judgments
on cnn dm bert large wwm slightly performs bert base and bert large
on xsum bert base slightly outperforms the other two bert variants
these results indicate that qags is fairly robust to the quality of the derlying qa model though we note that bert is a strong qa baseline and using weaker qa models might lead to larger performance dropoffs
to ablate qg quality we use models with questions cnn dm xsum model metric correct







random bert nli esim factcc qags




table pearson correlation coefcients between qags scores with varying number of questions and human judgments of correctness for summarization datasets
the correlation increases with the number of questions used but with decreasing marginal benet
table results on the sentence ranking task from falke et al

results using bert nli and esim are from falke et al
factcc is from kryscinski et al

qags outperforms previous work
creasing perplexity on the newsqa development set
results in table show that qags is robust to the qg model quality with some decrease in correlation with human judgments as perplexity creases on cnn dm and no clear trend on xsum
even the weakest qg model still signicantly performs all other automatic metrics in table
domain effects our approach relies on having a labeled dataset to train qg and qa models
ever for relatively niche domains such a labeled qa qg dataset may not exist
instead we may need to resort to using models trained on of domain data leading to domain shift effects that negatively impact the quality of the qags scores
we simulate this setting by ne tuning the qg model on squad which is of similar size to newsqa but drawn from wikipedia articles rather than cnn articles which exactly matches the genre of the summarization datasets
evaluating with this qg model we get relations of
and
with human ments on cnn dm and xsum respectively versus
and
when using the newsqa tuned qg model
the drop in performance indicates a negative domain shift effect
however using the squad tuned qg model still substantially forms all other automatic metrics again pointing to the robustness of qags
number of questions next we investigate the correlation with human judgments when varying the number of questions used
results in table show that increasing the number of questions used improves correlations with human judgments
we observe a large increase when moving from to questions and a smaller increase from to questions indicating decreasing marginal benet moving beyond questions
with just questions qags still substantially outperforms other automatic metrics indicating its robustness
answer similarity metric finally we consider using exact match as an alternative answer ilarity metric
exact match is another common evaluation metric for extractive qa and is more strictive than
when using em we obtain son correlations with human judgments of
and
on cnn dm and xsum as opposed to
and
when using
re ranking with qags several works explore the use of natural language inference nli models to detect factual tency in generated text welleck et al
falke et al

we compare against these methods by evaluating on the sentence ranking experiment from falke et al

the experiment uses triplets of source sentences from cnn dm and two summary sentences generated from the model from chen and bansal
one summary sentence is factually consistent with the source sentence and the other is inconsistent
a metric or model is evaluated based on how often it ranks the consistent sentence higher than the inconsistent sentence
we present the results in table
results using two nli models ne tuned on multinli williams et al
bert nli and esim chen et al
are from falke et al

factcc kryscinski et al
is an nli based checking model that is trained on a dataset tailor made for detecting factual inconsistencies in ated text
qags outperforms these methods while requiring no special supervision for this task
qualitative analysis interpreting qags the questions and answers produced in computing qags are directly pretable and highlight errors in summaries
we article on friday year old usman khan stabbed reportedly several people at fishmongers hall in london with a large knife then ed up london bridge
members of the public confronted him one man sprayed khan with a re extinguisher others struck him with their sts and took his knife and another a polish chef named ukasz harried him with a ve foot narwhal tusk



summary on friday afternoon a man named faisal khan entered a cambridge university building and started attacking people with a knife and a re extinguisher
question what did the attacker have article answer a large knife question when did the attack take place article answer friday question what is the attacker s name article answer usman khan question where did the attack take place article answer fishmongers hall summary answer cambridge university building summary answer a knife and a re extinguisher summary answer friday afternoon summary answer faisal khan article in ndings published on wednesday in the journal plos one an international team of scientists report ancient egyptians captured sacred ibises threskiornis aethiopicus from the wild for use in ritual sacrice rather than domesticating the birds



the team collected dna samples from mummied birds collected from six separate catacombs including sites at abydos saqqara and tuna el gebel with permission from the egyptian ministry of state for antiquity and several museums offered to send tissue samples from the mummied ibises in their collections



summary archaeologists have used dna samples from ancient ibis birds to determine whether the birds were domesticated or sacriced in ancient egypt question archaeologists have used what to determine whether the birds were domesticated article answer hatchery structures question who used dna samples to determine whether the birds were domesticated article answer no answer question what are archeologists using to determine whether the birds were domesticated article answer dna samples question where were the birds found article answer six separate catacombs summary answer archaeologists summary answer dna samples summary answer dna samples summary answer ancient egypt table example questions and answers generated when computing qags
the questions are overwhelmingly uent and relevant
the answers indicate which tokens in the summary are factually consistent or inconsistent
present examples of articles summaries and the qags questions and answers in table
on the rst example table top qags tects several factual inconsistencies in the ated summary the summary mistakes the rst name of the attacker the location of the attack and the weapons used
because the qg model focuses on these details qags is able to correctly penalize the summary for its hallucinations
because the answer candidates used are mostly named entities and noun phrases qags is particularly effective at detecting errors of this kind
using more verse answer candidates may broaden the set of inconsistencies that qags is able to detect
the second example table bottom trates failure modes of qags
for example the qa model incorrectly marks question as swerable
on question both answers produced are correct but because they have no common kens they are marked inconsistent by qags
error analysis the interpretability of qags lows for error analysis on the metric
we manually annotate triplets of generated questions article answers and summary answers that are produced in computing qags on the xsum summaries and label them by the quality of the generated questions predicted answers and answer similarity scores
among the generated questions
are sensical while
are well formed but swerable using the generated summary they were conditioned upon
these gures indicate that the vast majority of questions are understandable and on topic
we frequently observe multiple questions with slightly different wordings which is likely due to the low number of answer candidates in xsum summaries which are one sentence long and due to beam search

of questions are well formed but unanswerable using the source which is usually due to a hallucinated fact in the summary that the qg model turns into a question
among predicted answers
of questions are potentially answerable using the summary but are incorrectly answered
this percentage creases to
for the article which indicates that the transfer ability of the qa model is lacking
in a small number of cases we found that while a question had a single answer in the summary it could have multiple answers in the article
finally for
of the examples the tion is answered correctly using both the article and summary but the answers have high lexical variation such that score fails to detect their similarity
while this happens in a relatively small number of cases exploring similarity metrics other than n gram based approaches could be useful
limitations we emphasize that qags and our overall framework are specically designed to tect factual inconsistencies in generated summaries relative to the source article
qags does not sure other desirable properties of generated text including uency readability or factual recall
we therefore recommend using qags in conjunction with complementary evaluation metrics
the choices of qg and qa models in qags are particular to abstractive summarization and may require adaptation to be used for other conditional text generation tasks
for example we expect that extractive summarization models may obtain nearly perfect qags scores because facts and statements are directly copied from the source article
related work automatic summarization and its evaluation are long standing lines of work in nlp dating at least as far back as the document understanding ferences chali and kolla
the primary evaluation metric then and now is rouge lin though much work has demonstrated the limited ability of rouge and its relatives to ate summaries dorr et al
liu and liu kedzie et al
i
a

other metrics have cused on specic aspects of summarization quality including content selection nenkova and neau relevance prediction daume iii and marcu and many more
there has been a recent resurgence of work aging nlu models for evaluating the factuality of generated text
goodrich et al
use mation extraction models to measure factual lap but facts are restricted to pre dened schemas
falke et al
investigate the use of nli els to evaluate the factual correctness of cnn dm summaries and conclude that current nli models are too brittle to be reliably used in this manner
kryscinski et al
train a nli based checking model by building a dataset of factual consistencies based on noise heuristic
our qa proach allows a ner grained analysis because nli operates on complete sentences whereas qags can ask many questions about the same sentence
most relatedly eyal et al
and scialom et al
use qa models to evaluate tion
we diverge from these works in two important ways
first both works use cloze style questions which are generated by masking entities in either the source document or the reference summary
we instead generate the questions with a model ing a much greater range of questions
second we produce questions conditioned on the generated summary rather than the reference summary or source article
producing questions from the ated summary is more appropriate for verifying the accuracy of the text whereas using the reference or source measures content selection
conclusion we introduce a framework for automatically ing factual inconsistencies in conditionally ated texts and use this framework to develop qags a metric for measuring inconsistencies in tive summarization
qags correlates with human judgments of factuality signicantly better than standard automatic evaluation metrics for rization and outperforms related nli based proaches to factual consistency checking
qags is naturally interpretable the questions and answers produced in computing qags indicate which kens in a generated summary are inconsistent and why
error analysis shows that future work should explore improved qa models
our approach can also be applied to diverse modalities such as lation and image captioning
overall we believe qags is useful in quantifying and incentivizing factually consistent text generation
references ekaterina ageeva mikel l
forcada francis m
ers and juan antonio perez ortiz

evaluating machine translation for assimilation via a task
in proceedings of the annual conference of the european association for machine tion pages antalya turkey
ziqiang cao furu wei wenjie li and sujian li

faithful to the original fact aware neural tive summarization
in thirty second aaai ence on articial intelligence
yllias chali and maheedhar kolla

in in proceedings of tion techniques at duc
the document understanding conference
citeseer
qian chen xiaodan zhu zhen hua ling si wei hui jiang and diana inkpen

enhanced lstm for in proceedings of the natural language inference
annual meeting of the association for tational linguistics volume long papers pages
yen chun chen and mohit bansal

fast tive summarization with reinforce selected sentence rewriting
in proceedings of the annual ing of the association for computational linguistics volume long papers pages
hal daume iii and daniel marcu

bayesian summarization at duc and a suggestion for extrinsic evaluation
in proceedings of the document standing conference vancouver usa
jacob devlin ming wei chang kenton lee and kristina toutanova

bert pre training of deep bidirectional transformers for language standing
in proceedings of the conference of the north american chapter of the association for computational linguistics human language nologies volume long and short papers pages
bonnie dorr christof monz douglas oard david zajic and richard schwartz

sic evaluation of automatic metrics for rization
technical report maryland univ college park inst for advanced puter studies
xinya du junru shao and claire cardie

ing to ask neural question generation for reading in proceedings of the comprehension
nual meeting of the association for computational linguistics volume long papers pages
matan eyal tal baumel and michael elhadad

question answering as an automatic evaluation in ric for news article summarization
ings of the conference of the north american chapter of the association for computational guistics human language technologies volume long and short papers pages
tobias falke leonardo fr ribeiro prasetya ajie ido dagan and iryna gurevych

utama ranking generated summaries by correctness an teresting but challenging application for natural guage inference
in proceedings of the ence of the association for computational tics pages
angela fan mike lewis and yann dauphin

in proceedings erarchical neural story generation
of the annual meeting of the association for computational linguistics volume long papers pages
sebastian gehrmann yuntian deng and alexander rush

bottom up abstractive summarization
in proceedings of the conference on cal methods in natural language processing pages
ben goodrich vinay rao peter j
liu and mad saleh

assessing the factual accuracy of generated text
in proceedings of the acm sigkdd international conference on knowledge discovery data mining kdd pages new york ny usa
acm
karl moritz hermann tomas kocisky edward stette lasse espeholt will kay mustafa suleyman and phil blunsom

teaching machines to read and comprehend
in advances in neural information processing systems pages
ari holtzman jan buys maxwell forbes and yejin choi

the curious case of neural text ation
arxiv preprint

chris kedzie kathleen mckeown and hal daume iii

content selection in deep learning models of in proceedings of the summarization
ference on empirical methods in natural language processing pages
kalpesh krishna and mohit iyyer

generating question answer hierarchies
in proceedings of the annual meeting of the association for putational linguistics pages florence italy
association for computational linguistics
wojciech kryscinski nitish shirish keskar bryan cann caiming xiong and richard socher

neural text summarization a critical evaluation
in proceedings of the conference on empirical methods in natural language processing volume long and short papers
wojciech kryscinski bryan mccann caiming xiong and richard socher

evaluating the factual consistency of abstractive text summarization
alon lavie and abhaya agarwal

meteor an automatic metric for mt evaluation with high levels in of correlation with human judgments
ings of the second workshop on statistical machine translation pages
association for tational linguistics
logan lebanoff john muchovej franck dernoncourt doo soon kim seokhwan kim walter chang and fei liu

analyzing sentence fusion in tive summarization
in proceedings of the shop on new frontiers in summarization pages
mike lewis yinhan liu naman goyal jan ghazvininejad abdelrahman mohamed omer levy ves stoyanov and luke zettlemoyer

bart denoising sequence to sequence training for natural language generation translation and comprehension
arxiv preprint

chin yew lin

rouge a package for automatic in text summarization evaluation of summaries
branches out pages
chia wei liu ryan lowe iulian serban mike worthy laurent charlin and joelle pineau

how not to evaluate your dialogue system an pirical study of unsupervised evaluation metrics for dialogue response generation
in proceedings of the conference on empirical methods in natural language processing pages
feifan liu and yang liu

exploring correlation between rouge and human evaluation on meeting ieee transactions on audio speech summaries
and language processing
ilya loshchilov and frank hutter

decoupled weight decay regularization
alexander miller will feng dhruv batra antoine bordes adam fisch jiasen lu devi parikh and jason weston

parlai a dialog research in proceedings of the ware platform
ference on empirical methods in natural language processing system demonstrations pages
ramesh nallapati bowen zhou cicero dos santos c aglar gulcehre and bing xiang

tive text summarization using sequence to sequence in proceedings of the rnns and beyond
signll conference on computational natural guage learning pages berlin germany
association for computational linguistics
shashi narayan shay b
cohen and mirella lapata

do nt give me the details just the summary topic aware convolutional neural networks for in proceedings of the treme summarization
conference on empirical methods in natural guage processing brussels belgium
ani nenkova and rebecca passonneau

ating content selection in summarization the mid method
in proceedings of the human language technology conference of the north american ter of the association for computational linguistics hlt naacl pages
myle ott sergey edunov alexei baevski angela fan sam gross nathan ng david grangier and michael auli

fairseq a fast extensible toolkit for sequence modeling
naacl hlt page
kishore papineni salim roukos todd ward and jing zhu

bleu a method for automatic in proceedings of uation of machine translation
the annual meeting on association for tational linguistics pages
association for computational linguistics
gabriel pereyra george tucker jan chorowski lukasz kaiser and geoffrey hinton

izing neural networks by penalizing condent output distributions
pranav rajpurkar robin jia and percy liang

know what you do nt know unanswerable tions for squad
in proceedings of the annual meeting of the association for computational guistics volume short papers pages
thomas scialom sylvain lamprier benjamin wowarski and jacopo staiano

answers unite unsupervised metrics for reinforced in proceedings of the rization models
ference on empirical methods in natural language processing and the international joint ence on natural language processing ijcnlp pages
kaitao song xu tan tao qin jianfeng lu and yan liu

mass masked sequence to quence pre training for language generation
in ternational conference on machine learning pages
adam trischler tong wang xingdi yuan justin ris alessandro sordoni philip bachman and heer suleman

newsqa a machine in proceedings of the hension dataset
shop on representation learning for nlp pages
sean welleck jason weston arthur szlam and kyunghyun cho

dialogue natural language inference
in proceedings of the annual ing of the association for computational linguistics pages florence italy
association for computational linguistics
adina williams nikita nangia and samuel bowman

a broad coverage challenge corpus for tence understanding through inference
in ings of the conference of the north american chapter of the association for computational guistics human language technologies volume long papers pages
thomas wolf lysandre debut victor sanh julien chaumond clement delangue anthony moi ric cistac tim rault remi louf morgan icz et al

transformers state of the art ral language processing
arxiv preprint

tianyi zhang varsha kishore felix wu kilian q weinberger and yoav artzi

bertscore arxiv preprint uating text generation with bert


some answers if there are more than
the model predicts the question after seeing an answer and the article
during decoding we use beam search with beam size length penalty
and trigram repetition blocking
we experimented with top k man et al
and top p fan et al
but the outputted questions while diverse were quite noisy
generations have minimum length and max length
to lter the questions we rst use simple tics including removing everything after the rst question mark in a question exact duplicates questions shorter than three tokens long for the remaining questions we use our qa model to answer each question and we remove questions for which the qa model deems unanswerable
we then take the top most probable questions dom sampling some of the ltered questions if there were too few
question answering we ne tune bert for question answering following the original work
we optimize using adamw loshchilov and ter with initial learning rate
we train for epochs with a warmup ratio of

we use the model with the best development set mance
we use
because we found the swerable questions useful for ltering out questions and questions based on hallucinated facts in the summary should be unanswerable using the source article
similar to the qg setting we append the question and answer to the source article with tervening special marker tokens
a human evaluation task design we restrict our pool of workers to us based ers
workeres are required to have at least approved hits with an acceptance rate of at least
the base reward for our task is

for each summary we include automatic quality checks cluding time checks workers who complete the task under fail the check attention checks we include exact copies of article sentences and corrupted mixtures of two article sentences as positive and negative control task
if a worker fails to answer both of these examples correctly they fail the check explanation checks for each sentence in the summary the worker is required to provide a short explanation of their decision if a worker passes all checks they are awarded a
bonus totalling
per correct tion
according to turkerview
com workers of our hit are paid well in excess of
on age
we show our annotation interfaces for the tation task for cnn dm and xsum respectively in figures and
we use slightly different tions to accommodate for the quirks of each dataset
for xsum we prepend the reference summary back onto the source article as without it workers were struggling to identify factual inconsistencies
b model and generation details question generation we ne tune bart for question generation using the same tuning parameters as the original work
we optimize label smoothed cross entropy with smoothing parameter
pereyra et al
and a peak learning rate of
we optimize for steps with warmup steps and use the model with the best perplexity on the development set
to turn newsqa into an answer conditional qg dataset we concatenate the answer to the source article with a special marker token in between
we then concatenate another special marker token and the question
at test time we get named entities and noun phrases as answer candidates using the en web sm spacy model
we downsampling if there are more than and randomly duplicating figure annotation interface and instructions for cnn dm factual consistency task
figure annotation interface and instructions for xsum factual consistency task

