abstractive text summarization based on language model conditioning and locality modeling dmitrii aksenov julian moreno schneider peter bourgonje robert schwarzenberg leonhard hennig georg rehm dfki gmbh alt moabit berlin germany rstname

r a m l c
s c v
v i x r a abstract we explore to what extent knowledge about the pre trained language model that is used is benecial for the task of abstractive summarization
to this end we experiment with conditioning the encoder and decoder of a transformer based neural model on the bert language model
in addition we propose a new method of bert windowing which allows chunk wise processing of texts longer than the bert window size
we also explore how locality modeling i
e
the explicit restriction of calculations to the local context can affect the summarization ability of the transformer
this is done by introducing dimensional convolutional self attention into the rst layers of the encoder
the results of our models are compared to a baseline and the state of the art models on the cnn daily mail dataset
we additionally train our model on the swisstext dataset to demonstrate usability on german
both models outperform the baseline in rouge scores on two datasets and show its superiority in a manual qualitative analysis
keywords summarisation language modeling information extraction information retrieval bert locality modeling
introduction text summarization is an nlp task with many real world applications
the ever increasing amount of unstructured information in text form calls for methods to cally extract the relevant information from documents and present it in condensed form
within the eld of marization different paradigms are recognised in two mensions extractive vs
abstractive and single document vs
multi document
in extractive summarization those sentences or words are extracted from a text which carry the most important information directly presenting the sult of this as the summary
abstractive summarization methods paraphrase the text and by changing the text aim to generate more exible and consistent summaries
thermore single document summarization works on gle documents while multi document summarization deals with multiple documents at once and produces a single summary
in this paper we concentrate on single document abstractive summarization
most recent abstractive models utilize the neural network based sequence to sequence proach
during training such models calculate the tional probability of a summary given the input sequence by maximizing the loss function typically cross entropy
most approaches are based on the encoder decoder work where the encoder encodes the input sequence into a vector representation and the decoder produces a new mary given the draft summary which is the part of the summary generated during previous iterations
the last layer of a decoder the generator maps hidden states to ken probabilities
we use a state of the art transformer for sequence to sequence tasks which is built primarily on the attention mechanism vaswani et al

we attempt to improve performance of abstractive text marization by improving the language encoding ties of the model
recent results have shown that the main contribution of the transformer is its multi layer tecture allowing self attention to be replaced with some other technique without a signicant drop in performance domhan wu et al

following this egy we develop a model that introduces convolution into the vanilla self attention allowing to better encode the cal dependencies between tokens
to overcome the data sparsity problem we use a pre trained language model for the encoding part of the encoder decoder setup which ates a contextualized representation of the input sequence
specically we use bert due to its bi directional context conditioning multilingualism and state of the art scores on many other tasks devlin et al

furthermore we propose a new method which allows applying bert on longer texts
the main contributions of this paper are designing two new abstractive text summarization models based on the ideas of conditioning on the pre trained guage model and application of convolutional self attention at the bottom layers of the encoder
proposing a method of encoding the input sequence in windows which ates bert s input and allows the processing of longer input texts
evaluating the performance of our models on the english and german language by conducting an ablation study on cnn dail mail and swisstext datasets and comparing it with other state of the art methods

related work

pre trained language models traditionally non contextualized embedding vectors were used for pre training neural based nlp models mikolov et al
pennington et al

recently trained language models exploiting contextualized dings such as elmo bert and xlnet raised the bar in many nlp tasks peters et al
radford et al
devlin et al
yang et al

recent tempts to use these models for text summarization can process sequences with a maximum of tokens
strated their suitability by achieving new state of the art sults zhang et al
liu liu and lapata


neural abstractive text summarization the neural approach toward abstractive summarization was largely adopted by state of the art models shi et al

a signicant contribution was the pointer generator work see et al

it uses a special layer on top of the decoder network to be able to both generate tokens from the dictionary and extract them from the input text
it uses the coverage vector mechanism to pay less attention to kens already covered by previous iterations
an example of earlier work adapting reinforcement learning rl is
the pure rl model described by paulus et al
achieved high and rouge l scores but duced unreadable summaries
its combination with cal cross entropy optimization achieved high scores to inating the unreliability problem
liu et al
the best of our knowledge were the rst to use the former model for summarization
it was only used in the decoder on top of the extraction model with various tion compression techniques to increase the size of the put sequence
zhang et al
incorporate bert into the transformer based model
they use a two stage dure exploiting the mask learning strategy
others attempt to improve their abstractive summarization models by corporating an extractive model
for example li et al
use the key information guide network to guide the summary generation process
in bottom up summarization gehrmann et al
the extractive model is used to crease the precision of the pointer generator mechanism
another strand of research adapts existing models to cope with long text
cohan et al
present the aware attention model which introduces hierarchy in the attention mechanism via calculating an additional attention vector over the sections of the input text
subramanian et al
showed that the language model trained on the combination of the original text extractive summaries erated by the model and the golden summary can achieve results comparable to standard encoder decoder based marization models

approach our text summarization model is based on the transformer architecture
this architecture adopts the original model of vaswani et al

on top of the decoder we use a pointer generator formula to increase the extractive pabilities of the network we later refer to this architecture as copytransformer
where is the probability of copying a specic word w from the source document psof is the probability of generation a word calculated by the tive summarization model and pgen is the probability of copying instead of generation


convolutional self attention the transformer like any other self attention network has in many a hierarchical multi layer architecture
figure model overview ments it was shown that this architecture tends to learn cal information in the rst layers sentence level patterns in the middle and the semantics in the upper layers raganato and tiedemann tenney et al

the tage of this approach is that during the attention operation it considers all tokens as equally important whereas tic information is mostly concentrated in certain local areas
this problem is usually specied as the problem of locality modeling
as syntactic information can help in identifying more important words or phrases it could be benecial to focus attention on these regions
a successful approach to the locality modeling task are the so called convolutions local self attention networks yang et al

essentially the problem is dealt with by the application of a dimensional convolution to the self attention operation at the network s lower layers
this strengthens dependencies among neighboring elements and makes the model distance aware when it searches for level patterns in a sequence
in other words it restricts the attention scope to the window of neighboring elements
the convolution applied to attention is illustrated in mulas and
kh i m


kh i


kh m i m


vh i


vh m oh i where qh region centered at the position i
i is the query and m m i is its attention the convolution can be extended to the dimensional area by taking interactions between features learned by the ferent attention heads of the transformer into account
in the original transformer each head independently models a distinct set of linguistic properties and dependencies among tokens raganato and tiedemann
by applying dimensional convolution where the second dimension is the index of attention head we explicitly allow each head to interact with learned features for their adjacent sub spaces
the shortcoming of the original implementation is that the rst and the last heads do not interact as they are assumed not to be adjacent
thus we assume that considering the heads sub spaces periodically we can increase the model s effectiveness by applying circular convolution to the second dimension
in section we evaluate both the original sion and our modication
n





n n





n oh i where m n h is the window region over heads and stands for the union of keys and values from different subspaces
the convolutional self attention has been shown to be very effective in machine translation and several other nlp tasks
however to our knowledge it was never applied to the text summarization problem
for the experiments ported on in this paper we created our implementation of the local attention and the convolutional self attention work transformer
it supports both and modes having the size of the kernels as system parameters
as in yang et al
we incorporate convolutional attention in the transformer encoder by positioning it in the place of the self attention in the lower layers
in tion we show that the low level modeling capabilities of our encoder provides a strong boost to the model s tion accuracy in the text summarization task


bert conditioned encoder the main task of the encoder is to remember all the mantic and syntactic information from the input text which should be used by the decoder to generate the output
knowledge transfer from the language model should oretically improve its ability to remember the important formation due to the much larger corpus used in its training phase compared to the corpus used in the text marization training phase
we thus condition our encoder on the bert language model
for the encoder conditioning we used the most forward strategy recommended for the bert based model placing the pre trained language model in the encoder as an embeddings layer
this should make the embeddings of the system context dependent
we decided not to tune the encoder on bert for the sake of memory and time economy
instead we follow the general recommendations by concatenating the hidden states of the last four layers of bert into a dimensional embedding vector vlin et al

we use two variations of the bert based encoder
the rst model uses only bert to encode the put sequence and the second model feeds bert s generated embeddings into the vanilla transformer encoder


bert windowing one of the key features of our approach is its ability to overcome the length limitations of bert allowing it to deal with longer documents
bert s maximum supported sequence length is which is smaller than the average size of texts used in most summarization datasets
our method relies on the well known method of ing which to our knowledge was never used before neither are not tokens in the traditional sense but so called wordpiece tokens see devlin et al

in the bert based models nor in abstractive text rization research figure
we apply bert to the dows of texts with strides and generate n matrices every matrix embedding one window
then we combine them by doing the reverse operation
the vectors at the ping positions are averaged by summing and dividing by the number of overlapping vectors
as a result we have the matrix of embeddings with the shape of the hidden size times the length of the text
the drawback of this approach is that we reduce the size of the context as each resulted vector is calculated based on maximum twice the window size number of tokens
besides the split of the text to equal size windows will aggravate the consistency of the input as some sentences will be split in an arbitrary manner between two adjacent windows
despite this drawback we assume that this procedure will nevertheless improve the accuracy of the encoder trained on the non truncated texts
we set the window size to the maximum size of tokens and the stride to
we consider this stride size optimal due to a trade off between the average context size and tational requirements of the model number of windows
by this trade we ensure every token to have a context except for the initial and nal tokens that only have tokens context
figure integration of bert generated contextual sentations from two windows

bert conditioned decoder in the decoder pre training was applied in a similar way
the main difference is that instead of the nal output of bert we use only its word embedding matrix without sitions
the reason behind this is that in the decoder the generated probability distribution is conditioned on the complete text previous summary draft output while bert implicitly assumes consistent and completed input zhang et al

as context independent embeddings are not enough to represent the minimum set of features to make a meaningful prediction the custom transformer decoder is always stacked on top of bert
our whole bert based model is similar to one stage bert zhang et al
and bertsumabs liu and pata but differs in the usage of the four last hidden states of bert to create contextualized representation in presence of pointer generator and capabilities to process figure two different approaches for the integration of the bert conditioning with convolutional self attention method rouge l copytransformer conv
conv
circular conv












table ablation study of model with convolutional attention on the cnn daily mail dataset kernel sizes are and long texts
in figure we show the schema of the sic model with the bert conditioned convolutional attention encoder and bert conditioned decoder


integration of bert and convolutional self attention we evaluated two different ways of integrating the conditioning with the convolutional self attention of the model s encoder figure
stacking this approach comprises feeding the generated embeddings to the convolutional self attention transformer encoder
a potential problem with this proach is that convolutional self attention is assumed to be benecial when applied in the lower layers as its locality modeling feature should help in modeling of local dencies e
g
syntax
at the same time bert is a chical model where the last layers target high level patterns in the sequences e
g
semantics
we assume that the plication of the network detecting the low level patterns on bert s output can undermine its generalization abilities
concatenation because of the considerations raised above we also develop a second approach which we call concatenation
we split the convolutional self attention transformer encoder into two networks where the rst one uses only convolutional self attention and the second inal self attention identical to the transformer encoder
then we feed the original sequences into bert and into the convolutional self attention network in parallel
the sulting embedding vectors are concatenated and fed into the transformer encoder
in this way we model the locality at the lower layers of the encoder at the cost of a smaller depth of the network assuming the same number of layers

datasets we aim to develop a system that works in a independent way
it assumes that either the upstream components are available in the respective language or they are themselves language independent such as the multi lingual version of bert
since most summarization datasets are in english however we use english for the evaluation and additionally include german to check if of our model can be applied to another language


cnn daily mail our experiments are mainly conducted on the cnn daily mail dataset hermann et al
nallapati et al

it contains a collection of news articles paired with sentence summaries published on the cnn and daily mail websites
this dataset is the standard for training summarization models
we use the non anonymized data as was used for training of the most recent state of the art models e
g
see et al

the raw dataset consists of separate text les each representing a single article or a summary
we use the data in its preprocessed version as provided by gehrmann et al
it has training pairs validation pairs and test pairs
to align the data with the vocabulary of bert we enized it using the bpe based wordpiece tokenizer vlin et al

as all samples in bert s training data are prepended with the special token we follow
figure effect of the window size on model rouge l transformer copytransformer bert encoder transformer decoder bert transformer encoder transformer decoder bert transformer encoder bert transformer decoder transformer full text bert transformer encoder transformer decoder full text transformer copytransformer bert transformer encoder transformer decoder bert transformer encoder bert transformer decoder transformer full text bert transformer encoder transformer decoder full text





































table ablation study of the bert based model on truncated and original cnn daily mail dataset model rouge l table ablation study of the bert based model on the truncated and original swisstext dataset this and add it to every source text in our dataset
in the resulting dataset the average lengths of an article and a summary are and tokens respectively
in most of our experiments we use the clipped version of the training and validation datasets with each article truncated to tokens
in the experiments on bert windowing we use the full text version


swisstext dataset to evaluate the efciency of the model in a multi lingual multi domain environment we conduct a series of ments on the german swisstext dataset
this dataset was created for the german text summarization challenge at the swiss text analytics conference swisstext zhaw
it was designed to explore ent ideas and solutions regarding abstractive tion of german texts
to the best of our knowledge it is the rst long document summarization dataset in the man language that is publicly available
the data was tracted from the german wikipedia and represents mostly biographical articles and denitions of various concepts
the dataset was tokenized by the multilingual wordpiece tokenizer devlin et al
and preprocessed in the same way as the cnn daily mail dataset
it was split into the training validation and testing sets containing and samples respectively
the average length of a source sequence is tokens which makes this dataset suitable for our experiments on windowing

experiments our system is built on the opennmt library
for training we use cross entropy loss and the adam optimizer with the noam decay method kingma and ba
tion is made via dropout and label smoothing
for uation we calculate the scores for rouge using the library
the rouge evaluation is made on the sequences of wordpiece tokens


locality modeling to evaluate the effect of convolution on self attention we introduce it in the rst layer of the encoder
we use the same kernel sizes as in yang et al

in these iments to accelerate the training process we use a small model with a hidden size of four self attention heads and three layers in the encoder and decoder
all models are trained for training steps with the coverage penalty
as a baseline we use our implementation of former
in contrast to see et al
we do not re use the attention layer for the decoder but train a new generator layer from scratch
the results are presented in table
we see that both volutions over tokens and over attention heads improve the rouge scores
standard convolution outperformed lar convolution on and rouge l by

and
percent respectively
we also investigated the effect of the window size of the dimensional convolution on rouge scores figure
in contrast to ndings in machine translation we found that size returns the best result for the summarization task


bert conditioning all hyperparameters were set equal to nd the optimal architecture of the bert based stractive summarizer we conducted an ablation study table
to the ones in experiments in convolutional self attention
three different on cnn daily main dataset we test models bert decoder transformer decoder and transformer transformer decoder
the version of bert used in the experiments is bert base
as the baseline we use the transformer without pointer erator
from the results we observe that bert improves the efciency of the model when it is used in both encoder and decoder
besides bert in the encoder is more tive when it is used to produce embeddings to be used by the standard transformer encoder than when it is used solely as an encoder
even without a pointer generator our model outperformed the copytransformer baseline by

and
on and rouge l
to evaluate our bert windowing method we conducted the experiments on the full text
our approach outperforms the baseline which proves that the method can be fully applied to texts longer than tokens
the nal formance of this model is still lower than that of the model trained on the truncated text but as the same pattern can be observed for the baselines we assumed this relates to the specics of the dataset that is prone to having important information in the rst sentence of a text
on swisstext data we use the multilingual version of bert base
we evaluated two models with bert transformer encoder and transformer and transformer decoders table
the introduction of bert into the transformer increased the and rouge l scores by

and
percent spectively
at the same time the usage of bert in the decoder decreased the overall score
we assume that the reason behind this is that in multilingual bert due to its language independence the embedding matrix outputs less precise contextualized representations which undermines their benets for the summarization task
on the non truncated texts usage of the bert transformer encoder increased the rouge scores by

and
percent
furthermore it gives us higher scores pared to the same model on truncated texts
this strates the usability of bert windowing for this lar dataset
we assume that the difference in performance on the cnn daily mail datasets reects the difference in distribution of the useful information within the text
ticularly that in the swisstext dataset it is spread more uniformly than in the cnn daily mail dataset
we ducted a small experiment comparing the average rouge score between a golden summary and the head and the tail of a document taking the rst or last n sentences where n correlates to the length of the gold summary on both datasets
the difference between taking the head and a tail on the swisstext dataset rouge l of
vs

respectively was much smaller than on cnn daily mail rouge l of
vs

respectively which rms our hypothesis
integration strategies

to evaluate the integration strategies we trained two els with the respective bert based baselines
both models have in their encoder two transformer layers and one volutional transformer layer placed on top of bert or in parallel respectively table
the method of stacking does not provide any signicant improvement
with the introduction of convolutional attention only increased by
percent while dropped by
and rouge l remained the same
considering that in many domains imally correlates with human assessment see section we dismiss this method
the concatenation strategy volution is shown to be much more efcient increasing rouge scores by

and
percent
this rms our hypothesis that locality modeling is the most cient when applied at the bottom on the non contextualized word representations
unfortunately this model failed to outperform the stacking baseline
we conclude that the concatenating architecture undermines the performance of the transformer model and the convolutional self attention is not benecial when used together with pre trained guage models
hence we decided to train our two nal models separately


model comparison for the nal comparison of our model to other state of art methods we conducted experiments on the cnn daily mail dataset
we set the hidden state to the number of transformer layers in the encoder and layers to six and the number of self attention heads to eight
hence our baseline is smaller than the original copytransformer gehrmann et al
which may be the reason why it performs slightly worse table
bert conditioning was used in both the encoder and decoder
the sizes of convolution kernels are set to and three
the networks were trained for training steps on a single nvidia geforce gtx ti
the generation of the summary was made via the beam search algorithm with the beam size set to four
finally the generated summaries were detokenized back to the quences of words separated by spaces
for the bert based model we set the minimum length of a generated summary to as we found that without such restriction the model was prone to generate shorter sequences than in the test dataset
the model outperformed the baseline by
on
on and
on rouge l
this is better than the scores of stage bert but still worse than the two stage and sumabs models
for the convolutional copytransformer we use tional self attention in the rst three layers of the encoder
it increased and rouge l by

and

furthermore we present the rst publicly available mark for the swissdata dataset table
all comparability with our other model we include results method of integration model rouge l stacking concatenation copytransformer copytransformer











table different strategies for integrating language models with convolutional self attention cnn daily mail dataset method rouge l bilstm pointer generator coverage see et al
ml intra attention paulus et al
copytransformer gehrmann et al
bottom up summarization gehrmann et al
one stage bert zhang et al
two stage bert zhang et al
ml intra attention rl paulus et al
key information guide network li et al
sentence rewriting chen and bansal bertsumabs liu and lapata copytransformer our implementation convolutional copytransformer enc
dec






































table rouge scores for various models on the cnn daily mail test set
the rst section shows different state of the art models the second section presents our models and baseline
method rouge l copytransformer our implementation convolutional copytransformer enc









table rouge scores for our models on the swisstext test set eters are equal to the cnn daily mail baseline
conditioning was used only in the encoder
the networks were trained on the truncated texts in training steps
from the results we see that the convolutional former showed much more efciency than on cnn daily mail dataset outperforming the baseline by
percent on
on and
on rouge l
the bert based model achieved the highest scores

qualitative analysis as rouge evaluation is not always a valid method for quality assessment we perceive the need for an additional manual evaluation
the best solution would be to conduct a ne grained study of the models outputs by manually ing them in terms of semantic coherence grammaticality
however due to the time consuming nature of such an evaluation we reverted to a qualitative analysis comparing several summaries generated by different models
figure includes the reference summary and those generated by the different models
comparing the rst sentence we see that the vanilla transformer model performed worse by copying only part of the original sentence omitting some characters in the word meteorological
the model with convolution has copied the whole sentence but still made a spelling ror
finally only the bert based model succeeded to erate the right token meteorological
also we see that while the bert based model s summary conveys the same meaning as the gold summary the convolutional former generates one and transformer two sentences that are not present in the gold summary
overall on the given for the bigger model
at the same time we found that the smaller model without the copy mechanism achieved higher scores with

and
rouge l
this needs to be explored in future work
example all models provided a summary of extractive ture and only the bert based model shows some level of abstractiveness merging parts of the two sentences into the single one in the second summary s sentence
this is far from the gold summary where every sentence in some way paraphrases the original text
hence given this particular example our models demonstrate some explicit ments
still abstractive summarization remains ing
the paraphrasing capabilities of all state of the art systems are low and the models are not guaranteed to duce summaries which follow the initial order of the quence of events

discussion summarization evaluation rouge lin is the most widely adopted metric used for evaluating automatic text summarization approaches
the evaluation is made though comparison of a set of system generated candidate summaries with a gold dard summary
the availability of the corresponding ware and its performance contributed to its popularity han and goharian
despite its adoption in many studies the metric faced some key criticisms
the main criticism of rouge is that it does not take into account the meaning expressed in the sequences
the ric was developed based on the assumption that a high ity generated candidate summary should share many words with a single human made gold standard summary
this sumption may be very relevant to extractive but not to stractive summarization where different terminology and paraphrasing can be used to express the same meaning han and goharian
this results in the metric ing low scores to any summary not matching the gold dard on the surface level
this also allows cheating the metric by generating ungrammatical and nonsensical gold summary researchers are developing a computer that can write weather forecasts
it takes meteorological data and writes a report designed to mimic a human
this process is known as natural language generation lrb nlg rrb
a prototype system will be tested on the bbc website later this year
transformer researchers from london and edinburgh have developed a computer that can collateological information
these puter generated weather updates are being tested by scientists at heriot watt university and university college london
if the project is successful a prototype system will be tested by generating local weather reports on the bbc s website
currently the bbc website features reports written by meteorologists
convolutional transformer researchers from london and edinburgh have developed a computer that can collate meterological information and then produce forecasts as if they were written by a human
it uses a process known as natural language generation lrb nlg rrb
these computer generated weather updates are being tested by scientists at heriot watt university and university college london
if the project is successful a prototype system will be tested by generating local weather reports on the bbc s website
bert transformer researchers from london and edinburgh have developed a computer that can collate meteorological information and produce forecasts as if they were written by a human
using met ofce data it uses a process known as natural language generation lrb nlg rrb
if the project is successful a prototype system will be tested by generating local weather reports on the bbc s website
figure comparison of the output of models on an example form cnn daily mail testset
surface realisation mistakes are highlighted in green and a typical abstractive feature illustrating re arranging of the sentence is highlighted in blue
maries having very high rouge scores
sjobergh show how this can be achieved by choosing the most quent bigrams from the input document
rouge adoption relies on its correlation with human in the rst research on the duc and sessment
datasets containing news articles rouge indeed showed a high correlation with the human judgments lin dorr et al

however more recent research tions the suitability of rouge for various settings
roy and dang show that on duc data the tic and responsiveness scores of some systems do not respond to the high rouge scores
cohan and ian demonstrate that for summarization of scientic texts and rouge l have very low correlations with the gold summaries
rouge n correlates better but is still far from the ideal case
this follows the result of murray et al
showing that the unigram match tween the candidate summary and gold summary is not an accurate metric to assess quality
another problem is that the credibility of rouge was demonstrated for the systems which operated in the scoring range
peyrard show that different rization evaluation metrics correlate differently with human judgements for the higher scoring range in which state the art systems now operate
furthermore improvements measured with one metric do not necessarily lead to provements when using others
this concern led to the development of new evaluation rics
peyrard dene metrics for important cepts with regard to summariazion redundancy vance and informativeness in line with shannon s entropy
from these denitions they formulate a metric of tance which better correlates to human judgments
clark et al
propose the metric of sentence mover s larity which operates on the semantic level and also better correlates with human evaluation
a summarization model trained via reinforcement learning with this metric as ward achieved higher scores in both human and based evaluation
despite these drawbacks the broad adoption of rouge makes it the only way to compare the efciency of our model with other state of the art models
the evaluation of our system on the swissdata dataset conrms that its ciency in terms of rouge is not restricted to cnn daily mail data only

conclusion we present a new abstractive text summarization model which incorporates convolutional self attention in bert
we compare the performance of our system to a baseline and to competing systems on the cnn daily mail data set for english and report an improvement over state of art results using rouge scores
to establish suitability of our model to languages other than english and domains other than that of the cnn daily mail data set we apply our model to the german swisstext data set and present scores on this setup
a key contribution of our model is the ability to deal with texts longer than bert s window size which is limited to wordpiece tokens
we present a cascading approach and evaluate this on texts longer than this window size and demonstrate its performance when dealing with longer input texts
the source code of our system is publicly available
a functional service based on the model is currently being tegrated as a summarization service in the platforms lynx moreno schneider et al
qurator rehm et al
and european language grid rehm et al

acknowledgements the work presented in this paper has received funding from the european union s horizon research and tion programme under grant agreement no
lynx and from the german federal ministry of education and research bmbf through the project qurator tumskern no


com axenov bert summ opennmt
bibliographical references chen y

and bansal m

fast abstractive marization with reinforce selected sentence rewriting
in proceedings of the annual meeting of the tion for computational linguistics volume long pers pages melbourne australia july
ciation for computational linguistics
clark e
celikyilmaz a
and smith n
a

sentence mover s similarity automatic evaluation for multi sentence texts
in proceedings of the annual meeting of the association for computational tics pages florence italy july
association for computational linguistics
cohan a
and goharian n

revisiting rization evaluation for scientic articles
available line arxiv
cohan a
dernoncourt f
kim d
s
bui t
kim s
chang w
and goharian n

a discourse aware attention model for abstractive summarization of long documents
in naacl hlt
conroy j
m
and dang h
t

mind the gap gers of divorcing evaluations of summary content from linguistic quality
in proceedings of the tional conference on computational linguistics coling pages manchester uk august
coling organizing committee
devlin j
chang m

lee k
and toutanova k

bert pre training of deep bidirectional formers for language understanding
in proceedings of the conference of the north american chapter of the association for computational linguistics human language technologies volume long and short pers pages minneapolis minnesota june
association for computational linguistics
domhan t

how much attention do you need a granular analysis of neural machine translation tectures
in proceedings of the annual meeting of the association for computational linguistics volume long papers pages melbourne tralia july
association for computational linguistics
dorr b
monz c
president s
schwartz r
and jic d

a methodology for extrinsic evaluation of text summarization does rouge correlate in ceedings of the acl workshop on intrinsic and sic evaluation measures for machine translation summarization pages ann arbor michigan june
association for computational linguistics
gehrmann s
deng y
and rush a

bottom up abstractive summarization
in proceedings of the conference on empirical methods in natural language processing pages
hermann k
m
kocisky t
grefenstette e
espeholt l
kay w
suleyman m
and blunsom p

in teaching machines to read and comprehend
ceedings of the international conference on neural information processing systems volume pages cambridge ma usa
mit press
kingma d
and ba j

adam a method for stochastic optimization
learning representations
international conference on li c
xu w
li s
and gao s

guiding eration for abstractive text summarization based on key information guide network
in proceedings of the conference of the north american chapter of the sociation for computational linguistics human guage technologies volume short papers pages new orleans louisiana june
association for putational linguistics
lin c


rouge a package for automatic uation of summaries
in text summarization branches out pages barcelona spain july
association for computational linguistics
liu y
and lapata m

text summarization with pretrained encoders
proceedings of the ence on empirical methods in natural language cessing and the international joint conference on natural language processing emnlp ijcnlp
liu p
j
saleh m
pot e
goodrich b
sepassi r
kaiser l
and shazeer n

generating wikipedia by summarizing long sequences
in tional conference on learning representations
liu y

fine tune bert for extractive tion
available online arxiv
mikolov t
sutskever i
chen k
corrado g
s
and dean j

distributed representations of words and phrases and their compositionality
in c
j
c
burges al
editors advances in neural information processing systems pages
curran ciates inc
moreno schneider j
rehm g
montiel ponsoda e
rodriguez doncel v
revenko a
karampatakis s
khvalchik m
sageder c
gracia j
and maganza f

orchestrating nlp services for the legal main
in nicoletta calzolari al
editors proceedings of the language resources and evaluation ference lrec marseille france
european language resources association elra
accepted for publication
submitted version available as preprint
murray g
renals s
and carletta j

tive summarization of meeting recordings
in speech eurospeech european conference on speech communication and technology lisbon tugal september pages
nallapati r
zhou b
dos santos c
c
and xiang b

abstractive text summarization using sequence to sequence rnns and beyond
in ings of the signll conference on computational natural language learning pages berlin germany august
association for computational guistics
paulus r
xiong c
and socher r

a deep forced model for abstractive summarization
in tional conference on learning representations
pennington j
socher r
and manning c
d

glove global vectors for word representation
in empirical methods in natural language processing emnlp pages
pers
shi t
keneshloo y
ramakrishnan n
and reddy c
k

neural abstractive text summarization with sequence to sequence models
available online arxiv
sjobergh j

older versions of the rougeeval marization evaluation system were easier to fool
mation processing management
text summarization
subramanian s
li r
pilault j
and pal c

on extractive and abstractive neural document tion with transformer language models
available online arxiv
tenney i
das d
and pavlick e

bert ers the classical nlp pipeline
proceedings of the annual meeting of the association for computational linguistics
vaswani a
shazeer n
parmar n
uszkoreit j
jones l
gomez a
n
kaiser l
and polosukhin i

attention is all you need
in proceedings of the ternational conference on neural information ing systems pages usa
curran associates inc
wu f
fan a
baevski a
dauphin y
and auli m

pay less attention with lightweight and dynamic convolutions
in international conference on learning representations
yang b
wang l
wong d
f
chao l
s
and tu z

convolutional self attention networks
ceedings of the conference of the north
yang z
dai z
yang y
carbonell j
salakhutdinov r
r
and le q
v

xlnet generalized toregressive pretraining for language understanding
in h
wallach al
editors advances in neural tion processing systems pages
curran associates inc
zhang h
cai j
xu j
and wang j

pretraining based natural language generation for text summarization
in proceedings of the conference on computational natural language learning conll pages hong kong china november
ation for computational linguistics
zhaw

german text summarization challenge
swiss text analytics conference
available online
peters m
neumann m
iyyer m
gardner m
clark c
lee k
and zettlemoyer l

deep alized word representations
in proceedings of the conference of the north american chapter of the ation for computational linguistics human language technologies volume long papers pages new orleans louisiana june
association for computational linguistics
peyrard m

a simple theoretical model of tance for summarization
in proceedings of the nual meeting of the association for computational guistics pages florence italy july
ation for computational linguistics
peyrard m

studying summarization evaluation metrics in the appropriate scoring range
in proceedings of the annual meeting of the association for putational linguistics pages florence italy july
association for computational linguistics
radford a
wu j
child r
luan d
amodei d
and sutskever i

language models are unsupervised multitask learners
available online
raganato a
and tiedemann j

an analysis of encoder representations in transformer based machine translation
in proceedings of the emnlp shop blackboxnlp analyzing and interpreting neural networks for nlp pages brussels belgium november
association for computational linguistics
rehm g
berger m
elsholz e
hegele s
kintzel f
marheinecke k
piperidis s
deligiannis m
nis d
gkirtzou k
labropoulou p
bontcheva k
jones d
roberts i
hajic j
hamrlova j
kacena l
choukri k
arranz v
vasiljevs a
anvari o
lagzdin s a
melnika j
backfried g
dikici e
janosik m
prinz k
prinz c
stampler s
aniola d
perez j
m
g
silva a
g
berro c
mann u
renals s
and klejch o

european language grid an overview
in nicoletta calzolari et al
editors proceedings of the language sources and evaluation conference lrec seille france
european language resources ation elra
accepted for publication
rehm g
bourgonje p
hegele s
kintzel f
schneider j
m
ostendorff m
zaczynska k
berger a
grill s
rauchle s
rauenbusch j
rutenburg l
schmidt a
wild m
hoffmann h
fink j
schulz s
seva j
quantz j
bottger j
matthey j
fricke r
thomsen j
paschke a
qundus j
a
hoppe t
karam n
weichhardt f
fillies c
neudecker c
gerber m
labusch k
rezanezhad v
schaefer r
zellhofer d
siewert d
bunk p
pintscher l
aleynikova e
and heine f

qurator innovative gies for content and data curation
in adrian paschke et al
editors proceedings of qurator the conference for intelligent content solutions berin many
ceur workshop proceedings volume
january
see a
liu p
j
and manning c
d

get to the point summarization with pointer generator networks
proceedings of the annual meeting of the tion for computational linguistics volume long
