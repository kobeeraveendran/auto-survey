neural extractive text summarization with syntactic compression jiacheng xu and greg durrett department of computer science the university of texas at austin jcxu
utexas
edu p e s l c
s c v
v i x r a abstract recent neural network approaches to rization are largely either selection based traction or generation based abstraction
in this work we present a neural model for single document summarization based on joint extraction and syntactic compression
our model chooses sentences from the document identies possible compressions based on stituency parses and scores those sions with a neural model to produce the nal summary
for learning we construct acle extractive compressive summaries then learn both of our components jointly with this supervision
experimental results on the cnn daily mail and new york times datasets show that our model achieves strong performance comparable to state of the art systems as evaluated by rouge
over our approach outperforms an off shelf compression module and human and manual evaluation shows that our model s put generally remains grammatical
introduction neural network approaches to document marization have ranged from purely extractive cheng and lapata nallapati et al
narayan et al
to abstractive rush et al
nallapati et al
chopra et al
tan et al
gehrmann et al

tractive systems are robust and straightforward to use
abstractive systems are more exible for ied summarization situations grusky et al
but can make factual errors cao et al
li et al
or fall back on extraction in practice see et al

extractive and compressive tems berg kirkpatrick et al
qian and liu durrett et al
combine the strengths of both approaches however there has been tle work studying neural network models in this vein and the approaches that have been employed figure diagram of the proposed model
extraction and compression are modularized but jointly trained with supervision derived from the reference summary
typically use based sentence compression chen and bansal
in this work we propose a model that can bine the high performance of neural extractive systems additional exibility from compression and interpretability given by having discrete pression options
our model rst encodes the source document and its sentences and then quentially selects a set of sentences to further press
each sentence has a set of compression tions available that are selected to preserve ing and grammaticality these are derived from syntactic constituency parses and represent an panded set of discrete options from prior work berg kirkpatrick et al
wang et al

the neural model additionally scores and chooses which compressions to apply given the context of the document the sentence and the decoder model s recurrent state
a principal challenge of training an extractive and compressive model is constructing the cle summary for supervision
we identify a set of high quality sentences from the document with beam search and derive oracle compression labels in each sentence through an additional renement process
our model s training objective combines these extractive and compressive components and learns them jointly
we conduct experiments on standard single document news summarization datasets cnn daily mail hermann et al
and the new compression module extraction module input documentoutput summary ref compression rules we refer to the rules rived in li et al
wang et al
and durrett et al
and design a concise set of syntactic rules including the removal of
itive noun phrases
relative clauses and bial clauses
adjective phrases in noun phrases and adverbial phrases see figure
dive verb phrases as part of noun phrases see ure
prepositional phrases in certain rations like on monday
content within these s and other parentheticals
figure shows examples of several sion rules applied to a short snippet
all binations of compressions maintain ity though some content is fairly important in this context the vp and pp and should not be deleted
our model must learn not to delete these elements
compressability summaries from different sources may feature various levels of sion
at one extreme a summary could be fully sentence extractive at another extreme the editor may have compressed a lot of content in a tence
in section we examine this question on our summarization datasets and use it to motivate our choice of evaluation datasets
universal compression with rouge while we use syntax as a source of compression options we note that other ways of generating compression options are possible including using labeled pression data
however supervising compression with rouge is critical to learn what information is important for this particular source and in any case labeled compression data is unavailable in in section we compare our many domains
model to off the shelf sentence compression ule and nd that it substantially underperforms our approach
model our model is a neural network model that encodes a source document chooses sentences from that document and selects discrete compression tions to apply
the model architecture of sentence extraction module and text compression module are shown in figure and

extractive sentence selection a single document consists of n sentences d sn
the i th sentence is denoted as wim where wij is the figure text compression example
in this case timate well known with their furry friends and featuring


friends are deletable given compression rules
york times annotated corpus sandhaus
our model matches or exceeds the state of the art on all of these datasets and achieves the largest improvement on cnn
rouge over our extractive baseline due to the more compressed nature of cnn summaries
we show that our model s compression threshold is robust across a range of settings yet tunable to give length summaries
finally we investigate the ency and grammaticality of our compressed tences
the human evaluation shows that our system yields generally grammatical output with many remaining errors being attributed to the parser
compression in summarization sentence compression is a long studied problem dealing with how to delete the least critical mation in a sentence to make it shorter knight and marcu martins and smith cohn and lapata wang et al
li et al

many of these approaches are syntax driven though end to end neural models have been proposed as well filippova et al
wang et al

past non neural work on summarization has used both syntax based kirkpatrick et al
woodsend and lapata and discourse based carlson et al
hirao et al
li et al
compressions
our approach follows in the syntax driven vein
our high level approach to summarization is shown in figure
in section we describe the models for extraction and compression
our pression depends on having a discrete set of valid compression options that maintain the cality of the underlying sentence which we now proceed to describe
code are model jiacheng xu neu compression sum available at full model output and the pre trained
featuringintimateportraitsnpnpnpnnsvbgjjnnswith their furry friendswell knownartistsjjppvp figure sentence extraction module of jecs
words in input document sentences are encoded with bilstms
two layers of cnns aggregate these into sentence representations hi and then the document representation vdoc
this is fed into an attentive lstm decoder which selects sentences based on the decoder state d and the tations hi similar to a pointer network
th word in
the content selection module learns to pick up a subset of d denoted as d sk d where k sentences are selected
sentence document encoder we rst use a bidirectional lstm to encode words in each sentence in the document separately and then we apply multiple convolution ers and max pooling layers to extract the resentation of every sentence
specically him wim and hi him where hi is a resentation of the i th sentence in the document
this process is shown in the left side of ure illustrated in purple blocks
we then gregate these sentence representations into a ument representation vdoc with a similar bilstm and cnn combination shown in figure with ange blocks
decoding the decoding stage selects a number of sentences given the document representation vdoc and sentences representations hi
this cess is depicted in the right half of figure
we use a sequential lstm decoder where at each time step we take the representation h of the last selected sentence the overall document tor vdoc and the recurrent state and produce a distribution over all of the remaining sentences excluding those already selected
this approach resembles pointer network style approaches used in past work zhou et al

formally we write this as dt hk vdoc scoret whhi hk vdoc hi i where hk is the representation of the sentence lected at time step t
is the decoding figure text compression module
a neural classier scores the compression option with their furry friends in the sentence and broader document context and cides whether or not to delete it
den state from last time step
wd wh wm and parameters in lstm are learned
once a sentence is selected it can not be selected again
at test time we use greedy decoding to identify the most likely sequence of sentences under our model

text compression after selecting the sentences the text sion module evaluates our discrete compression options and decides whether to remove certain phrases or words in the selected sentences
ure shows an example of this process for ing whether or not to delete a pp in this sentence
this pp was marked as deletable based on rules described in section
our network then encodes this sentence and the compression combines this information with the document context vdoc and decoding context hdec and uses a feedforward work to decide whether or not to delete the span
our experiments we decode for a xed number of sentences tuned for each dataset as in prior extractive work narayan et al

we experimented with dynamically choosing a number of sentences and found this to make little difference
often cats can be overlooked in favour of their cuter canine counterparts
but a new book artists and their cats is putting felines back on the map
philadelphia based artist and journalist alison nastasi has collated a collection of intimate portraits featuring well known artists with their furry friends
in this image spanish surrealist painter salvador dali poses with his cat babou a colombian wild cat
blstm cnnblstm cnn blstm doccnnphiladelphia well known artists with their furry friends
contextualized encoder compressionencsentence classicationlabel let ci cil denote the possible pression spans derived from the rules described in section
let yi c be a binary variable equal to if we are deleting the cth option of the ith sentence
our text compression module models st as described in the following section
compression encoder we use a contextualized encoder elmo peters et al
to compute contextualized word representations
we then use cnns with max pooling to encode the sentence shown in blue in figure and the candidate pression shown in light green in figure
the sentence representation vsent and the compression span representation vcomp are concatenated with the hidden state in sentence decoder hdec and the document representation vdoc
compression classier we feed the catenated representation to a feedforward ral network to predict whether the sion span should be deleted or kept which is formulated as a binary classication lem
this classier computes the nal probability st vdoc vcomp si
the overall probability of a summary s y where s is the sentence oracle and y is the is the product of compression label tion and compression models s
cci heuristic deduplication inspired by the gram avoidance trick proposed in paulus et al
to reduce redundancy we take full tage of our linguistically motivated compression rules and the constituent parse tree and allow our model to compress deletable chunks with dant information
we therefore take our model s output and apply a postprocessing stage where we remove any compression option whose unigrams are completely covered elsewhere in the summary
we perform this compression after the model diction and compression
training our model makes a series of sentence extraction decisions s and then compression decisions y
to supervise it we need to derive gold standard bels for these decisions
our oracle identication approach relies on rst identifying an oracle set of sentences and then the oracle compression reference artist and journalist alison nastasi put gether the portrait collection
also features images of casso frida kahlo and john lennon
reveals quaint sonality traits shared between artists and their felines
document


philadelphia based artist and ist alison nastasi has collated a collection of intimate portraits featuring well known artists with their furry friends



compression rbf
label ratio raf philadelphia based intimate well known featuring


their furry friends







del del del keep table oracle label computation for the text pression module
rbf and raf are the rouge scores before and after compression
the ratio is dened as raf
rouge increases when words not appearing in rbf the reference are deleted
rouge can decrease when terms appearing in the reference summary like ing are deleted
tions

oracle construction sentence extractive oracle we rst identify an oracle set of sentences to extract using a beam search procedure similar to maximal marginal relevance mmr carbonell and goldstein
for each additional sentence we propose to add we compute a heuristic cost equal to the rouge score of a given sentence with respect to the reference summary
when pruning states we calculate the rouge score of the combination of sentences currently selected and sort in ing order
let the beam width be
the time plexity of the approximate approach is where in practice n and n
we set and n which means we only consider the rst sentences in the document
the beam search procedure returns a beam of different sentence combinations in the nal beam
we use the sentence extractive oracle for both the extraction only model and the joint compression model
oracle compression labels to form our joint extractive and compressive oracle we need to give the compression decisions binary labels yi in each set of extracted sentences
for simplicity and computational efciency we assign each sentence on which sentences are extracted different compression decisions may be optimal however re deriving these with a dynamic oracle goldberg and nivre is prohibitively expensive during training
category cnn dm nyt bad weak positive strong positive table compressibility the oracle label distribution over three datasets
compressions in the bad gory decrease rouge and are labeled as negative do not delete while weak positive less than rouge improvement and strong positive greater than both represent rouge improvements
cnn features much more compression than the other datasets
a single yi c independent of the context it occurs in
for each compression option we assess the value of it by comparing the rouge score of the sentence with and without this phrase
any tion that increases rouge is treated as a pression that should be applied
when calculating this rouge value we remove stop words include stemming
we run this procedure on each of our oracle tractive sentences
the fraction of positive and negative labels assigned to compression options is shown for each of the three datasets in table
cnn is the most compressable dataset among cnn dm and nyt
ilp based oracle construction past work has derived oracles for extractive and compressive systems using integer linear programming ilp gillick and favre berg kirkpatrick et al

following their approach we can directly optimize for rouge recall of an extractive or compressive summary in our framework if we specify a length limit
however we evaluate on rouge as is standard when comparing to ral models that do nt produce xed length maries
optimizing for rouge can not be mulated as an ilp since computing precision quires dividing by the number of selected words making the objective no longer linear
we perimented with optimizing for rouge directly by nding optimal rouge recall maries at various settings of maximum summary length
however these summaries frequently tained short sentences to ll up the budget and the collection of summaries returned tended to be less diverse than those found by beam search

learning objective avoid committing to a single oracle summary for the learning process
our procedure from tion
can generate m extractive oracles s i let s i t denote the gold sentence for the i acle at timestep t
past work narayan et al
chen and bansal has employed icy gradient in this setting to optimize directly for rouge
however because oracle summaries usually have very similar rouge scores we choose to simplify this objective as lsent
put another m way we optimize the log likelihood averaged across m different oracles to ensure that each has high likelihood
we use m oracles during training
the oracle sentence indices are sorted cording to the individual salience rouge score rather than document order
log i s the objective of the compression module is ned as lcomp i s where i c is the probability of the target sion for the th compression options of the i th sentence
the joint loss function is l lsent lcomp
we set in practice
log experiments we evaluate our model on two axes
first for tent selection we use rouge as is standard
ond we evaluate the grammaticality of our model to ensure that it is not substantially damaged by compression

experimental setup datasets we evaluate the proposed method on the three popular news summarization datasets new york times corpus sandhaus cnn and dailymail dm hermann et al

as discussed in section compression will give different results on different datasets ing on how much compression is optimal from the standpoint of reproducing the reference maries which changes how measurable the impact of compression is
in table we show the pressability of these three datasets how valuable various compression options seem to be from the standpoint of improving rouge
we found that cnn has signicantly more positive compression options than the other two
critically cnn also has the shortest references words on average often many oracle summaries achieve very to ilar rouge values
we therefore want details about the experimental setup tion details and human evaluation are provided in the pendix
model lead ours refresh narayan et al
latsum zhang et al
banditsum dong et al
leaddedup leadcomp extraction extlstmdel jecs cnn








r l

















table experimental results on the test sets of cnn
indicates models evaluates with our own rouge rics
our model outperforms our extractive model and lead based baselines as well as prior work
compared to for daily mail see appendix
in our experiments we rst focus on cnn and then evaluate on the other datasets
models we present several variants of our model to show how extraction and compression work jointly
in extractive summarization the lead baseline rst k sentences is a strong line due to how newswire articles are written
leaddedup is a non learned baseline that uses our heuristic deduplication technique on the lead sentences
leadcomp is a compression only model where compression is performed on the lead sentences
this shows the effectiveness of the compression module in isolation rather than in the context of abstraction
extraction is the tion only model
jecs is the full joint extractive and compressive summarizer
we compare our model with various abstractive and extractive summarization models
neusum zhou et al
uses a model to dict a sequence of sentences indices to be picked up from the document
our extractive approach is most similar to this model
refresh narayan et al
banditsum dong et al
and latsum zhang et al
are extractive rization models for comparison
we also compare with some abstractive models including cov see et al
fars chen and bansal and cbdec jiang and bansal
we also compare our joint model with a pipeline model with an off the shelf compression module
we implement a deletion based bilstm model for sentence compression wang et al
and run the model on top of our extraction output
reimplemented the authors model following their specication and matched their accuracy
for fair model cnndm lead ours refresh narayan et al
neusum latsum zhang et al
latsum compression banditsum cbdec jiang and bansal fars chen and bansal leaddedup leadcomp extraction jecs























r l











table experimental results on the test sets of ndm
the portion of cnn is roughly one of tenth of dm
gains are more pronounced on cnn because this dataset features shorter more compressed ence summaries
the pipeline model is denoted as extlstmdel

results on cnn table shows experiments results on cnn
we list performance of the lead baseline and the formance of competitor models on these datasets
starred models are evaluated according to our rouge metrics numbers very closely match the originally reported results
our model achieves substantially higher formance than all baselines and past systems rouge compared to any of these
on this dataset compression is substantially useful
pression is somewhat effective in isolation as shown by the performance of leaddedup and leadcomp
but compression in isolation still gives less benet on top of lead than when combined with the extractive model jecs in the joint framework
furthermore our model beats the pipeline model extlstmdel which shows the necessity of training a joint model with rouge supervision

results on combined cnndm and nyt we also report the results on the full cnndm and nyt although they are less compressable
ble and table shows the experimental results on these datasets
our models still yield strong performance pared to baselines and past work on the cnndm son we tuned the deletion threshold to match the sion rate of our model other choices did not lead to better rouge scores
model lead leaddedup leadcomp extraction jecs









r l




table experimental results on the dataset
and is reported
jecs tially outperforms our lead based systems and our tractive model
dataset
the extraction model achieves parable results to past successful extractive proaches on cnndm and jecs improves on this across the datasets
in some cases our model slightly underperforms on
one sible reason is that we remove stop words when constructing our oracles which could mate the importance of bigrams containing words for evaluation
finally we note that our compressive approach substantially outperforms the compression augmented latsum model
that model used a separate model for ing which is potentially harder to learn than our compression model
on nyt we see again that the inclusion of compression leads to improvements in both the lead setting as well as for our full jecs model

grammaticality we evaluate grammaticality of our compressed summaries in three ways
first we use zon mechanical turk to compare different pression techniques
second to measure absolute grammaticality we use an automated out of box tool grammarly
finally we conduct manual analysis
human evaluation we rst conduct a human evaluation on the amazon mechanical turk form
we ask turkers to rank different sion versions of a sentence in terms of icality
we compare our full jecs model and the off the shelf pipeline model extlstmdel which have matched compression ratios
we also propose another baseline extractdropout which randomly drops words in a sentence to match the compression ratio of the other two et al
do not use the dataset so our results are not directly comparable to theirs
durrett et al
use a different evaluation setup with a hard limit on the summary length and evaluation on recall only
model preference error ext extdrop extlstmdel jecs



table human preference rouge and grammarly grammar checking results
we asked turkers to rank the models output based on grammaticality
error shows the number of grammar errors in sentences reported by grammarly
our jecs model achieves the highest rouge and is preferred by humans while still making relatively few errors
els
the results are shown in table
ers give roughly equal preference to our model and the extlstmdel model which was learned from supervised compression data
however our jecs model achieves substantially higher rouge score indicating that it represents a more effective compression approach
we found that absolute grammaticality ments were hard to achieve on mechanical turk turkers ratings of grammaticality were very noisy and they did not consistently rate true article sentences above obviously noised variants
fore we turn to other methods as described in the next two paragraphs
automatic grammar checking we use marly to check sentences sampled from the outputs of the three models mentioned above from cnn
both extlstmdel and jecs make a small number of grammar errors not much higher than the purely extractive baseline
one major source of errors for jecs is having the wrong article after the deletion of an adjective like an awesome style
manual error analysis to get a better sense of our model s output we conduct a manual ysis of our applied compressions to get a sense of how many are valid
we manually examined model summaries comparing the output with the raw sentences before compression and tied the following errors
eight bad deletions due to parsing errors like a uk jj national from london

eight inappropriate adjective deletions causing correctness issues with respect to the erence document like former president and clear weapon

three other errors partial tion of slang inappropriate pp attachment tion and an unhandled grammatical construction reference summary prediction with compressions mullah omar the reclusive founder of the afghan taliban is still in charge a new biography claims
an ex taliban insider says there have been rumors that the one eyed militant is dead
cnn mullah mohammed omar is still the leader of the taliban s declared islamic emirate of afghanistan
the taliban s cultural sion released the page document in several different translations on the movement s website ostensibly to commemorate the anniversary of an april meeting in afghanistan s kandahar province when an assembly of afghans swore allegiance to omar
rebecca francis photo with a raffe was shared by ricky gervais
francis was threatened on twitter for the picture
francis a hunter said the giraffe was close to death and became food for locals
cnn five years ago rebecca francis posed for a photo while lying next to a dead giraffe
the trouble started monday when comedian ricky gervais tweeted the photo with a question
francis who has appeared on the nbc sports network outdoor lifestyle show eye of the hunter and was the ject of an interview with hunting life in late march responded in a statement to huntinglife
com on tuesday which was posted on its facebook page
president barack frida ghitis obama is right to want a deal but this one gives iran too much
she says the framework agreement starts lifting iran sanctions much too soon
cnn president barack obama tied himself to the mast of a nuclear deal with iran even before he became the democratic candidate for president
reaching a good solid agreement with iran is a worthy desirable goal
but the process has unfolded under the destructive inuence of political considerations ening america s hand and strengthening iran
table examples of applied compressions
the top two are sampled from among the most compressed examples in the dataset
our jecs model is able to delete both large chunks especially temporal pps giving dates of events as well as individual modiers that are nt determined to be relevant to the summary e

the specication of the anniversary
the last example features more modest compression
students rst athletes second
examples of output are shown in table
the rst two examples are sampled from the top of the most compressed examples in the corpus
we see a variety of compression options that are used in the rst two examples including removal of temporal pps large subordinate clauses tives and parentheticals
the last example tures less compression only removing a handful of adjectives in a manner which slightly changes the meaning of the summary
improving the parser and deriving a more semantically aware set of compression rules can help achieving better grammaticality and ity
however we note that such errors are largely orthogonal to the core of our approach a more ned set of compression options could be dropped into our system and used without changing our fundamental model
compression analysis compression threshold compression in our model is an imbalanced binary classication lem
the trained model s natural classication threshold probability of del
may not be optimal for downstream rouge
we experiment with varying the classication threshold from no deletion only heuristic deduplication to all compressible pieces removed
the results on cnn are shown in figure where we show the figure effect of changing the compression old on cnn
the y axis shows the average of the of and
the dotted line is the tive baseline
the model outperforms the extractive model and achieves nearly optimal performance across a range of threshold values
average rouge value at different compression thresholds
the model achieves the best mance at
but performs well in a wide range from
to

our compression is therefore robust yet also provides a controllable parameter to change the amount of compression in produced summaries
compression type analysis we further break down the types of compressions used in the model
table shows the compressions that our model ends up choosing at test time
pps are often pressed by the deduplication mechanism because the compressible pps tend to be temporal and tion adjuncts which may be redundant across





averageextraction node type len of comps comp acc dedup jj pp advp prn



table the compressions used by our model on cnn average lengths and the fraction of that constituency type among compressions taken by our model
comp acc indicates how frequently that compression was taken by the oracle note that error especially keeping constituents that we should nt may have minimal pact on summary quality
dedup indicates the age of chosen compressions which arise from cation as opposed to model prediction
proposed a deep generative model for text compression
zhang et al
explored the compression module after the extraction model but the separation of these two modules hurt the formance
for this work we nd that relying on syntax gives us more easily understandable and controllable compression options
contemporaneously with our work mendes et al
explored an extractive and pressive approach using compression integrated into a sequential decoding process however their approach does not leverage explicit syntax and makes several different model design choices
tences
without the manual deduplication nism our model matches the ground truth around of the time
however a low accuracy here may not actually cause a low nal rouge score as many compression choices only affect the nal rouge score by a small amount
more details about compression options are in the tary material
related work neural extractive summarization neural works have shown to be effective in extractive summarization
past approaches have structured the decision either as binary classication over sentences cheng and lapata nallapati et al
or classication followed by ranking narayan et al

zhou et al
used a seq to seq decoder instead
for our model text compression forms a module largely orthogonal to the extraction module so additional improvements to extractive modeling might be expected to stack with our approach
syntactic compression prior to the explosion of neural models for summarization syntactic compression martins and smith send and lapata was relatively more mon
several systems explored the usage of stituency parses berg kirkpatrick et al
wang et al
li et al
as well as based approaches hirao et al
durrett et al

our approach follows in this vein but could be combined with more sophisticated neural text compression methods as well
neural text compression filippova et al
presented an lstm approach to based sentence compression
miao and blunsom conclusion in this work we presented a neural network work for extractive and compressive tion
our model consists of a sentence extraction model joined with a compression classier that cides whether or not to delete syntax derived pression options for each sentence
training the model involves nding an oracle set of extraction and compression decision with high score which we do through a combination of a beam search procedure and heuristics
our model outperforms past work on the cnn daily mail corpus in terms of rouge achieves substantial gains over the extractive model and appears to have acceptable grammaticality according to human evaluations
acknowledgments this work was partially supported by nsf grant a bloomberg data science grant and an equipment grant from nvidia
the thors acknowledge the texas advanced ing center tacc at the university of texas at austin for providing hpc resources used to duct this research
results presented in this paper were obtained using the chameleon testbed ported by the national science foundation hey et al

thanks as well to the anonymous reviewers for their helpful comments
references taylor berg kirkpatrick dan gillick and dan klein

jointly learning to extract and compress
in proceedings of the annual meeting of the ciation for computational linguistics human guage technologies pages
association for computational linguistics
ziqiang cao furu wei wenjie li and sujian li

faithful to the original fact aware neural tive summarization
in aaai conference on cial intelligence
jaime carbonell and jade goldstein

the use of mmr diversity based reranking for reordering documents and producing summaries
in ings of the annual international acm sigir conference on research and development in mation retrieval sigir pages new york ny usa
acm
lynn carlson daniel marcu and mary ellen okurovsky

building a discourse tagged corpus in the framework of rhetorical structure in proceedings of the second sigdial theory
workshop on discourse and dialogue
yen chun chen and mohit bansal

fast tive summarization with reinforce selected tence rewriting
in proceedings of the annual meeting of the association for computational guistics volume long papers pages
association for computational linguistics
jianpeng cheng and mirella lapata

neural summarization by extracting sentences and words
in proceedings of the annual meeting of the sociation for computational linguistics volume long papers pages
association for putational linguistics
sumit chopra michael auli and alexander m
rush

abstractive sentence summarization with in tentive recurrent neural networks
ings of the conference of the north american chapter of the association for computational guistics human language technologies pages
association for computational linguistics
trevor cohn and mirella lapata

sentence pression as tree transduction
j
artif
int
res

yue dong yikang shen eric crawford herke van hoof and jackie chi kit cheung

ditsum extractive summarization as a contextual bandit
in proceedings of the conference on empirical methods in natural language ing pages
association for tional linguistics
greg durrett taylor berg kirkpatrick and dan klein

learning based single document rization with compression and anaphoricity in proceedings of the annual straints
ing of the association for computational linguistics volume long papers pages
ciation for computational linguistics
katja filippova enrique alfonseca carlos a
menares lukasz kaiser and oriol vinyals

sentence compression by deletion with lstms
in proceedings of the conference on cal methods in natural language processing pages
association for computational tics
sebastian gehrmann yuntian deng and alexander rush

bottom up abstractive in proceedings of the conference on tion
empirical methods in natural language ing pages
association for tional linguistics
dan gillick and benoit favre

a scalable global in proceedings of the model for summarization
workshop on integer linear programming for ral language processing pages
association for computational linguistics
yoav goldberg and joakim nivre

a dynamic oracle for arc eager dependency parsing
in ceedings of coling pages
the coling organizing committee
max grusky mor naaman and yoav artzi

newsroom a dataset of
million summaries in with diverse extractive strategies
ings of the conference of the north can chapter of the association for computational linguistics human language technologies ume long papers pages
association for computational linguistics
karl moritz hermann tomas kocisky edward grefenstette lasse espeholt will kay mustafa leyman and phil blunsom

teaching chines to read and comprehend
in c
cortes n
d
lawrence d
d
lee m
sugiyama and r
garnett editors advances in neural information processing systems pages
curran associates inc
tsutomu hirao yasuhisa yoshida masaaki nishino norihito yasuda and masaaki nagata

single document summarization as a tree in proceedings of the sack problem
ference on empirical methods in natural language processing pages
association for putational linguistics
yichen jiang and mohit bansal

closed book training to improve summarization encoder ory
in proceedings of the conference on pirical methods in natural language processing pages
association for computational linguistics
kate keahey pierre riteau dan stanzione tim erill joe mambretti paul rad and paul ruth

chameleon a scalable production testbed for puter science research
in contemporary high formance computing from petascale toward cale edition volume of chapman hall crc computational science chapter pages
crc press boca raton fl
diederik p kingma and jimmy ba

adam a method for stochastic optimization
arxiv preprint

kevin knight and daniel marcu

based summarization step one sentence in proceedings of the seventeenth pression
tional conference on articial intelligence and twelfth conference on innovative applications of articial intelligence pages
aaai press
kevin knight and daniel marcu

tion beyond sentence extraction a probabilistic approach to sentence compression
artif
intell

chen li yang liu fei liu lin zhao and fuliang weng

improving multi documents marization by sentence compression based on panded constituent parse trees
in proceedings of the conference on empirical methods in ral language processing emnlp pages
association for computational linguistics
haoran li junnan zhu jiajun zhang and chengqing zong

ensure the correctness of the mary incorporate entailment knowledge into in proceedings stractive sentence summarization
of the international conference on tational linguistics pages
association for computational linguistics
junyi jessy li kapil thadani and amanda stent

the role of discourse units in near extractive summarization
in proceedings of the annual meeting of the special interest group on discourse and dialogue pages
association for putational linguistics
chin yew lin

rouge a package for matic evaluation of summaries
in text tion branches out
christopher manning mihai surdeanu john bauer jenny finkel steven bethard and david mcclosky

the stanford corenlp natural language processing toolkit
in proceedings of annual meeting of the association for computational guistics system demonstrations pages
sociation for computational linguistics
andre martins and noah a
smith

tion with a joint model for sentence extraction and in proceedings of the workshop on compression
integer linear programming for natural language processing pages
association for tional linguistics
afonso mendes shashi narayan sebastiao miranda zita marinho andre f
t
martins and shay b
cohen

jointly extracting and compressing documents with summary state representations
in proceedings of the conference of the north american chapter of the association for tional linguistics human language technologies volume long and short papers
yishu miao and phil blunsom

language as a latent variable discrete generative models for sentence compression
in proceedings of the conference on empirical methods in natural guage processing pages
association for computational linguistics
ramesh nallapati feifei zhai and bowen zhou

summarunner a recurrent neural work based sequence model for extractive marization of documents
in aaai conference on articial intelligence
ramesh nallapati bowen zhou cicero dos santos caglar gulcehre and bing xiang

stractive text summarization using sequence sequence rnns and beyond
in proceedings of the signll conference on computational natural language learning pages
association for computational linguistics
shashi narayan shay b
cohen and mirella lapata

ranking sentences for extractive tion with reinforcement learning
in proceedings of the conference of the north american ter of the association for computational linguistics human language technologies volume long pers pages
association for tional linguistics
romain paulus caiming xiong and richard socher

a deep reinforced model for abstractive in international conference on summarization
learning representations
matthew peters mark neumann mohit iyyer matt gardner christopher clark kenton lee and luke zettlemoyer

deep contextualized word resentations
in proceedings of the conference of the north american chapter of the association for computational linguistics human language technologies volume long papers pages
association for computational linguistics
xian qian and yang liu

fast joint compression in and summarization via graph cuts
ings of the conference on empirical methods in natural language processing pages
association for computational linguistics
alexander m
rush sumit chopra and jason ston

a neural attention model for tive sentence summarization
in proceedings of the conference on empirical methods in natural language processing pages
association for computational linguistics
evan sandhaus

the new york times annotated corpus
linguistic data consortium philadelphia
abigail see peter j
liu and christopher d
ning

get to the point summarization with pointer generator networks
in proceedings of the annual meeting of the association for tational linguistics volume long papers pages
association for computational tics
jiwei tan xiaojun wan and jianguo xiao

stractive document summarization with a in proceedings based attentional neural model
of the annual meeting of the association for computational linguistics volume long pers pages
association for tional linguistics
liangguo wang jing jiang hai leong chieu chen hui ong dandan song and lejian liao
improving an
can syntax help based sentence compression model for new in proceedings of the annual mains
ing of the association for computational linguistics volume long papers pages canada
association for computational guistics
lu wang hema raghavan vittorio castelli radu rian and claire cardie

a sentence pression based framework to query focused in proceedings of the document summarization
annual meeting of the association for tational linguistics volume long papers pages
association for computational tics
kristian woodsend and mirella lapata

ing to simplify sentences with quasi synchronous grammar and integer programming
in proceedings of the conference on empirical methods in natural language processing pages
sociation for computational linguistics
xingxing zhang mirella lapata furu wei and ming zhou

neural latent extractive document in proceedings of the summarization
ference on empirical methods in natural language processing pages
association for putational linguistics
qingyu zhou nan yang furu wei shaohan huang ming zhou and tiejun zhao

neural ument summarization by jointly learning to score in proceedings of the and select sentences
annual meeting of the association for tional linguistics volume long papers pages
association for computational tics
dataset train dev test ref len doc len cnn dm table statistics of the cnn daily mail and see text datasets
cnn features the shortest reference summaries overall and this is where we nd sion is most effective
node type len of comps oracle comp pp jj sbar advp



table statistics of compression options in cnn
we show the top four constituency types that are pressible along with the average length the fraction of available compressions it accounts for and how quently the oracle says to compress these constituents
of the adjectives are compressable without hurting the rouge
supplementary material a experimental setup data preprocessing we preprocess the datasets with the scripts provided by see et al
which uses stanford corenlp tokenization ning et al

we use the non anonymized version of the cnn dm as in previous rization work
for the new york times corpus we lter out the examples with abstracts shorter than words following the criteria in durrett et al
yielding the nyt dataset
the tics of the datasets are listed in table
ing sentence selection we always select tences for cnn dm and sentences for nyt which gave the best performance
for our tic analysis all datasets are parsed with the stituency parser in stanford corenlp manning et al

implementation details we use the same trained word embeddings used in narayan et al

the size of the sentence and document representation vectors is
for the sion module we use elmo as the ized encoder without ne tuning the parameter and project the vectors back to dimensions ter the elmo layer
dropout is applied after word embedding layers and lstm layers at a rate of

we use the adam optimizer kingma and ba with the initial learning rate at

the model converges after epochs of training
in tial experiments we also found elmo to be useful for sentence selection as well
however to plify comparisons with past work and due to ing issues we use it for compression only
we use rouge lin for evaluation
during acle construction we use simplied unigram and bigram scores as a faster approximation to the full rouge
b turk instructions figure shows the interface for amazon turk man evaluation
c type analysis in table we show the statistics of the sion options in cnn
pp attachment and adjectives are the top compression options and according to the oracle more than half of pp and almost all line parameters figure the interface for amazon turk human evaluation
all of the examples are fully shufed

