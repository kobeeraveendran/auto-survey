r a l c
s c v
v i x r a the annals of applied statistics vol
no
doi
institute of mathematical statistics concise comparative summaries ccs of large text corpora with a human by jinzhu luke bin yu brian gawalt laurent el ghaoui luke barnesmoore and sophie clavier peking university harvard university university of california berkeley and san francisco state university in this paper we propose a general framework for topic specic summarization of large text corpora and illustrate how it can be used for the analysis of news databases
our framework concise tive summarization ccs is built on sparse classication methods
ccs is a lightweight and exible tool that oers a compromise tween simple word frequency based methods currently in wide use and more heavyweight model intensive methods such as latent dirichlet allocation lda
we argue that sparse methods have much to oer for text analysis and hope ccs opens the door for a new branch of research in this important eld
for a particular topic of interest e

china or energy css tomatically labels documents as being either or ally via keyword search and then uses sparse classication methods to predict these labels with the high dimensional counts of all the other words and phrases in the documents
the resulting small set of phrases found as predictive are then harvested as the summary
to validate our tool we using news articles from the new york times international section designed and conducted a human survey to compare the dierent summarizers with human understanding
we demonstrate our approach with two case studies a media analysis of the framing of egypt in the new york times throughout the arab spring and an informal comparison of the new york times and wall street journal s coverage of energy
overall we nd that the received february revised october
in part by nsf grant under the cyber enabled covery and innovation cdi nsf grant aro grant nsf grant nsf cmmi grant and

jia and l
miratrix are authors and are listed in alphabetical order
key words and phrases
text summarization high dimensional analysis sparse ing lasso regularized logistic regression co occurrence tf idf normalization
this is an electronic reprint of the original article published by the institute of mathematical statistics in the annals of applied statistics vol
no

this reprint diers from the original in pagination and typographic detail
j
jia et al
lasso with normalization can be eectively and usefully used to summarize large corpora regardless of document size

introduction
stuart wrote the media are part of the dominant means of ideological production
what they produce is precisely tions of the social world images descriptions explanations and frames for understanding how the world is and why it works as it is said and shown to work
given this in order to understand how the public constructs its view of the world we need to be able to generate concise comprehensible summaries of these representations
automatic concise summaries thus come quite useful for comparing themes across corpora or screening corpora for further readings
our approach to obtain such summaries is by rst identifying a corpus that we believe contains substantial information on prespecied topics of interest and then using automated methods to extract summaries of those topics
these summaries ideally show the connections between our topics and other concepts and ideas
the two corpora we investigate in this paper are all the articles in the international section of the new york times from to just after and all the headlines from both the new york times and the wall street journal from to
our approach however could be applied to other corpora such as the writings of shakespeare books published in statistics in or facebook wall writings of some community
since such corpora are large only a very tiny fraction of them could ever be summarized or read by humans
there are many ways one might study a corpus
one common and eective method for text study is comparison
for example a media analyst interested in investigating how the topic of china is framed or covered by nyt s international section in could form an opinion by comparing articles about china to those not about china
a shakespeare scholar could gain understanding on shakespeare s view on romance by comparing the author s romantic plays with his nonromantic plays
in this paper we propose and validate by human survey a topic driven concise comparative summarization ccs tool for large text corpora
our ccs tool executes the comparison idea through statistical sparse cation methods
we rst automatically label blocks of text in a corpus as positive examples about a topic or negative control examples
we then use a machine learning predictive framework and sparse regression methods such as the lasso tibshirani to form a concise summary of the positive examples out of those phrases selected as being predictive of this labeling
in james watson s article representing realities an overview of news framing
concise comparative summaries ccs a novel advantage of our tool is the exible nature of its labeling cess
it allows dierent ways of forming positive and negative examples to provide snapshot summaries of a corpus from various angles
for stance we could label articles that mention china as positive examples and the rest as negative examples we could also take the same positive examples and use only those articles that contain other asian countries but not china as the negative examples
because the summaries are concise it is possible for researchers to quickly and eectively examine and compare multiple snapshots
therefore changes in coverage across time or between sources can be presented and understood even when the changes are dimensional and complex
even though our tool takes a classication framework as its foundation our interest is in understanding text rather than classifying it
therefore we validated our tool through a systematic randomized human survey described in section where human subjects evaluated our summaries based on their reading of samples from the original text
this provided some best practices for generating summaries with the highest overall quality as measured by essentially relevance and clarity
our ccs tool can be used to provide conrmatory evidence to support pre existing theories
extending the work of clavier et al
in section media analyst co authors of this paper use this tool and framing theory an analytical framework from media studies described later to compare the evolution of news media representations of countries across dierent distinct periods dened by signicant events such as revolutionary upheaval or tions with existing international relations theory
our tool can also be used to explore text in a generative manner helping researchers better stand and theorize about possible representations or framing mechanisms of a topic in a body of text
in our second case study we utilize ccs to compare the headlines of the new york times to the wall street journal in particular for the topic of energy
the rest of the paper is organized as follows
before presenting our posed approach concise comparative summary ccs we briey review related work in section
section describes the ccs framework which consists of three steps
the labeling scheme what rule to use to automatically label a document unit as positive or negative
preprocessing when building and expanding on a bag of words sentation of a corpus we must decide which document unit to use article vs
paragraph and how to rescale counts of phrases appropriately and
feature selection how to select the summary phrases
for preprocessing we describe tf idf and rescaling
for feature selection we discuss the lasso penalized logistic regression correlation j
jia et al
and co occurrence
note that the former two fall into the predictive work while the last do not but are included because of their wide use
the human validation experiment to compare dierent combinations in the ccs framework over labeling rescaling unit choice and feature selection choice is described in section with results in section

section presents the two case studies introduced above using the lasso with normalization the method found to be the most robust in the human validation experiment
section concludes with a discussion

related works
automated tools aimed at understanding text pecially newspaper text are becoming more and more important with the increased accumulation of text documents in all elds of human activities
in the last decade we have seen the emergence of computational social science a eld connecting statistics and machine learning to anthropology sociology public policy and more lazer et al

automatic summarization is in wide use google news trends twitter s trending topics zubiaga et al
and crimson hexagon s brand analysis all use text summaries to attempt to make sense of the vast volumes of text generated in public course
these all illustrate the great potential of statistical methods for text analysis including news media analysis
we hope our proposed ccs work will help advance this new and exciting eld
most text summarization approaches to date aside from natural and grammar based approaches use word or phrase including sentence counts or frequencies
they can be considered along two axes
the rst axis is whether an approach generates topics on its own or summarizes without regard to topic unsupervised or is supplied a topic of interest supervised
the second axis is whether the word and phrase rates of appearance are modeled or simply reweighted


unsupervised model based approaches
topic modeling where uments in a corpus are described as mixtures of latent topics that are in turn described by words and phrases is a rapidly growing area of text analysis
these methods take text information as input and produce a usually ative model t to the data
the model itself captures structure in the data and this structure can be viewed as a summary
the set of topics generated can serve as a summary of the overall and individual documents can be summarized by presenting those topics most associated with them
a popular example is the latent dirichlet allocation lda model blei ng and jordan which posits that each word observed in the text stands in for a hidden latent topic variable
these models are complex and dense all words play a role in all the topics
however one can still present the most prominent words in a topic as the summary which produces cogent and reasonable topics see chang et al
where humans evaluate the concise comparative summaries ccs internal cohesion of learned topics by identifying impostor words inserted into such lists
grimmer et al
combine such a model with clustering to organize documents by their topics
they also extensively evaluate dierent models under their framework with human survey experiments
summarizing or presenting the generated topics with this method can be problematic
for example taking the most probable words of a topic to represent it can lead to overly general representations
bischof and airoldi propose focusing on how words discriminate between topics as well as overall frequency essentially a comparative approach to better identify overall topics
these issues notwithstanding lda style approaches are quite powerful and can be used comparatively
for example paul zhai and girju use lda to score sentences from opposite viewpoints to summarize dierences between two ideologies


unsupervised simple weighting approaches
google trends charts are calculated by comparing the number of times a prespecied word of est appears to the overall volume of news for a specied time period within the news outlets that google compiles
even this simple approach can show how topics enter and leave public discourse across time
twitter s trending topics appear to operate similarly although it selects the hottest topics by those which are gaining in frequency most quickly
these approaches are similar in spirit to the normalized simpler methods co occur and tion screen that we compare with ccs in this paper
hopkins and king extrapolate from a potentially nonrandom ple of hand coded documents to estimate the proportion of documents in several predened categories
this can be used for sentiment analysis e

estimating the proportion of blogs showing approval for some specied public gure
their work drives crimson hexagon a company currently oering brand analysis to several companies
our approach instead identies key phrases most associated with a given topic or subject
there is a wide literature on text summarization as compared to topic modeling above by key phrase extraction rose et al
senellart and blondel frank et al
and sentence extraction hennig goldstein et al
neto freitas and kaestner
these approaches score potential key phrases or sentences using metrics such as position in a paragraph sentence length or frequency of occurrence and then select the highest scorers as the summary
while typically used for individual documents goldstein et al
did extend this approach to multiple documents by scoring and selecting sentences sequentially with future sentences penalized by similarity to previously selected sentences
in monroe colaresi and quinn the authors take a comparative approach as we do
they merge all text into two super documents the positive and negative examples and then score individual words based on j
jia et al
their rates of appearance normalized by their overall frequency
we analyze the corpus through individual document units


supervised approaches
supervised versions of lda that rate a given topic labeling in the hierarchical bayesian model blei and mcaulie do exist
although these methods are computationally expensive and produce dense models requiring truncation for ity they are powerful indications of the capabilities of computer assisted topic based summarization
hennig applies a latent topic model ilar to lda for topic specic summarization of documents
here the topic is represented as a set of documents and a short narrative of the desired content and sentences are then extracted by a scoring procedure that pares the similarity of latent sentence representations to the provided topic of interest
classication of text documents using the phrases in those documents as features and a given prespecied labeling of those documents is iar and well studied genkin lewis and madigan zhang and oles
however while we extensively build on this work our focus is not on the ability to classify documents but rather on the interpretable features that enable classication
interpreting these features allows for tion of the quality of the text in relation to other variables of interest
for example eisenstein smith and xing use similar approaches to amine the relationship between characteristics of dierent authors and their patterns of lexical frequencies

our approach concise comparative summarization ccs via sparse predictive classication
in science and engineering applications cal models often lend themselves to believable generative stories
for social science applications such as text analysis however models are more likely to be descriptive than generative
as simple methods are more transparent they are arguably more appealing for such descriptive purposes
our all goal is to develop computationally light as well as transparent tools for text analysis and by doing so to explore the limits of methods that are not extensively model based
our ccs framework is composed of three main steps
automatically label the text units for a given topic label
preprocess the possible summarizing phrases and phrase counts weight and
sparsely select a comparative phrase list of interest using classication methods on the automatic labels summarize
for a given topic or subject e

egypt in a given context e

the nyt international section in ccs produces summaries in the form of concise comparative summaries ccs a list of key phrases
to illustrate table contains four sample summaries
here we labeled an article as a positive example if it contains the word of the country under various forms at least twice
as we can see in this table sometimes fragments are selected as stand ins for complete phrases for example the phrase president felipe appears in the mexico column signifying president felipe
these summaries are suggestive of the aspects of these countries that are most covered in the new york times in relative to other topics even now nazis and the world wars were tied to germany iraq and afghanistan were also tied closely gen as in the military title general and combat were the major focus in iraq
the coverage of mexico revolved around the swine drug cartels and concerns about the border
russia had a run in with europe about gas and nuclear involvement with iran
we use sparse classication tools such as the lasso or penalized logistic regression in step these are fast and dierent from the modeling methods described earlier
our approach is fundamentally about contrasting sets of documents and using found dierences as the relevant summary which allows for a more directed process of summarization than unsupervised methods
this also allows for multiple snapshots of the same topic in the same document corpus using dierent contrasting sets which gives a more nuanced understanding of how the topic is portrayed
to situate concise comparative summarization of a given topic in a nary classication framework we now introduce some notation
a table four dierent countries in
the method used a count rule with a threshold of the lasso for feature selection and tf idf reweighting of features was one of the best identied for article unit analysis by our validation experiment iraq russia germany mexico american and afghanistan baghdad brigade combat gen in afghanistan invasion nuri pentagon saddam sergeant sunni troops war and who a medvedev caucasus europe gas georgia interfax news agency iran moscow nuclear president dmitri republics sergei soviet vladimir angela merkel berlin chancellor angela european france and frankfurt group of mostly hamburg marwa alsherbini matchxing minister karltheodor zu munich nazi world war and border protection antonio betancourt cancn chihuahua denise grady drug cartels guadalajara inuenza oaxaca outbreak president felipe sinaloa swine texas tijuana j
jia et al



p


n j tive framework consists of n units each with a class label yi and a collection of p possible features that can be used to predict this class is attributed a value xij for each label
each unit i ture j matrix x
the n units are blocks of text taken from the corpus e

entire articles or individual paragraphs the class labels generally built automatically with keyword searches indicate whether document unit i contains content on a subject of interest and the features are all the possible key phrases that could be used to summarize the subject or topic

these xij form an n i x is built from c where c is a representation of text often called the bag of phrases model each document is represented as a vector with the jth element being the total number of times that the specic phrase j appears in the document
stack these row vectors to make the document term matrix rnp of counts
from c we build x by rescaling the elements of c c to account for dierent rates of appearance between the phrases
c and x have one row for each document and one column for each phrase and they tend to be highly sparse most matrix elements are
given the processed text x and we can construct summarizers by labeling weighting and selecting phrases
we can make dierent choices for each step
we now present several such choices and then discuss a human validation experiment that identies the best combination of these elements


automatic and exible labeling of text units
to start based on ject knowledge the user of our tool e

the media analyst translates a topic or subject of interest into a set of topic phrases
for instance he she might translate the topic of china into a topic list china chinas nese
energy might be oil gas electricity coal solar
arab spring might be arab spring arab revolution arab uprising
given a topic list the user can apply dierent rules to generate the beling
for example label a text unit as a positive example for the topic of china if the text unit contains any of the phrases in the topic set or alternatively if a more stringent criterion is desired label it as positive if it contains more than two topic set phrases
the general rules for labeling by query count we used are as follows topics can be rened and expanded if initially generated summaries return other phrases that are essentially the same
for example in one of our case studies we ran ccs using the above energy list as a query
when we saw the term natural surface as a summary word we realized our query set could be improved with the addition of the query natural gas ccs helped us discover a useful addition to the query set leading to a broader more useful summarization from a second pass using the expanded query set
topic modeling and keyword expansion methods could also be of use here
concise comparative summaries ccs count k a document i is given a label yi if a query term appears or fewer query k or more times in the document
documents with k hits receive a label of yi
hard count k or hcount k as above but drop all documents with hits from the analysis as their relationship to the query tween and k may be ambiguous
in other cases labeling is straightforward
for directly comparing the nyt to the wsj the labeling was for nyt headlines and for wsj lines
for comparing a period of time to the rest labeling would be built from the dates of publication
the labeling step identies a set of documents to be summarized in the context of another set
generally we summarize compared to the overall background of all remaining documents but one could drop uncertain documents for example those with only one topic phrase but not more than one or irrelevant ones for example those not relating to any asian country at all
dierent choices here can unveil dierent aspects of the pus see section
for a case study that illustrates this


preprocessing weighting and stop word removal
it is well known that baseline word frequencies impact information retrieval methods and so raw counts are often adjusted to account for commonality and rarity of terms e

monroe colaresi and quinn salton and buckley
in the predictive framework this adjustment is done with the construction of the feature matrix x
we consider three constructions of x all built on the bag of phrases representation c
regardless of the weighting approach we also remove any columns corresponding to any phrases used to generate the labeling to prevent the summary from being trivial and circular
salton and buckley examine a variety of weighting approaches for document retrieval in a multi factor experiment and found choice of approach to be quite important we compare the ecacy of dierent choices in our human validation survey see section
each of the following methods stop word removal rescaling and tf idf weighting transform a base bag of words matrix c into a feature matrix x
stop words removal
stop words are high frequency but low information words such as and or the
high frequency words have higher variance and eective weight in many methods often causing them to be erroneously selected as features due to sample noise
to deal with these nuisance words many text processing methods use a xed hand built stop word list and emptively remove all features on that list from consideration e

zhang and oles ifrim bakir and weikum genkin lewis and madigan
for our framework this method generates x from c by dropping the columns of c which correspond to a stop word feature while letting x take on c s values exactly in the retained nonstop word feature columns
j
jia et al
this somewhat method does not adapt automatically to the vidual character of a given corpus and this presents many diculties
stop words may be context dependent
for example in us international news united states or country seem to be high frequency and low information
switching to a corpus of a dierent language would require new stop word lists
more importantly when considering phrases instead of single words the stop word list is not naturally or easily extended
rescaled
as an alternative appropriately adjusting the document tors can act in lieu of a stop word list by reducing the variance and weight of high frequency features
we use the corpus to estimate baseline appearance rates for each feature and then adjust the matrix c by a function of these rates see mosteller and wallace and monroe colaresi and quinn
we say x is a rescaled version of c if each column of c is rescaled to have unit length under the norm that is rescaling xij where zj cij zj n ij
under this rescaling the more frequent a phrase the lower its weight
tf idf weighting
an alternative rescaling comes from the popular tf idf heuristic salton and buckley salton which attempts to emphasize commonly occurring terms while also accounting for each ment s length
x is a tf idf weighted version of c if tf idf xij cij qi log n where i and appears at least once
p p cij p cij is the sum of the counts of all key phrases in document n is the number of documents in which term j

feature selection methods
many prediction approaches yield els that give each feature a nonzero weight
we however want to ensure that the number of phrases selected is small so the researcher can easily read and evaluate the entire summary and compare it to others
these summaries can even be automatically translated to other languages to more easily compare foreign language news sources dai et al

given the feature matrix x and document labels for a topic we extract phrases corresponding to columns of x to constitute the nal summary
we seek a subset of phrases with cardinality as close as possible to but no larger than a target k the desired summary length
we typically use k phrases but or might also be desirable depending on the context
we require selected phrases to be distinct meaning that we do k j concise comparative summaries ccs not count sub phrases
for example united states and united are both selected we drop united
the constraint of short summaries renders the summarization problem a sparse feature selection problem as studied in for example forman lee and chen yang and pendersen
in other domains regularized methods are useful for sparse model selection they can identify relevant features associated with some outcome within a large set of mostly irrelevant features
in our domain however there is no reasonable tion of an underlying true model that is sparse we expect dierent phrases to be at least somewhat relevant
our pursuit of a sparse model is motivated instead by a need for results which can be described concisely a constraint that crowds out consideration of complicated dense or nonlinear tion models
we nonetheless employ the sparse methods hoping that they will select only the most important features
we examine four methods for extraction or selection detailed below
two of them co occurrence and correlation screening are scoring schemes where each feature is scored independently and top scoring features are taken as a summary
this is similar to traditional key phrase extraction techniques and to other methods currently used to generate word clouds and other text visualizations
the other two are regularized least squares linear gression the lasso and logistic regression
table displays four summaries for china in one from each feature selector choice ters greatly
we systematically evaluate this diering quality with a human validation experiment in section



co occurrence and correlation screening
co occurrence is a ple method included in our experiments as a useful baseline
the idea is to take phrases that appear most often or have greatest weight in the tively marked text as the summary
this method is often used in tools such as newspaper charts showing the trends of major words over a year such as google news or word or tag clouds created at sites such as
correlation screening selects features with the largest absolute pearson correlation with the topic labeling
both methods give each phrase a relevance score sj rank the phrases by these sj and then take the top phrases dropping any sub phrases as the summary
for co occurrence the relevance score sj of feature j for all j is j co occurrence sj i xij xii
google
com trends

wordle

j
jia et al
table comparison of the four feature selection methods
four sample summaries of news coverage of china in
documents labeled via on articles x from rescaling
note increased prevalence of stop words in rst column and redundancies in second column co occurrence correlation lasso and by contributed research for global has jintao in beijing its of that the to xinhua year beijing and beijings contributed research from beijing global in beijing li minister wen jiabao president hu jintao prime minister wen shanghai the beijing tibet xinhua the zhang asian beijing contributed research euna lee global hong kong jintao north korea shanghai staterun uighurs wen jiabao xinhua asian beijing contributed research exports global hong kong jintao north korea shanghai tibet uighurs wen jiabao xinhua i i yi that is sj is the average weight of phrase j in where the positively marked examples
if x c that is it is not weighted then and this method sj is the average number of times feature appears in selects those phrases that appear most frequently in the positive examples
the weighting step however reduces the co occurrence score for common words that appear frequently in both the positive and negative examples
i for correlation screening score each feature as sj correl
screen where xj and y are the mean values of feature j and the labels respectively across the considered documents
y n p pp pp n y n


penalized methods lasso and
the lasso tibshirani is an penalized version of linear regression and is the rst of two feature selection methods examined in this paper that address our sparsity for interpretability constraint explicitly rather than via ing
imposing an penalty on a least squares problem regularizes the vector of coecients allowing for optimal model t in high dimensional p n regression settings
furthermore penalties typically result in sparse feature vectors which is desirable in our context
the lasso also concise comparative summaries ccs takes advantage of the correlation structure of the features to to a certain extent avoid selecting highly correlated terms
the lasso can be dened as an optimization problem
arg min m k xt i y j
xj we solve this convex optimization problem with a modied version of the bbr algorithm genkin lewis and madigan
the phrases sponding to the nonzero elements of comprise our summary
the penalty term governs the number of nonzero elements of and would ally be chosen via cross validation to optimize some reasonable metric for prediction
we however select to achieve a desired prespecied summary length that is a desired number of nonzero
we nd by a line search
not tuning for prediction raises concerns of serious or
generally in order to have short summaries we indeed
ally since our labeling is not very accurate in general prediction performance might even be misleading
the main question is whether a human readable signal survives imperfect labeling and over regularized summaries both of which allow for easier exploration of text
these concerns motivate the man validation study we discuss in section
similar to the lasso penalized logistic regression is typically used to obtain a sparse feature set for predicting the log odds of an outcome variable being either or
it is widely studied in the classication ature including text classication see genkin lewis and madigan ifrim bakir and weikum zhang and oles
for an overview of the lasso penalized logistic regression and other sparse methods see for example hastie tibshirani and friedman
for details of our plementation along with further discussion see jia et al

co occurrence correlation screening and the lasso are all related
the occurrence score sj can be seen as the average count or weighted count for a reweighted feature matrix of phrase j in the positively marked examples noted as y
correlation screening is related but slightly dierent calculations show that y is proportional to and hence is the dierence between the positive and negative ples see jia et al
for details
both co occurrence and correlation screening methods are greedy procedures
since the lasso can be solved via e zhao and yu the lasso procedure can also be interpreted as greedy
it is an iterative correlation search procedure the rst step is to get the word phrase with the highest correlation then we modify the labels to remove the inuence of this word phrase and then get the highest correlated word phrase with this modied label vector and so on and so forth
y j
jia et al
table computational speed chart
average running times for the four feature selection methods over all subjects considered
second column includes time to generate y and adjust x
final column is percentage increase in total time over co occurrence the baseline method phrase selection sec total time sec percent increase co occurrence correlation screen the lasso







the primary advantages of co occurrence and correlation screening are that they are fast scalable and easily distributed across multiple cores for parallel processing
unfortunately as they score each feature independently from the others they can not take advantage of any dependence between features to aid summarization
the lasso and can to a certain tent
the down side is that the sparse methods are more computationally intensive than co occurence and correlation screening
however this could be mitigated by for example moving to a parallel computing environment or doing clever preprocessing such as safe feature elimination el ghaoui viallon and rabbani
for our current implementation which is our modied form of the bbr algorithm genkin lewis and madigan we timed the lasso as being currently about times and more than times slower than the baseline co occurrence
see table

the human validation survey
consider the four sample summaries on table
these particular summaries came from a specic combination of choices for the reweighting rescaling labeling and feature lection steps co occurrence correlation and the lasso
but are these summaries better or worse than the summaries from a dierent summarizer with another specic combination comparing the ecacy of dierent summarizers requires systematic uation
to do this many researchers use corpora with existing summaries such as human encoded key phrases in academic journals such as in frank et al
or baseline human generated summaries such as the tipster data set used in neto freitas and kaestner
we however give a single summary for many documents and so we can not use an annotated evaluation corpus or summaries of individual documents
alternatively numerical measures such as prediction accuracy or model t might be used to compare dierent methods
however the major purpose of text summarization is to help humans gather information so the quality of summarization should be compared to human understanding based on the same text
while we hypothesize that prediction accuracy or model t concise comparative summaries ccs should correlate with summary quality as measured by human evaluation to a certain extent there are no results to demonstrate this
indeed some research indicates that the correlation between good model t and good summary quality may be absent or even negative in some experiments gawalt et al
chang et al

in this section therefore we design and conduct a study where humans assess summary quality
we compare our four feature selection methods under dierent text segmenting labeling and weighting choices in a crossed and randomized experiment
nonexperts read both original documents and our summaries in the experiment and judge the quality and relevance of the output
even though we expect individuals judgements to vary we can average the responses across a collection of respondents and thus get a measure of overall generally shared opinion


human survey through a multiple choice questionnaire
we carried out our survey in conjunction with the xlab a uc berkeley lab dedicated to helping researchers conduct human experiments
we recruited dents undergraduates at a major university from the lab s respondent pool via a generic nonspecic message stating that there was a study that would take up to one hour of time
for our investigation we used the international section of the new york times for
see our rst case study in section for details on this data set
we evaluated dierent summarizers built from dierent combinations along the following four dimensions document unit when building c the document units corresponding to the matrix rows may be either full articles or the individual graphs in those articles
labeling documents can be labeled according to the rules described in the preceding section or
rescaling matrix x can be built from c via stop word removal rescaling or tf idf weighting
feature selection data x y can be reduced to a summary using co occurrence correlation screening the lasso or
together for any given query there exist ccs summary methods available
we dropped and for paragraphs giving tested
we applied each summarizer to the set of all articles in the new york times international section from for dierent countries of interest
these countries are china iran iraq afghanistan israel pakistan russia france india germany japan mexico south korea egypt and turkey
the frequency of appearance in our data for these countries can be found j
jia et al
in table of jia et al

we then compared the ecacy of these binations by having respondents assess through answering multiple choice questions the quality of the summaries generated by each summarizer
for our survey paid respondents were convened in a large room of kiosks where they assessed a series of summaries and articles presented in blocks of questions each
each block considered a single randomly selected topic from our list of
within a block respondents were rst asked to read four articles and rate their relevance to the specied topic
respondents were then asked to read and rate four summaries of that topic randomly chosen from the subject library of
respondents could not go back to previous questions
only the rst words of each article were shown
consultation with journalists suggests this would not have a detrimental impact on content presented as a traditional newspaper article s inverted pyramid structure moves from the most important information to more minute details as it progresses pottker
all respondents nished their full survey and fewer than of the questions were skipped
time to completion ranged from to minutes with a mean completion time of minutes
see jia et al
for further details and for the wording of the survey


human survey results
we primarily examined an aggregate ity score taken as the mean of the assessed content relevance and dancy of the summaries
figure shows the raw mean aggregate outcomes for the article unit and paragraph unit data
the rightmost plot suggests that the lasso and performed better overall than co occurrence and correlation screen
we analyze the data by tting the respondents responses to the rizer characteristics using linear regression although all plots here show raw unadjusted data
the adjusted plots show similar trends
the full model cludes terms for respondent subject unit type rescaling used labeling used and feature selector used as well as all interaction terms for the latter four factors
in all models there are large respondent and topic eects
some ics were more easily summarized than others and some respondents more critical than others
interactions between the four summarization method factors are unsurprisingly present df f
under anova
there are signicant three way interactions between unit
and labeling feature selector and rescaling selector and rescaling p p

interaction plots figure suggest that the sizes of these actions are large making interpretation of the marginal dierences for each factor potentially misleading
table shows all signicant two way tions and main eects for the full model as well as for models run on the article unit and paragraph unit data separately
concise comparative summaries ccs fig

aggregate results
outcome is aggregate score based on the raw data
there are major dierences between article unit analysis and paragraph unit analysis when ering the impact of choices in preprocessing
error bars are unadjusted se based only on subset of scores at given factor combinations
as the unit of analysis heavily interacts with the other three factors we conduct further analysis of the article unit and paragraph unit data rately
the article unit analysis is below
the paragraph unit analysis not shown is summarized in section
s discussion on overall ndings
article unit analysis
the left column of figure shows for the unit data plots of the three two way interactions between feature selector labeling scheme and rescaling method
there is a strong interaction between the rescaling and feature selection method df f
log p top left plot and no evidence of a labeling by feature selection interaction or a labeling by rescaling interaction
model adjusted plots not shown table main eects and interactions of factors
main eects along diagonal in bold
a number denotes a signicant main eect or pairwise interaction for aggregate scores and is the rounded log of the value

denotes lack of signicance at the
level
all data is all data in a single model without and fourth order interactions
article unit and paragraph unit indicate models run on only those data for summarizers operating at that level of granularity all data article unit paragraph unit factor unit feat
lab
resc
feat
lab
resc
feat
lab
resc
unit feat
select labeling rescaling









j
jia et al
fig

aggregate quality plots
pairwise interactions of feature selector labeling and rescaling technique
left hand side is for article unit summarizers right for paragraph nit
see testing results for which interactions are signicant
akin to figure do not dier substantially in character
table shows all signicant
main eects and pairwise interactions
concise comparative summaries ccs the lasso is the most consistent method maintaining high scores under almost all combinations of the other two factors
in figure note how the lasso has a tight cluster of means regardless of the rescaling method used in the top left plot and how the lasso s outcomes are high and consistent across all labeling in the middle left plot
though or co occurrence may be slightly superior to the lasso when coupled with tf idf they are not greatly so and regardless both these methods seem fragile varying a great deal in their outcomes based on the text preprocessing choices
validating its long history of use tf idf seems to be the best overall ing technique consistently coming out ahead regardless of choice of labeling or feature selection method
note how its curve is higher than the rescaling and stop word curves in both the and bottom left plots in figure
weighting by tf idf brings otherwise poor feature selectors up to the level of the better selectors
we partially ordered the levels of each factor by overall marginal impact on summary quality
for each factor we t a model with no interaction terms for the factor of interest to get its marginal performance and within this model performed pairwise testing for all levels of the factor adjusting the resulting p values to control familywise error rate with tukey s honest signicant dierence to address the multiple testing problem within each factor
these calculations showed which choices are overall good performers ignoring interactions
see table for the resulting rankings
co occurrence and correlation screening performed signicantly worse than and the lasso correlation vs
gives t
p

the labeling method options are indistinguishable
the rescaling method options are ordered with table quality of feature selectors
this table compares the signicance of the separation of the feature selection methods on the margin
order is always from lowest to highest estimated quality
a denotes a signicant separation
all p values corrected for multiple pairwise testing
the last seven lines are lower power due to subsetting the data data included order article order paragraph all tf idf only only stop only cooc only corr only lasso only only cooc corr lasso stop resc tf idf cooc corr lasso stop resc no dierences cooc lasso corr lasso cooc corr lasso corr lasso stop resc tf idf stop tf idf no dierences no dierences no dierences no dierences cooc lasso stop resc no dierences no dierences tf idf resc j
jia et al
tf idf signicantly better than rescaling t
log p is better than stop word removal t
p

which in turn discussion
comparing the performance of the feature selectors is dicult due to the dierent nature of interactions for paragraph and article units
that said the lasso consistently performed well
when building c at the article unit level lasso was a top performer
for the paragraph unit it did better than most but was not as denitively superior
if appropriately staged also performs well
simple methods such as co occurrence are sensitive to the choice of weighting method and generally speaking it is hard to know what ing is best for a given corpus
this sensitivity is shared by
under the lasso however these decisions seem unimportant regardless of unit size
we therefore recommend using the lasso as it is far less sensitive to the choice of weights
a note on tf idf and rescaling
the main dierence between the unit and article unit data is that tf idf is a poor choice of rescaling and rescaling is the best choice for paragraph unit
we conducted a further investigation to understand why this was the case and found that any given stop word will appear in most articles due to the articles lengths which under tf idf will result in very small weights
low weight words are hard to select and thus those terms are dropped
for the paragraph unit level ever the weights are not shrunk by nearly as much since many paragraphs will not have any particular low content word
for example prepositions like among or with
the recalling however maintains the low weights as the weight cally depends on total counts across the corpus
if one makes histograms of these weights not shown this shift is readily apparent
for short units of text rescaling is a stronger choice since it is not sensitive to document length
of course the lasso makes these decisions less relevant

case studies
here we illustrate our ccs tool by conducting two ample analyses that demonstrate how researchers can explore corpora lect evidence for existing theories and generate new theories
that is we here attempt to meaningfully connect our methodology to actual practice an orientation to research argued for in for example wagsta
given the validation of the human reader survey we restrict ccs to use the lasso with regularization over full articles with a rule a combination determined most eective overall by the human experiment
in the rst study we conduct an analysis of how egypt was covered by the international section of the new york times throughout the arab spring
in the second we compare the headlines of the new york times to those of the wall street journal on the topics of energy
concise comparative summaries ccs

egypt as covered by the international section of the new york times
we here investigate how egypt was framed across time in the international section of the new york times from the beginning of through july
through this analysis we hope to illuminate both consistent and changing trends in the coverage of egypt as well as the impact of dierent stages of the arab spring on how egypt was editorially framed
though of course there are a myriad of frames and narratives we selected a few of the most inuential recognizable and contextually established narratives to remain within the scope of this paper and to provide a basic overview of possible applications for these tools in the analysis of media representation
this study demonstrates how ccs can be used to examine how the ing of countries and political entities can evolve throughout the progression of political situations such as revolutions and elections
we show that our tool can also help determine the more macro frames of narration that ture coverage of a region
we argue the ndings from our tool allow an analyst to better understand the basic logic of reporting for a region and how events such as uprisings and key elections impact that coverage
articles were scraped from the new york times rss and the html markup was stripped from the text
we obtained articles
the new york times upon occasion will edit an article and repost it under a dierent headline and link these multiple versions of the articles remain in the data set
by looking for similar articles as measured by a small angle between their feature vectors in the document term matrix c we estimate that around have near duplicates
the number of paragraphs in an article ranges from to
typical have about paragraphs with an inter quartile range iqr of to paragraphs
however about of the articles the world brieng articles are a special variety that contain only one long paragraph
among the more typical brieng articles the distribution of article length as number of paragraphs is bell shaped and unimodal
longer articles with a median length of words have much shorter paragraphs median of words generally than the word brieng single paragraph articles median of words
in the early entman posited that our learning of the world is built on frames which he denes as information processing schemata that erate by selecting and highlighting some features of reality while omitting and barnesmoore are conducting a larger study on the topic

nytimes
com nyt rss world
for example
nytimes
world
for example
nytimes
world html
cuba
html
j
jia et al
others entman page
media studies incorporate these tions by showing the role of the media in creating these frames stating for example that through choice and language and repetition of certain story schemas the media organizes and frames reality in distinctive ways mcleod kosicki and pan
following goman we agree that the analysts task therefore is to identify frames in media discourse within the understanding that media framing under the guise of informing can liberately inuence public opinion
indeed most of the literature on framing and subsequent agenda setting literature argues that frames are purposely created
according to entman to frame is to select some aspects of a ceived reality and make them more salient in a communicating text in such a way as to promote a particular problem moral evaluation treatment recommendation entman
in terms of portrayal of other countries frames tend to be easy to observe as popular news media tend to establish simplied dichotomies of we sus other and they classify data under those two categories often outlined as mirror images of positive attributes versus negative ones kiousis and wu kunczik
given that frames in the media center around repeated and often simplied elements our tools seem to naturally lend themselves to the extraction of a frame s ngerprint
at core our ods extract relevant phrases that are often repeated in conjunction with a topic of interest
these phrases when read as news arguably build links in readers minds to the topic and thus contribute to the formation and solidication of how the topic is framed
to capture the evolving frames of egypt and elections across time we generated several sequences of summaries
we summarized within specic windows of time with boundaries determined by major political events such table overview of the nyt windows for the egypt summary
columns encode stats during each period time period name start and stop dates total number of articles number of articles about egypt number of egypt articles per week and egypt article volume as a percentage of total volume period start stop art
egypt eg
egypt before uprisings revolution post mumbarak parl elections post elections whole corpus















concise comparative summaries ccs as the beginning of the uprisings in tunisia december or tian parliamentary elections february
see table
we present summaries of dierent periods of time an alternate approach would be to tempt to link articles and present a graph of relationships
see for example shahaf guestrin and horvitz or el ghaoui et al

we rst generated ccs summaries using the lasso with rescaling over full article document units comparing all articles mentioning egypt to all other articles
we subsequently compared egypt vs
the other articles within only those articles that also contained variants of election and examined other arab countries e

tunisia as well as phrases such as arab and arab spring
this process generated several graphical displays of summaries all examining dierent facets of news coverage from the nyt
for an example see figure which shows the overall framing of egypt across time
we identied articles as egypt related if they contained any of egypt egypts egyptian egyptians cairo mubarak the rule
we alyzed at the article level and used the lasso with tf idf regularization
after looking at the rst list we removed arab and hosni as uninformative and re ran our summarizer to focus the summary on more content relevant phrases
such an iterative process is we argued a more natural and cipled way of discovering and eliminating low content features in this case hosni is mubarak s rst name and arab tends to show up in articles in this region as compared to other regions
neither of these words would be found on any typical stop word list
from figure and others similar to it we can consider consistent and changing trends in the coverage of egypt as well as the impact of dierent stages of the arab spring on how egypt was framed
we then sampled text fragments and sentences containing these phrases from the corpus to ensure we were interpreting them correctly
for example the arab in cally but not always appears before world as in for example mostly from the arab world
we now give an overview of the resulting analysis
pre arab spring columns and
the summaries shown as the rst three columns of figure are for most of and for the time just prior to the uprisings in tunisia
coverage of the arab world prior to the arab spring is dominated by concern for israel and narratives concerning the war on terror
note the appearance of israel hamas gaza and palestinian
there are two probable reasons for the appearance of these words
first israel bombed egypt in
second following the camp david accords of the united state s political economic and military strategies within the mena region became reliant on sustaining these cords
and indeed the mubarak regime sustained this treaty in the face of mass opposition by the egyptian people
overall we see egypt as being covered in the context of its connection of israel and the israeli palestinian conict
j
jia et al
fig

framing of egypt
columns correspond to prespecied windows of time
concise comparative summaries ccs we also see for the period just prior to the uprisings cats and milan
these phrases are overall rare words that happened to appear at portionate rates in the positively marked articles and are thus selected as indicative
this can happen when there are few positive examples only in this time span in an analysis
arab spring columns and
we divided the arab spring into three rough periods the initial revolution during the late months of column the time just after the fall of president mumbarak through column and the time leading up to the parliamentary election at the end of into column at which point a nominal government had been established
throughout this time we see a shift in coverage most obviously indicated by the appearance of the words protests protesters and revolution
the arab which indicated either the arab world or the arab league before now indicates the arab world or the arab spring as found by examining text snippets containing the found summary phrases
we see that us foreign policy imperatives retain their importance as shown by the continued appearance of israel hamas and gaza
note the entrance of discussion concerning the military and military cils e

the military and military council in egyptian coverage as elections approach
the heightened appearance comes at a time when much discussion concerning the elections is dominated by the islamist nature of the major parties running for oce see e

islamists and hood in column for the time just prior to the parliamentary elections
as the military regime in egypt could be perceived by many in western circles as a keystone for regional peace with israel this frame of narration arguably lends a sense of stability concerning the status quo
after the parliamentary elections column
following the initial tions in egypt the frame of israel gaza and hamas remain but we also see islamist morsi and brotherhood suggesting a developing frame of an islamic threat to the western domestic sphere posed by groups like the muslim brotherhood
the shift comes as the western media begins to cover the elections in egypt
as the u
s
has supported the elections as being gitimate the western media is now faced with the assumption that the will of the egyptian public might be more fully actualized in an open racy
existing american and israeli fears of islamic extremism mixed with the prevalence of islamist parties in the elections combine to form a new frame of coverage
this frame however is in many cases still dominated by the relationship of the islamist parties to the u
s
and its close ally israel


comparing the new york times to the wall street journal
in our second case study we as readers of the wall street journal wsj and the j
jia et al
new york times nyt use ccs to understand the dierences and ities of these two major newspapers across time
we focus on headlines
as headlines are quite short we based on the human experiment results used the lasso with rescaling and no stop word removal
our data set consists of headlines from the new york times and headlines from the wall street journal scraped from their rss feeds for four years from jan through the end of
as a rst exploratory step we labeled nyt headlines as positive ples and wsj headlines as negative examples and applied ccs
the initial results gave phrases such as sports review and arts as indicating a headline being from the nyt
exploration of the raw data revealed that the nyt precedes many headlines with a department name for example arts briey giving this result
however other phrases for example for and of also repeatedly appear in the summaries as being indicative of the nyt
this coupled with the fact that very few phrases indicated the wsj suggests that the nyt has a more identiable signal for tion that is a more distinctive headline style
for further content focused investigations we then dropped these department related words and phrases e

sports review
as potential features
we then conducted a content focused analysis to compare the nyt and wsj with respect to how they cover energy as represented by headlines containing general words such as oil solar gas energy and electricity

of the wsj had headlines containing these words while
of the nyt s headlines contained these terms
see table
we actually investigated dierently broad interpretations of this topic
one version cluded energy only and another included words such as oil natural gas solar
also with an iterative process we can conduct an informal keyword expansion to rene the representation of their topic of interest in the text of the corpus being examined by updating the labeling process
for example we here included natural as a keyword after seeing it nently in connection with energy as a rst pass
for a rst summary we did a head to head or between source parison as follows we rst dropped all headlines that did not mention any of the energy related terms
we then labeled nyt energy related headlines as and wsj energy related headlines as and applied ccs
this gave data prices stocks green ink and crude as being in the wsj s frame and spill greenhouse world business and music review as being the nyt s
see figure
these latter two phrases are after several similar terms had ready been removed
world business is a department label for articles about international aairs and its appearance connects coverage of energy with international news
music review is due to music review articles using energy in headlines such as energy abounds released by a urry of beats or molding sound to behave like a solar eclipse
a head to head concise comparative summaries ccs table summary of headlines for energy investigation headlines energy headlines energy headlines nyt wsj total nyt wsj total nyt wsj total














year all comparison will capture stylistic dierences between the corpora as well as dierences in what content is covered
to eectively remove dierences in style we can select dierent lines for comparison
in particular we conduct a dierence of dierences approach by comparing nyt energy headlines to nyt nonenergy lines to subtract out general trends in nyt style doing the same for the wsj and comparing the two resulting summaries to each other
in particular to do this second phase within source analysis we within the nyt headlines only labeled energy related headlines as left the rest as baseline and applied ccs
we then did the same for the wsj
this gives two summaries for each year and two for the overall ison
we then directly read and compared these lists
we see some of the same words in the resulting lists as our head to head analysis but ally have other more content specic words that give a richer picture
note for the nyt renewable greenhouse shale and pipeline
the based words do not tend to appear
the within wsj comparison produces an overlapping set of words to the nyt comparison indicating similar erage between the two sources see renewable there as well
the dierences are however suggestive greenhouse is indicated for the nyt each year and the wsj in only
opec appears in for the wsj and only in for the nyt
by shifting what the baseline is in this case comparing the energy lines of the nyt to the nonenergy headlines of the nyt instead of the energy headlines of the wsj dierent aspects of the topic and dierent aspects of the corpus are foregrounded
in the within source comparison we come to understand in general what energy headlines are for the tive sources
in the between source comparison we focus specically on what dierentiates the two outlets which foregrounds style of writing as well as dierential coverage of content
looking at both seems important for beginning to understand how these themes play out in the media
j
jia et al
fig

the nyt vs
the wsj with regards to energy
first columns are the between comparison of the nyt vs
the wsj
second are an internal within comparison of energy to nonenergy within the nyt
third set are the same for the wsj
red indicates wsj and green nyt
within each set columns correspond to and respectively
all is all four years combined
concise comparative summaries ccs
conclusions
news media signicantly impacts our day to day lives public knowledge and the direction of public policy
analyzing the news however is a complicated task
the labor intensity of hand coding and the amount of news available strongly motivate automated methods
we proposed a sparse predictive framework for extracting meaningful summaries of specic subjects or topics from document corpora
these maries are contrast based built by comparing two collections of documents to each other and identifying how a primary set diers from a baseline set
this concise and comparative summarization ccs framework expands the horizon of possible approaches to text data mining
we oer it as an example of a simpler method that is potentially more manipulable exible and pretable than those based on generative models
in general we believe that there is a rich area between similar naive methods such as simple counts and more heavyweight methods such as lda
sparse regression at the heart of ccs lies in this area and has much to oer
to better understand the performance of our approach and to ately tune it to maximize the quality and usability of the summaries duced we conducted a human validation experiment to evaluate dierent summarizers based on human understanding
based on the human ment we conclude that features selected using a sparse prediction framework can generate informative key phrase summaries for subjects of interest
we also found these summaries to be superior to those from simpler ods currently in wide use such as co occurrence
in particular the lasso is a good overall feature selector quite robust to how the data is cessed and computationally scalable
when not using the lasso proper data preparation is quite important
in this case tf idf is a good overall choice for article length documents but not when the document units are small e

paragraphs and presumably headlines online comments and tweets in which case an scaling should be used
we illustrated the use of our summarizers by evaluating two media ing questions
the summarizers indeed allowed for insight and evidence collection
one of the key aspects of our tool is its interactive capacity a researcher can easily work with resulting summary phrases using them as topics in their own right adding them to the concept of the original topic or dropping them altogether
overall we argue that ccs allows searchers to easily explore large corpora of documents with an eye to taining concise portrayals of any subject they desire
a shortcoming of the tool is that both generating the labeling and interpreting resulting phrases can depend on fairly detailed knowledge of the topic being plored
to help with this we are currently extending the tool to allow for sentence selection so researchers can contextualize the phrases more rapidly
j
jia et al
acknowledgments
we are indebted to the sta of the xlab at uc berkeley for their help in planning and conducting the human validation study
we are also grateful to hoxie ackerman and saheli datta for help assembling this publication
luke miratrix is grateful for the support of a graduate research fellowship from the national science foundation
jinzhu jia s work was done when he was a postdoc at uc berkeley supplemented by nsf
references bischof j
m
and airoldi e
m

summarizing topical content with word quency and exclusivity
in proceedings of the international conference on machine learning
edinburgh scotland
blei d
and mcauliffe j

supervised topic models
in advances in neural formation processing systems j
c
platt d
koller y
singer and s
roweis eds

mit press cambridge ma
blei d
m
ng a
y
and jordan m
i

latent dirichlet allocation
j
mach
learn
res

chang j
boyd graber j
gerrish s
wang c
and blei d

reading tea leaves how humans interpret topic models
in advances in neural information processing systems y
bengio d
schuurmans j
lafferty c
k
i
williams and a
culotta eds

vancouver bc canada
clavier s
el ghaoui l
barnesmoore l
and li g


all the news that s t to compare comparing chinese representations in the american press and us representations in the chinese press
dai x
jia j
el ghaoui l
and yu
b

sba term sparse bilingual tion for terms
in fifth ieee international conference on semantic computing icsc
stanford univ
palo alto ca
eisenstein j
smith n
a
and xing e
p

discovering sociolinguistic sociations with structured sparsity
in proceedings of the annual meeting of the association for computational linguistics human language technologies
association for computational linguistics portland or
el ghaoui l
viallon v
and rabbani t

safe feature elimination in sparse supervised learning
technical report no
uc
eecs dept
univ
california berkeley
el ghaoui l
li g

duong v

pham v
srivastava a
and bhaduri k

sparse machine learning methods for understanding large text corpora plication to ight reports
in conference on intelligent data understanding
mountain view ca
entman r
m

framing toward clarication of a fractured paradigm
journal of communication
entman r
m

projections of power framing news public opinion and u
s
foreign policy
univ
chicago chicago il
forman g

an extensive empirical study of feature selection metrics for text classication
j
mach
learn
res

frank e
paynter g
w
witten i
h
gutwin c
and nevill manning c
g

domain specic keyphrase extraction
in proceedings of the sixteenth tional joint conference on articial intelligence
morgan mann san francisco ca
concise comparative summaries ccs gawalt b
jia j
miratrix l
w
ghaoui l
yu b
and clavier s

covering word associations in news media via feature selection and sparse classication
in proceedings of the international conference on multimedia information retrieval
philadelphia pa
genkin a
lewis d
d
and madigan d

large scale bayesian logistic gression for text categorization
technometrics
goffman e

frame analysis an essay on the organization of experience
vard univ
press cambridge ma
goldstein j
mittal v
carbonell j
and kantrowitz m

document summarization by sentence extraction
in naacl anlp workshop on automatic summarization
seattle wa
grimmer j
shorey r
wallach h
and zlotnick f

a class of bayesian semiparametric cluster topic models for political texts
hastie t
tibshirani r
and friedman j
h

the elements of statistical learning vol

springer new york
hennig l

topic based multi document summarization with probabilistic latent semantic analysis
in recent advances in natural language processing ranlp
association for computational linguistics borovets bulgaria
hopkins d
and king g

a method of automated nonparametric content ysis for social science
american journal of political science
ifrim g
bakir g
and weikum g

fast logistic regression for text tion with variable length n grams
in acm sigkdd international conference on knowledge discovery and data mining
acm new york
jia j
miratrix l
w
gawalt b
yu b
and el ghaoui l

what is in the news on a subject automatic and sparse summarization of large document corpora
technical report dept
statistics univ
california berkeley
kiousis s
and wu x

international agenda building and agenda setting ing the inuence of public relations counsel on us news media and public perceptions of foreign nations
the international communications gazette
kunczik m

globalisation news media images of nations and the ow of national capital with special reference to the role of rating agencies
j
international communication
lazer d
pentland a
adamic l
aral s
barabasi a

brewer d
christakis n
contractor n
fowler j
gutmann m
jebara t
king g
macy m
roy d
and van alstyne m

computational social science
science
lee l
and chen s

new methods for text categorization based on a new feature selection method and a new similarity measure between documents
lecture notes in comput
sci

mcleod m
kosicki g
m
and pan z

on understanding and standing media eects
edward arnold london
monroe b
l
colaresi m
p
and quinn k
m

fightin words lexical feature selection and evaluation for identifying the content of political conict
political analysis
mosteller f
and wallace d
l

applied bayesian and classical inference the case of the federalist papers ed
springer new york
neto j
l
freitas a
a
and kaestner c
a
a

automatic text marization using a machine learning approach
in advances in articial intelligence
lecture notes in computer science
springer berlin
j
jia et al
paul m
j
zhai c
and girju r

summarizing contrastive viewpoints in ionated text
in proceedings of the conference on empirical methods in natural language processing
association for computational linguistics stroudsburg pa
pottker h

news and its communicative quality the inverted pyramid when and why did it appear journalism studies
rose s
engel d
cramer n
and cowley w

automatic keyword extraction from individual documents
in text mining applications and theory m
w
berry and j
kogan eds

wiley chichester
salton g

developments in automatic text retrieval
science
salton g
and buckley c

term weighting approaches in automatic text trieval
information processing and management
senellart p
and blondel v
d

automatic discovery of similar words
in survey of text mining ii
springer berlin
shahaf d
guestrin c
and horvitz e

trains of thought generating formation maps
in proceedings of the international conference on world wide web
acm lyon france
tibshirani r

regression shrinkage and selection via the lasso
j
r
stat
soc
ser
b stat
methodol

wagstaff k
l

machine learning that matters
in international conference on machine learning
edinburgh scotland
yang y
and pendersen i
o

a comparative study on feature selection in text categorization
in international conference on machine learning
nashville tn
zhang t
and oles f
j

text categorization based on regularized linear ication methods
information retrieval
zhao p
and yu b

stagewise lasso
j
mach
learn
res

zubiaga a
spina d
fresno v
and martnez r

classifying trending ics a typology of conversation triggers on twitter
in proceedings of the acm ternational conference on information and knowledge management
acm new york
j
jia lmam school of mathematical sciences and center for statistical science peking university beijing china e mail
pku
edu
b
yu department of statistics and department of eecs university of california berkeley berkeley california usa e mail
berkeley
edu l
miratrix department of statistics harvard university oxford street cambridge massachusetts usa e mail
harvard
edu b
gawalt l
el ghaoui department of eecs university of california berkeley berkeley california usa e mail
com
edu concise comparative summaries ccs l
barnesmoore s
clavier department of international relations college of liberal creative arts san francisco state university san francisco california usa e mail
sfsu
edu
edu
