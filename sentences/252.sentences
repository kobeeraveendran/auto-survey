improving abstractive text summarization with history aggregation pengcheng chuang xiaojun and xiaofei c e d l c
s c v
v i x r a abstract
recent neural sequence to sequence models have vided feasible solutions for abstractive summarization
however such models are still hard to tackle long text dependency in the summarization task
a high quality summarization system usually depends on strong encoder which can rene important information from long input texts so that the decoder can generate salient maries from the encoders memory
in this paper we propose an gregation mechanism based on the transformer model to address the challenge of long text representation
our model can review history information to make encoder hold more memory capacity
cally we apply our aggregation mechanism to the transformer model and experiment on cnn dailymail dataset to achieve higher ity summaries compared to several strong baseline models on the rouge metrics
introduction the task of text summarization is automatically compressing long text to a shorter version while keeping the salient information
it can be divided into two approaches extractive and abstractive rization
the extractive approach usually selects sentences or phrases from source text directly
on the contrary the abstractive approach rst understands the semantic information of source text and erates novel words not appeared in source text
extractive rization is easier but abstractive summarization is more like the way humans process text
this paper focuses on the abstractive approach
unlike other sequence generation tasks in language processing such as machine translation in which the lengths of input and output text are close the summarization task ists severe imbalance on the lengths
it means that the summarization task must model long distance text dependencies
as rnns have the ability to tackle time sequence text variants of sequence to sequence based on them have emerged on a large scale and can generate promising results
to solve the distance text dependencies bahdnau et al
rst propose the tion mechanism which allows each decoder step to refer to all hidden states
rush et al
rst incorporate attention anism to summarization task
there are also other attention based models to ease the problem of long input texts for summarization task like bahdnau hierarchical based and simple
celikyilmaz et al
segment text and encode segmented text independently then cast their encoding to others
though these systems are promising school of cyber security university of chinese academy of sciences institute of information engineering chinese academy of sciences jing china jing china corresponding author chuang zhang
ac
cn they exhibit undesirable behaviors such as producing inaccurate tual details and repeating themselves as it is hard to decide where to attend and where to ignore for one pass encoder
modeling an effective encoder for representing a long text is still a challenge and we are committed to solving long text dependency problems by aggregation mechanism
the key idea of the tion mechanism is to collect history information then computes tion between the encoder nal hidden states and history information to re distribute the encoder nal states
it suggests that the encoder can read long input texts a few times to understand the text clearly
we build our model by reconstructing the transformer via incorporating our novel aggregation mechanism
empirically we rst analyze the features of summarization and translation dataset
then we experiment with different encoder and decoder layers and the results reveal that the ability of the encoder layer is more portant than the decoder layer which implies that we should cus more on the encoder
finally we experiment on cnn dailymail dataset and our model generates higher quality summaries compared to strong baselines of pointer generator and transformer models on rouge metrics and human evaluations
the main contributions of this paper are as follows we put forward a novel aggregation mechanism to redistribute context states of text with collected history information
then we equip the transformer model with the aggregation mechanism
our model outperforms

and
rouge l scores on cnn dailymail dataset and

and
rouge l scores on our build chinese news dataset compared to transformer baseline model
related work in this section we rst introduce extractive summarization then troduce abstractive summarization

extractive summarization extractive summarization aims to select salient sentences from source documents directly
this method is always modeled as a sentence ranking problem via selecting sentences with high sequence label or integer ear
the models above mostly leverage manual gineered features but they are now replaced by the neural network to extract features automatically
cheng et al
get sentence sentation using convolutional neural and document representation using recurrent neural and then select sentences words using hierarchical extractor
nallapati et al
treat the summarization as a sequence labeling task
they get sentence and document representations using rnns and after a classication layer each sentence will get a label which indicates whether this tence should be selected
zhou et al
present a model for tive summarization by jointly learning score and select sentences
zhang et al
put forward a latent variable model to tackle the problem of sentence label bias

abstractive summarization abstractive summarization aims to rewrite source texts with standable semantic meaning
most methods of this task are based on sequence to sequence models
rush et al
rst incorporate the attention mechanism to abstractive summarization and achieve state of the art scores on and gigaword datasets
chopra et al
improve the model performance via rnn decoder
nallapati et al
adopt a hierarchical network to process long source text with hierarchical structure
gu et al
are the rst to show that a copy mechanism can take advantage of both extractive and tive summarization by copying words from the source text extractive summarization and generating original words abstractive rization
see et al
incorporate copy and coverage mechanisms to avoid generating inaccurate and repeated words
celikyilmaz et al
split text to paragraph and apply encoder to each paragraph then broadcast paragraph encoding to others
recently vaswani et al
give a new view of sequence to sequence model
it employs the self attention to replace rnn in sequence to sequence model and uses multi head attention to capture different semantic information
lately more and more researchers focus on combine abstractive and extractive summarization
hsu et al
build a unied model by using inconsistency loss
gehrmann et al
rst train selector to select and mask salient information then train the tive model pointer generator to generate abstractive tion
model in this section we rst describe the attention mechanism and the transformer baseline model after that we introduce the pointer and bpe mechanism
our novel aggregation mechanism is described in the last part
the code for our model is available online
notation we have pairs of texts x y where x is a long text and y y is the summary of corresponding
the lengths of d and y is ld and ly respectively
each text d is composed by a sequence of words w and we embed word w into vector e
so we represent document d with embedding vector


eld and we can get representation of y the same as

attention mechanism the attention mechanism is widely used in text summarization els as it can produce word signicance distribution in source text for disparate decode steps
bahdanau et al
rst propose the attention mechanism where attention weight distribution can be calculated whhi bt et i attentiont sof
com pc liao where hi is the encoder hidden states in ith word st is decoder hidden states at time step t
vector v ws wh and scalar bt i are able parameters
attentiont is probability distribution that sents the importance of different source words for decoder at time step t
transformer redenes attention mechanism more concisely
in practice we compute the attention function on a set of queries multaneously packed together into a matrix q
the keys and values are also packed together into matrices k and v
k v sof tmax qk dk v where is transpose function q rndk k rmdk v rmdv r is the real eld n m are the lengths of query and key value sequences dk dv are the dimensions of key and value
for summarization model we assume k v
self attention can be dened from basic attention with q k v
and multi head attention concatenates multiple basic attentions with different eters
we formulate multi head attention as m k v


where hdi wq i kwk i wmh are learnable parameters
i wv i wk i v wv i and vector
transformer baseline model our baseline model corresponds to the transformer model in nmt tasks
the model is different from previous sequence to sequence models as it applies attention to replace rnn
the transformer model can divide into encoder and decoder and we will discuss them respectively below
input the attention dened in the transformer is the bag of model so we have to add extra position information to the input
the position encodes with heuristic sine and cosine tion p dmodel p dmodel where pos is the position of word in text i is the dimension dex of embedding and the dimension of model is dmodel
the put of network u is equal to source text word embeddings ew


eld added position embeddings ep


pld
encoder the goal of encoder is extracting the features of input text and map it to a vector representation
the encoder stacks with n encoder layer
each layer consists of multi head self attention and position wise feed forward sublayers
we employ a residual tion around each of the two sublayers followed by layer tion
from the multi head attention sublayer we can extract different semantic information
then we compute each encoder layer s nal hidden states using position wise feed forward
the lth encoder layer is formulated as s el n f f s k l s s s s s v l s s where s connection and n orm
is layer normalization function is the multi head self attention output after residual el means figure
aggregation transformer model overview
compared with the transformer baseline model we apply the aggregation layer between encoder and decoder
the aggregation layer can collect history information to redistribute the encoder s nal hidden states
s v l s k l the output of encoder layer l
s u if l or s are learnable parameters and p f f
is the position wise feed forward sublayer
this sublayer also can be described as two convolution erations with kernel size
s k l vector s v l s s and scalar s s el decoder the decoder is used for generating salient and uent text from the encoder hidden states
decoder stacks with n decoder ers
each layer consists of masked multi head self attention head attention and feed forward sublayers
similar to the encoder we employ residual connections around each of the sublayers lowed by layer normalization
and we take lth decoder layer as ample
we use the masked multi head attention to encode summary as vector ms ms ms v l ms k l ms ms n h ms v l ms k l ms ms v l ms k l in other layers
where ms egw egp in the rst layer and is the output of the l decoder layer egw egp is the word embeddings and position embeddings of generated words respectively
the m h
is masked multi head self attention and the mask is similar with the transformer decoder
then we execute multi head attention between encoder and decoder dl dl kd vd d where ms is hidden states of decoder masked multi head attention and kd vd hn el is the last encoder layer output states
finally we use position wise feed forward and layer normalization sublayers to compute nal states dl d d d dl n f f d d d where vector d are learnable parameters
and projecting the decoder nal hidden states to vocab size then we can get vocabulary probability distribution pvocab
and scalar d
pointer and bpe mechanism in generation tasks we should deal with the out of the generated text problem
if we do not tackle this problem only contains a limited vocabulary words and replaces oovs with unk
things get worse in summarization task the specic name place
with low frequency is the key mation of summary however the vocabulary built with top k words with the most frequent occurrence while those specic nouns may not occur in vocabulary
the pointer and byte pair encoder bpe mechanism are both used to tackle the oov problem
the original bpe mechanism is a simple data compression technique that replaces the most frequent bytes pair with unused byte
sennrich et al
rst use this technique for word segmentation via merging characters instead of bytes
so the xed vocabulary can load more subwords to alleviate the problem of oov
the pointer mechanism allows both copying words from the source text and generating words from a xed vocabulary
for pointer mechanism at each decoder time step the generation probability pgen can be calculated pgen dl bgen where vector wdl and scalar bgen are learnable parameter
hn dl is the last decoder output states
we compute the nal word distribution via pointer network sof bcopy pcopy zi ld pf inal pgen pvocabpgen where u is representation of input zi is one hot indicator vector for wi pcopy is probability distribution of source words and pf inal is nal probability distribution

aggregation mechanism the overview of our model is in figure
to enhance memory ity we add the aggregation mechanism between encoder and coder for collecting history information
the aggregation nism reconstructs the encoder s nal hidden states by reviewing tory information
and we put forward two primitive aggregation proaches that can be proved effective in our task
historyhidden stateattnencoder layerencoder pos pos embeddingoutputshift rightaggregationencoderword embeddingword embedding figure
the overview of projection aggregation mechanism with coder layers
the rst approach is using full connected networks to collect torical
this approach rst goes through normal encoder layers to get the outputs of each layer and we lect middle l layers outputs then concatenate them as input of full connected networks to obtain history information h hh
finally we compute multi head attention between history state h and the output of the last encoder layer
this process can be formulated as hh l el


el bh where vector wh and scalar bh are learnable parameters l is parameter to be explored
then we add the multi head attention layer between the last encoder layer output hn el and history information hh
the output of attention is the nal states of encoder ha m k p v p where qp is history information hp and k p v p hn el
ly t where n l n is index of selected encoder layers is previous history state and k v is encoder output hl el
iteratively calculating history information until the last selected encoder layer we can get nal history hidden states ha and make the states as the nal states of the encoder
finally we dene the objective function
given the golden mary y


yly and input text x we minimize the negative log likelihood of the target word sequence
the training objective function can be described log where is model parameter and n is the number of source summary text pairs in training set
the loss for one sample can be added by the loss of generated word yt in each time step t log


x where


x can be calculated in decoder t time step t is total decoding steps
experiments in this section we rst dene the setup of our experiment and then analyze the results of our experiments

experimental setup dataset we conduct our experiments on cnn dailymail which has been widely used for long document marization tasks
the corpus is constructed by collecting online news articles and human generated summaries on cnn daily mail site
we choose the non anonymized which is not placing named entity with a unique identier
the dataset contains pairs of articles and summaries
the details of this dataset are in tion

training details we conduct our experiments with nvidia tesla
during training and testing time we truncate the source text to words and we build a shared vocabulary for encoder and decoder with small vocabulary size due to the using of the pointer or bpe mechanism
word embeddings are learned during training time
we use adam optimizer with initial learning rate and parameter

in training phase
we adapt the learning rate according to the loss on the validation set half learning rate if validation set loss is not going down in every two epochs
and we use regulation with all dropout

the training process converges about steps for each model
in the generation phase we use the beam search algorithm to duce multiple summary candidates in parallel to get better summaries and add repeated words to blacklist in the processing of search to avoid duplication
for fear of favoring shorter generated summaries we utilize the length penalty
in detail we set beam size repeated n gram size and length penalty parameter

we also constrain the maximum and minimum length of the generated mary to and respectively
figure
the overview of attention aggregation mechanism with encoder layers
the second approach is using attention mechanism to collect tory figure
we select middle l encoder layers outputs to iteratively compute multi head attention between current encoder layer output and previous history information
and the lth history information can be calculated as follows m k v
com abisee cnn dailymail encoder layer layer layer layer layer layer layer layer table
comparison of different model results on cnn daliymail test dataset using scores of rouge l with condence interval
the rst part is previous abstractive baseline models the second part is the transformer baseline model and our transformer model with aggregation mechanism
the best scores are bolded
model temp att pointer generator coverage pointer generator coverage cbdec inconsistency loss rnn ext abs rl rerank transformer aggregation rouge l


























we evaluate our system using f measures of rouge l metrics which respectively represent the overlap of gram and the longest common sequence between the golden mary and the system summary
the scores are computed by python package
experiment explorations we explore the inuence of different experiment hyper parameters setup for the model s performance which includes different experiment settings
firstly we explore the number of transformer encoder decoder layers see table
secondly we dig out the different aggregation methods with gregation layer see table
the exploration includes our baseline and transformer model with add tion aggregation and attention aggregation
thirdly we also explore the different performance of different number of aggregation layers see table
there are groups of experiments with different number of aggregation layers former adding last and last transformer with projection aggregation method using and and transformer with attention aggregation method using and
for all models except the exploration of encoder decoder layers we use encoder and decoder layers
human evaluation the rouge scores are widely used in the automatic evaluation of summarization but it has great limitations in semantic and syntax information
in this case we use manual ation to ensure the performance of our models
we perform a small scale human evaluations where we randomly select about ated summaries from each of the generator former and aggregation transformer and randomly shufe the order of summaries to anonymize model identities then let mous volunteers with excellent english literacy skills score random summaries for each models range from to score means high quality summary
then we using the average score of each mary as their nal score
the evaluation criteria are as follows salient summaries have the important point of the source text ency summaries are consistent with human reading habits and have few grammatical errors non repeated summaries do not contain too much redundancy word

results dataset analysis to demonstrate the difference between rization and translation tasks we compare the dataset for two tasks see table
the summarization dataset cnn dailymail contains
org project table
the comparison of translation and summarization datasets
we move sentence tags in the source text and split sentences with blank then count maximal and average length token in each dataset
dataset cnn max token avg token abs our max token avg token de max token en avg token en en max token avg token train valid test training pairs validation pairs and test pairs
the translation dataset and have training pairs validation pairs and test pairs respectively
then we nd the characteristics of those two different tasks after comparison
the summarization source text can include more than words and the average length of the source text is times longer than the target text while the translation task contains at most words and the average length of the source text is about the same as the target text
because of that we need a strong encoder with memory ability to decide where to attend and where to ignore
quantitative analysis the experimental results are given in ble
overall our model improves all other in their articles for scores while our model gets a lower rouge l score than the rl reinforcement learning
from celikyilmaz et al
the rouge l score is not correlated with summary quality and our model generates the most novel words compared with other baselines in novelty experiment
the novel words are harmful to l scores
this result also account for our models being more abstractive
figure shows the ground truth summary the generated maries from the transformer baseline model and our aggregation transformer using the attention aggregation method
the source text is the main fragment of the truncated text
compared with the gation transformer the summary generated by the transformer line model have two problems
firstly the summary of the baseline model is lack of salient information marked with red in the source text
secondly it contains unnecessary information marked with blue in the source text
we hold the opinion that the transformer baseline model has weak memory ability compared to our model
therefore it can not remind source





national grid has revealed the uk s rst new pylon for nearly years
called the t pylon artist s illustration shown it is a third shorter than the old lattice pylons
but it is able to carry just as much power volts
it is designed to be less obtrusive and will be used for clean energy purposes
national grid is building a training line of the less obtrusive t pylons at their eakring training academy in nottinghamshire
britain s rst pylon erected in july near edinburgh was designed by chitectural luminary sir reginald blomeld inspired by the greek root of the word pylon meaning gateway of an egyptian temple
the campaign against them they were unloved even then was run by rudyard kipling john nard keynes and hilaire belloc
ve years later the biggest peacetime construction project seen in britain the connection of power stations by miles of cable was completed
it marked the birth of the national grid and was a major ing of the nation s industrial engine and a vital asset during the second world war





ground truth national grid has revealed the uk s rst new pylon for nearly years
called the t pylon it is a third shorter than the old lattice pylons
but it is able to carry just as much power volts
it is designed to be less obtrusive and will be used for clean energy
transformer baseline the t pylon artist shown it is a third shorter than the old lattice pylons
but it is able to carry just as much power volts
it is designed to be less obtrusive and will be used for clean energy purposes
our model national grid has revealed the uk s rst new lon for nearly years
called the t pylon it is a third shorter than the old lattice pylons
but it is able to carry just as much power volts
it is designed to be less obtrusive and will be used for clean energy purposes
figure
the comparison of ground truth summary and generated maries of abstractive summarization models on cnn dailymail dataset
the red represents missed information the blue means unnecessary tion and the green signify appropriate information
the information far from its current states which will lead to missing some salient information and it may remember irrelevant information which will lead to unnecessary words generated in summaries
our model uses the aggregation mechanism that can review the primitive information to enhance the model memory capacity
therefore the aggregation mechanism makes our model generate salient and repetitive words in summaries
table
we compare different layers of and and report results on cnn dailymail test dataset using precision recall scores of rouge
e d r














r














rouge r














encoder decoder layers analysis the rst exploration iment consists of transformer models using different encoder and decoder layers
and we only experiment if the number of coder decoder layers is no more than
we also tried encoder and decoder layers however there is no notable difference with encoder and decoder layers and increasing a lot of parameters and taking more time to converge
therefore we make the transformer baseline model have encoder and decoder layers
if we decrease the layers of encoder or decoder respectively the results are shown in table
it can be concluded from the comparison of each model results that we can get lower precision but higher recall score when the encoder layers are decreasing and we have opposite results on the decoder layers decreasing experiments
meanwhile we can get a higher score and lower l scores in the model decreasing each decoder layer compared to that decreasing each encoder layer
therefore we can conclude that the encoder captures the features of the source text while the decoder makes summaries consistently
table
the aggregation mechanism experiments
our experiments use aggregation methods with different aggregation layers
model layer layer layer layer layer layer













rouge l






aggregation mechanism analysis the second exploration periment consists of our baseline and aggregation transformer model using different aggregation in table
if we use baseline model adding the last l the result scores will decrease beyond our tion
however simply adding the last l can re distribute the encoder nal states with history states it will average the portance weights of those layers and that maybe get things worse
compared with the baseline model the result scores of our gation are boosting
we compute attention between and encoder nal value to re distribute the nal states so that the encoder obtains the ability to fusing history information with different importance
the third exploration contains groups experiments add projection and attention
the aggregation transformer models here use different gation layers
we also experiment with the model in the above groups with aggregation layers but they all get extraordinary low rouge scores all models have

rouge l
roughly
they all incorporate the output of the rst encoder layer which may not have semantic information which may be harmful to the re distributing of the encoder nal states
so we do not compare with those models explicitly
for add aggregation group we increase the added layers while the rouge scores will get down
if we add more layers the nal state distributions will tend to be the uniform distribution which makes decoder confused about the key ideas of source text
for that reason we may get worse scores when we add more layers
for the projection aggregation group we increase the aggregation layers and the rouge scores will rise
if we aggregate more layers the history states will contain more information which will lead to performance improvement
however we will lose a lot of tion when the aggregation layers increasing
and we achieve the best result with aggregation layers
for the attention aggregation group we get the best score with aggregation layer but the rouge scores will decline if we increase the aggregation layers
we just need one layer attention to focus on history states because too much attention layers may have an sive dependency on history states
if the encoder nal distribution focus more on shallow layers which introduced a lot of useless mation it is harmful to the encoder to capture salient features
figure
the statistics of novel n grams and sentences
our model can generate far more novel n grams and sentences than pointer generator and transformer baseline
abstractive analysis figure shows that our model copy whole sentences from source texts and the copy rate is almost close to reference summaries
however there is still a huge gap in n grams generation and this is the main area for improvement
in particular the pointer generator model tends to examples with few novel words in summaries because of its lower rate of novel words generation
the transformer baseline model can generate novel summaries and our model get great improvement with



novelty improvement for n compared to the transformer baseline model
because our model reviews history states and re distribute encoder nal states we get more accurate semantic representation
it also proves that our gation mechanism can improve the memory capability of encoder
table
human evaluation of three models
we compare the average score of salient uency and non repeated
the best scores are bolded
model pointer generator pointer generator coverage transformer transformer aggregation salient



fluency



non repeated



human evaluation we conduct our human evaluation with setup in section
and the results show in table
we only compared three models on salient uency and non repeated criteria and our model gets the highest score in all criteria
but in uency criterion none of the models scores well which means it is hard to understand semantic information for all models now
the pointer generator is our baseline abstractive summarization approach and has the lowest scores
the pointer generator uses the coverage mechanism to avoid generating overlap words which can make summaries more uent and less repetitive
the transformer is a new abstractive tion based on attention mechanism and it can get better performance than the pointer generator model
we equip the transformer model with the aggregation mechanism and it can get great improvement on all criteria

our chinese experiments we build our chinese summarization dataset via crawling news and process the raw web page contents to character based texts
the details of our dataset show in table where our dataset has a similar average length of source texts and summaries compared cnn dm dataset
it is a temporary dataset which only contains pairs of text totally for now and we are still adding data to our dataset
table
experiments on our chinese dataset
we only experiment on three baseline models and evaluate results with rouge f metrics
the best scores are bolded
model pointer generator pointer generator coverage transformer transformer aggregation









rouge l




we also experiment on our chinese dataset and evaluate the result with metrics
our model gets the highest score while the pointer generator model gets rather high rouge scores see table
because the dataset does not contain many novel words where it is suitable for the pointer generator model
our dataset contains



novel and
novel sentences by comparison the novel n gram and sentences frequency of cnn dm in figure is



tively
and the pointer generator model generates summaries taining less novel words and sentences which leads to high scores in our chinese dataset
finally we compare our model with the former baseline model and our results improve
in
in and
in rouge l scores
conclusions in this paper we propose a new aggregation mechanism for the transformer model which can enhance encoder memory ability
the addition of the aggregation mechanism obtains the best performance compared to the transformer baseline model and pointer tor on cnn dailymail dataset in terms of rouge scores
we plore different aggregation methods add projection and attention methods in which attention method performs best
we also explore the performance of different aggregation layers to improve the best score
we build a chinese dataset for the summarization task and give the statistics of it in table
our proposed method also achieves the best performance on our chinese dataset
in the future we will explore memory network to collect history information and try to directly send history information to the coding processing to improve the performance in the summarization task
and the aggregation mechanism can be transferred to other eration tasks as well
acknowledgment this work was supported by the national natural science tion of china grant no


thepaper

com tensorflow master utils rouge
py gramsentencenovelty pointer generatortransformerour modelground truth references dzmitry bahdanau kyunghyun cho and yoshua bengio neural machine translation by jointly learning to align and translate arxiv preprint

asli celikyilmaz antoine bosselut xiaodong he and yejin choi deep communicating agents for abstractive summarization arxiv preprint

yenchun chen and mohit bansal fast abstractive summarization with reinforce selected sentence rewriting arxiv computation and guage
jianpeng cheng and mirella lapata neural summarization by ing sentences and words arxiv preprint

sumit chopra michael auli and alexander m rush abstractive tence summarization with attentive recurrent neural networks in ceedings of the conference of the north american chapter of the association for computational linguistics human language nologies pp

john m conroy and dianne p oleary text summarization via hidden markov models
xiangyu duan mingming yin min zhang boxing chen and hua luo zero shot cross lingual abstractive sentence summarization through teaching generation and attention in proceedings of the annual meeting of the association for computational linguistics pp

jonas gehring michael auli david grangier denis yarats and yann n dauphin convolutional sequence to sequence learning the international conference on machine in proceedings of learning volume pp

jmlr
org
sebastian gehrmann yuntian deng and alexander m rush up abstractive summarization arxiv preprint

jiatao gu zhengdong lu hang li and victor o k li incorporating copying mechanism in sequence to sequence learning
caglar gulcehre sungjin ahn ramesh nallapati bowen zhou and yoshua bengio pointing the unknown words arxiv computation and language
han guo ramakanth pasunuru and mohit bansal soft layer specic multi task summarization with entailment and question generation arxiv computation and language
karl moritz hermann tomas kocisk edward grefenstette lasse peholt will kay mustafa suleyman and phil blunsom teaching chines to read and comprehend arxiv computation and language
wanting hsu chiehkai lin mingying lee kerui min jing tang and min sun a unied model for extractive and abstractive summarization using inconsistency loss
yichen jiang and mohit bansal closed book training to improve marization encoder memory
panagiotis kouris georgios alexandridis and andreas stafylopatis abstractive text summarization based on deep learning and semantic content generalization
julian m kupiec jan o pedersen and francine r chen a trainable document summarizer
logan lebanoff kaiqiang song franck dernoncourt doo soon kim seokhwan kim walter chang and fei liu scoring sentence tons and pairs for abstractive summarization arxiv computation and language
chin yew lin rouge a package for automatic evaluation of maries
hui lin and vincent ng abstractive summarization a survey of the state of the art
junyang lin xu sun shuming ma and qi su global encoding for abstractive summarization arxiv computation and language
linqing liu yao lu min yang qiang qu jia zhu and hongyan li generative adversarial network for abstractive text summarization arxiv computation and language
yang liu and mirella lapata hierarchical transformers for document summarization
yang liu and mirella lapata text summarization with pretrained coders arxiv computation and language
konstantin lopyrev generating news headlines with recurrent neural networks arxiv computation and language
takuya makino tomoya iwakura hiroya takamura and manabu mura global optimization under length constraint for neural text marization
edward moroshko guy feigenblat haggai roitman and david konopnicki an editorial network for enhanced document tion
arxiv computation and language
ramesh nallapati feifei zhai and bowen zhou summarunner a recurrent neural network based sequence model for extractive rization of documents arxiv computation and language
ramesh nallapati bowen zhou and mingbo ma classify or select neural architectures for extractive document summarization arxiv computation and language
ramesh nallapati bowen zhou cicero nogueira dos santos caglar gulcehre and bing xiang abstractive text summarization using sequence to sequence rnns and beyond
shashi narayan shay b cohen and mirella lapata ranking tences for extractive summarization with reinforcement learning
myle ott sergey edunov alexei baevski angela fan sam gross nathan ng david grangier and michael auli fairseq a fast sible toolkit for sequence modeling
tatsuro oya yashar mehdad giuseppe carenini and raymond t ng a template based abstractive meeting summarization leveraging summary and source text relationships
alexander m rush sumit chopra and jason weston a neural tion model for abstractive sentence summarization arxiv tion and language
abigail see peter j liu and christopher d manning get to the point summarization with pointer generator networks
rico sennrich barry haddow and alexandra birch neural machine translation of rare words with subword units arxiv computation and language
ilya sutskever oriol vinyals and quoc v le sequence to sequence learning with neural networks in advances in neural information cessing systems pp

sho takase jun suzuki naoaki okazaki tsutomu hirao and masaaki nagata neural headline generation on abstract meaning tion

jiwei tan xiaojun wan and jianguo xiao abstractive document summarization with a graph based attentional neural model
ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan n gomez lukasz kaiser and illia polosukhin attention is all you need arxiv computation and language
oriol vinyals meire fortunato and navdeep jaitly pointer networks kristian woodsend and mirella lapata automatic generation of story
highlights
kaichun yao libo zhang tiejian luo and yanjun wu deep inforcement learning for extractive document summarization computing
yongjian you weijia jia tianyi liu and wenmian yang improving abstractive document summarization with salient information ing
wenyuan zeng wenjie luo sanja fidler and raquel urtasun cient summarization with read again and copy mechanism arxiv computation and language
haoyu zhang yeyun gong yu yan nan duan jianjun xu ji wang ming gong and ming zhou pretraining based natural language eration for text summarization
arxiv computation and language
xingxing zhang mirella lapata furu wei and ming zhou neural latent extractive document summarization
xingxing zhang furu wei and ming zhou hibert document level pre training of hierarchical bidirectional transformers for document summarization
qingyu zhou nan yang furu wei shaohan huang ming zhou and tiejun zhao neural document summarization by jointly learning to score and select sentences

