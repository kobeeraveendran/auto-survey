n a j l c
s c v
v i x r a neural related work summarization with a joint context driven attention mechanism yongzhen xiaozhong zheng of maritime economics and management dalian maritime university dalian china of informatics computing and engineering indiana university bloomington bloomington in usa group hangzhou china
com
edu
iu
edu abstract conventional solutions to automatic related work summarization rely heavily on engineered features
in this paper we develop a neural data driven summarizer by ing the paradigm in which a joint context driven attention mechanism is posed to measure the contextual relevance within full texts and a heterogeneous raphy graph simultaneously
our motivation is to maintain the topic coherency between a related work section and its target document where both the textual and graphic contexts play a big role in characterizing the ship among scientic publications accurately
experimental results on a large dataset show that our approach achieves a considerable provement over a typical summarizer and ve classical summarization baselines
introduction in scientic elds scholars need to contextualize their contribution to help readers acquire an derstanding of their research papers
for this pose the related work section of an article serves as a pivot to connect prior domain knowledge in which the innovation and superiority of current work are displayed by a comparison with ous studies
while citation prediction can assist in drafting a reference collection nallapati et al
consuming all these papers is still a rious job where authors must read every source document carefully and locate the most relevant content cautiously
as a solution in saving authors efforts tomatic related work summarization is tially a topic biased multi document problem cong and kan which relies heavily on human engineered features to retrieve snippets corresponding author from the references
most recently neural works enable a data driven architecture to sequence for natural language ation bahdanau et al
where an coder reads a sequence of words sentences into a context vector from which a decoder yields a sequence of specic outputs
nonetheless pared to scenarios like machine translation with an end to end nature aligning a related work section to its source documents is far more challenging
to address the summarization alignment mer studies try to apply an attention mechanism to measure the saliency novelty of each candidate word sentence tan et al
with the aim of locating the most representative content to retain primary coverage
however toward summarizing a related work section authors should be more ative when organizing text streams from the ence collection where the selected content ought to highlight the topic bias of current work rather than retell each reference in a compressed but anced fashion
this motivates us to introduce the contextual relevance and characterize the ship among scientic publications accurately
generally speaking for a pair of documents a larger lexical overlap often implies a higher ilarity in their research backgrounds
yet such a hypothesis is not always true when sampling tent from multiple relevant topics
take as an example from viewpoint of the abstract ilarity those references investigating information retrieval latent semantic model or through data mining could be of more tance in correlation and should be greatly sampled for the related work section
but in reality this ticle spends a bit larger chunk of texts about to elaborate deep learning during the ture review which is quite difcult for machines deep structured semantic models for web search using clickthrough data huang et al
to grasp the contextual relevance therein
in tion other situations like emerging new concepts also suffer from the terminology variation or phrasing in varying degrees
in this study we utilize a heterogeneous ography graph to embody the relationship within a scalable scholarly database
over the recent past there is a surge of interest in exploiting diverse lations to analyze bibliometrics ranging from erature recommendation yu et al
to topic evolvement jensen et al

in a graphical sense interconnected papers transfer the credit among each other directly indirectly through ious patterns such as paper citation author laboration keyword association and releasing on series of venues which constitutes the graphic context for outlining concerned topics
nately a variety of edge types may pollute the formation inquiry where a slice of edges are not so important as the others on sampling content
meanwhile most existing solutions in mining erogeneous graphs depend on the human sion e

hyperedge bu et al
and ath swami et al

this is usually not easy to access due to the complexity of graph schemas
our contribution is threefold first we explore the edge type usefulness distribution eud on a heterogeneous bibliography graph which ables the relationship discovery between any pair of papers for sampling the interested tion
second we develop a novel rizer for the automatic related work tion where a joint context driven attention anism is proposed to measure the contextual evance within both textual and graphic contexts
third we conduct experiments on papers with native related work sections and tal results show that our approach outperforms a typical summarizer and ve classical summarization baselines signicantly
related work this study touches on several strands of research within automatic related work summarization and summarizer as follows
the idea of creating a related work section tomatically is pioneered by cong and kan who design two rule based strategies to extract sentences for general and detailed topics tively
subsequently hu and wan ploit probabilistic latent semantic indexing to authors cong and kan hu and wan widyantoro and amin chen and hai number of papers table data scales of previous studies on automatic related work summarization
topic biased split candidate texts into different parts then apply several regression models to learn the importance of each sentence
similarly widyantoro and amin transform the marization problem into classifying rhetorical egories of sentences where each sentence is resented as a feature vector containing word quency sentence length and
most recently chen and hai construct a graph of sentative keywords in which a minimum steiner tree is gured out to guide the summarization as nding the least number of sentences to cover in general compared the discriminated nodes
to traditional summaries the automatic related work summarization receives less concerns over the past
however these existing solutions not work without manual intervention which its the application scale to an extremely small size see table
the earliest summarizer stems from rush et al
which utilizes a feed forward network for compressing sentences and later is expanded by chopra et al
with a current neural network rnn
on this basis nallapati et al
c and chen et al
both present a set of rnn based models to dress various aspects of abstractive tion
typically cheng and lapata pose a general summarizer where an encoder learns the representation of documents while a decoder generates each word sentence ing an attention mechanism
with further search nallapati et al
extend the tence compression by trying a hierarchical tion architecture and a limited vocabulary during the decoding phase
next narayan et al
leverage the side information as an attention cue to locate focus regions for summaries
recently inspired by pagerank tan et al
introduce a graph based attention mechanism to tackle the saliency problem
nonetheless these methods all discuss the single document scenario which is far from the nature of automatic related work rization
in this study derived from the general summarizer of cheng and lapata we pose a joint context driven attention mechanism to measure the contextual relevance within full texts and a heterogeneous bibliography graph neously
to our best knowledge we make the rst attempt to develop a neural data driven solution for the automatic related work summarization and the practice of using the joint context as an tion cue is also less explored to date
besides this study is launched on a dataset with up to pers which is much greater than previous studies and makes our results more convincing
text since summarization via word word generation is not mature at present cheng and lapata nallapati et al
tan et al
we adopt the extractive tial fashion for our summarizer where a related work section is created by extracting and linking sentences from a reference collection
meanwhile this study follows the mode of cong and kan who assume that the collection is given as part of the input and do not consider the citation sentences of each reference
methodology
problem formulation to adapt the paradigm we formulate the automatic related work summarization into a quential text generation problem as follows
given an unedited paper t target document and its n size reference collection rt rt n we draw up a related work section for t by ing sentences from rt
to be specic each ence source document will be traversed one time sequentially and without loss of generality in the descending order of their signicance to t
sequently all sentences to be selected are nated into an m length sequence st st m to feed the summarizer
for each candidate sentence j once being visited a label yt st j will be determined synchronously based on whether or not this sentence should be covered into the output
our objective is to maximize the likelihood probability of observed labels yt yt m under rt st and summarizer parameters as shown below
max log j rt st m x
random walk on heterogeneous bibliography graph author coauthor written by cite contribute join paper relevant publish venue contribute keyword investigate contribute figure heterogeneous bibliography graph
prior works have illustrated that one of the most promising channels for information dation is the community network guo and liu
in this study we verify this hypothesis toward the content sampling of scientic rization by investigating heterogeneous relations among different kinds of objects such as papers authors keywords and venues
z for measuring the relationship among tic publications we introduce a directed graph g v e to contain various bibliographical nections as shown in figure which involves four objects and ten edge types in total
each edge ej i e is assigned a value to cate the transition probability between two nodes vj v where i r returns the known edge type usefulness of ej i and r is a normalizing weight
for most of edge types we model the weight as one divided by the number of outgoing links of the same kind
but ing the contribution category the weight eling is accomplished by pagerank with priors white and smyth
note that different edge types usually take very uneven importance in one particular task yu et al
and it is quite cult to enable the classical heterogeneous graph mining without expert dened paths for random walk bu et al
swami et al

in this study we propose an unsupervised proach to capture the connectivity diversity by introducing an optimal eud for navigating dom walkers on the heterogeneous bibliography graph
given a target document t the optimized usefulness assignment can help those walkers lock a top n recommendation rt to best match the erence collection rt as shown in eq

on this basis a well performing algorithm grover and leskovec is adopted to duct an unsupervised random walk to vectorize ery node v v into a dimensional ding rd so that any edge e e can be calculated therefrom
specically we employ evolutionary algorithm ea to tune the eud which enjoys advantages over conventional ent methods in both convergence speed and racy
arg max log j rt eud n x t x ea setup we use an array of real numbers to code an individual in the population where denotes the usefulness of j th edge type
given an eud pagerank page runs on graph to infer the relative importance of each node for each target document and a tness tion is applied to judge how well this eud es locating the ground truth references as eq
j rt n in which if rt j belongs to rt then returns the ranking of rt j within rt and otherwise a big penalty coefcient to prevent irrelevant erences to be recommended
like most other timizations this procedure starts with a randomly generated population
max pt p j n j ea operator we choose the operator from ferential evolution das and suganthan to generate offsprings for each individual
the basic idea is to utilize the difference between different individuals to disturb each trial object
first three distinct individuals are sampled randomly from current population to create a as shown in eq
where r ant xvar dicates the scaling factor
next xvar is crossed to build a hybrid one xhyb with a trial object xtri as eq
in which denotes the crossover factor and u represents an uniform random number
at last the tnesses of xtri are compared and the better one will be saved as the offspring into a new round of evolution
and xhyb j xvar j f j j xhyb xvar j xtri j if u c otherwise
neural extractive summarization as figure shows we model our rizer with a hierarchical encoder and an based decoder as described below
hierarchical encoder our encoder consists of two major layers namely a convolutional ral network cnn and a long short term ory rnn
specically the cnn deals with word level texts to derive level meanings which are then taken as inputs to the rnn for handling longer range dependency within lager units like a paragraph and even a whole paper
this conforms to the nature of ment that is composed from words sentences and higher levels of abstraction narayan et al

where each word wt j i can be represented by a j i rd
dimensional embedding ous studies have illustrated the strength of cnn in presenting sentences because of its capability to learn compressed expressions and address tences with variable lengths kim
first a convolution kernel k rdqd is applied to each possible window of q words to construct a list of feature maps as consider a sentence of p words j wt gt j i tanh k i where rd denotes the bias term
next over time pooling collobert et al
is formed on all generated features to obtain the tence embedding as max gt where i denotes the i th row of matrix
given a sequence of sentences st st m we then take the rnn to yield an equal length array of hidden states in which lstm has proved to leviate the vanishing gradient problem when ing long sequences hochreiter and schmidhuber
each hidden state can be viewed as a cal representation with focusing on current and former sentences together which is updated as ht j lstm in practice we use multiple kernels with ous widths to produce a group of embeddings for j ht rd
t t t








t t thi t t


hidden state node embedding t t


attention j context vector t t


tyi t t


binary decision trn t j th j


m attention based decoder t j ty j


tym max over time pooling convolution average ts feature map sentence embedding t j t j


t j word embedding hierarchical encoder figure framework of our summarizer
each sentence and average them to capture the information inside different n grams
as figure bottom shows the sentence st j involves six words and two kernels of widths two orange and three green abstract a set of ve and four ture maps respectively
meanwhile since cal structure theory mann and thompson points out that association must exist in any two parts of coherent texts rnn is only applicable to manage the sentence relation within a single ument because we can not expect the dependency between two sections from different references
attention based decoder our decoder labels each sentence st j as sequentially according to whether it is salient or novel enough plus if vant to the target document t or not
as shown in figure top the binary decision yt j is made by j and the context vector ht both the hidden state ht j from an attention mechanism grey background
in particular this attention red dash line is acted as an intermediate stage to determine which tences to highlight so as to provide the contextual information for current decision bahdanau et al

given ht ht m this decoder returns the probability of yt as below rt st sigmoid j ht j ht j aj iht i m x j ht r denotes a fully connected where j and ht layer with as input the concatenation of ht j and aj i is the attention weight indicating how much the supporting sentence st i contributes to extracting the candidate one st j
apart from saliency and novelty two traditional attention factors chen et al
tan et al
we focus on the contextual relevance within both textual and graphic contexts to distinguish the relationship from near to far as shown in eq
and eq

to be specic htt i i to st sents the saliency of st j dtt i cates the novelty of st i to the dynamic output dt j to t from the textual context i refers to the relevance from the graphic context
more i denotes the relevance of st i wsht j wnht concretely w rd characterizes the learnable matrix returns the average of hidden states from t and return the node dings of both t and the source document that ht i belongs to respectively
note that and represent two distinct embedding spaces where the former reects the lexical collocations of pus and the latter embodies the connectivity terns of associated graph
aj wsht htt i saliency dtt j wnht i novelty i dt j rt st ht i x the basic idea behind our attention mechanism if a supporting sentence more is as follows sembles a candidate one or overlaps less with the dynamic output or is more relevant to the target document then it can provide more contextual formation to facilitate current decision on being extracted or not thereby taking a higher weight in the generated context vector
this innovative tention will guide our goal related work section to maximize the representativeness of selected tences saliency novelty while minimizing the semantic distance to the target document vance
this is consistent with the way that ars consume a reference collection with the max objective in their minds
experiment
experimental setup this section presents the experimental setup for assessing our approach including dataset used for training and testing implementation details contrast methods and evaluation metrics
dataset we conduct experiments on a created from the acm digital library where data and full texts are derived from pdf les
to be detailed this dataset includes papers help outcome we while
com readers share copyrighted the experiment is reproduce the of information experiment data removed
part the authors keywords and venues in total
note that we ignore the keyword with frequency below a certain threshold and adopt greedy matching of guo et al
to generate pseudo keywords for papers lacking topic tions
for each target document the references are traversed by the descending order of the cited number in related work section primary and in full paper secondary successively
we rst ply a series of pre processings such as lowercasing and stemming to standardize candidate sentences then remove those which are too short long or words
on this basis a total of papers are selected to evaluate our approach each containing more than references found in the dataset and a related work section of at least words
but as for the heterogeneous bibliography graph all source data have to be imported to sure the structural integrity of communities
sides this graph should be constructed year year to preclude the effect of later publications on earlier ones
implementation we use tensorow for mentation where both the dimensions of ding and hidden state are equally
for the cnn mikolov et al
is utilized to initialize the word embeddings which can be further tuned during the training phase
while we follow the work of kim to ply a list of kernels with widths
as for the rnn each lstm module is set to one single layer and all input documents are padded to the same length along with a mark to indicate the real number of sentences
based on these settings we train our summarizer using adam with the default in kingma and ba and perform mini batch cross entropy training with a batch of one target document for epochs
to create training data for our summarizer each reference needs to be annotated with the ground i
e
candidate sentences are truth in advance tagged with for indicating summary worthy or not
specically we follow a heuristic practice of cao et al
and nallapati et al
to compute score lin and hovy for each sentence in terms of the native related work sections gold standards
next those tences with high scores are chosen as the positive samples and the rest as the negative ones such that the total score of selected sentences is imized with respect to the gold standard
as for testing we relax the number of sentences to be lected and focus on the classication probability from eq

in this study cross validation is plied to split the dataset into ten parts equally at random in which nine are used for training and the other one for testing
evaluation we adopt the widely used toolkit rouge lin and hovy to evaluate the summarization performance automatically
in ticular we report and gram and bigram overlapping as a way to assess the informativeness and rouge l the longest common subsequence as a means to assess the uency in terms of xed bytes of gold standards
to validate the proposed attention nism we compare our approach denoted as p
against six variants including p
void a plain summarizer without tions p
s use the saliency as an only leverage both the saliency tion factor p
and novelty p
incorporate the relevance from the textual context p
gain the relevance from the graphic context of a geneous citation graph p
utilize the heterogeneous bibliography graph but with each edge type the same usefulness
in addition we also select six representative summarization methods as a benchmark group
the rst one is the general summarizer by cheng and lapata denoted as net which employs an attention mechanism to tract sentences directly after reading them
lowing are ve classical generic solutions ing luhn luhn a heuristic rization based on word frequency and tion mmr carbonell and goldstein a diversity based re ranking to produce summaries lexrank erkan et al
a graph based summary technique inspired by pagerank and hits sumbasic nenkova and vanderwende a frequency based summarizer with plication removal nltksum acanfora et al
a natural language tookit implementation for summarization
for clarity luhn lexrank and sic are analogous to the work of hu and wan which extracts sentences scoring the and they are also trasted in the latest studies on neural ers chen et al
tan et al

while mmr often serves as a part post processing in signicance of existing techniques to avoid the redundancy cohan and goharian and we introduce nltksum to investigate the impact of ical semantic analysis to the automatic related work summarization
note that former studies specially for this task require extensive human volvements see table thus we can not apply them to such a large dataset of this study

results and discussion table reports the evaluation comparison over rouge metrics
from the top half all scores pear a gradual upward trend with incorporation of saliency novelty relevance from both textual and graphic contexts and eud into consideration one after another which demonstrates the validity of our attention mechanism for summarizing related work sections
to be specic we further reach the following conclusions p
void vs
p
s vs
p
both saliency and novelty are two effective factors to locate the quired content for summaries which is consistent with prior studies
p
vs
p
contextual relevance does contribute to address the alignment between a lated work section and its source documents
p
vs
p
textual context alone can not provide entire evidence to characterize the relationship among scientic publications exactly
p
vs
p
heterogeneous liography graph involves richer contextual mation than a homogeneous citation graph
p
vs
p
eud plays an indispensable role in organizing accurate tual relevance on a heterogeneous graph
figure number of extracted words on each reference cluster under different attention factors
continuing the dssm figure visualizes the number of extracted words on each reference methods p
void p
s p
p
p
p
p
luhn mmr lexrank sumbasic nltksum pointernet rouge l






































indicates wilcoxon signed rank test
compared with p
table rouge evaluation on papers from acm digital library
under different attention factors
it can be seen that only after adding the relevance pecially that from the graphic context into tions our summarizer can correctly sample the content from deep learning yellow line and eliminate that originated from other sources by a big margin green line
as this example falls into the methodology transferring a host of its volved word collocations are not idiomatic binations yet such as deep neural network occurs with clickthrough data that is more quently related to latent semantic analysis at that time which results in a somewhat biased tual context
by contrast the graphic context will suffer less from this bias because it characterizes the connectivity patterns real time setup instead of n gram statistics thus offering a more robust measure for the contextual relevance
the bottom half of table illustrates the ority of our approach over six representative marization methods
above all luhn lexrank and mmr three summarizers that simply exploit shallow text features word frequency and ciated sentence similarity to measure either nicance or redundancy fall far behind the plain variant p
void which partly reects the strength of paradigm in summarizing a related work section
second with combination of nicance and redundancy sumbasic achieves a drastic increase on and a mild raise on pack the references cited in the same subsection of the related work section as one reference cluster
respectively but it still can not improve rouge l marginally
this is because simple text statistics can not present deeper levels of ral language understanding to catch larger grained units of co occurrence
third nltksum benets from a nltk library so as to access cal semantic supports thereby having the best formativeness and among the ve generic baselines and meanwhile a parable uency rouge l with our approach
finally as a deep learning solution although pointernet takes both hidden states and previously labeled sentences into account at each decoding step it focuses on only current and just one vious sentences lacking a comprehensive eration on saliency novelty and more importantly the contextual relevance p

to better verify the summarization mance we also conduct a human evaluation on papers containing more than references in the dataset
we assign a number of raters to pare each generated related work section against the gold standard and judge by three independent aspects as how compliant is the related work section to the target document how intuitive is the related work section for readers to grasp the key content how useful is the related work section for researchers to prepare their nal ature reviews note that we do not allow any ties during the comparison and each property is sessed with a point scale of worst to best
table displays how often raters rank each summarizer as the and so on in terms of methods luhn mmr lexrank sumbasic nltksum pointernet p










































mean ranking













table human evaluation proportion on papers with more than references in the dataset
best to worst
specically our approach comes the on of the time which is followed by nltksum that is considered the best on of the time almost half of ours and net with quite equal proportions on each ing
furthermore the other four summarizers count for obviously lower ratings in general
to attain the statistical signicance one way sis of variance anova is performed on the tained ratings and the results show that our proach is better than all six contrast methods nicantly p
which means that the clusion drawn by table is sustained
conclusion in this paper we highlight the contextual vance for the automatic related work tion and analyze the graphic context to terize the relationship among scientic tions accurately
we develop a neural data driven summarizer by leveraging the paradigm where a joint context driven attention mechanism is proposed to measure the contextual relevance within full texts and a heterogeneous phy graph simultaneously
extensive experiments demonstrate the validity of the proposed attention mechanism and the superiority of our approach over six representative summarization baselines
in future work an appealing direction is to ganize the selected sentences in a logical ion e

by leveraging a topic hierarchy tree to determine the arrangement of the related work section cong and kan
we also would like to take the citation sentences of each ence into consideration which is another concise and universal data source for scientic tion chen and hai cohan and goharian
at the end of this paper we believe that extractive methods are by no means the nal lutions for literature review generation due to giarism concerns and we are going to put forward a fully abstractive version in further studies
acknowledgement we would like to thank the anonymous reviewers for their valuable comments
this work is partially supported by the national science foundation of china under grant no

references joseph acanfora marc evangelista david keimig and myron su

natural language ing generating a summary of ood disasters
cell
dzmitry bahdanau kyunghyun cho and yoshua gio

neural machine translation by jointly arxiv preprint learning to align and translate


dzmitry bahdanau jan chorowski dmitriy serdyuk philemon brakel and yoshua bengio

to end attention based large vocabulary speech the ieee in proceedings of recognition
icassp international conference on acoustics speech and signal processing shanghai china pages
jiajun bu shulong tan chun chen can wang hao wu lijun zhang and xiaofei he

music ommendation by unied hypergraph combining cial media information and music content
in ceedings of the acm sigmm international ference on multimedia amsterdam netherlands pages
ziqiang cao wenjie li sujian li furu wei and ran li

attsum joint learning of focusing and summarization with neural attention
arxiv preprint

jaime carbonell and jade goldstein

the use of mmr diversity based reranking for reordering uments and producing summaries
in proceedings of the international acm sigir conference on research and development in information trieval new york usa pages
jingqiang chen and zhuge hai

summarization in proceedings of related work through citations
of the ieee skg international conference on semantics knowledge and grids beijing china pages
qian chen xiaodan zhu si wei si wei and hui jiang

distraction based neural networks for in proceedings of the acm modeling documents
ijcai international joint conference on articial intelligence new york usa pages
jianpeng cheng and mirella lapata

neural summarization by extracting sentences and words
in proceedings of the acl annual meeting of the association for computational linguistics berlin germany
sumit chopra michael auli and alexander m
rush

abstractive sentence summarization with in proceedings tentive recurrent neural networks
of the naacl conference of the north american chapter of the association for computational guistics san diego usa pages
arman cohan and nazli goharian

tic article summarization using citation context arxiv preprint and article s discourse structure

pages
ronan collobert jason weston michael karlen ray kavukcuoglu and pavel kuksa

natural language processing almost from scratch
journal of machine learning research
duy vu hoang cong and min yen kan

towards in automated related work summarization
ceedings of the acm coling international conference on computational linguistics beijing china pages
swagatam das and ponnuthurai nagaratnam than

differential evolution a survey of the state of the art
ieee transactions on evolutionary computation
erkan radev and r dragomir

lexrank based lexical centrality as salience in text rization
journal of qiqihar junior teachers lege
aditya grover and jure leskovec

scalable feature learning for networks
in ings of the acm sigkdd international ference on knowledge discovery and data mining san francisco usa pages
chun guo and xiaozhong liu

automatic ture generation on heterogeneous graph for music in proceedings of the recommendation
ternational acm sigir conference on research and development in information retrieval ago chile pages
chun guo jinsong zhang and xiaozhong liu

scientic metadata quality enhancement for arly publications
ischools
sepp hochreiter and jrgen schmidhuber

long short term memory
neural computation
yue hu and xiaojun wan

automatic ation of related work sections in scientic papers in proceedings of the an optimization approach
acl emnlp conference on empirical methods in natural language processing doha qatar pages
po sen huang xiaodong he jianfeng gao li deng alex acero and larry heck

learning deep structured semantic models for web search using in proceedings of the clickthrough data
acm cikm international conference on tion knowledge management san francisco usa pages
scott jensen xiaozhong liu yingying yu and stasa milojevic

generation of topic evolution trees from heterogeneous bibliographic networks
nal of informetrics
yoon kim

convolutional neural networks for sentence classication
eprint arxiv
diederik kingma and jimmy ba

adam a method for stochastic optimization
computer ence
chin yew lin and eduard hovy

matic evaluation of summaries using n gram occurrence statistics
in proceedings of the naacl the annual conference of the north american chapter of the association for computational guistics stroudsburg usa pages
h
p
luhn

the automatic creation of literature abstracts
ibm corp
william c
mann and sandra a
thompson

rhetorical structure theory toward a functional ory of text organization
text talk
tomas mikolov kai chen greg corrado and jeffrey dean

efcient estimation of word tations in vector space
computer science
ramesh nallapati bing xiang and bowen zhou

sequence to sequence rnns for text rization
in proceedings of the international ence on learning representations workshop track san juan puerto rico
ramesh nallapati feifei zhai and bowen zhou

summarunner a recurrent neural network based sequence model for extractive summarization of documents
arxiv preprint

ramesh nallapati bowen zhou cicero nogueira dos santos caglar gulcehre and bing xiang

abstractive text summarization using arxiv preprint to sequence rnns and beyond


ramesh m
nallapati amr ahmed eric p
xing and william w
cohen

joint latent topic models in proceedings of the for text and citations
acm sigkdd international conference on edge discovery and data mining las vegas usa pages
shashi narayan nikos papasarantopoulos shay b
cohen and mirella lapata

neural tive summarization with side information
arxiv preprint

ani nenkova and lucy vanderwende

the pact of frequency on summarization
microsoft search
l page

the pagerank citation ranking ing order to the web online manuscript
stanford digital libraries working paper
alexander m rush sumit chopra and jason weston

a neural attention model for abstractive in proceedings of the acl tence summarization
emnlp conference on empirical methods in ural language processing lisbon portugal pages
ananthram swami ananthram swami and thram swami

scalable resentation learning for heterogeneous networks
in proceedings of the acm sigkdd national conference on knowledge discovery and data mining halifax canada pages
jiwei tan xiaojun wan jianguo xiao jiwei tan aojun wan and jianguo xiao

abstractive document summarization with a graph based tional neural model
in proceedings of the acl annual meeting of the association for tional linguistics vancouver canada pages
scott white and padhraic smyth

algorithms in for estimating relative importance in networks
proceedings of the acm sigkdd international conference on knowledge discovery and data ing washington usa pages
dwi h widyantoro and imaduddin amin

tation sentence identication and classication for in proceedings of related work summarization
the icacsis international conference on advanced computer science and information systems pages
yingying yu xiaozhong liu and zhuoren jiang

random walk and feedback on scholarly network
in proceedings of the acm national workshop on graph search and beyond santiago chile pages

