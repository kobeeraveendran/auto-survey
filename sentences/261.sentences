interpretable multi headed attention for abstractive summarization at controllable lengths ritesh sarkhel moniba keymanesh arnab nandi srinivasan parthasarathy department of computer science and engineering the ohio state university sarkhel




edu v o n l c
s c v
v i x r a abstract abstractive summarization at controllable lengths is a challenging task in natural language cessing
it is even more challenging for domains where limited training data is available or scenarios in which the length of the summary is not known beforehand
at the same time when it comes to trusting machine generated summaries explaining how a summary was constructed in human understandable terms may be critical
we propose multi level summarizer mls a supervised method to construct abstractive summaries of a text document at controllable lengths
the key enabler of our method is an interpretable multi headed attention mechanism that putes attention distribution over an input document using an array of timestep independent mantic kernels
each kernel optimizes a human interpretable syntactic or semantic property
exhaustive experiments on two low resource datasets in english language show that mls performs strong baselines by up to
in the meteor score
human evaluation of the maries also suggests that they capture the key concepts of the document at various length budgets
introduction great progress has been made in recent years on abstractive summarization of text documents
among existing works sequence to sequence networks with attention gehring et al
liu et al
have been one of the clear front runners
being able to constrain the length of a summary while ing its desirable properties has many real world applications
one such application is content tion for variable screen sizes
online content creators such as news portals blogs and advertisement agencies with audiences on multiple platforms customize their content based on display area for best perience
however there has not been much work on summarization at controllable lengths until recently
high variance in screen sizes often require extensive human supervision to perform these modications
as most sequence to sequence networks rush et al
nallapati et al
do not enforce the length of a summary for scenarios as mentioned above one may need to employ an ensemble of works to cover all possible lengths
there are two major challenges in following this approach for world applications
first training sequence to sequence networks is a resource intensive task strubell et al

to train a network for generating summaries budgeted at length b we need a parallel corpus of text documents and their gold standard summaries at length b
constructing a large enough corpus with summaries budgeted at b may not be possible cost efcient for a number of mains
this is one of the main reasons why most existing works on abstractive summarization evaluate their model on large scale news corpus datasets nallapati et al
hermann et al
leaving out a number of important but low resource domains magooda and litman parida and motlicek where the number of available training documents is limited
second the range of possible budgets may not always be known beforehand
in many scenarios it can be known as late as during run time
therefore we formalize the summarization task addressed in this paper as follows
problem denition given a document s of length n tokens and a maximum token budget of b we aim to construct an abstractive summary sb that satises the following conditions information the rst two authors contributed equally to this work this work is licensed under a creative commons attribution
international license
license details
org licenses
input text police are hunting a man aged between and suspected of robbing a bank in broad daylight and running off with in cash
the robbery took place at
at a lloyds bank branch in fairwater cardiff police said
detectives have issued cctv images of the suspect who is to ft in to ft and was wearing black clothing
the white male suspect who has greying black hair and wore glasses was captured on camera inside the bank
detectives said no one was injured during the robbery and they were condent the public would be able to identify the suspect
detective sergeant andy miles from fairwater cid said inquiries are continuing to identify the culprit
the cctv is clear and i am condent that members of the public will know his identity



truncated summary at compression budget police are hunting a man aged between and suspected of robbing a bank in broad daylight and running off with in cash
the robbery took place at
at a lloyds bank branch in fairwater cardiff police said
the white male suspect who has greying black hair and wore glasses was captured on camera inside the bank
detectives have issued cctv images of the suspect who is to ft in to ft and was wearing black clothing
detective sergeant andy miles from fairwater cid said inquiries are continuing to identify the culprit
robbery took place at
at a lloyds bank branch in fairwater cardiff
detectives have issued cctv images of the suspect who is to
detective sergeant andy miles from fairwater cid said inquiries are continuing to identify the culprit
prototype summary figure mls expands the highlighted sentences in the prototype summary to the boldfaced tokens in the input text to construct a summary budgeted at half length of the input text redundancy is minimized in sb coverage of the major topics of s is maximized in sb length of sb is maximal within the specied budget without adversely affecting the conditions and i
e
b sc such that
and ensure that the properties of a high quality summary is preserved in sb whereas ensures that sb is the largest possible summary that can be constructed within budget without compromising its quality
note that and are seemingly contradictory to each other as the length of the summary increases
our goal is to nd the optimal tradeoff
early works on incremental summarization buyukkokten et al
yang and wang leveraged structural tags supported by document markup languages to generate summaries at various lengths thus imposing a serious constraint on the document formats e

xml html that come under the purview of such methods
incremental sampling of sentences based on a salience score otterbacher et al
campana and tombros can partially solve this problem by constructing extractive summaries of the input document
we show in section that these sampling based methods often fail to preserve the desirable properties of a high quality summary
among recent works kikuchi et al
were the rst to propose a supervised method for controlling length during abstractive summarization
their work was later extended by fan et al
who introduced the length of a summary as an input to the network
however instead of exact input they approximate the length to a predened value range often failing to adhere to the allocated budget in a number of cases
liu et al
address this issue by proposing a convolutional encoder decoder network introducing the desired summary length as an input to the initial state of the decoder
we compare and report its performance on two datasets in our experimental setup in section
unfortunately when it comes to these models i
e
how the summaries came to be the answer still remains illusive
explaining how a machine generated summary was constructed has come a necessity under the newly introduced general data privacy regulation act itgp cially for applications in enterprise sarkhel and nandi keymanesh et al
and biomedical domain moradi and ghadiri sarkhel et al

some recent efforts have proposed using terpretable heatmaps baan et al
generated from the attention distribution over an input sequence for interpreting model behaviour
however they are still quite limited jain and wallace in sistently explaining all aspects of a neural summarizer
this leaves a gap in the ongoing efforts song et al
song et al
to generate abstractive summaries that are guided by human interpretable semantic syntactic qualities
briey the main goal of attention mechanism in a encoder decoder work is to assign a softmax score to every encoder hidden state based on its relevance to the token ability to explain or to present in understandable terms to a human doshi velez and kim being decoded and amplify those that are assigned high scores through a weighted average
target attention nallapati et al
relies on another sequence for computing these scores whereas self attention vaswani et al
paulus et al
operates over the elements in the current input sequence
a multi headed attention mechanism allows a neural model to speed up training by enabling parallelization across timesteps
the number of operations in the computation of self attention however scales quadratically with input length making it a computationally expensive operation for long input sequences
training such a network for a summarization task would require a large parallel corpus of input documents and their corresponding gold standard summaries budgeted at b
the role of some of the attention heads during abstractive summarization is also not transparent baan et al

to dress these we replace self attention with a lightweight interpretable alternative
instead of projecting each input sequence multiple at every timestep we encode an input sequence only once using a timestep independent kernel learned in an unsupervised or distantly supervised way from the put document
each kernel has a human interpretable syntactic semantic role
every attention head in this multi headed mechanism computes an attention distribution over the input sequence using a unique kernel recycling it at every timestep
compared to self attention our proposed attention mechanism scales linearly with the input sequence length and leverages signicantly less number of trainable eters
as we will show in section this allows us to train our network on limited training samples in low resource datasets
we propose mls a supervised method to erate abstractive summaries at arbitrary lengths in this paper
it computes a sb summary constrained length b by budgeted at soft switching a between copy and expand operation over a prototype summary sp constructed from the ument
the key enabler in this process is an terpretable multi headed attention mechanism
we develop length aware network encoder decoder called the pointer magnier leverages this network that attention mechanism to construct summaries within a specied length
we train our network on limited training samples from two cross domain datasets the msr narrative ouyang et al
and thinking machines dataset brockman
exhaustive evaluation on a range of success metrics shows that mls performs competitively or better against strong baseline methods
subsequent human evaluation of summaries generated by mls suggests that they accurately capture the main concepts of the input document
to summarize some of the major contributions of this work are as follows figure an overview of mls architecture
the pg network left constructs a prototype summary sp from the input document
the pointer magnier network right constructs the length constrained summary from sp using interpretable sentence level attention a we propose mls a supervised approach to generate abstractive summaries of a text document at controllable lengths
we develop a length aware encoder decoder network that leverages an interpretable multi headed attention mechanism to construct length constrained summaries
experimental results on two cross domain datasets show that trained on limited training samples mls was able to generate summaries that are coherent and captured the key concepts of a document
time each to compute the query key and value matrix vaswani et al
from the input sequence proposed methodology mls constructs a length constrained summary of a document in two steps
first it derives a prototype summary sp from the document covering its major concepts
then it expands or shortens it depending on the length budget to create the nal summary
we employ a pair of encoder decoder networks at both steps
for the rst step we extend the pg network see et al

we develop a length aware decoder network for the second step
we describe both steps in greater detail in the following sections

generating the prototype summary we extend pg network by see et al
to construct the prototype summary sp of a document
we tokenize the document and feed it to the encoder network sequentially
as the encoder hidden states are updated the decoder network constructs the prototype summary one token at a time by soft selecting between tokens in the input document and an external vocabulary
the decoding process is guided by an attention computed over the input document and the external vocabulary
an overview of this network is shown in fig
we point the readers to the work by see et al
for more background on this network
an example prototype summary is shown in figure
contrary to existing prototype text guided summarization methods liu et al
saito et al
we do not specify the length of the prototype summary as an input of the network rather infer it by outputting tokens until the eos token is produced
we discuss the training and parameter settings of the network used in our experiments in section

it is worth mentioning here that one of the main reasons to select the pg network as our architecture of choice for this step is due to its capability to construct a summary by looking up a learned language model
other networks with similar capabilities can also be used as this step has a transitive effect on the next phase of our approach

constructing the length constrained summary to construct a summary within length budget b we develop the pointer magnier network a aware interpretable encoder decoder network
an overview of the network is shown in fig

it consists of a multiplex layer an encoder yellow rectangles layer and a decoder green rectangles layer
the encoder layer takes the prototype summary constructed in the previous step as input
the decoder layer outputs the nal summary
we describe each layer in detail below
a
the multiplex layer and interpretable kernels in an effort to build a transparent network we embody three qualitative properties that are associated with a high quality summary in our network
a high quality summary maximizes the coverage of the major topics and keywords appearing in the input document while minimizing the amount of redundant information
we encode each property using a semantic kernel learned using an unsupervised or distantly supervised way from the input document itself
every kernel plays a unique human interpretable syntactic semantic role in constructing the nal summary
one of the key components in this process is the multiplex layer m
physically it is a nested matrix of dimensions shared between the encoder and decoder layer
each row in m contains the following information a distance metric disti a scalar value wi and c a semantic kernel where wi i i wi
during inference each of these kernels measures the contribution of every sentence in the prototype summary towards ing one of the properties i i mentioned above
wi represents the relative weight assigned to the property i in constructing the nal summary
we compute the kernels as a preprocessing step
dening the kernels to encode the property we dene as a matrix of dimensions where each row of represents one of the three most dominant topic vectors of the input document as a dimensional vector
we use an unsupervised lda based model blei et al
to derive these topic vectors
symmetric kl divergence is used as the distance metric
similarly we encode the property as a single dimensional vector of length where each vector component represents the relative frequency of one of the most frequent keywords in the input document
we use rake rose et al
a publicly available library to identify the keywords of a document
closely followed the ofcial implementation at
com abisee pointer generator symmetric kl divergence is used as the distance metric
finally we encode as a matrix of dimensions p where the ith row of represents an embedding of the ith sentence in the input document
we compute the embedding vector of each sentence using a pretrained model le and mikolov on english wikipedia
cosine similarity is used as the distance metric
our choice of unsupervised distantly supervised kernels reects our motivation see section to leverage a limited number of training samples from the experimental dataset to construct the nal summary
we discuss the role played by each semantic kernel distance metric disti and weight wi in constructing the nal summary from sp in the following section
b
the encoder layer the encoder layer consists of allelly stacked encoder blocks
each encoder block see fig
contains an embedding layer and a local attention layer
at every timestep t a sentence from sp is fed into the ding layer of each of the three encoder blocks
it computes a xed length embedding of the sentence and propagates it to the local attention layer
each encoder block in our network is mapped to a unique triplet disti wi in the multiplex layer
to compute local attention ci attributed to a sentence in sp by the ith encoder block we embed it in the same semantic space as and compute its distance from in that encoding space eq

figure the encoder layer consists of parallelly stacked encoder blocks i r disti r ni ni ci t in eq
represents a kernel of dimensions r ni and represents an embedding vector of length ni
the embedding layer represents each sentence in sp in the same encoding space as the nel associated with that block
we compute the local attention ci by taking a column wise average of the distance matrix i eq

the kernel is reused for all the sentences fed to the ith encoder block
the distribution


obtained this way is then normalized to derive the local attention tion over sp
the nal attention distribution over sp at timestep t is computed by normalizing the weighted average eq
of local attention distributions computed by each attention head
norm i wi m m it is worth noting here that attributing each encoder block with a distinct attention head ensures that there is a dedicated pathway to compute local attentions for every encoder block
this allows us to parallelize the network and speed up the decoding process when constructing the nal summary
c
the decoder layer similar to the encoder the decoder layer also consists of parallelly stacked decoder blocks
each decoder block contains an embedding layer and a local attention layer
eters of the i th encoder block and i decoder block are shared
we construct a length constrained summary sb of the input document by processing each sentence in sp sequentially
depending on the remaining length budget at each timestep the nal summary is constructed by soft switching between a copy and expand operation
this process is guided by a sentence level attention distribution eq
computed over sp
if the copy operation is selected a sentence from sp is copied into the nal summary whereas the expand operation replaces a sentence with similar content from the input document in sb
the original ordering of sentences is preserved
the copy operation the probability of copying a sentence s from the prototype summary that has not been included in the nal summary sb till timestep t into sb is dened as follows initialized as where t represents a sentence level attention distribution over sp at timestep t
eq
we update the attention distribution at each timestep after a copy or expand operation
if s represents the sentence copied into sb at timestep t we update the attention distribution by zeroing out the probability of s in and renormalizing the resulting distribution
the expand operation if the length of our prototype summary sp is less than the length budget b mls can choose to expand a set of sentences from sp
for each sentence s sp we dene its set as the sentence n gram that is most similar to s in the input document
we determine the expansion set of a sentence by using beam search over all n grams in the input document that are yet to be included in the nal summary
our search objective being maximizing e
the rst term in denotes the average pairwise cosine similarity between s and the sentences in whereas the second term denotes the fraction of tokens in s that appear in
to minimize across sentence repetitions in the summary top candidates identied from the search process are re ranked chen and bansal based on the number of repeated word bigrams and trigrams if the expansion set is included in the nal summary
we obtained best performance by initializing n with and changing it to at later iterations of the decoding process
if i denotes the embedding vector of the k th sentence in computed by the embedding layer of the i decoder block we dene the probability of expanding a sentence s from the prototype summary to in the nal summary as follows
e i k r k t r e i k e ni i e m e wi ni m in eq
denotes the semantic kernel shared between the i th encoder block and decoder block
we compute the probability of including the kth sentence of into the nal summary by computing i k towards optimizing the qualitative property i encoded by rst eq

its contribution ce repeating this process for all the sentences in followed by normalization provides us with the distribution e i represents the probability distribution over
to obtain the expansion probability of a sentence in we repeat this process for all attention heads and average them eq

the probability of expanding a sentence s from the prototype summary is obtained by averaging the expansion probability of all sentences in
once a sentence s has been expanded into the nal summary we update the attention distribution by zeroing out the probability at s and renormalizing the resulting distribution




here e ce ce soft selection between copy and expansion we dene the probability of selecting between the copy and expand operation for a sentence s in the prototype summary as follows
if b if b in eq
denotes the partially constructed summary till timestep t
if the length budget b is smaller than the length of the prototype summary sp the probability of including a sentence from sp into the nal summary depends on the attention distribution t over sentences in sp that are not included in the nal summary till timestep t
in all other scenarios acts as a soft switch between copying or expanding a sentence in sp
a sentence can be expanded only if doing so does not exceed the length budget
once the probability of each sentence its expansion set has been computed the decoder attends to the position with the highest probability and copies expands it into the nal summary
reaches b
we observed that the probability of expanding a sentence generation stops once from the prototype summary instead of copying it increases with the allocated length budget

training the networks we trained pg network and the pointer magnier network separately on a nvidia titan xp gpu with a batch size of
we pretrained the pg network on the cnn dailymail dataset nallapati et al
and then ne tuned it on training samples of our experimental datasets
using the evaluation script provided by nallapati et al
we obtained a training set of pairs and validation set of pairs for this dataset
all encoder decoder weights were allowed to be updated during ne tuning stage following a transfer pan and yang of weights from the pretrained network
the external vocabulary used in both pretraining and ne tuning stage consisted of k most frequent tokens in the training samples of the cnn dailymail dataset our experimental dataset or both
learning rate and initial accumulator values were set to
and
respectively
we used adagrad duchi et al
to train the network
the encoder was fed a maximum tokens and the decoder generated tokens during pretraining
these values were increased to and respectively during ne tuning
to prevent overtting we stopped training after iterations during the ne tuning stage
with respect to the pointer magnier network we learn the optimal values of wi i associated with each attention head by grid searching over the interval with the learning objective of maximizing score on the validation set
the optimal weights assigned to the attention head corresponding to topic coverage and keyword coverage were positive whereas information redundancy was assigned a negative weight for both of our datasets
experiments index dataset size max median mean we seek to answer three key questions in our experiments
given a length constrained mary sb how similar is sb to a gold standard summary is it coherent and representative of the input document and how abstractive is sb we answer the rst two questions by evaluating the summaries generated by mls over a range of success metrics on datasets belonging to two low resource domains
we also conduct a user study to measure how representative are the summaries with respect to the input documents
a representative summary covers the main topics of the document
we answer the third question by computing the percentage of n grams in sb that do not appear in the input document generated from the external vocabulary
table the minimum maximum median and erage number of sentences in datasets and msr narrative thinking machines

a
datasets we evaluate mls on two publicly available datasets from two low resource domains the msr narrative ouyang et al
dataset and the thinking machines brockman dataset
the msr narrative dataset contain personal stories shared by users on a social networking website
the thinking machines dataset on other hand contains position papers on a popular scientic topic published in an educational website
each document in both datasets is paired with a gold standard summary
we randomly selected document pairs to construct the training set and document pairs to construct a validation set for both datasets
the rest comprised the test corpus
we present an overview of some of the important properties of both datasets in table
b
metrics we compare the summaries constructed by mls against gold standard summaries using meteor banerjee and lavie and rouge lin
the average score of and rouge l metrics obtained for both datasets are shown in table
to measure the representativeness of a summary we compute the average kl divergence score between used py rouge benjamin heinzerling and the nltk library to compute the rouge and meteor score respectively
dataset metric budget budget mls mls mls budget mls budget mls budget

rouge l
meteor


rouge l
meteor
























































































































































table rouge and meteor scores of the budgeted summaries constructed by mls highlighted column and the baseline methods for the msr narrative and thinking machines dataset the topic vectors of a summary and its input document
following srinivasan et al
we measure the coherence of a summary by computing the average cosine similarity between consecutive sentences
we report the absolute difference between the coherence score computed for a summary and its input document in table
we also report the kl divergence score between sentiment vectors of a summary and the input document to check for potential biases in its polarity distribution
we used a publicly available library hutto and gilbert to derive the sentiment vectors

note that lower values of coherence and kl divergence score are desirable for a high quality summary
c
baselines we compare mls against three baseline methods
two of them follow a sampling based approach while our nal baseline method employs a convolutional network to construct length budgeted summaries
our rst baseline follows a systematic sampling based approach to construct length controlled summaries
initialized with a randomly selected sentence from the rst sentences of the input document it constructs the nal summary by including the k th sentence from the last sampled position
we set k in all of our experiments for both datasets
sampling terminates when the budget limit is exceeded or the end of document is reached
our second baseline method follows a weighted graph based sampling strategy to construct budgeted summaries
it represents each sentence in the input document as a node in an undirected complete weighted graph
the weight assigned to an edge in this graph is equal to the pairwise cosine similarity between the connecting nodes
to construct the budgeted summary we sample the top k nodes of this graph using a weighted pagerank algorithm halcea and tarau
sampling stops when the budget is reached
our third and nal baseline method is a convolutional approach proposed in liu et al

it is a sequence to sequence network with gated linear units dauphin et al
that takes the desired length of a summary as an additional input to the initial state of the decoder network
similar to our training protocol we pretrain this network on the cnn dailymail dataset rst and ne tune it on the training samples from both of our experimental datasets
we allowed all weights to be updated during the ne tuning phase

results and discussion we report the performance of all competing methods at ve length budgets
we specify the length budget to construct a summary as a product of the number of tokens in the input document and a budget c
results from our experiments are presented in tables and
the best performance achieved for each metric is boldfaced
we highlight some of our key ndings below


qualitative evaluation at ve compression budgets in general the abstractive methods mls and outperform sampling based approaches see table on both datasets
mls performs consistently well on all budgets although performance is relatively better on smaller budgets
we obtain an absolute improvement of
and
in score
and
in meteor score over the convolutional baseline for datasets and at compression budget
at higher budgets our performance was comparable with
in terms of coherence mls performs comparably or better than see table
smaller coherence score than and suggests that mls generated more coherent summaries than these two baseline methods
small kl divergence between the topic distribution of a budgeted summary and input document shows that mls generated summaries are representative of the document for both datasets
in fact topic coverage in summaries generated by mls is at least better than the convolutional baseline liu et al
although performance becomes comparable at larger budgets as more dataset metric budget budget mls mls mls budget mls budget mls budget topic sentiment coherence topic sentiment coherence























































































































table coherence and completeness of the budgeted summaries constructed by mls highlighted column and the baseline methods for msr narrative and thinking machines dataset sentences from the prototype summary are expanded to make the nal summary
mls outperfoms and in terms of staying true to the sentiment distribution of the input document
this can be seen from the small kl divergence scores obtained for the sentiment distribution achieved by mls in table
mls generated summaries were more abstractive at higher budgets fig

at sion budget
tokens in the summaries constructed for dataset and
tokens for dataset were contributed by the external vocabulary


ablative analysis figure abstractiveness of mls generated summaries to investigate the effects of pretraining on end to end results we compare the score of maries constructed by mls against an ablative baseline mls
it is identical to mls except that the pg network was not pretrained
in our second experiment we compare mls against an ablative baseline that constructs the prototype summary following a greedy heuristics otterbacher et al
instead of the pg network
mls outperforms both baselines fig
on both datasets thereby ing that using pg network in our framework and pretraining it on the cnn dailymail dataset improved the quality of our nal summaries
finally to investigate the effects of the semantic kernels introduced in the pointer magnier network we iteratively replaced each of the three semantic kernels section
with a randomized kernel by shufing its rows and columns
we observed an absolute decrease of up to
in score and
in meteor score for with bigger impacts in performance at higher length budgets
placing with a randomized kernel on other hand decreased the average coherence score by for dataset and for for summaries constructed at compression budget approximately figure score of mls and the ablative baselines and mls on datasets and i
e
half length of the input document


human evaluation of length controlled summaries we conducted a study to evaluate the completeness of the summaries constructed by mls
more ically we considered a scenario where the user needs to complete a fact checking task
we chose three documents from both datasets randomly and asked each participant to verify the presence of some key facts of the document in the summaries constructed by mls a baseline method
each participant was instructed to complete the task solely based on the content of the summary and not depending on any previous knowledge
for example the question does the story tell us why the narrator was red was paired with the following summary i tried to return a lost wallet to a customer who accused me of stealing it and then grabbed my hair
we got in a physical ght and i was red from my job
the participants had to chose between yes no and more information required
if a participant selected the third option a longer summary was shown with the same question
the task was terminated wise
in addition to mls the stronger extractive baseline in our experimental setup and we add two extreme settings the full content setting in which the original document was shown and the no content setting where no textual content other than the question itself was shown to a participant
the full content setting ensured that the question could indeed be answered from the article whereas the no content setting ensured whether the questions contained any hint about the answer
fc nc mls



index dataset















accuracy duration s accuracy duration s table mean accuracy and completion time using mls no content nc and full content fc settings the task started by showing each ipant a summary generated at compression budget
if they opted for more information to be shown we provided a summary generated by the same method by doubling the compression budget each time until the user responded with a yes or no or we reached the budget of
the key intuition here is that if users are given a complete and representative summary they should be able to answer the questions accurately as a good summarization model would pick up the key concepts of the document even at shorter length budgets without requiring for it to be expanded further
with this in mind we recorded task completion time and user response for each treatment
all budgeted summaries were constructed beforehand
we invited graduate students to participate in the study
each participant was shown summaries generated by at most two different methods in random order
no information on the method used was revealed to a participant at any stage
to prevent information retention each participant was shown a summary generated from the same document only once
using a balanced incomplete block design aschbacher each of the settings methods datasets was assigned to subjects
the average accuracy and task completion time recorded for each treatment is shown in table
the accuracy of the no content setting is for both datasets indicating that the questions did not contain any hint to the correct answer whereas the full content setting shows that overall the questions could have been answered from the original documents
when using summaries generated by mls the participants responded as accurately as the full content setting on dataset while being more than two times faster outperforming and
for dataset participants were more accurate using summaries constructed by mls than
mls performed better than on one document comparable on one and worse on one document with an average accuracy of

conclusion we have proposed mls a supervised approach to construct abstractive summaries at controllable lengths
following an extract then compress paradigm we develop the pointer magnier network a length aware encoder decoder network that constructs length constrained summaries by shortening or expanding a prototype summary inferred from the document
the key enabler of this network is an array of semantic kernels with clearly dened human interpretable syntactic semantic roles in constructing the summary given a budget length
we train our network on limited training samples from two cross domain datasets
experiments show that the summaries constructed by mls are coherent and reectively capture the main concepts of the document
our human evaluation study also suggest the same
in the future we would like to extend our work to construct task driven summaries for interactive question answering tasks
personalizing a summary based on user s past interaction model is another exciting direction of future work
acknowledgement we would like to thank professors micha elsner joel bloch marie catherine de marneffe and michael white for valuable discussions and comments
we would also like to thank the reviewers and folks who participated in our study for sharing critical feedback that helped improve our work
references michael aschbacher

on collineation groups of symmetric block designs
journal of combinatorial theory series a
joris baan maartje ter hoeve marlies van der wees anne schuth and maarten de rijke

do transformer attention heads provide transparency in abstractive summarization arxiv preprint

satanjeev banerjee and alon lavie

meteor an automatic metric for mt evaluation with improved correlation with man judgments
in proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation summarization
anders johannsen benjamin heinzerling

a python wrapper for the rouge summarization evaluation package
https
org project
david m blei andrew y ng and michael i jordan

latent dirichlet allocation
jmlr
john brockman

thinking machines
what do you think about machines that think

edge
org annual orkut buyukkokten hector garcia molina and andreas paepcke

seeing the whole in parts text summarization for web browsing on handheld devices
in the web conference
marco campana and anastasios tombros

incremental personalised summarisation with novelty detection
in fqas
yen chun chen and mohit bansal

fast abstractive summarization with reinforce selected sentence rewriting
arxiv

works
in icml


tion
jmlr
yann n dauphin angela fan michael auli and david grangier

language modeling with gated convolutional finale doshi velez and been kim

towards a rigorous science of interpretable machine learning
arxiv preprint john duchi elad hazan and yoram singer

adaptive subgradient methods for online learning and stochastic angela fan david grangier and michael auli

controllable abstractive summarization
arxiv

jonas gehring michael auli david grangier denis yarats and yann n dauphin

convolutional sequence to sequence learning
in icml
jmlr
org
karl moritz hermann tomas kocisky edward grefenstette lasse espeholt will kay mustafa suleyman and phil blunsom

teaching machines to read and comprehend
in advances in neural information processing systems pages
clayton j hutto and eric gilbert

vader a parsimonious rule based model for sentiment analysis of social media text
in icwsm
itgp

eu general data protection regulation gdpr
it governance limited
sarthak jain and byron c wallace

attention is not explanation
in proceedings of the conference of the north american chapter of the association for computational linguistics human language technologies volume long and short papers pages
moniba keymanesh micha elsner and srinivasan parthasarathy

toward domain guided controllable summarization of privacy policies
natural legal language processing workshop at kdd
yuta kikuchi graham neubig ryohei sasano hiroya takamura and manabu okumura

controlling output length in neural encoder decoders
arxiv

quoc le and tomas mikolov

distributed representations of sentences and documents
in icml
chin yew lin

rouge a package for automatic evaluation of summaries
text summarization branches out
fei liu jeffrey flanigan sam thomson norman sadeh and noah a smith

toward abstractive summarization using semantic representations
arxiv

yizhu liu zhiyi luo and kenny zhu

controlling length in abstractive summarization using a convolutional neural chunyi liu peng wang jiang xu zang li and jieping ye

automatic dialogue summary generation for customer network
in emnlp
service
in sigkdd
ahmed magooda and diane litman

abstractive summarization for low resource data using domain transfer and data synthesis
arxiv preprint

rada mihalcea and paul tarau

textrank bringing order into text
in emnlp
milad moradi and nasser ghadiri

different approaches for identifying important concepts in probabilistic biomedical text summarization
articial intelligence in medicine
ramesh nallapati bowen zhou caglar gulcehre bing xiang al

abstractive text summarization using sequence sequence rnns and beyond
arxiv preprint

jahna otterbacher dragomir radev and omer kareem

news to go hierarchical text summarization for mobile devices
in sigir
corpora
in eacl
jessica ouyang serina chang and kathy mckeown

crowd sourced iterative annotation for narrative summarization sinno jialin pan and qiang yang

a survey on transfer learning
tkde
shantipriya parida and petr motlicek

abstract text summarization a low resource challenge
in emnlp
association for computational linguistics november
romain paulus caiming xiong and richard socher

a deep reinforced model for abstractive summarization
in international conference on learning representations
stuart rose dave engel nick cramer and wendy cowley

automatic keyword extraction from individual documents
text mining applications and theory
alexander m rush sumit chopra and jason weston

a neural attention model for abstractive sentence summarization
arxiv

yuji matsumoto


itsumi saito kyosuke nishida kosuke nishida atsushi otsuka hisako asano junji tomita hiroyuki shindo and arxiv
length controllable abstractive summarization by guiding with summary prototype
ritesh sarkhel and arnab nandi

visual segmentation for information extraction from heterogeneous visually rich documents
in proceedings of the international conference on management of data pages
ritesh sarkhel jacob j socha austin mount campbell susan moffatt bruce simon fernandez kashvi patel arnab nandi and emily s patterson

how nurses identify hospitalized patients on their personal notes findings from analyzing brains headers with multiple raters
in proceedings of the international symposium on human factors and ergonomics in health care volume pages
sage publications sage ca los angeles ca
abigail see peter j liu and christopher d manning

get to the point summarization with pointer generator networks
arxiv

kaiqiang song logan lebanoff qipeng guo xipeng qiu xiangyang xue chen li dong yu and fei liu

joint parsing and generation for abstractive summarization
in proceedings of the aaai conference on articial intelligence
kaiqiang song bingqing wang zhe feng liu ren and fei liu

controlling the amount of verbatim copying in abstractive summarization
in proceedings of the aaai conference on articial intelligence
balaji vasan srinivasan pranav maneriker kundan krishna and natwar modani

corpus based content construction
in coling
arxiv preprint

emma strubell ananya ganesh and andrew mccallum

energy and policy considerations for deep learning in nlp
ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan n gomez ukasz kaiser and illia sukhin

attention is all you need
in nips
christopher c yang and fu lee wang

fractal summarization for mobile devices to access large documents on the web
in the web conference

