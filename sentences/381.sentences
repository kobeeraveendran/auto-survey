n a j v c
s c v
v i x r a how good is a video summary a new benchmarking dataset and evaluation framework towards realistic video summarization vishal suraj anshul rishabh and ganesh of computer science indian institute of technology bombay of computer science university of texas at dallas abstract automatic video summarization has attracted a lot of interest
however it is still an unsolved problem due to several challenges
we take steps towards making automatic video tion more realistic by addressing the following challenges
the currently available datasets either have very short videos or have few long videos of only a particular type
we introduce a new benchmarking video dataset called visiocity video summarization based on continuity intent and diversity which comprises of longer videos across six dierent categories with dense concept annotations capable of supporting dierent avors of video summarization and other vision problems
secondly for long videos human reference summaries necessary for supervised video summarization techniques are dicult to obtain
we explore strategies to automatically generate multiple reference summaries from indirect ground truth present in visiocity
we show that these summaries are at par with human summaries
we also present a study of ent desired characteristics of a good summary and demonstrate how especially in long videos it is quite possible and frequent to have two good summaries with dierent characteristics
thus we argue that evaluating a summary against one or more human summaries and using a single measure has its shortcomings
we propose an evaluation framework for better quantitative assessment of summary quality which is closer to human judgment
lastly we present insights into how a model can be enhanced to yield better summaries
sepcically when multiple diverse ground truth summaries can exist learning from them individually and using a combination of loss functions measuring dierent characteristics is better than learning from a single combined oracle ground truth summary using a single loss function
we demonstrate the eectiveness of doing so as compared to some of the representative state of the art techniques tested on visiocity
we release visiocity as a benchmarking dataset and invite researchers to test the eectiveness of their video summarization algorithms on visiocity
introduction and motivation the unprecedented rise in the amount of video data has also made it dicult to consume them
this has given rise to the need for automatic video summarization techniques which aim at producing much shorter videos without signicantly compromising on the key information contained in them
consequently there has been a lot of work pushing the state of the art for newer algorithms and model architectures and datasets
however the literature also talks of a few fundamental challenges that need to be addressed before we have a more realistic video summarization that works in practice
in this work we take steps towards addressing the following challenges dataset for a true comparison between dierent techniques a benchmark dataset is critical
almost all recent techniques have reported their results on tvsum and summe which have emerged as benchmarking datasets of sorts
however since the average video length in these datasets is of the order of only minutes they are far from being eective in real world settings characterized by long videos
while there have been several attempts at creating datasets for video summarization they either a have very short videos or they have very few long videos and often of only a particular type
a large dataset with a lot of dierent types of full length videos with rich annotations to be able to support dierent techniques was one of the recommendations in is still not a reality and is clearly a need of the hour
we introduce visiocity to address this need
visiocity is a diverse collection of long videos spanning across six dierent categories with dense concept annotations
furthermore dierent avors of video summarization for example query focused video summarization are often treated dierently and on dierent datasets
with its rich annotations visiocity can lend itself well to other avors of video summarization and also other computer vision video analysis tasks like captioning or action recognition
also since the videos span across dierent well dened domains visiocity is suitable for more in depth domain specic studies on video summarization
reference summaries for supervised learning supervised techniques tend to work better than unsupervised techniques because of learning directly from human summaries
video summaries are highly context dependent depends on the purpose behind getting a video summary subjective even for the same purpose preferences of two persons do nt match and depends on high level semantics of the video two visually dierent scenes could capture the same semantics or visually similar looking scenes could capture dierent semantics
as an example of context one may want to summarize a surveillance video either to see a gist of what all happened or to quickly spot any abnormal activity
as an example of personal preferences or subjectivity while summarizing a friends video a popular tv series two users may have dierent opinion on what is important or interesting
similar example for higher level semantics is that closeup of a player in soccer can be considered important if it is immediately followed by a goal while not so important when it occurs elsewhere even though both look visually the same
thus there is no single right answer and two human summaries could be quite dierent in their selections
in a race to achieve better performance most state of the art techniques are based on deep architectures and are thus data hungry
the larger the dataset and more the number of reference summaries to learn from the better
unfortunately for long videos getting human summaries is very time consuming
it becomes increasingly expensive and beyond a point infeasible to get these reference summaries from humans
also this is not scalable to experiments where reference summaries of dierent lengths are desired
to alleviate this problem in this work we explore strategies to automatically generate ground truth reference summaries which can be used to train a model
further most supervised learning approaches are trained using a combined ground truth mary either in form of combined scores from multiple ground truth summaries or scores or in form a set of ground truth selections as in dpplstm
however combining them into one misses out on the separate avors captured by each of them
combining many into one set of scores also runs the risk of giving more emphasis to importance over and above other desirable characteristics of a summary like continuity diversity
this is also noted by where they argue that supervised learning approaches which rely on the use of a combined truth summary can not fully explore the learning potential of such architectures
the necessity to deal with dierent kind of summaries in dierent ways was also observed by
use this argument to advocate the use of unsupervised approaches
we leverage visiocity to demonstrate that better results can be achieved when a supervised model learns from individual ground truth summaries using multiple loss functions each measuring deviation from dierent desired characteristics of summaries
evaluation a video summary is typically evaluated by comparing it against human summaries for example using score dened as harmonic mean of precision ratio of temporal overlap between candidate and reference summary to duration of summary and recall ratio of temporal overlap between candidate and reference summary to video duration
to accommodate multiple human summaries either average or max is reported
however a good candidate may get a low score just because it was not fortunate to have a matching human summary a likely scenario in case of long videos
furthermore score has some limitations
due to the segmentation used as a post processing step in typical video summarization pipeline even random summaries can get good scores
also is not designed to measure aspects like continuity and diversity of a summary
two summaries may have same score and yet one may be more continuous and hence visually more pleasurable to watch than another
we propose an evaluation framework where a summary is assessed on its own merit using the rich annotations in visiocity as against comparing it against available human summaries using a suite of measures to capture various aspects of a summary like continuity diversity redundancy importance
as against over dependence on one measure
related work
datasets currently available datasets for video summarization either have very short videos or have few long videos of only a particular type
table compares visiocity with other existing datasets for video summarization
med summaries dataset consists of annotated videos of length minutes with event categories like birthday wedding feeding
the annotation comprises of segments and their importance scores
tvsum consists of videos average length minutes from categories with importance scores provided by annotators for each second snippet
the videos correspond to very short events like changing vehicle tires making sandwich
though number of categories in tvsum and medsummaries appear to be large the notion of categories there is of events like making a sandwich or attempting bike tricks quite dierent from dierent domains in visiocity with an intent of studying the characteristics of summaries of dierent types of videos like sports or tv shows
the ut egocentric dataset consists of long and annotated videos captured from head mounted cameras
however though each video is very long there are only videos and they are of one type i
e
egocentric
summe consists of videos with an average length of about min
the annotation is in form of user summaries of length between to
each video has summaries
the vsumm dataset consists of two datasets
youtube consists of videos min and ovp consists of videos of about min from the open video project
each video has user summaries in the form of set of key frames
consists of videos with a total duration of hours and is designed primarily for multi video summarization
it is a collection of videos of a tourist place
the average duration of each video is about mins
tv episodes dataset consists of tv show videos each of mins
the total duration is hours
a recent dataset oers videos with user generated summaries each
lol consists of online esports videos from the league of legends
it consists of videos with each video being between mins
the associated summary videos are mins long
while this dataset is signicantly larger compared to the other datasets it is limited only to a single domain i
e
esports
have extended the ute dataset to videos and have provided concept annotations but they are limited to only egocentric videos and do not support any concept hierarchy
the scores annotations as in tvsum
are richer annotations but are limited only to importance scores
visiocity on the other hand comes with dense concept annotations for each snippet
to the best of our knowledge visiocity is one of its kind large dataset with many long videos spanning across multiple categories and annotated with rich concept annotations for each snippet
total duration cat name summe tvsum med summaries ut egocentric youtube youtube tv episodes lol visiocity ours videos duration of videos avg min secs avg min sec dur min avg
min avg min sec dur min avg min sec dur min avg min sec avg min avg min dur min dur mins avg mins
hours
hours hours
hours
hours
hours hours hours hours table comparison of visiocity with other datasets in literature
means the corresponding information was not available
dur stands for duration and cat is of event categories available in a dataset
techniques for automatic video summarization a number of techniques have been proposed to further the state of the art in automatic video summarization
most video summarization algorithms try to optimize several criteria such as diversity coverage importance and representation
some techniques do this through lar functions some use lstms some use determinantal point processes dpps some use reinforcement learning and some use attention attempts to address video summarization via attention aware and adversarial training
els

evaluation evaluation of video summaries is challenging task owing to the multiple denitions of success
early approaches involved user studies but with the obvious demerit of cost and reproducibility
with a move to automatic evaluation every new technique of video summarization came with its own evaluation criteria making it dicult to compare results dierent techniques
some of the early approaches included viper which addresses the problem by dening a specic ground truth format which makes it easy to evaluate a candidate summary and superseiv which is an unsupervised technique to evaluate video summarization algorithms that perform frame ranking
vert on the other hand was inspired by bleu in machine translation and rouge in text summarization
other techniques include pixel level distance between keyframes objects of interest as an indicator of similarity and precision recall scores over key frames selected by human annotators
it is not surprising thus that observed that researchers should at least reach a consensus on what are the best procedures and metrics for evaluating video abstracts
they concluded that a detailed research that focuses exclusively on the evaluation of existing techniques would also be a valuable addition to the eld
this is one of the aims of this work
more recently computing overlap between reference and generated summaries has become the standard framework for video summary evaluation
however all these methods which require comparison with ground truth summaries suer from the challenges highlghted earlier
yeung et al
observed that visual need not mean semantic and hence proposed a text based approach of evaluation called videoset
the candidate summary is converted to text and its similarity is computed with a ground truth textual summary
that text is better equipped at capturing higher level semantics has been acknowledged in the literature and form the motivation behind our proposed evaluation measures
however our measures are dierent in the sense that a summary is not converted to text domain before evaluating
rather how important its selections are or how diverse its selections are is computed from the rich textual annotations in visiocity
this is similar in spirit to but there it has been done only for egocentric videos
as noted by the limited number of evaluation videos and annotations further magnify this ambiguity problem
our visiocity framework precisely hits the nail by not only oering a larger dataset but also in proposing a richer evaluation framework better equipped at dealing with this ambiguity
visiocity dataset
videos visiocity is a diverse collection of videos spanning across six dierent categories tv shows friends sports soccer surveillance education tech talks birthday videos and wedding videos
the videos have an average duration of about mins
summary statistics for visiocity are presented in table
publicly available soccer friends techtalk birthday and wedding videos were downloaded from internet
tv shows contains videos from a popular tv series friends
they are typically more aesthetic in nature and professionally shot and edited
in sports category visiocity contains soccer videos
these videos typically have well dened events of interest like goals or penalty kicks and are very similar to each other in terms of the visual features
under surveillance category visiocity covers diverse settings like indoor outdoor classroom oce and lobby
the videos were recorded using our own surveillance cameras
these videos are in general very long and are mostly from static continuously recording cameras
under educational category visiocity has tech talk videos with static views or inset views or dynamic views
in personal videos category visiocity has birthdays and wedding videos
these videos are typically long and unedited
the videos are available to see and download from the project website at
github
domain videos duration total duration sports soccer tvshows friends surveillance educational personal videos birthday personal videos wedding all
hours
hours
hours
hours hours
hours
table key statistics of visiocity
third column is in minutes min max avg
annotations the ground truth in visiocity is not direct in form of the user summaries but indirect in form of concepts marked for each snippet
being at a higher level indirect ground truth can be seen as a generator of ground truth summaries and thus allows for multiple solutions reference summaries of dierent lengths with dierent desired characteristics and is easy to scale
it also makes the annotation process more objective and easier than asking the users to directly produce reference ground truth summaries
concepts are carefully selected list of verbs and nouns based on the type of the video and are given importance ratings based on the knowledge of the particular domain
the concepts are organized in categories instead of a long at list
example categories include actor entity action scene number of people
categories provide a natural structuring to make the annotation process easier and also support for at least one level hierarchy of concepts for driven summarization
in addition to concepts we ask annotators to group those consecutive snippets as mega events which together constitute a cohesive event
for example a few snippets preceeding a goal in a soccer video the goal snippet and a few snippets after the goal snippet together would constitute a mega event
a model trained to learn importance scores only would do well to pick up the goal snippet
however such a summary will not be very pleasing to watch because what is required in a summary in this case is not just the ball entering the goal post but the build up to this event and probably a few snippets as a followup
thus this notion of mega events helps us to model the notion of continuity
textual annotations vs ratings or scores as indirect ground truth while past work has made use of other forms of indirect ground truth like asking annotators to give a score or a rating to each shot using textual concept annotations oers several advantages
first cially for long videos it is easier and more accurate for annotators to mark all keywords applicable to a shot snippet than for them to tax their brain and give a rating especially when it is quite subjective and requires going back and forth over the video for considering what is more important or less important
second when annotators are asked to provide ratings they often suer from chronological bias
one work addresses this by showing the snippets to the annotators in random order but it does nt work for long videos because an annotator can not remember all of these to be able to decide the relative importance of each
third the semantic content of a snippet is better captured through text
this is relevant from an importance perspective as well as diversity perspective
as noted earlier two snippets may look visually dierent but could be semantically same and vice versa
text captures the right level of semantics desired by video summarization
also when two snippets have same rating it is not clear if they are semantically same or they are semantically dierent but equally important
textual annotations brings out such similarities and dissimilarities more eectively
fourth as already noted textual annotations make it easy to adapt visiocity to a wide variety of problems
annotation protocol a group of professional annotators were tasked to annotate videos without listening to the audio by marking all applicable keywords on a snippet shot through a python gui application developed by us for this task
it allows an annotator to go over the video unit by unit shot snippet and select the applicable keywords using a simple and intuitive gui figure
it provides convenience features like copying the annotation from previous snippet which comes in handy where there are are a lot of consecutive identical snippets for example in surveillance videos
figure annotation and visualization tool developed by us used in visiocity framework special caution was exercised to ensure high quality annotations
specically the guidelines and protocols were made as objective as possible the annotators were trained through sample annotation tasks and the annotation round was followed by two verication rounds where both precision how accurate the annotations were and recall whether all events of interest and continuity information has been captured in the annotations were veried by another set of tators
whatever inconsistencies or inaccuracies were found and could be automatically detected were included in our automatic sanity checks which were run on all annotations
proposed evaluation framework literature talks about certain desirable good characteristics of a video summary
for example a good video summary is supposed to be diverse non redundant continuous or visually pleasing without abrupt shot transitions representative of the original video and contain important or interesting snippets from the video
in what follows we dive deeper into these teristics and propose measures to assess the candidate summaries on those characteristics
diversity a summary which does good on diversity is non redundant
it contains segments quite dierent from one another
dierent could mean dierent things in terms of content alone i
e
one does nt want two similar looking snippets in a summary or in terms of content and time i
e
one does nt want visually similar consecutive snippets but does want visually similar snippets that are separated in time or in terms of the concepts covered one does not want too many snippets covering the same concept and would rather want a few of all concepts
in surveillance videos for example one would like to have a summary which does nt have too many visually similar consecutive and hence redundant snippets but does have visually similar snippets that are separated in time
for instance consider a video showing a person entering her oce at three dierent times of the day
though all three look similar and will have identical concept annotations as well all are desired in the summary
with regards to the quantitative formulation we dene the rst avor of diversity as where x is a subset of snippets
dij is iou measure between snippets i and j based on their concept vectors
for the other two avors of diversity we dene diversity clustered max min i jx dij max jxci rj where c are the clusters which can be dened over time divtime all consecutive similar snippets form a cluster or concepts divconcept all snippets covering a concept belong to a cluster and rj is the importance rating of a snippet j
when optimized this function leads to the selection of the best snippet from each cluster
this can be easily extended to select a nite number of snippets from each cluster instead of the best one
megaeventcontinuity element of continuity makes a summary pleasurable to watch
since only a small number of snippets are to be included in a summary some discontinuity in the summary is expected
however the less the discontinuity at a semantic level more pleasing is the summary to watch
there is a thin line between modelling redundancy and continuity when it comes to visual cues of frames
some snippets might be redundant but are important to include in the summary from a continuity perspective
to model the continuity visiocity has the notion of mega events as dened earlier
to ensure no redundancy within a mega event the mega event annotations are as tight as possible meaning they contain bare minimum snippets just enough to indicate the event
a non mega event snippet is continuous enough to exist in the summary on its own and a mega event snippet needs other adjacent snippets to be included in the summary for semantic continuity
we measure mega event continuity as follows m e where e is the number of mega events in the video annotation is the rating of the mega event mi and is equal to maxsmi a is the annotation of video v that is a set of snippets such that each snippet s has a set of keywords k s and information about mega event m is a set of all mega events such that each mega event mi i e is a set of snippets that constitute the mega event mi importance interestingness this is the most obvious characteristic of a good summary
for some domains like sports there is a distinct importance of some snippets over other snippets for eg
score changing events
this however is not applicable for some other domains like tech talks where there are few or no distinctly important events
with respect to the annotations available in visiocity importance of a shot or snippet is dened by the ratings of the keywords of a snippet
these ratings come from a mapping function which maps keywords to ratings for a domain
the ratings are dened from to with rated keyword being the most important and indicated an undesirable snippet
we assign ratings to keywords based on their importance to the domain and average frequency of occurence
given the ratings of each keyword rating of a snippet is dened as otherwise
here k s is the set of keywords of a snippet s rs if i and rks i
thus importance function can be dened as and rs maxi rks is the rating of a particular keyword k s i i i note that when both importance and mega event continuity is measured we dene the importance only on the snippets which are non mega events since the mega event continuity term above already takes care of the importance of the mega event snippets
as discussed earlier since there are mutliple right answers with varying characteristics we hypothesize that these are orthogonal characteristics and vary across dierent human summaries
for example one human good summary could contain more important but less diverse segments while another human good summary could contain more diverse and less important segments depending on the intent behind summarization or user subjectivity
also in assessing summaries one measure could be more relevant than another depending on the type of the video
for example in sports videos because of well dened events of interest importance is more relevant in evaluating a summary
we verify our hypotheses experimentally
motivated by this we propose using a suite of measures as dened above instead of overly depending on any one of them
the measures are computed using the annotations present in visiocity
we summarize them in table
we propose that a true and wholesome assessment of a candidate summary can only be done when this suite of measures including the existing measures like f score are used
results and observations from our extensive experiments corroborate this fact
measure diversitysim concept mega event continuity importance expression max mini jx dij maxjxci rj table some of the proposed measures in visiocity
x is the candidate summary cis are clusters of consecutive similar snippets or concepts denotes rating and m denotes mega events
ground truth summaries for supervised learning in practice it is dicult to acquire many human summaries with diverse characteristics especially for long videos
we explore strategies to automatically generate the reference ground truth summaries of desired lengths using the annotations present in visiocity
specifcially we use the above posed assessment measures as scoring functions and maximize them to get the desired ground truth summaries
we note that maximizing a particular scoring function would yield a summary rich in that particular characteristic but it may fall short on other characteristics
for example a mary maximizing importance will capture the goals in a soccer video but some snippets preceeding the goal and following the goal will not be in the summary and the summary will not be visually pleasing example illustration at
github

hence a weighted mixture of such measures need to be maximized to arrive at optimal yet diverse reference summaries
this ite scoring function weighted mixture takes an annotated video keywords and mega events dened over snippets shots and generates a set of candidate ground truth summaries which supervised or semi supervised summarization algorithms can use
mathematically given x a set of snippets of a video v let be dened as m this scoring function is parameterized on and is approximately optimized via a greedy rithm to arrive at the ground truth summaries
dierent conguration of generates dierent summaries
we explore two dierent strategies of identifying the right s for producing desired diverse reference summaries pareto optimality which is based on brute force search and proportional fairness for which there is a known ecient greedy algorithm with a provable imation guarantee for fairness
pareto optimality pareto optimality is a situation that can not be modied so as to make any one individual or preference criterion better o without making at least one individual or preference criterion worse o
beginning with a random element a possible conguration of the lambdas in the pareto optimal set we iterate over remaining elements to decide whether a new element should be added or old should be removed or new element should be discarded
this is decided on the basis of the performance on various measures
a conguration is better than another when it is better on all measures otherwise it is not
proportional fairness consider each conguration as an allocation to some agents the ent scoring terms such that an allocation yields dierent performance on dierent measures the value seen by the agents
specically for a performance measure p an tion conguration i yields value
this setting allows us to use the notion of fair public decision making applying nash social welfare equation where the best allocation is one which is proportionally fair to all agents
we borrow from the approach in which studies a fairness erty called core that generalizes the notion of proportional fairness and pareto optimality
here the problem is reduced to maximizing the equation f for every congurations in the conguration space
thus computing the top t congurations will take time n k
we verify experimentally that the automatic ground truth summaries so generated are at par with the human summaries both qualitatively and quantitatively
we use them in training the models tested on visiocity
towards a new state of the art following we formulate the problem of automatic video summarization as a subset selection problem where a weighted mixture of set functions is maximized to produce an optimal summary for a desired budget
specically given a video v as a set of snippets yv the problem reduces to picking y yv which maximizes our objective such that k k being the budget
y is the predicted summary xv the feature representation of the video snippets and y is the weighted mixture of components
y argmax y y xv y our mixture model comprises of a submodular facility location term and modular importance terms
the facility location function is dened as ff vv maxxx where v is an element from the ground set v and measures the similarity between element v and element
facility location thus models representativeness
the importance scores are taken from the vasnet model and the vslstm model trained on visioicty
the weights of the model are learnt ing the large margin framework as described in using many automatic ground truth summaries and a margin loss which combines the feedback from dierent evaluation measures
specically given n pairs of a video and an automatic reference summary v ygt we learn the weight vector w by optimizing the following large margin formulation min n n where is the generalized hinge loss of training example n and w is the weight vector
max yy n v wt xn v wt xn v gt this objective is chosen so that each ground truth summary scores higher than any other summary by some margin
for training example n the margin we chose is denoted by and is a linear combination of the normalized losses reported by our proposed measures
we call our proposed method visiocity sum
we show that a simple model like this out performs the current niques state of the art on tvsum and summe on visiocity dataset because of learning from multiple ground truth summaries and learning from mutliple loss functions each capturing dierent characteristics of a summary
experiments and results
implementation details for analysis of and comparison with human summaries we generated automatic summaries per video of about the same length as the human summaries
score of any candidate summary is computed with respect to the human ground truth summaries following
we report both avg and max
to calculate scores of human summaries with respect to human summary we compute max and avg in a leave one out fashion
for analysis of and comparison of dierent techniques on the visiocity dataset we report their scores computed against the automatically generated summaries as a proxy for human summaries
we generate automatic summaries for each video
all target summaries are generated such that their lengths are to of the video length
we test the performance of three dierent representative state of the art techniques on the visiocity benchmark vslstm is a supervised technique that uses bilstm to learn the variable length context in predicting important scores
it learns from a combined ground truth in terms of aggregated scores
vasnet is a supervised technique based on a simple attention based network without computationally intensive lstms and it learns from a combined ground truth in terms of aggregated scores and outputs a bilstms
predicted score for each frame in the video
dr dsn is an unsupervised deep reinforcement learning based model which learns from a combined diversity and representativeness reward on scores predicted by a bilstm decoder
it outputs predicted score for every frame of a video
to generate a candidate machine generated summary from the importance scores predicted by vslstm vasnet and dr dsn we follow to convert them into machine generated summary of desired length max of original video
our proposed model visiocity sum learns from multiple ground truth summaries and outputs a machine generated summary as a subset of snippets
in all tables refers to avg score refers to max score nearest neighbor score imp mc dt dc and dsi refer to the importance score mega event continuity score time score diversity concept score and diversity similarity score respectively as calculated by the proposed measures
all gures are in percentages
figure dierent human summaries of same video perform dierently on dierent measures
dierent human summaries have dierent characteristics we asked a set of users dierent from the annotators to create human summaries for two randomly sampled videos of each domain
the users were asked to look at the video without the audio and mark segments they feel should be included in the summary such that the length of the summary remains between to of the original video
the procedure followed was similar to that of summe
we assess these human summaries qualitatively and quantitatively using the proposed set of performance measures and make the following observations
the human summaries are consistent with each other in as much as there are important scenes in the video for example goals in soccer videos
in the absence of such clear interesting events the human summaries exhibit more inconsistency with each other
a representative plot for the scores of human summaries of friends video is presented in figure
we note the following proposed measures get good values on the human summaries as compared to uniform and random summaries thus ascertaining their utility b a human summary could score low on one measure and high on another measure the desired characteristics dier slightly across dierent domains for example importance seems to be more important for soccer videos than diversity
automatically generated reference summaries are at par with human summaries in our experiments we search for fair congurations using both pareto optimality and proportional fairness using the ecient greedy algorithm
table shows the average scores of both kinds of automatic ground truth summaries as compared to human summaries uniform summaries and random summaries
we see that both approaches yield comparable performance
we use the automatic summaries generated using pareto optimality in the rest of our experiments
we compare automatically generated reference summaries with human summaries on our posed measures and present the quantitative results in table
we see that automatic and human summaries are much better than random on all the evaluation criteria
next we see that both the human and the automatic summaries are close to each other in terms of the metric
the automatic summaries have the highest importance continuity and diversity scores
this is not surprising as they are obtained at the rst place by optimizing a combination of these criteria
ure shows a representative plot for min mean max of dierent measures for dierent summaries domain human uniform random auto pareto auto prop fri soc we d sur tec bir table performance of human and auto summaries on videos across all the domains
pareto and auto prop stand for automatic summaries generated using pareto optimality and tional fairness respectively
figure behavior of dierent measures for dierent types of summaries for soccer videos
cont summaries are visually continuous summaries assembled by picking a set of continuous snippets of soccer videos
we also compare the human and automatic summaries qualitatively
as an example figure compares the selections by human summaries left and automatic ground truth summaries right for friends video
we see a considerable similarity in selections though a perfect match of selections is neither possible nor expected keeping with the spirit of multiple correct answers
some human mary videos and automatic ground truth summary videos are reported at
github

we see that it is very hard to distinguish the automatic summaries from human summaries and they form very good visual summaries in themselves

visiocity benchmark performance of dierent models on siocity we test some state of the art models and our simple enhancement visiocity sum on visiocity and report the numbers in table
we make the following observations dr dsn tries to generate a summary which is diverse
as we can see in the results it almost always gets high score on the diversity term
please note that the way we have dened these diversity measures diversity concept dc and diversity time domain soccer friends surveillance techtalk birthday wedding technique human uniform random auto human uniform random auto human uniform random auto human uniform random auto human uniform random auto human uniform random auto imp mc dt dc dsi table performance of human and auto summaries for dierent domains
techtalk videos do not have megaevents
dt have an element of importance in them also
on the other hand diversity sim dsi is a pure diversity term where dr dsn almost always excels
due to this nature of dr dsn when it comes to videos where the interestingness stands out and importance clearly plays a more important role dr dsn does nt perform well
in such scenarios vslstm is seen to perform better closely followed by vasnet
it is also interesting to note that while two techniques may yield similar scores on one measure for example vslstm and vasnet for soccer videos table one of them in this case vslstm does better on mega event continuity and produces a desirable characteristic in the summary
this further strengthens our claim of having a set of measures evaluating a technique or a summary rather than over dependence on one which may not fully capture all desirable characteristics of good summaries
we also note that even though dr dsn is an unsupervised technique it is a state of the art technique when tested on tiny datasets like tvsum or summe but when it comes to a large dataset like visiocity with more challenging videos it does nt do well especially on those domains where there are clearly identiable important events for example in soccer goal save penalty
and birthday videos cake cutting

in such cases models like vslstm and vasnet perform better as they are geared towards learning importance
in contrast since the interstingness level in videos like surveillance and friends is more spread out dr dsn does relatively well even without any supervision
visiocity sum does better than all techniques on account of learning from individual ground truth summaries and a figure shot numbers selected by some human summaries left and by some automatic ground truth summaries right for the friends video combination of loss functions
conclusion in order to improve the objectivity and consistency in the design of video summarization benchmark datasets as well as their use in evaluating video summarization models we present visiocity a large benchmarking dataset and demonstrated its eectiveness in real world setting
to the best of our knowledge it is the rst of its kind in the scale diversity and rich concept annotations
we introduce a recipe to automatically create ground truth summaries typically needed by the supervised techniques
motivated by the fact that dierent good summaries have dierent teristics and are not necessarily better or worse than the other we propose an evaluation framework better geared at modeling human judgment through a suite of measures than having to overly depend on one measure
finally we report the strengths and weaknesses of some representative state of the art techniques when tested on this new benchmark and demonstrate the eectiveness of our simple extension to a mixture model making use of individual ground truth summaries and a combination of loss functions
we hope our attempt to address the multiple issues currently rounding video summarization as highlighted in this work will help the community advance the state of the art in video summarization
we make visiocity available through the project page at
github
and invite the researchers to test their algorithms on visiocity benchmark
acknowledgements this work is supported in part by the ekal fellowship www
ekal
org and national center of lence in technology for internal security iit bombay ncetis
iitb
ac
in domain soccer friends surveillance techtalk birthday wedding technique
auto
dr dsn
vasnet
vslstm
ours
random
auto
dr dsn
vasnet
vslstm
ours
random
auto
dr dsn
vasnet
vslstm
ours random
auto
dr dsn
vasnet
vslstm
ours
random
auto
dr dsn
vasnet
vslstm
ours
random
auto
dr dsn
vasnet
vslstm
ours random































imp mc




























































dt




























dc




























dsi

































table comparison of dierent techniques on visiocity
techtalk videos do not have megaevents references e
apostolidis e
adamantidou a
i
metsai v
mezaris and i
patras
unsupervised video summarization via attention driven adversarial learning
in international conference on timedia modeling pages
springer
s
e
f
de avila a
p
b
lopes a
da luz jr and a
de albuquerque araujo
vsumm a mechanism designed to produce static video summaries and a novel evaluation method
pattern recognition letters
d
doermann and d
mihalcik
tools and techniques for video performance evaluation
in icpr page
ieee


b
fain k
munagala and n
shah
fair allocation of indivisible public goods
corr j
fajtl h
s
sokeh v
argyriou d
monekosso and p
remagnino
summarizing videos with attention
in asian conference on computer vision pages
springer
c

fu j
lee m
bansal and a
c
berg
video highlight prediction using audience chat reactions
arxiv preprint

t

fu s

tai and h

chen
attentive and adversarial learning for video summarization
in ieee winter conference on applications of computer vision wacv pages
ieee
b
gong w

chao k
grauman and f
sha
diverse sequential subset selection for pervised video summarization
in advances in neural information processing systems pages
m
gygli h
grabner h
riemenschneider and l
van gool
creating summaries from user videos
in european conference on computer vision pages
springer
m
gygli h
grabner h
riemenschneider and l
van gool
creating summaries from user videos
in eccv
m
gygli h
grabner and l
van gool
video summarization by learning submodular tures of objectives
in proceedings of the ieee conference on computer vision and pattern recognition pages
m
huang a
b
mahajan and d
f
dementhon
automatic performance evaluation for video summarization
technical report maryland univ college park inst for advanced computer studies
z
ji k
xiong y
pang and x
li
video summarization with attention based encoder decoder networks
ieee transactions on circuits and systems for video technology
s
kannappan y
liu and b
tiddeman
human consistency evaluation of static video maries
multimedia tools and applications
v
kaushal r
iyer k
doctor a
sahoo p
dubal s
kothawade r
mahadev k
dargan and g
ramakrishnan
demystifying multi faceted video summarization tradeo between diversity representation coverage and importance
in ieee winter conference on applications of computer vision wacv pages
ieee
v
kaushal s
subramanian s
kothawade r
iyer and g
ramakrishnan
a framework towards domain specic video summarization
in ieee winter conference on applications of computer vision wacv pages
ieee
a
khosla r
hamid c

lin and n
sundaresan
large scale video summarization using in proceedings of the ieee conference on computer vision and pattern web image priors
recognition pages
a
kulesza b
taskar et al
determinantal point processes for machine learning
foundations and trends in machine learning
s
lan r
panda q
zhu and a
k
roy chowdhury
ffnet video fast forwarding via forcement learning
in proceedings of the ieee conference on computer vision and pattern recognition pages
y
j
lee j
ghosh and k
grauman
discovering important people and objects for tric video summarization
in computer vision and pattern recognition cvpr ieee conference on pages
ieee
z
lei c
zhang q
zhang and g
qiu
framerank a text processing approach to video summarization
arxiv preprint

y
li and b
merialdo
vert automatic evaluation of video summaries
in proceedings of the acm international conference on multimedia pages
acm
y
li l
wang t
yang and b
gong
how local is the local diversity reinforcing sequential determinantal point processes with dynamic ground sets for supervised video summarization
in proceedings of the european conference on computer vision eccv pages
z
lu and k
grauman
story driven summarization for egocentric video
in proceedings of the ieee conference on computer vision and pattern recognition pages
y

ma l
lu h

zhang and m
li
a user attention model for video summarization
in proceedings of the tenth acm international conference on multimedia pages
acm
m
minoux
accelerated greedy algorithms for maximizing submodular set functions
in mization techniques pages
springer
m
otani y
nakashima e
rahtu and j
heikkila
rethinking the evaluation of video maries
in proceedings of the ieee conference on computer vision and pattern recognition pages
r
panda n
c
mithun and a
k
roy chowdhury
diversity aware multi video tion
ieee transactions on image processing
b
a
plummer m
brown and s
lazebnik
enhancing video summarization via language embedding
in computer vision and pattern recognition volume
d
potapov m
douze z
harchaoui and c
schmid
category specic video summarization
in european conference on computer vision pages
springer
a
sharghi a
borji c
li t
yang and b
gong
improving sequential determinantal point processes for supervised video summarization
in proceedings of the european conference on computer vision eccv pages
a
sharghi b
gong and m
shah
query focused extractive video summarization
in european conference on computer vision pages
springer
a
sharghi j
s
laurel and b
gong
query focused video summarization dataset evaluation and a memory network based approach
in the ieee conference on computer vision and pattern recognition cvpr pages
y
song j
vallmitjana a
stent and a
jaimes
tvsum summarizing web videos using titles
in proceedings of the ieee conference on computer vision and pattern recognition pages
y
song j
vallmitjana a
stent and a
jaimes
tvsum summarizing web videos using titles
in cvpr pages
ieee computer society
b
taskar v
chatalbashev d
koller and c
guestrin
learning structured prediction models in proceedings of the international conference on machine a large margin approach
learning pages
acm
b
t
truong and s
venkatesh
video abstraction a systematic review and classication
acm transactions on multimedia computing communications and applications tomm
a
b
vasudevan m
gygli a
volokitin and l
van gool
query adaptive video summarization via quality aware relevance estimation
in proceedings of the acm international conference on multimedia pages
acm
s
xiao z
zhao z
zhang x
yan and m
yang
convolutional hierarchical attention network for query focused video summarization
arxiv preprint

b
xiong y
kalantidis d
ghadiyaram and k
grauman
less is more learning highlight detection from video duration
in proceedings of the ieee conference on computer vision and pattern recognition pages
s
yeung a
fathi and l
fei fei
videoset video summary evaluation through text
arxiv preprint

l
yuan f
e
tay p
li l
zhou and j
feng
cycle sum cycle consistent adversarial lstm networks for unsupervised video summarization
arxiv preprint

k
zhang w

chao f
sha and k
grauman
summary transfer exemplar based subset selection for video summarization
in proceedings of the ieee conference on computer vision and pattern recognition pages
k
zhang w

chao f
sha and k
grauman
video summarization with long short term memory
in european conference on computer vision pages
springer
b
zhou a
lapedriza j
xiao a
torralba and a
oliva
learning deep features for scene recognition using places database
in advances in neural information processing systems pages
k
zhou y
qiao and t
xiang
deep reinforcement learning for unsupervised video in thirty second aaai conference on marization with diversity representativeness reward
articial intelligence

