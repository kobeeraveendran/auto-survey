concept pointer network for abstractive summarization wang gao huang and zhou school of computer science and technology beijing institute of technology beijing china beijing engineering research center of high volume language information processing and cloud computing applications
edu
cn t c o l c
s c v
v i x r a abstract a quality abstractive summary should not only copy salient source texts as summaries but should also tend to generate new conceptual words to express concrete details
inspired by the popular pointer generator sequence sequence model this paper presents a concept pointer network for improving these aspects of abstractive summarization
the network ages knowledge based context aware tualizations to derive an extended set of date concepts
the model then points to the most appropriate choice using both the cept set and original source text
this joint proach generates abstractive summaries with higher level semantic concepts
the ing model is also optimized in a way that adapts to different data which is based on a novel method of distantly supervised learning guided by reference summaries and testing set
overall the proposed approach provides tistically signicant improvements over eral state of the art models on both the and gigaword datasets
a human ation of the model s abstractive abilities also supports the quality of the summaries duced within this framework
introduction abstractive summarization abs has gained overwhelming success owing to a tremendous development of sequence to sequence model and its variants rush et al
chopra et al
paulus et al
guo et al
gao et al

in tandem with els pointer generator was developed by see et al
as a solution to tackle the rare words and out of vocabulary oov problem associated with generative based models
the idea behind is to use attention as a pointer to determine the ability of generating a word from both a corresponding author figure only copies keyword from the source text while generates new concepts to convey the meaning
ulary distribution and the source text
pointer generator networks have also been extensively cepted by the abs community due to their cacy with long document summaries chen and bansal hsu et al
title tion sun et al

however the current power of abstractive marization falls short of their potential
as the ample in figure shows a model with a pointer mechanism marked as the direct pointer is likely to merely copy parts of the original text to form a summary using keywords and phrases such as athletes
conversely a more like summary would be based on one s own derstanding of the detail in the words expressed as higher level concepts drawn from world edge like using the word group to replace athletes and ofcials
this indicates that a good summary should not simply copy original rial it should also generate new and even abstract concepts that reect high level semantics
therefore a pointer generator network that solely considers the source material to generate a summary does not adequately satisfy the needs of high quality abstractive summarization
we argue that concepts have a greater ability to press deeper meanings than verbatim words
as such it is essential to explore the potential of ing concepts from world knowledge to assist with abstractive summarization
our developed model not only points to informative source texts but also leverages conceptual words from human edge in the summaries it generates
hence in this paper we propose a novel model based on a concept pointer generator that ages the generation of conceptual and abstract words
as a hidden benet the model also viates the oov problems
our model uses pointer network to capture the salient information from a source text and then employs another pointer to generalize the detailed words according to their upper level of expressions
finally the output is also consistent with language model by the generator
unique to our concept pointer is a set of concept candidates particular for a word that is drawn from a huge knowledge base
the set of candidates adheres to a concept distribution where the probability of each concept being erated is linked to how strongly the candidate resents each word
moreover the concept bution is iteratively updated to better explain the target word given the context of the source rial and inherent semantics in the texts
hence the learned concept pointer points to the most suitable and expressive concepts or words
the tion function is adaptive so as to cater for different datasets with distantly supervised training
the network is then optimized end to end using forcement learning with the distant supervision strategy as a complement to further improve the summary
overall the contributions of this paper are a novel concept pointer generator network that leverages context aware conceptualization and a concept pointer both of which are jointly tegrated into the generator to deliver tive and abstract oriented summaries a novel distant supervision training strategy that favors model adaptation and generalization which sults in performance that outperforms the accepted evaluation based reinforcement learning optimization on a test only dataset in terms of rouge metrics a statistical analysis of titative results and human evaluations from parative experiments with several state of the art models that shows the proposed method provides promising performance
related work abstractive summarization supposedly digests and understands the source content and consequently the generated summaries are typically a zation of the wording that sometimes form new sentences
historically abstractive tion has been performed through rule based tence selection dorr et al
key tion extraction genest and lapalme tactic parsing bing et al
and so on
ever more recently models with attention have played a more dominant role in generating abstractive summaries rush et al
chopra et al
nallapati et al
zhou et al

extensions to the approach include an intra decoder attention paulus et al
and coverage vectors see et al
to decrease etition in phrasing
copy mechanism gu et al
has been integrated into these models to tackle oov problem
zhou et al
went on to propose which copies complete sequences from an input sentence to further tain the readability of the generated summary
pointer mechanism vinyals et al
has drawn much attention in text summarization see et al
because this technique not only vides a potential solution for rare words and oov but also extends abstractive summarization in a exible way c elikyilmaz et al

further pointer generator models can effectively adaptive to both extractor and abstractor networks chen and bansal and summaries can be erated by incorporating a pointer generator and multiple relevant tasks guo et al
such as question or entailment generation or multiple source texts sun et al

however work particularly targets the problem of the abstraction is rare
abstract meaning resentation amr is used to transform a sentence into a concept graph then merge those similar concept nodes to form a new summary graph liu et al

concepts are also incorporated as auxiliary features guo et al

kryscinski et al
and weber et al
dene the number of new n grams as the primary criteria of abstractiveness
this makes sense in most cases
but we believe that abstraction means ing detailed content with higher level semantically related concepts which has motivated the opment of the model proposed in this paper
the proposed model
concept pointer generator neural abstractive summarization can be scribed as a generation process where a tial input is summarized into a shorter sequential output through a neural network
suppose that the sequential input


xi


xn is a quence of n number of words and i is the index of the input
the shorter i
e
summarized sequence of output is denoted as y


yt


ym with number of m words and t indicates a time step
as figure shows our model consists of two sub modules an encoder decoder module and the proposed concept pointer generator ule

encoder decoder framework this process is formulated as an encoder decoder framework that consists of an encoder and an attention equipped decoder
we use a two layer bi directional lstm rnn encoder and one layer uni directional lstm rnn decoder along with attention mechanism
h


h


h n and formally the encoder produces sequential den states as h n in the corresponding positions and the bi directional hi flst m
each word xi in the sequence can be represented as a concatenation i
e
hi of the bi directional hidden states h i
the decoder generates a target h i mary from a vocabulary distribution which is based on a context vector h t through the following process p t x h where st is the hidden state of the decoder at time step t and h t is the context vector at time step t
w w are trainable parameters and sfm is short for softmax function
the context vector h t is computed by a weighted sum of the hidden representations of the source text and the weight is denoted as the tion t i
h t t ihi t i hhi w sst the softmax function normalizes the vector of a distribution over the input position and v w h w s are trainable parameters
pointer networks use attention as a pointer to lect segments of the input as outputs vinyals et al

as such a pointer network is a suitable mechanism for extracting salient tion while remaining exible enough to interface with a model for generating an abstractive summarization see et al

our proposed model is essentially an upgrade to this tion that integrates a new concept pointer network within a unied framework


context aware conceptualization understanding the instances of a word requires a taxonomic knowledge base that relates those words to a concept space
in our model we use an isa taxonomy called the microsoft concept wang et al
to serve this purpose for two reasons wang and wang
first this graph provides a huge concept space with multi word terms that cover concepts of worldly facts as concepts instances relationships and
second the relationships between concepts and entities are probabilistic as a measure of how strongly they are related
moreover the bilities are trustworthy given they have been rived from evidence found in billions of webpages search log data and other existing taxonomies
our model is data driven and therefore is more easily adaptable with probabilities
all these acteristics make the microsoft concept graph a suitable choice for our model
more detailed amples are available in appendix a
the concept graph species the probability that each instance belongs to a concept c
given a word we have a distribution over a set of related concepts
yet this raises the question of how to identify a context appropriate concept for a word from the distributional set of candidate cepts
for instance apple in the context of an ple is good for you health tends to be associated with the concept of fruit instead of company
mally given a word xi in a training sentence a set of k concept candidates ci i is linked to the word from the knowledge base with distributional probabilities over the concepts i
e
i ck i microsoft concept graph was derived from probase project
the public data can be downloaded via the provided api
research
microsoft
com home api current version is mined from billions of web pages containing
m unique concepts
m unique entities and m isa relations
figure the architecture of our model
blue bar represents the attention distribution over the inputs
purple bar represents the concept distribution over the inputs
noted that this distribution can be sparse since not every word has its upper concept
green bar represents the vocabulary distribution generated from component
i p is to nd the most suitable concept updated context represented by the vector h equation at each time step t
i
the task i to t the t in in the case of generating summaries given dated contexts a weighted update of the tional concept candidates needs to be performed
in the model the updated weight denoted as j i is estimated by a softmax classier that is jointly conditioned on the hidden representation of the word hi the context vectors h t and each of cept vectors j i h t i where j w is a trainable parameter and cj i is the vector of the jth concept candidate which is a representation of the input embeddings
together with the concept probability from the existing knowledge base i and the updated weights based on the contexts j i a context aware conceptualized probability of jth concept for the word xi p c i j is nally estimated as i j i j p c i i ck where is a tunable parameter
theoretically we will end up with a number of k relevant concepts for each word ci i with a bility distribution over the set which is learned as i j p i p c p c

concept pointer generator the basic pointer generator network contains two sub modules one is the pointer network and the p c i k
other is the generation network
these two modules jointly determine the probabilities of the words in the nal generated summary
the ation probability pgen for the generation network see et al
is learned by pgen hh t w sst w bgen where is a sigmoid function
for the pointer network our model consists of a pointer to the source text and a further concept pointer to the relevant concepts that have arisen from the source content
these two separate ers are calculated as follows
the rst pointer is taken based on the attention distribution t i over the source text
the second concept pointer is erated over a concept distribution of the source text that is scaled element wise by the attention bution
to train the model given the likelihood of each concept in the current context the updates could be performed in two ways
in a hard assignment the concept that receives the highest score would be selected for the update i argmax p c c i a where a arg max j i j where a is the index of maximized generated weight based on the contexts and p i a is obtained by eqs

in random selection each of the concept didates could be trained randomly to update the parameters i random p c c i j p c i where j represents the selected concept index
considering the above baseline generation work and both the pointer networks our nal put distribution is pgen t i c i t i i wi w i wi w can be updated by where c i arg max or i c the difference between these two i random
choices is demonstrated in the experiments tion

objective learning

basic mle the baseline objective is derived by ing the likelihood training for the eration given a reference summary y y for document
the ing objective is to minimize the negative likelihood of the target word sequence y y lm le log p y t y

evaluation based reinforcement learning rl similar to paulus et al
policy gradient methods can directly optimize discrete target uation metrics such as rouge
the basic idea is to explore new sequences and compare them to the best greedy baseline sequence
once the baseline sequence y or sampled sequence ys are ated they are compared against the reference quence y to compute the rewards and respectively
in the rl training stage two arate output candidates at each time step are duced ys is sampled from the probability tion p ys and y is the baseline output
the training objective is then to minimize ys ys log p ys lrl it is noteworthy that the samples ys are selected from a wide range of vocabularies extended by all the concept candidates
this strategy ensures that the model learns to generate sequences with higher rewards by better exploring a set of close concepts
thus the combination of these two objectives yield improved task specic scores while ing a better language model lf inal lrl le where is a soft switch between the two objectives
the model is pre trained with mle loss then switch to the nal loss


distant supervision ds for model adaption our intuition is that if the summary document pairs are dissimilar to the testing set the model could be retrained to adapt to weaken the inuence of the dissimilarity on the nal loss
the result would be a training model that better ts the cic testing data
the challenge is that there are no explicit supervision labels to indicate whether the training set is close to the testing set so a new training paradigm is needed
in answer to this need and also to provide end to end ity in the model we developed a simple approach for labeling summary document pairs by ing the kullback leibler kl divergence between each training reference summary and a set of ing documents
in this way the training pairs are distantly labelled for training the model
specically the representations of the ence summaries and the testing set are computed by summing all the involved word embeddings
given a testing document xd l where n d and n d is the size of the testing corpus the based representation of one document is xd l where is the number of ment words involved
the reference summary is represented by y t
we malize these vectors through a softmax function to cater for kl calculation
the model adaption with the distant labels is dened as y xd xd lds n d l lmle where dkl
indicates the kl divergence tween y and xd l and is a constant ter that is tuned via adaption to the testing set
the divergences are averaged within the testing set which indicates the overall distances between testing set and each of the reference document pairs
in this way the samples in the training corpus are distantly annotated as either relevant or irrelevant for model adaption noting that the model is pre trained with the mle loss before switching to distantly supervised training
experiments datasets to evaluate the effectiveness of our proposed model we conducted training and ing on two popular datasets
the rst was the glish gigaword fifth edition corpus parker et al

we replicated the pre processing steps in rush et al
to obtain the same ing testing data
after pre processing the corpus contained about
m sentence summary pairs as training set and k pairs as the development set
once pairs with empty titles were removed the testing set numbered pairs
the second dataset was only used for testing
this dataset consists of document headline mary pairs where each document is paired with four reference summaries written by humans
evaluation metrics we used rouge lin as the evaluation metric which measures the quality of a summary by computing the lapping lexical elements between the candidate summary and a reference summary
following previous practice we assessed unigram bigram and rg l longest common quence lcs
noted that the english testing set contains references of different lengths while the testing set xes the mary length to bytes
training setups we initialize word dings with d vectors and ne tune them ing training
concepts share the same embeddings with the words
the vocabulary size was set to for both the source and target text
the den state size was set to
the vocabulary size is increased from around to cepts w

t the different number k of concept candidates for each word
note that the generated concepts with unks were subsequently deleted
our code is available on github
com wprojectsn codes and the vocabularies and candidate concepts are also cluded
we trained our models on a single gtx tan gpu machine
we used the adagrad mizer with a batch size of to minimize the loss
the initial learning rate and the accumulator value were set to
and
respectively
we used gradient clipping with a maximum gradient norm of
at the time of decoding the summaries were produced through a beam search of size
the hyper parameter settings were


on and
on word
we trained our concept pointer generator for iterations yielded the best performance then took the optimization using rl rewards for rg l at k iterations on and at k iterations on gigaword
we took the supervised training at k iterations on and at
k iterations on gigaword
baselines the following state of the art lines were used as comparators
rush et al
is a tuned abs model with tional features
luong nmt luong et al
is a two layer lstm encoder decoder
elman chopra et al
is a convolution coder and an elman rnn decoder with tion
is two layer bilstm encoder and one layer lstm decoder equipped with lsent nallapati et al
uses tention
temporal attention to keep track of the past tive weights of the decoder and restrains the etition in later sequences
seass zhou et al
includes an additional selective gate to trol information ow from the encoder to the coder
pointer generator see et al
is an integrated pointer network and model
we implemented this baseline without its coverage mechanism since this is not our focus
baseline models also include two pointer generator based extensions guo et al
li et al

cgu lin et al
sets a convolutional gated unit and self attention for global encoding
results and analysis the following analysis focuses on investigating whether our model is rst able to generate stract and new concepts and second how the overall quality performs against the baselines

quantitative analysis the results are presented in table
we observe that our model outperformed all the strong of the art models on both datasets in all metrics in terms of the except for on gigaword
pointer generator performance the improvements made by our concept pointer are statistically nicant p
across all metrics
rouge evaluation option is rouge evaluation option is oov and summary length oov is another major challenge for current abstractive table rouge evaluation results on the gigaword and rouge recall on test set
the results with mark are taken from the corresponding papers
underlined scores are the best without additional optimization
bold scores are the best between the two optimization strategies
mark indicates the improvements from the baselines to the concept pointer are statistically signicant using a two tailed t test p

models rush et al
luong nmt luong et al
ras elman chopra et al
lsent nallapati et al
seass zhou et al
impl
pointer generator impl
see et al
pointer cov


guo et al
sel
eram li et al
cgu lin et al
concept pointer concept concept gigaword rg l rg l







































































table oov problem analysis percentages no
unk no
all generated words of generating unk w

t the following three models on gigaword and datasets
method pointer generator concept pointer gigaword





table abstractiveness percentage of novel n grams on gigaword dataset
models pointer generator concept pointer reference summary novel n gram gram gram gram








rization models
although generating longer maries or less unks is not our focus our model still showed improvements in this regard table
we counted the number of unks and all generated summary words and measured the portions in both testing sets
the oov ages dropped from
to
on gigaword and from
to
on which demonstrates that our model is effective at viating oov problems
this result also supports the superior of the concept pointer over the line pointer generator
from the statistics we found that the summaries generated by our cept pointer averaged around
words while the pointer generator summaries averaged
words per summary on
this shows the concept pointer is able to capture more salient content by generating relatively longer summaries
abstractiveness according and bansal abstractiveness scores are puted as the percentage of novel n grams in to chen the generated summaries that are not included in the source documents
as shown in table compared with human written summaries which receive the highest novelty in terms of abstractiveness our concept pointer generator achieves closest performance with human written summaries against this result demonstrates a further advantage of our model in producing new and abstract concepts
our model is designed to improve semantic relevance and promote higher abstraction
more generated summary examples can be found in appendix b
the baseline

analysis on training strategies to evaluate the relative impact of each training strategy with the model we tested different nations for comparison with each other and against the baselines
context aware conceptualization to gate the impact of training with both the ber of concepts k and the concept update egy mentioned in eqs
and we chose a table human evaluation scoring of three models in terms of abstraction and overall quality by human uators the higher the better
the score range could be
indicates the improvements from the baselines to the concept pointer are statistically signicant
method pointer generator concept pointer abstraction


overall quality


has a noticeable effect when the testing set is stantially semantically different from the training set but provides less improvement than rl when the two are close
from this analysis we conclude that the ds strategy is better for model adaption with abstractive summarization

human evaluations to explore the correctness of our model using man judgment we conducted a manual tion with post graduate volunteers
we ily used the following criteria to assess the ated summaries abstraction i
e
are the abstract concepts contained in the summary appropriate and overall quality i
e
is the summary readable informative relevant
to conduct the uation we randomly selected examples from the duc testing set and asked the teers to subjectively assess the summaries
each example consisted of an article and three maries i
e
a summary by the model the pointer generator model and our proposed cept pointer model
the volunteers chose the best summaries for each of the articles according to the above criteria can be multiple choices
viously the summaries were randomly shufed and the model used to produce each was unknown to prevent bias
the scores for each model were ranked by how many times the volunteers chose a summary w

t each criteria averaged by the ber of participants
the results are presented in table which show that our model outperformed both the model and the pointer generator see et al
in both criteria
as a last step we manually inspect the maries generated by our model and some ples are presented in appendix b
we found that the summaries were not as abstract as written summary would likely be
the ing tendency of the model is still to copy segments of the source text and rearrange the phrases into gigaword c i random c i random gigaword c i arg max c i arg max figure rouge metrics on gigaword and w

t a different number of concept dates
updates were conducted by hard assignment i arg max and random selection c c i random
ferent number of concept candidates i
e
k to for the context aware ization update strategy
performance was fully evaluated with the three rouge metrics as shown in figure
the results only vary slightly ing to the number of concepts with the random lection strategy eq
as shown in figure and
this indicates that a random strategy is not very sensitive to the number of extracted ics
this is in part because the concept pointer may or may not be able to point to the correct cepts from multiple candidates
while in figure and the optimum settings are clearly parent i
e
k on gigaword and k on
overall the hard assignment strategy eq
provided the best performance in practical terms while random selection eq
performs stably with different settings
training with ds vs
rl as shown in ble our model with either a distant supervision strategy concept or reinforcement learning concept were both rior to the basic concept pointer generator on both datasets
further the relative improvement of the concept over the concept ranged from
to
on but was inferior to concept on gigaword
in comparing the results it is clear that ds training l a summary
however the overall approach does produce more high level concepts with correct lations compared to the baselines which strates that our solution is a promising research rection to further pursue
additionally the ated summaries are long uent and informative
conclusion this paper presents a novel concept pointer ator model to improve the abstractive tion model and generate concept oriented maries
we also propose a novel distant vision strategy for model adaption to different datasets
both empirical and subjective ments show that our model makes a statistically signicant quality improvement over the state the art baselines on two popular datasets
acknowledgement this work was supported by national ral science foundation of china no
partially supported by the research foundation of beijing municipal science nology commission no
and partially supported by ministry of tion china mobile research foundation no

references lidong bing piji li yi liao wai lam weiwei guo and rebecca j
passonneau

abstractive multi document summarization via phrase selection in proceedings of the annual and merging
meeting of the association for computational guistics and the international joint conference on natural language processing of the asian ation of natural language processing acl volume long papers pages
asli c elikyilmaz antoine bosselut xiaodong he and yejin choi

deep communicating agents for in proceedings of the abstractive summarization
conference of the north american chapter of the association for computational linguistics man language technologies naacl hlt volume long papers pages
yen chun chen and mohit bansal

fast tive summarization with reinforce selected sentence rewriting
arxiv preprint

sumit chopra michael auli and alexander m
rush

abstractive sentence summarization with tentive recurrent neural networks
in conference of the north american chapter of the association for computational linguistics human language nologies pages
bonnie dorr david zajic and richard schwartz

hedge trimmer a parse and trim approach to headline generation
in proceedings of the naacl on text summarization workshop volume pages
association for computational guistics
yang gao yang wang luyang liu yidi guo and heyan huang

neural abstractive rization fusing by global generative topics
neural computing and applications
pierre etienne genest and guy lapalme

framework for abstractive summarization using the text to text generation
workshop on monolingual text to text generation pages
in proceedings of jiatao gu zhengdong lu hang li and victor ok incorporating copying mechanism in li

in proceedings of sequence to sequence learning
the annual meeting of the association for putational linguistics volume long papers volume pages
han guo ramakanth pasunuru and mohit bansal

soft layer specic multi task summarization in with entailment and question generation
ceedings of the annual meeting of the tion for computational linguistics acl ume long papers pages
yidi guo heyan huang yang gao and chi lu

conceptual multi layer neural network model for headline generation
in chinese computational guistics and natural language processing based on naturally annotated big data pages
springer
wan ting hsu chieh kai lin ming ying lee kerui min jing tang and min sun

a ed model for extractive and abstractive arxiv preprint rization using inconsistency loss


wojciech kryscinski romain paulus caiming xiong and richard socher

improving abstraction in text summarization
in proceedings of the conference on empirical methods in natural guage processing pages
haoran li junnan zhu jiajun zhang and chengqing zong

ensure the correctness of the incorporate entailment knowledge into mary stractive sentence summarization
in proceedings of the international conference on computational linguistics coling pages
chin yew lin

rouge a package for matic evaluation of summaries
text summarization branches out
acm international on conference on tion and knowledge management pages
acm
noah weber leena shekhar niranjan nian and kyunghyun cho

controlling coding for more abstractive summaries with based networks
arxiv preprint

qingyu zhou nan yang furu wei and ming zhou

selective encoding for abstractive sentence summarization
in proceedings of the annual meeting of the association for computational guistics volume long papers pages
qingyu zhou nan yang furu wei and ming zhou

sequential copying networks
in proceedings of the thirty second aaai conference on articial intelligence pages
junyang lin xu sun shuming ma and qi su

global encoding for abstractive summarization
in proceedings of the annual meeting of the sociation for computational linguistics acl volume short papers pages
fei liu jeffrey flanigan sam thomson norman sadeh and noah a smith

toward tive summarization using semantic representations
arxiv preprint

thang luong hieu pham and christopher d
ning

effective approaches to attention based in proceedings of the neural machine translation
conference on empirical methods in natural language processing emnlp pages
ramesh nallapati bowen zhou ccero nogueira dos santos c aglar gulcehre and bing xiang

abstractive text summarization using sequence sequence rnns and beyond
in proceedings of the signll conference on computational natural language learning conll pages
robert parker david graff junbo kong ke chen and kazuaki maeda

english gigaword fth tion
dvd
philadelphia linguistic data consortium
romain paulus caiming xiong and richard socher

a deep reinforced model for abstractive marization
arxiv preprint

alexander m
rush sumit chopra and jason weston

a neural attention model for abstractive in proceedings of the tence summarization
conference on empirical methods in natural guage processing emnlp pages
abigail see peter j
liu and christopher d
ning

get to the point summarization with in proceedings of the pointer generator networks
annual meeting of the association for putational linguistics acl volume long papers pages
fei sun peng jiang hanxiao sun changhua pei wenwu ou and xiaobo wang

multi source pointer network for product title summarization
in proceedings of the acm international ence on information and knowledge management pages
acm
oriol vinyals meire fortunato and navdeep jaitly

pointer networks
in advances in neural formation processing systems pages
zhongyuan wang and haixun wang

in the association for standing short texts
putational linguistics acl tutorial
zhongyuan wang haixun wang ji rong wen and yanghua xiao

an inference approach to in proceedings of the sic level of categorization

