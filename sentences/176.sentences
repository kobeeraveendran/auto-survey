automatic text document summarization using semantic based analysis thesis submitted to the jawaharlal nehru university for the award of the degree of doctor of philosophy by chandra shekhar yadav school of computer and systems sciences jawaharlal nehru university new delhi india july i dedicated to my parents school of computer and systems sciences jawaharlal nehru university new delhi declaration this is to certify that the thesis entitled automatic text document summarization using semantic based analysis is being submitted to the school of computer and systems sciences jawaharlal nehru university new delhi in partial fulfillment of the requirements for the award of the degree of doctor of philosophy is a record of bonafide work carried out by me under the supervision of dr
aditi sharan
this thesis contains less than words in length exclusive tables figures and bibliographies
the matter embodied in the thesis has not been submitted in part or full to any university or institution for the award of any other degree or diploma
chandra shekhar yadav enrollment no
school of computer and systems sciences jawaharlal nehru university new india iii school of computer and systems sciences jawaharlal nehru university new delhi certificate this is to certify that the thesis entitled automatic text document summarization using semantic based analysis submitted by mr
chandra shekhar yadav to the school of computer and systems sciences jawaharlal nehru university new delhi for the award of degree of doctor of philosophy is a research work carried out by him under the supervision of dr
aditi sharan
supervisor dr
aditi sharan dean prof
d
k lobiyal school of computer and systems sciences school of computer and systems sciences jawaharlal nehru university jawaharlal nehru university new india new india iv v acknowledgments no goal is achievable without guidance
for this i am very glad to express my sincere gratitude and utmost regards to my supervisor dr
aditi sharan for his guidance with many helpful discussions and his intellectual inputs to make thesis work worthy
his extensive research experiences were very helpful in my thesis
the most important thing is his helping nature and friendly behavior that contributed an important share in the fulfillment of this work
the methodology philosophy and problem solving methods suggested by him have been a great help in this work and would be afterwards too
i am grateful to my all the teachers of sc ss jnu especially prof
k
k
bharadwaj prof
c
p
katti prof
karmeshu prof
d
p
vidyarthi prof
s
n
minz prof
r
k
agrawal prof
d k lobiyal prof
t
v
vijay kumar and dr
buddha singh for their guidance support and motivation whenever i feel depressed due to rejection or hard comments of my research papers
i would like to express my thanks to dean sc ss jnu for his support to pursue my work in the school
also i extend my thanks to the school administration especially chandra sir meena maam and ashok sir librarian of sc ss library and the dr
b
r
ambedkar central library of jnu for supporting me
the work was carried out in lab which has a very healthy and friendly work culture
the regular lab discussions exchange of ideas and opinions are part of it
i am thankful to dr
hazra imran ubc canada ms
manju lata joshi dr
nidhi malik dr
jagendra singh dr
mayank saini dr
sifatullah siddiqi dr
sonia mr
vikrant vaish mr
ashish mrs
sheeba siqqique ms
bhawana gupta all the support during my stay in lab sc ss jnu
special thanks to mr
rakesh kumar and miss
payal biswas for encouraging and helping me in writing the thesis
it was wonderful to stay in the lab and lifetime memorable for me
the students in are very much helpful supportive as well as courageous to each other
i am thankful to all my seniors juniors fellow students and friends
i would like to thank dr
kapil gupta nit k dr
neetesh kumar iiitm gwalior dr
vipin mgcu dr
navjot mnnit dr
yogendra meena du dr
raza abbas haidri du dr
gaurav baranwal bhu mr
prem yadav dte up mr
harendra vi pratap du mr
sumit kumar mr
krishan veer du dr
dinesh kumar cuh mr
utkarsh nit d mr
dhirendra dtu and those people who always encouraged and motivate me whenever i feel depressed during my work
i am thankful to director sliet dr
shailendra jain and all my colleagues in computer science department dr
major singh h
o
d cse dr
damanpreet singh dr
birmohan singh dr
manoj sachan dr
sanjeev singh mrs
gurjinder kaur dr
vinod verma mr
jaspal sigh mr
manminder singh mr
rahul gautam mrs
preetpal kaur dr
sanjeev singh electrical sliet mr
pankaj das electronics sliet dr
amit rai chemical sliet and mr
jonny singla mechanical sliet
thanks to my wonderful family for bearing with me as i am
i was blessed with the love compassion supplications and support of my family members during this period
my deepest gratitude goes to them for their selfless love care and indulgence throughout my life this thesis was simply impossible without them
my heart goes out in reverence to my respected father mr
padam singh yadav and dear mother mrs
suvita devi for their blessings sacrifices tremendous affection and patience
i am also indebted to my sisters mrs
rinkesh yadav and my younger brother mr
kesari singh yadav for their unconditional love and blind faith in me and also for their continuous motivation in my tough time
finally i would like to express thanks to each person who is directly or indirectly related to my work
also i would like to thank the university grant commission ugc and council of scientific industrial research csir india for its research fellowship during the period
chandra shekhar yadav enrollment no
school of computer and systems sciences jawaharlal nehru university new india vii abstract since the advent of the web the amount of data on wen has been increased several million folds
in recent years web data generated is more than data stored for years
one important data format is text
to answer user queries over the internet and to overcome the problem of information overload one possible solution is text document summarization
this not only reduces query access time but also optimize the document results according to specific user s requirements
summarization of text document can be categorized as abstractive and extractive
most of the work has been done in the direction of extractive summarization
extractive summarized result is a subset of original documents with the objective of more content coverage and lea redundancy
our work is based on extractive approaches
in the first approach we are using some statistical features and semantic based features
to include sentiment as a feature is an idea cached from a view that emotion plays an important role
it effectively conveys a message
so it may play a vital role in text document summarization
the second work in extractive summarization dimensions based on latent semantic analysis
in this document are represented in the form of a matrix where rows represent concepts that cover different dimensions and columns represents documents
lsa has the ability to be mapped to the same concept space
lsa can hold synonyms relations since the mapping of the same concepts
based on svd decomposition and concepts of entropy we find most informative concepts and sentences
since lsa can not hold polysemy relations so to extend this work we have used wordnet relations to handle all relationships among words sentences
in third work a lexical network has been created and most informative sentences extracted based on that
to extend lexical chain based work we have designed an optimization function to optimize content coverage and redundancy along with length constraints
since the nature of function is linear constraints are also linear so we have applied integer linear programming to find a solution
viii contents declaration

























































































































iii certificate



























































































































acknowledgments















































































































vi abstract





























































































































viii list of figures






















































































































xii list of tables






















































































































xiv chapter introduction to automatic text document summarization




































introduction

























































































































































flavors of summarization





































































































































extractive and abstractive summarization



































































































single and multi document summarization

































































































query focused and generic summarization
































































































personalized summarization

























































































































guided summarization

































































































































indicative informative and critical summary





























































































state of art approach in summarization













































































































corpus description















































































































































corpus description for hybrid approach






































































































duc dataset








































































































































summary evaluation










































































































































content based












































































































































task based


















































































































































readability

















































































































































thesis objective
















































































































































chapter hybrid approach for single text document summarization using statistical and sentiment features













































































































introduction























































































































































literature work

















































































































































background
























































































































































random forest












































































































































binary logistic regression


























































































































features used in text document summarization
































































































the location feature



















































































































ix

the aggregation similarity feature































































































frequency feature
























































































































centroid feature



























































































































sentiment feature























































































































summarization procedure


































































































































algorithm




















































































































































detailed approach description


















































































































experiment and results






































































































































experiment













































































































































experiment













































































































































experiment













































































































































experiment












































































































































concluding remark











































































































































chapter a new latent semantic analysis and entropy based approach for automatic text document summarization



































































































introduction























































































































































background
























































































































































introduction to latent semantic analysis



































































































lsa and summarization






























































































































introduction to entropy and information



































































































related work





















































































































































gongliu model




























































































































murray model




























































































































sj model





































































































































model





























































































































model





























































































































proposed model
















































































































































working of lsa with an example
















































































































mincorrelation model






























































































lsacs model














































































































lsass model













































































































proposed model to measure redundancy in summary















































































experiment and results


















































































































































































































































































































































































































































































































































































































































































































concluding remark











































































































































chapter lexnetwork based summarization and a study of impact of wsd techniques and similarity threshold over lexnetwork

















































































introduction























































































































































related work





















































































































































background
























































































































































word sense disambiguation wsd












































































































lexical chain
















































































































































lexalytics algorithms


































































































































centrality




















































































































































proposed work


















































































































































experiments and results






































































































































performance of different centrality measure over selected lesk algorithm with different threshold















































































































































impact of wsd technique and threshold for different centrality measures
















comparisons with lexalytics algorithms








































































overall performance
































































































































concluding remark









































































































































chapter modeling automatic text document summarization as multi objective optimization






















































































































introduction





















































































































































literature work
















































































































































related work












































































































































baseline mdr model





























































































































background






















































































































































linear programming

































































































































proposed multi objective optimization model

































































































outline of proposed model























































































































detail description of proposed model





































































































experiments and results












































































































































































































































































cosine based criteria to minimize redundancy























































































a b lexical based criteria to reduce redundancy





































































































































































































































a co relation analysis between different centrality based proposed model

























































































































































































concluding remark









































































































































chapter conclusion and future work












































































references























































































































xi list of figures
showing inshorts app s news





































































































google snippet example













































































































three step process for text document summarization





























































sentence length y axis vs sentence number x axis





























































two of six optimal summaries with scus













































































figure
random forest algorithm









































































































figure
algorithm for summarization





































































































opinosis generated summary

































































































precision curve

























































































































recall curve































































































































showing f score























































































































f score summary












































































































figure
comparative performance of model

and model



















































and vt matrix



















showing mechanism of lsa matrix a decomposed into u
lsa based summarization procedure






















































































lsa decomposition on a matrix in reduced dimension




















































vector representation in d space























































































u and vt matrix in d space representing words and sentences









































showing improved performance using our proposed proposed








showing improved performance using our entropy based proposed


















rouge score for the entropy based system showing as the number of words increasing rouge score also decreasing


































































































example of lexical chain creation dataset available elhadad






































showing normalized degree centrality for an undirected graph












































showing a graph with some nodes and edges









































































showing eight node with degree regular graph



































































steps in summarization













































































































in graphical form directed graph
refers to sentence from
and from
corresponding weighted edges between two sentences are shown here
























performance evaluation using rouge score of different centrality measures adapted lesk as wsd and similarity threshold









































































































performance evaluation using rouge score of different centrality measures cosine lesk as wsd and similarity threshold









































































































performance evaluation using rouge score of different centrality measures cosine lesk as wsd and similarity threshold











































































































performance evaluation using rouge score of different centrality measures simple lesk as wsd and similarity threshold









































































































performance evaluation using rouge score of different centrality measures simple lesk as wsd and similarity threshold











































































































impact of wsd technique and similarity threshold over subgraph based centrality










impact of wsd technique and similarity threshold over page rank as a centrality measure

































































































































































xii
impact of wsd technique and similarity threshold over bonpow based centrality









impact of wsd technique and similarity threshold over between ness centrality












impact of wsd technique and similarity threshold over closeness centrality




















impact of wsd technique and similarity threshold over alpha centrality
alpha

semantrica lexalytics vs our proposed system improved only




































showing overall performance of some system for the comparative study

























graphical method of solution of w

t and


























































figure
outline of summarizer system































































































three module process for automatic text summarization



















































showing procedure stepwise example from duc healthcare data set file number
txt
































































































































figure
comparative performance of proposed and mdr baseline model






















different scatter plots showing different directions and strengths of correlation















figure
graph showing comparative performance of different relevance based measure where sign denoting hybridization of different features




















































































xiii list of tables
different features scores and total score for sentences






































our system generated summary using proposed algorithm






























microsoft system generated summary

































































mead system generated summary






































































summary generated by our algorithm considered as system summary another summary as a model summary




























































































summary generated by our algorithm as system summary another summary of a model summary


















































































































summary generated by our algorithm as system summary another summary of a model summary


















































































































summary generated by different as system summary human generated summary as a model summary











































































































summary generated by different system considered as system summary generated summary as a model summary












































































f score summary
























































































different rouge score for summary generated using different approaches





different rouge score is shown on duc dataset














































table
showing performance of model

































































table
showing performance o model




































































frequency based words documents matrix a















































w in reduced space






















































































w is processed by retaining only positive related sentences and rowsum is calculated to find the probability

























































































in reduced space r information contained by each concept and corresponding sentence




























































































































xiv
w in reduced space






















































































w is processed by keeping only positive related sentences and column sum is find to measure the probability


































































































information contained by each sentence in reduced space r
































rouge score of previously proposed models























































performance of proposed models with proposed or proposed









































































































































performance of model with entropy based model













































generally by increasing summary length rouge score is also increasing and sometimes reducing












































































































showing average information contains in summary generated by different systems































































































































average n gram presents in different systems generated summary



















summarization of




m stands for model











showing all sentences of
txt from duc dataset for the creation of lexical network

































































































showing lexical and semantic relations present between sentences



















performance of different centrality measures using adapted lesk as wsd and similarity threshold













































































































performance of different centrality measures using adapted lesk as wsd and similarity threshold











































































































performance of different centrality measures using cosine lesk as wsd and similarity threshold











































































































performance of different centrality measures using cosine lesk as wsd and similarity threshold











































































































performance of different centrality measures using simple lesk as wsd and similarity threshold










































































































xv
performance of different centrality measures using simple lesk as wsd and similarity threshold











































































































impact of wsd technique and similarity threshold over subgraph based centrality









































































































































impact of wsd technique and similarity threshold over eigen value based centrality


























































































































impact of wsd technique and similarity threshold over eigen value based centrality


























































































































impact of wsd technique and similarity threshold over page rank as a centrality measure




























































































































impact of wsd technique and similarity threshold over bonpow based centrality









































































































































impact of wsd technique and similarity threshold over closeness centrality



comparison of semantrica lexalytics algorithm with difference centrality based measure in which adapted lesk used as wsd showing only improved system w

semantrica























































































































comparison of semantrica lexalytics algorithm with difference centrality based measure in which cosine lesk used as wsd showing only improved system w

semantrica























































































































comparison of semantrica lexalytics algorithm with difference centrality based measure in which simple lesk used as wsd showing only improved system w

semantrica























































































































top performance by centrality based measures wsd technique used and similarity threshold











































































































precision recall and f score of model mdr model with subgraph centrality





precision recall and f score of model mdr model with pagerank centrality




precision recall and f score of our proposed model when centrality based measure is subgraph centrality


























































































precision recall and f score of our proposed model when centrality based measure is betweenness centrality



















































































xvi
precision recall and f score of our proposed model when centrality based measure is pagerank









































































































precision recall and f score of our proposed model when centrality based measure is evencentrality

































































































precision recall and f score of our proposed model when centrality based measure is

































































































precision recall and f score of our proposed model when centrality based measure is closeness









































































































precision recall and f score of our proposed model when centrality based measure is bonpow











































































































showing pairwise pearson s correlation coefficient symmetric
























showing pairwise spearman s correlation coefficient symmetric





















showing pairwise kendall s correlation coefficient symmetric

























precision recall and f score of our proposed model when relevance is decided by hybridizing subgraph and bonpow





















































































precision recall and f score of our proposed model when relevance is decided by hybridizing subgraph and betweenness












































































xvii chapter introduction to automatic text document summarization
introduction the internet is defined as the worldwide interconnection of individual networks that is being operated by government industry academia and private parties
originally the internet served to interconnect laboratories engaged in government research and since it has been expanded to serve millions of users and a multitude of purposes in all parts of the world
in december only million internet users were present that was
of the world population and in june this became million people that are
of world population
according to an ibm marketing cloud study of the data on the internet has been created since
people businesses and devices have all become data factories that are pumping out incredible amounts of information to the web each day
the size of digital data can be understood by the given statistics
i
since the number of tweets each minute has increased to more than tweets per minute in
ii
every minute on facebook comments are posted statuses are updated and photos are uploaded
iii
google searches are conducted worldwide each minute of every day
iv
worldwide texts are sent every minute v
new data are producing social media users each day
vi
million tweets per day according to internet and mobile association of india imai and livemint s article published on march the number of internet users in india were expected to reach million by june up from million in december
due to all this information is increasing day by day that leads to information overload
this is termed as information glut and data smog
information overload occurs when the amount of input to a system exceeds its processing capacity
decision makers have fairly limited cognitive processing capacity
consequently when information overload occurs it is likely that a reduction in decision quality will occur
information overload can be dealt with up to a certain level by the representation of concise information
as most of the information on the web is textual so efficient text summarizer can concisely represent textual information on the web
radev hovy mckeown have outlined a summary as a text that is produced from one or more texts which conveys important information in the original and that is no longer than half of the original and usually significantly less than that
this simple definition captures three important aspects

the summaries may be produced from a single document or multiple documents

the summaries should preserve important information

the summaries should be short
as explained by alguliev aliguliyev mehdiyev automatic text document summarization is a task of interdisciplinary research area from computer science including artificial intelligence statistics data mining linguistic and psychology
torres moreno has defined an automatic summary as a text generated by software that is coherent and contains a significant amount of relevant information from the source text
sakai sparck jones defines a summary as a reductive transformation of source text into a summary text by extraction or generation
mani maybury states that a summary is a document containing several text units words terms sentences or paragraphs that are not present in the source document
text summarization has various applications in accounting research and efficient utilization of results
a text document summarization based real life system is ultimate research assistant was developed by hoskinson
their system performs text mining on internet search
in other work takale kulkarni shah have highlighted the applications of text document summarization in search engine as google
another system newsblaster proposed by mckeown et al
that automatically collects cluster categorize and summarize news from different websites like cnn reuters
this provides a facility for users to browse the results
hovy lin introduced summarist system to create a robust text summarization system a system that works on three phases which can describe in the form of an equation like summarization topic identification interpretation generation
an application of summarization is news summarization by inshorts app that is shown in figure
and google snippet generation in


showing inshorts app s news
google snippet example broadly summarization task can be categorized into two type extractive summarization and abstractive summarization
abstractive summarization focus on the human like summary
extractive summarization is based on extractive entities entities may be a sentence subpart of a sentence phrase or a word
till now this extractive based summarization relies on standard features like sentence position sentence length frequency of words combination of and as tf idf score selection of candidate word as nouns verbs cue words numbers present in the text uppercase bold letter words sentiment of words sentences aggregate similarity centrality
the goal of feature based summarization either alone or a mix of different strategy is to find salient sentences which can include in the summary
this process can be shown in figure


three step process for text document summarization
flavors of summarization due to overlapping of summarization techniques these can not categorize fairly
in this section we are trying to present a reasonable overview of existing techniques


extractive and abstractive summarization broadly summarization task can be categorized into two type abstractive summarization and extractive summarization
abstractive summarization is a more human like summary which is the actual goal of text document summarization
as defined by mani maybury and wan abstractive summarization needs three things as information fusion sentences compression and reformation
the actual challenge in abstractive summarization is generation of new sentences new phrases along with produced summary must retain the same meaning as the same source document has
according to balaji geetha parthasarathi abstractive summarization requires semantic representation of data inference rules and natural language generation
they have proposed a semi supervised bootstrapping approach to identify relevant components for an abstractive summary generation
a study was done by goldstein mittal carbonell callan state that human generated summary also varies from person to person the reason of this is maybe the setup of the human mind domain knowledge and interest in the particular domain
extractive summarization is based on extractive entities entities may be a sentence subpart of sentence phrase or a word
till now most work is done on extractive summarization because extraction is easy because this is based on some scoring criteria of words sentences phrases and evaluation of extractive summary is easy because it is just based on word counts or word sequences
our work is focused on extractive based technique


single and multi document summarization according to ou khoo goh single document summarization can be defined as a process of representing the main content of one document and multi document summarization also a process of representing the main content of a set of related documents on a topic instead of only one document
there are two possible approaches for multi document summarization in the first approach combine all documents in the single document then apply single document summary
the second possible approach generates a summary for each document then combine all summary into one document later perform single document summarization on the combined summary to get a multi document summary
whereas according to sood it is important to note that concatenation of individual single document summaries may not necessarily produce a multi document summary
since the issue with the later approach first generate single summary and then again combine to generate a summary is that in this process relative sentences position changes and coherence lost and this research gap opens new dimensions in research


query focused and generic summarization a text document may contain several topics like social or economic development political views common people s views environment or entertainment
someone may be interested only in one angle so there is a need for specific text from all given text
to full fill this requirement user may give a query q on document d after certain similarity the system will return desired documents
this is called query focused summarization
first tombros sanderson have proposed query focused summarization for text document to develop an information retrieval system
while white jose ruthven have extended it for use in web document summarization by combining it with other features including text formatting of the page along with query dependent features
the rationale behind their approach with which we concur is that the words in the query should be included in the generated summary
another type of summarizer system is generic summarizer that is regardless of user need


personalized summarization according to dolbear et al
personalization can be defined as this is the technology that enables a summarizer system to harmonize between differently available contents its applications as well as user interaction modalities to a user stated and system s learned preferences
the main objective of personalization is to enable the system for content offerings to be closely targeted user desires
this can be achieved via different methods like content filtering that extract contents appropriate to a user preferences from a set of available content and give recommendations that provides content to a user based on various criteria which may include the user previous acceptance of related content or on the consumption of related content by a peer group


guided summarization this is an extension of query focused summarization but instead of the single question there are set of question
guided summarization may be seen as template based summarization
the template is a set of question that fired on a text document and the system returns a summary in the form of question answers
sometimes this method works well if the developer has good domain knowledge and accurate predictor of the question shortly
if we consider any disaster example then set of question or theme will be based on the cause of the accident how many killed how many are in critical and normal condition relief measure is any political visit held during this and compensation paid to effective people
it leads to the production of the much focused summaries concerning the questions raised


indicative informative and critical summary hahn mani has defined several kinds of summary as indicative summaries follow the classical information retrieval approach they provide enough content to alert users to relevant sources which users can then read in more depth
informative summaries act as substitutes for the source mainly by assembling relevant or novel factual information in a concise structure
critical summaries or reviews besides containing an informative gist incorporate opinion statements on content
they add value by bringing expertise to bear that is not available from the source alone
a critical summary of the gettysburg address might be the gettsyburg address though short is one of the greatest of all american speeches with its ending words being especially powerful that government of the people by the people for the people shall not perish from the earth

state of art approach in summarization according to the state of approaches summarization procedure can be classified into following part this is not limited to that
linguistic structure cohesion has introduced by halliday hasan it captures the intuition
this is a technique for sticking together different textual unit of the text
cohesion can achieve through the use of semantically related terms like coreference conjunctions and ellipsis
among the different cohesion building devices lexical cohesion is the most easily identifiable and most frequent type and it can be a very important source for the flow of informative content
centroid and cluster in this approach documents are divided into several units units may be document itself paragraphs and sentences
based on some criteria some clusters can be created
some famous criteria s are like cosine ngt vector based similarity
after clustering summarizer system picks one unit from each cluster that is considered representative of that cluster and later that added to summary
this approach can be applied for single and documents
machine learning generally we talk about extractive summarization
it is based on either statically based feature or linguistic features or its hybridization
machine learning based summarization is more effective because it learns features weights from given data and later learned weight can be used on test data
only required for this is labeled data
multi objective the main concern about the summary is that it should be more informative and length constraints
informative constraints can be designed by reducing redundancy and increasing coverage
so in this approach mostly all authors designed a function in such a way to reduce redundancy increase coverage and length constraints
later that function can be optimized using different techniques

corpus description in chapter we used a dataset that was created by us details about this dataset are mentioned in section

this experiment also repeated on standard dataset duc
rest of the works relies on only dataset
details about duc dataset is described in section




corpus description for hybrid approach on june was a multi day cloudburst centered on the north indian state of uttarakhand caused devastating floods along with landslides and became the country worst natural disaster
though some parts of western nepal tibet himachal pradesh haryana delhi and uttar pradesh in india experienced the flood over of the casualties occurred only in uttarakhand
as of july according to figures provided by the uttarakhand government more than people were presumed dead uttrakhand flood
corpus is self designed taken from various newspapers ex
the hindu times of india
this dataset is also published in paper c
s
yadav sharan joshi chandra shekhar yadav sharan
here we are showing some statistically and linguistic statics about our dataset used
statistical statistics total no
of sentences in document length of the document after stop word removed total number of distinct words minimum sentence length words maximum sentence length words average sentence length is

in our experiment we used a sql stopword list which is available at
mysql
com doc
en fulltext stopwords
html
by seeing the
we can interpret that sentences are between length and and sentences are between length and

sentence length y axis vs sentence number x axis linguistic statistics in linguistic me are analyzing number and nature of significant entities it is like
nn nnp nns dt jj jjr jjs vb vbn vbd vbz vbg vbp where different abbreviation stands for nn noun singular mass nns nounplural proper noun singular nnps proper noun plural vb verb vbd verb past tense vbg verb gerund vbn verb past participle vbp verb non person singular vbz verb person singular jj adjective jjr adjective comparative jjs adjective superlative dt determinant
note means x is entity type and is its count


duc dataset the document sets are produced using data from the text retrieval conference trec disks used in the question answering track in
this dataset includes data from wall street journal ap newswire san jose mercury news financial times la times from disk and fbis from disk
each set average has ten documents with at least ten words no maximum length is defined
there is single text document abstract for each document with around a hundred words long
the multi document abstract is divided into four parts according to two hundred one hundred fifty and ten words long
each document is divided into four sets and set categories are following

single natural disaster event and created within at most a seven day window

single event in any domain and created within at most a seven day window

multiple distinct events of a single type no limit on the time window

documents that contain biographical information mostly about a single individual

summary evaluation to find a good summary lot of work done but to decide the quality of the summary still a challenging task due to several dimensions like length constraints different writing styles and lexical usage i
e
the context in which used
research is done by goldstein et al
they conclude that even human judgment of the quality of a summary varies from person to person only little overlap among the sentences picked by people human judgment usually does find concurrence on the quality of a given summary
hence it is sometimes confusing i
e
tedious to measure the quality of the text summary
summary evaluation can be done by based and task based both are explained in the sub sections
content based measure evaluate summary on the presence of textual units i
e
n gram in peer summary and standard summary
example of content based measures is rouge blue

content based pyramid



rouge for evaluation most of the researchers are using the recall oriented understudy for gisting evaluation rouge introduced by c

lin and duc has officially adopted this for summarization evaluation model
rouge compares system generated summary with different
model summaries
it has been considered that rouge is an effective approach to measure document summarizes so widely accepted
rouge measure overlaps words between the system summary and standard summary gold summary human summary
overlapping words are measured based on n gram co occurrence statistics where n gram can be defined as the continuous sequence of n words
multiple rouge metrics have been defined for the different value of n and different models like lcs weighted rouge s su with and without lower upper case matching stemming

standard rouge n is defined by
here n stands for the length of the n gram is the number of n grams present in the reference summaries and the maximum number of n grams co occurring in the system summary the set of reference summaries is rouge measures generally gives three basic score precision recall and f score
since the score is not sufficient indicator for summarizer performance so another variation of rouge is rouge n rouge l w rouge s rouge su
in our evaluation we are using six rouge measure l w s and su
taken
since task about to generate single document summary about words so we are evaluating summary of first words
as mentioned earlier in the abstract we are using dataset category two which is about a single event in any domain and created within almost a seven day window as per guidelines
recall and precision defined by following
and
since we are using simple f score so in our evaluation we put in our results we are showing only f score
f score is given by the harmonic mean of precision and recall in
we are representing fuzz f score
pr


rouge n measures n grams uni gram bi gram tri gram and higher order n gram overlap rouge l measure lcs largest common subsequence the advantage rouge l over rouge n is that it does nt require consecutive matches and this does nt define n gram length in prior
if x is reference summary and y is candidate summary and its length is m and n respectively then lcs based precision recall f score can be defined by equation


in duc document understanding conference is set to large quantity as

another variant is rouge s where s stands for skipping bigram
skip bigram allows maximum two words gap between lexical units
this can be understood by an example for the phrase cat in the hat then the skip bigrams are following cat in cat the cat hat in the in hat the hat
y is the number of skip bigram matches between x and y c is combination function is to control relative importance of and
skip bigram based precision recall and score are given by equation


another measure is rouge su it measures skip bigram count between peer summary and standard summary to find out the similarity between these two summaries
this measure is quite sensitive to word order without considering consecutive matches
uni gram matches are also included in this measure to give credit to a candidate sentence if the sentence does not have word pair co occurring with its reference
recall precision and f measure are calculated in the following manner









bleu


pyramid the bleu method was proposed for automatic evaluation of machine translation system
the primary programming task for a bleu implementer is to compare n grams of the candidate with the n grams of the reference translation and count the number of matches
these matches are position independent
the more the matches the better the candidate translation is
two kinds of the summary are generated one is system generated i
e
peer summary and another human generated the summary i
e
reference summary
using reference summary content units scu is find out and using a set of the same words pyramid is constructed
in this evaluation method peer summary contributor i
e
each lexical unit in a summary or scu are matched against scu in the pyramid
the advantage of the pyramid method is that it evaluates summary along with it tells the idea of how the summary is chosen
best result in this method is obtained with unigram overlap similarity and single link clustering
in this whole process to evaluate summary user required many reference summaries

two of six optimal summaries with scus in figure
higher weight scu is placed on the top of the pyramid and less weighted scus of weight is placed at the bottom
this is reflecting the fact that fewer scus are more probable in all the summaries compare to two three and so on


task based they try to measure the prospect of using summaries for a certain task
we mention the three most important tasks document categorization information retrieval and question answering
for a given text document first we have to develop a summarizer system and get a concise summary from it
let summary is s now according to task based evaluation we have to fire queries on s
for example in question answering task for a given set of question we will measure precision and recall of queries response
it will decide the quality of summary and summarizer system


readability in text analysis conference tac and automatically evaluating summaries of peers aesop task the focus was on developing automatic metrics that can measure summary content on the system level
in tac a new task is introduced to evaluate for participant ability to measure summary readability both on the level of summarizers as well as on individual summaries
to measure the readability of the summaries it accessed based on five linguistic based criteria such as grammaticality correctness nonredundancy of lexical units referential clarity focus of summary text and structure and coherence
humans evaluated peer summaries based on these five linguistic questions and assigned a different score on a five point scale one to five where one represents worst and five for the best summary

thesis objective this thesis is divided into six chapters
this work is about extractive summarization techniques with a focus on how semantic features can be used for summarization
first chapter about introduction of text summarization
in second chapter we are proposing a hybrid model for a single text document summarization
this model is an extraction based approach which is a combination of statistical and semantic technique
the hybrid model depends on the linear combination of statistical measures sentence position tf idf aggregate similarity centroid and semantic measure
in this work we will show the impact of sentiment feature in summary generation and will find an optimal feature weight for better results
for comparison we will generate different system summaries using proposed work mead system microsoft system opinosis system and human generated summary
evaluation of the summary will be done by content based measure rouge
in the third chapter we are proposing three models based on two approaches for sentence selection that relies on lsa
in the first proposed model two sentences are extracted from the right singular matrix to maintain diversity in the summary
second and third proposed model is based on shannon entropy in which the score of a latent concept in the second approach and sentence in the third approach is extracted based on the highest entropy
in this work we will propose a new measure to measure the redundancy in the text
in the fourth chapter we will present lexical network based a new method for ats
this work is divided into three different objectives
in the first objective we will construct a lexical network
in the second objective after constructing the lexical network we will use different centrality measures to decide the importance of sentences
since wsd is an intermediate task in text analysis so in third objective we will do an analysis how the performance of centrality measure is changing over the change of wsd technique in an intermediate step and cosine similarity threshold in a post processing step
in the fifth chapter we will present an optimization based criteria for automatic text document summarization
this is based on three steps first preprocessing of sentences and output goes to the second stage that concern about lexical network creation
the output of is network and importance of sentences given by betweenness centrality score
in the final module we will decide some optimization criteria using a combination of centrality and lexical network
to solve this objective criterion we will use ilp integer linear programming to find a solution i
e
which sentences to extract in summary
aligned with the means stated above the objectives of this thesis are as follows proposing a statistical and semantic feature based hybrid model for text document summarization proposing a lsa based model which also captures the linguistic feature of the text proposing a new summary evaluate measure based on information contains to maintain the syntactic and semantic property of the text create a lexical network to find based on the previous objective create a new objective function to optimize and expect a out lexical unit for summarization better summary
chapter hybrid approach for single text document summarization using statistical and sentiment features
introduction in this chapter we are proposing a hybrid method for single text document summarization
that is a linear combination of statistical features proposed in the past and a new kind of semantic feature that is sentiment analysis
the idea is to include sentiment analysis as a feature in summary generation is derived from the concept that emotions play an important role in communication to effectively convey any message
hence it can play a vital role in text document summarization
for comparison we are using different system summaries as mead system microsoft system opinosis system and human generated summary
evaluation is done using content based measure rouge

literature work till now most of the research has done in the direction of extractive summarization based approaches
in extractive summarization the important the task is to find informative sentences a subpart of sentence or phrase and include these extractive elements into the summary
here we are presenting work done in two categories early category work and recent work done
the early work in document summarization has started on single text document by luhn
he has proposed a frequency based model in which frequency of words plays a crucial role to decide the importance of any sentence in the given document
another work of baxendale was introduced a position based statistical model
in his research he has found that starting and ending sentences are more informative in summary generation
position based measure works well for newspapers summarization but is not better for scientific research paper documents
in continuation of position based work edmundson suggests that sentences in the first and last paragraphs and the first and last sentences of each paragraph should be assigned higher weights than other sentences in a document
while kupiec pedersen chen have assigned relatively a higher weight to the first ten paragraphs and last five paragraphs in a document
but radev jing sty tam have followed different positional value position i
e
pi of an ith sentence is calculated using the equation
here n is representing the number of sentences in the document i represents the ith sentence position of the sentence inside the text and cmax is the score of the sentence that has the maximum centroid value
radev blair goldensohn zhang have proposed mead system for single and document summarization
their sentence score depends on three features centroid and position
for each sentence these three features find out and importance of a sentence is decided by the sum of all the features
the position score which proposed by them is linear and monotonic decreasing function
ganapathiraju carbonell yang have considered keyword occurrence as a feature because as per their understanding keywords of the document represent the theme of the document title keywords are also indicative of the theme
they have assigned higher score to first and the last location
uppercase word feature containing proper names are included for summary generation
indicative phrases like this report short length sentences a sentence with the pronoun she they it are used to reduce the score of the sentence and generally not included in the summary
rambow shrestha chen lauridsen have proposed a method for e mail summarization that is based on some conventional feature which is common and used by other authors and some new features
conventional features are an absolute position centroid based on idf length of sentence
jagadeesh pingali varma have divided features into two type sentence level and word level
sentences level features include the position of sentences in the given document the presence of the verbs in the sentences referring pronouns in sentences and length of the sentence in terms of a number of words
word level features include term frequency tf word length parts of speech tag and familiarity of the word
as per their analysis smaller words has higher frequency occur more frequently than the larger words so to negate this effect they considered the word length as a feature for summarization feature
the familiarity of the word is derived from the
wordnet relations
familiarity can indicate the ambiguity of the word
as per author words which have less familiarity were given higher weight
the sigmoid function is used to calculate the importance of the word given by

some other features used by them are named entity tag occurrence in headings or subheadings and font style
the score of the sentence is given by combining all the features
in most of the work it is widely considered that leading sentences are more important compared to preceding sentences
but according to ouyang li lu zhang this is always not true for actual data
importance of sentences varies according to the user and user writing style
instead of sentences position they focus on word position and claims that word position features are superior to traditional sentence position features
they have defined different word position features direct proportion inverse proportion geometric sequence and binary function
finally the score to sentence is given by
where is found using one of the features
karanikolas galiotou have defined features into three category term weighting position and keyword based
term weighting is done for sentence weighting and comprises different ways as local and global weighting
for term weighting he has proposed three different ways shown in following

and








here tij is representing the weight of the jth term in the document di fij is the frequency of the jth term in the document di max fi is the frequency of the most frequent term in document di and fi is the sum of frequencies of the index terms existing in document di
for and standard approach followed
they have introduced new feature ridf that is residual idf
residual idf of a jth term in each document di is defined as the difference between the observed idf expected idf under the assumption that the terms follow a poisson distribution
to give sentences position they have followed a new kind of model that is proposed in the news articles algorithm hariharan
their score method considers both paragraph location and sentence location in the paragraph
where sp is the number of paragraphs in the document p is the position of the paragraph sip is the number of sentences in the paragraph and ssip is the sentence position inside the paragraph
the third kind of feature was title words keywords
the final score is given by combining all the features scores
feature used by luo zhuang he shi were position of the sentence the length of the sentence likelihood of the sentence the number of thematic words the number of low frequency words the lsa score of the sentence the number of two gram keywords number of words appearing in other sentences the entropy of the sentence the relevance of the sentence
they have defined relevance measures as intra sentence relationships between sentences and entropy based feature denotes the quantity of information implied by the sentence
they have mentioned that long sentences are likely to cover a number of aspects in the document compared to short sentences
therefore the long sentence has comparative more entropy than a short length sentence
hence a large entropy of sentence possibly implies a large converge
shimada tadano endo have proposed a method for multi aspects review summarization based on evaluative sentence extraction
they proposed three features ratings of aspects tf idf value and the number of mentions with a similar topic
ratings of aspects were divided into different levels from low to high and the rating has given between one to five

zhang li gao ouyang have considered following words for summarization features like cue words and phrases abbreviations and acronyms non cue words opinion words vulgar words emoticons such as twitter specific symbols and rt
tofighy raj javad have used six features like word frequency keywords in the sentence headline word cue word no of cue word in no of cue word in a paragraph sentence location and sentence length
to give the sentence position scores they have used the following method which gives equal importance to first and last second and second last
position score is represented by the following
padmalahari kumar prasad have proposed the first feature is keyword based
in their work keyword are nouns and determined using
the keyword has found using morphological analysis noun phrase extraction and clustering and scoring
second feature are position based and the third feature is term frequency that is calculated using both the unigram and bigram frequency
only nouns are considered for computing of bigram frequencies
the fourth feature is the length of the word the fifth feature is parts of speech tag in which tags are ranked and assigned weights that are based on the information contribution of the sentence
other linguistic features are a proper noun and pronouns
rautray balabantaray bhardwaj have proposed eight features to score the sentence
the first feature is title feature that is based on similarity overlapping between the sentence the document title divide by a total number of words in sentences and title
the second feature is sentence length longer sentences given more weight compared to small length sentences
the third feature is frequency based the fourth feature is position based that depends on both positions in a paragraph and paragraph s position
the fifth feature is an aggregate similarity the sixth feature is based on counting on proper nouns seventh is thematic word score based on word frequency eight is the numerical data based score
roul sahoo goel have used length of the sentence weight of the sentence that is given by tf idf sentence density presence of named entities in the sentence belong to the number of categories like names of organization locations quantities

presence of phrases in the sentence as in summary our investigation in conclusion the paper describes important the best hardly significantly in particular
relative offset of
the sentence sentences that are located at the beginning or towards the end of a document tends to be more imperative as they carry relevant information like definitions and conclusions
such type of sentences receive a score either or presence of title words in the sentence score of the sentence is given by common textual units between the document title and total number of words in the title presence of specially emphasized text i
e
quoted text in the sentence generally situated within double quotation marks then it receives the score either score else presence of upper case letters in the sentence these uppercase words or phrases are usually to refer to the important acronyms like names and places
such sentences have also received either score else sentence density represented by ration of total count of keywords in a sentence and the total count of words which including all stop words of the sentence
in this section we are describing a different model used for weight learning
in this section we have divided our dataset into training and testing
random forest used predictive modeling and machine learning technique
it is an ensemble classifier made using many decision models
ensemble models combine the results from different models
it is a versatile algorithm capable of performing both regression and classification
this performs an implicit feature selection
according to breiman random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest
the algorithm of random forest is presented by figure
boulesteix janitza kruppa knig
one characteristic of this algorithm is that due to a large number of trees generated in this technique therefore no issue of overfitting and it is always convergent

background

random forest figure
random forest algorithm

binary logistic regression if y be a binary response variable or dependent feature
yi is true or if condition satisfies otherwise yi is false or and xk is set of independent features xi may be discrete continuous or a combination
xi is the observed value of independent ith observation
are to learn
the model is represented by equation


the
shows how parameter estimation done by logistic regression
parameter estimation is done by maximizing equation




features used in text document summarization we are proposing a hybrid model for salient sentence extraction for single text document summarization
this is based on two types of features statistical features i
e
location frequency tf idf aggregate similarity centroid and semantic feature that is sentiment feature
in this section we are presenting detailed features description used in our sentence selection approach


the location feature baxendale has introduced a position feature
although his work was almost manual
later this measure used widely in sentence scoring
the author has concluded that leading sentences of an article are important
the model given by them is explained by
where n is a total number of sentences
the used model is where i n and

the aggregation similarity feature kim kim hwang have defined aggregate similarity as the score of a sentence is as the sum of similarities with other all sentence vectors in document vector space model
it is given by
and

where wik is defined as the binary weight ok kth word in an ith sentence
similarity measure plays an important role in text document summarization
in literature it is proposed that different similarity measure affects the outcome
in our implementation we are using cosine similarity based criteria
let we have two sentences vector
wim and
wjm
standard cosine similarity between si and sj given by

value of i and j vary from to m





frequency feature the early work in document summarization started on single document summarization by luhn at ibm in the
the author has proposed a frequency based model
the frequency of word plays a crucial role to decide the importance of any word or sentence in a given document
in our method we are using the traditional method of tf idf measure defined by
i
e
tf stands for term frequency idf for inverse document frequency
where tfi is the term frequency of the ith word in the document nd represents a total number of documents and idfi is the document frequency of the ith word in the whole data set
in our implementation to calculate the importance of word wi for tf we are considering the sentence as a document and for idf entire document as a dataset


centroid feature radev et al
have defined as a centroid is a set of words that are statistically important to a cluster of documents
as such centroids can be used both to identify salient sentences in a cluster and classify relevant documents
the centroid score ci for sentence si is computed as the sum of the centroid scores cw i of all words appeared in the particular sentence
that is presented in



sentiment feature in previous sections we mentioned statistical measures used by us and in this part we are elaborating semantic based feature
we are calling this feature as a semantic feature because in


this a set of things are related to one other
as defined by mani maybury semantic summary generation may be done using shallow level analysis and deep level analysis
in the shallow approach to the most analysis done on the sentence level is syntactic but important to note that word level analysis may be semantic level
in deep analysis at least a sentential semantic level of representation is done
so our approach i
e
sentiment feature is semantic and low level analysis because at the entity level
for finding sentiment score for a sentence first we have found out all the entities present in the sentence then find sentiment scores of each entity and then do the sum of all entity sentiment score i
e
sentiment strength
if the sentiment of entity is neutral then we are scorning it as zero if entity sentiment is positive then considering as same and adding to find the total score of a sentence but if the sentiment score is negative we are multiplying it by minus one to covert in positive score then adding this score to find the total score given in

reason for considering negative score to positive score is that we are interested only in sentiment strength which may be positive or negative i
e
if sentiment score of an entity if
it means sentiment of entity is negative and strength is

detail procedure is explained in section

s fifth feature
here a representing mode a i
e
a a

summarization procedure our summarization approach is based on salient sentence selection and extraction
the importance of any sentence is decided by the combined score given by the sum of statistical measures and semantic measure
in the next section

we are explaining our approach algorithm used in this work and section

with detail explanation
basically our work of summarization has been divided into three passes sentence scoring sentence extraction and evaluation


algorithm this algorithm is divided into three passes sentence scoring according to linear combinations of different measures
salient sentence extraction summary generation
evaluation of summary

pass sentence scoring input documents output scored sentences step score the sentence given with five different measures
the outcome is m n matrix m no
of sentences n no
of measures aggregate cosine similarity position sentiment of sentence centroid score of the sentence tfidf
step normalized columns of the matrix step add all the features for every sentence we are calling this sum as a score of the sentence
step sort according to score the highest score representing most significant sentence
pass algorithm for redundancy input number of sentences descending according to total score output extracted sentences step summary scored sentence step for to number of sentences parameter initialization empty summary similarity threshold l required length summary
if ith sentence and length summary l then summary ith sentence step rearrange summary sentences as given in source document to maintaining cohesiveness
pass evaluation of summary input different summaries as standard summaries and peer summaries
output precision recall and f score step generate different summary different length using mead microsoft opinosis human humans and our proposed algorithm
for experiment for experiment precision recall and f measure
model summary mead microsoft opinosis peer summary our proposed algorithm model summary human generated summary peer summary mead microsoft opinosis our proposed method step used rouge n to rouge l rouge w we set
rouge s rouge su measure to find figure
algorithm for summarization

detailed approach description here we are describing detail approach used as described in section


pass
sentence scoring and extraction in algorithm defined in section

most things are covered and gives the main idea of algorithm
still some micro points are needs to specify
is the sum of the linear combination of five different measures four are statistically dependent i
e
aggregate similarity position tf idf centroid and the fifth measure is semantically dependent i
e
sentiment
demonstration of working of this model is shown in

the first feature is the position of the sentences
position is an important indicator for informative sentence
it has been analyzed that first or leading sentences mostly contain important information
in our implementation we are using

results are shown in
s second column
the second feature is tf idf
we are using the standard formula as defined in the previous section
normalized tf idf score is given in
third columns
the third feature is an aggregate similarity cosine score of a sentence vector can be calculated as the sum of similarities with other all sentence vectors in document vector space model
the significance of this is to find sentences which are highly similar to all other sentences
after representing all sentences in a vector space and then find vector cosine similarity with all other sentences as a defined standard formula by

normalized aggregate cosine similarity in
column four
since other scores centroid position sentiment are between so we need to normalized score
normalization of values means adjusting to values measured on different scales to one notionally common scale that removes the chance to be biased w

t
some values
in our implementation we are just using column normalization instead of matrix normalization
normalization of a column vector
xn is done using

where xi is the ith element in the column and n is the size of the column



b



let a is a given matrix which size is and column one and two has does have values between then we are doing normalization of only column one and two but not third column and b is the give normalized matrix in our case
the fourth feature is centroid based radev et al
defined as centroid as a set of words that arestatisticall important to a cluster of documents
in our approach using mead centroid score output as our input
the centroid value of a sentence is given by the summation of each word centroid score present in the sentence
the fifth feature is sentiment score
this is a novelty in our work to find this feature we are dependent on alchemy api which is available at
alchemyapi

we have considered that it is finding sentiment score is a semantic approach and fall under shallow level approach as defined in section


for any sentence or words we can define three kinds of sentiment neutral negative and positive
neutral sentiment value means that words or that sentence sentiment score are zero most important to note that it is easy to find sentiment score based on cue word like good bad pleasant
but still due to so much complexity in text words limitation of nlp
it is not possible to find the correct sentiment score
sometimes even it is also not possible to detect sentiment due to hidden sentiments
the overall working of the fifth feature can be understood clearly by following documents
document
naac accredited jnu with the cgpa of
on a four point scale of a grade highest grade by naac and sentiment of this is neutral
document
jnu ranked in top in times higher education asia and brics top ranking and sentiment of this document is positive and the score is

naac stands for national assessment and accreditation council naac and brics stands for five nations brazil russia india china and south africa here
and
both are representing positive news about jnu
but the sentiment of
is neutral and sentiment of
is positive with
score
still we have to discover an approach which can find correct sentiment hidden sentiment
some results are displayed in
with sentiment results
in our implementation to find the sentiment score of a sentence we are using alchemy api first finding all entities present in the sentence and their sentiment score then we add all entity sentiment score
for example consider document number meanwhile bjp spokesperson prakash javadekar has said that party president


has five entities as follow prakash javadekar person name
rajnath singh person name
uttarakhand state county
bjp company
president jobtitle
triplet x y z representing x is entity name y is entity type z sentiment score
and to give sentiment score of sentence we add all so sentiment score
for sentence
but if we see the
result in row and column sentiment score the value is

here we are considering only positive sentiment scores if the entity sentiment score is negative then by multiplying to convert it into positive score
the obvious goal of the procedure is to give equal importance if magnitude is same
let consider one the childhood story about the fox and the grapes in


the fox and the grapes story
if here we consider two sentences given below document
and
as a document both are important in the story and about the same things grapes
document
just these sweat and juicy grapes to quench my thirst and document
they re probably sour anyway
with alchemy system if we find a sentiment of both these sentences then the sentiment of
is positive and the score is
where the sentiment of
is negative with

for us the only magnitude is important reasons to consider as value are we are interested to find sentiment strength it may be negative or positive and both are important for us and if we will add a negative score to find the total score then the value will reduce
in the next step we are finding the total score of a sentence by adding all scores
the total score can be represented by the
given below
in our implementation to n
detail result of the individual score is given in
and the last column is the total score of all scores

different features scores and total score for sentences sent no
position score tf idf aggregate cosine sim
centroid score sentiment score sum of all






















































































pass
redundancy to remove redundancy we have used the same model as proposed by sarkar which the topmost sentence according to total score defined in the equation
is add in summary
we added next sentence in the summary if the similarity is less than threshold
the algorithm is described in section

s input in this pass is a number of sentences which are sorted according to descending total score
we need to initialize some parameter to get the desired summary parameter like summary initially empty given similarity threshold and l for the desired length summary
even in our system l stands for the maximum length of the desired summary but due to the limitation here the length of sentences we can guarantee minimum maximum length summary
we will add the next sentence in summary if still summary length is less than l and similarity new sentence summary
the output of this step is a summary with minimal redundancy and length nearly equal to l but the position of the sentence is zigzag that lost the sequence and cohesiveness
to maintain the sequence we need to rearrange the sentences according to given in the initial index
in
we are representing the summary generated by our system in which the similarity threshold is
and the desired summary length is
we can define arbitrary l in a number of words or percentage of summary required
here we have chosen small
if we put large like
or
then the sentences which are in summary will depend only on the total score as in

in other words the summary only depends on total scores as shown in
but our objective is also to minimize redundancy
note before calculating sentence summary we are eliminating stopwords stopwords play a big role to increase the similarity between two sentences
with different stopwords list we will get different similarity score
mead microsoft and our model generated summary with different length are shown below in



the truth is we do have the model behind microsoft generated summary
this summarizer is inbuilt inside microsoft office package
when we observed that microsoft summarizer is not reducing redundancy sentence and are almost similar sentences it is shown in

pass
evaluation goldstein et al
have concluded two things even human judgment of the quality of a summary varies from person to person human judgment usually does find concurrence on the quality of a given summary hence it is difficult to judge the quality of a summary
for evaluation of any summary we need two summaries first one is a system generated the summary and another summary is user generated model summary or standard summary
to generate different model summary we used three approaches we give our text data set to people and tell them to write a summary in about to to words we generate summary by mead tool in this approach we taking linear combination of position and centroid score with and third model summary is generated by opinosis by ganesan zhai han summary given in figure
microsoft system
to evaluate summary we are using rouge evaluation package
duc adopts rouge for official evaluation metric for both single text document summarization and multi document summarization
rouge finds recall precision and f score for evaluation results
based on gram co occurrence statistic it rouge measures how much the system generated summary machine summary overlaps with the standard summary human summaries model summary
where an n gram is a contiguous sequence of n words
in our evaluation we are adopting different measures of rouge as rouge n to rouge w rouge l rouge s and rouge su

experiment and results in this section we are presenting four experiments done by us
in the first experiment we took our own summary generated by algorithm discussed in section

as system generate summary and another summary as a model summary
in the second experiment we are comparing different system generated summary w

t
human summary
in the third experiment we are showing the significance of sentiment feature
in fourth experiment we are finding best feature weight combination using regression and random forest


experiment
as explained in section

s third pass we have created four types of the model summary human summary via we gave data set to persons to summarize it based on their experience with instruction to summarize it within to words length
due to limitations and user experiences the generated summary varies from to words length mead microsoft and opinosis system
different system generated summaries are given in



since opinosis summarizer is the abstractive type in figure
we are giving summarization result length summary generated by opinosis system



presenting different summaries generated by different systems

our system generated summary using proposed algorithm
microsoft system generated summary
mead system generated summary
opinosis generated summary in the first experiment we took our own summary generated by algorithm discussed in section

as system generate summary and another summary as a model summary
in the next step we find different rouge scores to rouge l rouge w where
rouge s and rouge as defined by c
lin rey and followed by others like sankarasubramaniam ramanathan ghosh
rouge scores are given in with

it measures three things recall precision and f score for any system generated summary and model summary or reference summary
we are comparing our system generated summary with other as model summary same length summary
the result of this is given in

and
for length respectively due to the limitation of space we are providing only three tables

showing f measure with different model summaries of the length of nearly and our summary length is nearly
in simple term we can define high precision means that an algorithm retrieved substantially more relevant than irrelevant and high recall means that an algorithm returns most of the relevant result precision and recall wiki

from

and
for summary length we are getting high precision f score w

t
mead reference summary and high recall w

t microsoft generated a summary

summary generated by our algorithm considered as system summary another summary as a model summary measure mead microsoft opiosis summary r p f r p f r p f

































































rouge l








rouge w








rouge s








rouge su









summary generated by our algorithm as system summary another summary of a model summary measure mead opiosis summary r p f r p f



























































rouge l





rouge w





rouge s





rouge su






summary generated by our algorithm as system summary another summary of a model summary measure mead mead microsoft microsoft opiosis summary r p f r p f r p f r p f r p f





















































































































































rouge l














rouge w














rouge s















precision curve
recall curve
showing f score

experiment
in the second experiment we are comparing different system generated summary w

t
human summary or in other words here model reference summary is a human generated summary and other summaries are system generated summary i
e
mead microsoft opinosis our algo are system generated summary
the result is shown in
and


is representing different rouge scores for the summary length of

summary generated by different as system summary human generated summary as a model summary measure my method mead opinious user r p f r p f r p f r p f























































































































rouge l











rouge w











rouge s











rouge su












summary generated by different system considered as system summary human generated summary as a model summary measure my mead user summary r p f r p f r p f r p f























































































































rouge l











rouge w











rouge s











rouge su











from
we can say that we are getting the high f score comparison to mead microsoft system and opinosis system
except for rouge w in mead s and opinosis rouge w
f score is represented in

we are getting high precision to compare to mead and opinosis but microsoft system leading in rouge l rouge w rouge s rouge su only
we are getting high recall comparison to mead in to and higher recall comparison to opinosis and microsoft in all measures except opinosis getting higher than our system
in
we are representing a comparison of different system generated summary length using f measure and
comparison of length summary and we representing here only f score
from
we can say that mead system and microsoft perform better in term of recall but our system is performing better compared to opinosis
our method gets higher precision to compare to opinsis all rouge score p and higher precision achieved compared to mead except to
we are getting low f score compares to mead and microsoft system but higher w

t opinosis

f score summary
f score summary

experiment
in the third experiment we are showing the significance of sentiment feature
the purpose of this experiment to show is really sentiment score performs a significant role in salient sentence extraction
to generate a good quality summary of limited words like hundred words is a tedious task
in our experiment we have considered five different features
we have tried all combinations of all five features and using this combination we are trying to prove this feature is playing a significant role in summarization
if a number of features are n then with at least one feature the total number of combinations so in our experiment we are trying all combinations calling approaches
here we are generating a summary using single stand alone feature based summary and summary in which sentiment score is playing a role
here first we generate approximate word summary to evaluate this summary we have taken three generated summaries as a gold reference summary
motivated by duc task we are evaluating only the first words of the summary
let stands for tf idf feature stands for aggregate similarity score stands for position based feature stands for the centroid based feature stands for sentiment based score feature showing a collective score of tf idf aggregate similarity position based features

different rouge score for summary generated using different approaches
approach measures

























rouge l












rouge w












rouge s












rouge su












approach measures



















rouge l









rouge w









rouge s









rouge su









here we are presenting different features combination to find a summary
by seeing
we can conclude that position based feature approach highlighted in green is performing best among all
this is due to that in all three human reference summary out of five summary three are extractive type extractive type summary is available at address which are used for evaluation contains top sentences and which is almost words and the model which we are using for score position giving higher preference for leading sentences
it is widely considered that position based score ca perform well in all cases for example in scientific article
so we need some more features
from
this is clear when we are taking sentiment feature i d along with other features we are getting an improved summary
more rouge score means more accurate summary
the conclusion of this experiment is that out of approaches when sentiment feature added nine times we are getting an improved summary by adding sentiment feature
for example if we take collective features and by adding sentiment feature we are getting improved results highlighted in red color
here position based feature performing best among all approaches the reason is given above and we ca depend only on position based feature so we need more features
in approach i
e
aggregate and position when we add sentiment score done in approach i
e
highlighted in blue color the performance is reduced this is due to position based score not preferred i
e
position based feature is not dominating here as in approach
results are obtained from three human summaries as reference summary and summary obtains from different approaches consider as system summary along with document are available at c
s
yadav al

to remove biases and evaluate the first world summary we use i
e
initial word summary and to evaluate rouge w we have taken

we have performed this experiment on duc dataset and after the results are shown in

we have found out that after incorporating sentiment feature the performance has been improved
this experiment is performed with and without stop words

different rouge score is shown on duc dataset rouge location location sentiment location stop word removed location sentiment stopword removed






rouge l
rouge w

rouge s
rouge s









experiment














in this experiment we are finding the best optimal feature weights to get more improved results
to decide the feature weights we are using a supervised that is random forest and logistic regression
in this internal estimates monitor error strength correlation and it is used to evaluate the variable importance
this experiment has been performed on duc dataset
in
we are showing correlation between independent features those are used by us
by visualizing we can say there is no correlation or less correlation between features

showing the correlation between feature used for summarization
we have divided our data into training and testing and develop a model to find best feature weight combination
in this regard we have developed two models random forest and logistic regression
in
features weights are suggested with corresponding model currency
table
showing optimal feature weights using different models model location tf idf centroid random forest



aggregate similarity sentiment
model accuracy






logistics regression from table
we can say that in logistic regression model sentiment score weight is
is the highest feature weight this signifies importance of sentiment is highest here
in logistic regression based model sentiment feature weight is second highest
between these two models random forest and logistic regression logistic regression suggesting feature weights




corresponding to location tf idf centroid aggregate similarity and sentiment with model accuracy

so here we are testing the

and

represented by corresponding equation
and

table
showing performance of model

p r f
int
p
int
r
int
f



































rouge l








rouge w









rouge s














rouge su








table
showing performance o model

p r f
int
p
int
r
int
f



































rouge l








rouge w









rouge s








rouge su








model

is simple linear model which is proposed and tested in previous sections on self made data set
in this section we have implemented that model on duc dataset
in this experiment
we have implemented the previous model as

and new model

after feature weight learning using regression analysis
results are attached in table
and

performance of model is decided based on precision p recall r and f score f and confidence interval shows the probability of lying values of p r f score between given range
figure
shows comparative performance analysis based on content based rouge measure
these tables
and
and figure
give a proof of significant change in results in model

over


performance of model

and





rouge l rouge w rouge s
su p model

r model

f model

p model

r model

f model

figure
comparative performance of model

and model


concluding remark in this work we have presented a hybrid model for text document summarization which is based on a linear combination of different statistical measures and semantic measures
in our hybrid approach we have considered statistical measures like sentence position centroid tf idf as well as a semantic approach doing sentiment analysis that is based on word level analysis
sentiment score of a sentence is given as the sum of sentiment score of every entity present in a sentence
we are getting three polarities for any entity as neutral negative and positive
if any entity sentiment is negative then we are multiplying every score by to treat it as positive score
the reason for doing this we want to select a sentence in which strong sentiment is present it may be either negative or positive and both have the same importance for us
to calculate the score or importance for a sentence we have just added all the scores for every sentence and pick up the highest scoring sentence
in the next step if the similarity between summary and sentences is lower than the threshold to maintain diversity then we have added it in the summary
our stopping criteria is summary length constraints
to generate several summaries of different length we have used methods like mead microsoft opinosis and human generated summary
evaluation is done by rouge measure
in this chapter we have done four experiments
in the first approach we took our proposed algorithm based generated summary as system summary and all other as a model summary
in this experiment it has shown that we are getting high precision almost every time
this signifies that we covered most relevant results
in the second experiment we have compared different system generated a summary mead microsoft opinosis and our algorithm to the model summary human generated
in this we find that our explained algorithm performed well for generated summary for almost every time but in mead system generates a summary leading in some way but here also we are getting higher recall comparatively to mead
in the third experiment section

we have shown that when we are adding sentiment score as a feature we are getting improved results to compare to without a sentiment score
the third experiment is showing that sentiment score has a contribution in the extraction of more appropriate sentences
in the fourth experiment we have divided our data into training and testing and proposed a feature weighted approach using random forest and regression
the logistic regression model is giving better accuracy
since logistic regression is producing better model so feature weights are assigned as per regression model
a new experiment again performed to show the model improvement using feature weight analysis
chapter a new latent semantic analysis and entropy based approach for automatic text document summarization
introduction in this chapter we are proposing latent semantic analysis lsa based model for text summarization
in this work we have proposed three models those are based on two approaches
in the first proposed model two sentences are extracted from the right singular matrix to maintain diversity in the summary
second and third proposed model is based on shannon entropy in which the score of a latent concept in the second approach and sentences in the third approach are extracted based on the highest entropy
the advantage of these models is that these are not length dominating model giving better results and low redundancy
along with these three new models entropy based summary evaluation criteria are proposed and tested
we are also showing that our entropy based proposed model is statistically closer to s standard gold summary
in this work we are using dataset taken from document understanding

background in this section we are presenting an introduction to lsa advantages and limitation of lsa how lsa can be used for summarization and how to find information content by using the principle of information theory


introduction to latent semantic analysis in text mining direction early application of lsa has started by deerwester dumais furnas landauer harshman their objective was indexing of text document now it has been used for various applications in automatic text document summarization
first the objective is to convert a given document d into matrix representation a
matrix a is term document matrix in which elements aij document
this can be designed by combining local and global weight as

represents the weighted term frequency of ith term in ith in the lsa is a vector space approach that involves the projection of the given matrix
matrix amn usually is represented in reduced dimension r denoted by ar such that m
using lsa input matrix a is decomposed as
and


showing mechanism of lsa matrix a decomposed into u and vt matrix where u is an mn column orthonormal matrix which columns are called left singular vector n n is a nn diagonal covariance matrix whose diagonal elements are non negative singular values which are sorted in descending order as in

vt right singular matrix also an orthonormal matrix with size nn which columns are called right singular vector
let r then matrix matrix s properties can be express as
where i i r and r is representing diagonal elements n several methods decomposition of factorization is available like ulv low rank orthogonal semi discrete decomposition and svd
we are using lsa based on svd decomposition the reason of this is simultaneously

svd decompose matrix a into orthogonal factors that represent both types and documents
vector representation of both types and documents are achieved





and





second the svd sufficiently capture for adjusting the representation of types and documents in the vector space by choosing the number of dimensions

computation of the svd in manageable for the large data set
the interpretation of applying the svd to the term by sentence matrix a can be made from two
transformation point of view the svd derives a mapping between the m dimensional space spanned by the weighted term frequency vectors and the r dimensional singular
the semantic point of view the svd derives the latent semantic structure from the document represented by matrix a
there are some obvious reasons for using lsa but sometimes this has some limitations in viewpoints vector space
implementations like this advantage first all the textual units including documents and word have the ability to be mapped to the same concept space
in this concept space we can cluster words documents and easy to find out how these clusters coincide so we can retrieve documents based on words and second the concept of space has immensely fewer k dimensions compared to the designed vice versa
original matrix m i
e

third lsa is an inherently global algorithm but lsa can also be usefully combined with a more local algorithm to be more useful
limitations lsa considers a frobenius norm and gaussian distribution which is not fit for all problems
in our problem our data follow a poisson distribution i
e
depends on word counts as studied by s
m
katz
lsa ca handle polysemy multiple meanings
lsa depend on singular value decomposition svd which is computationally expensive and hard to update if the case of new documents required to add


lsa and summarization lsa is an algebraic statistical technique
svd over given document can be understood regarding independent concepts
svd is a method which models relationships between words and sentences
the beauty of lsa is that it can find out semantically similar words and sentences it has the capability of noise reduction which leads to improved accuracy
our lsa based summarization is presented in
which comprises three major steps input matrix creation svd and sentence extraction selection which is briefly explained in this section
note document and sentence are interchangeable for us sentences are documents

lsa based summarization procedure input matrix creation in most of the papers input matrix creation is based on a bag of word approach or word count approach because it is easy to create as well as widely accepted
input matrix may be considered as a combination of local weight lij and global
local weight criteria define weight in the range of document it may be a sentence paragraph or set of paragraphs and global weight is decided among all documents
let we want to use a bag of word approach and then some common local weighting methods are mentioned in
n number of documents input matrix creation a svd u
s
sentence selection extraction


s
no
local weight approach global weight approach binary if term exists in the binary if term in document i document else term frequency lij tfij the
number of occurrence of term in document j log tfij is the gf idf where gfi is the total number of times term i occur in
number of occurrence of term in the whole collection and is the number of documents in document j which term i occurs
semantic in matrix a with size location aij can connect to location afg if there is any semantic connection available between words m i j g n
semantic connection examples are synonyms anatomy polysemy hypernymy hyponym
where gfi is the total number of times term i occur in the whole collection tfij is the number of occurrence of term i in document j n total documents pij probability of occurrence of ith term in jth document

local and global weight model decomposition lsa uses svd for matrix decomposition
if we have an input matrix a m n number of words a number of sentences
then svd decomposes a matrix decomposed into three matrixes such as left singular u described as words concepts with m n size is a matrix with n n size represents scaling values and v matrix n n size sentences concepts
sentence selection right singular matrix gives information about sentences and concepts if we want information about sentence we can use v
detailed about selection method is presented in related work
some authors like gong liu used s and v both for sentence selection


introduction to entropy and information entropy was originally defined by shannon to study the amount of information contained in the transmitted message
in information theory entropy may be considered as the measure of the amount of the information that is missing from transmitted information and received information and is sometimes called as shannon entropy
the definition of the information entropy can express regarding a discrete set of probabilities
for example in the case of a transmitted message these probabilities p xi were the probabilities that a particular message xi actually transmitted and the entropy of the message system was a measure of how much information is present in the message
let in a message m there are n symbols as



mn are present
this message m transmitted from source s to destination d
suppose further that the source s transmits the symbols mi i n with probabilities


respectively
if symbol mi is repeated t times then the probability of occurrence of mi will be given by tpi
thus in t independent observations the total information i given by

it is assumed that the symbols are emitted independently the average information of symbols i t will be calculated by

and

in our case we are also using the same entropy function






related work in this section we are mentioning five previous different models which have been used for comparison purpose
all these five models have based on lsa based decomposition
each model is presented along with its pros cons


gongliu model this model has been proposed by gong liu
authors have used vt matrix for sentence selection
according to them each row of vt representing one topic concept and corresponding to that topic selects only one sentence which has the highest corresponding value on that topic
they have added extracted sentences in the summary and the process is repeated until the required length summary is achieved
we have observed the following advantages and drawbacks in the
salient drawback new and unique approach
in this approach the summary is depended only on vt as per author convenient they can select some rows i
e
from reduced dimensions of vt
the maximum selected summary sentences will be equal to the reduced dimension r
if a case if reduced dimension r then no way is suggested to select more sentences
top concepts represent more information compared to bottom concepts
sometimes in this approach extraction of sentences that may belong to less important concepts
let system choose an ith concept for sentence selection
from the selected concept if two sentences have high values like
and
then gong approach will choose only one sentence
as we know both are highly related but still the second sentence is not respected in this approach
vt
all selected concepts give equal importance but some may not so much importance in the this is well known that concepts are independent of each other so this is expected that those sentences are extracted from concepts are also independent
ideally it is not possible in text data especially due to linking of name entities stopwords and pronoun words present in the text
in simple words if we want diversity in summary then our matrix a should be noiseless data i
e after removing stop words
so the better output from this approach depends only on the input matrix a


murray model this model proposed by murray renals carletta have used both vt and s matrix for sentences selection
instead of selecting only one sentence from vt which depends on the highest index value selects the number of sentences based on matrix s
the number of sentence selection from one concept depends on getting the percentage of the related singular values over the sum of all values or in reduced space r
we have observed the following advantage and drawbacks in the
overcomes the problem of gong liu approach which selects only one sentence from each concept
some starting values from matrix will play a significant role and will dominate
salient drawback

sj model steinberger jeek have proposed another model for document summarization
they are also using both and v matrix transpose of vt for sentence selection
to add a sentence in summary the author finds the length of a sentence using s and v in reduced space r that is given by

s matrix is multiplied by v matrix to give more emphasis on topics i
e
by property s is sorted in decreasing order and so the first topic is multiplied with higher value compare with later occur topics
we have observed the following advantage and drawbacks in the

salient drawbacks sentence selection is based on new reduced space so consideration about only preferred concepts and highest length sentence
long length sentences may dominate by calculating si
no explicit criteria to increase diversity in the summary
even all sentences are somehow related to all the concepts in preprocessing authors have considered some sentence as unrelated sentences


model another model cross method was proposed by ozsoy alpaslan cicekli in which vt matrix is preprocessed that represents only core sentences as per
remove sentences that have index value less than mean
then vt matrix is multiplied with matrix to give importance for topics shown in
is calculatedces length are calculated by adding columns of vt matrix i
e


highest length sentence is added to the summary this process is repeated until the required length summary
we have observed the following advantages and drawbacks in the
drawback diversity in summary is not considered i
e
no explicit criteria
length dominating summary i
e
long length sentences extracted and added to the summary


model another topic based approach has proposed by ozsoy et al
in that preprocessing is same as done in cross approach w

t
vt i
e
vt is representing only core sentences for each topic

reduces space r

instead of sentence length approach as explained as sj model they have found out important concept
based on those concepts sentence selection is made
the important concept is found by concept concept matrix
this matrix is formed by summing up the cell values that are common to these concepts
we have observed the following advantage and drawbacks in the
salient drawback finding the main topic based on concept concept matrix is giving better results
higher concept score is showing that a concept is much more related to other concepts
still following gong liu approach from sentence selection i
e
select only one sentence from one concept
if some chosen a concept k for sentence selection in this concept if two sentences have high values gong approach choosing only one sentence
with one example if there is a case that concept k is related to
with sentence si and
with sentence
as we know these are highly related then also this is not respected
even some sentences are somehow related to concepts in preprocessing author put this to zero is same as showing that respective sentence and concept are unrelated
this is said that




con n
sign a b showing a is preferred over b property not considered
extracted sentences are assumed to be implicitly diverse


proposed model sentence selection is done from either vt matrix or with a combination of diagonal matrix and vt matrix
as we know the concepts in vt assumed that among extracted sentence have minimum similarity with each other
matrix is independent of each other
so this is inherently singular values are always in sorted order and this is assumed by gong liu concepti is preferred over but approach followed by steinberger jeek cross approach by ozsoy al
and ozsoy cicekli alpaslan are not considering this phenomenon and sentence selection is based on the longest length sentence given by
and
respectively
in both of these approach topics concept with the highest strength is chosen
we are proposing two different approaches first one is based on vector concept relatedness and termed as proposed and the second approach is based on entropy that is further explored and represented with and
before explaining these model or approaches first we are taking an example in section

representing a document as d
creating a matrix a of words sentences
over that matrix a we are applying svd decomposition and this is represented by adecomposed that can further use in different ways to generate a summary
in section

first we are taking a document d in
creating matrix word frequency based matrix a and computing

vt in section

proposed in section

proposed and section

contains proposed


working of lsa with an example in this section first we are representing an lsa based
i
e
sentences are numbered from to and corresponding word sentences matrix a is represented in table

this example is taken from
cc the reason for doing this is it widely available and this example expresses the property of lsa very well in reduced dimensions i
e
representation of given document term document matrix is same as lsa in dimensional space only
this has been shown at the same link via graph representation in

let consider an example with nine given sentences

the neatest little guide to stock market investing

investing for dummies edition
of stock market returns

the little book of value investing

value investing from graham to buffett and beyond

investing in real estate edition

stock investing for dummies
investors miss

the little book of common sense investing the only way to guarantee your fair share
rich dad guide to investing what the rich invest in that the poor and the middle class do not

rich dad advisors the abc of real estate investing the secrets of finding hidden profits most here we considered a document d that is equal to u





u where si representing ith sentence and considering only underlined words book dads dummies estate guide investing market real rich stock value as a set of words
finally based on presence not presence words in sentences input matrix a word sentence matrix is constructed that is given in

rows of a representing the presence of words in a different sentence with frequency and columns representing the presence of words
in this example keyword selection is arbitrary but for keywords selection we can follow many approaches as proposed by beliga metrovi martini ipi sharan siddiqi singh
s s s s s s s s s index words book dads dummies estate guide investing market real rich stock value titles
frequency based words documents matrix a
now svd a u

vt in reduced space r a is given by composition of this matrix in
where u denotes is left decomposed matrix and v for the right decomposed matrix s matrix non negative diagonal matrix

lsa decomposition on a matrix in reduced dimension


mincorrelation model this model is based on and that is using vt right decomposed matrix
as explained in gong liu approach author used vt matrix and only one sentence is selected from each concept
we can fix number to decide k sentences to extract from each concept
in our first we are selecting a concept as done by gong liu
then instead of selecting just one sentence that is highly related to that concept select two sentences in such a way that one most related and second least related to that concept i
e
more related means higher value corresponding to entry concept sentence less related means less value corresponding to entry concept sentence
this approach is to cover two different topics sentences from the same concept
graphically this procedure can be understood by
in a two dimensional space

has four vectors and each vector is maintaining some angular distance from x axis
if corresponding to x axis we want to select two vectors then our choice will be because is closed to x axis and is least related to
corresponding to x axis we want to select two vectors then our choice is if we are interested in selecting three vectors then the selection set will be
in our application
are a reference to documents
if the total number of the vector is n then objective may give by

here co relation may be some similarity measure or other measure but in this work co relation is measured from co occurrence matrix find by vt by lsa

vector representation in d space objective function denoted by
return set of vector sentences those are minimal co related
above given example can be understood by
and

in figure
co relation can be considered as angular distance between vectors


lsacs model lsa based concept selection lsacs is based on which is based on entropy technique
in this model we have to find out concepts that contain more information
how much information is contained in a concept is decided based on entropy
detail about information and entropy are presented in section


concepts hidden latent with high entropy is showing more degree of freedom that means the particular concept is related to the number of dimensions concepts sentences
since concepts are hidden in svd so we can not show these latent but just for consideration if we consider dimensions x y and z as a latent concept then we are interested to find which dimension is more representative anyone and we are finding this using shandon entropy proposed in shannon
in
sentences are represented using triangle sign blue color from to in
to words are represented by star sign red color underlined words in example

the x y and z axis which are assumed as like hidden latent and not possible to show in the physical diagram is represented by the arrow sign in red color
below we are presenting the proposed algorithm
the objective of this to find the one latent axis among x y or z which represents more information

u and vt matrix in d space representing words and sentences
input document d i
e
the content of words sentences paragraphs predefined length of summary initially similarity threshold
output summary
decompose the given document d into sentences

sn extract keywords and use these keywords to
for given document d construct a matrix a i
e
term sentence matrix
a can design by various ways form the master sentence s
we are using tf idf approach

perform the svd on a as
for every sentence in reduced space r by using right singular matrix vt and compute w matrix
columns of w are representing sentences and rows are representing concepts latent
i
similar to gong approach or ii
case similar to approach or
preprocess on w by selecting only core sentences corresponding to concepts by strikeout least related
for each row in w compute where this is representing the probability of sentences

sentence j appear in concept i

find the information contains in each concept using shannon s entropy method as follow section


select most informative concept cp reduce space
using w matrix and selected concept cp select a sentence si that is most related to cp if l lmax similarity si sj i
e
sj summary sentences add sj to the summary and l goto else choose next sentence in
is representing preprocessing on given document d
in step two creations of matrix a is done by here approach is followed by us
in step three svd decomposition is performed on a matrix
svd a decomposed the matrix into three matrices in which right singular matrix represents information about words left singular matrix informs about sentences and is covariance matrix that can be used to decide the importance of parameters i
e
the importance of concepts sentences words
have two cases of how to design w based on two sub approaches given by gong liu i
e
and steinberger jeek i
e
model
in w each row is representing concepts and column for sentences
in
we are demonstrating of w formation that using covariance matrix and vt
in this example for demonstration purpose we are using i
e
reduced dimension three

is representing the relatedness of sentences to corresponding to to reduced dimension r
in based on the negative sign we will eliminate these entries i
e
neglect those sentences which are least informative
this is represented in

in step we are finding the probability of relatedness between sentences and concepts over w
the value in is representing i
e
the probability of jth sentence in ith concept
in each concept all sentences are represented in terms of probability such that where xi denotes ith concept
this is shown in

in finding the information contained in by each concept that is given by the
and in


is representing freedom of concept or information in the concept


in we will choose the highest informative concept and from a selected concept in sentences are added to the summary based on certain cosine similarity criteria matched that is given by

this is iteratively repeated until length constraints satisfied
sentences


























w in reduced space sentences




























w is processed by retaining only positive related sentences and rowsum is calculated to find the probability sentences total entropy w

t
sentences w

t
sentences w

t
sentences



















in reduced space r information contained by each concept and corresponding sentence in this proposed work we are interested to find a concept that is more closely related to all sentences i
e
relatedness is considered in term of probability
valueij indexed at location ith concept jth sentence showing that concepti is related to sentencej with strength valueij
a higher value is representing that a concept is more related to a particular sentence
according to our
proposed approach the concept is more informative if that is related to more number of sentences with high score
for example let a concepti related to sentencej with probability then the information contain in concepti is given by
here n representing refined sentences since entropy is given by the concept that has more negative value contains more information
so we will choose for sentence selection and sentences are selected from this in an iterative way till required length summary achieved
for given an example the order of sentences to include in the summary will be
to resolve a tie between two sentences concepts we are using first come first serve fcfs approach
to maintain the diversity we have set a similarity threshold to add the next sentence in the summary
the similarity may be user defined we are using cosine similarity which is given by

by experimenting we have found if is too small then summary contains too much diversity resulting low performance so we have to set it carefully
in the experiments we chose

we are interested to find cosine similarity between summary sentences and newly selected sentences next to add in summary
here k to m representing the length of master sentence i j for ith and jth sentence if total n sentences


lsass model lsa based sentence selection lsass is also based on entropy based
in this proposed model we have to find sentences that contain the highest information
a sentence with high entropy is showing more degree of freedom that means the particular sentence is related to the number of dimensions concepts sentences
in
sentences are represented using sign corresponding label in rectangle diagrams words are represented by sign with the corresponding label in rectangle diagram
below we are presenting the proposed algorithm


to n

input document d i
e
the content of words sentences paragraphs predefined length of summary initially similarity threshold
output summary
decompose the given document d into sentences

sn extract keywords and use these keywords to form the master sentence s

for given document d construct matrix a i
e
term sentence matrix

perform the svd on a as follow
for every sentence in reduced k by using preprocessed vt and s compute matrix multiplication w vt s
column of w is representing sentences and row representing concepts

preprocess w select only core sentence by strikeout negative related sentences


for each column in w compute where representing the probability of a sentence to appear in latent i i r r reduced space
find the information contains in each sentence using shannon s entropy method as follow
select next most informative sentence sj m if l lmax similarity si sj summary sentences and sj sentence to be added in the summary add sj to the summary and l goto else goto in
to are concerned about preprocessing and svd decomposition
in the next step a new matrix w is obtained by combining the right decomposed matrix v and covariance matrix w vt and further reducing the dimension to
the rows of matrix w signify concepts in the document and the column signifies the sentences of the document
in
we are representing the score of sentences to corresponding to to here
in step the matrix is further reduced by eliminating the rows containing
negative values
here the negative values signify less informative sentences shown in

in we are interested to find the probability of concepts to appear in sentences
we are calculating the probability value of which denotes by j
is representing the probability of a concept i to appear in sentence j shown in

for each sentence the sum of the probability of concepts is as
here xi denotes ith concept
in we are willing to find out information contained in a sentence corresponding to all concepts
it is computed by summing the columns corresponding to the sentence
the resultant value signifies the information contained in the sentences

in the sentence is extracted based on information content and added in the summary
this step will continue until similarity threshold criteria hold and required summary length constraint is satisfied
if a sentencej related to concepti with probability then the information contained will be given by
sentences sentences


























w in reduced space





































w is processed by keeping only positive related sentences and column sum is find to measure the probability sentences w

t




w

w








entropy





information contained by each sentence in reduced space r we can re write
as so entropy can also give by
the sentence that contains more information highest entropy extracted and added in summary after measuring the similarity between summary sentences and selected sentences
so we will include in summary and then up to the required length summary is achieved
for given an example the order of sentences to include in the summary will be
to resolve a tie between two sentences we are using first come first serve fcfs approach
to maintain diversity we can set a similarity threshold to add the next sentence in summary if the similarity between previous sentences and the next sentence is less than
similarity may be user defined we are using cosine similarity which is given by this formula in



proposed model to measure redundancy in summary the text is a collection of units of words the summary has important characteristics like coverage redundancy and summary length
coverage and redundancy are reciprocal to each other when we increase other another reduces
a lot of research like alguliev aliguliyev isazade alguliev aliguliyev isazade alguliev et al
happen in this direction to obtain an optimal solution using multi objective optimization
in our approach we are generating just word summary because words reference summary is available for each file
in this section we are calculating information based on n gram content entropy information from gram to gram
denotes redundant information in text files and it measures using the
by experiments we find that to reach some conclusion we have to consider only if if we consider for then gram will dominate in a text file because of trivial presence
the experiment is done on files so to find average information i
e
redundancy in the dataset we are using
where s is a total number of documents
gram s representing a count of n grams for sth sentence

experiment and results in this section we are presenting a wide number of experiments done by us following



in the first experiment we have implemented all previously proposed models and that are presented in section
and compare their performance in this experiment
its comparative performance in shown in

among these five proposed models highest efficiency w

t
rouge score is given by that is proposed by ozsoy et al
and lowest ranked model is by gong liu
since all five models are based on lsa and it is assumed that concepts are independent of each other so extracted sentences also best representative of the document
even two concepts are independent there may be a number of units words are common to each other
if we are interested in more

coverage or less redundancy we need to update our sentence selection approach
this is done in experiment second using and in section



and


original r l r w
r s r su






























rouge score of previously proposed models


in the second experiment to get more diversity i
e
or less redundancy we have used proposed proposed in section

to select k sentences from the same concepts we set
first we will select a concept then we select only two sentences from each concept one is most related and one is the most unrelated w
r
t
chosen concept latent
this approach is an extension of and follow section
so we presented our approach as and
using
and
we can say that our proposed approach is not only performing better to respective model w

t and w

t
but also is performing better compared to all previous models
results are shown in
and


is showing how much gain in performance by using the proposed approach
rouge score r l r w
r s r su












performance of proposed models with proposed or proposed
showing improved performance using our proposed proposed


next third experiment which is based on entropy based approach second approach as defined in section

and section


have two cases as using vt which is termed as using svt which is termed as
the sole objective of this model is to find a concept that is more related to all the sentences and summary sentences are extracted from this concept latent
in continuation with the third experiment the next model is that is termed as
the objective of this model is to find sentences which are strongly related to all the concepts and that is decided based on entropy
all entropy based model performance is shown in
and we are getting improved results w

t
previously proposed models
exception case and
exception case
we are generating and evaluating only hundred word length summary
since long sentences are representative of more information and extract long sentences so this is length dominating model hence performing better
the entropy based model proposed by us section

and section

is not length dominating so the performance of is sometimes better to compare to our proposed approaches
playing better role compare to based model but not always when the user is interested in length summary
this is true that long sentence contains more information but when repetition selection of long sentences starts there is more chance to have more redundancy and less coverage therefore performance will negatively impact
this is shown in the next experiment
so if we want evergreen model from these existing choices better will be a selection of an entropy based model for summarization problem
the performance of previous approaches and by incorporating entropy in these models is shown by the graph in

rouge measure r l r w
r s r su


















performance of model with entropy based model
showing improved performance using our entropy based proposed


experiment four is showing redundancy and information contained in different summaries generated by different previous proposed models and our proposed approaches
since information is represented by words and in short
is followed
and
we are showing that as we are increasing the number of words in a text rouge score also increasing
sometimes it is reduced because of is increasing and reducing simultaneously follow rouge score formula given by shown by the pair in the bold red letter
we represented information in terms of entropy given in
and

in
the information score represented by
and
is proportional to the rouge score given in

and
which is obvious trivial case and we are trying to show that redundancy is also increasing in text along this which is not our target of summarizer system
words l













































count cov

generally by increasing summary length rouge score is also increasing and sometimes reducing in this experiment we are trying to measure the redundancy of information which is given by

this equation is based on n gram count entropy function explained in section

more score means more redundancy
we are counting n grams only if gram the reason of this by experiment we find that if we take all gram then hard to reach any conclusion
we are following a simple approach giving equal weight to all n grams

rouge score for the entropy based system showing as the number of words increasing rouge score also decreasing
but statistical as n go large like

n then it will contain more information and if the respective count increase than redundancy will increase more i
e
coverage will be less
from this experiment we find out that even is performing better follow
but this contains more redundancy so low coverage shown in
and

model previously proposed models entropy based gold











showing average information contains in summary generated by different systems model previous models proposed entropy based gold peer proposed summaries frequency gram gram gram gram gram gram gram gram gram gram gram gram average count

































average n gram presents in different systems generated summary since rouge score directly depends on gram if redundancy increases rouge also score increases
from
previously proposed models best performer has more redundancy i
e
more number of n grams hence this is performing better which is too much greater than peer gold summary n grams counts


vs


instead of this our entropy based proposed method is close to standard summary
the closeness of two texts can find using
where golden gram refers n gram present in gold summary and system gram refers to summary generated from different systems results are shown in



and
can be summarized in

in this
we are representing only rouge l score calculated for different described systems models and an average count of gram gram gram with
from
we can interpret different results like
our proposed system entropy is performing better compare to other in term of rouge score
the system proposed by ozsoy et al
is performing at second place but redundancy is high compared to allotherrsystemsm
from
we can conclude that statistically our second entropy based proposed approach is closer to the gold peer summary i
e
in term of and and are performing low and redundancy also low
rouge l gram gram gram model score model score model score model score model score gram
rank


















































summarization of




m stands for model
concluding remark in this work we have proposed two new approaches three new models for automatic text document summarization and a novel entropy based approach for summary evaluation
both the approaches for summary generation is based on svd based decomposition
in the first approach we are using right singular matrix vt for processing and selects a concept one by one top to bottom till required
previous approaches are focused on selecting only one sentence of the highest information content
in our first approach we are selecting two sentences w

t each concept such that is highest related to concept and least related to the concept
this approach is based on assumption that by doing this we are covering two different topics
as a result it leads to more coverage and diversity
the second approach is based on entropy which formulate into two different models and
in first we are selecting a highest informative concept and from that concept we are selecting summary sentences
in repeatedly we are selecting highest informative sentences i
e
a sentence which is related to all the concepts with high score
the advantage of the entropy based model is that these are not length dominating models giving a better rouge score statistically closer to the gold summary
during experiment we have found out that rouge score depends only on the count of matched words on increasing the summary length sometimes rouge score decreases and on increasing redundancy rouge score also increases
we have pointed out that rouge score does nt measure redundancy i
e
count matched sentences
we have also realized the need for new measure for summary evaluation that provide a tradeoff between redundancy countmatch and entropy based criteria has proposed
during testing of new proposed measure on different summary generated by previous models and our proposed models we have found that our entropy based summary is closer to standard summary
from the experiment results it is clear that our model works well for summary evaluation especially for higher length summary because as summary length increases redundancy also increases and in this measure we are measuring redundancy
currently we are giving equal importance to all n gram but theoretically and practically we should give more weight to higher n gram because of high redundancy of information in case of repetition
chapter lexnetwork based summarization and a study of impact of wsd techniques and similarity threshold over lexnetwork
introduction in this chapter we are presenting a lexical chain based method for automatic text document summarization
this work is divided into three objectives
in the first objective we are constructing a lexical network in which nodes are representing sentences and edges drawn based on lexical and semantic relations between these sentences
in the second objective after constructing the lexical network we are applying different centrality measures to decide the importance of the sentences
sentences have extracted and added in the summary based on these measures
the third study third objective done in this work is based on wsd
since wsd is an intermediate task in text analysis so we are presenting how the performance of centrality measure is changing over the change of wsd technique in an intermediate step and cosine similarity threshold in post processing step

related work morris hirst have proposed a logical description for the implementation of lexical chain using roget thesaurus and barzilay elhadad had developed the first text document summarizer using lexical chain
their algorithm was exponential time taking
silber mccoy had followed the research of barzilay elhadad for lexical chains creation and proposed a linear time algorithm for lexical chain creation
according to kulkarni apte the concept of using lexical chains helps to analyze the document semantically and the concept of correlation of sentences
using lexical chain text summary generation has three stages first step candidate word selection for chain building as noun verb second step is lexical chain construction and chain scoring model to represent the original document and the third step is chain selection and chain extraction for summary generation
in literature chain scoring strategy is mostly based on tf idf the distinct number of words position in the text
gurevych nahnsen have proposed a lexical chains construction in which candidate words are selected based on pos tagging of nouns and
used
gonzlez fort have used wordnet or eurowordnet lexical databases proposed an algorithm for lexical chain construction which is based on a global function optimization through relaxation labeling
authors have categorized relations into an extra strong strong and medium strong relations
chains are distinguished extracted based on strong medium and lightweight
pourvali abadeh have proposed an algorithm for single document summarization based on two different knowledge source wordnet and wikipedia for words which are not present in the wordnet
erekhinskaya moldovan have used different knowledge source wordnet wn extended wordnet xwn and extended wordnet knowledge base xwn kb
gonzlez fort have used wordnet or eurowordnet lexical databases proposed an algorithm for lexical chain construction which is based on a global function optimization through relaxation labeling
authors have categorized relations into an extra strong strong and medium strong relations
chains are distinguished extracted based on strong medium and lightweight
y
chen wang guan have used chinese wordnet hownet
kulkarni apte have used wordnet relations
y
chen liu wang have used chinese language resources like hownet and tongyicicilin
stokes have used lexical cohesion to generate very short summaries for given number of news articles
they have used different relations and distinct weights are assigned like for extra strong relation repetition assigned
strong relation synonym assigned
strong relation hypernym hyponym meronym holonym antonym assigned
medium strength relation assigned
statistical relation assigned

after that author identifies the highest scoring noun and proper noun chains using the above relations
along with these works another closely related work has done by vechtomova karamuftuoglu robertson and ercan cicekli
the author has considered keywords and sort version of document summary and proposed lexical chain for keyword extraction
steinberger poesio kabadjov jez have used anaphoric information another linguistic task in latent semantic analysis and show anaphoric task giving better results for summarization purpose
chandra shekhar yadav sharan c
s
yadav et al
have used position tf idf centrality positive sentiment and negative sentiment based semantic feature centrality
j
yeh j

yeh ke yang meng have used static information like position positive and negative keyword centrality and the resemblance to the title to generate an extractive summary
along with this lsa and ga also integrated into their work
doran stokes carthy dunnion also have proposed lexical chain based summarization in which the chain score is calculated based on the frequency and relationship present between words chain member using wordnet
the score of word pairs depends on the sum of the frequencies of the two words and multiplied by the relationship score between them
synonym relations have assigned value
specialization or generalization and part whole or whole part
proper nouns chain scores strength depends on the type of match
assigned for an exact match
for a partial match and
for a fuzzy match
sentences have ranked according to the sum of the scores of the words in each sentence and later extracted
medelyan has presented a graph based approach for computing lexical chains where nodes are document s terms and edges reflecting some semantic relations between these nodes
based on graph diameter given by the longest shortest distance between any two different nodes in the graph concept strong cohesive weakly cohesive and moderately cohesive chains are computed
plaza stevenson daz have proposed a summarization system for the biomedical domain that represents documents like a graph designed from concepts and relations present in the umls metathesaurus unified medical language system
jdi algorithm and personalized pagerank algorithm used for wsd
y

chen huang yeh lee have represented the document in graph like structure in which node are sentences and edges are drawn by topical similarity and the random walk applied for summary generation
in the same way xiong ji have proposed a novel hypergraph based vertex reinforced random walk
gonzlez fort have used wordnet or eurowordnet lexical databases proposed an algorithm for lexical chain construction which is based on a global function optimization through relaxation labeling
authors have categorized relations into extra strong strong and medium strong relations
chains are distinguished extracted based on strong medium and lightweight
plsa lda approaches have been proposed by steinberger et al
chiru rebedea ciotec study summarization result based on lsa lda and lexical chain
two most important results from them are lsa and lda have shown the strongest correlation and the results of the lexical chain are not much correlated neither with lsa nor lda
therefore according to them during performing semantic analysis for different nlp applications lexical chains may be used as complementary to lsa or lda
j
yeh have used static information like position positive keyword and negative keyword centrality and the resemblance to the title to generate an extractive summary
along with this lsa and ga are also integrated into their work
steinberger et al
have used anaphoric information another linguistic task in latent semantic analysis and showed anaphoric task giving better results for summarization purpose

background lexical network construction is the first target of this work and during the construction of lexnetwork wsd is an intermediate task
after construction of network we have applied different centrality measures to score the sentences and later extract
in this section we are presenting wsd centrality techniques and lexalytics algorithm used in this work


word sense disambiguation wsd in nature various human language exists and one common problem for all languages is word ambiguity i
e
multiple sense of a word according to the context in which words occurs
wsd word sense disambiguation is an intermediate task and technique in nlp applications to computationally decide which sense of a particular word is active by its use in a particular sense
erekhinskaya moldovan have studied the ambiguity of word originates from a variety of factors like the approach of representation of the word sense and dependency of knowledge source used like wordnet roget thesaurus extended wordnet knowledge source
without knowledge either internal or external it would be impossible for both humans and machines to find the correct sense
wordnet lists five senses for the word pen pen a writing implement with a point from which ink flows
pen an enclosure for confining livestock
playpen pen a portable enclosure in which babies may be left to play
penitentiary pen a correctional institution for those convicted of major crimes
pen female swan
in this work we are using a variant of the lesk algorithm for wsd proposed by lesk
the lesk algorithm is based on the presumption that the given words to disambiguate the sense and neighborhood of this word will tend to share a common topic
since words are ambiguous and their sense relies on knowledge source so in this work we are using three different algorithms for wsd
these are lesk adapted lesk and cosine lesk



simple lesk algorithm a simplified version of the lesk algorithm is to compare the ordinary dictionary definition called gloss found in a traditional dictionary such as oxford advance learner of an ambiguous word with the terms contained in its neighborhood

initialization put words into word vector for which disambiguate sense need select context window i
e
a neighbor of the word

for every possible sense of the word to disambiguate one should count some words that are in both neighborhoods of that word and in the dictionary definition of that sense

the sense that is considered best possible sense which has the highest number of overlapping counts
the advantage of this technique is that this is non syntactic and not dependent on global information the disadvantage of this does use previous sense used i
e
for next word it will compute again so that it is time consuming and performance varies according to the selection of neighboring words



adapted lesk


cosine lesk adapted lesk algorithm has been proposed by banerjee pedersen simple lesk algorithm uses knowledge sources as a standard dictionary for gloss definition wherein adapted lesk algorithm in place of standard dictionary author using electronic database wordnet
wordnet provides a rich hierarchy of semantic relations
the cosine lesk has proposed by tan is a vector space distributional space version to calculate lesk overlaps also known as signatures in the original lesk paper
for example let us take and here the ambiguous word is deposit with two different contexts as below in and sentence i got to the bank to deposit my money
financial institute and withdraw and transact money
the strip by river where soil deposit resident
instead of a global vector distributional space built from a corpus we can do it locally for the given ambiguous word
so given the meaning word deposit and the context sentence we have the vocabulary financial institute deposit withdraw transact money strip river soil resides i go and the following vector will be assigned to meaning meaning and then and is computed
the highest similarity would give us the closest meaning of the word deposit given the context sentence
this is cheaper to compute and there no need for a corpus and also it can be done on the fly with wordnet without storing the vector space in memory before disambiguating our text
but the lack of memory usage would also mean that meaning vectors are computed and recomputed as we disambiguate and that might be wasteful regarding computing resources and time


lexical chain a lexical chain is a sequence of related words in writing spanning short or long distances generally limited to next few lines
a lexical chain is independent of the grammatical structure of the text and in effect it is a list of words that captures a portion of the cohesive structure of the text
applications of lexical chains are keyphrase extraction keyword extraction event detection document clustering text summarization
this can be explained by the following
and

the
showing a lexical chain created for a document available at elhadad
example
rome capital city inhabitant example
wikipedia resource web
example of lexical chain creation dataset available elhadad
mechanism of how to create has been given in barzilay elhadad
chains are constructed between words so it lost the meaning of the text and grammatical structure
there is no way to find out that particular word belongs to how many sentences
to sort out these issues we have proposed a lexicalnetwork that overcome all these problems


lexalytics algorithms lexalytics offers text mining software for companies and enterprises around the globe which is available at
lexalytics

this system can access through java or python api and excel add ons
text analytics tasks like named entity extraction finding the meaning of text via classification and tagging sentiment analysis context finding and summary generation
lexical chaining is an intermediate step in mentioned applications
various lexalytics algorithms heavily rely on it like summarization
in lexical chaining nouns are selected for lexical chain construction
if the nouns are related to each other we can find that the conceptual lexical chain in the given content even when many other unrelated sentences separate those sentences
the score of all lexical chain is based on the length of the chain and the relationships lexical or semantic between nouns
in this system location based features also considered deciding sentence priority because of an initial sentence always more informative
relation used for chain construction purpose are same word antonym synonym meronym hyper holonyms

in this system even if those sentences are not adjacent to each other in the text they are lexically related to each other and can thus be associated with each other
during summary generation it extracts best sentences from the chain and shifts out nonessential sentences from the summary


centrality a degree centrality centrality scores
in graph theory or social network analysis to decide the relative importance of a node one existing approach is centrality based measures
some used approached are mentioned by us
the first and simplest type of centrality is degree centrality w
n
venables team
if we define a graph e where is some nodes and stands for the number of edges then on graph g degree centrality of a node v can be defined as the number of edges incident towards a node
graph g may be directed and undirected if g is di graph then two type of degree is defined as in degree and out degree
for a given node v v v in degree of a node v is some links approaching towards the v and out degree of v is the counts of links that the node v directs towards other nodes v v
the degree centrality of a vertex v for a given graph g is defined by
general degree centrality and
normalized centrality n is the total number of vertices in
two graphs are shown with normalized degree deg


showing normalized degree centrality for an undirected graph b eigen value centrality in linear algebra an eigenvector of a square matrix let a can define as a vector that does not change its direction under the associated linear transformation
alternatively it can be defined as if v is a vector then it is an eigenvector of a square matrix a if av is a scalar multiple of v and
this condition can be written as the following
where is the scalar known as the eigenvalue associated with the eigenvector v
a more advanced version of the degree centrality is eigen vector centrality
degree centrality of a node is just based on a simple count of the number of connections
where eigenvector centrality acknowledges that not all connections are equal or in other words the eigenvector centrality defined in this way accords each vertex a centrality that depends both on the number of connection i
e
degree of a node and the quality of its connections
in social life this is followed connections to people who are themselves influential will lend a person more influence compare to in connections with less influential persons
if we denote the centrality of ith vertex by xi then we can allow for this effect by making xi proportional to the average of the centralities of ith network neighbors newman represented in



c closeness centrality in graph theory closeness is a one of the centrality measures defined for a vertex in the graph g v e
vertices which are shallow short geodesic distances to other vertices have given higher closeness value
closeness can be defined in a various way as defined by freeman the closeness centrality of a vertex is defined by the inverse of the average length of the shortest paths to from all the other vertices in the graph given below by
d alpha centrality if we denote an adjacency matrix a in which aij equivalent to aij means i contributes in j s status and if is a vector of centrality scores then this can be written by
in alpha centrality status score of an individual can be seen as a function of the status of those who choose him given by upper
which can rewrite as
at is denoting the transpose of matrix a
in short alpha centrality s status is a linear function of another node to which node is connected
this could be understood with two examples first in a community power study an actor s status value is increased much if he she nominated from those who themselves are receiving more nominations compare to other
same as an upper example in a school also a student s popularity is increased more by receiving voted from other students who are themselves more popular w

t other students
one solution to this problem is solution eigenvector centrality but this does produce justifiable results for the networks given in and with


ax ax








graph
showing a graph with some nodes and edges a possible solution of this kind problem is possible if we allow every individual here students with some score that does nt depend on its connection to others
like in a class or school each student s popularity that depends on its external status characteristics
let e be a vector of these exogenous sources of status or information then we can replace the
with a new
in which the parameter reflects the relative importance of endogenous versus exogenous factors in the determination of centrality
from
this has the matrix solution
in
s for any value of position of labeled vertex a is the most central
in the order is xa xb xc xd xe for any
the alpha centrality based measure is almost identical to the measure has proposed by l
katz
katz has suggested that influence could measure by a weighted sum of all the powers of the adjacency matrix a here powers of a gives indirect paths connecting points
e betweenness centrality in a graph e betweenness centrality given by freeman a measure of a vertex is based on the shortest path metric
vertices v v those occur in some shortest paths between other tx

vertices have higher betweenness centrality compare to those do nt
for a graph g with vertices the betweenness for a vertex is computed by
in
denotes the counting of shortest paths from node s to node t and is the total number ofpassedst paths from node s to node t that is passing via a vertex v
f bonacich s power centrality bonpow let given matrix a is an adjacency matrix then bonacich s power centrality has proposed by bonacich is defined by
where is an attenuation parameter from
is the reciprocal of the which is the largest eigenvalue of adjacency matrix a this is to within a constant multiple of the familiar eigenvector centrality score for other values of else than the behavior of the measure is quite different
in particular gives positive and negative weight to even and odd walks respectively as can be seen from the series expansion in

which converges so long as
holds from w
n
venables team the magnitude of controls the influence of distant actors on ego s centrality score with larger magnitudes indicating slower rates of decay
high rates hence imply a greater sensitivity to edge effects
interpretively the bonacich power measures correspond to the notion that the power of a vertex is recursively defined by the sum of




the power of its alters
the power exponent controls the nature of the recursion involved positive values imply that vertices become more powerful as their alters become more powerful as occurs in cooperative relations while negative values imply that vertices become more powerful only as their alters become weaker as occurs in competitive or antagonistic relations
the magnitude of the exponent indicates the tendency of the effect to decay across long walks i
e
higher magnitudes imply slower decay
one interesting feature of this measure is its relative instability to changes in exponent magnitude particularly in the negative case
g hub authority hyperlink induced topic search known as hubs authorities developed by kleinberg is a type of link analysis algorithm that rates web pages
a set of web pages can consider as a connected graph
the algorithm assigns two different scores hub and authority score for all pages
the authority score estimates the value of the content of the particular page on a node in the graph and its hub score estimates the value of its links edge in the graph to all other pages
in other words this can interpret as that a good hub represents a web page that points too many other pages and a good authority represent a page that was linked by many different hubs
the hits algorithm relies on an iterative method and converges to a stationary solution
each node i in the graph is assigned two non negative scores an authority score xi let and a hub score yi let
the xi and yi are initialized with any arbitrary nonzero value and scores will update according to iterative ways present in
and
later the weights are normalized as
h subgraph centrality






let g v e is a graph then any subgraph e will hold this v v and e e
estrada rodriguez velazquez a centrality measure which based on the participation of each node in all subgraphs in the network
in their work smaller sub graph gives more weight compared to larger ones which makes this measure appropriate for characterizing the network motifs define as designates those patterns that occur in the network far more often than in random networks with the same degree sequence by freeman
degree centrality can be considered like as direct influence but this is unable to cover long term relations or in another word indirect influence in the network l
katz
there is another centrality based measures like betweenness centrality closeness centrality but these measures are justifiable in a connected network because the path distance between unconnected nodes is not defined
eigenvector centrality ec also do nt depend on to paths
ec measure simulates a mechanism in which each node affects all of its neighbors simultaneously bonacich but the eigenvalue based centrality ec measure can not consider as a measure of centrality in which nodes are ranked according to their participation in different network subgraphs
let in
which is regular graphs with eight vertices
vertices set is forming part of a triangle part of three squares and form part of only two and the rest do not form part of any
these groups are distinguishable according to their participation in the different subgraphs although ec can not distinguish them

showing eight node with degree regular graph subgraph based centrality is based on the number of closed walks starting and ending at the node
closed walks are weighted such that their influence on the centrality decreases as the order of the walk increases
in this technique all closed walk is associated with a connected subgraph which points out that this measure counts the times that a node takes part in the differently connected subgraphs of the network
here smaller subgraphs have higher importance corresponding to larger
the subgraph centrality is represented via graph angles as noticed by estrada velazquez given in
according to
if the graph is walk regular then every node has an identical subgraph denote the nodes of the graph g by


n
let m where m be the distinct eigenvalues of given adjacency matrix a with multiplicities respectively and
be the basis of the eigenspace corresponding to the eigenvalue i



m
above
defines the angle alpha corresponding to the node u of g and the eigenspace
according to gleich due to several features of rage rank algorithm simplicity guaranteed existence generality uniqueness and fast computation make this technique applicable far beyond its origins in google web search proposed by page brin motwani winograd
pagerank is used as a network centrality measure proposed by koschtzki et al
and our work also motivated from this work
if we assume that a given page and other pages




tn which pointing towards page called citations
the parameter d is a damping factor which can be set between to usually set to

is defined as the number of links going out of page
the pagerank score of page is given by
centrality
i pagerank our used variation is given by












pagerank technique forms a probability distribution over web pages or in the graph such that the sum of all web pages pagerank score will be one

proposed work in this section we are presenting just a layout of our which is four step process including preprocessing lexical network creation sentence raking i
e
computing importance of each sentence and summary generation based on required length
the first step is preprocessing this step is required for proper sentence read because we can not set an arbitrary delimiter for a regular expression like just
or
a dot followed by the new line new tab
to get the better accuracy we transform some special form like u
s
to the u
s
deleted just bank space in the form of

the second step takes the input from the output of the first step it reads sentences one by one and creates a lexical network among all sentences
detail about this is presented in algorithm in next section



procedure procedure procedure
steps in summarization in the third step we are applying different graph based centrality measures for finding informative sentences and in the last step we set some constraints like threshold summary length for summary generation according to a user request


algorithm in this section we are presenting algorithm for a summary generation in three main steps basically four as per previous figure
but in this section step and step are merged in step of this algorithm
algorithm
lexical network construction in this section we are presenting the summarization algorithm proposed by us
our work comprises three steps in the first step is preprocessing done which is a most important step in text mining that can improve harm the efficiency of the algorithm in step second we are creating lexical network
meanwhile wsd is done with the help of the lesk algorithm and in step sentence scoring and sentence extraction was done
input collection of sentences output extracted sentences procedure preprocessing proper fragmentation of sentences set proper delimiter
tagging of sentences using pos tagger
procedure in the document we are given some sentences if we apply any regular expression for proper bifurcation of sentences is not proper possible because of some words like mr
dr
u
s
so for proper sentence extraction we are doing some preprocessing and then we are tagging these sentences using nltk natural language tool kit parser
the result of this step is stored in list format in sentence variable which can be randomly accessed i
e
like array
procedure lex network creation step initialize matrix step for ith i for each actual sentence extract nouns verb called modified sentences contains only significant units for ith modified sentences locating corresponding same sentence takeith unit to word ith unit correct sense word ithactual sentence for modified sentences takejth unit j if relation correct sense jth unit then procedure sentence scoring and extraction paired sentence centrality score sort paired sentences based on centrality score set similarity measure threshold and summary extract ith sentence from pool after step diversity maintained pick jth sentence from summary pool ith add ith sentence to summary procedure in this procedure we are constructing a lexical network of nn matrix
where n is some sentences i
e

we are picking each sentence on by one and extracting only nouns like a proper noun verbs from this sentence because only these provide information and this is added to the modified sentence example from duc healthcare data set file number
txt actualsentence an administration task force has been studying possible health care solutions but has yet to offer a comprehensive proposal
taggedsentence administration nn task nn force nn studying vbg possible jj health nn care nn solutions nns offer vb comprehensive jj proposal nn modifiedsentence administration task force studying health care solutions offer proposal
the actual lexical network is construed for modifiedsentences here
we are finding is any relation either lexical or sematic is present between two modifiedsentence
to find the relation present between sentences we need to disambiguate the sense of the word this is done with the help of lesk algorithm
here lesk algorithm takes two arguments one word to disambiguate sense and other is ith actual sentence
based on that sense this algorithm finds hyper hypo synonyms meronym and antonym relation present in other remaining sentences
at a time only one relation will be available and according to us every relation is given the same weight
so we will assign an edge between sentences and each time increase by one if more relation is present between sentences
in
we are showing lexical network connection between sentences
txt from duc dataset
last sentence is empty so there is neither lexical nor semantic similarity available so corresponding to to entries are zero vice versa
corresponding to

showing a graphical view of this lexical network in which node are sentences and edges are a corresponding total similarity
note when we will split any text with delimiter dl if the total number of delimiter are n then after split operation number of segmentation will appear so here









showing all sentences of
txt from duc dataset for the creation of network washington health and human services secretary louis sullivan called for a summit with the chief executives of major insurance companies to discuss ways of paring the administrative costs of health care
but in a speech here dr
sullivan indicated that the bush administration is likely to put forth a broad legislative proposal to overhaul the country health
rather he advocated focusing on ways to improve the current system
administration health officials said the meeting with insurers tentatively scheduled for nov
is likely to commence a series of discussions with players in the nation billion health care system on problems of cost and access
administrative costs like excessive paper work have burdened the health care system with billions of dollars in unnecessary costs many observers care system
believe
some studies have put the price at more than billion a year but hhs officials believe it is more like billion to billion
while numerous health care reform proposals have been introduced in congress this year mostly by democrats the comments of dr
sullivan and other hhs
officials yesterday suggest the bush administration is interested in striving for systematic revision and will push for limited fixes and incremental changes i will not propose another grand sweeping speculative scheme dr
sullivan said
rather i want a public debate to focus on some immediate practical options that address our most urgent healthcare concerns

some of the options he said are making health insurance more affordable to small businesses

easing barriers to high quality cost effective managed and coordinated care

researching the effectiveness of various medical procedures to encourage the use of the most cost effective ones

altering the tax code to increase consumer awareness of the true cost of health care and distribute current tax subsidies more equally
among other things administration officials have been looking at the possibility of limiting the tax exemption for employer paid health insurance premiums
increasing the availability of primary care to the neediest people

reducing the administrative costs of health care

the u
s
spends more per capita on health care than any country yet more than million americans lack health insurance

in january president bush ordered an administration task force to study problems of health and access but it has yet to propose solutions
dr
sullivan repeated his dislike for the two most widely discussed health care revision proposals establishing a nationwide federally sponsored health care system similar to canada or mandating employers to either provide basic health benefits to workers or pay a tax to finance a public health plan

such approaches he said would be inflationary smother competition and lead to rationing and waiting lists

he said experimentation in health care reform should be left to the states
local solutions for local problems should be our working philosophy as should learning from local mistakes in order to avoid harm to the nation as a whole
he said


showing lexical and semantic relations present between sentences sent

in graphical form directed graph
refers to sentence from
and from
corresponding weighted edges between two sentences are shown here
procedure after getting a lexical network we are applying centrality based measures for sentence scoring
sentence with high centrality score is decided more important compared to low centrality score
to extract sentence for summary generation we decide cosine similarity threshold we add the next sentence in summary if x x summarysentence cosine similarity x
this work is extended with the impact of different wsd techniques and different similarity thresholds in the post processing step
to find the similarity between two sentences we are using cosine similarity
if we have two vectors a and b each of length n then cosine similarity is given by

ai and bi are a component of vector a and b this similarity measure also used in c
s
yadav al
chandra shekhar yadav sharan
in graph theory or network analysis is to decide the relative importance of a node or to find central figures one existing approach is centrality based measures
for example in the social network to decide the importance of a person can do with the help of centrality
some based measures are degree centrality eigenvector centrality katz centrality pagerank betweenness centrality closeness centrality alpha centrality bonpow centrality hub authority subgraph centrality
detailed description of different centrality measure used by us are
experiments and results in this section we are presenting different experiments performed by us along with result analysis
first we are presenting performance of different centrality measure over selected lesk algorithm with different threshold then impact of wsd technique and threshold for different centrality measures



performance of different centrality measure over selected lesk algorithm with different threshold in this section we are showing the performance of different centrality based measures over fixed wsd algorithm like adapted lesk cosine lesk and simple lesk respectively and different cosine similarity threshold in post processing
we have selected a cosine similarity threshold range
due to the limitation of space and sufficient need we are interested to show result only for and threshold



adapted lesk th in
we are using adapted lesk algorithm for wsd for lexical network creation and in the third step we set a cosine similarity threshold of
after lexical network construction procedure in algorithm we have applied different centrality measure like alpha centrality coded as for different values
from the corresponding graph of
in
we can interpret that in alpha centrality when

performance is continuously increasing but when then the performance of summarizer system is strangely reduced
eigen value and hub authority are performing equal with the highest performance and second highest performance is done by subgraph based centrality

performance of different centrality measures using adapted lesk as wsd and similarity threshold score








bonpow between close eigen page sub ness ness value authority rank graph
















rouge l
















rouge w
















rouge s
















rouge su

















performance evaluation using rouge score of different centrality measures adapted lesk as wsd and similarity threshold


adapted lesk th in
we are using adapted lesk algorithm for wsd for lexical network creation and in the third step we set with similarity threshold
after lexical network construction procedure in algorithm we have applied different centrality measure like alpha centrality coded as for different values
from the corresponding graph of
in
we can interpret that in alpha centrality when

performance is continuously increasing exception
but when then the performance of summarizer system is reduced compared to this
eigen value and hub authority are performing equally with second highest performance and subgraph based centrality achieves the highest performance

performance of different centrality measures using adapted lesk as wsd and similarity threshold score








between bon close eigen page sub ness pow ness value authority rank graph
















rouge l
















rouge w
















rouge s
















rouge su

















performance evaluation using rouge score of different centrality measures adapted lesk as wsd and similarity threshold


cosine lesk th in
we are using cosine lesk algorithm for wsd for lexical network creation and in the third step we set with cosine similarity threshold
after lexical network construction procedure in algorithm we have applied different centrality measure like alpha centrality coded as for different values
from the corresponding graph of
and
we can interpret that in alpha centrality when

performance is continuously increasing exception
but when then the performance of summarizer system instantly reduced
eigen value and hub authority are performing equal third highest with page rank second highest and subgraph based centrality top performance but the thing here to notice that all these measures are performing almost same

performance of different centrality measures using cosine lesk as wsd and similarity threshold score








bonpow between close eigen page sub ness ness value authority rank graph
















rouge l
















rouge w
















rouge s
















rouge su

















performance evaluation using rouge score of different centrality measures cosine lesk as wsd and similarity threshold


cosine lesk th in
we are using cosine lesk algorithm for wsd for lexical network creation and in the third step we set a cosine similarity threshold
after lexical network construction procedure in algorithm we have applied different centrality measure like alpha centrality coded as for different values
from the corresponding graph of
and
we can interpret that in alpha centrality when

performance is continuously increasing exception
but when then the performance of summarizer system is strangely reduced
eigen value and hub authority are performing equal with the highest performance and second highest performance is measured by subgraph based centrality

performance of different centrality measures using cosine lesk as wsd and similarity threshold score








bonpow between close eigen page sub ness ness value authority rank graph
















rouge l
















rouge w
















rouge s
















rouge su

















performance evaluation using rouge score of different centrality measures cosine lesk as wsd and similarity threshold


simple lesk th in
we are using simple lesk algorithm for wsd for lexical network creation and in the third step we set a cosine similarity threshold of
after lexical network construction procedure in algorithm we have applied different centrality measure like alpha centrality coded as for different values
from the corresponding graph of
in
we can interpret that in alpha centrality when

performance is continuously increasing exception
but when then the performance of summarizer system is strangely reduced
eigen value and hub authority are performing equal with third highest performance pagerank gives the second highest performance and subgraph based centrality measures the highest performance

performance of different centrality measures using simple lesk as wsd and similarity threshold score








between bon close eigen page sub ness pow ness value authority rank graph
















rouge l
















rouge w
















rouge s
















rouge su

















performance evaluation using rouge score of different centrality measures simple lesk as wsd and similarity threshold


simple lesk th in
we are using simple lesk algorithm for wsd for lexical network creation and in the third step we set a cosine similarity threshold
after lexical network construction procedure in algorithm we have applied different centrality measure like alpha centrality coded as for different values
from the corresponding graph of
in
we can interpret that in alpha centrality when

performance is continuously increasing exception
but when then the performance of summarizer system is strangely reduced
eigen value and hub authority are performing equal with third highest performance pagerank measures second highest performance and subgraph based centrality measures the highest performance

performance of different centrality measures using simple lesk as wsd and similarity threshold score








between bon close eigen page sub ness pow ness value authority rank graph
















rouge l
















rouge w
















rouge s
















rouge su

















performance evaluation using rouge score of different centrality measures simple lesk as wsd and similarity threshold


impact of wsd technique and threshold for different centrality measures in this section we are presenting different wsd technique and different threshold impacts in summarization over the subgraph based centrality
here code stands for adapted lesk technique for cosine lesk and technique for simplified lesk algorithm where threshold means cosine similarity threshold and means cosine similarity threshold is



subgraph based here we are presenting the performance of subgraph based centrality in different conditions
subgraph based centrality put more emphasis on small subgraphs and this covers long term relations
here we are presenting the subgraph based centrality measure with different wsd techniques and thresholds
from
and
we can conclude that simple wsd technique adapted lesk and cosine lesk with threshold is giving a better results followed by simple lesk adapted lesk cosine lesk with threshold

impact of wsd technique and similarity threshold over subgraph based centrality score
rouge l
rouge w
rouge s




rouge su

















impact of wsd technique and similarity threshold over subgraph based centrality







eigenvalue centrality based analysis eigen value centrality depends on both numbers of connections and quality of the connection
from
and
we can conclude that simple wsd technique with threshold cosine lesk adapted lesk and simple lesk but for threshold cosine adapted simple lesk is performing in an order higher to lower

impact of wsd technique and similarity threshold over eigen value based centrality score





rouge l





rouge w





rouge s





rouge su






impact of wsd technique and similarity threshold over eigen value based centrality


page rank page rank is simple ensure the guaranteed existence of a solution and it is fast in computation
it forms a probability distribution over a network
in our experiment we have set dumping factor

from
and
we can conclude that simple wsd technique with threshold simple cosine lesk and adapted lesk but for threshold adapted lesk simple lesk and cosine are performing in order highest to lowest

impact of wsd technique and similarity threshold over page rank as a centrality measure score





rouge l





rouge w





rouge s





rouge su






impact of wsd technique and similarity threshold over page rank as a centrality measure


bonpow in bonpow centrality measure the importance of a node is decided by alters
from
and
we can conclude that simple wsd technique with threshold adapted lesk cosine lesk and simple lesk but for threshold adapted lesk simple lesk and cosine are performing in order highest to lowest

impact of wsd technique and similarity threshold over bonpow based centrality score





rouge l




rouge w




rouge s







rouge su






impact of wsd technique and similarity threshold over bonpow based centrality


betweenness betweenness centrality measures the importance of each node based on a factor of participation of node between shortest paths any nodes in a network
from
and
we can conclude that adapted lesk algorithm with a threshold of is performing better than adapted lesk algorithm with threshold
cosine lesk simple lesk cosine lesk simple lesk threshold

impact of wsd technique and similarity threshold over between ness centrality score





rouge l





rouge w





rouge s





rouge su






impact of wsd technique and similarity threshold over between ness centrality


closeness closeness centrality measure importance of node based on the shortest path from that node to all other nodes
from
and
we can conclude that with threshold cosine lesk simple lesk and adapted lesk for threshold also cosine lesk simple lesk and adapted lesk are performing in order highest to lowest

impact of wsd technique and similarity threshold over closeness centrality score





rouge l





rouge w





rouge s





rouge su






impact of wsd technique and similarity threshold over closeness centrality


alpha the alpha centrality score depends on the importance of neighboring nodes
in
we have shown different rouge score using alpha centrality
to for ranking of sentences extraction and summarization
from
this is clear that when alpha is between
to
including this is unpredicted
and
no pattern is achieved
but when alpha is
to
including rouge score is increasing with some exception at alpha
but at efficiency is again reduced for all
note for adapted lesk for cosine lesk for simplified lesk algorithm means cosine similarity threshold means threshold

impact of wsd technique and similarity threshold over alpha centrality
alpha


comparisons with lexalytics algorithms semantrica lexalytics is a system available at
lexalytics
that can access through java and python api for different tasks like summarization sentiment entity
the summarization task of this system depends on lexical chain creation
the lexical chain is created using semantic and lexical relations location also gives priority in this system
in this section we are describing that where system proposed by us lead by semantrica
even this is performing better many times and many times our proposed methods lead this is shown in


and


comparison of semantrica lexalytics algorithm with difference centrality based measure in which adapted lesk used as wsd showing only improved system w

t semantrica adapted similarity threshold adapted similarity threshold lexalytics

eigen value authority page rank subgraph


authority page rank subgraph

































































comparison of semantrica lexalytics algorithm with difference centrality based measure in which cosine lesk used as wsd showing only improved system w

t semantrica score cosine lesk as wsd and similarity threshold cosine lesk as wsd and similarity threshold lexalytics eigen value authority page rank subgraph
eigenvalue authority page rank subgraph







































score l w
rouge s su l w
rouge s su










comparison of semantrica lexalytics algorithm with difference centrality based measure in which simple lesk used as wsd showing only improved system w

t semantrica subgrap

h authorit page ran subgrap h simple lesk as wsd and similarity simple lesk as wsd and similarity score sematrica lexalytics rouge




rouge rouge
rouge s rouge






















authorit y page ran eige n valu e





















eige n valu e
y




























semantrica lexalytics vs our proposed system improved only

overall performance from





and





this is clear that when alpha to
we are unable to decide the performance pattern but when

performance is increasing sometime exception is
and again reduced at
most of the time subgraph based centrality is giving a better result the reason of this subgraph measure gives importance to subgraph in the graph the smaller subgraph is given more importance and this is natural that in the large graph there will be some subgraphs cycles
in
we are showing some top performances of all three wsd techniques for different threshold here and in which subgraph based centrality is performing better
it may look hard to interpret x axis for clarification on x axis different centrality based measure are shown stands for adapted lesk algorithm
for cosine lesk and for simple lesk algorithm shown only stands for cosine similarity threshold and respectively
for example very first entry on x axis is eigenvalue and means we are using adapted lesk algorithm for wsd and cosine similarity threshold set is using eigenvalue centrality measure
same as highest performance is achieved by subgraph which can interpret as simplified lesk algorithm is used because of with similarity threshold and subgraph based centrality measured is used

showing overall performance of some system for the comparative study in the next from
we are presenting performance ranking based on all used centrality measure like subgraph eigen value

total eight with different wsd and two different thresholds
out of these eight centrality measures for threshold simple lesk times adapted lesk times cosine lesk time and similarly for threshold simple lesk times adapted lesk times cosine lesk times are performing best w

t
any centrality measure
in the last row of
we have presented alpha centrality s top measures out of total measures due to different centrality measures three different wsd technique and two different thresholds
from this result we ca nt say that only particular type wsd is better all time
but we can say that the threshold is better to compare to threshold and subgraph based centrality is better vs
all other measures

top performance by centrality based measures wsd technique used and similarity threshold centrality measure wsd lesk threshold reference performance ranking technique









simple adapted cosine simple adapted cosine simple cosine adapted cosine adapted simple simple cosine adapted adapted simple cosine adapted cosine simple adapted simple cosine adapted adapted cosine










subgraph eigenvalue and hub authority page rank bonpow between ness closeness alpha top best performances out of methods simple cosine simple cosine simple adapted cosine simple adapted adapted adapted simple cosine simple simple simple simple cosine simple cosine cosine




adapted
adapted













adapted
cosine
adapted
simple adapted adapted cosine




concluding remark in this chapter we have presented the lexical network concept for automatic text document summarization which is a little bit different from previously proposed lexical chain based techniques
in previous techniques author concentrate to create a number of lexical chains that creates ambiguity which chain to prefer even this problem efficiently can handle with chain scoring techniques
still lexical chains have problem that if one particular word let donald trump is coming in two or more sentences then which sentence to prefer
another problem with this technique that they consider only nearby sentences for a chain construction i
e
window of two or three sentences was selected so this is unable to handle long term relationship between sentences
our lex net handle long term relationship between sentences
nodes are scored and higher score sentence gave priority
so both problems are handled in this way
our lexical network is based on the number of lexical and semantic relations
to decide the importance of sentences we have applied centrality based measure on lexical network
we have found that subgraph based centrality is performing best among all
since human language is highly ambiguous here english so we need to find a correct sense of a particular word in a given context
the solution to this ambiguity problem done with simplified lesk cosine lesk and adapted lesk algorithm
in this work we have done a study on the impact of wsd techniques and cosine similarity threshold

due to space limitation we are showing results only with and
the less value of represents more diversity compared to high valued
more diversity if is less is not good for summary because it will maintain diversity but less relatedness between sentences which is harming good summary property
for comparison purpose we have used semantrica lexalytics algorithm in which we have found that system proposed by us is working better many times especially when we are using subgraph based centrality
from this work we have reached on number of conclusions like for alpha centrality when alpha is
to
inclusion the performance of summarizer system is arbitrary up and down but after that
to
inclusion for all centrality measures for different value of threshold performance is continuously increasing some time exception at
and again reduced at
second from a set of cosine similarity as



we are suggesting that similarity threshold is better to get enough diversity and better summary as per rouge score
third hub authority based ranking is the same as eigenvalue based centrality
fourth subgraph based centrality measure is performing better to all the reason of this is the higher score for small subgraph which recognizes a small subgraph and this cover various subgraph can be considered as cluster like structure
fifth we are not suggesting any particular wsd is better all the time
chapter modeling automatic text document summarization as multi objective optimization
introduction text document summary may be achieved from many features for examples location tf idf length of summary relatedness redundancy based scores
these stand alone features will not generate a good quality summary so it can be hybridized to get better summary
when we take care of multiple features then many solutions are possible and it is time taking
hence we always try to optimize these functions
in this work we are presenting multi objective function to get a better summary
our optimization function maximizes diversity and minimizes redundancy
here constraints are defined in terms of summary length
since objective function and constraints are linear so we are using ilp integer linear programming to find an optimized solution i
e
which in section

we are presenting related work that is categorized according to the research area and in section

we are mentioning the mdr model baseline model used for comparison that sentences to extract for the summary

literature work is proposed by mcdonald


related work alguliev aliguliyev mehdiyev have proposed a multiobjective based approach for multi document summarization
their model is based on reducing redundant information
they formalize sentence extraction based multi document summarization as an optimization problem
formulation of the model depends on three aspects redundancy content coverage summary length and shown in

where si is ith sentence o is the mean vector of the collection d


sn sim is cosine similarity xij is a binary variable which is one if a pair of sentences and sj are selected to be included in the summary else zero
in
l is required length summary li is the length of ith sentence tolerance is represented sign
in this model denominator evaluating the correlation between the sentences and sj
the numerator provides the coverage of the main content of the document collection while the denominator reduces the redundancy in the summary
model is using sentence to sentence and sentence to document relations
alguliev aliguliyev hajirahimova mehdiyev have proposed multi document summarization as an integer linear programming
they state this model presented by
which is suitable for single and multi document summarization
this system optimizes three property redundancy relevance and length
similarity function which is used here is based similarity
for better calculation author calculate si as feature vector by removing stop words
in combined objective function fcos and fngt stands for cosine similarity measure and ngt normalized google distance based similarity measure used in the objective function
the model is represented by



and

the
is representing objective of maximization of similarity may be drawn by cosine or ngt based between summary s and document d
for length constraints
representing combined function to maximize similarity between summary and documents but minimum among summary sentences
is about length constraints and
represents function f to optimize where alpha can be decided as per user preference
f is maximized the combination of cosine and ngt based function
here xij is a binary variable
this optimization function
w

t constraint in
is elaborated by following equations

ijij sim



in another work alguliev aliguliyev hajirahimova have optimized multi document summarization issues as an aspect of coverage and redundancy
in this formulation coverage is represented by
and redundancy is represented by


and
are representing a combined function
let xi be binary variable xi when si is selected

is representing length summary length constraints where l is summary length li is si sentence s length
weight w is decided based on the importance of coverage or redundancy
alguliev al
have proposed constraint driven document summarization and the constraints are defined in term of diversity in summarization and sufficient coverage
the model is formulated as a quadratic integer programming qip problem to solve the problem discrete pso is used
diversity constraint is defined by equations


and
that covers the relation between a sentence and document and content coverage is defined by

and
this shows sentence to sentence relations
where si ith sentence o is the mean vector of the collection d


sn sim is cosine similarity xij is a binary variable which is one if a pair of sentences and sj are selected to be included to in summary else
l is the required length summary li is length of ith sentence diver specifies high diversity in summary parameter cont we can control the content coverage in summary













alguliev aliguliyev isazade proposed a modified median problem for multi document summarization
this approach expresses sentence to sentence summary to document and summary to subtopics relationships
they cover four aspects of summarization relevance content coverage diversity length and jointly optimizing all aspects
formally their assumption is considering all vertices of a graph as potential medians
p median is defined as a subset of vertices with p cardinality
sj ith sentence dij distance between vertices i and j xij yi are binary variables o is representing the center of collection documents s is summary
in this formulation the objective is to find out the binary assignment x such that high relevancy best content coverage and the less high diversity w

t summary length is at most l
the p median problem can be expressed by equations




the model used for summarization is formulated by


and






ijijdiversimssxxxii














s is selected as

baseline mdr model we compare our results with mcdonald work author used a multi objective optimization technique to optimize various criteria and for that they used ilp
objective criteria taken by them is given in following

for simplicity we represent the document collection simply as the set of all textual units from all the documents in the collection i
e
d


tn where ti d iff ti dj d
we let s d be the set of textual units constituting a summary

representing length constraints and

and
sure about the selection of binary variables xi band yij
in implementation to determine relevance we are using pagerank and subgraph based centrality
in objective function
function relevance and redundancy can describe as follows
relevance summaries should contain informative textual units that are relevant to the user
redundancy summaries should not contain multiple textual units that convey the same





n









information
length lmax summaries are bounded in length
the similarity between two sentences is calculated by cosine similarity
where n total number of the sentence importance si decide the importance of ith sentence higher value given higher importance li represent the length of ith sentences and lmax required max length summary
the xi binary variable indicates whether or not the corresponding sentence is selected in summary
in the same way the yi j binary variables indicating whether or not both and sj are included in the summary

background in this section we are presenting some intermediate techniques followed in this work
handling with text is not easy because output depends on how text is handled how preprocessing and processing done
preprocessing is required before lex network lexical network creation
some preprocessing task is to remove stop words convert to lower remove punctuation remove the number remove whitespaces stemming lemmatization and wsd word sense disambiguation
here we are not presenting detailed wsd techniques and centrality measures because that are explained in the previous chapter
introduction to linear programming is explained here


linear programming linear programming lp also called linear optimization is a method to achieve the best outcome minimum cost maximum profit in a mathematical model in which requirements are represented by linear relationships
in linear programming contains three parts decision variable optimization function to maximize or minimize and constraints
while an integer programming ip problem is a mathematical optimization or feasibility program in which some or all of the variables are restricted to be integers
in other words ilp in which the given objective function and the constraints other than the integer constraints are linear
integer programming is np hard
there is various method to solve ilp
broadly this can be categorized into three parts exact algorithms that guarantee to find an optimal solution but time complexity may be exponential examples are cutting planes branch and bound and dynamic programming
heuristic algorithms provide a suboptimal solution but without a guarantee on its quality
running time is not guaranteed to be polynomial but provide a fast solution
approximation algorithms provide a solution in polynomial time a suboptimal solution together with a bound on the degree of optimality
let an example given by
we want to maximize given z
subject to the constraints maximize z where y



using graphical method solving equation
w

t


in
solution is shown at point

graphical method of solution of w

t and if the above optimization function is represented by then to solve this process should follow if







the limitation of ilp is that when we want to solve the function given in
with constraints in
and
then we need to expand per
and constraints as


and


proposed multi objective optimization model in this section we are presenting a basic outline of our model
this modeling is based on chapter i
e
based on lexical chain network


outline of proposed model in figure
a diagram is represented to show a brief introduction
in this figure it is shown that how the output of the last chapter lexical network is used as input for this work
lexical network creation and finding centrality score of each sentence is fetched from there
here centrality sore is used to decide the importance of sentence edged present in lexicalnetwork is used to find relatedness
we want to maximize information coverage and minimization of redundancy
input sentences preprocessesing lexical network construction lexical network centrality multi objective optimization criteria sentences selection for summary document figure
outline of summarizer system optimization function is given by
and constraints are given by equation

and

this objective function contains two parts before minus sign guaranteed to maximize the highest informative sentences and the second part after minus sign is take care of redundant sentences
eigenvalcentrality si are representing the importance of ith sentence decided by centrality measure and sj represents the weight of the edge between sentence si and sj from lexicalnetwork
xi xj yij are binary variables to decide no two same sentences will be part of the summary


detail description of proposed model this proposed work has been divided into three modules
the outline of this framework is shown in

is comprised sentences selection tagging and creation of another set of sentences with stop word removed i
e
that is later used for lexical network creation
in we are creating lexical network and computing centrality score using degree centrality and betweenness centrality
subgraph centrality performing better in the past the outcome of this step later used in summary generation

three module process for automatic text summarization



ijiijjstyxyx

module in a text document there are many sentences which are not properly readable by the system
first we should apply any regular expression for proper handling of sentences so that words tokens like mr
dr
u
s


take care properly in sentence extraction
we are doing some preprocessing and then we are tagging these sentences using nltk natural language tool kit parser
this is explained by an example in

after sentence identification tagging is done then only significant units i
e
verb and nouns are considered for lexical network construction that are represented in modifiedsentences shown in

in this module we are constructing a lexical network of nn matrix
where n is a number of sentences i
e

we are picking each sentence on by one and extracting only nouns and verbs from this sentence because only these provides information and this is added to the module modified sentence
an administration task force has been studying possible health care solutions but has yet to offer a comprehensive proposal
actualsentence sentences administration nn task nn force nn studying vbg possible jj health nn care nn solutions nns offer vb comprehensive jj proposal nn administration task force studying health care solutions offer proposal

showing procedure stepwise example from duc healthcare data set file number
txt the actual lexical network is construed for modifiedsentences here
we are finding is any relation either lexical or semantic is present between two tagged sentence modifiedsentence modifiedsentence
to find the relation present between sentences we need to disambiguate the sense of the word this is done with the help of lesk algorithm
here lesk algorithm takes two argument word to disambiguate sense and ith actual sentence
based on that sense this algorithm finds hyper hypo synonyms meronym and antonym relation present in other remaining sentences
at a time only one relation will available and according to us every relation is treated of the same importance so we will assign an edge between sentence and each time increase by one if more relation is present between sentences
after getting a lexical network represented by
for given sentences in table
we are applying centrality based measures for sentence scoring
sentence with high centrality score is decided more important compared to low centrality score and output from this module lexical network and centrality score given input in next module
module in this module after getting sentences lexicalnetwork and centrality score we are going to optimize our function given by

this objective function divided into two parts the first section of optimization function that is is to maximizing coverage
maximization of coverage can be linked by maximum informative sentences
the second section that is sj in this we are trying to reduce redundancy between selected sentences for the summary
in equation
n is the total number of sentences xi xj and yij are binary variables
xi equals one if ith sentences selected and added in summary yij equals one will donate that sentences ith and jth are present in summary and i is not equal to j
these objecive criteria are represented by

and
that will guarantee to add a new sentence with optimality condition

experiments and results in this section we are presenting different experiments and results
in

coverage or importance of sentences is decided by our proposed centrality based measures and in

importance is decided by cosine based measure



in this experiment we are maximizing the baseline objective function a section and our proposed model b section
in both the objective function the relevance is decided by centrality measures
in the baseline method redundancy is decided by cosine measure and in the proposed model the lexical network is considered for minimization of redundancy
in tables we have presented precision p recall r and f score f with confidence interval
for example if confidence interval is

with confidence then the significance of this can interpreted like there is a chance that the actual value of unstandardized coefficient is between


a cosine based criteria to minimize redundancy in this experiment we are using a baseline model in which we are trying to maximize the centrality score and minimize cosine similarity with given length constraints
the combined optimization function is represented by

in
precision recall and f score are presented with confidence
from the paper mcdonald it is not clearly mentioned how to decide the importance of the sentences but in our implementation we are using subgraph and pagerank based centrality measure

precision recall and f score of model mdr model with subgraph centrality rouge r cosine p cosine f cosine confidence r confidence p confidence f cosine cosine cosine

















rouge l








rouge









rouge s








rouge su









precision recall and f score of model mdr model with pagerank centrality rouge r cosine p cosine f cosine confidence r confidence p confidence f cosine cosine cosine

















rouge l








rouge









rouge s








rouge su








from
and
it is clear that we are getting improved results
b lexical based criteria to reduce redundancy in this experiment we are using our proposed model in which we are trying to maximize the relevance score that is found out by various degree centrality measures and minimize lexical similarity getting from the lexical network w

t
given constraints
the combined optimization function is represented by

in

precision recall and f score are presented with confidence

precision recall and f score of our proposed model when centrality based measure is subgraph centrality rouge r lexical p lexical f lexical confidence r confidence p confidence f lexical lexical lexical

















rouge l








rouge









rouge s








rouge su









precision recall and f score of our proposed model when centrality based measure is betweenness centrality rouge r lexical p lexical f lexical confidence r confidence p confidence f lexical lexical lexical

















rouge l








rouge









rouge s








rouge su









precision recall and f score of our proposed model when centrality based measure is pagerank rouge r lexical p lexical f lexical confidence r confidence p confidence f lexical lexical lexical

















rouge l








rouge









rouge s








rouge su









precision recall and f score of our proposed model when centrality based measure is evencentrality rouge r lexical p lexical f lexical confidence r confidence p confidence f lexical lexical lexical

















rouge l








rouge









rouge s








rouge su









precision recall and f score of our proposed model when centrality based measure is rouge r lexical p lexical f lexical confidence r confidence p confidence f lexical lexical lexical

















rouge l








rouge









rouge s








rouge su









precision recall and f score of our proposed model when centrality based measure is closeness rouge r lexical p lexical f lexical confidence r confidence p confidence f lexical lexical lexical

















rouge l








rouge









rouge s








rouge su









precision recall and f score of our proposed model when centrality based measure is bonpow rouge r lexical p lexical confidence lexical r lexical p lexical f lexical

















rouge l








rouge









rouge s








rouge su








figure
comparative performance of proposed and mdr baseline model from table

and figure
we can say that in proposed model when subgraph based centrality is used to measure relevance and lexical graph used for redundancy then we are getting best mode performance
the second best model is mdr based with subgraph based centrality measure
when comparison w

mdr based page rank model all the proposed model s performance is improved



in this experiment we are finding a correlation between different centrality based measure
the significance of this experiment highlights the relation between centrality measures
since the different centrality measure returns different summary sentences and we are not doing summary to summary analysis i
e
what kind of sentences return by different measures
this experiment gives a glimpse of the summary to summary analysis
a co relation analysis between different centrality based proposed model our optimization function is described by
in that after changing the centrality we are getting different results and here we are finding the correlation between these methods
in statistics correlation coefficients are used to measure relationship is between two given variables
it may be strong and poor
the value return is between to
where indicates a positive correlation indicates negative correlation and indicates no correlation or better to say no relation exists between these two variables
in our study we used pearson s correlation spearman s correlation kendall s co relation
co relation can be shown by the following examples available at goo
gl
we are using
calculating the average correlation r


different scatter plots showing different directions and strengths of correlation


pearson correlation it is also known by ppmc pearson product moment correlation it shows the linear relationship between two sets of data
ppmc answers are it possible to draw a line graph to represent the data a potential problem with ppmc is that it is not able to differentiate between dependent and independent variables
this can be expressed by

where and y denote two classes of observation with n cardinality

showing pairwise pearson s correlation coefficient symmetric authority between bonpow closeness evencen hub pagerank subgraph authority





between






bonpow






closeness







evencen






hub





pagerank






subgraph









spearman coefficient as per goo
gl avljmp spearman correlation determines the strength and direction of the monotonic relationship between your two variables rather than the strength and direction of the linear relationship between your two variables which is what pearson correlation determines
this can represent by

denotes mean of and y and xi and yi is ith observation

showing pairwise spearman s correlation coefficient symmetric authority between bonpow closeness evencen hub pagerank subgraph authority





between






bonpow






closeness






evencen






hub





pagerank






subgraph









kendall the advantage of kendall test is that this not only provides the relationship between variables but also provides distribution free test of independence
spearman rank correlation is satisfactory
for testing a null hypothesis of independence between two variables but it is difficult to interpret when the null hypothesis is rejected
the kendall rank correlation
improves upon this by reflecting the strength of the dependence between the variables being compared
kendall s rank correlation coefficient depends on concordant discordant
concordant nc is defined by yi else xi yi pair are discordant nd and here n is the total number of observations nc and nd are counting of concordant and discordant

showing pairwise kendall s correlation coefficient symmetric authority between bonpow closeness evencen hub pagerank subgraph authority





between






bonpow






closeness






evencen






hub





pagerank






subgraph






using

and
we can interpret that there are eight different measures to find the importance of sentence and rank it
using
we find that the following pair are highly correlated as authority evencen authority hub authority pagerank authority subgraph between closeness evencen hub evencent pagerank evencen subgraph hub pagerank hub subgraph and pagerank subgraph
those pairs are highly related their combination will not give better results in the case when we use its combination



in this experiment we are hybridizing different centrality measure to maximize the relevance of the sentences and minimization of redundancy has done by the lexical network
since we have eight different measures so total hybridize combinations are possible
in limited time
constraints we can not check all the combination possible
to decide a better combination we are considering experiment

there we will combine those measures which are minimum correlated because highly correlated will return the same results
to make it simple here we are combining two centrality measures
from experiment
we can conclude that the best performer is subgraph based centrality
as per our requirement to get a better combination we can select subgraph bonpow and subgraph between based measures

precision recall and f score of our proposed model when relevance is decided by hybridizing subgraph and bonpow
rouge r lexical p lexical f lexical confidence r lexical p lexical f lexical












rouge l





rouge







rouge s






rouge su



















precision recall and f score of our proposed model when relevance is decided by hybridizing subgraph and betweenness
confidence rouge r lexical p lexical f lexical r lexical p lexical f lexical rouge l


rouge

rouge s
rouge su
















































in table
and
we have presented precision recall and f score of hybridized feature and
figure
representing comparative performance of hybridize centrality w

t
independent centrality measure
from

and
we can conclude that hybridize relevance measure returning better s su and for rouge l rouge
highest performance derived by subgraph based centrality
figure
graph showing comparative performance of different relevance based measure where sign denoting hybridization of different features

concluding remark in this chapter we have proposed optimization based summarization that guarantees maximize coverage and minimum redundancy
we have tested our model by selecting centrality for coverage relevance and lexical network as a function of redundancy between sentences
we have compared our approach with cosine similarity based redundancy measure between sentences
in both approaches proposed and baseline relevance measure has decided by centrality score
we have performed three experiments in the first experiments we have implemented the baseline model using subgraph and pagerank based centrality measure used for relevance measure and cosine similarity matrix for redundancy
in this work we have implemented all the centrality measure as relevance and lexical network for redundancy
where we have found out that based centrality is giving better results
even in the baseline model when subgraph centrality is used it is giving respectable results
in the second experiment we have found out the correlation between the summary results when different centrality based measure used for relevance and lexical network used for redundancy
here we have suggested that the high correlation pair is not suitable for hybridization and the low correlation pair is expected for better results
in the third experiment we have hybridized different centrality measures and improved results are shown by tables and figures
in the third experiment several feature combinations are possible but to our limitation we have tested only limited possibility
chapter conclusion and future work this work our focus is on an important aspect of information retrieval s task automatic text document summarization
this work is divided into five chapters
the first chapter gives a brief introduction about text summarization evaluating techniques and datasets on which this wide range of models developed and experimented
in the second chapter hybrid approach for single text document summarization using statistical and sentiment features we have presented a linear combination of different statistical measures and semantic measures
in our hybrid approach we have taken statistical measures like sentence position centroid tf idf and word level analysis based semantic approach that is sentiment analysis
the sentiment score of a sentence is computed as the sum of the sentiment score of every entity present in the sentence
since for any entity sentiment score have three polarities as neutral negative and positive
we are more interested in such kind of sentences those have a high semantic score either negative or positive
so if entity sentiment is negative then we are multiplying it by minus one to treat it as a positive score
to generate several summaries of different length we have used different approaches like mead microsoft opinosis and human based
in this chapter we have done four experiments
in the first experiment we have considered our summary generate from a proposed algorithm as a system summary and all others as a model summary
after evaluating this has shown that we are getting high precision almost every time that denotes we covered most relevant results
in the second experiment we have compared different system generated summary mead microsoft opinosis our algorithm to the model summary human generated
in this we find that our explained algorithm performed well for generated summary for almost every time but in mead system generates a summary leading in some way but here also we are getting higher recall to compare to mead
the third experiment is showing the contribution of sentiment score in the selection of most informative sentences
we have shown that when we are adding sentiment score as a feature we are getting improved results to compare to without a sentiment score
initially for all experiments have done by assigning equal importance for every feature
to score a sentence we took the sum of all the feature s score and pickup highest score sentence and added that into the summary
in the next step we are selecting next sentence based on the next highest score and add it to the summary if the similarity between summary and sentence is lower that threshold to maintain redundancy and coverage
we will repeat this iterative process until the desired length summary is achieved
in experiment four we have extended this approach
we have suggested and tested a better combination of feature weights
here parameter estimation is done by regression and random forest
in third chapter a new latent semantic analysis and entropy based approach for automatic text document summarization we have proposed two new approaches three new models for automatic text document summarization and a novel entropy based approach for summary evaluation
both the approaches for summary generation is based on svd based decomposition
in the first approach we are using right singular matrix vt for processing and selects a concept one by one top to bottom till required
previous approaches are focused on selecting only one sentence of the highest information content
in our approach we are selecting two sentences w

t each concept such that is highest related to concept and least related to the concept
this approach is based on assumption that by doing this we are covering two different topics
as a result it leads to more coverage and diversity
in future we can increase the selection of number of sentences at a glass
the second approach is based on entropy which formulate into two different models and
in first we are selecting a higher informative concept and from that concept we are selecting summary sentences
in repeatedly we are selecting highest informative sentences i
e
a sentence which is related to all the concepts with a high score
the advantage of the entropy based model is that these are not length dominating models giving a better rouge score statistically closer to standard gold summary
during an experiment we have found out that rouge score depends only on the count of matched words an increasing the summary length sometimes rouge score decreases and on increasing redundancy rouge score also increases
we have pointed out that rouge score does nt measure redundancy i
e
count matched sentences
we have also realized the need of a new measure for summary evaluation that provide a tradeoff between redundancy countmatch and entropy based criteria are proposed
during testing of the new proposed measure on different summary generated by previous models and our proposed models we have find that our entropy based summary is closer to standard summary
from the experiment results it is clear that our model works well for summary evaluation especially for higher length summary because as summary length increases redundancy also increases and in this measure we are measuring redundancy
currently we are giving equal importance to all n gram but theoretically and practically we should give more weight to higher n gram because of high redundancy of information in case of repetition
in future we may assign different feature weights to get better results
in fourth chapter lexnetwork based summarization and a study of impact of wsd techniques and similarity threshold over lexnetwork we have presented a lexical network concept for automatic text document summarization
this approach is a little bit different from previously proposed lexical chain based techniques
in previous techniques author concentrate to create a number of lexical chains that creates ambiguity which chain to prefer even this problem efficiently can handle with chain scoring techniques
still lexical chains have problem that if one particular word let donald trump is coming in two or more sentences then which sentence to prefer
another problem with this technique that they consider only nearby sentences for a chain construction i
e
window of two or three sentences was selected so this is unable to handle long term relationship between sentences
our lex net handle long term relationship between sentences
nodes are scored and high score sentence given priority
so both problems are handled in this way
our lexical network is based on the number of lexical and semantic relations
to decide the importance of sentences we have used centrality based measure on lexical network
since human language is highly ambiguous here english so we need to find a correct sense of a particular word in given context
the solution to this ambiguity problem done with simplified lesk cosine lesk and adapted lesk algorithm
in this work we have studied the impact of wsd techniques and cosine similarity threshold

less value of represents more divesity compare to high valued
more diversity if is less is not good for summary because it will maintain diversity but the less relatedness between sentences which is harming good summary property
for comparison purpose we are used semantrica lexalytics algorithm in which we find that system proposed by us is working better many times
from number of experiments we have find out that subgraph based centrality is performing best among all
from this work we have reached on number of conclusions for alpha centrality when alpha is
to
inclusion the performance of summarizer system is arbitrary up and down but after that
to
inclusion for all centrality measures for different value of threshold performance is continuously increasing some time exception at
and again reduced at from a set of cosine similarity as



we are suggesting that similarity threshold is better to get enough diversity and better summary as per rouge score hub authority based ranking is same as eigenvalue based centrality subgraph based centrality measure is performing better to all the reason of this is higher score for small subgraph which recognizes small subgraph and this cover various subgraph can be considered as cluster like structure we are not suggesting any particular wsd is better all time
during lexnetwork creation we have used lexical semantic relations and in this work we have assigned equal weight assigns for each relation presents between sentences
in literature different priority assigned to all relations
in the future that may be considered for network creation and it may improve the ranking of sentences
during the experiment we have find out some corelation between eigen value based centrality and hub based centrality measures at present this is not objective of this work why it is in future we will try to answer this
in fifth chapter modeling automatic text document summarization as multi objective optimization we have proposed optimization based summarization that guarantee maximize coverage and minimum redundancy
we have tested our model by selecting coverage as based score and similarity function as relatedness between sentences
we have compared our proposed lexnetwork based approach with cosine similarity based redundancy and relevance has been measured by centrality based measures in both the approaches
in this work we have performed three experiments in the first experiments we have implemented the baseline model using subgraph and pagerank based centrality measure used for relevance measure and cosine similarity matrix for redundancy
in this work we have implemented all the centrality measure as relevance and lexical network for redundancy
where we have found out that subgraph based centrality is giving better results
even in the baseline model when subgraph centrality is used it is giving respectable results
in the second experiment we have found out the correlation between the summary results when different centrality based measure used for relevance and lexical network used for redundancy
here we have suggested that the high correlation pair is not suitable for hybridization and the low correlation pair is expected for better results
in the third experiment we have hybridized different centrality measures and improved results are shown by tables and figures
in future this work can be extended to other combinations and an aggregate based model may be proposed and tested along with optimal combination using soft computing techniques
references alguliev r
m
aliguliyev r
m
hajirahimova m
s

expert systems with applications gendocsum mclr generic document summarization based on maximum coverage and less redundancy
expert systems with applications


j
eswa


alguliev r
m
aliguliyev r
m
hajirahimova m
s
mehdiyev c
a

expert systems with applications mcmr maximum coverage and minimum redundant text summarization model
expert systems with applications


j
eswa


alguliev r
m
aliguliyev r
m
isazade n
r

docsum differential evolution with self adaptive mutation and crossover parameters for multi document summarization
based systems
alguliev r
m
aliguliyev r
m
isazade n
r

expert systems with applications cdds constraint driven document summarization models
expert systems with applications


j
eswa


alguliev r
m
aliguliyev r
m
isazade n
r

formulation of document summarization as a nonlinear programming problem
computers industrial engineering
alguliev r
m
aliguliyev r
m
mehdiyev c
a

sentence selection for generic document summarization using an adaptive differential evolution algorithm
swarm and evolutionary computation


j
swevo


balaji j
geetha t
v parthasarathi r

abstractive summarization a hybrid approach for the compression of semantic graphs
international journal on semantic web and information systems ijswis
banerjee s
pedersen t

an adapted lesk algorithm for word sense disambiguation using wordnet
in international conference on intelligent text processing and computational linguistics pp

springer
barzilay r
elhadad m

using lexical chains for text summarization
baxendale p
b

machine made index for technical literature an experiment
ibm journal of research and development
beliga s
metrovi a
martini ipi s

selectivity based keyword extraction method
international journal on semantic web and information systems ijswis
bonacich p

power and centrality a family of measures
american journal of sociology
boulesteix a
janitza s
kruppa j
knig i
r

overview of random forest methodology and practical guidance with emphasis on computational biology and bioinformatics
wiley interdisciplinary reviews data mining and knowledge discovery
breiman l

random forests
machine learning
chen y

huang y
yeh c

lee l


spoken lecture summarization by random walk over a graph constructed with automatically extracted key terms
in twelfth annual conference of the international speech communication association
chen y
liu b
wang x

automatic text summarization based on textual cohesion
journal of electronics china
chen y
wang x
guan y

automatic text summarization based on lexical chains
in international conference on natural computation pp

springer
chiru c

rebedea t
ciotec s

comparison between lsa lda lexical chains
in webist pp

creation a
abstracts l

the automatic creation of literature abstracts april
deerwester s
dumais s
t
furnas g
w
landauer t
k
harshman r

indexing by latent semantic analysis
journal of the american society for information science
dolbear c
hobson p
vallet d
fernndez m
cantadorz i
castellsz p

personalised multimedia summaries
in semantic multimedia and ontologies pp

springer
doran w
stokes n
carthy j
dunnion j

comparing lexical chain based summarisation approaches using an extrinsic evaluation
gwc
edmundson h
p
n


new methods in automatic extracting
elhadad m

no title
retrieved from serv
cs
bgu
ac
il subsys
html ercan g
cicekli i

using lexical chains for keyword extraction
information processing erekhinskaya t
n
moldovan d
i

lexical chains on wordnet and extensions
in flairs estrada e
rodriguez velazquez j
a

subgraph centrality in complex networks
physical management
conference
review e

freeman l
c

a set of measures of centrality based on betweenness
sociometry
freeman l
c

centrality in social networks conceptual clarification
social networks ganapathiraju k
carbonell j
yang y

relevance of cluster size in mmr based summarizer a report self paced lab in information retrieval
ganesan k
zhai c
han j

opinosis a graph based approach to abstractive summarization of highly redundant opinions august
gleich d
f

pagerank beyond the web
siam review
goldstein j
mittal v
carbonell j
callan j

creating and evaluating multi document sentence extract summaries
in proceedings of the ninth international conference on information and knowledge management pp

acm
gong y
liu x

creating generic text summaries
gonzlez e
fort m
f

a new lexical chain algorithm used for automatic summarization
in ccia pp

gurevych i
nahnsen t

adapting lexical chaining to summarize conversational dialogues
in proceedings of the recent advances in natural language processing conference pp

hahn u
mani i

the challenges of automatic summarization
computer
halliday m
a
k
hasan r

cohesion in
english longman london
hariharan s

multi document summarization by combinational approach
international journal of computational cognition
hoskinson a

creating the ultimate research assistant
computer
hovy e
lin c

automated text summarization in summarist
jagadeesh j
pingali p
varma v

sentence extraction based single document summarization
international institute of information technology hyderabad india
karanikolas n
n
galiotou e

a workbench for extractive summarizing methods


pci

katz l

a new status index derived from sociometric analysis
psychometrika
katz s
m

distribution of content words and phrases in text and language modelling
natural language engineering
kim j

kim j

hwang d

korean text summarization using an aggregate similarity
in proceedings of the fifth international workshop on on information retrieval with asian languages pp

acm
kleinberg j
m

authoritative sources in a hyperlinked environment
journal of the acm jacm
koschtzki d
lehmann k
a
peeters l
richter s
tenfelde podehl d
zlotowski o

centrality indices
in network analysis pp

springer
kulkarni a
r
apte s
s

an automatic text summarization using lexical cohesion and correlation of sentences
international journal of research in engineering and technology
kupiec j
pedersen j
chen f

a trainable document summarizer
in proceedings of the annual international acm sigir conference on research and development in information retrieval pp

acm
lesk m

automatic sense disambiguation using machine readable dictionaries how to tell a pine cone from an ice cream cone
in proceedings of the annual international conference on systems documentation pp

acm
lin c


rouge a package for automatic evaluation of summaries
text summarization branches out
lin c
rey m

r ouge a package for automatic evaluation of summaries
luhn h
p

the automatic creation of literature abstracts
ibm journal of research and development
luo w
zhuang f
he q
shi z

effectively leveraging entropy and relevance for summarization
in asia information retrieval symposium pp

springer
mani i
maybury m
t

advances in automatic text summarization reviewed by mark sanderson university of sheffield
mani i
maybury m
t

automatic summarization
mcdonald r

a study of global inference algorithms in multi document summarization
in european conference on information retrieval pp

springer
mckeown k
barzilay r
chen j
elson d
evans d
klavans j
sigelman s

columbia s newsblaster new features and future directions
companion volume of the proceedings of naacl demonstrations
medelyan o

computing lexical chains with graph clustering
in proceedings of the annual meeting of the acl student research workshop pp

association for computational linguistics
morris j
hirst g

lexical cohesion computed by thesaural relations as an indicator of the structure of text
computational linguistics
murray g
renals s
carletta j

extractive summarization of meeting recordings
newman m
e
j

the mathematics of networks
the new palgrave encyclopedia of economics
normalisation statistics

wikipedia
ou s
khoo c
s
g
goh d
h


automatic text summarization in digital libraries
in handbook of research on digital libraries design development and impact pp

igi global
ouyang y
li w
lu q
zhang r

a study on position information in document summarization
in proceedings of the international conference on computational linguistics posters pp

association for computational linguistics
ozsoy m
g
alpaslan f
n
cicekli i

journal of information science june


ozsoy m
g
cicekli i
alpaslan f
n

text summarization of turkish texts using latent semantic analysis
in proceedings of the international conference on computational linguistics pp

association for computational linguistics
padmalahari e
kumar d
v
n
s
prasad s

automatic text summarization with statistical and linguistic features using successive thresholds
in advanced communication control and computing technologies icaccct international conference on pp

ieee
page l
brin s
motwani r
winograd t

the pagerank citation ranking bringing order to the web
stanford infolab
plaza l
stevenson m
daz a

resolving ambiguity in biomedical text to improve summarization
information processing management
pourvali m
abadeh m
s

automated text summarization base on lexicales chain and graph using of wordnet and wikipedia knowledge base
arxiv preprint

precision and recall
n


radev d
r
blair goldensohn s
zhang z

experiments in single and multi document summarization using mead
ann arbor
radev d
r
hovy e
mckeown k

introduction to the special issue on summarization
computational linguistics
radev d
r
jing h
sty m
tam d

centroid based summarization of multiple documents


j
ipm


rambow o
shrestha l
chen j
lauridsen c

summarizing email threads
in proceedings of hlt naacl short papers pp

association for computational linguistics
rautray r
balabantaray r
c
bhardwaj a

document summarization using sentence features
international journal of information retrieval research ijirr
roul r
k
sahoo j
k
goel r

deep learning in the domain of multi document text summarization
in international conference on pattern recognition and machine intelligence pp

springer
sakai t
sparck jones k

generic summaries for indexing in information retrieval
in proceedings of the annual international acm sigir conference on research and development in information retrieval pp

acm
sankarasubramaniam y
ramanathan k
ghosh s

text summarization using wikipedia
information processing and management


j
ipm


sarkar k

syntactic trimming of extracted sentences for improving extractive multi document summarization
journal of computing
shannon c
e

a mathematical theory of communication bell system technical journal vol
july october
sharan a
siddiqi s
singh j

keyword extraction from hindi documents using statistical approach
in intelligent computing communication and devices pp

springer
shimada k
tadano r
endo t

multi aspects review summarization with objective information


j
sbspro


silber h
g
mccoy k
f

an efficient text summarizer using lexical chains
in proceedings of the first international conference on natural language generation volume pp

association for computational linguistics
sood a

towards summarization of written text conversations
international institute of information technology india
steinberger j
jeek k

text summarization and singular value decomposition
in international conference on advances in information systems pp

springer
steinberger j
poesio m
kabadjov m
a
jez k

two uses of anaphora resolution in summarization


j
ipm


stokes n

applications of lexical cohesion analysis in the topic detection and tracking domain
university college dublin department of computer science
takale s
a
kulkarni p
j
shah s
k

an intelligent web search using multi document summarization
international journal of information retrieval research ijirr
tan l

examining crosslingual word sense disambiguation
nanyang technological university nanyang avenue
tofighy s
m
raj r
g
javad h
h
s

ahp techniques for persian text summarization
malaysian journal of computer science
tombros a
sanderson m

advantages of query biased summaries in information retrieval
in proceedings of the annual international acm sigir conference on research and development in information retrieval pp

acm
torres moreno j


automatic text summarization
john wiley sons
vechtomova o
karamuftuoglu m
robertson s
e

on document relevance and lexical cohesion between query terms
information processing management
w
n
venables d
m
s
team and the r
c

an introduction to r
wan x

using only cross document relationships for both generic and topic focused document summarizations
information retrieval
white r
w
jose j
m
ruthven i

a task oriented study on the influencing effects of biased summarisation in web searching
information processing management
xiong s
ji d

query focused multi document summarization using hypergraph based ranking
information processing management
yadav c
s
sharan a

hybrid approach for single text document summarization using statistical and sentiment features
international journal of information retrieval research ijirr
yadav c
s
sharan a
joshi m
l

semantic graph based approach for text mining
in proceedings of the international conference on issues and challenges in intelligent computing techniques icict


icicict

yadav c
s
sharan a
kumar r
biswas p

a new approach for single text document summarization
advances in intelligent systems and computing vol



yeh j

ke h

yang w

meng i


text summarization using a trainable summarizer and latent semantic analysis
information processing management
yeh j

text summarization using a trainable summarizer and latent semantic analysis q


j
ipm


zhang r
li w
gao d
ouyang y

automatic twitter topic summarization with speech acts
ieee transactions on audio speech and language processing

