n a j l c
s c v
v i x r a length controllable abstractive summarization by guiding with summary prototype itsumi kyosuke kosuke atsushi hisako junji hiroyuki yuji ntt media intelligence laboratories ntt nara institute of science and riken center for advanced intelligence itsumi
saito

ntt
co
jp abstract we propose a new length controllable abstractive tion model
recent state of the art abstractive tion models based on encoder decoder models generate only one summary per source text
however controllable rization especially of the length is an important aspect for practical applications
previous studies on length controllable abstractive summarization incorporate length embeddings in the decoder module for controlling the summary length
though the length embeddings can control where to stop coding they do not decide which information should be cluded in the summary within the length constraint
unlike the previous models our length controllable abstractive marization model incorporates a word level extractive ule in the encoder decoder model instead of length dings
our model generates a summary in two steps
first our word level extractor extracts a sequence of important words we call it the prototype text from the source text ing to the word level importance scores and the length straint
second the prototype text is used as additional input to the encoder decoder model which generates a summary by jointly encoding and copying words from both the type text and source text
since the prototype text is a guide to both the content and length of the summary our model can generate an informative and length controlled summary
periments with the cnn daily mail dataset and the room dataset show that our model outperformed previous models in length controlled settings
introduction neural summarization has made great progress in recent years
it has two main approaches extractive and tive
extractive methods generate summaries by selecting important sentences zhang et al
zhou et al

they produce grammatically correct summaries however they do not give much exibility to the summarization cause they only extract sentences from the source text
by contrast abstractive summarization enables more exible summarization and it is expected to generate more uent and readable summaries than extractive models
the most commonly used abstractive summarization model is the pointer generator see liu and manning which erates a summary word by word while copying words from source text various types of renewable energy such as solar and wind are often touted as being the solution to the world s growing energy crisis
but one researcher has come up with a novel idea that could trump them all a biological solar panel that works around the clock
by harnessing the electrons generated by plants such as moss he said he can create useful energy that could be used at home or elsewhere
a university of cambridge scientist has revealed his green source of energy
by using just moss he is able to generate enough power to run a clock shown
he said panels of plant material could power appliances in our homes
and the technology could help farmers grow crops where electricity is scarce



reference summary university of cambridge scientist has revealed his green source of energy
by using just moss he is able to generate enough power to run a clock
he said panels of plant material could power appliances in our homes
and the tech could help farmers grow crops where electricity is scarce
outputs extracted prototype he said panels of plant material could power in our abstractive summary panels of plant material could power appliances
outputs extracted prototype university of cambridge scientist has vealed his he said panels of plant material could power ances in our homes and the technology could help farmers grow crops where is scarce abstractive summary university of cambridge scientist has revealed his green source of energy
he said panels of plant terial could power appliances in our homes
figure output examples of our model
our model extracts the top k important words which are colored red k and blue k as a prototype from the source text
it generates an abstractive summary based on the prototype and source texts
the length of the generated summary is controlled in accordance with the length of the prototype text
the source text and generating words from a pre dened cabulary set
this model can generate an accurate summary by combining word level extraction and generation
although the idea of controlling the length of the mary was mostly neglected in the past it was recently pointed out that it is actually an important aspect of tive summarization liu luo and zhu fan ier and auli
in practical applications the summary length should be controllable in order for it to t the device that displays it
however there have only been a few ies on controlling the summary length
kikuchi et al
proposed a length controllable model that uses length beddings
in the length embedding approach the summary length is encoded either as an embedding that represents the remaining length at each decoding step or as an initial bedding to the decoder that represents the desired length
liu luo and zhu proposed a model that uses the desired length as an input to the initial state of the decoder
these previous models control the length in the decoding module by using length embeddings
however length beddings only add length information on the decoder side
consequently they may miss important information because it is difcult to take into account which content should be cluded in the summary for certain length constraints
we propose a new length controllable abstractive rization that is guided by the prototype text
our idea is to use a word level extractive module instead of length dings to control the summary length
figure compares the previous length controllable models and the proposed one
the yellow blocks are the modules responsible for length control
since the word level extractor controls which tents are to be included in the summary when a length straint is given it is possible to generate a summary ing the important contents
our model consists of two steps
first the word level extractor predicts the word level portance of the source text and extracts important words according to the importance scores and the desired length
the extracted word sequence is used as a prototype of the summary we call it the prototype text
second we use the prototype text as an additional input of the encoder decoder model
the length of the summary is kept close to that of the prototype text because the summary is generated by ring to the prototype text
figure shows examples of put generated by our model
our abstractive summaries are similar to the extracted prototypes
the extractive module produces a rough overview of the summary and the decoder module produces a uent summary based on the tracted prototype
our idea is inspired by extractive and abstractive rization
extractive and abstractive summarization rates an extractive model in an abstractive encoder decoder model
while in the simple encoder decoder model one model identies the important contents and generates ent summaries the extractive and abstractive model has an encoder decoder part that generates uent summaries and a separate part that extracts important contents
several ies have shown that separating the problem of nding the important content and the problem of generating uent maries improves the accuracy of the summary gehrmann deng and rush chen and bansal
our model can be regarded as an extension of models that work in this previous length controllable models proposed model source summary source enc dec model summary enc dec model length embeddings desired length k prototype k words extractive model desired length k figure comparison of previous length controllable els and proposed model
our model controls the summary length in accordance with the length of the prototype text
way however this is the rst to extend the extractive ule such that it can control the summary length
ours is the rst method that controls the summary length using an extractive module and that achieves both high racy and length controllability in abstractive summarization
our contributions are summarized as follows we propose a new length controllable prototype guided abstractive summarization model called lpas controllable prototype guided abstractive tion
our model effectively guides the abstractive marization using a summary prototype
our model trols the summary length by controlling the number of words in the prototype text
our model achieved state of the art rouge scores in length controlled abstractive summarization settings on the cnn dm and newsroom datasets
task denition our study denes length controllable abstractive rization as two pipelined tasks prototype extraction and prototype guided abstractive summarization
the problem formulations of each task are described below
task prototype extraction given a source text x c


xc with l words x xc l and a desired summary length k the model estimates importance scores p l and extracts the top k important words x p


pext pext


xp xp k as a prototype text on the basis of p ext
the desired summary length k can be set to an arbitrary value
note that the original word order is preserved in x p x p is not bag of words
task prototype guided abstractive summarization given the source text and the extracted prototype text x p the model generates a length controlled abstractive mary y


yt
the length of summary t is trolled in accordance with the prototype length k
proposed model
overview our model consists of three modules the prototype tor joint encoder and summary decoder figure
the last two modules comprise task the prototype guided tive summarization
the prototype extractor uses bert summary pointer generator dual encoder block source dual encoder block prototype decoder block source shared encoder block shared encoder block decoder block prototype positional encoding positional encoding glove source text glove prototype text positional encoding glove joint encoder sec

summaization decoder sec

prototype text top k words sigmoid linear bert source text prototype extractor sec

figure architecture of proposed model while the joint encoder and summary decoder use the former architecture vaswani et al

prototype extractor
the prototype extractor tracts the top k important words from the source text
joint encoder
the joint encoder encodes both the source text and the prototype text
summary decoder
the summary decoder is based on the pointer generator model and generates an abstractive summary by using the output of the joint encoder

prototype extractor since our model extracts the prototype at the word level the prototype extractor estimates an importance score pext of each word xc l x c
bert has achieved sota on many classication tasks so it is a natural choice for the prototype extractor
our model uses bert and a task specic forward network on top of bert
we tokenize the source text using the bert and ne tune the bert model
the importance score pext l l pext l is dened as c l where bert is the last hidden state of the pre trained bert
rdbert and are learnable parameters
is a sigmoid function
dbert is the dimension of the last hidden state of the pre trained bert
to extract a more uent prototype than when using only the word level importance we dene a new weighted tance score pextw that incorporates a sentence level tance score as a weight for the word level importance score l pextw l pext l pext sj pext sj nsj xl xlsj pext l where nsj is the number of words in the j th sentence sj x c
our model extracts the top k important words as a prototype from the source text on the basis of pextw
it controls the length of the summary in accordance with the number of words in the prototype text k
l
com google research
joint encoder embedding layer this layer projects each of the one hot vectors of words xc of size v into a dword dimensional l vector space with a pre trained weight matrix w e rdwordv such as glove pennington socher and ning
then the word embeddings are mapped to dmodel dimensional vectors by using the fully connected layer and the mapped embeddings are passed to a relu function
this layer also adds positional encoding to the word embedding vaswani et al

transformer encoder blocks the encoder encodes the embedded source and prototype texts with a stack of former blocks vaswani et al

our model encodes the two texts with the encoder stack independently
we denote s rdmodelk s rdmodell and ep these outputs as ec respectively
transformer dual encoder blocks this block calculates the interactive alignment between the encoded source and prototype texts
specically it encodes the source and totype texts and then performs multi head attention on the other output of the encoder stack i
e
ec s
we denote the outputs of the dual encoder stack of the source text and prototype text by m c rdmodell and m p rdmodelk respectively
s and ep
summary decoder embedding layer the decoder receives a sequence of words in an abstractive summary y which is generated through an auto regressive process
at each decoding step t this layer projects each of the one hot vectors of the words yt in the same way as the embedding layer in the joint encoder
transformer decoder blocks the decoder uses a stack of decoder transformer blocks vaswani et al
that form multi head attention on the encoded representations of the prototype m p
it uses another stack of decoder former blocks that perform multi head attention on those of the source text m c on top of the rst stack
the rst stack rewrites the prototype text and the second one complements the rewritten prototype with the original source information
the subsequent mask is used in the stacks since this nent is used in a step by step manner at test time
the output of the stacks is m s rdmodelt
copying mechanism our pointer generator model copies the words from the source and prototype texts on the basis of the copy distributions for efcient reuse
copy distributions the copy distributions of the source and prototype words are described as follows p tk tl xk xl xc l where p tl are respectively the rst attention heads of the last block in the rst and second stacks of the decoder
tk and c final vocabulary distribution the nal vocabulary tribution is described as follows joint encoder and summary decoder the main loss for the encoder decoder is the cross entropy loss g s cc tl m c l t t cc t cp tkm p p cp t bv xl s xk bg lmain gen n t n t log x c x p
moreover we add the attention guide loss of the summary decoder
this loss is designed to guide the estimated tion distribution to the reference attention
where w v bv w g rdmodelv and bg rv are learnable parameters
training our model is not trained in an end to end manner the totype extractor is trained rst and then the encoder and coder are trained

generating training data x c
rl is if xc l l and label rl pairs xc prototype extractor since there are no supervised data for the prototype extractor we created pseudo training data like in gehrmann deng and rush
the training data consists of word xc l rl for all xc is included in the summary otherwise it is
to construct the paired data automatically we rst extract oracle source sentences soracle that mize the rouge r score in the same way as in hsu et al

then we calculate the word by word alignment tween the reference summary and soracle using a dynamic programming algorithm to consider the word order
finally we label all aligned words with and other words including the words that are not in the oracle sentence with
joint encoder and summary decoder we have to create triple data of x c x p y consisting of the source text the gold prototype text and the target text for training our encoder and decoder
we use the top k words in terms of pextw eq
in the oracle sentences soracle as the gold l totype text to extract a prototype closer to the reference mary and improve the quality of the encoder decoder ing
k is decided using the reference summary length t
to obtain a natural summary close to the desired length we quantize the length t into discrete bins where each bin resents a size range
we set the size range to in this study
that is the value nearest to the summary length t among multiples of is selected for k

loss function prototype extractor we use the binary cross entropy loss because the extractor estimates the importance score of each word eq
which is a binary classication task
lext n l n l rl log pext l rl pext l where n is the number of training examples
lsum attn log c t lproto attn log proto t n t n t n t n t proto t is the rst attention head of the last block in the joint encoder stack for the prototype
denotes the absolute sition in the source text corresponding to the t th word in the sequence of summary words
the overall loss of the tion model is a linear combination of these three losses
lgen lmain gen attn attn and were set to
in the experiments
inference during the inference period we use a beam search and ranking chen and bansal
we keep all nbeam mary candidates provided by the beam search where nbeam is the size of the beam and generate the nbeam best maries
the summaries are then re ranked by the number of repeated n grams the smaller the better
the beam search and this re ranking improve the rouge score of the output as they eliminate candidates that contain repetitions
for the length controlled setting we set the value of k to the sired length
for the standard setting we set it to the average length of the reference summary in the validation data
experiments
datasets and settings dataset we used the cnn dm dataset hermann et al
a standard corpus for news summarization
the maries are bullet points for the articles shown on their spective websites
following see liu and manning we used the non anonymized version of the corpus and truncated the source documents to tokens and the get summaries to tokens
the dataset includes training pairs validation pairs and test pairs
we also used the newsroom dataset grusky man and artzi
newsroom contains various news sources different news sites
we used pairs of data for training
we sampled pairs for validation data and the number of the test pairs was
to uate the length controlled setting for newsroom dataset we randomly sampled samples from the test set
length model avg lenemb lpas lc lenemb lpas lc lenemb lpas lc lenemb lpas lc lenemb lpas lc lenemb lpas



































r l

















table rouge scores of abstractive summarization models with different lengths on the cnn dm dataset words
avg indicates the average rouge score for the ve different lengths
luo and zhu model congurations we used the same congurations for the two datasets
the extractor used the pre trained bertlarge model devlin et al

we ne tuned bert for two epochs with the default settings
our encoder and coder used pre trained dimensional glove embeddings
the encoder and decoder transformer have four blocks
the number of heads was and the number of dimensions of ffn was
dmodel was set to
we used the adam optimizer kingma and ba with a scheduled learning rate vaswani et al

we set the size of the input cabulary to and the output vocabulary to

evaluation metrics we used the rouge scores including and rouge l r l as the evaluation metrics lin
we used the toolkit for culating the rouge

results does our model improve the rouge score in the length controlled setting we used two types of controllable models as baselines
the rst one is a based length controllable model lc that uses the desired length as an input to the initial state of the cnn based coder
liu luo and zhu
the second one lenemb embeds the remaining length and adds them to each decoder step kikuchi et al

since there are no previous results on applying lenemb to the cnn dm dataset we mented it as a transformer based encoder decoder model
specically we simply added the embeddings of the ing length to the word embeddings at each decoding step

com t g n e l t u t u o desired length figure results in the length controlled setting on cnn dm
a rouge l recall precision and f scores for different lengths left
output length distribution right
table shows that our model achieved high rouge scores for different lengths and outperformed the previous length controllable models in most cases
our model was about points more accurate on average than lenemb
our model selected the most important words from the source text in accordance with the desired length
it was thus tive at keeping the important information even in the controlled setting
figure shows the precision recall and f score of rouge for different lengths
our model tained a high f score around the average length around words this indicates that it can select important information and generate stable results with different lengths
does our model generate a summary with the desired length figure shows the relationship between the sired length and the output length
the axis indicates the desired length and the y axis indicates the average length and standard deviation of the length controlled output mary
the results show that our model properly controls the summary length
this controllable nature comes from the training procedure
when training our encoder decoder we set the number of words k in the prototype text according to the length of the reference summary therefore the model learns to generate a summary that has a similar length to the prototype text
how good is the quality of the prototype text to uate the quality of the prototype we evaluated the rouge scores of the extracted prototype text
table shows the sults
in the table lpas ext sents means the three sentences were extracted using pext
interestingly sj and scores of the lpas ext top k words were higher than those of the sentence level tive models
this indicates that word level lpas ext is fective at nding not only important words but also important phrases
also we can see from table that whole lpas improved the rouge l score of lpas ext
this indicates that our joint encoder and mary decoder generate more uent summaries with the help of the prototype text
does our abstractive model improve if the quality of the prototype is improved we evaluated our model in the following two settings in order to analyze the relationship between the quality of the abstractive summary and that of the prototype
in the gold length setting we only gave the gold length k to the prototype extractor
in the gold bottom up bottom up lpas ext sents top k words











r l











table rouge scores of our prototype extractor lpas ext on cnn dm
deng and rush et al
wei and zhou average length gold length gold sentences gold length





r l


table rouge scores of abstractive summarization models with gold settings on the cnn dm dataset
tences the gold length setting we gave the gold sentences soracle and gold length see

table shows the results
these results indicate that selecting the correct number of words in the prototype improves the rouge scores
in this study we simply selected the average length when ing the prototype for all examples in the standard setting however there will be an improvement if we adaptively lect the number of words in the prototype for each source text
moreover the rouge score largely improved in the gold sentence and gold length settings
this indicates that the quality of the generated summary will signicantly prove by increasing the accuracy of the extractive model
is our model effective on other datasets to verify the effectiveness of our model on various other summary styles we evaluated it on a large and varied news summary dataset newsroom
table and figure show the results in the length controlled setting for newsroom
our model achieved higher rouge scores than those of lenemb
from figure we can see that the f value of the rouge score was highest around words
this is because the erage word number is about words
moreover figure shows that our model also acquired a length control ity for a dataset with various styles
how well does our model perform in the standard ting table shows that our model achieved the rouge scores comparable to previous models that do not consider the length constraint on the cnn dm dataset
we note that the current state of the art models use pre trained decoder models while the encoder and decoder of our model except for prototype extractor were not pre trained
we also examined the results of generating a summary from only the prototype lpas source or the source length model avg lenemb lpas lenemb lpas lenemb lpas lenemb lpas lenemb lpas lenemb lpas























r l











table rouge scores of abstractive summarization models with different lengths on the newsroom dataset
t g n e l t u t u o desired length figure results in the length controlled setting on room
a rouge l recall precision and f scores for ferent lengths left
output length distribution right
lpas prototype
here using only the prototype turned out to have the same accuracy as using only the source but the model using the source and the prototype simultaneously had higher accuracy
these results indicate that our prototype extraction and joint encoder effectively incorporated the source text and prototype information and contributed to improving the accuracy
the results for the newsroom dataset under dard settings are shown in table
to consider differences in summary length between news domains we evaluated our model in the average length and domain level average length denoted as domain length settings
the results cate that our model had signicantly higher rouge scores compared with the ofcial baselines and outperformed our baseline lpas prototype
they also indicate that our model is effective on datasets containing text in ous styles
moreover we found that considering the domain length has positive effects on the rouge scores
this dicates that our model can easily reect differences in mary length among various styles
related work and discussion length control for summarization kikuchi et al
were the rst to propose using length embedding for controlled abstractive summarization
fan grangier and auli also used length embeddings at the beginning of the decoder module for length control
liu luo and zhu proposed a cnn based length controllable marization model that uses the desired length as an model pre trained encoder decoder model pointer pointer generator key information guide unied sentence bottom exconsumm etads lpas prototype source pre trained encoder decoder model r l












































table rouge scores of abstractive tion models on cnn dm
liu and manning et al
et al
and bansal deng and rush et al
et al
et al
et al
et al
et al

lpas prototype denotes a simple transformer based generator which is our model without the prototype tor and the joint encoder
lpas source denotes a model that generates a summary only from the prototype text
pointer generator lpas k average length k domain length lpas prototype









r l




table rouge scores of proposed models on room dataset
naaman and artzi put to the initial state of the decoder
takase and okazaki introduced positional encoding that represents the remaining length at each decoder step of the based encoder decoder model
it is almost equivalent to the model lenemb we implemented
these previous models use length embeddings for controlling the length in the coding module whereas we use the prototype extractor for controlling the summary length and to include important formation in the summary
neural extractive and abstractive summarization hsu et al
gehrmann deng and rush and you et al
incorporated a and word level extractive model in the pointer generator model
their models weight the copy probability for the source text by using an extractive model and guide the pointer generator model to copy portant words
li et al
proposed a keyword guided abstractive summarization model
chen and bansal proposed a sentence extraction and re writing model that trains in an end to end manner by using reinforcement ing
cao et al
proposed a search and rewrite model
mendes et al
proposed a combination of level extraction and compression
the idea behind these models is word level weighting for the entire source text or sentence level re writing
on the other hand our model guides the summarization with a length controllable type text by using the prototype extractor and joint encoder
utilizing extractive results to control the length of the mary is a new idea
large scale pre trained language model bert vlin et al
is a new pre trained language model that uses bidirectional encoder representations from former
bert has performed well in many natural language understanding tasks such as the glue benchmarks wang et al
and natural language inference williams gia and bowman
liu used bert for their sentence level extractive summarization model
zhang wei and zhou trained a new pre trained model that siders document level information for sentence level tive summarization
we used bert for the word level totype extractor and veried the effectiveness of using it in the word level extractive module
several researchers have published pre trained encoder decoder models very cently wang et al
lewis et al
raffel et al

wang et al
pre trained a transformer based pointer generator model
lewis et al
pre trained a normal transformer based encoder decoder model using large unlabeled data and achieved state of the art results
dong et al
extended the bert structure to handle sequence to sequence tasks
reinforcement learning for summarization ment learning rl is a key summarization technique
rl can be used to optimize non differential metrics or ple non differential networks
narayan cohen and lapata and dong et al
used rl for extractive marization
for abstractive summarization paulus xiong and socher used rl to mitigate the exposure bias of abstractive summarization
chen and bansal used rl to combine sentence extraction and pointer generator els
our model achieved high rouge scores without rl
in future we may incorporate rl in our models to get a further improvement
conclusion we proposed a new length controllable abstractive rization model
our model consists of a word level prototype extractor and a prototype guided abstractive summarization model
the prototype extractor identies the important part of the source text within the length constraint and the stractive model is guided with the prototype text
this acteristic enabled it to achieve a high rouge score in dard summarization tasks
moreover our prototype tor ensures the summary will have the desired length
periments with the cnn dm dataset and the newsroom dataset show that our model outperformed previous els in standard and length controlled settings
in future we mendes a
narayan s
miranda s
marinho z
martins a
f
t
and cohen s
b

jointly extracting and pressing documents with summary state representations
in naacl
narayan s
cohen s
b
and lapata m

ranking sentences for extractive summarization with reinforcement learning
in naacl
paulus r
xiong c
and socher r

a deep reinforced model for abstractive summarization
corr

pennington j
socher r
and manning c
d

glove global vectors for word representation
in emnlp
raffel c
shazeer n
roberts a
lee k
narang s
matena m
zhou y
li w
and liu p
j

ing the limits of transfer learning with a unied text to text transformer
arxiv e prints
see a
liu p
j
and manning c
d

get to the point summarization with pointer generator networks
in acl
takase s
and okazaki n

positional encoding to control output sequence length
in naacl
vaswani a
shazeer n
parmar n
uszkoreit j
jones l
gomez a
n
kaiser l
u
and polosukhin i

attention is all you need
in nips

wang a
singh a
michael j
hill f
levy o
and bowman s
r

glue a multi task benchmark and analysis platform for natural language understanding
in
wang l
zhao w
jia r
li s
and liu j

ing based sequence to sequence pre training for text ation
in emnlp to appear
williams a
nangia n
and bowman s
r

a broad coverage challenge corpus for sentence understanding through inference
in naacl hlt
you y
jia w
liu t
and yang w

improving stractive document summarization with salient information modeling
in acl
zhang x
lapata m
wei f
and zhou m

neural latent extractive document summarization
in emnlp
association for computational linguistics
zhang x
wei f
and zhou m

hibert document level pre training of hierarchical bidirectional transformers for document summarization
in acl
zhou q
yang n
wei f
huang s
zhou m
and zhao t

neural document summarization by jointly ing to score and select sentences
in acl
would like to incorporate a pre trained language model in the abstractive model to build a higher quality tion model
references cao z
li w
li s
and wei f

retrieve rerank and rewrite soft template based neural summarization
in acl
chen y

and bansal m

fast abstractive rization with reinforce selected sentence rewriting
in acl
devlin j
chang m
lee k
and toutanova k

bert pre training of deep bidirectional transformers for guage understanding
corr
dong y
shen y
crawford e
van hoof h
and cheung j
c
k

banditsum extractive summarization as a contextual bandit
in emnlp
dong l
yang n
wang w
wei f
liu x
wang y
gao j
zhou m
and hon h


unied language model pre training for natural language understanding and generation
in advances in neural information processing systems

fan a
grangier d
and auli m

controllable abstractive summarization
in
gehrmann s
deng y
and rush a

bottom up abstractive summarization
in emnlp
grusky m
naaman m
and artzi y

newsroom a dataset of
million summaries with diverse extractive strategies
in acl
hermann k
m
kocisky t
grefenstette e
espeholt l
kay w
suleyman m
and blunsom p

teaching machines to read and comprehend
in nips

hsu w

lin c

lee m

min k
tang j
and sun m

a unied model for extractive and abstractive summarization using inconsistency loss
in acl
kikuchi y
neubig g
sasano r
takamura h
and okumura m

controlling output length in neural encoder decoders
in emnlp
kingma d
p
and ba j

adam a method for stochastic optimization
in iclr
lewis m
liu y
goyal n
ghazvininejad m
hamed a
levy o
stoyanov v
and zettlemoyer l

bart denoising sequence to sequence pre training for natural language generation translation and sion
arxiv e prints
li c
xu w
li s
and gao s

guiding generation for abstractive text summarization based on key information guide network
in acl
lin c


rouge a package for automatic evaluation of summaries
in acl
liu y
luo z
and zhu k

controlling length in abstractive summarization using a convolutional neural work
in emnlp
liu y

fine tune bert for extractive summarization
corr


