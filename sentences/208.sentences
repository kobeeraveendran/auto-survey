sequence generation from both sides to the middle long jiajun chengqing and heng of chinese academy of sciences beijing china laboratory of pattern recognition casia beijing china center for excellence in brain science and intelligence technology shanghai china intelligence technology lab alibaba group long
zhou jjzhang
ia
ac
cn yuheng
inc
com n u j l c
s c v
v i x r a abstract the encoder decoder framework has achieved promising process for many sequence generation tasks such as neural machine translation and text summarization
such a framework usually ates a sequence token by token from left to right hence this autoregressive decoding procedure is time consuming when the output sentence becomes longer and it lacks the guidance of future text which is crucial to avoid under translation
to alleviate these issues we propose a synchronous bidirectional sequence generation sbsg model which predicts its outputs from both sides to the middle simultaneously
in the sbsg model we able the left to right and right to left generation to help and interact with each other by leveraging interactive bidirectional attention work
experiments on neural machine translation ende chen and enro and text rization tasks show that the proposed model nicantly speeds up decoding while improving the generation quality compared to the autoregressive transformer
introduction the neural encoder decoder framework has been widely adopted in sequence generation tasks including neural chine translation nmt sutskever et al
bahdanau et al
vaswani et al
text summarization rush et al
zhou et al
li et al
and image tioning xu et al
vinyals et al

in this work the encoder models the semantics of the input sentence and transforms it into a context vector representation which is then fed into the decoder to generate the output sequence token by token in a left to right manner
although the framework has obtained great success the sequence to sequence model suffers from the decoding ciency problem gu et al

most of the models use autoregressive decoders that operate one step at a time and they become slow when generating long sequences because a computationally intensive neural network is used to predict each token
several recently proposed models avoid rence at training time by leveraging convolutions gehring et figure the illustration of synchronous bidirectional decoding for sequence to sequence model
the bidirectional decoder predicting its outputs from left to right and from right to left simultaneously and interactively can produce two tokens at each time step
al
or self attention vaswani et al
as parallelizable alternatives to recurrent neural networks but the decoding process can not share the speed strength of allelization due to the autoregressive generation schema in the decoder
more importantly this left to right decoding can not take advantage of future contexts which can be generated in a right to left decoding zhang et al

to avoid this autoregressive property gu et al
proposed a non autoregressive model to speed up machine translation by directly generating target words without ing on any previous predictions
oord et al
modied a convolutional network for non autoregressive modeling of speech synthesis
lee et al
introduced a conditional non autoregressive neural sequence model based on iterative renement
however in spite of their improvement in ing speed non autoregressive models typically suffer from the substantial drop in generation quality
in this paper we propose a synchronous bidirectional quence generation sbsg model to achieve a better ment on both generation quality and decoding speed
instead of producing output sentences token by token or predicting its outputs in a totally parallel manner the sbsg model ates two tokens at a time
as shown in figure the tional decoder can generate output sentences from both sides to the middle with both left to right and right to left directions
furthermore we introduce an interactive bidirectional attention network to bridge and puts
more specically at each moment the generation of target side tokens does not only rely on its previously ated outputs history information but also depends on ously predicted tokens of the other generation direction ture information
specically the contributions of this paper can be to right decoding to left rized as two folds we propose a novel sbsg model that employs one coder to predict outputs from both sides to the middle simultaneously and interactively
to the best of our knowledge this is the rst work to perform sequence generation from both ends to the middle
we extensively evaluate the proposed model on cal sequence generation tasks namely neural machine translation and text summarization
in the case of chine translation we not only obtain approximately

speedup for decoding than autoregressive transformer with beam search greedy search but also get an improvement of



and

bleu points of translation quality in ende nist chen and enro tively which also signicantly outperforms previous non autoregressive models gu et al
lee et al
kaiser et al

for text summarization the proposed model is able to decode approximately
faster while achieving better generation quality relative to the autoregressive counterparts
related work autoregressive decoding
recent approaches to sequence to sequence learning typically leverage recurrence sutskever et al
convolution gehring et al
or attention vaswani et al
as basic building blocks
particularly relying entirely on the attention mechanism the transformer introduced by vaswani et al
can improve the training speed as well as model performance
to ate autoregressive architecture mi et al
introduced a sentence level vocabulary which is able to reduce computing time and memory usage
devlin focused on fast and accurate neural machine translation decoding in cpu
zhang et al
proposed an average attention network aan as an alternative to the self attention network in the decoder of transformer
despite their remarkable success they are difcult to parallelize and this unidirectional decoding work limits its potential liu et al

non autoregressive decoding
in terms of speeding up the decoding of the neural transformer gu et al
modied the autoregressive architecture to speed up machine translation by directly generating target words in parallel
however the main drawback of this work is the need for tensive policy gradient ne turning techniques as well as the issue that this method only works for machine translation and can not be applied to other sequence generation tasks
in allel to gu et al
oord et al
presented a cessful non autoregressive sequence model for speech form
besides kaiser et al
rst auto encoded the get sequence into a shorter sequence of discrete latent ables and then decoded the output sentence from this shorter latent sequence in parallel
lee et al
introduced a conditional non autoregressive neural sequence model based on iterative renement
concurrently to our work wang et al
presented a semi autoregressive transformer for faster translation without changing the autoregressive erty in global
however these approaches improved the allelizability but signicantly reduced generation quality
towards bidirectional decoding
liu et al
posed an agreement model to encourage the agreement tween a pair of target directional lstms which generated more balanced targets
similarly some work attempted at target bidirectional decoding for smt or nmt watanabe and sumita finch and sumita liu et al
sennrich et al
liu et al

recently zhang et al
and zhou et al
proposed an asynchronous and synchronous bidirectional decoding for nmt tively
serdyuk et al
presented the twin networks to encourage the hidden state of the forward network to be close to that of the backward network used to predict the same ken
nevertheless the above studies are not to speed up the decoding procedure and even sacrice speed in exchange for quality improvement
our work differs from those by ducing a novel sequence generation model which aims at ing full advantage of both left to right and right to left ing to accelerate and improve sequence generation
the framework our goal in this work is to achieve a better improvement on both generation quality and decoding speed
we introduce a novel method for decoding with both left to right and to left manners simultaneously and interactively in a unied model
as demonstrated in figure our proposed model consists of an encoder and a bidirectional decoder in which two special labels and at the beginning of output sentence are utilized to guide the sequence generation from left to right or right to left
the bidirectional decoder reads the encoder representation and generates two output tokens at each time step by using interactive bidirectional attention networks
next we will detail individual components and introduce an algorithm for training and inference

the neural encoder given an input sentence


xm the new former leverages its encoder to induce input side semantic and dependencies so as to enable its decoder to recover the encoded information in an output sentence
the encoder is composed of a stack of n identical layers each of which has two sub layers hl where the superscript l indicates layer depth ln is layer malization ffn means feed forward networks and mhatt denotes the multi head attention mechanism as follows
scaled dot product attention
an attention function can be described as mapping a query and a set of key value pairs to an output
the output is computed as a weighted sum of the values where the weight assigned to each value is computed by a compatibility function of the query and the ing key
scaled dot product attention operates on a query q key k and a value v as k v softmax qk t dk v figure the new transformer architecture with the proposed rectional multi head and attention network
instead of producing output sentence token by token or predicting its outputs in totally parallel the proposed model generates two tokens one from left to right the other one from right to left at a time indicated by two special labels
where dk is the dimension of the key
multi head attention
we use the multi head version with h heads
it obtains h different representations of q k v computes scaled dot product attention for each tion concatenates the results and projects the concatenation with a feed forward layer
k v


o v w v headi q i and w o are parameter matrices
i kw k i w k i w v i where w q
the bidirectional decoder the bidirectional decoder performs decoding in both left right and right to left manners under the guidance of ously generated forward and backward outputs
we apply our bidirectional attention network to replace the self attention network in its decoder part and illustrate the overall tecture in figure
next we will present those two tional attention models and integrate them into the decoder of transformer
k v q q keys k and values bidirectional scaled dot product attention figure left shows our particular attention
the input v sists of queries which are all concatenated by forward states and h j and backward ward states
the new forward states h j can be obtained by bidirectional dot product scaled states h j it can be calculated as attention
for new forward states k t j dk k t j dk v j softmax v j softmax j att j att k j k j v j v j q j q j h f q j q j h b where j is obtained by conventional scaled dot product j contains the tention as introduced in equation and tentional future information from decoding
then we h b h figure left bidirectional scaled dot product attention operates on forward and backward queries q keys k values v
right bidirectional multi head intra attention consists of several attention layers in parallel
h f use a linear interpolation method to integrate the forward formation h j integration j and backward information h f h j where is a hyper parameter decided by the performance on development set
h b j j h b j h b j for decoding similar to the calculation of forward h j can be h j the backward hidden states hidden states puted as follows
att q j q j h f h b h j integration j att k j k j h j v j v j h b j where integration is the same as introduced in equation
we refer to the whole procedure formulated in equation as bsdpa
h j h j bsdpa it is worth noting that and be calculated in parallel
q j q j h j and v j k j k j h j can improve each other v j bidirectional multi head intra attention different from the mask multi head attention equation we can obtain the new forward and backward hidden states simultaneously as shown in figure right where i tention head with j th target token can be computed using bsdpa headi j h i j k j i and w v h i bsdpa k k v j i q q q j i v v i w k where w q i are parameter matrices which are the same as standard multi head attention introduced in tion
by contrast bidirectional multi head inter attention is composed of two standard multi head attention models which do not interact with each other
integrating bidirectional attention into decoder we use our bidirectional attention network to replace the multi head attention in the decoder part as demonstrated in figure
for each layer in bidirectional decoder the rst sub layer is the bidirectional multi head intra attention multi head intra attentionn


ffnffnffnffnbidirectional multi head intra attentionffnffnffnffnbidirectional multi head inter to right decoding attentionright to left decoding predictor











bidirectional scaled dot product attentionjvjvjkjkjqjqjkjkjvjvjfhfjhbjhbjhjhjhhhvvkkqq biattintra which is capable of combining history and future information s l d s l s s s s s where sl denotes l layer hidden states or embedding tors when and subscript d denotes the decoder informed intra attention representation
the second sub layer is the bidirectional multi head inter attention biattinter which integrates the tion of the corresponding source sentence by performing to right and right to left decoding attention respectively as shown in figure
e s l d s l where e denotes the encoder informed inter attention sentation and hn is the source hidden state of top layer
l d hn hn hn hn s l the third sub layer is a position wise fully connected forward neural network s l s l e s l e
finally we employ a linear transformation and softmax tivation to compute the probability of the j th tokens based on j s n n sn namely the nal hidden states of forward and backward decoding
s n j j n j y j n where w denotes the weight matrix and is the shared rameters for and decoding
j w j w
training and inference training
given a parallel sentence pair y we design a smart strategy to enable synchronous bidirectional ation within a decoder
we rst divide the output sentence y into two halves and reverse the second half
second we separately add the special labels and at the beginning of each half sentence y and y to guide erating tokens from left to right or right to left
finally we propose a smoothing model to better connect both rectional generational results
as shown in figure if the output length is odd we add the additional tag fore in forward or backward sentence randomly
in other words our model is capable of generating a null word when necessary
following previous work gu et al
wang et al
we also use knowledge distillation niques kim and rush to train our model
given a set of training examples the training algorithm aims to nd the model parameters that maximize the that we follow vaswani et al
vaswani et al
to use residual connection and layer normalization in each decoder layer which are omitted in the presentation for simplicity
figure the smoothing model introduced to connect and results smoothly
when the output sentence has odd tokens we domly insert which means null word and can be removed in postprocessing
figure the bidirectional beam search process of our proposed model which produces tokens from left to right and right to left multaneously under the guidance of two special labels and
by using bidirectional attention model left to right and to left decoding can help and interact with each other
hood of the training data z z log z j z j y z j log z j z j y z j inference
once the proposed model is trained we employ a simple bidirectional beam search algorithm to predict the output sequence
as illustrated in figure with two special start tokens which are optimized during the training process we let half of the beam to keep decoding from left to right and allow the other half beam to decode from right to left
the blue blocks denote the ongoing expansion of the esis and decoding terminates when the end of sentence ag is predicted
more importantly by using the bidirectional head intra attention the two decoding manners can help and interact with each other in one beam search process
tively we can also use greedy search to our model
application to neural machine translation we use bleu papineni et al
to evaluate the proposed model on translation tasks

setup we verify our model on three translation datasets of different sizes english ende nist chen english enro whose training sets consist of
m
m
m sentence pairs respectively
we tokenize the corpora using a script from moses koehn et al
and segment each word into subword units using bpe sennrich et al

we use k and k shared bpe tokens for ende and enro respectively
for ende we use as the dation set and as the test set
for chen we utilize bpe to encode chinese and english respectively and limit the source and target vocabularies to the most frequent k tokens
we use nist as the validation set nist
statmt
org translation task
html and
corpora include
statmt
org translation task
html decoding decoding attention between and























system architecture english german chinese english english romanian quality speed quality speed gu et al
lee et al
kaiser et al
wang et al
beam search wang et al
greedy search this work beam search this work greedy search speed n a n a


n a



quality existing nmt systems









our nmt systems











nat nat d nat d nat adaptive lt lt sat sat sat sat transformer transformer our model transformer transformer our model







































table translation quality bleu and speed on ofcial test sets
translation speed is measured on the amount of translated sentences in one second
for comparison we also list results reported by gu et al
lee et al
kaiser et al
wang et al

note that we and sat use different size corpus and different preprocessing methods for chinese english translation
although the autoregressive or semi autoregressive nmt models have greater potential in speedup decoding than ours the major drawback is translation quality degradation
by making full use of the history information and future information our sbsg model can get a signicant bleu improvement p
than autoregressive semi autoregressive and non autoregressive models
as our test sets
for enro we use and as development and test sets
we implement the proposed model based on the toolkit
for our bidirectional transformer model we employ the adam optimizer with

and
we use the same warmup and decay strategy for learning rate as vaswani et al
with warmup steps
during training we employ label smoothing of value

we use three gpus to train ende and one gpu for the other two language pairs
for evaluation we use beam search with a beam size of and length penalty

sides we use encoder and decoder layers hidden size attention heads feed forward inner layer dimensions

results and analysis parameters
nat gu et al
adopts encoder decoder architecture with additional fertility predictor model
nat lee et al
has two decoders and needs more parameters than conventional transformer
our bidirectional nmt model uses one single encoder decoder model which can predict the target tokens in left to right and right to left manners simultaneously
hence our sbsg model does not increase any parameters except for a hyper parameter pared to the standard transformer
inference speed
as shown in table the proposed sbsg model is capable of decoding approximately
faster than autoregressive transformer with beam search in three lation tasks
besides our model obtains
ende
chen and
enro speedup than former in greedy search
as a compromise solution
com tensorow tween autoregressive and non autoregressive models the speed of our model is relatively slower than nat d nat and lt kaiser et al

besides our proposed model is capable of obtaining comparable translation speed compared to sat wang et al
with
translation quality
table shows translation mance of ende chen and enro translation tasks
the proposed model behaves better than nat d nat lt in all test datasets
in particular our model with beam search signicantly outperforms nat d nat and lt by

and
bleu points in large scale english german lation respectively
although the sat has a faster decoding speed than the sbsg model when k becomes bigger it fers from the translation quality degradation relative to the toregressive nmt
compared to autoregressive transformer our proposed model with beam search is able to behave better in terms of both decoding speed and translation quality
thermore our model with greedy search does not only perform autoregressive transformer by

and
bleu points of translation quality in ende chen and enro respectively but also signicantly speedups the coding of conventional transformer
length analysis
we follow bahdanau et al
to group sentences of similar lengths together and compute a bleu score and the averaged length of translations per group
figure shows that the performance of transformer and transformer drops rapidly when the length of the input sentence increases
our sbsg model alleviates this problem by generating a sequence from both sides to the dle which in general encourages the model to produce more accurate and long sentences
source reference sbsg the logo was made up of a westbound sailboat braving the wind and waves churning arc shaped spindrifts in light blue color and words that say zheng he s anniversary to the west

the logo is composed of sailboats that break the wind and break the waves rolling light blue water wave owers
of zheng he the anniversary of the west
the logo is composed of sailing boat by wind and waves rolling light blue shaped owers and words to the west
table a chinese english translation example of baselines and our proposed model
our model can alleviate the unbalanced output problems liu et al
by generating a sentence from both sides to the middle
abs selective enc transformer sbsg beam sbsg greedy











rg l





speed


table rouge recall evaluation results on duc test set
for comparison we also list results reported by rush et al
lapati et al
zhou et al

results with mark are taken from the corresponding papers
our proposed sbsg model icant outperforms the conventional transformer model in terms of both decoding speed and generation quality

results and analysis in table we report the rouge score and speed for duc test set
experiments show that the generation ity of our proposed model is on par with the state of the art text summarization models
we observe approximately
faster decoding than the autoregressive transformer while achieving better generation quality
specially our model with beam search greedy search is capable of decoding

faster than conventional transformer on english gaword test set
conclusions in this work we propose a novel sbsg model that performs bidirectional decoding simultaneously and interactively
stead of producing output sentence token by token the posed model makes decoding much more parallelizable and generates two tokens at each time step
we extensively uate the proposed sbsg model on neural machine tion ende chen and enro and text tion english gigaword tasks
different from previous autoregressive models gu et al
lee et al
kaiser et al
which suffer from serious quality dation our sbsg model achieves a signicant improvement in both generation quality and decoding speed compared to the state of the art autoregressive transformer
acknowledgments the research work described in this paper has been funded by the national key research and development program of china under grant no
and the natural science foundation of china under grant no
and
this work is also supported by grants from nvidia nvail program
figure length analysis performance of the generated lations with respect to the lengths of the source sentences
the proposed sbsg model can alleviate under translation by producing longer translation on long sentences
case study
in table we present a translation example from nist chinese english
liu et al
show that model produces the translation with good prex and sometimes omits the second half sentence but model usually generates the generation with better sufxes
our sults conrm these ndings
the proposed sbsg model can alleviate the errors by generating sequences from both sides to the middle
application to text summarization we further verify the effectiveness of our proposed sbsg model on text summarization which is another real world plication that encoder decoder framework succeeds rush et al


setup abstractive sentence summarization aims to provide a like summary for a long sentence
we conduct text rization experiments on english gigaword
the allel corpus is produced by pairing the rst sentence and the headline in the news article with some heuristic rules
the tracted corpus contains about
m sentence summary pairs for the training set and k examples for the development set
we employ a shared vocabulary of about k word types and use duc used by rush et al
as our test set
the model structure is the same as that used in neural machine translation
we employ rouge as our evaluation metric which is all widely adopted evaluation metric for text summarization

com harvardnlp sent summary of source of source of references bahdanau et al
dzmitry bahdanau kyunghyun cho and yoshua bengio
neural machine translation by jointly learning to align and translate
in iclr
devlin jacob devlin
sharp models on dull ware fast and accurate neural machine translation ing on the cpu
in emnlp pages
finch and sumita andrew finch and eiichiro sumita
bidirectional phrase based statistical machine translation
in emnlp pages
gehring et al
jonas gehring michael auli david grangier denis yarats and yann dauphin
convolutional sequence to sequence learning
in icml
gu et al
jiatao gu james bradbury caiming xiong victor ok li and richard socher
autoregressive neural machine translation
arxiv preprint

kaiser et al
ukasz kaiser aurko roy ashish vaswani niki pamar samy bengio jakob uszkoreit and noam shazeer
fast decoding in sequence models using discrete latent variables
arxiv preprint

kim and rush yoon kim and alexander m
rush
sequence level knowledge distillation
in emnlp
koehn et al
philipp koehn hieu hoang alexandra birch chris callison burch marcello federico nicola bertoldi brooke cowan wade shen christine moran richard zens chris dyer ondrej bojar alexandra stantin and evan herbst
moses open source toolkit for statistical machine translation
in acl
lee et al
jason lee and kyunghyun cho
deterministic non autoregressive neural in emnlp sequence modeling by iterative renement
pages
elman mansimov et al
haoran li junnan zhu jiajun zhang and chengqing zong
ensure the correctness of the mary incorporate entailment knowledge into abstractive sentence summarization
in coling
liu et al
lemao liu andrew m finch masao utiyama and eiichiro sumita
agreement on bidirectional lstms for sequence to sequence learning
in aaai
liu et al
lemao liu masao utiyama andrew agreement on finch and eiichiro sumita
bidirectional neural machine translation
in naacl
liu et al
yuchen liu long zhou yining wang yang zhao jiajun zhang and chengqing zong
a rable study on model averaging ensembling and reranking in nmt
in nlpcc pages
mi et al
haitao mi zhiguo wang and abe cheriah
vocabulary manipulation for neural machine translation
in acl pages
oord et al
aaron van den oord yazhe li igor babuschkin karen simonyan oriol vinyals koray kavukcuoglu george van den driessche edward hart luis c cobo florian stimberg al
lel wavenet fast speech synthesis
arxiv preprint

papineni et al
kishore papineni salim roukos todd ward and weijing zhu
bleu a methof for matic evaluation of machine translation
in acl
rush et al
alexander m
rush sumit chopra and jason weston
a neural attention model for abstractive sentence summarization
in emnlp
sennrich et al
rico sennrich barry haddow and alexandra birch
edinburgh neural machine translation systems for wmt
in wmt
sennrich et al
rico sennrich barry haddow and alexandra birch
neural machine translation of rare words with subword units
in acl pages
serdyuk et al
dmitriy serdyuk nan rosemary ke alessandro sordoni adam trischler chris pal and yoshua bengio
twin networks matching the future for sequence generation
in iclr
sutskever et al
ilya sutskever oriol vinyals and quoc vv le
sequence to sequence learning with ral networks
in nips pages
vaswani et al
ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan n gomez ukasz kaiser and illia polosukhin
attention is all you need
in nips pages
vinyals et al
oriol vinyals alexander toshev samy bengio and dumitru erhan
show and tell a ral image caption generator
in cvpr
wang et al
chunqi wang ji zhang and haiqing chen
semi autoregressive neural machine translation
arxiv preprint

watanabe and sumita taro watanabe and eiichiro sumita
bidirectional decoding for statistical machine translation
in coling
xu et al
kelvin xu jimmy ba ryan kiros kyunghyun cho aaron courville ruslan salakhutdinov richard zemel and yoshua bengio
show attend and tell neural image caption generation with visual tion
computer science pages
zhang et al
biao zhang deyi xiong and jinsong su
accelerating neural transformer via an average tion network
in acl pages
zhang et al
xiangwen zhang jinsong su yue qin yang liu rongrong ji and hongji wang
chronous bidirectional decoding for neural machine lation
in aaai
et al
qingyu zhou nan yang furu wei and ming zhou
selective encoding for abstractive sentence summarization
in acl pages
et al
long zhou chengqing zong
machine translation
in tacl pages
and synchronous bidirectional neural jiajun zhang
