hooks in the headline learning to generate headlines with controlled styles di zhijing joey tianyi lisa peter mit web services singapore college
edu zhijing

hku
hk
a star
edu
sg
edu a m l c
s c v
v i x r a abstract current summarization systems only produce plain factual headlines but do not meet the practical needs of creating memorable titles to increase exposure
we propose a new task stylistic headline generation shg to enrich the headlines with three style options humor romance and clickbait in order to attract more readers
with no style specic article headline pair only a standard headline summarization dataset and mono style corpora our method titlestylist generates style specic headlines by combining the summarization and struction tasks into a multitasking framework
we also introduced a novel parameter sharing scheme to further disentangle the style from the text
through both automatic and human evaluation we demonstrate that titlestylist can generate relevant uent headlines with three target styles humor romance and bait
the attraction score of our model erated headlines surpasses that of the state the art summarization model by
and even outperforms human written references
introduction every good article needs a good title which should not only be able to condense the core meaning of the text but also sound appealing to the ers for more exposure and memorableness
ever currently even the best headline generation hg system can only fulll the above requirement yet performs poorly on the latter
for example in figure the plain headline by an hg model summ leopard frog found in new york city is less eye catching than the style carrying ones such as what s that chuckle you hear it may be the new frog from nyc
corresponding author
code is available at
titlestylist
figure given a news article current hg models can only generate plain factual headlines failing to learn from the original human reference
it is also much less attractive than the headlines with humorous romantic and click baity styles
to bridge the gap between the practical needs for attractive headlines and the plain hg by the current summarization systems we propose a new task of stylistic headline generation shg
given an article it aims to generate a headline with a target style such as humorous romantic and click baity
it has broad applications in reader adapted title generation slogan suggestion for online post headlines and many others
shg is a highly skilled creative process and ally only possessed by expert writers
one of the most famous headlines in american publications sticks nix hick pix could be such an example
in contrast the current best summarization systems are at most comparable to novice writers who vide a plain descriptive representation of the text body as the title cao et al
a lin et al
song et al
dong et al

these systems usually use a language generation model that mixes styles with other linguistic patterns and inherently lacks a mechanism to control the style new frog species discovered in new york city area
it has adistinctive croak scientists nd
leopard frog yet have a name
ribbit frog species found in new york city has a croakof its ownoriginal headline articlesumm leopard frog found innew york city hg model output what that chuckle you hear it may be thenew frog from nychumorous a new frog with a croak of its own awaitsits name in the roads of facts about the new frog with a croak ofits ownclick baity explicitly
more fundamentally the training data comprise of a mixture of styles e

the gigaword dataset rush et al
obstructing the models from learning a distinct style
in this paper we propose the new task shg to emphasize the explicit control of style in headline generation
we present a novel headline generation model titlestylist to produce enticing titles with target styles including humorous romantic and click baity
our model leverages a multitasking framework to train both a summarization model on headline article pairs and a denoising coder dae on a style corpus
in particular based on the transformer architecture vaswani et al
we use the style dependent layer ization and the style guided encoder attention to disentangle the language style factors from the text
this design enables us to use the shared content to generate headlines that are more relevant to the articles as well as to control the style by plugging in a set of style specic parameters
we validate the model on three tasks humorous romantic and click baity headline generation
both automatic and human evaluations show that titlestylist can generate headlines with the desired styles that peal more to human readers as in figure
the main contributions of our paper are listed below to the best of our knowledge it is the rst research on the generation of attractive news headlines with styles without any supervised style specic article headline paired data
through both automatic and human tion we demonstrated that our proposed tlestylist can generate relevant uent lines with three styles humor romance and clickbait and they are even more attractive than human written ones
our model can exibly incorporate multiple styles thus efciently and automatically viding humans with various creative headline options for references and inspiring them to think out of the box
headline generation as summarization headline generation is a very popular area of search
traditional headline generation methods mostly focus on the extractive strategies using guistic features and handcrafted rules luhn edmundson mathis et al
salton et al
jing and mckeown radev and eown dorr et al

to enrich the versity of the extractive summarization abstractive models were then proposed
with the help of ral networks rush et al
proposed based summarization abs to make banko et al
s framework of summarization more erful
many recent works extended abs by ing additional features chopra et al
takase et al
nallapati et al
shen et al
tan et al
guo et al

other variants of the standard headline generation ting include headlines for community question swering higurashi et al
multiple headline generation iwama and kano user specic generation using user embeddings in tion systems liu et al
bilingual headline generation shen et al
and question style headline generation zhang et al

only a few works have recently started to cus on increasing the attractiveness of generated headlines fan et al
xu et al

fan et al
focuses on controlling several features of the summary text such as text length and the style of two different news outlets cnn and lymail
these controls serve as a way to boost the model performance and the and style control shows a negligible improvement
xu et al
utilized reinforcement learning to courage the headline generation system to generate more sensational headlines via using the readers comment rate as the reward which however can not explicitly control or manipulate the styles of lines
shu et al
proposed a style transfer approach to transfer a non clickbait headline into a clickbait one
this method requires paired news articles headlines data for the target style however for many styles such as humor and romance there are no available headlines
our model does not have this limitation thus enabling transferring to many more styles
related work text style transfer our work is related to summarization and text style transfer
our work is also related to text style transfer which aims to change the style attribute of the text while preserving its content
first proposed by shen et al
it has achieved great progress in recent years xu et al
lample et al
zhang et al
fu et al
jin et al
yang et al
jin et al

however all these methods demand a text corpus for the target style however in our case it is expensive and technically challenging to collect news headlines with humor and romance styles which makes this category of methods not applicable to our problem
methods
problem formulation is trained on a source dataset s the model and target dataset t
the source dataset s consists of pairs of a news article a and its plain headline
we assume that the source corpus has a distribution p a h where and h a
the target corpus t comprises of sentences written in a specic style e

humor
we assume that it conforms to the distribution p t
note that the target corpus t only contains carrying sentences not necessarily headlines it can be just book text
also no sentence t is paired with a news article
overall our task is to learn the conditional distribution p t using only s and t
this task is fully unsupervised because there is no sample from the joint distribution p a t

model architecture for summarization we adopt a sequence sequence model based on the former architecture vaswani et al

as in figure it consists of a layer encoder e e and a layer decoder g g with a hidden size of and a feed forward lter size of
for better generation quality we initialize with the mass model song et al

mass is pretrained by masking a sentence fragment in the encoder and then predicting it in the decoder on large scale english monolingual data
this training is adopted in the current state of the art systems across various summarization benchmark tasks including hg
figure the transformer based architecture of our model
figure training scheme
multitask training is adopted to combine the summarization and dae tasks
supervised training for es and gs with the source domain dataset s based on the encoder decoder architecture we can learn the ditional distribution p by training zs and hs to solve the supervised learning task where zs is the learned tent representation in the source domain
the loss function of this task is gs log es gs where es and gs are the set of model ters of the encoder and decoder in the source main and denotes the overall probability of generating an output sequence h given the input article a which can be further expanded as follows es gs


zs gs l
multitask training scheme where l is the sequence length
to disentangle the latent style from the text we adopt a multitask learning framework luong et al
training on summarization and dae taneously as shown in figure
dae training for et and gt for the target style corpus t since we only have the sentence t without paired news articles we train zt et t and t gt zt by solving an unsupervised multi head self attentionlayer normmlplayer normembembembencoderdecodermulti head encoder attentionmlpmulti head self attentionstyle dependent layer normstyle dependent querytransformationstyle dependent layer normembembemb construction learning task where zt is the learned latent representation in the target domain and t is the corrupted version of t by randomly deleting or blanking some words and shufing the word orders
to train the model we minimize the reconstruction error lt lt et gt ett log type
style layer normalization inspired by previous work on image style transfer dumoulin et al
we make the scaling and shifting rameters for layer normalization in the transformer architecture un shared for each style
this style layer normalization approach aims to transform a layers activation into a normalized activation specic to the style s where et and gt are the set of model eters for the encoder and generator in the target domain
we train the whole model by jointly imizing the supervised training loss ls and the unsupervised denoised auto encoding loss lt via multitask learning so the total loss becomes gs et gt gs et gt where is a hyper parameter

parameter sharing scheme more constraints are necessary in the multitask training process
we aim to infer the conditional distribution as p t gt
however without samples from p a t this is a ing or even impossible task if es and et or gs and gt are completely independent of each other
hence we need to add some constraints to the network by relating es and et and gs and gt
the simplest design is to share all parameters tween es and et and apply the same strategy to gs and gt
the intuition behind this design is that by exposing the model to both tion task and style carrying text reconstruction task the model would acquire some sense of the target style while summarizing the article
however to encourage the model to better disentangle the tent and style of text and more explicitly learn the style contained in the target corpus t we share all parameters of the encoder between two domains i
e
between es and et whereas we divide the parameters of the decoder into two types independent parameters ind and style dependent parameters dep
this means that only the independent parameters are shared between gs and gt while the style dependent parameters are not
more specically the parameters of the layer normalization and encoder attention modules are made style dependent as detailed below
s where and are the mean and standard deviation of the batch of x and s and s are style specic parameters learned from data
specically for the transformer decoder tecture we use a style specic self attention layer normalization and nal layer normalization for the source and target domains on all six decoder layers
type
style guided encoder attention our model architecture contains the attention nism where the decoder infers the probability of the next word not only conditioned on the ous words but also on the encoded input hidden states
the attention patterns should be different for the summarization and the reconstruction tasks due to their different inherent nature
we insert this thinking into the model by introducing the style guided encoder attention into the multi head attention module which is dened as follows q query w s q k key wk v value wv k v softmax v qktr dmodel where query key and value denote the triple of inputs into the multi head attention module w s q wk and wv denote the scaled dot product matrix for afne transformation dmodel is the dimension of the hidden states
we specialize the dot product matrix w s q of the query for different styles so that q can be different to induce diverse attention patterns
experiments
datasets we compile a rich source dataset by combining the new york times nyt and cnn as well as three target style corpora on humorous romantic and click baity text
the average sentence length in the nyt cnn humor romance and clickbait datasets are



and
words respectively


source dataset the source dataset contains news articles paired with corresponding headlines
to enrich the ing corpus we combine two datasets the new york times k and cnn k
after ing these two datasets we randomly selected pairs as the validation set and another pairs as the test set
we rst extracted the archival abstracts and headlines from the new york times nyt pus sandhaus and treat the abstracts as the news articles
following the standard processing procedures kedzie et al
we ltered out advertisement related articles as they are very different from news reports resulting in news abstracts headlines pairs
we then add into our source set the cnn marization dataset which is widely used for ing abstractive summarization models hermann et al

we use the short summaries in the original dataset as the news abstracts and cally parsed the headlines for each news from the dumped news web and in total collected news abstract headline pairs


three target style corpora humor and romance for the target style datasets we follow chen et al
to use mor and romance novel collections in pus zhu et al
as the humor and romance datasets
we split the documents into sentences tokenized the text and collected k sentences as our datasets
clickbait we also tried to learn the writing style from the click baity headlines since they have shown superior attraction to readers
thus we used the examiner spamclickbait news dataset noted as the clickbait dataset
we collected k headlines for our use

com summarization datasets use cnn instead of the dailymail dataset since lymail headlines are very long and more like short summaries

nyu

smashwords

kaggle
com some examples from each style corpus are listed in table
style examples humor romance clickbait the crowded beach like houses in the burbs and the line ups at walmart
berthold stormed out of the brewing argument with his violin and bow and went for a walk with it to practice for the much more receptive polluted air
i can face it joyously and with all my heart and soul she said
with bright blue and green buttercream scales sparkling eyes and purple candy melt wings it sat majestically on a rocky ledge made from chocolate
year old girl and year old boy cused of attempting to kill mother who is the adult chilly dry weather welcomes to south florida end segregation in alabama bryce hospital sale offers a golden opportunity table examples of three target style corpora humor romance and clickbait

baselines we compared the proposed titlestylist against the following ve strong baseline approaches
neural headline generation nhg we train the state of the art summarization model mass song et al
on our collected news abstracts headlines paired data
gigaword mass we test an off the shelf line generation model mass from song et al
which is already trained on gigaword a large scale headline generation dataset with around million articles
neural story teller nst it breaks down the task into two steps which rst generates headlines from the aforementioned nhg model then applies style shift techniques to generate style specic headlines kiros et al

in brief this method uses the skip thought model to encode a sentence into a representation vector and then manipulates its style by a linear transformation
afterward this transformed representation vector is used to ize a language model pretrained on a style specic so that a stylistic headline can be generated

com examine the examiner sent summary more details of this method can refer to the ofcial website
fine tuned we rst train the nhg model as mentioned above then further ne tuned it on the target style corpus via dae training
multitask we share all parameters between es and et and between gs and gt and trained the model on both the summarization and dae tasks
the model architecture is the same as nhg

evaluation metrics to evaluate the performance of the proposed tlestylist in generating attractive headlines with styles we propose a comprehensive twofold egy of both automatic evaluation and human ation


setup of human evaluation we randomly sampled news abstracts from the test set and asked three native speaker annotators for evaluation to score the generated headlines
specically we conduct two tasks to evaluate on four criteria relevance attractiveness language uency and style strength
for the rst task the human raters are asked to evaluate these outputs on the rst three aspects relevance attractiveness and language uency on a likert scale from to integer values
for relevance human annotators are asked to evaluate how tically relevant the headline is to the news body
for attractiveness annotators are asked how tractive the headlines are
for uency we ask the annotators to evaluate how uent and readable the text is
after the collection of human evaluation results we averaged the scores as the nal score
in addition we have another independent human uation task about the style strength we present the generated headlines from titlestylist and lines to the human judges and let them choose the one that most conforms to the target style such as humor
then we dene the style strength score as the proportion of choices


setup of automatic evaluation apart from the comprehensive human evaluation we use automatic evaluation to measure the eration quality through two conventional aspects summarization quality and language uency
note
com neural storyteller that the purpose of this two way automatic uation is to conrm that the performance of our model is in an acceptable range
good automatic evaluation performances are necessary proofs to compliment human evaluations on the model tiveness
summarization quality we use the standard tomatic evaluation metrics for summarization with the original headlines as the reference bleu pineni et al
meteor denkowski and lavie rouge lin and cider vedantam et al

for rouge we used the toolkit and for other rics we used the pycocoeval toolkit
language fluency we ne tuned the medium model radford et al
on our lected headlines and then used it to measure the perplexity ppl on the generated outputs

experimental details we used the fairseq code base ott et al

during training we use adam optimizer with an initial learning rate of and the batch size is set as tokens for each gpu with the parameters update frequency set as
for the dom corruption for dae training we follow the standard practice to randomly delete or blank the word with a uniform probability of
and domly shufed the word order within tokens
all datasets are lower cased
is set as
in ments
for each iteration of training we randomly draw a batch of data either from the source dataset or from the target style corpus and the sampling strategy follows the uniform distribution with the probability being equal to
results and discussion
human evaluation results the human evaluation is to have a comprehensive measurement of the performances
we conduct experiments on four criteria relevance attraction uency and style strength
we summarize the man evaluation results on the rst three criteria in table and the last criteria in table
note that through automatic evaluation the baselines nst fine tuned and gigaword mass perform poorer than other methods in section
thereby we
com pltrdy
com maluuba nlg eval on the development set is
removed them in human evaluation to save essary work for human raters
settings relevance attraction fluency style none humor romance clickbait nhg human multitask titlestylist multitask titlestylist multitask titlestylist























table human evaluation on three aspects relevance attraction and uency
none represents the original headlines in the dataset
relevance we rst look at the relevance scores in table
it is interesting but not surprising that the pure summarization model nhg achieves the highest relevance score
the outputs from nhg are usually like an organic reorganization of several keywords in the source context as shown in ble thus appearing most relevant
it is thy that the generated headlines of our titlestylist for all three styles are close to the original written headlines in terms of relevance validating that our generation results are qualied in this pect
another nding is that more attractive or more stylistic headlines would lose some relevance since they need to use more words outside the news body for improved creativity
attraction in terms of attraction scores in ble we have three ndings the written headlines are more attractive than those from nhg which agrees with our observation in section
our titlestylist can generate more attractive headlines over the nhg and multitask baselines for all three styles demonstrating that adapting the model to these styles could improve the attraction and specialization of some ters in the model for different styles can further hance the attraction
adapting the model to the clickbait style could create the most attractive headlines even out weighting the original ones which agrees with the fact that click baity lines are better at drawing readers attention
to be noted although we learned the clickbait style into our summarization system we still made sure that we are generating relevant headlines instead of too exaggerated ones which can be veried by our relevance scores
fluency the human annotated uency scores in table veried that our titlestylist generated lines are comparable or superior to the written headlines in terms of readability
style strength we also validated that our tlestylist can carry more styles compared with the multitask and nhg baselines by summarizing the percentage of choices by humans for the most morous or romantic headlines in table

automatic evaluation results apart from the human evaluation of the overall eration quality on four criteria we also conducted a conventional automatic assessment to gauge only the summarization quality
this evaluation does not take other measures such as the style strength into consideration but it serves as important plimentary proof to ensure that the model has an acceptable level of summarization ability
table summarizes the automatic evaluation results of our proposed titlestylist model and all baselines
we use the summarization related uation metrics i
e
bleu rouge cider and meteor to measure how relevant the generated headlines are to the news articles to some extent by comparing them to the original human written headlines
in table the rst row nhg shows the performance of the current state of the art marization model on our data and table provides two examples of its generation output
our mate goal is to generate more attractive headlines than these while maintaining relevance to the news body
from table the baseline gigaword mass scored worse than nhg revealing that directly plying an off the shelf headline generation model to new in domain data is not feasible although this model has been trained on more than times larger dataset
both nst and fine tuned baselines present very poor summarization performance and the reason could be that both of them cast the lem into two steps summarization and style fer and the latter step is absent of the tion task which prevents the model from ing its summarization capability
in contrast the multitask baseline involves the summarization and style transfer via tion training processes at the same time and shows superior summarization performance even pared with nhg
this reveals that the vised reconstruction task can indeed help improve news abstract turkey s bitter history with kurds is guring nently in its calculations over how to deal with bush administration s request to use turkey as the base for thousands of combat troops if there is a war with iraq recep tayyip erdogan leader of turkey s governing party says publicly for the rst time that future of iraq s kurdish area which abuts border region of turkey also heavily populated by kurds is weighing heavily on negotiations hints at what turkish ofcials have been saying privately for weeks if war comes to iraq riding turkish objective would be less helping icans topple saddam hussein but rather preventing kurds in iraq from forming their own state
reunied berlin is commemorating anniversary of the start of construction of berlin wall almost years since germans jubilantly celebrated reopening between east and west and attacked hated structure with sledgehammers some germans are championing the preservation of wall at the time when little remains beyond few crumbling remnants to remind berliners of unhappy division that many have since worked hard to heal and put behind them what little remains of physical wall embodies era that germans have yet to resolve for themselves they routinely talk of wall in the mind to describe social and cultural differences that continue to divide easterners and westerners
human nhg turkey assesses question of kurds turkey s bitter history with kurds what if there is a war with kurds humor romance what if the kurds say no to iraq clickbait for turkey a long hard road the wall berlin ca nt quite demolish construction of berlin wall is commemorated the berlin wall years later is still there the berlin wall from the past to the present east vs west berlin wall lives on table examples of style carrying headlines generated by titlestylist
style nhg multitask titlestylist humor romance clickbait








table percentage of choices for the most ous or romantic headlines among titlestylist and two baselines nhg and multitask
the supervised summarization task
more tantly we use two different types of corpora for the reconstruction task one consists of headlines that are similar to the news data for the summarization task and the other consists of text from novels that are entirely different from the news data
however unsupervised reconstruction training on both types of data can contribute to the summarization task which throws light on the potential future work in summarization by incorporating unsupervised learning as augmentation
we nd that in table titlestylist f achieves the best summarization performance
this implicates that compared with the multitask baseline where the two tasks share all parameters specialization of layer normalization and encoder attention ters can make gs focus more on summarization
it is noteworthy that the summarization scores for titlestylist are lower than titlestylist f but still comparable to nhg
this agrees with the fact that the gt branch more focuses on bringing in tic linguistic patterns into the generated summaries thus the outputs would deviate from the pure marization to some degree
however the relevance degree of them remains close to the baseline nhg which is the starting point we want to improve on
later in the next section we will further validate that these headlines are faithful to the new article through human evaluation
we also reported the perplexity ppl of the erated headlines to evaluate the language uency as shown in table
all outputs from baselines nhg and multitask and our proposed titlestylist show similar ppl compared with the test set used in the ne tuning stage ppl
indicating that they are all uent expressions for news headlines

extension to multi style we progressively expand titlestylist to include all three target styles humor romance and clickbait to demonstrate the exibility of our model
that is we simultaneously trained the summarization task on the headlines data and the dae task on the three target style corpora
and we made the layer normalization and encoder attention ters specialized for these four styles fact humor romance and clickbait and shared the other rameters
we compared this multi style version titlestylist versatile with the previously presented single style counterpart as shown in table
from this table we see that the bleu and rouge l scores of titlestylist versatile are comparable to titlestylist for all three styles
besides we ducted another human study to determine the better headline between the two models in terms of tion and we allow human annotators to choose both style corpus model bleu rouge l cider meteor ppl len
ratio none humor romance clickbait nhg gigaword mass nst fine tuned multitask titlestylist titlestylist f nst fine tuned multitask titlestylist titlestylist f nst fine tuned multitask titlestylist titlestylist f







































































































































table automatic evaluation results of our titlestylist and baselines
the test set of each style is the same but the training set is different depending on the target style as shown in the style corpus column
none means no style specic dataset and humor romance and clickbait corresponds to the datasets we introduced in section


during the inference phase our titlestylist can generate two outputs one from gt and the other from gs
outputs from gt are style carrying so we denote it as titlestylist outputs from gs are plain and factual thus denoted as titlestylist f
the last column len
ratio denotes the average ratio of abstract length to the generated headline length by the number of words
model bleu rg l pref
headlines than state of the art hg models
style none humor romance clickbait titlestylist versatile titlestylist versatile titlestylist titlestylist versatile titlestylist titlestylist versatile titlestylist



















table comparison between titlestylist versatile and titlestylist
rg l denotes rouge l and pref
denotes preference
options if they deem them as equivalent
the result is presented in the last column of table which shows that the attraction of titlestylist versatile outputs is competitive to titlestylist
versatile thus generates multiple headlines in ent styles altogether which is a novel and efcient feature
conclusion we have proposed a new task of stylistic headline generation shg to emphasize explicit control of styles in headline generation for improved traction
to this end we presented a multitask framework to induce styles into summarization and proposed the parameters sharing scheme to enhance both summarization and stylization bilities
through experiments we validated our proposed titlestylist can generate more attractive acknowledgement we appreciate all the volunteer native speakers shreya karpoor lisa orii abhishek mohan paloma quiroga
for the human evaluation of our study and thank the reviewers for their ing comments
joey tianyi zhou is partially ported by the agency for science technology and research under its ame programmatic funding scheme project no

references michele banko vibhu o mittal and michael j brock

headline generation based on cal translation
in proceedings of the annual meeting on association for computational tics pages
association for computational linguistics
ziqiang cao wenjie li sujian li and furu wei

retrieve rerank and rewrite soft template based neural summarization
in acl
ziqiang cao furu wei wenjie li and sujian li

faithful to the original fact aware neural abstractive summarization
in thirty second aaai conference on articial intelligence
cheng kuan chen zhu feng pan ming yu liu and min sun

unsupervised stylish image scription generation via domain layer norm
in the thirty third aaai conference on articial gence aaai the thirty first innovative plications of articial intelligence conference iaai the ninth aaai symposium on educational advances in articial intelligence eaai olulu hawaii usa january february pages
aaai press
sumit chopra michael auli and alexander m rush

abstractive sentence summarization with tentive recurrent neural networks
in proceedings of the conference of the north american ter of the association for computational linguistics human language technologies pages
michael denkowski and alon lavie

meteor universal language specic translation evaluation for any target language
in proceedings of the ninth workshop on statistical machine translation pages
li dong nan yang wenhui wang furu wei xiaodong liu yu wang jianfeng gao ming zhou and hsiao wuen hon

unied language model pre training for natural language arxiv preprint understanding and generation


bonnie dorr david zajic and richard schwartz

hedge trimmer a parse and trim approach to headline generation
in proceedings of the naacl on text summarization workshop volume pages
association for computational guistics
vincent dumoulin jonathon shlens and manjunath kudlur

a learned representation for artistic style
arxiv preprint

hp edmundson

problems in automatic ing
communications of the acm
angela fan david grangier and michael auli

in controllable abstractive summarization
ceedings of the workshop on neural machine translation and generation bourne australia july pages
sociation for computational linguistics
zhenxin fu xiaoye tan nanyun peng dongyan zhao and rui yan

style transfer in text ration and evaluation
in thirty second aaai ference on articial intelligence
yidi guo heyan huang yang gao and chi lu

conceptual multi layer neural network model for headline generation
in chinese computational guistics and natural language processing based on naturally annotated big data pages
springer
karl moritz hermann tomas kocisky edward stette lasse espeholt will kay mustafa suleyman and phil blunsom

teaching machines to read and comprehend
in advances in neural information processing systems pages
tatsuru higurashi hayato kobayashi takeshi suyama and kazuma murao

extractive line generation based on learning to rank for nity question answering
in proceedings of the international conference on computational tics pages
kango iwama and yoshinobu kano

multiple news headlines generation using page metadata
in proceedings of the international conference on natural language generation
association for computational linguistics
di jin zhijing jin joey tianyi zhou and peter szolovits

unsupervised domain adaptation for neural machine translation with iterative back translation
arxiv preprint

zhijing jin di jin jonas mueller nicholas matthews and enrico santus

unsupervised text tribute transfer via iterative matching and translation
in ijcnlp
hongyan jing and kathleen mckeown

the composition of human written summary sentences
chris kedzie kathleen mckeown and hal daume iii

content selection in deep learning models of summarization
arxiv preprint

ryan kiros yukun zhu ruslan r salakhutdinov richard zemel raquel urtasun antonio torralba and sanja fidler

skip thought vectors
in advances in neural information processing systems pages
guillaume lample eric michael marcaurelio ranzato
multiple attribute text rewriting
in iclr
ludovic subramanian denoyer and y lan boureau
sandeep smith chin yew lin

rouge a package for automatic in text summarization evaluation of summaries
branches out pages
junyang lin xu sun shuming ma and qi su

global encoding for abstractive summarization
in acl
tianshang liu haoran li junnan zhu jiajun zhang and chengqing zong

review headline eration with user embedding
in chinese tional linguistics and natural language processing based on naturally annotated big data china national conference ccl and national symposium nlp nabd changsha china october proceedings pages
hans peter luhn

the automatic creation of erature abstracts
ibm journal of research and velopment
minh thang luong quoc v
le ilya sutskever oriol vinyals and lukasz kaiser

corr task sequence to sequence learning


betty a mathis james e rush and carol e young
improvement of automatic abstracts by the
use of structural analysis
journal of the american society for information science
ramesh nallapati bowen zhou caglar gulcehre bing xiang al

abstractive text tion using sequence to sequence rnns and beyond
arxiv preprint

myle ott sergey edunov alexei baevski angela fan sam gross nathan ng david grangier and michael auli

fairseq a fast ble toolkit for sequence modeling
arxiv preprint

kishore papineni salim roukos todd ward and jing zhu

bleu a method for automatic uation of machine translation
in proceedings of the annual meeting of the association for tational linguistics pages
dragomir r radev and kathleen r mckeown

generating natural language summaries from tiple on line sources
computational linguistics
alec radford jeffrey wu rewon child david luan dario amodei and ilya sutskever

language models are unsupervised multitask learners
alexander m rush sumit chopra and jason a neural attention model for arxiv preprint ston

stractive sentence summarization


alexander m rush seas harvard sumit chopra and jason weston

a neural attention model for in aclweb
proceedings sentence summarization
of the conference on empirical methods in natural language processing
gerard salton amit singhal mandar mitra and chris buckley

automatic text structuring and marization
information processing management
evan sandhaus

the new york times annotated corpus
linguistic data consortium philadelphia
shi qi shen yan kai lin cun chao tu yu zhao yuan liu mao song sun al

recent vances on neural headline generation
journal of computer science and technology
shiqi shen yun chen cheng yang zhiyuan liu and maosong sun

zero shot cross lingual ieee acm trans
audio ral headline generation
speech language processing
shiqi shen yu zhao zhiyuan liu maosong neural headline generation arxiv preprint sun et al

with sentence wise optimization


tianxiao shen tao lei regina barzilay and tommi s
jaakkola

style transfer from non parallel in advances in neural text by cross alignment
information processing systems annual ference on neural information processing systems december long beach ca usa pages
kai shu suhang wang thai le dongwon lee and huan liu

deep headline generation for bait detection
ieee international conference on data mining icdm pages
kaitao song xu tan tao qin jianfeng lu and yan liu

mass masked sequence to sequence pre training for language generation
arxiv preprint

sho takase jun suzuki naoaki okazaki tsutomu hirao and masaaki nagata

neural line generation on abstract meaning representation
in proceedings of the conference on cal methods in natural language processing pages
jiwei tan xiaojun wan and jianguo xiao

from neural sentence summarization to headline in ijcai generation a coarse approach
pages
ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan n gomez ukasz kaiser and illia polosukhin

attention is all in advances in neural information you need
cessing systems pages
ramakrishna vedantam c lawrence zitnick and devi parikh

cider consensus based image in proceedings of the ieee scription evaluation
conference on computer vision and pattern tion pages
jingjing xu xu sun qi zeng xiaodong zhang ancheng ren houfeng wang and wenjie li

unpaired sentiment to sentiment translation a cled reinforcement learning approach
in acl
peng xu chien sheng wu andrea madotto and cale fung

clickbait sensational headline generation with auto tuned reinforcement learning
arxiv

zichao yang zhiting hu chris dyer eric p
xing and taylor berg kirkpatrick

unsupervised text style transfer using language models as in advances in neural information tors
ing systems annual conference on neural mation processing systems neurips december montreal canada pages
ruqing zhang jiafeng guo yixing fan yanyan lan jun xu huanhuan cao and xueqi cheng

question headline generation for news articles
in proceedings of the acm international ence on information and knowledge management cikm torino italy october pages
zhirui zhang shuo ren shujie liu jianyong wang peng chen mu li ming zhou and enhong chen

style transfer as unsupervised machine lation
arxiv

yukun zhu ryan kiros rich zemel ruslan dinov raquel urtasun antonio torralba and sanja fidler

aligning books and movies towards story like visual explanations by watching movies and reading books
in proceedings of the ieee national conference on computer vision pages

