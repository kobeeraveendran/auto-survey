klearn background knowledge inference from summarization data maxime peyrard epfl maxime

ch robert west epfl robert

t c o l c
s c v
v i x r a abstract the goal of text summarization is to press documents to the relevant information while excluding background information ready known to the receiver
so far rization researchers have given considerably more attention to relevance than to background knowledge
in contrast this work puts ground knowledge in the foreground
ing on the realization that the choices made by human summarizers and annotators contain implicit information about their background knowledge we develop and compare niques for inferring background knowledge from summarization data
based on this work we dene summary scoring functions that explicitly model background knowledge and show that these scoring functions t man judgments signicantly better than lines
we illustrate some of the many tential applications of our framework
first we provide insights into human information importance priors
second we demonstrate that averaging the background knowledge of multiple potentially biased annotators or pora greatly improves summary scoring formance
finally we discuss potential cations of our framework beyond tion
introduction summarization is the process of identifying the most important information pieces in a document
for humans this process is heavily guided by ground knowledge which encompasses tions about the task and priors about what kind of information is important mani
despite its fundamental role background edge has received little attention from the rization community
existing approaches largely focus on the relevance aspect which enforces ilarity between the generated summaries and the source documents peyrard
figure a summary s results from the tion of the background knowledge k and the source document d
following peyrard s is lar to d relevance measured by a small but also brings new information compared to ground knowledge informativeness measured by a large
we can infer the unobserved k from the choices unexplained by the relevance criteria
in previous work background knowledge has usually been modeled by simple aggregation of large background corpora
for instance using tfidf sparck jones one may ize background knowledge as the set of words with a large document frequency in background corpora
however the assumption that frequently cussed topics reect what is on average known does not necessarily hold
for example sense information is often not even discussed liu and singh
also information present in background texts has already gone through the portance lter of humans e

writers and ers
in general a particular difculty preventing the development of proper background knowledge models is its latent nature
we can only hope to infer it from proxy signals
besides there is at present no principled way to compare and evaluate background knowledge models
s should be similar small relevance should be dierent large informativeness in this work we put the background knowledge in the foreground and propose to infer it from marization data
indeed choices made by human summarizers and human annotators provide plicit information about their background edge
we build upon a recent theoretical model of information selection peyrard which tulates that information selected in the summary results from desiderata low redundancy the mary contain diverse information high relevance the summary is representative of the document and high informativeness the summary adds new information on top of the background knowledge
the tension between these elements is encoded in a summary scoring function k that explicitly depends on the background knowledge k
as trated by fig
the latent k can then be inferred from the residual differences in information tion that are not explained by relevance and dancy
for example the black information unit in fig
is not selected in the summary despite being very prominent in the source document
intuitively this is explained if this unit is already known by the receiver
to leverage this implicit signal we view k as a latent parameter learned to best the observed summarization data
contributions
we develop algorithms for ring k in two settings when only pairs of ments and reference summaries pairs are observed sec

and when pairs of document and summaries are enriched with human judgments sec


in sec
we evaluate our inferred ks with spect to how well the induced scoring function k correlates with human judgments
our proposed algorithms signicantly surpass previous baselines by large margins
in sec
we give a geometrical perpespective on the framework and show that a clear geometrical structure emerges from real summarization data
the ability to infer interpretable importance ors in a data driven way has many applications some of which we explore in sec

sec

tatively reveals which topics emerge as known and unkown in the tted priors
moreover we can fer k based on different subsets of the data
by training on the data of one annotator we get a prior specic to this annotator
similarly one can nd domain specic k s by training on different datasets
this is explored in sec

where we alyze annotators and different summarization datasets yielding interesting insights

ing several potentially biased annotator specic or domain specic k s results in systematic alization gains
finally we discuss future work and tial applications beyond summarization in sec

our code is available at
epfl dlab klearn related work the modeling of background knowledge has ceived little attention by the summarization munity although the problem of identifying tent words was already encountered in some of the earliest work on summarization luhn
a simple and effective solution came from the eld of information retrieval using techniques such as tfidf on background corpora sparck jones
similarly dunning proposed the likelihood ratio test to identify highly descriptive words
these techniques are known to be useful for news summarization harabagiu and lacatusu
later approaches include heuristics to tify summary worthy bigrams riedhammer et al

also hong and nenkova proposed a supervised model for predicting whether a word will appear in a summary or not using a large set of features including global indicators from the new york times corpus which can then serve as a prior of word importance
conroy et al
proposed to model ground knowledge by aggregating a large random set of news articles
delort and alfonseca used bayesian topic models to ensure the extraction of informative summaries
finally louis vestigated background knowledge for update marization with bayesian surprise
these ideas have been generalized in an abstract model of importance peyrard discussed in the next section
background this work builds upon the abstract model duced by peyrard whose relevant aspects we briey present here
let t be a text and a function mapping a text to its semantic representation of the following form pt


pt n the semantic representation is a probability tribution p over so called semantic units jn
many different text representation techniques can be chosen e

topic models with topics as mantic units or a properly renormalized semantic vector space with the dimensions as semantic units
in the summarization setting the source ment d and the summary s are represented by ability distributions over the semantic units pd and ps
similarly k the background knowledge is represented as a distribution pk over semantic units
intuitively j is high whenever j is known
a summary scoring d or simply since the document d is never ambiguous can be derived from simple requirements d k where red captures the redundancy in the summary via the entropy h
rel reects the relevance of the summary via the kullback leibler kl divergence between the summary and the document
a good summary is expected to be similar to the original document i
e
the kl divergence should be low
finally inf models the informativeness of the summary via the kl divergence between the summary and the latent background knowledge k
the summary should bring new information i
e
the kl divergence should be high
in this work we x
the klearn framework as laid out in our framework texts are viewed as distributions over a choice of semantic units jn
we aim to infer a general k as the bution over these units that best explains rization data
we consider two types of data with and without human judgments

inferring k without human judgments assume we have access to a dataset xi of pairs of documents di and their associated summaries si di si
under the assumption that the si are good summaries e

generated by humans we infer the background knowledge k that best explains the observation of these summaries
deed if these summaries are good we assume that information has been selected to minimize dancy maximize relevance and maximize tiveness
use k and pk interchangeably when there is no guity
direct score maximization
a straightforward proach is to determine the k that maximizes the k score of the observed summaries
formally this corresponds to maximizing the function xi where acts as a regularization term ing k to remain similar to a predened distribution p
here p can serve as a prior about what k should be
the factor controls the emphasis put on the regularization
a rst natural choice for the prior p can be the uniform distribution u over semantic units
in this case we show in appendix b that maximizing eq
yields the following simple solution for k j j
si with the choice note that j is always positive as expected
this solution is fairly itive as it simply counts the prominence of each semantic unit in human written summaries and siders the ones often selected as interesting i
e
as having low values in the background knowledge
we denote this technique as to indicate the maximum score with uniform prior
surprisingly it does not involve documents whereas intuitively k should be a function of both the summaries and documents
however if such a simplistic model works well it could be applied to broader ios where the documents may not even be fully observed
alternatively we can choose the prior p to be the source documents di
then as shown in appendix b the solution becomes j j j
si here a conservative choice for to ensure the j itivity of j is min j
this model j is also intuitive as the resulting value of j would be higher if j is prominent in the document but not selected in the summary
this is for ple the case for the black semantic unit in fig

furthermore choosing d as the prior implies ing the documents as the only knowledge available and makes a minimal prior commitment as to what k should be
we denote this approach as
probabilistic model
when directly maximizing the score of observed summaries there is no antee that the scores of other unobserved maries remain low
a principled way to address this issue is to formulate a probabilistic model over the observations xi di si s where the partition function is computed over the set of all possible summaries of ment di
in practice we draw random summaries as negative samples to estimate the partition tion negative samples for each positive
then k is learned to maximize the likelihood of the data via gradient descent
to enforce the constraint of k being a probability tribution we parametrize k as the softmax of a vector k


of scalars
the vector k is trained with mini batch gradient descent to mize the negative log likelihood of the observed data
this approach is denoted as pm

inferring k with human judgments next we assume a dataset annotated with man judgments
observations come in the form si di hi where hi is a human assessment of how good si is as a summary of di
we can use this extra information to enforce high scoring low scoring summaries to also have a high low k scores
regression
as a rst solution we propose sion with the goal of minimizing the difference between the predicted k and the corresponding human scores on the training set
more formally the task is to minimize the following loss a xi where a is a scaling parameter to put k and hi on a comparable range
to train k with gradient descent we again parametrize k as the softmax of a vector of scalars
sec


we denote this approach as hreg
preference learning
in practice regression fers from annotation inconsistencies
in particular the human scores for some documents might be on average higher than for other documents which easily confuses the regression
preference learning pl is robust to these issues by learning the ative ordering induced by the human scores gao al

pl can be formulated as a binary sication task maystre where the input is a pair of data points si di hi s j d j h j and the output is a binary ag indicating whether si is better than s j i
e
hi h j j h j i j where is the logistic sigmoid function and l can be for example the binary cross entropy
again we perform mini batch gradient descent to train k
we denote this approach as hpl
comparison of approaches to compare the usefulness of various k s we need a way to evaluate them
fortunately there is a natural evaluation setup i plug k into k the summary scoring function described by eq
use the induced k to score summaries si and compute the agreement with human scores hi
to distinguish between the algorithms duced in sec
we adopt the following naming convention for scoring functions if the background knowledge k was computed using algorithm a we denote the corresponding scoring function by a e

hpl is the scoring function where k was ferred by hpl
data
we use two datasets from the text ysis conference tac shared task and
they contain and ics respectively
each topic was summarized by about systems and humans
all system maries and human written summaries were ually evaluated by nist assessors for readability content selection with pyramid nenkova and sonneau and overall responsiveness dang and owczarzak
in this evaluation we focus on the pyramid score as the framework is built to model the content selection aspect
semantic units
as in previous work peyrard we use words as semantic units
in sec
we also experiment with topic models
however different choices of text representations can be ily plugged in the proposed methods
words have the advantage of being simple and directly rable to existing baselines

nist

nist
kendall mr baselines lr icsi idf u





without human judgments ours pm


with human judgments ours
hpl
best training data optimal hpl












table comparison of background knowledge based on how well the induced k correlates with humans kendall s higher is better and how far written summaries are ranked compared to system maries mr lower is better
the improvements of and hpl over the baselines are signicant paired t test p

baselines
for reference we report the summary scoring functions of several baselines lexrank lr erkan and radev is a graph based approach whose summary scoring function is the average centrality of sentences in the summary
icsi gillick and favre scores summaries based on their coverage of frequent bigrams from the source documents
and haghighi and vanderwende measure vergences between the distribution of words in the summary and in the sources
js divergence is a symmetrized and smoothed version of kl gence
additionally we report the performance of choosing the uniform distribution for k denoted u and an idf baseline where k is built from the document frequency computed using the english wikipedia denoted as idf
for reference we report the performance of training and evaluating hpl on all data denoted as optimal
this sures the ability of hpl to t the training data
results
table reports the fold validation averaged over all topics in both and
the rst column reports the kendall s correlation between humans and the various mary scoring functions
the second column reports the mean rank mr of reference summaries among figure multi dimensional scaling projection of uments summaries and k inferred by hpl
the clidean distance in the projection approximates to kl divergence in the original space
the geometrical tuition that summaries documents and k should form a line with documents in the middle is simultaneously respected for different randomly selected topics from tac datasets
all summaries produced in the shared tasks when ranked according to the summary scoring functions
thus lower mr is better
first note that even techniques that do not rely on human judgments can signicantly outperform previous baselines
the results of are ticularly strong with large improvements despite the simplicity of the algorithm
indeed and have a time complexity of where n is the number of topics and run much faster than any other algorithm seconds on a single cpu to infer k from a tac dataset
despite being more principled pm does not outperform
improvements over baseline are also obtained by hpl which leverages the ne grained tion of human judgments
however even without beneting from supervision performs larly to hpl without signicant difference
also as expected the preference learning setup hpl is stronger and more robust than the regression setup hreg which does not signicantly outperform the uniform baseline u
therefore we use hpl when human judgments are available and when only summary pairs are available
a geometric view previously see fig
we mentioned that a good k corresponds to a distribution such that the mary s is different from k is large but still similar to the document d is small
furthermore the regularization term in eq
with p d enforcing small makes minimal commitment as to what k should look like i
e
no a priori information except the documents is assumed
viewing these distributions as points in clidean space the optimal arrangement for s d and k is on a line with d in between s and k
since human written summaries s and documents d are given inferring k intuitively consists in ing the point in high dimensional space matching this property for all document summary pairs
interestingly we can easily test whether this ometrical structure appears in real data with our inferred k
to do so we perform a simultaneous multi dimensional scaling mds embedding of documents di human written summaries si and k
in this space two distributions are close to each other if their kl divergence is low
we plot such an embedding in fig
for randomly chosen topics from and k inferred by hpl
we deed observe documents summaries and k nicely aligned such that the summaries are close to their documents but far away from k
this nding also holds for k inferred by
these observations are important for two sons
they show that general framework troduced in fig
is an appropriate model of the summarization data for any given topic the erence summaries are arranged on one side of the document
they deviate from the document in a systematic way that is explained by the repulsive tion of the background knowledge
human written summaries contain information from the document but not from the background knowledge which puts them on the border of the space
our models can be seen to infer an appropriate background knowledge that is common to a wide spectrum of topics as shown by the fact that k occupies the central point in the embedding of fig

applications we now investigate some applications arising from our framework
as k is easily interpretable we explore which units receive high or low scores
one can also use different subsets or aggregations of training data
here we look into annotator specic k s and domain specic k s
known unknown said also like say told one kill liberty new nation announcement investigation table example of words known and unknown according to the best k inferred by hpl
a word j is known unknown according to k when j is high low

qualitative analysis to understand what is considered as known j is high or unknown j is low we t our best model hpl using all tac data for two choices of semantic units i words and lda topics trained on the english wikipedia topics
in table we report the top known and known words
frequent but uninformative words like said or also are considered known and thus undesired in the summary
on the contrary known words are low frequency specic words that summarization systems systematically failed to extract although they were important according to humans
we emphasize that the inferred ground knowledge encodes different information than a standard idf
we provide a detailed ison between k and idf in appendix e
when using a text representation given by a topic model trained on wikipedia we obtain the lowing top most known topics described by words
government election party united state litical minister president

book published work new wrote life novel
air aircraft ship navy army service training well
ight
known topics appeared
suit
the following are identied as the top
series show episode tv lm season
card player chess game played hand team
university research college science sor research degree published
topics related to military and politics receive higher scores in k
given that these topics tend to be the most frequent in news datasets k trained with man annotations learns to penalize systems tting on the frequency signal within source uments
on the contrary series games and figure multi dimensional scaling projections of a annotators and domains
the euclidean distance in the projected space represents kl divergence in the original space
the disk size is proportional to how well the k performs on the full tac datasets as uated by the correlation kendall s between the duced k and human judgments
versity topics receive low scores and should be extracted more often by systems to improve their agreement with humans

inferring and domain specic background knowledge within the tac datasets the annotations are also tagged with an annotator id
it is thus possible to infer a background knowledge specic to each notator by applying our algorithms on the subset of annotations performed by the respective annotator
in and combined annotators are identied resulting in different k s
instead of analyzing only news datasets with human annotations like tac we can infer ground knowledge from any summarization dataset from any domain as long as document summary pairs are observed
to illustrate this we consider a large collection of datasets covering domains such as news legal documents product reviews pedia articles
these do not contain human annotations so we employ our algorithm to infer a k specic to each dataset
the detailed scription of these datasets is given in appendix c
structure of differences
to visualize the ences between annotators we embed them in using mds with two annotators being close if their k are similar
in fig
a each annotator is a dot whose size is proportional to how well its k generalizes to the rest of the tac datasets as uated by the correlation kendall s between the induced k and human judgments
the same dure is applied to domains and is depicted in fig
b
figure correlation with human judgments on tac datasets news domain resulting from averaging annotator specic k s and domain specic k s
distance to the optimal k computed by running hpl on the full tac datasets
news datasets appear at the center of all domains meaning that the news domain can be seen as an average of the peripheral non news domains
thermore the k s trained on different news datasets are close to each other indicating a good level of intra domain transfer and unsurprisingly news datasets also exhibit the best transfer performance on tac
improvements due to averaging
based on vious observations we make the hypothesis that averaging different annotator specic k can lead to better correlation with human judgments on the unseen part of the tac dataset
similarly news domains generalize better than other domains
we hypothesized that averaging domains may also sult in improved correlations with humans in the news domain
in fig
we report the improvements in relation with human judgments on tac news main resulting from averaging an increasing ber of annotators or domains
the error bars sent condence intervals arising from ing a different subset to compute the average
as we see increasing the number of annotators aged results in clear and signicant improvements
since the error bars are small which annotators are











newsdomains figure several k distributions visualized as density in glove embedding space
included in the averaging has little impact on the results
source documents as illustrated by initial results in appendix d
similarly averaging different domains also sults in signicant improvements
in particular averaging several non news domains gives better generalization to the news domain
furthermore fig
shows in the glove et al
embedding space the k s resulting from averaging a all annotators k s ferred by hpl all news datasets k s inferred by and c all non news datasets k s ferred by in comparison to the optimal k learned with hpl trained on all data from tac datasets
to produce these visualizations we form a density estimation of the k s in the projection of word embeddings
all averaged k s tend to be similar to the mal k
it indicates that only one prior produces strong results on the news datasets and it can be obtained by averaging many biased but different k s
this is further conrmed by fig
where the distance to the optimal k measured in terms of kl divergence signicantly decreases when more annotators are averaged
conclusion we focus on the often ignored background edge for summarization and infer it from implicit signals from human summarizers and annotators
we introduced and evaluated different approaches observing strong abilities to t the data
the newly gained ability to infer interpretable priors on importance in a data driven way has many potential applications
for example we can scribe which topics should be extracted more quently by systems to improve their agreement with humans
using pretrained priors also helps systems to reduce overtting on the frequency signal within that the y axis has been normalized to put the ent divergences on a comparable scale
an important application made possible by this framework is to infer k on any meaningful subset of the data
in particular we learned specic k s which yielded interesting insights some annotators exhibit large differences from the others and averaging several potentially biased k s results in generalization improvements
we also inferred k s from different summarization datasets and also found increased performance on the news domain when averaging k s from diverse domains
for future work different choices of semantic units can be explored e

learning k directly in the embedding space
also we xed to get comparable results across methods but cluding them as learnable parameters could provide further performance boosts
investigating how to infuse the tted priors into summarization systems is another promising direction
more generally inferring k from a sense task like summarization can provide insights about general human importance priors
inferring such priors has applications beyond tion as the framework can model any information selection task
acknowledgments we gratefully acknowledge partial support from facebook google microsoft the swiss national science foundation grant and the european union grant tailor
references jean carletta simone ashby sebastien bourban mike flynn mael guillemot thomas hain jaroslav kadlec vasilis karaiskos wessel kraaij melissa kronenthal guillaume lathoud mike lincoln iain mccowan wilfried post agnes lisowska dennis reidsma and pierre wellner

the in ami meeting corpus a pre announcement
proceedings of the second international conference on machine learning for multimodal interaction page berlin heidelberg
springer verlag
filippo galgani paul compton and achim hoffmann

citation based summarisation of legal texts
in pricai trends in articial intelligence pages berlin heidelberg
springer berlin heidelberg
jianpeng cheng and mirella lapata

neural summarization by extracting sentences and words
in proceedings of the annual meeting of the sociation for computational linguistics volume long papers pages
association for putational linguistics
arman cohan franck dernoncourt doo soon kim trung bui seokhwan kim walter chang and zli goharian

a discourse aware attention model for abstractive summarization of long uments
in proceedings of the conference of the north american chapter of the association for computational linguistics human language nologies volume short papers pages new orleans louisiana
association for tional linguistics
john m
conroy judith d
schlesinger and dianne p
topic focused multi document oleary

summarization using an approximate oracle score
in proceedings of the coling acl main ference poster sessions pages sydney australia
association for computational tics
hoa trang dang and karolina owczarzak

overview of the tac update summarization task
in proceedings of the text analysis conference tac workshop notebook papers and results pages
hoa trang dang and karolina owczarzak

overview of the tac update summarization task
in proceedings of the first text analysis ference tac pages
hoa trang dang and karolina owczarzak

overview of the tac summarization track
in proceedings of the text analysis conference tac pages
hoa trang dang and karolina owczarzak

overview of the tac summarization track
in proceedings of the first text analysis conference tac pages
jean yves delort and enrique alfonseca

alsum a topic model based approach for update summarization
in proceedings of the ence of the european chapter of the association for computational linguistics pages
ted dunning

accurate methods for the tics of surprise and coincidence
computational guistics
kavita ganesan chengxiang zhai and jiawei han

opinosis a graph based approach to stractive summarization of highly redundant ions
in proceedings of the international ference on computational linguistics coling pages
yang gao christian m
meyer and iryna gurevych

april interactively learning to summarise by combining active preference learning and forcement learning
in proceedings of the ference on empirical methods in natural language processing pages brussels belgium
association for computational linguistics
dan gillick and benoit favre

a scalable global in proceedings of the model for summarization
workshop on integer linear programming for ural language processing pages boulder colorado
association for computational tics
aria haghighi and lucy vanderwende

ing content models for multi document tion
in proceedings of human language gies the annual conference of the north american chapter of the association for tional linguistics pages
sanda harabagiu and finley lacatusu

topic themes for multi document summarization
in ceedings of the annual international acm gir conference on research and development in information retrieval pages
karl moritz hermann tomas kocisky edward stette lasse espeholt will kay mustafa suleyman and phil blunsom

teaching machines to read and comprehend
in advances in neural formation processing systems pages
kai hong and ani nenkova

improving the estimation of word importance for news in proceedings of the document summarization
conference of the european chapter of the sociation for computational linguistics pages gothenburg sweden
association for tational linguistics
manoel horta ribeiro kristina gligoric and robert west

message distortion in information in the world wide web conference page cades
new york ny usa
association for computing machinery
gnes erkan and dragomir r
radev

lexrank graph based lexical centrality as salience in text journal of articial intelligence summarization
research pages
byeongchang kim hyunwoo kim and gunhee kim

abstractive summarization of reddit posts in with multi level memory networks
ings of the conference of the north american chapter of the association for computational guistics human language technologies volume long and short papers pages neapolis minnesota
association for computational linguistics
mahnaz koupaee and william yang wang

ihow a large scale text summarization dataset
corr

chin yew lin

rouge a package for in text matic evaluation of summaries
rization branches out proceedings of the workshop pages barcelona spain
tion for computational linguistics
hugo liu and push singh

conceptnet a practical commonsense reasoning tool kit
bt technology journal
annie louis

a bayesian method to rate background knowledge during automatic text summarization
in proceedings of the annual meeting of the association for computational guistics volume short papers pages baltimore maryland
hans peter luhn

the automatic creation of literature abstracts
ibm journal of research velopment
inderjeet mani

advances in automatic text marization
mit press cambridge ma usa
lucas maystre

efcient learning from isons
epfl lausanne
ryan mcdonald

a study of global inference algorithms in multi document summarization
in proceedings of the european conference on formation retrieval research pages
shashi narayan shay b
cohen and mirella lapata

do nt give me the details just the summary topic aware convolutional neural networks for treme summarization
in proceedings of the conference on empirical methods in natural guage processing pages brussels gium
association for computational linguistics
ani nenkova and rebecca passonneau

ing content selection in summarization the in proceedings of the human mid method
guage technology conference of the north can chapter of the association for computational linguistics hlt naacl pages boston massachusetts usa
romain paulus caiming xiong and richard socher

a deep reinforced model for abstractive summarization
corr

jeffrey pennington richard socher and christopher manning

glove global vectors for word in proceedings of the representation
ference on empirical methods in natural language processing emnlp pages doha qatar
association for computational linguistics
maxime peyrard

a simple theoretical model of importance for summarization
in proceedings of the annual meeting of the association for putational linguistics pages florence italy
association for computational linguistics
maxime peyrard and judith eckle kohler

a general optimization framework for document summarization using genetic in proceedings of rithms and swarm intelligence
the international conference on computational linguistics coling pages
avinesh p
v
s
maxime peyrard and christian m
meyer

live blog corpus for summarization
in proceedings of the international ence on language resources and evaluation pages
korbinian riedhammer benoit favre and dilek hakkani tr

long story short global unsupervised models for keyphrase based speech communication ing summarization

evan sandhaus

the new york times annotated corpus
linguistic data consortium philadelphia
abigail see peter j
liu and christopher d
ning

get to the point summarization in proceedings with pointer generator networks
of the annual meeting of the association for computational linguistics volume long papers pages vancouver canada
association for computational linguistics
karen sparck jones

a statistical interpretation of term specicity and its application in retrieval
journal of documentation
wei zhao maxime peyrard fei liu yang gao tian m
meyer and steffen eger

moverscore text generation evaluating with contextualized beddings and earth mover distance
in proceedings of the conference on empirical methods in natural language processing and the tional joint conference on natural language cessing emnlp ijcnlp pages hong kong china
association for computational guistics
markus zopf maxime peyrard and judith kohler

the next step for multi document summarization a heterogeneous multi genre pus built with a novel construction approach
in proceedings of the international conference on computational linguistics technical papers pages
figure visualization of each annotator s k based on projection of glove word embedding
figure visualization of each domain s k based on projection of glove word embedding
a visualization of k for each annotator and each domain we produce visualizations in the embedding space with the same procedure as in fig

fig
depicts the annotators and fig
depicts the domains
it is interesting to observe much more diversity ing from the domains and the domain specic k s are more spread out in the semantic space
this reects the greater topic diversity discussed in ferent domains
in contrast each annotator s k is inferred based on the tac datasets which are in the same domain news
b derivation of approaches the direct score maximization model consists in maximizing l we use lagrange multipliers with the constraint that k is a valid distribution first with p u the uniform and we have the following derivatives j d j dl j j j j j j j setting the lagrange derivative to yields j j where is the normalizing constant
in particular when j j
note that choosing ensures that for all we have
second we consider the case p d the document and
u changes with every document summary pair and l becomes l d s d s l j then only the the derivative concerning is modied and becomes d j j j sumlegalreports which gives the following solution after setting the lagrange derivative to the ami corpus carletta et al
is a dard product review summarization dataset
j j j
here it is not clear that j is positive for ery units
to avoid such issue notice that we can choose min j j j
c datasets the summarization track at the text analysis ference tac was a direct continuation of the duc series
in particular the main tasks of dang and owczarzak and dang and owczarzak were renements of the pilot update summarization task of duc
a dataset of topics was released as part of the edition and new topics were created in
and became standard benchmark datasets
the new york times annotated corpus haus counts as one of the largest rization datasets currently available
it contains nearly million carefully selected articles from the new york times each with summaries written by humans
also the cnn daily mail dataset hermann et al
has been decisive in the recent opment of neural abstractive summarization see et al
paulus et al
cheng and lapata
it contains cnn and daily mail articles together with bullet point summaries
zopf et al
also viewed the quality wikipedia featured articles as summaries for which potential sources were automatically searched on the web
p
v
s
al
recently crawled the blog archives from the bbc and the guardian gether with some bullet point summaries reporting the main developments of the event covered
to evaluate their opinion oriented tion system ganesan et al
constructed the opinosis dataset
it contains articles discussing the features of commercial products e

ipod s battery life
furthermore we consider the large pubmed taset cohan et al
a collection of scientic publications
the reddit dataset kim et al
has been collected on popular sub reddits
koupaee and wang automatically crawled the wikihow website using the reported bullet points as summaries
the xsum dataset narayan et al
is a large collection of news articles with a focus on abstractive summaries
to measure the effect of information distortion in summarization cascades of scientic results horta ribeiro et al
collected manual maries of various lengths
we also included the legalreport dataset et al
where the task is to summarize legal documents
d extracting summaries example once k is specied the summary scoring tion k can be used to extract summaries
for extractive summarization this is an optimal set selection problem mcdonald
tunately k is not linear and can not be optimized with integer linear programming
it is also not modular and can not be optimized with the greedy algorithm for submodularity
we have to rely on generic optimization techniques which do not make any assumption about the objective function and can approximately optimize any arbitrary function
we use the genetic algorithm proposed by peyrard and eckle kohler which creates and tively optimizes summaries over time
we denote as k gen the summarization system mately solving the subset selection problem
we compare systems when k is inferred by when k is inferred by hpl and when k is the form distribution
for reference we report the standard summarization baselines described in the previous section
the summaries are evaluated with automatic evaluation metrics call with stopwords removed lin and a recent bert based evaluation metric mover zhao et al

the results reported in table are encouraging since the systems based on the learned priors outperform the uniform prior
they also perform well in comparison to baselines
the inferred prior can benet systems by preventing them from overtting on the frequency signal

com genetic swarm mds employ a fold cross validation setup
dataset creation man

input type genre summary length size topics doc topic scisumm cnn daily mail nyt corpus opinosis liveblogs hmds pubmed xsum reddit ami wikihow legalreports mdic m m a a a m a m a a a m a a m mds mds mds sds sds mds temporal mds sds sds sds mds sds sds cascade news news sci
news news review snippets heter
sci
news heter
meeting heter
legal sci
varying k k k k k k k table description of datasets used in the experiments visualization of optimal k renormalized idf and their absolute difference
one bar for word in the support words
table comparison of summarization systems based on maximizing the summary scoring function k duced by different background knowledge
mover mover baselines lr

icsi kl greedy

js gen ours u gen gen hpl gen























e comparison idf vs
optimal k to verify that our inferred k contains different formation from id we compare idf and our mal k see sec

to be comparable idf weights need to be malized as the idf weights of known unknown words would be low high whereas pk would be high low
thus we compute c j for each word where c j
in fig
we represent the full distributions over all words in the support of k and show the absolute difference with renormalized idf weights
furthermore fig
is a scatter plot where each dot represent a word and the coordinates are its idf and k weights
the low correlation between the two indicates that k learns a different signal than idf
scatter plot where each dot is a word and the coordinates are its probability in k and its renomalized idf
optimal krenormalized idfsabsolute differenceoptimal k inferred by hplidfs
