structsum summarization via structured representations vidhisha balachandran dheeraj rajagopal artidoro pagnoni jaime carbonell school of computer science carnegie mellon university pittsburgh usa vbalacha apagnoni jaylee dheeraj jgc
cmu
edu jay yoon lee yulia tsvetkov e f l c
s c v
v i x r a abstract abstractive text summarization aims at pressing the information of a long source ument into a rephrased condensed summary
despite advances in modeling techniques stractive summarization models still suffer from several key challenges i layout bias they overt to the style of training corpora ii limited abstractiveness they are optimized to copying n grams from the source rather than generating novel abstractive summaries iii lack of transparency they are not in this work we propose a pretable
work based on document level structure tion for summarization to address these lenges
to this end we propose incorporating latent and explicit dependencies across tences in the source document into end to end single document summarization models
our framework complements standard decoder summarization models by ing them with rich structure aware document representations based on implicitly learned latent structures and externally derived guistic explicit structures
we show that our summarization framework trained on the cnn dm dataset improves the coverage of content in the source documents generates more abstractive summaries by generating more novel n grams and incorporates pretable sentence level structures while forming on par with standard baselines
introduction text summarization aims at identifying important information in long source documents and ing it in human readable summaries
two nent methods of generating summaries are tive dorr et al
nallapati et al
where important sentences in the source article are lected to form a summary and abstractive rush and data available at
vidhishanair al
see et al
where the model restructures and rephrases essential content into a paraphrased summary
state of the art approaches to abstractive rization employ neural encoder decoder methods that encode the source document as a sequence of tokens producing latent document tions and decode the summary conditioned on the representations
recent studies suggest that these models suffer from several key challenges
first since standard training datasets are derived from news articles model outputs are strongly affected by the layout bias of the articles with models ing on the leading sentences of source documents kryscinski et al
kedzie et al

ond although they aim to generate paraphrased summaries abstractive summarization systems ten copy long sequences from the source causing their outputs to resemble extractive summaries lin and ng gehrmann et al

finally current methods do not lend themselves easily to interpretation via intermediate structures lin and ng which could be useful for identifying major bottlenecks in summarization models
to address these challenges we introduce sum a framework that incorporates structured ument representations into summarization els
structsum complements a standard decoder architecture with two novel components a latent structure attention module that adapts structured representations kim et al
liu and lapata for the summarization task and an explicit structure attention module that porates an external linguistic structure e

erence links
the two complementary components are incorporated and learned jointly with the coder and decoder as shown in figure
encoders with induced latent structures have been shown to benet several tasks including ument classication natural language inference liu and lapata cheng et al
and machine translation kim et al

our tent structure attention module builds upon liu and lapata to model the dependencies tween sentences in a document
it uses a variant of kirchhoff s matrix tree theorem tutte to model such dependencies as non projective tree

the explicit attention module is linguistically motivated and aims to incorporate inter sentence links from externally annotated ument structures
we incorporate a coreference based dependency graph across sentences which is then combined with the output of the latent ture attention module to produce a hybrid aware sentence representation

we test our framework using the cnn dm dataset hermann et al
and show in
that it outperforms the base pointer generator model see et al
by up to
l
we nd that the latent and explicit structures are complementary both contributing to the nal performance improvement
our modules are also orthogonal to the choice of an underlying decoder architecture rendering them exible to be incorporated into other advanced models
quantitative and qualitative analyses of maries generated by structsum and baselines reveal that structure aware summarization gates the news corpora layout bias by improving the coverage of source document sentences
ally structsum reduces the bias of copying large sequences from the source inherently making the summaries more abstractive by generating more novel n grams than a competitive baseline
we also show examples of the learned interpretable sentence dependency structures motivating further research for structure aware modeling
structsum framework consider a source document consisting of n tences s where each sentence is composed of a sequence of words
document tion aims to map the source document to a target summary y of m words y
a typical neural stractive summarization system is an attentional sequence to sequence model that encodes the put sequence as a continuous sequence of kens w using a standard encoder hochreiter and schmidhuber vaswani et al

the encoder produces a set of hidden representations h
a decoder maps the previously generated token to a hidden state and computes a soft attention probability distribution over encoder hidden states
a distribution p over the vocabulary is computed at every time step t and the network is trained using the negative log likelihood loss losst log
structsum modies the above architecture as lows
we aggregate the token representations from the encoder to form sentence representations as in hierarchical encoders yang et al

we then use and explicit structure attention ules to augment the sentence representations with sentence dependency information leveraging both a learned latent structure and an external structure from other nlp modules
the attended vectors are then passed to the decoder which produces the put abstractive summary
in the rest of this section we describe our framework architecture shown in figure in detail

sentence representations we consider an encoder which takes a sequence of words in a sentence w as input and produces contextual hidden representation for each word hwik where wik is the kth word of the ith sentence k q and q is the number of words in the sentence
the word hidden representations are max pooled at the sentence level and passed through a sentence encoder which produces new hidden sentence representations for each sentence hsi
the sentence hidden representations are then passed as inputs to the latent and explicit structure attention modules

latent structure ls attention we model the latent structure of a source document as a non projective dependency tree of sentences and force a pairwise attention module to cally induce this tree
we denote the marginal ability of a dependency edge as aij where zij is the latent variable representing the edge from sentence i to sentence j
we terize the unnormalized pairwise scores between sentences with a neural network and use the choff s matrix tree theorem tutte to pute the marginal probability of a dependency edge between any two sentences
specically we decompose the representation of a sentence si into a semantic vector gsi and structure vector dsi as hsi gsi
using the structure vectors dsi dsj we compute a score fij between sentence pairs i j where sentence i is figure structsum incorporates latent structure ls
and explicit structure es
attention to produce structure aware representations
here structsum augments the pointer generator model but the methodology that we proposed is general and it can be applied to other encoder decoder summarization systems the parent node of sentence j and a score ri where the sentence is the root node fij and ri where fp fc and fr are linear projection tions that build representations for the parent child and root nodes respectively and wa is the weight for bilinear transformation
here fij is the edge weight between nodes i j in a weighted cency graph f and is computed for all pairs of sentences
using fij and ri we compute ized attention scores aij and ar i using a variant of kirchhoff s matrix tree theorem where aij is the marginal probability of a dependency edge tween sentences i j and ar i is the probability of sentence i being the root
using these probabilistic attention weights and the semantic vectors gs we compute the tended sentence representations as n psi ajigsj ar i groot n csi aijgsi lsi psi csi possible parents of sentence i csi is the context vector gathered from possible children and groot is a special embedding for the root node
here the updated sentence representation lsi incorporates the implicit structural information

explicit structure es attention following durrett et al
who showed that modeling coreference knowledge through anaphora constraints leads to improved clarity or cality we incorporate cross sentence coreference links as the source of explicit structure
first we use an off the shelf coreference to identify coreferring mentions
we then build a coreference based sentence graph by adding a link between tences sj if they have any coreferring tions
this graph is converted into a weighted graph by incorporating a weight on the edge between two sentences that is proportional to the number of unique coreferring mentions between them
we normalize these edge weights for every sentence effectively building a weighted adjacency matrix k where kij is given by where psi is the context vector gathered from
com kij p zij mj mv where mi denotes the set of unique mentions in mj denotes the set of co referring tence si mi mentions between the two sentences and z is a latent variable representing a link in the ence sentence graph
is a smoothing hyperparameter
given contextual sentence representations hs and our explicit coreference based weighted jacency matrix k we learn an explicit aware representation as follows usi tsi kijusj p esi where fu and fe are linear projections and esi is an updated sentence representation which incorporates explicit structural information
finally to combine the two structural sentations we concatenate the latent and explicit sentence vectors as hsi lsi esi to form coder sentence representations of the source ument
to provide every token representation with the context of the entire document the ken representations are concatenated with their corresponding structure aware sentence tation hwij hwij hsi where si is the tence to which the word wij belongs
the resulting structure aware token representations can be used to directly replace previous token representations as input to the decoder
experiments dataset we evaluate our approach on the cnn daily mail hermann et al
nallapati et al
and use the same cessing steps as shown in see et al

the cnn dm has train val test samples respectively
the reference summaries have an average of tokens and
sentences
differing from see et al
we truncate source documents to tokens instead of in training and validation sets to model longer documents with more sentences
all our ments were trained on nvidia gtx titan x gpus
base model although structsum framework can be incorporated in any encoder decoder work with structure aware representations for our experiments we chose the pointer generator model see et al
as the base model due to its simplicity and ubiquitous usage as a neural tive summarization model across different domains liu et al
krishna et al

the word and sentence encoders are bilstm and the coder is a bilstm with a pointer based copy anism
we re implement the base pointer generator model and augment it with the structsum modules described in and hence our model can be directly compared to it
baselines in addition to the base model we compare structsum with the following baselines tan et al
this is a graph based attention model that is closest in spirit to the method we present in this work
a graph attention module is used to learn attention between sentences but it can not be easily used to induce interpretable ument structures since its attention scores are not constrained to learn structure
on top of latent and interpretable structured attention between tences structsum introduces an explicit structure component to inject external document structure which distinguishes it from tan et al

gehrmann et al
this work introduces a separate content selector which tags words and phrases to be copied
the diffmask variant is an end to end variant like ours and hence is included in our baselines
we compare structsum with the diffmask experiment
hyperparameters our encoder uses den states for both directions in the one layer stm and for the single layer decoder
we use the adagrad optimizer duchi et al
with a learning rate of
and an initial tor value of

we do not use dropout and use best results from gehrmann et al
outperform diffmask experiment but they use inference time hard ing which can be applied on ours
our baselines also exclude reinforcement learning rl based systems as they are not directly comparable but our approach can be introduced in an encoder decoder based rl system
since we do not rate any pretraining we do not compare with recent contextual representation based models liu and lapata

com atulkum pointer
nyu
summarizer model pointer generator see et al
pointer generator coverage see et al
graph attention tan et al
pointer generator diffmask gehrmann et al
rouge rouge rouge l











pointer generator re implementation pointer generator coverage re implementation latent structure ls attention explicit structure es attention ls es attention














table evaluation of summarization models on the cnn dm dataset
published abstractive summarization line scores are on top
the bottom section shows re implementations of see et al
and structsum results that incorporate latent and explicit document structure into the base models
structsum s utility is on par with the base models while introducing additional benets of better abstractiveness and intrepretability shown in
gradient clipping with a maximum norm of
we selected the best model using early stopping based on the rouge score on the validation dataset as our criteria
we also used the coverage penalty ing inference as shown in gehrmann et al

for decoding we use beam search with a beam width of
we did not observe signicant ments with higher beam widths
evaluation a standard rouge metric does not shed ingful light into the quality of summaries across important dimensions
as a recall based metric it is not suitable for assessing the abstractiveness of summarization it is also agnostic to layout biases and does not facilitate intrepretability of model cisions
we thus adopt automatic metrics tailored to evaluating separately each of these aspects
we compare structsum to our base model the generator network with coverage see et al
and the reference

automatic metrics we rst conduct a standard comparison of ated summaries with reference summaries using and l lin metric
table shows the results
we rst observe that introducing the latent structures and explicit structures dently improves our performance on rouge l
it suggests that modeling dependencies between sentences helps the model compose better long sequences compared to baselines
we see small improvements in and
org project ing that we retrieve similar content words as the baseline but compose them into better contiguous sequences
as both es and ls independently get similar performance the results show that ls tion induces good latent dependencies that make up for pure external coreference knowledge
finally our combined model which uses both tent and explicit structure performs the best with an improvement of
points in rouge l and
points in over base pointer generator model statistically signicant for samples at
using wilson condence test
it shows that the latent and explicit information are plementary and a model can jointly leverage them to produce better summaries
additionally we nd that structural inductive bias helps a model to converge faster
the combined attention model converges in k iterations in son to k iterations required for the generator network
while rouge is a popular metric used for uating summarization models it is limited to only evaluating n gram overlap while ignoring tic correctness
hence we compared our method with the baseline pointer generator model using the bertscore metric zhang et al

we observe that our model improves bertscore by points
for pointer generator v s
for structsum showing that our model is able to erate semantically correct content

abstractiveness despite being an abstractive model the generator model tends to copy very long sequences of words including whole sentences from the figure comparison of novel n grams between structsum pointer and the erence
here sent indicates full novel sentences
figure coverage of source sentences in summary
here the axis is the sentence position in the source article and y axis shows the normalized count of tences in that position copied to the summary
source document also observed by gehrmann et al

we use two metrics to evaluate the tiveness of the model copy length table shows a comparison of the average length copy len of ous copied sequences from the source document greater than length
we observe that the generator baseline on average copies
ous tokens from the source which shows the tive nature of the model
this indicates that pointer networks aimed at combining advantages from abstractive and extractive methods by allowing to copy content from the input document tend to skew towards copying particularly in this dataset
a sequence of this is that the model fails to interrupt copying at desirable sequence length
in contrast modeling document structure through structsum reduces the length of copied sequences to
words on average reducing the bias of copying sentences entirely
this average is closer to the reference
words in comparison without ricing task performance
structsum learns to stop when needed while still generating coherent maries
novel n grams the proportion of novel grams generated has been used in the literature to measure the degree of abstractiveness of rization models see et al

figure pares the percentage of novel n grams in structsum as compared to the baseline model
our model duces novel trigrams
of the time and copies whole sentences only
of the time
in parison the pointer generator network has only
novel trigrams and copies entire sentences
of the time
this shows that structsum on average generates
more novel n grams in comparison to the pointer generator baseline

coverage a direct outcome of copying shorter sequences is being able to cover more content from the source document within given length constraints
we serve that this leads to better summarization mance
we compute coverage by computing the number of source sentences from which ous sequences greater than length are copied in the summary
table shows a comparison of the coverage of source sentences in the summary tent
while the baseline pointer generator model only copies from
of the source sentences structsum copies content from
of the source sentences
additionally the average length of the summaries produced by structsum remains mostly unchanged at words on average compared to of the baseline model
this indicates that sum produces summaries that draw from a wider selection of sentences from the original article pared to the baseline models

layout bias neural abstractive summarization methods applied to news articles are typically biased towards ing and generating summaries based on the rst few sentences of the articles
this stems from the structure of news articles which present the salient information of the article in the rst few sentences copy len coverage depth structsum reference





structsum



table distribution of latent tree depth
table results of analysis of copying and coverage distribution over the source sentences on cnn dm test set
copy len denotes the average length of copied sequences coverage coverage of source sentences
coref ner precision recall





table precision and recall of es and ls shared edges and expand in the subsequent ones
as a result the lead baseline which selects the top three sentences of an article is widely used in the ture as a strong baseline to evaluate summarization models applied to the news domain narayan et al

kryscinski et al
observed that the current summarization models learn to exploit the layout biases of current datasets and offer limited diversity in their outputs
to analyze whether structsum also holds the same layout biases we compute a distribution of source sentence indices that are used for copying content copied sequences of length or more are considered
figure shows the distributions of source sentences covered in the summaries
the coverage of sentences in the reference summaries shows a high proportion of the top sentences of any article being copied to the summary
ally the reference summaries have a smoother tail end distribution with relevant sentences in all sitions being copied
it shows that a smooth bution over all sentences is a desirable feature
we notice that the pointer generator framework have a stronger bias towards the beginning of the cle with a high concentration of copied sentences within the top sentences of the article
in trast structsum improves coverage slightly having a lower concentration of top sentences and copies more tail end sentences than the baselines
ever although the modeling of structure does help our model has a reasonable gap compared to the reference distribution
we see this as an area of improvement and a direction for future work
analysis of induced document structures similar to liu and lapata we also look at the quality of the intermediate structures learned by the model
we use the chu liu edmonds gorithm chu and liu edmonds to extract the maximum spanning tree from the tion score matrix as our sentence structure
table shows the frequency of various tree depths
we nd that the average tree depth is
and the average proportion of leaf nodes is consistent with results from tree induction in document tion ferracane et al

further we compare latent trees extracted from structsum with rected graphs based on coreference on ner or on both
these are constructed similarly to our explicit coreference based sentence graphs in
by ing sentences with overlapping coreference tions or named entities
we measure the similarity between the learned latent trees and the explicit graphs through precision and recall over edges
the results are shown in table
we observe that our latent graphs have low recall with the tic graphs showing that our latent graphs do not capture the coreference or named entity overlaps explicitly suggesting that the latent and explicit structures capture complementary information
figure shows qualitative examples of induced structures along with summaries from the sum
the rst example shows a tree with sentence chosen as root which was the key sentence tioned in the reference
in both examples the tences in the lower level of the dependency tree contribute less to the generated summary
ilarly in the examples source sentences used to generate summaries tend to be closer to the root node
in the rst summary all source content tences used in the summary are either the root node or within depth of the root node
in the second ample out of source sentences were at in the tree
in both examples generated summaries diverged from the reference by omitting certain sentences used in the reference
these sentences are in the lower section of the tree providing sights on which sentences were preferred for the figure examples of induced structures and generated summaries
summary generation
we also see in example that the latent structures cluster sentences based on the main topic of the document
sentences differ from sentences in the topic discussed and our model clustered the two sets separately
related work data driven neural summarization falls into tive cheng et al
zhang et al
or abstractive rush et al
see et al
gehrmann et al
chen and bansal
pointer generator see et al
learns to either generate novel in vocabulary words or copy from the source
it has been the foundation for much work on abstractive summarization gehrmann et al
hsu et al
song et al

our model extends it by incorporating latent explicit structure but these extensions are applicable to any other encoder decoder architecture
for ple a follow up study has already shown benets of our method in multi document summarization chowdhury et al

in pre neural era document structure played a critical role in summarization leskovec et al
litvak and last liu et al
rett et al
kikuchi et al

more cently song et al
infuse source syntactic structure into the pointer generator using level syntactic features and augmenting them to decoder copy mechanism
in contrast we model sentence dependencies as latent structures and plicit coreference structures we do not use tics or salient features
li et al
propose structural compression and coverage regularizers incorporating structural bias of target summaries while we model the structure of the source ument
frermann and klementiev induce latent structures for aspect based summarization cohan et al
focus on summarization of scientic papers isonuma et al
reviews supervised summarization mithun and kosseim use discourse structures to improve ence in blog summarization and ren et al
use sentence relations for multi document rization
these are complementary directions to our work
to our knowledge structsum is the rst to jointly incorporate latent and explicit document structure in a summarization framework
conclusion and future work in this work we propose the framework sum for incorporating latent and explicit document documentlatent
leicester city have rejected approaches for striker tom lawrence from an astonishing nine clubs

the former manchester united forward has barely played for leicester since arriving from old trafford in the summer but manager nigel pearson wants to have all options available as he battles against the odds to keep leicester in the premier league

lawrence is poised to make his full international debut for wales in their european championship qualifier with israel on saturday but has only figured in four games for leicester this season and three as a substitute

leicester city have rejected approaches for striker tom lawrence from an astonishing nine clubs

championship promotion chasers bournemouth ipswich and wolves have all asked about lawrence

blackburn charlton leeds bolton rotherham and wigan have also made contact

however they are now looking at other options in a last gasp bid to bolster their squad
reference bournemouth ipswich wolves blackburn charlton leeds bolton rotherham and wigan have all asked about tom lawrence
the year old is poised to make his full international debut for wales
leicester manager nigel pearson wants to have options available
structsum leicester city have rejected approaches for tom lawrence
lawrence is poised to make his debut for wales in their european championship qualifier with israel on saturday
leicester city are looking at other options in last gasp bid to bolster their squad
lawrence from old trafford has only figured in four games for leicester this season and three as a substitute
the former manchester united star has
andrew henderson celebrated landing the london broncos coaching job on a permanent basis as halifax were beaten

henderson was given the nod by the london hierarchy this week after a mixed spell in caretaker charge since the departure of joey grima

his weakened side put on a fine show to crown his appointment though scoring four tries through daniel harrison matt garside iliess macani and brad dwyer whose score was the winning one

iliess macani pictured last year scored one of london broncos four tries in the win over halifax

james saltonstall ben heaton and mitch cahalane scored for halifax

henderson had spoken earlier in the week about how he felt broncos were moving in the right direction and their narrow victory put some substance to his words

the win was just their third in six in the kingstone press championship having been relegated from super league at the end of last season
andrew henderson won his first game as broncos full time coach
daniel harrison matt garside iliess macani and brad dwyer all scored
james saltonstall ben heaton and mitch cahalane scored for halifax
structsum andrew henderson celebrated landing the coaching job on a permanent basis
henderson was given the nod by london hierarchy this week after a mixed spell in the win over halifax
his weakened side put on fine show to crown his appointment though he felt broncos were moving in the right direction
the win was their third in six in the press championship having been relegated structure in neural abstractive summarization
we introduce a novel explicit attention module which incorporates external linguistic structures tiating it with coreference links
we show that our framework improves the abstractiveness and coverage of generated summaries and helps igate layout biases associated with prior models
we present an extensive evaluation of along abstractiveness coverage and layout titatively
future work will investigate the role of document structures in pretrained language models lewis et al
liu and lapata
acknowledgements their to the anonymous the authors are grateful invaluable feedback and reviewers for to sandeep subramanian waleed ammar and kathryn mazaitis for their help and support in ious stages of the project
this material is based upon work supported by the darpa semafor and nnsa doe programs
any opinions ndings and conclusions or recommendations expressed in this material are those of the and do not essarily reect the views of the darpa or nnsa
we would also like to thank amazon for providing gpu credits
references yen chun chen and mohit bansal

fast tive summarization with reinforce selected sentence rewriting
in proceedings of the annual ing of the association for computational tics volume long papers pages bourne australia
association for computational linguistics
jianpeng cheng li dong and mirella lapata

long short term memory networks for machine reading
in proceedings of the conference on empirical methods in natural language processing pages austin texas
association for putational linguistics
tanya chowdhury sachin kumar chakraborty

rization with structural attention


and tanmoy neural abstractive arxiv preprint yau chu and tung kuan liu

on the shortest arborescence of a directed graph
science sinica
arman cohan franck dernoncourt doo soon kim trung bui seokhwan kim walter chang and zli goharian

a discourse aware attention model for abstractive summarization of long in proceedings of the conference of ments
the north american chapter of the association for computational linguistics human language nologies volume short papers pages new orleans louisiana
association for tional linguistics
bonnie dorr david zajic and richard schwartz

hedge trimmer a parse and trim approach to headline generation
in proceedings of the naacl on text summarization workshop volume pages
association for computational guistics
john duchi elad hazan and yoram singer

adaptive subgradient methods for online learning journal of machine and stochastic optimization
learning research
greg durrett taylor berg kirkpatrick and dan klein

learning based single document tion with compression and anaphoricity constraints
in proceedings of the annual meeting of the sociation for computational linguistics volume long papers pages berlin germany
association for computational linguistics
jack edmonds

optimum branchings
journal of research of the national bureau of standards b
elisa ferracane greg durrett junyi jessy li and trin erk

evaluating discourse in structured text representations
in acl
lea frermann and alexandre klementiev

ducing document structure for aspect based in proceedings of the annual rization
ing of the association for computational linguistics pages
sebastian gehrmann yuntian deng and alexander m
rush

bottom up abstractive summarization
in emnlp
karl moritz hermann tomas kocisky edward stette lasse espeholt will kay mustafa suleyman and phil blunsom

teaching machines to read and comprehend
in advances in neural information processing systems pages
sepp hochreiter and jurgen schmidhuber

neural computation long short term memory

wan ting hsu chieh kai lin ming ying lee kerui min jing tang and min sun

a unied model for extractive and abstractive summarization using inconsistency loss
in acl
masaru isonuma junichiro mori and ichiro sakata

unsupervised neural single document marization of reviews via learning latent discourse structure and its ranking
in acl
chris kedzie kathleen mckeown and hal daume iii

content selection in deep learning models of in proceedings of the summarization
ference on empirical methods in natural language processing pages brussels belgium
association for computational linguistics
yuta kikuchi tsutomu hirao hiroya takamura abu okumura and masaaki nagata

single document summarization based on nested tree ture
in proceedings of the annual meeting of the association for computational linguistics ume short papers pages
yoon kim carl denton luong hoang and der m
rush

structured attention networks
in international conference on learning sentations iclr toulon france april conference track proceedings
kundan krishna sopan khosla jeffrey p bigham and zachary c lipton

generating soap notes from doctor patient conversations
arxiv preprint

wojciech kryscinski nitish shirish keskar bryan cann caiming xiong and richard socher

neural text summarization a critical evaluation
in proceedings of the conference on empirical methods in natural language processing and the international joint conference on natural guage processing emnlp ijcnlp pages hong kong china
association for tional linguistics
jure leskovec marko grobelnik and natasa frayling

learning sub structures of ment semantic graphs for document summarization
in linkkdd workshop pages
mike lewis yinhan liu naman goyal jan ghazvininejad abdelrahman mohamed omer levy ves stoyanov and luke zettlemoyer

bart denoising sequence to sequence pre training for natural language generation translation and comprehension
arxiv

wei li xinyan xiao yajuan lyu and yuanzhuo wang

improving neural abstractive document in marization with structural regularization
ceedings of the conference on empirical ods in natural language processing pages
chin yew lin

rouge a package for matic evaluation of summaries
text summarization branches out
hui lin and vincent ng

abstractive in rization a survey of the state of the art
ceedings of the aaai conference on articial ligence volume pages
multilingual information extraction and rization pages
association for tional linguistics
fei liu jeffrey flanigan sam thomson norman sadeh and noah a
smith

toward tive summarization using semantic representations
in proceedings of the conference of the north american chapter of the association for tional linguistics human language technologies pages denver colorado
association for computational linguistics
yang liu and mirella lapata

learning tured text representations
transactions of the ciation for computational linguistics
yang liu and mirella lapata

marization with pretrained encoders


text ijcnlp zhengyuan liu angela ng sheldon lee shao guang aiti aw and nancy f
chen

topic aware pointer generator networks for summarizing spoken conversations
ieee automatic speech nition and understanding workshop asru pages
shamima mithun and leila kosseim

discourse structures to reduce discourse incoherence in blog summarization
in proceedings of the international conference recent advances in natural language processing pages hissar bulgaria
association for computational linguistics
ramesh nallapati feifei zhai and bowen zhou

summarunner a recurrent neural network based quence model for extractive summarization of ments
in thirty first aaai conference on articial intelligence
ramesh nallapati bowen zhou cicero dos santos c aglar and bing xiang

tive text summarization using sequence to sequence in proceedings of the rnns and beyond
signll conference on computational natural guage learning pages berlin germany
association for computational linguistics
shashi narayan shay b
cohen and mirella lapata

do nt give me the details just the summary topic aware convolutional neural networks for in proceedings of the treme summarization
conference on empirical methods in natural guage processing pages brussels gium
association for computational linguistics
pengjie ren zhumin chen zhaochun ren furu wei liqiang nie jun ma and maarten de rijke

sentence relations for extractive summarization with deep neural networks
acm transactions on mation systems tois
marina litvak and mark last

graph based word extraction for single document summarization
in proceedings of the workshop on multi source alexander m
rush sumit chopra and jason weston

a neural attention model for abstractive in proceedings of the tence summarization
conference on empirical methods in natural guage processing emnlp lisbon portugal september pages
abigail see peter j
liu and christopher d
manning

get to the point summarization with generator networks
in proceedings of the nual meeting of the association for computational linguistics volume long papers pages vancouver canada
association for tional linguistics
kaiqiang song lin zhao and fei liu

infused copy mechanisms for abstractive tion
in coling
jiwei tan xiaojun wan and jianguo xiao

abstractive document summarization with a in proceedings based attentional neural model
of the annual meeting of the association for computational linguistics volume long papers pages
william thomas tutte

graph theory vol
of
encyclopedia of mathematics and its applications
ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan n gomez ukasz kaiser and illia polosukhin

attention is all in advances in neural information you need
cessing systems pages
zichao yang diyi yang chris dyer xiaodong he alexander j
smola and eduard h
hovy

erarchical attention networks for document cation
in hlt naacl
tianyi zhang v
kishore felix wu kilian q
berger and yoav artzi

bertscore evaluating text generation with bert
iclr
xingxing zhang mirella lapata furu wei and ming zhou

neural latent extractive document marization
in proceedings of the conference on empirical methods in natural language ing pages brussels belgium
association for computational linguistics

