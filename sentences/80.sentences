abstractive meeting summarization using dependency graph fusion siddhartha banerjee the pennsylvania state university university park pa usa
psu
edu prasenjit mitra qatar computing research institute tornado tower oor doha qatar
org
qa kazunari sugiyama national university of singapore computing drive singapore
nus
edu
sg p e s l c
s c v
v i x r a abstract automatic summarization techniques on meeting conversations veloped so far have been primarily extractive resulting in poor summaries
to improve this we propose an approach to generate abstractive summaries by fusing important content from several terances
any meeting is generally comprised of several discussion topic segments
for each topic segment within a meeting tion we aim to generate a one sentence summary from the most portant utterances using an integer linear programming based tence fusion approach
experimental results show that our method can generate more informative summaries than the baselines
categories and subject descriptors i

articial intelligence natural language processing guage generation keywords abstractive meeting summarization integer linear programming introduction
meeting summarization helps both participants and non participants by providing a short and concise snapshot of the most important content discussed in the meetings
a recent study revealed that people generally prefer abstractive summaries
table shows the human written abstractive summaries along with the generated extractive summaries from a meeting transcript
as can be seen the utterances are highly noisy and contain unnecessary information
even if an extractive summarizer can accurately sify these utterances as important and present them to a reader it is hard to read and synthesize information from such utterances
in contrast human written summaries are compact and readable
we propose an automatic way of generating short and concise abstractive summaries of meetings
any meeting conversation cludes dialogues on several topics
for example in table the participants converse on two topics design features and selling prices
given the most important sentences within a topic ment our goal is to generate a one sentence summary from each segment and appending them to form a comprehensive summary of the meeting
moreover we also aim to generate summaries that resemble human written summaries in terms of writing style
permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for prot or commercial advantage and that copies bear this notice and the full citation on the rst page
copyrights for party components of this work must be honored
for all other uses contact the owner
copyright is held by the author
www companion may florence italy
acm





table two sets of extractive summaries and gold standard human generated abstractive summaries from a meeting set follows set
set human generated extractive summary d um as well as uh characters d um different uh keypad styles and s symbols
d well right away i m wondering if there s um uh like with players if there are zones
a cause you have more complicated characters like european guages then you need more buttons
d i m thinking the price might appeal to a certain market in one region whereas in another it ll be different so d kay trendy probably means something other than just basic abstractive summary the team then discussed various features to consider in making the remote
set human generated extractive summary b like how much does you know a remote control cost
b well twenty ve euro i mean that s um that s about like eighteen pounds or something
d this is this gon na to be like the premium product kinda thing or b so i do nt know how how good a remote control that would get you
um
abstractive summary the project manager talked about the project nances and selling prices
to aggregate the information from multiple utterances we adapt an existing integer linear programming ilp based fusion nique
the fusion technique is based on the idea of merging dependency parse trees of the utterances
the trees are merged on the common nodes that are represented by the word and parts speech pos combination
each edge of the merged structure is represented as a variable in the ilp objective function and the tion will decide whether the edge has to be preserved or discarded
we modify the technique by introducing an anaphora resolution step and also an ambiguity resolver that takes the context of words into account
further to solve the ilp we introduce several straints such as desired length of the output
to the best of our knowledge our work is the rst to address the problems of readability grammaticality and content selection jointly for meeting summary generation without employing a based approach
we conduct experiments on the ami that consists of meeting transcripts and show that our best method performs extractive model signicantly on scores
vs


proposed approach dependency fusion on meeting data requires an algorithm that is robust for noisy data as utterances often have disuencies
our work applies fusion to all the important utterances within the topic segment to generate the best sub tree that satises the constraints and maximizes the objective function of the optimization problem

inf
ed
ac
uk ami table probabilities of relations from produced vbn

nsubjpass

aux
agent
advmod
auxpass
table content selection evaluation
rouge scores and and log likelihood score ll from the stanford dependency parser
method our abstractive model our abstractive model without anaphora resolution extractive model baseline


r


ll


these values using reuters corpora to obtain dominant relations from non conversational style of text
for example table shows the probabilities of outgoing edges from a node produced vbn
this term assigns the importance of grammatical relations to a node and only the relations that are more dominant from a node will be preferred
the term denotes the informativeness of a node calculated using hori and furui s formula
the last term in equation is based on the idea of lexical cohesion
towards the end of any segment generally more important discussions might happen that will conclude a particular topic and then start another
in order to take this fact into account we introduce the term px n where n and denote the total number of extracted utterances in a segment and the position of the utterance the edge belongs to in the set of n utterances respectively
in order to solve the above ilp problem we impose a number of constraints
some of the constraints have been directly adapted from the original ilp formulation
for example we use the same constraints for restricting one incoming edge per node as well as we impose the connectivity constraint to ensure a connected graph structure
further we restrict the subtree to have just one start edge and one end edge
this helps in preserving one root node as well as it limits to one end node for the generated tree
we also limit the generated subtree to have a maximum of nodes that controls the length of the summary sentence
we also add few linguistic constraints that ensure the coherence of the put such as every node can have maximum of one determinant
we also impose constraints to prevent cycles in the graph structure otherwise nding the best path from start and end nodes might be difcult
the nal graph is linearized to obtain a coherent sentence
in the linearization process we order the nodes based on their inal ordering in the utterance

experimental results the ami meeting corpus contains meeting transcripts in the test set along with their corresponding abstractive human written summaries as well as the annotations of topic segments
rouge is used to compare content selection of several approaches
we compared the content selection of our approach to an extractive summarizer which works as a baseline
we also compared our model without using anaphora resolution to see the impact of solving pronouns
all the summaries were compared against the human written summaries as reference
the results in table show that our method outperforms the other techniques on both and rouge r recall scores
over we computed a coarse estimate of grammaticality using the log likelihood score ll from the parser
our technique cantly outperforms the extractive method
in future work we plan to design an end to end framework for summary generation from meetings
figure a merged dependency graph structure edges in blue bold arrows to be retained to generate the summary for each topic segment
anaphora resolution step replaces pronouns with the original nouns in the previous utterance that they refer to in order to increase the chances of merging
consider the following utterances so we re designing a new remote control and um um as you can see it s supposed to be original without pronoun resolution these two utterances can not be merged
once we apply anaphora resolution it in the second utterance is modied to a new remote control and then both the utterances are fused into a common structure
the utterances are parsed using the stanford dependency parser
every individual utterance has an plicit root node
we add two dummy nodes in the graph the start node and the end node to ensure dened start and end points of the merged structure
the words from the utterances are tively added onto the graph
the words that have the same word form and pos tag are assigned to the same nodes
ambiguity resolver
suppose that a new word wi that has k biguous nodes where it can be mapped to
the k ambiguous nodes are referred to as mappable nodes
for every ambiguous mapping candidate we rst nd the words to the left and right of the pable node of the sentences and then compute the number of words in both the directions that are common to the words in either tion of the word wi
finally wi is mapped to the node that has the highest directed context
ilp formulation
figure shows the sub graph marked using blue bold arrows that we wish to retain from the merged graph structure to generate a one sentence summary from several merged utterances
all the sentences generated from each meeting script are concatenated to produce the nal abstractive summary
we need to maximize the information content of the generated tence keeping it grammatical
we model the problem as an teger linear programming ilp formulation similar to the dency graph fusion as proposed by fillipova and strube
the directed edges in the graph binary variables are represented as xg d l where g d and l denote the governor node dependent node and the label of an edge respectively
we maximize the following objective function xg d l g px n as shown in equation we introduce three different terms g and px n
each relation in a dependency graph consists of the governing node the dependent node and the relation type
the term g denotes the probabilities of the labels given a governor node g
for every node word and pos in the entire corpus the probabilities are represented as the ratio of the sum of the frequency of a particular label and the sum of the frequencies of all the labels emerging from a node
in this work we calculate acknowledgments this material is based upon work supported by the national science foundation under grant no


references k
filippova and m
strube
sentence fusion via dependency graph compression
in proc
of emnlp pages
c
hori and s
furui
a new approach to automatic speech summarization
ieee transactions on multimedia
g
murray and g
carenini
summarizing spoken and written conversations
in proc
of emnlp pages
g
murray g
carenini and r
ng
generating and validating abstracts of meeting conversations a user study
in proc
of inlg pages
t
rose m
stevenson and m
whitehead
the reuters corpus volume from yesterday s news to tomorrow s language resources
in proc
of lrec pages

