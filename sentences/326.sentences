extracting summary knowledge graphs from long documents zeqiu wu rik koncel kedziorski mari ostendorf hannaneh hajishirzi university of washington seattle wa usa kedzior ostendor
edu p e s l c
s c v
v i x r a abstract knowledge graphs capture entities and relations from long documents and can facilitate reasoning in many downstream applications
extracting compact knowledge graphs ing only salient entities and relations is important but lenging for understanding and summarizing long documents
we introduce a new text to graph task of predicting rized knowledge graphs from long documents
we develop a dataset of document graph pairs using automatic and human annotations
we also develop strong baselines for this task based on graph learning and text summarization and provide quantitative and qualitative studies of their effect
introduction knowledge graphs are popular representations of tant entities and their relationships
compact interpretable knowledge can graphs facilitate human data analysis as well as empower memory dependent knowledge based tions
this makes them ideal for modeling the content of documents
document level information extraction which captures relations across distant sentences can be used to construct knowledge graphs of documents jia wong and poon yao et al

these techniques focus on tracting all entities and relations from a document which for long and dense documents such as scientic papers may be hundreds or thousands
this poses a new challenge how do we determine the most important entities in a paper and the key relationships between them automatic summarization liu and lapata sunaga et al
addresses the problem of identifying salient information in a document but introduces the tional challenge of discourse structuring and in the tive case text generation as well
summarizing entities and relations directly as a rst step could decouple the mixed burdens on models and help assure the factual correctness of a summary in line with recent trends in evaluation for summarization wang cho and lewis durmus he and diab zhang et al

in this work we introduce the task of extracting from a scientic document a compact knowledge graph that sents its most important information
figure illustrates the situation a large knowledge graph can be extracted from the document but only a portion of entities and relations terize its main ideas colored nodes and thick edges while figure we introduce the task to extract a summary edge graph from a long document e

a scientic paper
this is an example from our dataset where the target mary graph should only contain entities and relations that are salient enough to be included in the abstract of the per
the entities or relations shown in light grey not found in the abstract should be removed
we omit entity and tion types for simplicity
the rest play a more minor role
our task emphasizes ing this salient subgraph
we support this task with a dataset of scientic document graph pairs that integrates matic and human annotations from existing knowledge sources in the scientic domain
we outline an evaluation paradigm that balances accuracy against redundancy while admitting the variability of textual reference to an entity
we develop and investigate two competitive baselines based on text summarization and graph learning models and compare to two simple frequency based methods
we vide an analysis of their tradeoffs and of the general lenges posed in the proposed dataset
for example we serve that missing entities and entity coreference errors in the predicted graphs have a large impact on relation racy
our hope is that this task and data will facilitate search into future models that can better capture these lenging but important textual relations
abstract


tree adjoining grammars have an extended domain of locality


two wide coverage lexicalized grammars of english lexsys and xtag which exploit edol


main papertree adjoining grammarsedolxtaglexsyslexicalized grammars of english background related work information extraction ie
most ie work focuses on extracting entities and their relational facts from a single sentence zhang et al
stanovsky et al

more recent work addresses document level ie which aims to capture relations across distant sentences jain et al
jia wong and poon yao et al
or to ll a pre dened metadata table with entities hou et al

jia wong and poon yao et al
and hou et al
formulate the task as classifying the relation type between each pair of truth entities expressed in the document
we do not assume the existence of ground truth entities and extract level relations directly from text
our work complements this trend of applying ie for long document understanding while addressing the need for focused compact knowledge representations
the closest work to our is jain et al
who separately explore the idea of identifying salient entities related to experimental sults using weak supervision provided by the papers with code dataset
in contrast we focus on identifying salient entities from the paper based on weak supervision from its abstract
this framing generalizes to a wider variety of uments and domains and supports diverse tasks including multi document summarization or building scientic edge bases
besides entity salience our task also requires models to identify the salience of relations which we show to be challenging

text summarization document summarization models create summaries by tifying the most important sentences from documents lapati zhai and zhou narayan cohen and lapata or using a decoder to generate abstractive summaries rush chopra and weston celikyilmaz et al

although text summarization tasks liu and lapata yasunaga et al
share our objective of distilling crucial information from documents they mix this objective with the goal of producing uent natural language text
we argue that summarizing entities and relations directly as the rst step could decouple the mixed burdens on els and help models to check the factual correctness of a summary
these advantages can benet other text tion tasks that rely on long document understanding and resentation such as generation grounded on long text
an increasing number of recent works wang cho and lewis durmus he and diab zhang et al
have proposed automatically evaluating summarization models by applying information extraction or question answering models to match entities or relations between generated and reference summaries
these newly proposed measures are found to have much higher correlation with human ments than standard measures
recent applications of large pretrained language models such as ribeiro et al
and kale show the promise of generating uent and accurate text from edge graphs highlighting the need for identifying correct with code paperswithcode
com underlying knowledge representations
in addition rized knowledge graphs from multiple documents can be naturally merged by collapsing shared entity nodes to bring even richer information
and such summarized structures can be more easily leveraged to facilitate downstream tasks
another line of work that is closely related to ours is graph based summarization which leverages graph tures of documents to facilitate the summary generation erkan and radev tan wan and xiao sunaga et al
huang wu and wang xu et al

these works try to leverage graphs that capture lations between sentences or discourse units
wang et al
incorporate graphs between entities extracted by sentence level ie systems without considering entity or lation salience

graph summarization in general graph summarization work can be categorized according to whether its goal is optimizing for memory or computational resources needed for processing or ing analysis liu et al

knowledge graph tion safavi et al
or node estimation park et al
are most relevant to our work but they normally take a huge knowledge graph for pruning based on a given query out any document context
unlike these works our task quires the model to handle different document contexts at inference where each document can contain a completely different knowledge graph with unique entities
our work is also similar to falke and gurevych who collect a small corpus data instances of concept map annotations from openie tuples for summarizing sets of documents
we choose a science specic annotation scheme which provides more structure as our target

scientic document understanding although our proposed idea of summarized knowledge graphs can be applied to documents in any domain we focus on scientic papers in this work
existing works toward derstanding scientic documents include but are not limited to information extraction luan et al
wadden et al
summarization cohan et al
collins stein and riedel yasunaga et al
fact tion wadden et al
and citation generation luu et al
xing fan and wan
recently attention to search in the scientic domain in the nlp community has grown even more wang et al
esteva et al
due to the urgent need for mitigating the global pandemic
task formulation we introduce a text to graph task of extracting a succinct structured knowledge graph which contains the most salient entities and relations from a document
more specically such a summarized knowledge graph should meet these ditions it contains only the most important entities from the document it includes relations between these entities only if they are crucial to understanding the main ideas of the text and each salient entity is only represented by a single node in the graph
these conditions are evaluated as entity salience relation salience and entity duplication rate respectively evaluation details in section
in our dataset see section where long documents are scientic papers the most salient entities and relations are dened to be those that can be included in paper abstracts
figure shows an example of this task
here only entities and relations that appear in the abstract should be included in the summary knowledge graph
those in grey should be moved even though they are mentioned in the paper because they are not necessary enough to describe or understand the main idea of the paper
formally we dene the problem as given a document d a pre dened entity type set tv and a relation type set tr predict a summarized knowledge graph g v e where each vi v represents a salient entity with entity type ti tv mentioned in d
each vi vj rk ij e represents an important edge from vi to vj with relation type rk ij tr
we note there can be multiple edges between vi and vj but ij rl rk ij l
each vi consists of a cluster of ni string names i from co referent entity mentions
i


mni i our dataset scigraphsumm we construct a text to graph dataset from a corpus of tic papers
our textual data consists of roughly puter science research papers taken from a corpus of million full text research papers and abstracts lo et al

we leverage abstracts to create summarized edge graphs for full papers
abstracts effectively contain summarized information from full documents and there are existing human annotation and information extraction ie systems that enable constructing relational graphs from stracts
due to the expense and difculty of annotation we only have access to a small number of human annotated summary graphs which we use to judge system performance
we call this data the human test set
for training and development we take a weakly supervised approach using automatically extracted summary graphs
we use a scientic ie system to extract summary graphs from abstracts and pair them with full papers from
entity relation graphs are also extracted for the full papers
we divide these graph paper ples into train dev and automatic test sets
the dev set can be used for parameter tuning purposes while the auto test set can be used to compare systems
randomly sampling examples we observe that over of extracted target entities for abstracts in the automatic test set are ful
moreover we will show in section that similar tem performance trends are observed in the human test set and automatic test set
table gives statistics about the number and size of the textual documents as well as the summary graphs for all data splits
the data collection and graph construction details are described in the sections that follow
the automatic test set is much larger than the human test set which reduces the problems of noise in automatic annotation
examples doc tokens graph entities graph relations train dev auto test human test











table statistics of each data split

manual summarized graph annotation we leverage the scierc scientic ie luan et al
for the human labeled test data
scierc consists of expert annotated paper abstracts labeled with entities types task method metric material other scientic term and generic co reference and relations types pare part of conjunction evaluate for feature of for hyponym of
we construct knowledge graphs from these annotations by collapsing coreferent mentions into a single node and linking all nodes via the annotated relations
of the abstracts in scierc have full texts able in
in order to guarantee information richness we discard pairs where annotated graphs have fewer than predicted relations
after such ltering our human test set consists of the knowledge graphs and full text of of these documents

automatic summarized graph annotation to facilitate model training and model development under a weakly supervised setting we automatically create target knowledge graphs for the remaining papers from their stracts
we leverage a state of the art scientic ie system that extracts entities relations and co references simultaneously wadden et al

we do not re train instead we use the pretrained model on scierc for all processing and modeling steps in this work
we construct knowledge graphs from the ie output by again collapsing coreferences to create entities which we associate with the list of coreferential text mentions
ie relations between tions become edges between the corresponding entities in the graph
the same sample ltering is applied

automatic full graph construction our task is designed to build a summarized graph directly from a document and in order to perform the task models do not have to use specic information extraction tools to build a full graph
however we provide the full edge graphs that we constructed from documents as part of our dataset for reproducibility and to encourage future ploration on graph learning models for our task
we process each full document text with in overlapping token windows reduce computation memory with each two consecutive chunks having one overlapped sentence to serve cross sentence co references
we collapse we additionally discard pairs with abstracts that are longer than tokens rare to avoid memory limitations of
discard sentences longer than tokens to guarantee one overlapped sentence between each consecutive chunks fewer than sentences discarded
tial mentions as previous steps and then collapse ence clusters from different windows with matching unique non generic mentions into a single graph node
a generic entity mention is a string excluding pronouns and determiners with more than one token or a unigram with an inverse document frequency idf in the training data that is higher than an empirically chosen threshold selected for high precision in identifying generic mentions
a generic entity mention is not clustered unless the model predicts it to be coreferent with some other entity mention
evaluation metrics the goal is to evaluate the correctness of the predicted mary knowledge graph compared with the ground truth mary graph
we rst align entity nodes in the predicted graph to nodes in the target graph
after the entity ment step we measure qualities of the predicted graphs entity salience relation salience and duplication rate der a relaxed alignment condition described next

entity alignment in the human test set we found annotated entity tions do not have exact string match in the main paper text
further analysis showed that many such cases are due to nor paraphrasing hyphenation differences or typos caused by ocr parsers that are used to process papers in pdf format
for example in domain monolingual pus in the abstract and in domain monolingual corpus in the paper are equivalent but do not have an exact match due to the hyphen difference
in addition as we do not assume any specic information extraction tools being used a similar sue for exact name matching may occur potentially due to different entity mention names being extracted by different models
therefore exact string match does not yield a good alignment and we instead use a relaxed alignment method that we found to be reasonably accurate for evaluation
another issue in aligning entities between two graphs is that the same entity can be referred to with multiple strings as each entity node represents a cluster of co referent tity mentions
to align a predicted node with a target node where either can have a cluster of mention types we nd the maximum similarity over all possible pairs
the similarity score between a target entity vi i and a j


mnj predicted entity vj i


mni i j is calculated as i mt j j max s t where we employ gestalt pattern matching ratcliff and metzener to calculate string similarity based on mon substrings
each predicted node is aligned with the get node that gives the highest similarity score subject to a minimum score

is selected such that in a set of relaxed but not exact alignments of them are manually inspected to be acceptable
the manually inspected samples fall into the ing categories paraphrases of target nodes we consider laxed alignment examples to be good if differences only involve typo hyphen item order or other paraphrases
for example log linear and linear interpolation versus linear and log linear interpolation different specicity level we consider aligned entities with different specicity level as relevant ments
for example speaker s intention prediction ules versus intention prediction modules
alignments with error we consider entities ing aligned that have distinct meanings to be bad ments
for example two dimensional analog of sorting versus one dimensional notion of sorting
in both human and auto test sets applying the relaxed alignment from full graph nodes to target nodes increases the percentage of aligned target nodes from by exact matching to

salience and duplication measures to calculate entity salience we align each predicted node with up to one target graph node collapsing multiple dicted nodes that map to the same target entity into a single node for calculating precision recall and scores
in other words if multiple predicted entities are aligned to the same target entity it is only counted once when calculating all scores
these metrics can be computed either with matching or ignoring entity types typed vs
untyped evaluation
as each entity should only have one entity type we adopt the dominating type among all mentions to be the entity type
since this process does not penalize predicted graphs where multiple nodes are aligned to a single target node we also calculate the duplication rate as the average number of dicted nodes which are aligned to each target node
based on entity alignment a target relation edge vi vj can be aligned to a predicted relation for vk vl if the sponding nodes align i
e
vi aligns to vk and vj aligns to vl
we evaluate relation salience based on such relation ments allowing for multiple relation types between each pair of entities
we report precision recall and scores for relation prediction with or without considering relation type and direction matching typed vs
untyped evaluation
when evaluating without relation type and direction we merge relations if multiple between an entity pair into a single edge
baseline models we develop two baseline models for the graph tion problem one using a text summarization model that tracts summary sentences from which we extract entities and relations and one that rst builds a full document graph and then applies a graph learning model to do graph pruning

text text graph ttg this model rst produces a text summary of the full ment text using the extractive summarizer bertsumext liu and lapata and subsequently uses entities and tions from the text summary to form a summary knowledge graph
we re train the original model on our dataset by placing the pre trained bert with scibert beltagy lo and cohan and increase the sequence length from to
entities and relations that pear within the text summary are used as the summarized graph

graph to graph g this model predicts a summary subgraph from the full graph extracted by described in section
we formulate subgraph selection as a node classication problem we encode the full graph with a gat velickovic et al
and use the resulting node representations to make a binary salience prediction
in the original gat a node vi is embedded with a able feature vector and contextualized via multi headed tention with its graph neighbors n vi in each graph tion layer
at each graph attention layer a vertex vi with neighborhood n vi is contextualized as vi vi ijwv vj jn vi ij zn vi here vi vi rh contextualized and original vector representations of vi
wv wk wq rhh are model parameters and ij are attention weights computed from the vertex representations
the formulation above was extended using multi head attention and layered with non linearities to produce the graph attention network
since the original gat does not consider different tion types between neighboring vertices to incorporate lation types into the model we use separate heads for ferent relation types in tr that is the head corresponding to relation type r tr is used to attend vi over those vj where vi and vj are connected by an edge with label r i
e
vi vj r e
as we have different relation types in our dataset we use heads in our gat
the sentations from all heads are concatenated and transformed via non linearity between model layers
at the node embedding layer we use four features to bed each entity node vi the number of mentions in the ument ni the section i d of the entity s rst appearance in the document the most frequent entity type among all mentions as predicted by ti and the pooled put representation from scibert beltagy lo and cohan of the longest mention string zi to encode each node as follows vi nin wssi wtti wes zi where n rh is a learnable unit feature vector for ni rns and ti rnt are the one hot vectors of si and ti respectively with ns and nt as the number of unique tion ids and node entity types in the dataset
s zi rhe is the hidden representation at the rst token from the nal layer of scibert
ws rhns wt rhnt we rhhe are trained model parameters
following the node embedding layer we contextualize each node representation with gat layers and pass each node through a nal binary classication layer to predict salience
to supervise the training of this model we apply the laxed alignment method in section to align full graph entities and target graph entities
all full graph entities that can be aligned are treated as positive examples and all ers as negative
finally we use a negative log likelihood loss function using all positive labeled as salient nodes and a negative sampling ratio of for training
we include all full graph relations between predicted tities in the output summary graph
we leave better relation prediction for future study
implementaion details we manually tune the rameters of gat based on dev set entity performance curve versus training steps
we x most of the model rameters e

vector dimension number of layers batch size
the only parameters being tuned are learning rate dropout rate and negative sampling ratio
but we only manually change each parameter value if we serve performance instability on the dev set for the rst training steps
the average number of tuning trials for each parameter is fewer than times
finally we set negative pling ratio dropout rate
and learning rate for all experiments with our g model
we use adam timizer
we do not netune scibert the base model used in gat
we run each experiment on a single titan rtx
we select the model checkpoint based on its typed relation score performance on valid set
experiments
evaluated systems we compare the ttg and g models in section with two frequency based baselines pagerank pr the top k most authorized entities in the full document graph with the highest pagerank scores page et al
where each edge weight is initialized by the number of relation mentions between the entity pair topk freq tkf the top k most frequent entities
k is selected to be for both of the els which is the average number of full graph nodes aligned to target nodes in the training set
in both cases relations from the full document graphs between the selected ties are added to generate the predicted summary graph
we also report performance of gold entity ge which vides the performance upper bound when relying on the tities and relations that can be extracted from the full text with
ge picks the full graph node with the est similarity score for each target node with a lower old of
for inclusion
again the predicted graph includes all relations found in the full graph between these entities

quantitative results table shows precision recall and scores for both typed and typed entity and relation prediction as well as entity duplication rates for the automatic test set
we see similar evaluation trends with untyped and typed evaluation with lower scores when type matching is required as pected
both g and ttg outperform other baselines in ent p ent r ent rel p rel r rel untyped typed ent p ent r ent rel p rel r rel e dup pr tkf ttg g



ge




























































table full results for untyped and typed entity relation evaluation p r for precision and recall and entity duplication rate e dup on auto test set
all scores are in except entity duplication rate
note that entity duplication rates are expected to be the same for both untyped and typed evaluation as entity alignment only considers string name matching
ent p ent r ent rel p rel r rel untyped typed ent p ent r ent rel p rel r rel e dup pr tkf ttg g



ge




























































table full results for untyped and typed entity relation evaluation p r for precision and recall and entity duplication rate e dup on human test set
all scores are in except entity duplication rate
human test ttg g ttg g auto test task method metric material



other scientic term

generic

















table entity relaxed match by entity type
both entity and relation evaluations
in particular g sistently performs the best in scores though ttg has higher precision
when evaluating entity duplication rate ttg consistently performs the best
however all systems performances are still far from ge which shows signicant room for improvement on this task in the future
table gives the same results for the human test set
most of the trends observed with the automatic test set hold for the human test set
the exception is that g shows better formance on entities but is less effective on typed relations
therefore we argue that the automatic test set is reasonable to be used as an extra set to test systems during development

qualitative analysis we looked at a handful of abstracts in the human test set to analyze the scoring criteria proposed for this task
the target graphs were further hand annotated to identify the most important entities among all salient entities in the stract eliminating some generic nodes e

method non essential other scientic term nodes and occasional duplicated nodes
in some cases we added entities that were not in the gold reference but were identied by one or more of the automatic algorithms and deemed appropriate
entities identied by the automatic algorithms were aligned to the reduced target set
for untyped entities the recall is higher on the reduced set for g and pagerank suggesting that these algorithms may be better capturing the most salient entities
errors in entity types often involved clear cases
trends in precision on the reduced graph were consistent with the automatic scoring
what we also noticed is that some of the aligned predicted nodes contain unrelated entities due to coreference errors from ie systems which in part explains the low relation scores together with the impact of missing inserted entities
we investigate the causes of tkf performing poorly pared with g and ttg
in specic we analyze why quency may not be a good indicator of salient entities in some cases
we rst calculate the average length of mention string names of predicted salient entities and nd out tkf has an average length of
while both ttg and g have
on average
furthermore we nd out that some tant entities tend to have long string names especially when paper authors start introducing some specic tasks or els
these entities tend to be split into smaller segments in later parts of the paper for more detailed explanations
such smaller segments tend to be mentioned more frequently and thus predicted by tkf although sometimes they are not comprehensive enough to qualify as salient entities
for ample a key entity bayesian semi supervised chinese word segmentation model can be detected as salient by both g and ttg while tkf only predicts chinese words and word segmentation as the closest salient entities
another ple is that tkf predicts kl one systems as a salient entity for a paper while the gold entity is kl one like knowledge representation systems which is predicted by both g and ttg
as noted in table the sizes of target graphs and full pers are different in the human and automatic test sets
we observe that ttg produces graphs of similar size for papers in the two test sets about nodes and edges
by contrast g produces graphs of different sizes for different ment lengths averaging nodes and edges for human test set but nodes and edges for automatic test set where documents are longer and target graphs bigger
we observe interesting trends regarding the sections where entities rst appear
ge entities appear in the rst tion of the full paper only of the time for both auto and human test sets
about and of them have their rst mention in middle sections and nal sections tively
these numbers are also consistent with both test sets
this observation highlights the fact that extracting the main idea of the paper needs the understanding of the full per
however partly due to the sequence length limitation of bertsumext ttg is extremely biased towards entities in the rst section
g is less vulnerable to such bias but still often fails to include entities from later paper sections in its summaries
table shows the entity score based on relaxed match for each different entity type
we calculate these scores by comparing subgraphs of the predicted and target graphs that contain entities of a certain type only
entities of type ric are the hardest to predict
this correlates with the fact that metric entities are least likely to appear in the rst section of a paper
vs
overall in human test set
another possible reason for this is that metric is the least frequent salient entity type
only and of all get salient entities have the type metric in auto and human test set respectively
figure sample output from g and ttg
figure shows an example where a target metric entity rst appearing in the second section of a paper is correctly predicted by g but missed by ttg
the red incorrect relation edges show that only relying on full graph relations limits relation prediction performance
this is evidenced in tables and where even ge gives low relation prediction scores due to the absence of gold target relations in the full document graphs
conclusions and future work we have described a new text to graph task for ing summary knowledge graphs from full text documents including a standard preprocessed open access dataset and evaluation techniques to facilitate further research
we have investigated graph classication and text summarization techniques for this task and detailed some of their qualities in our analysis
as we show that relation salience prediction is a rather challenging task in extracting summary knowledge graphs it would be an important further investigation
leveraging document level ie and graph learning techniques would also be an interesting direction to explore
models that can merge entity nodes better will lead to lower entity duplication rate and improved relation accuracy
one major shortcoming of our gat model is that we do not consider the context of each entity mention in the document incorporating tual information for entity mentions would also be a ing research direction
references beltagy i
lo k
and cohan a

scibert a trained language model for scientic text
in emnlp
celikyilmaz a
bosselut a
he x
and choi y

deep communicating agents for abstractive tion
in naacl
cohan a
dernoncourt f
kim d
s
bui t
kim s
chang w
and goharian n

a discourse aware tention model for abstractive summarization of long uments
in naacl
collins e
augenstein i
and riedel s

a vised approach to extractive summarisation of scientic papers
in conll
durmus e
he h
and diab m

feqa a question answering evaluation framework for faithfulness ment in abstractive summarization
in acl
erkan g
and radev d
r

lexrank graph based lexical centrality as salience in text summarization
in journal of articial intelligence research
esteva a
kale a
paulus r
hashimotoa k
yin w
radev d
and socher r

co search information retrieval with semantic search arxiv tion answering and abstractive summarization


falke t
and gurevych i

bringing structure into summaries crowdsourcing a benchmark corpus of cept maps
in emnlp
hou y
jochim c
gleize m
bonin f
and ganguly d

identication of tasks datasets evaluation metrics and numeric scores for scientic leaderboards tion
in acl
huang l
wu l
and wang l

knowledge augmented abstractive summarization with driven cloze reward
in acl
si corpusspeaker independent recognitionspeaker adaptationword error ratesi corpusspeaker independent recognitionspeaker adaptationword error ratesi corpusspeaker independent recognitionspeaker adaptationtarget velickovic p
cucurull g
casanova a
romero a
p
and bengio y

graph attention networks
in iclr
wadden d
lin s
lo k
wang l
l
zuylen m
v
han a
and hajishirzi h

fact or fiction verifying scientic claims
arxiv

wadden d
wennberg u
luan y
and hajishirzi h

entity relation and event extraction with alized span representations
in emnlp
wang a
cho k
and lewis m

asking and swering questions to evaluate the factual consistency of summaries
in acl
wang d
liu p
zheng y
qiu x
and huang x

heterogeneous graph neural networks for extractive ument summarization
in acl
wang l
l
lo k
chandrasekhar y
reas r
yang j
burdick d
eide d
funk k
katsis y
kinney r
li y
liu z
merrill w
mooney p
murdick d
rishi d
sheehan j
shen z
stilson b
wade a
wang k
wang n
x
r
wilhelm c
xie b
raymond d
weld d
s
etzioni o
and kohlmeier s

the in acl nlp covid open research dataset
workshop
xing x
fan x
and wan x

automatic generation of citation texts in scholarly papers a pilot study
in acl
xu j
gan z
cheng y
and liu j

aware neural extractive text summarization
in acl
yao y
ye d
li p
han x
lin y
liu z
liu z
huang l
zhou j
and sun m

docred a scale document level relation extraction dataset
in acl
yasunaga m
kasai j
zhang r
fabbri a
r
li i
friedman d
and radev d
r

scisummnet a large annotated corpus and content impact models for scientic paper summarization with citation networks
in aaai
zhang y
merck d
tsai e
b
manning c
d
and glotz c
p

optimizing the factual correctness of a summary a study of summarizing radiology reports
in acl
zhang y
zhong v
chen d
angeli g
and manning c
d

position aware attention and supervised data improve slot filling
in emnlp
jain s
zuylen m
v
hajishirzi h
and beltagy i

scirex a challenge dataset for document level tion extraction
in acl
jia r
wong c
and poon h

document level n ary relation extraction with multiscale representation learning
in naacl hlt
kale m

text to text pre training for data to text tasks
arxiv

liu y
and lapata m

text summarization with trained encoders
in emnlp
liu y
safavi t
dighe a
and koutra d

graph summarization methods and applications a survey
in acm computing surveys
lo k
wang l
l
neumann m
kinney r
and weld d
s

the semantic scholar open research corpus
in acl
luan y
he l
ostendorf m
and hajishirzi h

multi task identication of entities relations and erence for scientic knowledge graph construction
in emnlp
luu k
koncel kedziorski r
lo k
cachola i
and arxiv smith n
a

citation text generation


nallapati r
zhai f
and zhou b

summarunner a recurrent neural network based sequence model for tractive summarization of documents
in aaai
narayan s
cohen s
b
and lapata m

ing sentences for extractive summarization with ment learning
in naacl hlt
page l
brin s
motwani r
and winograd t

the pagerank citation ranking bringing order to the web
in technical report stanford infolab
park n
kan a
dong x
l
zhao t
and faloutsos c

estimating node importance in knowledge graphs using graph neural networks
in kdd
ratcliff j
w
and metzener d
e

pattern matching the gestalt approach
dr
dobb s journal
ribeiro l
f
r
schmitt m
schutze h
and gurevych i

investigating pretrained language models for to text generation
arxiv

rush a
m
chopra s
and weston j

a neural attention model for abstractive sentence summarization
in emnlp
safavi t
belth c
faber l
mottin d
muller e
and koutra d

personalized knowledge graph rization from the cloud to your pocket
in icdm
stanovsky g
michael j
zettlemoyer l
and dagan i

supervised open information extraction
in naacl
tan j
wan x
and xiao j

abstractive ment summarization with a graph based attentional ral model
in acl

