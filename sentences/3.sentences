a m l c
s c v s c v i x r a catching the drift probabilistic content models with applications to generation and summarization regina barzilay computer science and ai lab mit
mit
edu lillian lee department of computer science cornell university
cornell
edu abstract we consider the problem of modeling the tent structure of texts within a specic main in terms of the topics the texts address and the order in which these topics appear
we rst present an effective knowledge lean method for learning content models from annotated documents utilizing a novel tation of algorithms for hidden markov els
we then apply our method to two plementary tasks information ordering and tractive summarization
our experiments show that incorporating content models in these plications yields substantial improvement over previously proposed methods
publication info hlt naacl ceedings of the main conference pp

introduction the development and application of computational els of text structure is a central concern in natural guage processing
document level analysis of text ture is an important instance of such work
ous research has sought to characterize texts in terms of domain independent rhetorical elements such as schema items mckeown or rhetorical relations mann and thompson marcu
the focus of our work however is on an equally fundamental but domain dependent dimension of the structure of text content
our use of the term content corresponds roughly to the notions of topic and topic change
we desire models that can specify for example that articles about earthquakes typically contain information about quake strength location and casualties and that descriptions of casualties usually precede those of rescue efforts
but rather than manually determine the topics for a given domain we take a distributional view learning them directly from un annotated texts via analysis of word distribution patterns
this idea dates back at least to harris who claimed that various types of word recurrence patterns seem to characterize various types of discourse
advantages of a distributional perspective clude both drastic reduction in human effort and tion of topics that might not occur to a human expert and yet when explicitly modeled aid in applications
of course the success of the distributional approach depends on the existence of recurrent patterns
in trary document collections such patterns might be too variable to be easily detected by statistical means
ever research has shown that texts from the same domain tend to exhibit high similarity wray
cognitive psychologists have long posited that this similarity is not accidental arguing that formulaic text structure facilitates readers comprehension and recall bartlett
in this paper we investigate the utility of specic content models for representing topics and topic shifts
content models are hidden markov models hmms wherein states correspond to types of information characteristic to the domain of terest e

earthquake magnitude or previous quake occurrences and state transitions capture possible information presentation orderings within that domain
we rst describe an efcient knowledge lean method for learning both a set of topics and the relations tween topics directly from un annotated documents
our technique incorporates a novel adaptation of the standard hmm induction algorithm that is tailored to the task of modeling content
then we apply techniques based on content models to two complex text processing tasks
first we consider formation ordering that is choosing a sequence in which to present a pre selected set of items this is an tial step in concept to text generation multi document summarization and other text synthesis problems
in our formulaic is not necessarily equivalent to simple so automated approaches still offer advantages over manual techniques especially if one needs to model several domains
experiments content models outperform lapata s state of the art ordering method by a wide margin for one domain and performance metric the gap was centage points
second we consider extractive the compression of a document by choosing rization a subsequence of its sentences
for this task we velop a new content model based learning algorithm for sentence selection
the resulting summaries yield match with human written output which compares vorably to the achieved by the standard leading n sentences baseline
the success of content models in these two mentary tasks demonstrates their exibility and ness and indicates that they are sufciently expressive to represent important text properties
these observations taken together with the fact that content models are ceptually intuitive and efciently learnable from raw ument collections suggest that the formalism can prove useful in an even broader range of applications than we have considered here exploring the options is an ing line of future research
related work knowledge rich methods models employing manual crafting of typically complex representations of content have generally captured one of three types of knowledge rambow kittredge et al
domain edge e

that earthquakes have magnitudes independent communication knowledge e

that scribing an event usually entails specifying its location and domain communication knowledge e

that reuters earthquake reports often conclude by listing previous
formalisms exemplifying each of these edge types are dejong s scripts mckeown s schemas and rambow s domain specic schemas respectively
in contrast because our models are based on a tributional view of content they will freely incorporate information from all three categories as long as such formation is manifested as a recurrent pattern
also in comparison to the formalisms mentioned above content models constitute a relatively impoverished tion but this actually contributes to the ease with which they can be learned and our empirical results show that they are quite effective despite their simplicity
in recent work duboue and mckeown propose a method for learning a content planner from a tion of texts together with a domain specic knowledge base but our method applies to domains in which no such knowledge base has been supplied
does not qualify as domain knowledge because it is not about earthquakes per
hearst knowledge lean approaches distributional models of content have appeared with some frequency in search on text segmentation and topic based language beeferman et al
modeling florian and yarowsky chen et al
iyer and ostendorf gildea and hofmann wu and khudanpur
the methods we in fact employ for learning content models are quite closely related to techniques proposed in that literature see section for more details
however language modeling research whose goal is to predict text probabilities tends to treat topic as a useful auxiliary variable rather than a central concern for example topic based distributional information is ally interpolated with standard non topic based n gram models to improve probability estimates
our work in contrast treats content as a primary entity
in particular our induction algorithms are designed with the explicit goal of modeling document content which is why they differ from the standard baum welch or em algorithm for learning hidden markov models even though content models are instances of hmms
model construction we employ an iterative re estimation procedure that ternates between creating clusters of text spans with similar word distributions to serve as representatives of within document topics and computing models of word distributions and topic changes from the clusters so derived
formalism preliminaries we treat texts as sequences of pre dened text spans each presumed to convey mation about a single topic
specifying text span length thus denes the granularity of the induced topics
for concreteness in what follows we will refer to sentences rather than text spans since that is what we used in our experiments but paragraphs or clauses could potentially have been employed instead
our working assumption is that all texts from a given domain are generated by a single content model
a tent model is an hmm in which each state s corresponds to a distinct topic and generates sentences relevant to that topic according to a state specic language model ps note that standard n gram language models can fore be considered to be degenerate single state content models
state transition probabilities give the probability of changing from a given topic to another thereby turing constraints on topic shifts
we can use the forward algorithm to efciently compute the generation ity assigned to a document by a content model and the clarity we omit minor technical details such as the use of dummy initial and nal states
section
describes how the free parameters k t and are chosen
the athens seismological institute said the temblor s center was located kilometers miles south of the capital
seismologists in pakistan s northwest frontier province said the temblor s epicenter was about kilometers miles north of the provincial capital peshawar
the temblor was centered kilometers miles west of the provincial capital of kunming about kilometers miles southwest of beijing a bureau seismologist said
figure samples from an earthquake articles sentence cluster corresponding to descriptions of location
viterbi algorithm to quickly nd the most likely model state sequence to have generated a given ment see rabiner for details
in our implementation we use bigram language els so that the probability of an n word sentence def wn being generated by a state s is n
estimating the state bigram q bilities is described below
in topic induction as initial previous work florian and yarowsky iyer and ostendorf wu and khudanpur we initialize the set of ics distributionally construed by partitioning all of the sentences from the documents in a given domain specic collection into clusters
first we create k clusters via complete link clustering measuring sentence similarity by the cosine metric using word bigrams as features figure shows example output
then given our knowledge that documents may sometimes discuss new irrelevant content as well we create an etcetera cluster by merging together all clusters containing fewer than t sentences on the assumption that such clusters consist of outlier sentences
we use m to denote the number of clusters that results
determining states emission probabilities and tion probabilities given a set


cm of m ters where cm is the etcetera cluster we construct a content model with corresponding states


sm we refer to sm as the insertion state
for each state si i m bigram probabilities which induce the state s sentence emission probabilities are timated using smoothed counts from the corresponding cluster psi def where is the frequency with which word sequence y occurs within the sentences in cluster ci and v is the barzilay and lee proper names bers and dates are temporarily replaced with generic tokens to help ensure that clusters contain sentences describing the same event type rather than same actual event
vocabulary
but because we want the insertion state sm to model digressions or unseen topics we take the novel step of forcing its language model to be complementary to those of the other states by setting psm def maxi i m psi puv maxi i m
note that the contents of the etcetera cluster are ignored at this stage
our state transition probability estimates arise from considering how sentences from the same article are tributed across the clusters
more specically for two clusters c and c let be the number of documents in which a sentence from c immediately precedes one from c and let be the number of documents taining sentences from c
then for any two states and sj i m we use the following smoothed estimate of the probability of transitioning from si to sj cj m
viterbi re estimation our initial clustering ignores sentence order however contextual clues may indicate that sentences with high lexical similarity are actually on different topics
for instance reuters articles about earthquakes frequently nish by mentioning previous quakes
this means that while the sentence the temblor injured dozens at the beginning of a report is probably highly salient and should be included in a summary of it the same sentence at the end of the piece probably refers to a different event and so should be omitted
a natural way to incorporate ordering information is iterative re estimation of the model parameters since the content model itself provides such information through its transition structure
we take an em like viterbi proach iyer and ostendorf we re cluster the tences by placing each one in the new cluster ci i m that corresponds to the state most likely to have erated it according to the viterbi decoding of the ing data
we then use this new clustering as the input to the procedure for estimating hmm parameters described above
the cluster estimate cycle is repeated until the clusterings stabilize or we reach a predened number of iterations
evaluation tasks we apply the techniques just described to two tasks that stand to benet from models of content and changes in topic information ordering for text generation and formation selection for single document summarization
these are two complementary tasks that rely on joint model functionalities the ability to order a set of pre selected information bearing items and the ability to do the selection itself extracting from an ordered quence of information bearing items a representative sequence

information ordering the information ordering task is essential to many synthesis applications including concept to text tion and multi document summarization while ing for the full range of discourse and stylistic factors that inuence the ordering process is infeasible in many mains probabilistic content models provide a means for handling important aspects of this problem
we strate this point by utilizing content models to select propriate sentence orderings we simply use a content model trained on documents from the domain of interest selecting the ordering among all the presented candidates that the content model assigns the highest probability to

extractive summarization content models can also be used for single document summarization
because ordering is not an issue in this this task tests the ability of content models to adequately represent domain topics independently of whether they do well at ordering these topics
the usual strategy employed by domain specic marizers is for humans to determine a priori what types of information from the originating documents should be included e

in stories about earthquakes the number of victims radev and mckeown some systems avoid the need white et al

for manual analysis by learning content selection rules from a collection of articles paired with authored summaries but their learning algorithms ically focus on within sentence features or very coarse structural features such as position within a graph kupiec et al

our content model based summarization algorithm combines the advantages of both approaches on the one hand it learns all required formation from un annotated document summary pairs on the other hand it operates on a more abstract and global level making use of the topical structure of the entire document
our algorithm is trained as follows
given a content model acquired from the full articles using the method scribed in section we need to learn which topics resented by the content model s states should appear in our summaries
our rst step is to employ the viterbi gorithm to tag all of the summary sentences and all of the sentences from the original articles with a viterbi topic label or v topic the name of the state most likely to have generated them
next for each state s such that at least three full training set articles contained v topic sentences in a single document summary follow the order of appearance in the original document
domain average standard vocabulary length deviation earthquakes clashes drugs finance accidents









size type




table corpus statistics
length is in sentences
cabulary size and type token ratio are computed after placement of proper names numbers and dates
s we compute the probability that the state generates sentences that should appear in a summary
this ability is estimated by simply counting the number of document summary pairs in the parallel training data such that both the originating document and the summary contain sentences assigned v topic s and then malizing this count by the number of full articles taining sentences with v topic s
to produce a summary of a new article the gorithm rst uses the content model and viterbi decoding to assign each of the article s sentences a v topic
next the algorithm selects those states chosen from among those that appear as the v topic of one of the article s sentences that have the highest probability of generating a summary sentence as estimated above
sentences from the input article corresponding to these states are placed in the output summary
evaluation experiments
data for evaluation purposes we created corpora from ve domains earthquakes clashes between armies and rebel groups drug related criminal offenses nancial reports and summaries of aviation accidents
specically the rst four collections consist of ap articles from the north american news corpus gathered via a tdt style ment clustering system
the fth consists of narratives from the national transportation safety board s database previously employed by jones and thompson for event identication experiments
for each such set articles were used for training a content model cles for testing and for the development set used for parameter tuning
table presents information about ticle length measured in sentences as determined by the sentence separator of reynar and ratnaparkhi vocabulary size and token type ratio for each domain
there are more than sentences we prioritize them by the summarization probability of their v topic s state we break any further ties by order of appearance in the document

sls
csail
mit
struct
parameter estimation document is computed as our training algorithm has four free parameters two that indirectly control the number of states in the induced tent model and two parameters for smoothing bigram probabilities
all were tuned separately for each main on the corresponding held out development set ing powell s grid search press et al

the ter values were selected to optimize system performance on the information ordering
we found that across all domains the optimal models were based on sharper language models e



the optimal number of states ranged from to

ordering experiments

metrics the intent behind our ordering experiments is to test whether content models assign high probability to ceptable sentence arrangements
however one stumbling block to performing this kind of evaluation is that we do not have data on ordering quality the set of sentences from an n document can be sequenced in n different ways which even for a single text of ate length is too many to ask humans to evaluate
tunately we do know that at least the original sentence order oso in the source document must be acceptable and so we should prefer algorithms that assign it high probability relative to the bulk of all the other possible permutations
this observation motivates our rst ation metric the rank received by the oso when all mutations of a given document s sentences are sorted by the probabilities that the model under consideration signs to them
the best possible rank is and the worst is n
an additional difculty we encountered in setting up our evaluation is that while we wanted to compare our algorithms against lapata s state of the art tem her method does nt consider all permutations see below and so the rank metric can not be computed for it
to compensate we report the oso prediction rate which measures the percentage of test cases in which the model under consideration gives highest probability to the oso from among all possible permutations we expect that a good model should predict the oso a fair fraction of the time
furthermore to provide some assessment of the quality of the predicted orderings themselves we follow lapata in employing kendall s which is a sure of how much an ordering differs from the oso the underlying assumption is that most reasonable tence orderings should be fairly similar to it
specically for a permutation of the sentences in an n section
for discussion of the relation between the ordering and the summarization task
n where is the number of swaps of adjacent tences necessary to re arrange into the oso
the metric ranges from inverse orders to identical orders


results for each of the unseen test texts we exhaustively enumerated all sentence permutations and ranked them using a content model from the corresponding domain
we compared our results against those of a bigram guage model the baseline and an improved version of the state of the art probabilistic ordering method of pata both trained on the same data we used
lapata s method rst learns a set of pairwise ordering preferences based on features such as noun verb dependencies
given a new set of sentences the latest version of her method applies a viterbi style tion algorithm to choose a permutation satisfying many preferences lapata personal communication
table gives the results of our ordering test son experiments
content models outperform the tives almost universally and often by a very wide margin
we conjecture that this difference in performance stems from the ability of content models to capture global ument structure
in contrast the other two algorithms are local taking into account only the relationships tween adjacent word pairs and adjacent sentence pairs respectively
it is interesting to observe that our method achieves better results despite not having access to the guistic information incorporated by lapata s method
to be fair though her techniques were designed for a larger corpus than ours which may aggravate data sparseness problems for such a feature rich method
table gives further details on the rank results for our content models showing how the rank scores were tributed for instance we see that on the earthquakes main the oso was one of the top ve permutations in of the test documents
even in drugs and accidents the domains that proved relatively challenging to our method in more than of the cases the oso s rank did not exceed ten
given that the maximal possible rank in these domains exceeds three million we believe that our model has done a good job in the ordering task
we also computed learning curves for the different mains these are shown in figure
not surprisingly formance improves with the size of the training set for all domains
the gure also shows that the relative difculty from the content model point of view of the different domains remains mostly constant across varying set sizes
interestingly the two easiest domains finance the optimal such permutation is np complete
i e t a r n o i t c e r p o s o domain system rank content earthquakes lapata
n a bigram
content
lapata n a bigram

content lapata n a bigram
content
n a lapata bigram

content lapata n a bigram
oso pred















clashes drugs finance accidents table ordering results averages over the test cases
domain rank range earthquakes clashes drugs finance accidents table percentage of cases for which the content model assigned to the oso a rank within a given range
and earthquakes can be thought of as being more mulaic or at least more redundant in that they have the token type ratios see table that is in these domains words are repeated much more frequently on average

summarization experiments the evaluation of our summarization algorithm was driven by two questions are the summaries produced of acceptable quality in terms of selected content and does the content model representation provide tional advantages over more locally focused methods to address the rst question we compare summaries created by our system against the lead baseline which extracts the rst sentences of the original text spite its simplicity the results from the annual ment understanding conference duc evaluation gest that most single document summarization systems can not beat this baseline
to address question we consider a summarization system that learns extraction rules directly from a parallel corpus of full texts and their summaries kupiec et al

in this system earthquake clashes drugs finance accidents training set size figure ordering task performance in terms of oso prediction rate as a function of the number of documents in the training set
rization is framed as a sentence level binary tion problem each sentence is labeled by the available boostexter system schapire and singer as being either in or out of the summary
the tures considered for each sentence are its unigrams and its location within the text namely beginning third dle third and end third
hence relationships between sentences are not explicitly modeled making this system a good basis for comparison
we evaluated our summarization system on the quakes domain since for some of the texts in this domain there is a condensed version written by ap journalists
these summaries are mostly consequently they can be easily aligned with sentences in the original articles
from sixty document summary pairs half were randomly selected to be used for training and the other half for testing
while thirty documents may not seem like a large number it is comparable to the size of the training corpora used in the competitive system evaluations mentioned above
the average ber of sentences in the full texts and summaries was and respectively for a total of sentences in each of the test and full documents of the training sets
at runtime we provided the systems with a full ument and the desired output length namely the length in sentences of the corresponding shortened version
the resulting summaries were judged as a whole by the tion of their component sentences that appeared in the human written summary of the input text
the results in table conrm our hypothesis about the benets of content models for text summarization our model outperforms both the sentence level feature set yielded the best results among the several possibilities we tried
were dropped
one or two phrases or more rarely a clause y c a r u c c a n o i a z i r a m m u s system content based sentence classier words location leading n sentences extraction accuracy table summarization task results
model size ordering summarization table content model performance on earthquakes as a function of model size
ordering oso prediction rate summarization extraction accuracy
ber of states both metrics induce similar ranking on the models
in fact the same size model yields top mance on both tasks
while our experiments are limited to only one domain the correlation in results is ing optimizing parameters on one task promises to yield good performance on the other
these ndings provide support for the hypothesis that content models are not only helpful for specic tasks but can serve as effective representations of text structure in general
content model lead conclusions training set size number of summary source pairs figure summarization performance extraction racy on earthquakes as a function of training set size
focused classier and the lead baseline
furthermore as the learning curves shown in figure indicate our method achieves good performance on a small subset of parallel training data in fact the accuracy of our method on one third of the training data is higher than that of the sentence level classier on the full training set
clearly this performance gain demonstrates the effectiveness of content models for the summarization task

relation between ordering and summarization methods since we used two somewhat orthogonal tasks ordering and summarization to evaluate the quality of the model paradigm it is interesting to ask whether the same parameterization of the model does well in both cases
specically we looked at the results for different model topologies induced by varying the number of model states
for these tests we experimented with the earthquakes data the only domain for which we could evaluate summarization performance and exerted direct control over the number of states rather than utilizing the cluster size threshold that is in order to create exactly m states for a specic value of m we merged the smallest clusters until m clusters remained
table shows the performance of the different sized content models with respect to the summarization task and the ordering task using oso prediction rate
while the ordering results seem to be more sensitive to the in this paper we present an unsupervised method for the induction of content models which capture constraints on topic selection and organization for texts in a ticular domain
incorporation of these models in ing and summarization applications yields substantial provement over previously proposed methods
these sults indicate that distributional approaches widely used to model various inter sentential phenomena can be cessfully applied to capture text level relations cally validating the long standing hypothesis that word distribution patterns strongly correlate with discourse patterns within a text at least within specic domains
an important future direction lies in studying the respondence between our domain specic model and domain independent formalisms such as rst
by tomatically annotating a large corpus of texts with course relations via a rhetorical parser marcu soricut and marcu we may be able to rate domain independent relationships into the transition structure of our content models
this study could uncover interesting connections between domain specic stylistic constraints and generic principles of text organization
in the literature discourse is frequently modeled using a hierarchical structure which suggests that tic context free grammars or hierarchical hidden markov models fine et al
may also be applied for ing content structure
in the future we plan to investigate how to bootstrap the induction of hierarchical models ing labeled data derived from our content models
we would also like to explore how domain independent course constraints can be used to guide the construction of the hierarchical models
acknowledgments we are grateful to mirella lapata for providing us the results of her system on our data and to dominic jones and cindi thompson for supplying us with their document collection
we also thank eli lay sasha blair goldensohn eric breck claire cardie yejin choi marcia davidson pablo duboue noemie elhadad luis gravano julia hirschberg sanjeev danpur jon kleinberg oren kurland kathy mckeown daniel marcu art munson smaranda muresan cent ng bo pang becky passoneau owen rambow ves stoyanov chao wang and the anonymous reviewers for helpful comments and conversations
portions of this work were done while the rst author was a postdoctoral fellow at cornell university
this paper is based upon work supported in part by the national science tion under grants itr im and and by an alfred p
sloan research fellowship
any opinions ndings and conclusions or recommendations expressed above are those of the authors and do not essarily reect the views of the national science tion or sloan foundation
references f
c
bartlett

remembering a study in experimental and social psychology
bridge university press
barzilay and r
barzilay l
lee

learning to paraphrase an unsupervised approach using multiple sequence alignment
in hlt naacl main proceedings
beeferman et al
d
beeferman a
berger j

text segmentation using exponential ferty
models
in proceedings of emnlp
chen et al
s
f
chen k
seymore r
rosenfeld

topic adaptation for language modeling using unnormalized exponential models
in proceedings of icassp volume
g
dejong

an overview of the frump system
in w
g
lehnert m
h
ringle eds
strategies for natural language processing
lawrence erlbaum associates hillsdale new jersey
duboue and p
a
duboue k
r
own

statistical acquisition of content selection rules for natural language generation
in proceedings of emnlp
et al
s
fine y
singer n
tishby

the hierarchical hidden markov model analysis and plications
machine learning
florian and r
florian d
yarowsky

dynamic non local language modeling via erarchical topic based adaptation
in proceedings of the acl
z
harris

discourse and guage
in r
kittredge j
lehrberger eds
guage studies of language in restricted semantic domains
walter de gruyter berlin new york
m
hearst

multi paragraph tation of expository text
in proceedings of the acl
iyer and r
iyer m
ostendorf

modeling long distance dependence in language topic mixtures vs
dynamic cache models
in ings of icslp
jones and d
r
jones c
a
identifying events using similarity and son

context
in proceedings of conll
kittredge et al
r
kittredge t
korelsky o
bow

on the need for domain communication language
computational intelligence
kupiec et al
j
kupiec j
pedersen f
chen

a trainable document summarizer
in i
mani m
t
maybury eds
advances in automatic summarization
mit press cambridge ma
m
lapata

probabilistic text turing experiments with sentence ordering
in ceeding of the acl
mann and w
c
mann s
a
son

rhetorical structure theory toward a tional theory of text organization
text
d
marcu
ing of natural language texts
acl eacl

the rhetorical in proceedings of the k
r
mckeown

text ation using discourse strategies and focus straints to generate natural language text
bridge university press cambridge uk
press et al
w
h
press s
a
teukolsky w
t
terling b
p
flannery

numerical recipes in c the art of scientic computing
cambridge sity press second edition
l
rabiner

a tutorial on hidden markov models and selected applications in speech recognition
proceedings of the ieee
radev and d
r
radev k
r
own

generating natural language summaries from multiple on line sources
computational guistics
o
rambow

domain cation knowledge
in fifth international workshop on natural language generation
reynar and j
reynar a

a maximum entropy approach to parkhi
identifying sentence boundaries
in proceedings of the fifth conference on applied natural language processing
gildea and d
gildea t
hofmann

topic based language models using em
in proceedings of eurospeech
schapire and r
e
schapire y
singer

boostexter a boosting based system for text categorization
machine learning
soricut and r
soricut d
marcu

sentence level discourse parsing using syntactic and lexical the hlt naacl
in proceedings of information
white et al
m
white t
korelsky c
cardie v
ng d
pierce k
wagstaff

multi document summarization via information extraction
in ings of the hlt conference
a
wray

formulaic language and the lexicon
cambridge university press cambridge
wu and j
wu s
khudanpur

building a topic dependent maximum entropy guage model for very large corpora
in proceedings of icassp volume

