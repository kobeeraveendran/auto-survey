leveraging bert extractive text summarization lectures derek miller georgia institute technology atlanta georgia edu decades automatic extractive abstract text summarization lectures demonstrated useful tool collecting key phrases sentences best represent content current approaches utilize dated approaches producing sub par outputs requiring hours manual tuning produce meaningful results recently new machine learning architectures provided mechanisms extractive summarization clustering output embeddings deep learning models paper reports project called lecture summarization service python based restful service utilizes bert model text embeddings means clustering identify sentences closest centroid summary selection purpose service provide student utility summarize lecture content based desired number sentences summary work service includes lecture summary management storing content cloud collaboration results utilizing bert extractive text summarization promising areas model struggled providing future research opportunities improvement code results found com lecture summarizer author keywords lecture summary bert deep learning extractive summarization acm classification keywords natural language processing introduction approaching automatic text summarization different types abstractive extractive case abstractive text summarization closely emulates human summarization uses vocabulary specified text abstracts key points generally smaller size genest lapalme approach highly desirable subject research papers emulates humans summarize material difficult automatically produce requiring gpus train days deep learning complex algorithms rules limited generalizability traditional nlp approaches challenge mind lecture extractive summarization general extractive text summarization summarization service uses utilizes raw structures sentences phrases text outputs summarization leveraging content source material initial implementation service sentences summarization education automatic extractive text summarization lectures powerful tool extrapolating key points manual intervention labor context moocs transcripts video lectures available valuable information lecture challenging locate currently attempts solve problem nearly solutions implemented outdated natural language processing algorithms requiring frequent maintenance poor generalization limitations summary outputs mentioned tools appear random construction content year new deep learning approaches emerged proving state art results tasks automatic extractive text summarization need current tools lecture summarization lecture summarization service provides restful api command line interface cli tool serves extractive summaries lecture transcripts goal proving implementation expanded domains following sections explore background related work lecture summarization methodologies building service results metrics model example summarizations showing compare commonly tools textrank background related work order provide necessary context proposed solution automatic lecture summarization worth investigating previous research identifying pros cons approach early days lecture searching multimedia created manual applications summarizations lecture example lecture processing project uploaded large lectures including transcripts keyword searching summary content lecture glass hazen cyphers malioutov huynh barzilay limited content approach suffice data scales manual summary process inefficient motivation manual summarization mid poor quality extractive summary tools researchers created tool automatically extract corporate meeting summaries simple probabilistic models quickly found output far inferior human constructed summarizations murray renals carletta poor performance methodology led research papers aimed improve process summarization improvements lacking widespread use deep learning algorithms researchers attempted include rhetorical information lecture summaries help improve summarization performance zhang chan fung led decent performance gain created sub par outputs concluding technology potential needed research zhang chan fung years later engineers created industry product called openessayist output topics key points student essay aiding student completing assignment van labeke whitelock field pulman richardson product multiple types summarization options utilized algorithms textrank key sentence keyword extraction van labeke whitelock field pulman richardson demonstrated usefulness automatic summarization education field providing helpful topics sentences essay student differentiated prior research great initial approach algorithms textrank contain myopic view spoken context researchers balasubramanian doraisamy kanakarajan built similar application leveraging naive bayes algorithm determine phrases elements lectures slides descriptive summarization lecture balasubramanian doraisamy kanakarajan approach differentiated previous applications classification instead unsupervised learning create summaries naive bayes shown success nlp domain independent assumption features eliminate broader context lecture potentially creating summaries lack key components lacking number citations projects mentioned couple years variety new papers attempted summarization problem lectures small section book recent developments intelligent computing communication devices author implemented video subtitle extraction program summarize multimedia input utilizing idf garg approaches decent output similar reasons naive bayes algorithm idf struggles representing complex phrasing potentially missing key points lecture lecture transcript summarization project created specifically moocs similar objective lecture summarization service creating basic probabilistic algorithm achieved precision percent comparing manual summarizations che yang meinel best performance previously mentioned algorithms project specifically focused moocs supplying prior history domain text techniques extracting recent literature attempts lecture summarization lecture transcripts popular whiteboards slide decks utilizing information create summary research project authors created tool utilized deep learning extract written content whiteboard convert text format summarization kota davila stone setlur govindaraju deep learning performed lecture transcripts found research projects utilized sort deep learning algorithm extract information lecture summarization project focused extracting information slides authors utilized video audio processing tools retrieve content implemented idf extract keywords phrases final summarization shimada okubo yin ogata mentioned kota research authors state art approaches initial extraction ended selecting traditional nlp algorithms final summarization moving deep learning highlighting research projects implement deep learning lecture summarization transcripts modern projects plethora reasons use recently recurrent neural network long short term memory networks default approach natural requiring massive language processing applications amounts data expensive compute resources hours training achieve acceptable results suffering poor performance long sequences prone overfit vaswani fact mind researcher vaswani presented superior architecture called transformer completely moved away rnns convolutional neural networks cnn favor architecture comprised feed forward networks attention mechanisms vaswani transformer architecture alleviated problems rnns cnns sub human performance nlp tasks end researchers google built unsupervised learning architecture transformer architecture called bert bidirectional encoder representations transformers exceeded nearly existing models nlp space wide range tasks devlin chang lee toutanova publishing results model research team published pre trained models transfer learning multitude different domains tasks devlin chang lee toutanova component missing previous research project feature dynamic configurable summary sizes users lecture summarization applications want configure sentences lecture summary providing information based needs bert model outputs sentence embeddings sentences clustered size allowing dynamic summaries lecture celikyilmaz mind lecture summarization service implemented exact approach creating dynamic summarizations taking centroid sentence cluster static summaries fixed size motivation background related work missing element existing research projects lecture summarization service utilized students configurable lecture sizes leveraging date deep learning research fact provided development lecture summarization service based service ran inference bert model dynamically sized lecture summarizations motivation method lecture summarization service comprises main components feature management lecture transcripts summarizations allowing users create edit delete retrieve stored items component inference bert model produce embeddings clustering means model creating summary explores component detail outlining associated features motivation implementation extractive text summarization bert means creating summaries saved lectures lecture summarization service engine leveraged pipeline tokenized incoming paragraph text clean sentences passed tokenized sentences bert model inference output embeddings clustered embeddings means selecting embedded sentences closest centroid candidate summary sentences textual tokenization variability quality text lecture tokenization transcripts combination multiple techniques utilized passing input models transcripts derived udacity custom parser created convert data srt file format special format contains time stamps associated phrases standard paragraph form converted nltk library python extract sentences lecture breaking content passed subsequent models inference final step text tokenization consisted removing editing candidate sentences goal having sentences need additional context final summary example behavior removing sentences conjunctions beginning types sentences small large sentences removed example removing sentences mentioned udacity quizzes removed sentences rarely selected extractive summarization kept lecture change cluster outputs affecting centroids lead poorer summary candidates tokenization steps completed content ready inference bert text embedding superior performance nlp algorithms sentence embedding bert architecture selected bert builds transformer architecture objectives specific pre training step randomly masks words training data attempting predict masked words step takes input sentence candidate sentence predicting candidate sentence properly follows input sentence devlin chang lee toutanova process days train substantial gpus fact google released bert models public consumption million parameters contained million parameters devlin chang lee toutanova superior performance larger trained bert model ultimately selected lecture summarization service figure introduction health informatics lecture bert layer embeddings default pre trained bert model select multiple layers embeddings cls layer bert produces necessary matrix clustering number sentences embeddings dimension output cls layer necessarily produce best embedding representation sentences nature bert architecture outputs layers network produced embeddings equaled tokenized words issue embeddings averaged maxed produce matrix experiments udacity extractive summarizations udacity lectures determined second averaged layer produced best embeddings representations words ultimately determined visual examination clusters initial embedding process example differences different plots seen figure figure sample introduction health informatics course lecture initial hypothesis reason better sentence representation layer final cls layer bert network figure introduction health informatics lecture bert cls layer embeddings final layer biased classification tasks original training model lecture summarization service core bert implementation uses pytorch pretrained bert library huggingface organization core library pytorch wrapper google pre trained implementations models original bert model pytorch pretrained bert library contains openai model network expands original bert architecture examining sentence embeddings original bert model clear bert embeddings representative sentences creating larger figure ihi embeddings euclidean distances clusters example clustering embeddings ensembling models openai bert embeddings cls layer provided inferior results ensembling multiple architectures produced best results clusters euclidean distances clusters method inference time increased running multithreaded environment requiring substantial memory compute fact mind ensembling service needed trade inference performance speed clustering embeddings finally layer embeddings completed matrix ready clustering user perspective supply parameter represent number clusters requested sentences final summary output experimentation means gaussian mixture models clustering library implementation models similar performance means finally selected clustering incoming embeddings bert model clusters sentences closest centroids selected final summary sci kit learn utilizing lecture summarization service restful api provide sufficient interface bert clustered summarizations restful api place serve models inference necessary machine learning libraries required python flask library selected server summarization capabilities service contained lecture transcript management allowing users add edit delete update lectures contained endpoint convert srt files paragraph text form lecture saved system run extractive summarizations users supply parameters ratio sentences use summary properly organize resources summary completed stored sqlite database requiring compute resources users wanted obtain summarization given lecture server components containerized docker individuals run service locally deploy cloud provider currently free use public service exists aws accessed following link primary motivation restful service extensible developers providing opportunity future web applications command line interfaces built service command line interface users directly use restful api querying service command line interface tool included easier interaction allows users ability upload lecture files machine add service minimal parameters users create summaries list managed resources tool installed pip base github repository results section focus results bert model comparing output methodologies textrank golden truth summaries lectures metrics human comparison quality clusters discussed detail sections initial weaknesses found bert lecture summarization methodologies sufficiently summarizing large lectures difficulty handling context words language written dealing conversational transcripts common lectures model weaknesses larger lectures classified sentences challenge small ratio sentences properly representative entire lecture ratio sentences summarize higher context sustained making easier understand summary user hypothesis large lecture issue include multiple sentences cluster closest centroid allow context summary improving quality output requirement add clusters representative based centroids converged problem approach directly user ratio parameter adding sentences requested degrading user experience tool reason methodology included service weakness current approach occasionally select sentences contained words needed context brute force solution remove sentences contain words frequently change dramatically quality summarizations given time potential solution use nltk find parts speech attempt replace pronouns keywords proper values initially attempted lectures contained context words referenced sentences past making difficult determine item actually true context reduce examples sense performance worth looking summarized content comparing results traditional approach like textrank represents example summaries introduction health informatics udacity com courses learning udacity com courses lectures udacity reinforcement lecture subject health information exchange semantic interoperability semantic interoperability health exchanges summaries contain total thirty sentences single sub section bert summary better captured context creation technology data governance textrank model benefit introducing ihie summary beneficial user background time textrank inferior selecting sentences flowed summaries selecting candidates missing context words bert model contained sentences needing context model able collect sentences supplied broader context outputs agreed final sentence introducing jon duke interview bert output sophisticated form hie creates semantic interoperability bridging ways concepts expressed providers represented ehrs clinical systems regenstrief institute indiana created expensive sophisticated technology ihie architecture convenient data governance analysis reporting data stored centrally called data lockers remain control entity source data talk georgia tech colleague jon duke came regenstrief institute created technology indiana health information exchange textrank output premier example indiana health information exchange ihie pronounced support came regenstrief foundation philanthropic organization describes mission bring practice medicine modern scientific advances engineering business social sciences foster rapid dissemination medical practice new knowledge created research absent unfortunately rare funding source type hie usually economically impossible create case ihie curated data aggregated stored centrally built georgia tech fhir server omop data model talk georgia tech colleague jon duke came regenstrief institute created technology indiana health information exchange reinforcement learning reinforcement learning course content structured way conversational authors brings challenge summarizing content conversation example bert textrank summarizing content lecture reducing sentence size example strengths bert model seen addresses build maximum likelihood equivalent properly selects definition strings summary properly abstracts data textrank contains word maximum likelihood sentences random making difficult understand content given bert output right rule going rule gives different thing random way talking state transition know state going end taking expectation state reward plus discounted estimated value state yeah exactly maximum likelihood estimate supposed long probabilities state going match data shown far transition state textrank output way going compute value estimate state left transition epoch trajectory big previous value expect outcome look like average right yeah idea repeat update rule finite data got actually taking average respect seen transitions kind right thing infinite data issue run update rule data going effect having maximum likelihood model future improvements model future improvements strategy fine tune model udacity lectures current model default pre trained model google improvement fill gaps missing context summary automatically determine best number sentences represent lecture potentially sum squares clustering service database eventually need converted permanent solution sqlite having logins individuals manage summaries beneficial feature conclusion having capability properly summarize lectures powerful study memory refreshing tool university students automatic extractive summarization researchers attempted solve problem years producing research decent results approaches leave room improvement utilize dated natural language processing models leveraging current deep learning nlp model called bert steady improvement dated approaches textrank quality summaries combining context important sentences lecture summarization service utilizes bert model produce summaries users based specified configuration service automatic extractive summarization perfect provided step quality compared dated approaches references balasubramanian doraisamy kanakarajan multimodal approach extracting content descriptive metadata lecture videos journal intelligent information systems celikyilmaz hakkani june discovery topically coherent sentences extractive summarization proceedings annual meeting association computational linguistics human language technologies volume association computational linguistics che yang meinel automatic online lecture highlighting based multimedia analysis ieee transactions learning technologies devlin chang lee toutanova bert pre training deep bidirectional transformers language understanding arxiv preprint garg automatic text summarization video lectures subtitles recent developments intelligent computing communication devices springer singapore genest lapalme june framework abstractive summarization text text generation proceedings workshop monolingual text text generation association computational linguistics glass hazen cyphers malioutov huynh barzilay recent progress mit spoken lecture processing project eighth annual conference international speech communication association kota davila stone setlur govindaraju august automated detection handwritten whiteboard content lecture videos summarization international conference frontiers handwriting recognition icfhr ieee logeswaran lee efficient framework learning sentence representations arxiv preprint murray renals carletta extractive summarization meeting recordings shimada okubo yin ogata automatic summarization lecture slides enhanced student preview technical report user study ieee transactions learning technologies vaswani shazeer parmar uszkoreit jones gomez polosukhin attention need advances neural information processing van labeke whitelock field pulman richardson july essay saying extractive summarization motivate reflection redrafting aied workshops wolf sanh rault pytorch pretrained bert big extending repository pretrained transformers com huggingface pytorch bert zhang chan fung december improving lecture speech summarization rhetorical information automatic speech recognition understanding asru ieee workshop
