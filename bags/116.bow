conceptual text summarizer a new model in continuous vector space mohammad ebrahim khademi mohammad fakhredanesh seyed mojtaba hoseini faculty of electrical and computer engineering malek ashtar university of technology iran abstract traditional methods of summarization are not cost effective and possible today extractive summarization is a process that helps to extract the most important sentences from a text automatically and generates a short informative summary in this work we propose an unsupervised method to summarize persian texts this method is a novel hybrid approach that clusters the concepts of the text using deep learning and traditional statistical methods first we produce a word embedding based on corpus and a dictionary of word frequencies then the proposed algorithm extracts the keywords of the document clusters its concepts and finally ranks the sentences to produce the summary we evaluated the proposed method on pasokh single document corpus using the rouge evaluation measure without using any hand crafted features our proposed method achieves state the art results we compared our unsupervised method with the best supervised persian methods and we achieved an overall improvement of recall score of keywords extractive text summarization unsupervised learning language independent summarization continuous vector space word embedding introduction automatic text summarization of a large corpus has been a source of concern over the years from two areas of information retrieval and natural language processing the primary studies in this field began in baxendale edmundson and luhn have done research in those years automatic generation of summaries provides a short version of documents to help users in capturing the important contents of the original documents in a tolerable time now humans produce summaries of documents in the best way today with the growth of data especially in the big data domain it is not possible to generate all of these summaries manually because it s neither economical nor feasible there are two types of text summarization based on considering all or a specific part of a document generic summarization provides an overall summary of all information contained in a document it answers the question what is this document about a document is expected to contain several topics main topics are discussed extensively by many sentences minor topics have less sentence support and exist to support the main topics the specific goals of generic summarization are to choose k number of sentences as specified by the user from a given document that best describe the main topics of the document to minimize redundancy of the chosen sentences query relevant summarization is specific to information retrieval applications it attempts to summarize the information a document contains pertaining to a specific search term the summary indicates what this document says about the query the snippets bellow each result returned by a search engine is a common example for this type in addition there are two approaches to text summarization based on the chosen process of generating the summary extractive summarization this approach of summarization selects a subset of existing words phrases or sentences in the original text to form the summary there are of course limitations on choosing these pieces one of these limitations which is common in summarization is output summary length abstractive summarization this approach builds an internal semantic representation and then uses natural language generation techniques to create a summary that is expected to be closer to what the text want to express based on the current limitations of natural language processing methods extractive approach is the dominant approach in this field almost all extractive summarization methods encounter two key problems in assigning scores to text pieces choosing a subset of the scored pieces hitherto text summarization has traveled a very unpaved path in the beginning frequency based approaches were utilized for text summarization then lexical chain based approaches came to succeed with the blessing of using large lexical databases such as wordnet and farsnet hence valid methods such as latent semantic analysis lsa based approaches that do not use dedicated static sources requires trained human forces for producing became more prominent word embedding models learn the continuous representation of words in a low dimensional space in lexical semantics linear dimension reduction methods such as latent semantic analysis have been widely used non linear models can be used to train word embedding models word embedding models not only have a better performance but also lacks many problems of linear dimension reduction methods such as latent semantic analysis in this paper a novel method of extractive generic document summarization based on perceiving the concepts present in sentences is proposed therefore after unsupervised learning of the target language word embedding input document concepts are clustered based on the learned word feature vectors hence the proposed method is language independent after allocating scores to each conceptual cluster sentences are ranked and selected based on the significance of the concepts present in each sentence ultimately we achieved promising results on pasokh benchmark corpus the structure of the paper is as follows section two describes some related works section three presents the summary generation process section four outlines evaluation measures and experimental results section five concludes the paper and discusses the avenues for future research related works hitherto text summarization has traveled a very unpaved path in the beginning frequency based approaches were utilized for text summarization then lexical chain based approaches came to succeed with the blessing of using large lexical databases such as wordnet and farsnet since the most common subject in the text has an important role in summarization and lexical chain is a better criterion than word frequency for identifying the subject of text as a result a more discriminating diagnosis of the subject of text was made possible which was a further improvement in summarization however the great reliance of these methods on lexical databases such as wordnet or farsnet is the main weakness of these methods for the success of these methods depends on enriching and keeping up to date the vocabulary of these databases that is very costly and time consuming removing this weakness is not feasible hence valid methods such as latent semantic analysis lsa based approaches that do not use dedicated static sources requires trained human forces for producing became more prominent latent semantic analysis is a valid unsupervised method for an implicit representation of the meaning of the text based on the co occurence of words in the input document this method is unsupervised and it is considered an advantage but this method has many other problems the dimensions of the matrix changes very often new words are added very frequently and corpus changes in size the matrix is extremely sparse since most words do not co occur the matrix is very high dimensional in general quadratic cost to train i e to perform svd many of natural language processing nlp systems and methods consider words as separate units in such systems the similarity between words is not defined and words are considered as indexes of a dictionary this approach is generally adopted for the following reasons simplicity reliability advantage of training large data volume over using complex models as for the third reason according to past observations and experiences in general simple models that are trained on a vast amount of data are more effective than complex models that are trained on quantitative data today we can assume that n gram models can be trained on all existing data billions of words with the advent of machine learning methods in recent years training more complex models on much larger datasets has become possible lately the advancement in computing power of gpus and new processors have made it possible for hardwares to implement these more advanced models one of the most successful of these cases in recent years is the use of the distributed representation of vocabularies word embedding model was developed by bengio et al more than a decade ago the word embedding model w is a function that maps the words of a language into vectors with about to dimensions to initialize w random vectors are assigned to words this model learns meaningful vectors for doing some tasks in lexical semantics linear dimension reduction methods such as latent semantic analysis have been widely used non linear models can be used to train word embedding models word embedding models not only have a better performance but also lacks many problems of linear dimension reduction methods such as latent semantic analysis distributed representation of vocabularies word embedding is one of the important research topics in the field of natural language processing this method which in fact is one of the deep learning branches has been widely used in various fields of natural language processing in recent years among these we can mention the following neural language model sequence tagging machine translation contrasting meaning bengio et al mikolov et al and schwenk have shown that neural network based language models have produced much better results than n gram models although many text summarization methods are available for languages such as english little work is done in devising methods of summarizing persian texts in general these methods can be categorized as supervised and unsupervised while most of the proposed methods so far have been of the former type supervised summarization methods presented for persian documents are divided into four categories of heuristic lexical chain based graph based and machine learning or mathematical based methods heuristic method hassel and mazdak proposed farsisum as a heuristic method it is one of the first attempts to create an automatic text summarization system for persian the system is implemented as a http client server application written in perl it has used modules implemented in swesum dalianis a persian stop list in unicode format and a small set of heuristic rules lexical chain based methods zamanifar et al proposed a new hybrid summarization technique that combined term co occurrence property and conceptually related feature of farsi language they consider the relationship between words and use a synonym dataset to eliminate similar sentences their results show better performance in comparison with farsisum shamsfard et al proposed parsumist they presented single document and document summarization methods using lexical chains and graphs to rank and determine the most important sentence they consider the highest similarity with other sentences the title and keywords they achieved better performance than farsisum zamanifar and kashefi proposed azom a summarization approach that combines statistical and conceptual text properties and in regards of document structure extracts the summary of text azom performes better than three common structured text summarizers fractal yang flat summary and co occurrence shafiee and shamsfard proposed a single multi document summarizer using a novel clustering method to generate text summaries it consists of three phases first a feature selection phase is employed then farsnet a persian wordnet is utilized to extract the semantic information of words finally the input sentences are clustered their proposed method is compared with three known available text summarization systems and techniques for persian language their method obtains better results than farsisum parsumist and ijaz graph based method shakeri et al proposed an algorithm based on the graph theory to select the most important sentences of the document they explain their objective as the aim of this method is to consider the importance of sentences independently and at the same time the importance of the relationship between them thus the sentences are selected to attend in the final summary contains more important subjects and also have more contact with other sentences evaluation results indicate that the output of proposed method improves precision recall and metrics in comparison with farsisum machine learning and mathematical based methods kiyomarsi and rahimi proposed a new method for summarizing persian texts based on features available in persian language and the use of fuzzy logic their method obtains better results as compared with four previous methods tofighy et al proposed a new method for persian text summarization based on fractal theory whose main goal is using hierarchical structure of document to improve the summarization quality of persian texts their method achieved a better performance than farsisum but weaker than azom bazghandi et al proposed a textual summarization system based on sentence clustering collective intelligence algorithms are used for optimizing the methods these methods rely on semantic aspect of words based on their relations in the text their results is comparable to traditional clustering approaches tofighi et al proposed an analytical hierarchy process ahp technique for persian text summarization the proposed model uses the analytical hierarchy as a base factor for an evaluation algorithm their results show better performance in comparison with farsisum pourmasoumi et al proposed a persian single document summarization system called ijaz it is based on weighted least squares method their results proved a better performance as compared with farsisum they also proposed pasokh a popular corpus for evaluation of persian text summarizers as an unsupervised method honarpisheh et al proposed a new multi document multi lingual text summarization method based on singular value decomposition svd and hierarchical clustering success of lexical chain based methods and supervised machine learning methods depends on enriching and keeping up to date lexical databases and training labeled datasets respectively that is very costly and time consuming these methods often use language dependent features and can not be generalized to other languages on the other hand unsupervised methods such as svd based methods have many problems that are mentioned in the beginning of this section the proposed generic extractive method is a novel method that not only is unsupervised but also does not have many problems of svd based methods and without using any hand crafted features achieves much better performance compared to supervised methods proposed algorithm in this section we propose a novel method of extractive generic document summarization based on perceiving the concepts present in sentences it is an unsupervised and language independent method that does not have many problems of svd based methods for this purpose firstly the necessary preprocesses are performed on the corpus texts subsequently the persian word embedding is created by unsupervised learning of corpus then the input document keywords are extracted afterward the input document concepts are clustered based on the learned word feature vectors hence the proposed method can be generalized to other languages and the score of each of these conceptual clusters are calculated finally the sentences are ranked and selected based on the significance of the concepts present in each sentence the chart of this method is presented in figure conceptual text summarizer the following sections will be described based on this chart figure conceptual text summarizer text pre processing to learn a persian language model we use we need to produce a dictionary of vocabularies of to do this we tokenize the words of each text file of the corpus using hazm library hazm is an applicable open source natural language processing library in persian then we compose a dictionary out of these words by counting the frequency of each word throughout the corpus this dictionary will be used in succeeding steps we constitute a complete list of persian stopwords out of frequent words in the prepared dictionary along with stopword lists in other open source projects unsupervised learning of persian word embedding the corpus has text files in unlabeled text sections each of these files is a concatenation of hundreds of news and articles these news and articles are from different fields of cultural political social to construct a suitable persian word embedding set we use cbow model this model is a neural network with one hidden layer to learn the model a small window moves across the corpus texts and the network tries to predict the central word of the window using the words around it we assume a window with nine words length and it goes across the unlabeled texts of corpus to learn the weights of the network as persian word embedding vectors the first and the last four words of each window is assumed to be the input of the network the central word of the window is assumed to be the label of the output thus we have a rich labeled dataset completing the learning process of network weights on all windows of we will have a suitable persian word embedding set whose words dimension is equal to the size of the hidden layer of the network the hidden layer size is assumed to be in this work the persian word embedding generated at this stage maps every words of the to a vector in a dimensional vector space the generated persian word embedding set contains words the t sne method for visualization can be used to better understand the word embedding environment figure a persian word embedding visualization using t sne method part of the words of one of the texts of the pasokh corpus visualized in this figure in the mapping of the words in figure a persian word embedding visualization using t sne method part of the words of one of the texts of the pasokh corpus visualized in this figure similare words are closer to each other this issue can also be examined from other dimensions as another example in figure the closest vocabulary to the header terms is given using the proposed persian word embedding generated in this work the closest vocabularies to the header terms is given using the proposed persian word embedding generated in this work figure the closest vocabulary to the header terms is given using the proposed persian word embedding generated in this work in the proposed method using the relationship between words the concepts of the input document are represented in this method the importance of sentences is determined using semantic and syntactic similarities between words and instead of using single words to express concepts multiple similar words are used for example the occurrence of words computer keyboard display mouse and printer even though they are not frequently repeated singly in the input document express a certain concept as stated in the introduction the great reliance of lexical chain based methods on lexical databases is the main weakness of these methods at this stage to remove this weakness an appropriate word embedding for summarization is created that encompasses the semantic and syntactic communication of the words in a broader and more up to date lexical range than that of lexical databases the word embedding presented in this work is able to discover relationships present in the outside world that do not exist in common vocabulary databases for example this word embedding can detect the relation between the words of mashhad neyshabur and khorasan mashhad is the capital of khorasan province and neyshabur is one of the cities of this province figure the closest vocabulary to the header terms is given using the proposed persian word embedding generated in this work the common vocabulary databases that can not discover such relationships are comprehensive lexical databases that carry different meanings for each word along with relationships between them such as synonyms antonyms part of containing or more general more specific relationships but their construction is manual costly and time consuming extracting the keywords of the document for extracting the keywords of the input document we first tokenized the words of the document using hazm tokenizer the words of the document tokenized using then we excluded stopwords from input document tokens the score of each word of the input document calculated using equation where w is the intended word tf calculated from equation point ij idf i ij max where ij is frequency of the i th word in the j th document and max ij of the words in the input document the tf is normalized using this division is maximum frequency finally idf in equation was calculated from equation where n is the number of documents of the corpus and ni documents in the corpus that the i th word has been observed there idfi n is the number of if a word is not in the there will not be a score for it also due to the absence of a vector in the continuous vector space for this word it is deleted from the decision making cycle therefore learning word embedding on a richer persian corpus will cause to increase the accuracy of the method clustering concepts in this phase the concepts present in the input document are constructed using the persian word embedding obtained in section for this purpose first we sort the keywords of the previous phase according to their calculated scores then we map all input document terms into a dimensional space using the prepared persian word embedding we cluster the concepts of this document into ten different clusters using k means algorithm we consider the ten preferred keywords selected in section as the primary cluster centers and cluster the entire words of the input document each obtained cluster can be considered as a concept thus ten key concepts of the document are constructed cluster or concept finally we consider the nearest word to each cluster center as the criterion word for that the total score of each concept is calculated using the equation point w c point w where w is the word c is the concept and is the total score of each word that was calculated based on equation the indicates the closeness of each word in the intended concept to the concept s criterion word therefore the words nearer to the concept s criterion word will have larger linear coefficients and the words farther to that criterion word will have smaller linear coefficients thus the nearness of each word to its concept s criterion word affects the final score of the concept hence repetition of more closely situated words in the input document will result in a higher score than repetition of farther words sentence ranking for ranking sentences the following steps are taken first the input document is read line by line and the sentences of each line are separated using hazm sentence tokenizer for scoring extracted sentences equation is used score point c w s n where s is a sentence n is its number of words and is the score of the the intended word s concept by dividing the sentence score into its number of words we normalized the obtained score so that shorter and longer sentences would have equal chance of selection sentences are sorted according to their normalized scores according to the desired summary length some sentences with the highest score are selected and are displayed in the order they appear in the document experimental results in this section using rouge criterion our system generated summaries on single document pasokh corpus is evaluated and the obtained results are compared with other available persian summarizers evaluation measures rouge n is a measure for evaluation of summarizations this recall based measure is very close to human evaluation of summaries this measure calculates the number of common n grams between the system generated summaries and the reference human made summaries it s therefore a suitable measure for automatically evaluating summaries produced in all languages for this work two public rouge evaluation tools are studied rouge is a perl implementation of rouge measure that was developed by mr c lin et al at the university of southern california this implementation dose not support unicode and it generates unrealistic results for the persian summary evaluation after obtaining the exaggerated results of this tool for persian summaries we realized this great weakness rouge is a java implementation of rouge n measure developed by rxnlp team and is publicly accessible this tool supports unicode and the obtained results are accurate but it has only implemented rouge n and not any other variations of rouge measure in this work a python implementation of rouge n was developed based on mr c lin s paper this tool supports unicode and verifies the results of the implementation according to the above descriptions the is used for summary evaluation in this study pasokh corpus pasokh is a popular corpus for the evaluation of persian text summarizers this dataset consists of a large number of persian news documents on various topics it contains human written summaries of the documents in the forms of single document multi document extractive and abstractive summaries the single document dataset of pasokh contains persian news texts that five extractive and five abstractive summaries for each of these news are generated by different human agents one hundred news texts of the single document pasokh dataset were summarized using the proposed algorithm in this work the compression ratio of our system summaries was percent then we needed to calculate rouge n between each of our system generated summaries and the related pasokh extractive reference summaries human made summaries for this purpose rouge java implementation tool was used which is mentioned in evaluation tool section earlier the average of the rouge n is considered as the evaluation of each of our system summaries finally the average of system summary evaluations was calculated as the final evaluation result it should be noted that the news headlines of pasokh corpus has not been used in summarization process and the results are obtained without taking advantage of headlines pourmasoumi et al presented ijaz as an extractive single document summarizer of persian news in which is available online in this experiment one hundred news texts of the pasokh corpus were summarized using ijaz summarizer the compression ratio was percent and the results were obtained without using headlines the results are reported in table scores percent on pasokh single document dataset table scores percent on pasokh single document dataset and table scores percent on pasokh single document dataset score shafiee and shamsfard method our proposed method systems ijaz systems ijaz shafiee and shamsfard method our proposed method table scores percent on pasokh single document dataset score table scores percent on pasokh single document dataset score systems ijaz shafiee and shamsfard method our proposed method table scores percent on pasokh single document dataset thus our proposed method in this work has the following advantages over pourmasoumi et al method our proposed method achieves much better results than the proposed method of pourmasoumi et al in all and measures the method proposed by pourmasoumi et al has taken a supervised learning approach while our learning approach is unsupervised as defined by authorities supervised learning requires that the algorithm s possible outputs are already known and that the data used to train the algorithm is already labeled with correct answers while unsupervised machine learning is more closely aligned with what some call true artificial intelligence the idea that a computer can learn to identify complex processes and patterns without a human to provide guidance along the way although unsupervised learning is prohibitively complex for some simpler enterprise use cases it opens the doors to solving problems that humans normally would not tackle their proposed method is a persian specific method while our proposed method can be generalized to other languages shafiee and shamsfard proposed an approach in extractive single document persian summarization in unfortunately neither their summarizer nor summaries generated by their proposed algorithm are available for comparison therefore the algorithm has been implemented in this experiment one hundred news texts of the pasokh corpus were summarized using developed summarizer the compression ratio was percent and the results were obtained using headlines the results are reported in table scores percent on pasokh single document dataset table scores percent on pasokh single document dataset and table scores percent on pasokh single document dataset our approach has the following advantages over shafiee and shamsfard s approach our proposed method achieves much better results than the number of similar and related sentences method of shafiee and shamsfard in all and measures shafiee and shamsfard s method is supervised while ours is unsupervised in order to calculate a feature s weight they utilize one third of the pasokh single document corpus to compute a feature s weight the mean of f measure scores is calculated to be considered as the final weight of the selected feature for single document summarization their proposed method depends on enriching and keeping up to date the farsnet lexical database that is very costly and time consuming while our method depends on unsupervised learning of the target language word embedding their proposed method is a persian specific method while our proposed method can be generalized to other languages their method has used the news headlines in the summarization process while our method has obtained the results without using headlines hassel and mazdak created farsisum in as one of the first persian text summarizers reported in related literature the available version of farsisum summarizer in their website has a number of bugs for example the length of the summary farsisum produces has a significant difference with the requested compression ratio percentage according to previous studies the results of our proposed method on pasokh corpus are much higher than the results obtained by farsisum summarizer conclusion in this paper a novel method of extractive generic document summarization based on perceiving the concepts present in sentences is proposed therefore after unsupervised learning of the target language word embedding input document concepts are clustered based on the learned word feature vectors hence the proposed method can be generalized to other languages after allocating scores to each conceptual cluster sentences are ranked and selected based on the significance of the concepts present in each sentence one of the most important challenges in recent researches in the field of summarizing persian texts is the lack of a rich lexical database in persian language that can be used to measure semantic similarities in this research by constructing a persian word embedding using we were able to correctly answer this shortage and provide a new method for summarizing the texts according to the semantic and syntactic relations learned using the relationship between words the concepts discussed in the input document are represented in this method the importance of sentences is determined using semantic and syntactic similarities between words instead of using single words to express concepts different related words are used we evaluated the proposed method on pasokh single document dataset using the rouge evaluation mesure without using any hand crafted features our proposed method achieves state of the art results for system summaries generated with percent compression ratio on pasokh single document corpus and recall scores were and percent respectively evaluation of our proposed method for summarization of other languages is suggested for future works learning word embedding on richer persian corpuses may be effective in increasing the accuracy of our method using pagerank algorithm to produce the concept similarity graph and to find more significant concepts may also increase the accuracy of our concept selection algorithm using exploited mmr maximum marginal relevance greedy algorithm in sentence selection process may decrease the redundancy of the selected sentences in our proposed method references p b baxendale machine made index for technical literature an experiment ibm j res dev vol no pp h p edmundson new methods in automatic extracting j acm jacm vol no pp h p luhn the automatic creation of literature abstracts ibm j res dev vol no pp h khanpour sentence extraction for summarization and notetaking university of malaya a berger and v o mittal query relevant summarization using faqs in proceedings of the annual meeting on association for computational linguistics stroudsburg pa usa pp j tang l yao and d chen multi topic based query oriented summarization in proceedings of the siam international conference on data mining pp w song l c choi s c park and x f ding fuzzy evolutionary optimization modeling and its applications to unsupervised categorization and extractive summarization expert syst appl vol no pp f jin m huang and x zhu a comparative study on ranking and selection strategies for document summarization in proceedings of the international conference on computational linguistics posters pp g a miller wordnet a lexical database for english commun acm vol no pp m shamsfard developing farsnet a lexical ontology for persian in global wordnet conference szeged hungary m shamsfard et al semi automatic development of farsnet the persian wordnet in proceedings of global wordnet conference mumbai india vol t brants a c popat p xu f j och and j dean large language models in machine translation in in proceedings of the joint conference on empirical methods in natural language processing and computational natural language learning g e hinton j l mcclelland and d e rumelhart distributed representations parallel distributed processing explorations in the microstructure of cognition vol foundations mit press cambridge ma y bengio r ducharme p vincent and c jauvin a neural probabilistic language model j mach learn res vol no feb pp p d turney and p pantel from frequency to meaning vector space models of semantics j artif intell res vol pp s t roweis and l k saul nonlinear dimensionality reduction by locally linear embedding science vol no pp j b tenenbaum v de silva and j c langford a global geometric framework for nonlinear dimensionality reduction science vol no pp h schwenk continuous space language models comput speech lang vol no pp r collobert j weston l bottou m karlen k kavukcuoglu and p kuksa natural language processing almost from scratch j mach learn res vol no aug pp r collobert and j weston a unified architecture for natural language processing deep neural networks with multitask learning in proceedings of the international conference on machine learning pp j devlin r zbib z huang t lamar r m schwartz and j makhoul fast and robust neural network joint models for statistical machine translation in acl pp i sutskever o vinyals and q v le sequence to sequence learning with neural networks in advances in neural information processing systems pp z chen et al revisiting word embedding for contrasting meaning in acl pp t mikolov a deoras s kombrink l burget and j ernock empirical evaluation and combination of advanced language modeling techniques in twelfth annual conference of the international speech communication association m hassel and n mazdak farsisum a persian text summarizer in proceedings of the workshop on computational approaches to arabic script based languages pp a zamanifar b minaei bidgoli and m sharifi a new hybrid farsi text summarization technique based on term co occurrence and conceptual property of the text in software engineering artificial intelligence networking and parallel distributed computing ninth acis international conference on pp m shamsfard t akhavan and m e joorabchi persian document summarization by parsumist world appl sci j vol pp a zamanifar and o kashefi azom a persian structured text summarizer nat lang process inf syst pp f shafiee and m shamsfard similarity versus relatedness a novel approach in extractive persian document summarisation j inf sci h shakeri s gholamrezazadeh m a salehi and f ghadamyari a new graph based algorithm for persian text summarization in computer science and convergence springer pp f kiyomarsi and f r esfahani optimizing persian text summarization based on fuzzy logic approach in international conference on intelligent building and management m tofighy o kashefi a zamanifar and h h s javadi persian text summarization using fractal theory in international conference on informatics engineering and information science pp m bazghandi g t tabrizi m v jahan and i mashahd extractive summarization of farsi documents based on pso clustering jia vol p s m tofighy r g raj and h h s javad ahp techniques for persian text summarization malays j comput sci vol no pp p asef k mohsen t s ahmad e ahmad and q hadi ijaz an operational system for single document summarization of persian news texts vol no pp jan t strutz data fitting and uncertainty a practical introduction to weighted least squares and beyond vieweg and teubner b b moghaddas m kahani s a toosi a pourmasoumi and a estiri pasokh a standard corpus for the evaluation of persian text summarizers in computer and knowledge engineering iccke international econference on pp m a honarpisheh g ghassem sani and s a mirroshandel a multi document lingual automatic summarization system in ijcnlp pp hamshahri a standard persian text collection sciencedirect available sciencedirect com science article pii accessed hazm python library for digesting persian text sobhe t mikolov i sutskever k chen g s corrado and j dean distributed representations of words and phrases and their compositionality in advances in neural information processing systems c j c burges l bottou m welling z ghahramani and k q weinberger eds curran associates inc pp j leskovec a rajaraman and j d ullman mining of massive datasets cambridge university press c lin rouge a package for automatic evaluation of summaries rxnlp java implementation of rouge for evaluation of summarization tasks stemming stopwords and unicode support
