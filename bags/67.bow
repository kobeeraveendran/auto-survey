abstractive text summarization using sequence to sequence rnns and beyond ramesh nallapati ibm watson ibm com bowen zhou ibm watson ibm com cicero dos santos ibm watson ibm com aglar g ulehre universit de montral umontreal ca bing xiang ibm watson ibm com abstract in this work we model abstractive text summarization using attentional decoder recurrent neural networks and show that they achieve state of the art formance on two different corpora we propose several novel models that address critical problems in summarization that are not adequately modeled by the basic architecture such as modeling key words capturing the hierarchy of sentence word structure and emitting words that are rare or unseen at training time our work shows that many of our proposed models contribute to further improvement in performance we also propose a new dataset consisting of multi sentence maries and establish performance marks for further research introduction abstractive text summarization is the task of erating a headline or a short summary consisting of a few sentences that captures the salient ideas of an article or a passage we use the adjective stractive to denote a summary that is not a mere selection of a few existing passages or sentences extracted from the source but a compressed phrasing of the main contents of the document potentially using vocabulary unseen in the source document this task can also be naturally cast as ping an input sequence of words in a source ument to a target sequence of words called mary in the recent past deep learning based els that map an input sequence into another put sequence called sequence to sequence els have been successful in many problems such as machine translation bahdanau et al speech recognition bahdanau et al and video captioning venugopalan et al in the framework of sequence to sequence models a very relevant model to our task is the tional recurrent neural network rnn decoder model proposed in bahdanau et al which has produced state of the art formance in machine translation mt which is also a natural language task despite the similarities abstractive tion is a very different problem from mt unlike in mt the target summary is typically very short and does not depend very much on the length of the source document in summarization tionally a key challenge in summarization is to timally compress the original document in a lossy manner such that the key concepts in the original document are preserved whereas in mt the lation is expected to be loss less in translation there is a strong notion of almost one to one level alignment between source and target but in summarization it is less obvious we make the following main contributions in this work i we apply the off the shelf tional encoder decoder rnn that was originally developed for machine translation to tion and show that it already outperforms of the art systems on two different english pora motivated by concrete problems in marization that are not sufciently addressed by the machine translation based model we propose novel models and show that they provide tional improvement in performance we pose a new dataset for the task of abstractive marization of a document into multiple sentences and establish benchmarks the rest of the paper is organized as follows in section we describe each specic problem in abstractive summarization that we aim to solve and present a novel model that addresses it g u a l c s c v v i x r a tion contextualizes our models with respect to closely related work on the topic of abstractive text summarization we present the results of our periments on three different data sets in section we also present some qualitative analysis of the output from our models in section before cluding the paper with remarks on our future rection in section models in this section we rst describe the basic decoder rnn that serves as our baseline and then propose several novel models for summarization each addressing a specic weakness in the line encoder decoder rnn with attention and large vocabulary trick our baseline model corresponds to the neural chine translation model used in bahdanau et al the encoder consists of a bidirectional gru rnn chung et al while the decoder consists of a uni directional gru rnn with the same hidden state size as that of the encoder and an attention mechanism over the source hidden states and a soft max layer over target in the interest of space lary to generate words we refer the reader to the original paper for a tailed treatment of this model in addition to the basic model we also adapted to the tion problem the large vocabulary trick lvt described in jean et al in our approach the decoder vocabulary of each mini batch is stricted to words in the source documents of that batch in addition the most frequent words in the target dictionary are added until the vocabulary reaches a xed size the aim of this technique is to reduce the size of the soft max layer of the decoder which is the main computational neck in addition this technique also speeds up convergence by focusing the modeling effort only on the words that are essential to a given example this technique is particularly well suited to marization since a large proportion of the words in the summary come from the source document in any case capturing keywords using feature rich encoder in summarization one of the key challenges is to identify the key concepts and key entities in the document around which the story revolves in order to accomplish this goal we may need to go beyond the word embeddings based tation of the input document and capture tional linguistic features such as parts of speech tags named entity tags and tf and idf tics of the words we therefore create additional look up based embedding matrices for the ulary of each tag type similar to the embeddings for words for continuous features such as tf and idf we convert them into categorical values by discretizing them into a xed number of bins and use one hot representations to indicate the bin number they fall into this allows us to map them into an embeddings matrix like any other tag type finally for each word in the source document we simply look up its embeddings from all of its sociated tags and concatenate them into a single long vector as shown in fig on the target side we continue to use only word based embeddings as the representation figure feature rich encoder we use one embedding vector each for pos ner tags and discretized tf and idf values which are concatenated together with word based beddings as input to the encoder modeling rare unseen words using switching generator pointer often times in summarization the keywords or named entities in a test document that are central to the summary may actually be unseen or rare with respect to training data since the vocabulary of the decoder is xed at training time it can not emit these unseen words instead a most common way of handling these out of vocabulary oov words is to emit an unk token as a placeholder however this does not result in legible summaries in summarization an intuitive way to handle such oov words is to simply point to their location in the source document instead we model this hidden stateinput layeroutput layerwposnertfidfwposnertfidfwposnertfidfwposnertfidfattention mechanismencoderdecoder tion using our novel switching decoder pointer chitecture which is graphically represented in ure in this model the decoder is equipped with a switch that decides between using the tor or a pointer at every time step if the switch is turned on the decoder produces a word from its target vocabulary in the normal fashion however if the switch is turned off the decoder instead erates a pointer to one of the word positions in the source the word at the pointer location is then copied into the summary the switch is modeled as a sigmoid activation function over a linear layer based on the entire available context at each step as shown below p ws hhi ws ws cci e ws h ws where p is the probability of the switch turning on at the ith time step of the decoder hi is the hidden state is the embedding tor of the emission from the previous time step ci is the attention weighted context vector and c bs and vs are the switch ws ters we use attention distribution over word tions in the document as the distribution to sample the pointer from i j wa p a j ba wa p a pi arg max wa i for nd e c hd j in the above equation pi is the pointer value at ith word position in the summary sampled from the attention distribution pa i over the document word positions nd where p a i j is the probability of the ith time step in the decoder pointing to the jth position in the document and hd j is the encoder s hidden state at position j at training time we provide the model with plicit pointer information whenever the summary word does not exist in the target vocabulary when the oov word in summary occurs in multiple ument positions we break the tie in favor of its rst occurrence at training time we optimize the conditional log likelihood shown below with ditional regularization penalties log p i gi gi p where and are the summary and document words respectively gi is an indicator function that is set to whenever the word at position i in the summary is oov with respect to the decoder cabulary at test time the model decides ically at each time step whether to generate or to point based on the estimated switch probability p we simply use the arg max of the rior probability of generation or pointing to ate the best output at each time step the pointer mechanism may be more robust in handling rare words because it uses the encoder s hidden state representation of rare words to decide which word from the document to point to since the hidden state depends on the entire context of the word the model is able to accurately point to unseen words although they do not appear in the target vocabulary figure switching generator pointer model when the switch shows g the traditional generator consisting of the softmax layer is used to produce a word and when it shows p the pointer network is activated to copy the word from one of the source document positions when the pointer is activated the embedding from the source is used as input for the next time step as shown by the arrow from the encoder to the decoder at the bottom capturing hierarchical document structure with hierarchical attention in datasets where the source document is very long in addition to identifying the keywords in the document it is also important to identify the key sentences from which the summary can be drawn this model aims to capture this notion of two levels of importance using two bi directional when the word does not exist in the source lary the pointer model may still be able to identify the correct position of the word in the source since it takes into account the contextual representation of the corresponding unk ken encoded by the rnn once the position is known the corresponding token from the source document can be played in the summary even when it is not part of the training vocabulary either on the source side or the target side hidden stateencoderdecoderinput layeroutput layergpggg rnns on the source side one at the word level and the other at the sentence level the attention mechanism operates at both levels simultaneously the word level attention is further re weighted by the corresponding sentence level attention and normalized as shown below p p a a p a s a s where p a is the word level attention weight at jth position of the source document and is the id of the sentence at jth word position p a s l is the sentence level attention weight for the lth sentence in the source nd is the number of words in the source document and p is the re scaled attention at the jth word position the re scaled attention is then used to compute the weighted context vector that goes as input to the hidden state of the decoder further we also catenate additional positional embeddings to the hidden state of the sentence level rnn to model positional importance of sentences in the ment this architecture therefore models key tences as well as keywords within those sentences jointly a graphical representation of this model is displayed in figure figure hierarchical encoder with hierarchical attention the attention weights at the word level represented by the dashed arrows are re scaled by the corresponding level attention weights represented by the dotted arrows the dashed boxes at the bottom of the top layer rnn resent sentence level positional embeddings concatenated to the corresponding hidden states related work a vast majority of past work in summarization has been extractive which consists of ing key sentences or passages in the source ument and reproducing them as summary neto et al erkan and radev wong et al filippova and altun colmenares et al litvak and last k riedhammer and hakkani tur ricardo ribeiro humans on the other hand tend to paraphrase the original story in their own words as such man summaries are abstractive in nature and dom consist of reproduction of original sentences from the document the task of abstractive marization has been standardized using the and competitions the data for these tasks consists of news stories from various topics with multiple reference summaries per story generated by humans the best performing system on the task called topiary zajic et al used a combination of linguistically motivated compression techniques and an pervised topic detection algorithm that appends keywords extracted from the article onto the pressed output some of the other notable work in the task of abstractive summarization includes ing traditional phrase table based machine tion approaches banko et al compression using weighted tree transformation rules cohn and lapata and quasi synchronous mar approaches woodsend et al with the emergence of deep learning as a viable alternative for many nlp tasks collobert et al researchers have started considering this framework as an attractive fully data driven in rush et native to abstractive summarization al the authors use convolutional models to encode the source and a context sensitive tentional feed forward neural network to generate the summary producing state of the art results on gigaword and duc datasets in an extension to this work chopra et al used a similar volutional model for the encoder but replaced the decoder with an rnn producing further ment in performance on both datasets in another paper that is closely related to our work hu et al introduce a large dataset for chinese short text summarization they show promising results on their chinese dataset using an encoder decoder rnn but do not report iments on english corpora in another very recent work cheng and lapata used rnn based encoder decoder for tractive summarization of documents this model is not directly comparable to ours since their nist hidden stateword layerencoderdecoderinput layeroutput layerhidden statesentence layer eos sentence level attentionword level attention framework is extractive while ours and that of rush et al hu et al and chopra et al is abstractive our work starts with the same framework as hu et al where we use rnns for both source and target but we go beyond the standard architecture and propose novel models that dress critical problems in summarization we also note that this work is an extended version of lapati et al in addition to performing more extensive experiments compared to that work we also propose a novel dataset for document rization on which we establish benchmark bers too below we analyze the similarities and ences of our proposed models with related work on summarization feature rich encoder sec linguistic tures such as pos tags and named entities as well as tf and idf information were used in many extractive approaches to summarization wong et al but they are novel in the context of deep learning approaches for abstractive rization to the best of our knowledge switching generator pointer model sec this model combines extractive and abstractive approaches to summarization in a single end end framework rush et al also used a combination of extractive and abstractive proaches but their extractive model is a rate log linear classier with handcrafted features pointer networks vinyals et al have also been used earlier for the problem of rare words in the context of machine translation luong et al but the novel addition of switch in our model allows it to strike a balance between when to be faithful to the original source e for named entities and oov and when it is allowed to be ative we believe such a process arguably how human produces summaries for a more detailed treatment of this model and experiments on multiple tasks please refer to the parallel work published by some of the authors of this work gulcehre et al hierarchical attention model sec viously proposed hierarchical encoder decoder models use attention only at sentence level li et al the novelty of our approach lies in joint modeling of attention at both sentence and word levels where the word level attention is further uenced by sentence level attention thus ing the notion of important sentences and tant words within those sentences concatenation of positional embeddings with the hidden state at sentence level is also new experiments and results gigaword corpus in this series of we used the tated gigaword corpus as described in rush et al we used the scripts made available by the authors of this to preprocess the data which resulted in about m training examples the script also produces about k validation and test examples but we created a randomly pled subset of examples each for validation and testing purposes on which we report our formance further we also acquired the exact test sample used in rush et al to make precise comparison of our models with theirs we also made small modications to the script to extract not only the tokenized words but also generated parts of speech and named entity tags training for all the models we discuss below we used dimensional vectors mikolov et al trained on the same corpus to ize the model embeddings but we allowed them to be updated during training the hidden state mension of the encoder and decoder was xed at in all our experiments when we used only the rst sentence of the document as the source as done in rush et al the encoder lary size was and that of the decoder stood at we used adadelta zeiler for training with an initial learning rate of we used a batch size of and randomly shufed the training data at every epoch while sorting every batches according to their lengths to speed up training we did not use any dropout or ization but applied gradient clipping we used early stopping based on the validation set and used the best model on the validation set to report all test performance numbers for all our models we employ the large vocabulary trick where we strict the decoder vocabulary size to cause it cuts down the training time per epoch by nearly three times and helps this and all used kyunghyun cho s code com kyunghyuncho material as the ing point com facebook namas values improved performance only marginally but at the cost of much slower training quent models converge in only of the epochs needed for the model based on full ulary decoding at decode time we used beam search of size to generate the summary and limited the size of summary to a maximum of words since this is the maximum size we noticed in the pled validation set we found that the average tem summary length from all our models to agrees very closely with that of the ground truth on the validation set about words out any specic tuning computational costs we trained all our els on a single tesla gpu most models took about hours per epoch on an average except the hierarchical attention model which took hours per epoch all models typically converged within epochs using our early stopping criterion based on the validation cost the wall clock training time until convergence therefore varies between days depending on the model generating summaries at test time is reasonably fast with a throughput of about summaries per second on a single gpu using a batch size of evaluation metrics similar to nallapati et al and chopra et al we use the full length variant of to evaluate our tem although limited length recall was the ferred metric for most previous work one of its disadvantages is choosing the length limit which varies from to corpus making it difcult for researchers to compare performances length recall on the other hand does not impose a length restriction but unfairly favors longer maries full length solves this problem since it can penalize longer summaries while not ing a specic length restriction in addition we also report the percentage of tokens in the system summary that occur in the source which we call src copy rate in table we describe all our experiments and results on the gigaword below words this is the baseline attentional encoder decoder model with the large vocabulary trick this model is trained only on the rst tence from the source document as done in rush et al words this model is identical to the model above except for the fact that it is trained berouge com pages default aspx on the rst two sentences from the source on this corpus adding the additional sentence in the source does seem to aid performance as shown in table we also tried adding more sentences but the performance dropped which is probably because the latter sentences in this corpus are not pertinent to the summary words hieratt since we used two tences from source document we trained the erarchical attention model proposed in sec as shown in table this model improves mance compared to its atter counterpart by ing the relative importance of the rst two tences automatically feats here we still train on the rst two sentences but we exploit the parts of speech and named entity tags in the annotated gigaword as well as tf idf values to augment the input embeddings on the source side as described in sec in total our embedding vector grew from the original to and produced mental gains compared to its counterpart as shown in table demonstrating the utility of syntax based features in this task feats ptr this is the switching ator pointer model described in sec but in addition we also use feature rich embeddings on the document side as in the above model our periments indicate that the new model is able to achieve the best performance on our test set by all three rouge variants as shown in table comparison with state of the art we pared the performance of our model words with state of the art models on the sample created by rush et al as displayed in the bottom part of table we also trained another system which we call words which has a larger lvt vocabulary size of but also has much larger source and target vocabularies of k and k respectively the reason we did not evaluate our best dation models here is that this test set consisted of only sentence from the source document and did not include nlp annotations which are needed in our best models the table shows that despite this fact our model outperforms the model of rush et al with statistical signicance in addition our models exhibit better abstractive ability as shown by the src copy rate metric in the last column of the table further our larger model words outperforms the state of the art model of chopra et al with statistically signicant improvement on we believe the bidirectional rnn we used to model the source captures richer contextual mation of every word than the bag of embeddings representation used by rush et al and chopra et al in their convolutional tional encoders which might explain our superior performance further explicit modeling of portant information such as multiple source tences word level linguistic features using the switch mechanism to point to source words when needed and hierarchical attention solve specic problems in summarization each boosting mance incrementally duc corpus the duc comes in two parts the corpus consisting of document summary pairs and the corpus consisting of pairs since these corpora are too small to train large neural networks on rush et al trained their models on the gigaword corpus but combined it with an additional log linear extractive rization model with handcrafted features that is trained on the duc corpus they call the original neural attention model the abs model and the combined model chopra et al also report the performance of their elman model on this corpus and is the current state of the art since it outperforms all previously published baselines including non neural network based extractive and abstractive systems as sured by the ofcial duc metric of recall at bytes in these experiments we use the same ric to evaluate our models too but we omit ing numbers from other systems in the interest of space in our work we simply run the models trained on gigaword as they are without tuning them on the duc validation set the only change we made to the decoder is to suppress the model from emitting the end of summary tag and force it to emit exactly words for every summary since the ofcial evaluation on this corpus is based on limited length rouge recall on this corpus too since we have only a single sentence from source and no nlp annotations we ran just the models words and words the performance of this model on the test set nist gov tasks html is compared with abs and models elman from chopra et al as well as iary the top performing system on in table we note our best model words outperforms ras elman on two of the three ants of rouge while being competitive on model topiary abs ras elman words words rouge l table evaluation of our models using the limited length rouge recall at bytes on duc validation and test sets our best model although trained exclusively on the gaword corpus consistently outperforms the model which is tuned on the validation corpus in tion to being trained on the gigaword corpus cnn daily mail corpus the existing abstractive text summarization pora including gigaword and duc consist of only one sentence in each summary in this section we present a new corpus that comprises sentence summaries to produce this corpus we modify an existing corpus that has been used for the task of passage based question answering in this work the hermann et al thors used the human generated abstractive mary bullets from new stories in cnn and daily mail websites as questions with one of the ties hidden and stories as the corresponding sages from which the system is expected to swer the ll in the blank question the authors leased the scripts that crawl extract and generate pairs of passages and questions from these sites with a simple modication of the script we restored all the summary bullets of each story in the original order to obtain a multi sentence mary where each bullet is treated as a sentence in all this corpus has training pairs validation pairs and test pairs as dened by their scripts the source documents in the ing set have words spanning sentences on an average while the summaries consist of words and sentences the unique istics of this dataset such as long documents and ordered multi sentence summaries present esting challenges and we hope will attract future model name rouge l src copy rate full length on our internal test set words words words hieratt feats feats ptr rush et al words ras elman chopra et al words full length on the test set used by rush et al table performance comparison of various models indicates statistical signicance of the corresponding model with respect to the baseline model on its dataset as given by the condence interval in the ofcial rouge script we report statistical signicance only for the best performing models src copy rate for the reference data on our validation sample is please refer to section for explanation of notation model words words hieratt words temp att rouge l table performance of various models on cnn daily mail test set using full length rouge metric bold faced numbers indicate best performing system researchers to build and test novel models on it the dataset is released in two versions one consisting of actual entity names and the other in which entity occurrences are replaced with document specic integer ids beginning from in the since the vocabulary size is smaller anonymized version we used it in all our iments below we limited the source vocabulary size to k and the target vocabulary to k the source and target lengths to at most and words respectively we used dimensional embeddings trained on this dataset as input and we xed the model hidden state size at we also created explicit pointers in the ing data by matching only the anonymized ids between source and target on similar lines as we did for the oov words in gigaword computational costs we used a single tesla gpu to train our models on this dataset as well while the at models words and ptr took under hours per epoch the archical attention model was very expensive suming nearly hours per epoch convergence of all models is also slower on this dataset pared to gigaword taking nearly epochs for all models thus the wall clock time for ing until convergence is about days for the at models but nearly days for the hierarchical tention model decoding is also slower as well with a throughput of examples per second for at models and examples per second for the hierarchical attention model when run on a single gpu with a batch size of evaluation we evaluated our models using the full length rouge metric that we employed for the gigaword corpus but with one notable ence in both system and gold summaries we sidered each highlight to be a separate sentence results results from the basic attention decoder as well as the hierarchical attention model are displayed in table although this dataset is smaller and more complex than the gigaword pus it is interesting to note that the rouge bers are in the same range however the archical attention model described in sec outperforms the baseline attentional decoder only marginally upon visual inspection of the system output we noticed that on this dataset both these models duced summaries that contain repetitive phrases or even repetitive sentences at times since the summaries in this dataset involve multiple tences it is likely that the decoder forgets what part of the document was used in producing earlier highlights to overcome this problem we used the temporal attention model of sankaran et al that keeps track of past attentional weights of the decoder and expliticly discourages it from attending to the same parts of the document in ture time steps the model works as shown by the this dataset we used the pyrouge script pypi python org pypi that lows evaluation of each sentence as a separate unit tional pre processing involves assigning each highlight to its own a tag in the system and gold xml les that go as input to the rouge evaluation script similar evaluation was also done by cheng and lapata source document wanted lm director must be eager to shoot footage of golden lassos and invisible jets eos conrms that is leaving the upcoming movie the hollywood reporter rst broke the story eos was announced as director of the movie in november eos obtained a statement from that says given creative differences and have decided not to move forward with plans to develop and direct together eos and are both owned by eos the movie starring in the title role of the princess is still set for release on june eos it s the rst theatrical movie centering around the most popular female superhero eos will appear beforehand in v due out march eos in the meantime will need to nd someone new for the director s chair eos ground truth summary is no longer set to direct the rst theatrical movie eos left the project over creative differences eos movie is currently set for words conrms that is leaving the upcoming movie eos and have decided not to move forward with plans to develop eos conrms that is leaving the upcoming movie words hieratt is leaving the upcoming movie eos the movie is still set for release on june eos is still set for release on june words temp att conrms that is leaving the upcoming movie eos the movie is the rst lm to around the most popular female actor eos will appear in due out march table comparison of gold truth summary with summaries from various systems named entities and numbers are anonymized by the preprocessing script the eos tags represent the boundary between two highlights the temporal attention model words temp att solves the problem of repetitions in summary as exhibited by the models words and words hieratt by encouraging the attention model to focus on the uncovered portions of the document following simple equations t k t t t where t is the unnormalized attention weights vector at the tth time step of the decoder in other words the temporal attention model weights the attention weights at the current time step if the past attention weights are high on the same part of the document using this strategy the temporal attention model improves performance signicantly over both the baseline model as well as the hierarchical attention model we have also noticed that there are fewer repetitions of summay highlights duced by this model as shown in the example in table these results although preliminary should serve as a good baseline for future researchers to compare their models against qualitative analysis table presents a few high quality and poor ity output on the validation set from feats one of our best performing models even when the model differs from the target summary its summaries tend to be very meaningful and evant a phenomenon not captured by word phrase matching evaluation metrics such as rouge on the other hand the model sometimes prets the semantics of the text and generates a summary with a comical interpretation as shown in the poor quality examples in the table clearly capturing the meaning of complex sentences mains a weakness of these models our next example output presented in figure displays the sample output from the switching generator pointer model on the gigaword corpus it is apparent from the examples that the model learns to use pointers very accurately not only for named entities but also for multi word phrases despite its accuracy the performance ment of the overall model is not signicant we believe the impact of this model may be more nounced in other settings with a heavier tail bution of rare words we intend to carry out more experiments with this model in the future on cnn daily mail data although our models are able to produce good quality multi sentence summaries we notice that the same sentence or good quality summary output s a man charged with the murder last year of a british packer confessed to the slaying on the night he was charged with her killing according to police evidence presented at a court hearing tuesday ian douglas previte is charged with murdering caroline stuttle of yorkshire england t man charged with british backpacker s death confessed to crime police ofcer claims o man charged with murdering british backpacker fessed to murder s following are the leading scorers in the english premier league after saturday s matches alan shearer newcastle united james beattie t leading scorers in english premier league o english premier league leading scorers s volume of transactions at the nigerian stock exchange has continued its decline since last week a nse ofcial said thursday the latest statistics showed that a total of million shares valued at million naira about million us dollars were traded on wednesday in deals t transactions dip at nigerian stock exchange o transactions at nigerian stock exchange down poor quality summary output s broccoli and broccoli sprouts contain a chemical that kills the bacteria responsible for most stomach cancer say searchers conrming the dietary advice that moms have been handing out for years in laboratory tests the chemical unk killed helicobacter pylori a bacteria that causes stomach ulcers and often fatal stomach cancers t for release at unk mom was right broccoli is good for you say cancer researchers o broccoli sprouts contain deadly bacteria s norway delivered a diplomatic protest to russia on day after three norwegian sheries research expeditions were barred from russian waters the norwegian research ships were to continue an annual program of charting sh resources shared by the two countries in the barents sea gion t norway protests russia barring sheries research ships o norway grants diplomatic protest to russia s morgan chase s ability to recover from a slew of recent losses rests largely in the hands of two men who are both looking to restore tarnished reputations and may be considered for the top job someday geoffrey unk now the co head of j morgan s investment bank left goldman sachs co more than a decade ago after executives say he lost out in a bid to lead that rm t executives to lead j morgan chase on road to ery o morgan chase may be considered for top job table examples of generated summaries from our best model on the validation set of gigaword s source document t target summary o system output although we displayed equal number of good quality and poor quality summaries in the table the good ones are far more prevalent than the poor ones of the annual meeting on association for putational linguistics cheng and jianpeng cheng and mirella lapata neural summarization by extracting sentences and words in proceedings of the nual meeting of the association for computational linguistics cheng et al jianpeng cheng li dong and long short term corr mirella lapata memory networks for machine reading chopra et al sumit chopra michael auli and alexander m rush abstractive sentence summarization with attentive recurrent neural works in hlt naacl chung et al junyoung chung aglar glehre kyunghyun cho and yoshua bengio pirical evaluation of gated recurrent neural networks on sequence modeling corr cohn and trevor cohn and mirella ata sentence compression beyond word tion in proceedings of the international ference on computational linguistics volume pages collobert et al ronan collobert jason weston lon bottou michael karlen koray kavukcuoglu and pavel p kuksa guage processing almost from scratch corr natural colmenares et al carlos a colmenares marina litvak amin mantrach and fabrizio silvestri heads headline generation as sequence diction using an abstract feature rich space in ceedings of the conference of the north ican chapter of the association for computational linguistics human language technologies pages erkan and g erkan and d r radev lexrank graph based lexical centrality as salience in text summarization journal of articial intelligence research filippova and katja filippova and yasemin altun overcoming the lack in of parallel data in sentence compression the conference on empirical ceedings of methods in natural language processing pages figure sample output from switching generator pointer networks an arrow indicates that a pointer to the source sition was used to generate the corresponding summary word phrase often gets repeated in the summary we lieve models that incorporate intra attention such as cheng et al can x this problem by couraging the model to remember the words it has already produced in the past conclusion in this work we apply the attentional decoder for the task of abstractive summarization with very promising results outperforming of the art results signicantly on two different datasets each of our proposed novel models dresses a specic problem in abstractive rization yielding further improvement in mance we also propose a new dataset for sentence summarization and establish benchmark numbers on it as part of our future work we plan to focus our efforts on this data and build more bust models for summaries consisting of multiple sentences references bahdanau et al dzmitry bahdanau kyunghyun cho and yoshua bengio neural machine translation by jointly learning to align and translate corr bahdanau et al dzmitry bahdanau jan chorowski dmitriy serdyuk philemon brakel and yoshua bengio end to end based large vocabulary speech recognition corr gulcehre et al caglar gulcehre sungjin ahn ramesh nallapati bowen zhou and yoshua in gio pointing the unknown words ceedings of the annual meeting of the tion for computational linguistics banko et al michele banko vibhu o mittal and michael j witbrock headline tion based on statistical translation in proceedings hermann et al karl moritz hermann toms kocisk edward grefenstette lasse espeholt will kay mustafa suleyman and phil blunsom rush et al alexander m rush sumit chopra and jason weston a neural attention model corr for abstractive sentence summarization sankaran et al b sankaran h mi y onaizan and a ittycheriah temporal tion model for neural machine translation arxiv e prints august venugopalan et al subhashini venugopalan marcus rohrbach jeff donahue raymond j mooney trevor darrell and kate saenko corr sequence to sequence video to text vinyals et al o vinyals m fortunato and n jaitly pointer networks arxiv e prints june wong et al kam fai wong mingli wu and wenjie li extractive summarization using in supervised and semi supervised learning ceedings of the international conference on computational linguistics volume pages wong et al kam fai wong mingli wu and wenjie li extractive summarization using in supervised and semi supervised learning ceedings of the annual meeting of the tion for computational linguistics pages woodsend et al kristian woodsend yansong feng and mirella lapata title generation with quasi synchronous grammar in proceedings of the conference on empirical methods in ral language processing emnlp pages stroudsburg pa usa association for putational linguistics zajic et al david zajic bonnie j dorr and richard schwartz bbn umd at in proceedings of the north american topiary chapter of the association for computational guistics workshop on document understanding pages matthew d zeiler adadelta corr an adaptive learning rate method teaching machines to read and comprehend corr et al baotian hu qingcai chen and fangze zhu lcsts a large scale chinese short text summarization dataset in proceedings of the conference on empirical methods in natural guage processing pages lisbon gal september association for computational guistics jean et al sbastien jean kyunghyun cho roland memisevic and yoshua bengio on using very large target vocabulary for neural chine translation corr k riedhammer and hakkani b favre k riedhammer and d hakkani tur long story short a s global unsupervised models for keyphrase based meeting summarization in speech communication pages et al jiwei li minh thang luong and dan a hierarchical neural corr for paragraphs and documents jurafsky coder litvak and m litvak and m last for in coling pages graph based document summarization extraction keyword luong et al thang luong ilya sutskever quoc v le oriol vinyals and wojciech zaremba addressing the rare word problem in neural in proceedings of the machine translation annual meeting of the association for tional linguistics and the international joint conference on natural language processing of the asian federation of natural language processing pages mikolov et al tomas mikolov ilya sutskever kai chen greg corrado and jeffrey dean distributed representations of words and phrases and their compositionality corr nallapati et al ramesh nallapati bing xiang sequence to sequence iclr workshop and bowen zhou rnns for text summarization neto et al joel larocca neto alex alves itas and celso a a kaestner automatic text summarization using a machine learning proach in proceedings of the brazilian posium on articial intelligence advances in cial intelligence pages ricardo david martins de matos joco p neto anatole gershman jaime carbonell cardo ribeiro lu s marujo self ment for important passage retrieval in national acm sigir conference on research and development in information retrieval pages
