abstractive text summarization sequence sequence rnns ramesh nallapati ibm watson ibm com bowen zhou ibm watson ibm com cicero dos santos ibm watson ibm com aglar ulehre universit montral umontreal bing xiang ibm watson ibm com abstract work model abstractive text summarization attentional decoder recurrent neural networks achieve state art formance different corpora propose novel models address critical problems summarization adequately modeled basic architecture modeling key words capturing hierarchy sentence word structure emitting words rare unseen training time work shows proposed models contribute improvement performance propose new dataset consisting multi sentence maries establish performance marks research introduction abstractive text summarization task erating headline short summary consisting sentences captures salient ideas article passage use adjective stractive denote summary mere selection existing passages sentences extracted source compressed phrasing main contents document potentially vocabulary unseen source document task naturally cast ping input sequence words source ument target sequence words called mary recent past deep learning based els map input sequence sequence called sequence sequence els successful problems machine translation bahdanau speech recognition bahdanau video captioning venugopalan framework sequence sequence models relevant model task tional recurrent neural network rnn decoder model proposed bahdanau produced state art formance machine translation natural language task despite similarities abstractive tion different problem unlike target summary typically short depend length source document summarization tionally key challenge summarization timally compress original document lossy manner key concepts original document preserved lation expected loss translation strong notion level alignment source target summarization obvious following main contributions work apply shelf tional encoder decoder rnn originally developed machine translation tion outperforms art systems different english pora motivated concrete problems marization sufciently addressed machine translation based model propose novel models provide tional improvement performance pose new dataset task abstractive marization document multiple sentences establish benchmarks rest paper organized follows section describe specic problem abstractive summarization aim solve present novel model addresses tion contextualizes models respect closely related work topic abstractive text summarization present results periments different data sets section present qualitative analysis output models section cluding paper remarks future rection section models section rst describe basic decoder rnn serves baseline propose novel models summarization addressing specic weakness line encoder decoder rnn attention large vocabulary trick baseline model corresponds neural chine translation model bahdanau encoder consists bidirectional gru rnn chung decoder consists uni directional gru rnn hidden state size encoder attention mechanism source hidden states soft max layer target interest space lary generate words refer reader original paper tailed treatment model addition basic model adapted tion problem large vocabulary trick lvt described jean approach decoder vocabulary mini batch stricted words source documents batch addition frequent words target dictionary added vocabulary reaches xed size aim technique reduce size soft max layer decoder main computational neck addition technique speeds convergence focusing modeling effort words essential given example technique particularly suited marization large proportion words summary come source document case capturing keywords feature rich encoder summarization key challenges identify key concepts key entities document story revolves order accomplish goal need word embeddings based tation input document capture tional linguistic features parts speech tags named entity tags idf tics words create additional look based embedding matrices ulary tag type similar embeddings words continuous features idf convert categorical values discretizing xed number bins use hot representations indicate bin number fall allows map embeddings matrix like tag type finally word source document simply look embeddings sociated tags concatenate single long vector shown fig target continue use word based embeddings representation figure feature rich encoder use embedding vector pos ner tags discretized idf values concatenated word based beddings input encoder modeling rare unseen words switching generator pointer times summarization keywords named entities test document central summary actually unseen rare respect training data vocabulary decoder xed training time emit unseen words instead common way handling vocabulary oov words emit unk token placeholder result legible summaries summarization intuitive way handle oov words simply point location source document instead model hidden stateinput layeroutput layerwposnertfidfwposnertfidfwposnertfidfwposnertfidfattention mechanismencoderdecoder tion novel switching decoder pointer chitecture graphically represented ure model decoder equipped switch decides tor pointer time step switch turned decoder produces word target vocabulary normal fashion switch turned decoder instead erates pointer word positions source word pointer location copied summary switch modeled sigmoid activation function linear layer based entire available context step shown hhi cci probability switch turning ith time step decoder hidden state embedding tor emission previous time step attention weighted context vector switch ters use attention distribution word tions document distribution sample pointer arg max equation pointer value ith word position summary sampled attention distribution document word positions probability ith time step decoder pointing jth position document encoder hidden state position training time provide model plicit pointer information summary word exist target vocabulary oov word summary occurs multiple ument positions break tie favor rst occurrence training time optimize conditional log likelihood shown ditional regularization penalties log summary document words respectively indicator function set word position summary oov respect decoder cabulary test time model decides ically time step generate point based estimated switch probability simply use arg max rior probability generation pointing ate best output time step pointer mechanism robust handling rare words uses encoder hidden state representation rare words decide word document point hidden state depends entire context word model able accurately point unseen words appear target vocabulary figure switching generator pointer model switch shows traditional generator consisting softmax layer produce word shows pointer network activated copy word source document positions pointer activated embedding source input time step shown arrow encoder decoder capturing hierarchical document structure hierarchical attention datasets source document long addition identifying keywords document important identify key sentences summary drawn model aims capture notion levels importance directional word exist source lary pointer model able identify correct position word source takes account contextual representation corresponding unk ken encoded rnn position known corresponding token source document played summary training vocabulary source target hidden stateencoderdecoderinput layeroutput layergpggg rnns source word level sentence level attention mechanism operates levels simultaneously word level attention weighted corresponding sentence level attention normalized shown word level attention weight jth position source document sentence jth word position sentence level attention weight lth sentence source number words source document scaled attention jth word position scaled attention compute weighted context vector goes input hidden state decoder catenate additional positional embeddings hidden state sentence level rnn model positional importance sentences ment architecture models key tences keywords sentences jointly graphical representation model displayed figure figure hierarchical encoder hierarchical attention attention weights word level represented dashed arrows scaled corresponding level attention weights represented dotted arrows dashed boxes layer rnn resent sentence level positional embeddings concatenated corresponding hidden states related work vast majority past work summarization extractive consists ing key sentences passages source ument reproducing summary neto erkan radev wong filippova altun colmenares litvak riedhammer hakkani tur ricardo ribeiro humans hand tend paraphrase original story words man summaries abstractive nature dom consist reproduction original sentences document task abstractive marization standardized competitions data tasks consists news stories topics multiple reference summaries story generated humans best performing system task called topiary zajic combination linguistically motivated compression techniques pervised topic detection algorithm appends keywords extracted article pressed output notable work task abstractive summarization includes ing traditional phrase table based machine tion approaches banko compression weighted tree transformation rules cohn lapata quasi synchronous mar approaches woodsend emergence deep learning viable alternative nlp tasks collobert researchers started considering framework attractive fully data driven rush native abstractive summarization authors use convolutional models encode source context sensitive tentional feed forward neural network generate summary producing state art results gigaword duc datasets extension work chopra similar volutional model encoder replaced decoder rnn producing ment performance datasets paper closely related work introduce large dataset chinese short text summarization promising results chinese dataset encoder decoder rnn report iments english corpora recent work cheng lapata rnn based encoder decoder tractive summarization documents model directly comparable nist hidden stateword layerencoderdecoderinput layeroutput layerhidden statesentence layer eos sentence level attentionword level attention framework extractive rush chopra abstractive work starts framework use rnns source target standard architecture propose novel models dress critical problems summarization note work extended version lapati addition performing extensive experiments compared work propose novel dataset document rization establish benchmark bers analyze similarities ences proposed models related work summarization feature rich encoder sec linguistic tures pos tags named entities idf information extractive approaches summarization wong novel context deep learning approaches abstractive rization best knowledge switching generator pointer model sec model combines extractive abstractive approaches summarization single end end framework rush combination extractive abstractive proaches extractive model rate log linear classier handcrafted features pointer networks vinyals earlier problem rare words context machine translation luong novel addition switch model allows strike balance faithful original source named entities oov allowed ative believe process arguably human produces summaries detailed treatment model experiments multiple tasks refer parallel work published authors work gulcehre hierarchical attention model sec viously proposed hierarchical encoder decoder models use attention sentence level novelty approach lies joint modeling attention sentence word levels word level attention uenced sentence level attention ing notion important sentences tant words sentences concatenation positional embeddings hidden state sentence level new experiments results gigaword corpus series tated gigaword corpus described rush scripts available authors preprocess data resulted training examples script produces validation test examples created randomly pled subset examples validation testing purposes report formance acquired exact test sample rush precise comparison models theirs small modications script extract tokenized words generated parts speech named entity tags training models discuss dimensional vectors mikolov trained corpus ize model embeddings allowed updated training hidden state mension encoder decoder xed experiments rst sentence document source rush encoder lary size decoder stood adadelta zeiler training initial learning rate batch size randomly shufed training data epoch sorting batches according lengths speed training use dropout ization applied gradient clipping early stopping based validation set best model validation set report test performance numbers models employ large vocabulary trick strict decoder vocabulary size cause cuts training time epoch nearly times helps kyunghyun cho code com kyunghyuncho material ing point com facebook namas values improved performance marginally cost slower training quent models converge epochs needed model based ulary decoding decode time beam search size generate summary limited size summary maximum words maximum size noticed pled validation set found average tem summary length models agrees closely ground truth validation set words specic tuning computational costs trained els single tesla gpu models took hours epoch average hierarchical attention model took hours epoch models typically converged epochs early stopping criterion based validation cost wall clock training time convergence varies days depending model generating summaries test time reasonably fast throughput summaries second single gpu batch size evaluation metrics similar nallapati chopra use length variant evaluate tem limited length recall ferred metric previous work disadvantages choosing length limit varies corpus making difcult researchers compare performances length recall hand impose length restriction unfairly favors longer maries length solves problem penalize longer summaries ing specic length restriction addition report percentage tokens system summary occur source src copy rate table describe experiments results gigaword words baseline attentional encoder decoder model large vocabulary trick model trained rst tence source document rush words model identical model fact trained berouge com pages default aspx rst sentences source corpus adding additional sentence source aid performance shown table tried adding sentences performance dropped probably sentences corpus pertinent summary words hieratt tences source document trained erarchical attention model proposed sec shown table model improves mance compared atter counterpart ing relative importance rst tences automatically feats train rst sentences exploit parts speech named entity tags annotated gigaword idf values augment input embeddings source described sec total embedding vector grew original produced mental gains compared counterpart shown table demonstrating utility syntax based features task feats ptr switching ator pointer model described sec addition use feature rich embeddings document model periments indicate new model able achieve best performance test set rouge variants shown table comparison state art pared performance model words state art models sample created rush displayed table trained system words larger lvt vocabulary size larger source target vocabularies respectively reason evaluate best dation models test set consisted sentence source document include nlp annotations needed best models table shows despite fact model outperforms model rush statistical signicance addition models exhibit better abstractive ability shown src copy rate metric column table larger model words outperforms state art model chopra statistically signicant improvement believe bidirectional rnn model source captures richer contextual mation word bag embeddings representation rush chopra convolutional tional encoders explain superior performance explicit modeling portant information multiple source tences word level linguistic features switch mechanism point source words needed hierarchical attention solve specic problems summarization boosting mance incrementally duc corpus duc comes parts corpus consisting document summary pairs corpus consisting pairs corpora small train large neural networks rush trained models gigaword corpus combined additional log linear extractive rization model handcrafted features trained duc corpus original neural attention model abs model combined model chopra report performance elman model corpus current state art outperforms previously published baselines including non neural network based extractive abstractive systems sured ofcial duc metric recall bytes experiments use ric evaluate models omit ing numbers systems interest space work simply run models trained gigaword tuning duc validation set change decoder suppress model emitting end summary tag force emit exactly words summary ofcial evaluation corpus based limited length rouge recall corpus single sentence source nlp annotations ran models words words performance model test set nist gov tasks html compared abs models elman chopra iary performing system table note best model words outperforms ras elman ants rouge competitive model topiary abs ras elman words words rouge table evaluation models limited length rouge recall bytes duc validation test sets best model trained exclusively gaword corpus consistently outperforms model tuned validation corpus tion trained gigaword corpus cnn daily mail corpus existing abstractive text summarization pora including gigaword duc consist sentence summary section present new corpus comprises sentence summaries produce corpus modify existing corpus task passage based question answering work hermann thors human generated abstractive mary bullets new stories cnn daily mail websites questions ties hidden stories corresponding sages system expected swer blank question authors leased scripts crawl extract generate pairs passages questions sites simple modication script restored summary bullets story original order obtain multi sentence mary bullet treated sentence corpus training pairs validation pairs test pairs dened scripts source documents ing set words spanning sentences average summaries consist words sentences unique istics dataset long documents ordered multi sentence summaries present esting challenges hope attract future model rouge src copy rate length internal test set words words words hieratt feats feats ptr rush words ras elman chopra words length test set rush table performance comparison models indicates statistical signicance corresponding model respect baseline model dataset given condence interval ofcial rouge script report statistical signicance best performing models src copy rate reference data validation sample refer section explanation notation model words words hieratt words temp att rouge table performance models cnn daily mail test set length rouge metric bold faced numbers indicate best performing system researchers build test novel models dataset released versions consisting actual entity names entity occurrences replaced document specic integer ids beginning vocabulary size smaller anonymized version iments limited source vocabulary size target vocabulary source target lengths words respectively dimensional embeddings trained dataset input xed model hidden state size created explicit pointers ing data matching anonymized ids source target similar lines oov words gigaword computational costs single tesla gpu train models dataset models words ptr took hours epoch archical attention model expensive suming nearly hours epoch convergence models slower dataset pared gigaword taking nearly epochs models wall clock time ing convergence days models nearly days hierarchical tention model decoding slower throughput examples second models examples second hierarchical attention model run single gpu batch size evaluation evaluated models length rouge metric employed gigaword corpus notable ence system gold summaries sidered highlight separate sentence results results basic attention decoder hierarchical attention model displayed table dataset smaller complex gigaword pus interesting note rouge bers range archical attention model described sec outperforms baseline attentional decoder marginally visual inspection system output noticed dataset models duced summaries contain repetitive phrases repetitive sentences times summaries dataset involve multiple tences likely decoder forgets document producing earlier highlights overcome problem temporal attention model sankaran keeps track past attentional weights decoder expliticly discourages attending parts document ture time steps model works shown dataset pyrouge script pypi python org pypi lows evaluation sentence separate unit tional pre processing involves assigning highlight tag system gold xml les input rouge evaluation script similar evaluation cheng lapata source document wanted director eager shoot footage golden lassos invisible jets eos conrms leaving upcoming movie hollywood reporter rst broke story eos announced director movie november eos obtained statement says given creative differences decided forward plans develop direct eos owned eos movie starring title role princess set release june eos rst theatrical movie centering popular female superhero eos appear march eos meantime need new director chair eos ground truth summary longer set direct rst theatrical movie eos left project creative differences eos movie currently set words conrms leaving upcoming movie eos decided forward plans develop eos conrms leaving upcoming movie words hieratt leaving upcoming movie eos movie set release june eos set release june words temp att conrms leaving upcoming movie eos movie rst popular female actor eos appear march table comparison gold truth summary summaries systems named entities numbers anonymized preprocessing script eos tags represent boundary highlights temporal attention model words temp att solves problem repetitions summary exhibited models words words hieratt encouraging attention model focus uncovered portions document following simple equations unnormalized attention weights vector tth time step decoder words temporal attention model weights attention weights current time step past attention weights high document strategy temporal attention model improves performance signicantly baseline model hierarchical attention model noticed fewer repetitions summay highlights duced model shown example table results preliminary serve good baseline future researchers compare models qualitative analysis table presents high quality poor ity output validation set feats best performing models model differs target summary summaries tend meaningful evant phenomenon captured word phrase matching evaluation metrics rouge hand model prets semantics text generates summary comical interpretation shown poor quality examples table clearly capturing meaning complex sentences mains weakness models example output presented figure displays sample output switching generator pointer model gigaword corpus apparent examples model learns use pointers accurately named entities multi word phrases despite accuracy performance ment overall model signicant believe impact model nounced settings heavier tail bution rare words intend carry experiments model future cnn daily mail data models able produce good quality multi sentence summaries notice sentence good quality summary output man charged murder year british packer confessed slaying night charged killing according police evidence presented court hearing tuesday ian douglas previte charged murdering caroline stuttle yorkshire england man charged british backpacker death confessed crime police ofcer claims man charged murdering british backpacker fessed murder following leading scorers english premier league saturday matches alan shearer newcastle united james beattie leading scorers english premier league english premier league leading scorers volume transactions nigerian stock exchange continued decline week nse ofcial said thursday latest statistics showed total million shares valued million naira million dollars traded wednesday deals transactions dip nigerian stock exchange transactions nigerian stock exchange poor quality summary output broccoli broccoli sprouts contain chemical kills bacteria responsible stomach cancer searchers conrming dietary advice moms handing years laboratory tests chemical unk killed helicobacter pylori bacteria causes stomach ulcers fatal stomach cancers release unk mom right broccoli good cancer researchers broccoli sprouts contain deadly bacteria norway delivered diplomatic protest russia day norwegian sheries research expeditions barred russian waters norwegian research ships continue annual program charting resources shared countries barents sea gion norway protests russia barring sheries research ships norway grants diplomatic protest russia morgan chase ability recover slew recent losses rests largely hands men looking restore tarnished reputations considered job someday geoffrey unk head morgan investment bank left goldman sachs decade ago executives lost bid lead executives lead morgan chase road ery morgan chase considered job table examples generated summaries best model validation set gigaword source document target summary system output displayed equal number good quality poor quality summaries table good ones far prevalent poor ones annual meeting association putational linguistics cheng jianpeng cheng mirella lapata neural summarization extracting sentences words proceedings nual meeting association computational linguistics cheng jianpeng cheng dong long short term corr mirella lapata memory networks machine reading chopra sumit chopra michael auli alexander rush abstractive sentence summarization attentive recurrent neural works hlt naacl chung junyoung chung aglar glehre kyunghyun cho yoshua bengio pirical evaluation gated recurrent neural networks sequence modeling corr cohn trevor cohn mirella ata sentence compression word tion proceedings international ference computational linguistics volume pages collobert ronan collobert jason weston lon bottou michael karlen koray kavukcuoglu pavel kuksa guage processing scratch corr natural colmenares carlos colmenares marina litvak amin mantrach fabrizio silvestri heads headline generation sequence diction abstract feature rich space ceedings conference north ican chapter association computational linguistics human language technologies pages erkan erkan radev lexrank graph based lexical centrality salience text summarization journal articial intelligence research filippova katja filippova yasemin altun overcoming lack parallel data sentence compression conference empirical ceedings methods natural language processing pages figure sample output switching generator pointer networks arrow indicates pointer source sition generate corresponding summary word phrase gets repeated summary lieve models incorporate intra attention cheng problem couraging model remember words produced past conclusion work apply attentional decoder task abstractive summarization promising results outperforming art results signicantly different datasets proposed novel models dresses specic problem abstractive rization yielding improvement mance propose new dataset sentence summarization establish benchmark numbers future work plan focus efforts data build bust models summaries consisting multiple sentences references bahdanau dzmitry bahdanau kyunghyun cho yoshua bengio neural machine translation jointly learning align translate corr bahdanau dzmitry bahdanau jan chorowski dmitriy serdyuk philemon brakel yoshua bengio end end based large vocabulary speech recognition corr gulcehre caglar gulcehre sungjin ahn ramesh nallapati bowen zhou yoshua gio pointing unknown words ceedings annual meeting tion computational linguistics banko michele banko vibhu mittal michael witbrock headline tion based statistical translation proceedings hermann karl moritz hermann toms kocisk edward grefenstette lasse espeholt kay mustafa suleyman phil blunsom rush alexander rush sumit chopra jason weston neural attention model corr abstractive sentence summarization sankaran sankaran onaizan ittycheriah temporal tion model neural machine translation arxiv prints august venugopalan subhashini venugopalan marcus rohrbach jeff donahue raymond mooney trevor darrell kate saenko corr sequence sequence video text vinyals vinyals fortunato jaitly pointer networks arxiv prints june wong kam fai wong mingli wenjie extractive summarization supervised semi supervised learning ceedings international conference computational linguistics volume pages wong kam fai wong mingli wenjie extractive summarization supervised semi supervised learning ceedings annual meeting tion computational linguistics pages woodsend kristian woodsend yansong feng mirella lapata title generation quasi synchronous grammar proceedings conference empirical methods ral language processing emnlp pages stroudsburg usa association putational linguistics zajic david zajic bonnie dorr richard schwartz bbn umd proceedings north american topiary chapter association computational guistics workshop document understanding pages matthew zeiler adadelta corr adaptive learning rate method teaching machines read comprehend corr baotian qingcai chen fangze zhu lcsts large scale chinese short text summarization dataset proceedings conference empirical methods natural guage processing pages lisbon gal september association computational guistics jean sbastien jean kyunghyun cho roland memisevic yoshua bengio large target vocabulary neural chine translation corr riedhammer hakkani favre riedhammer hakkani tur long story short global unsupervised models keyphrase based meeting summarization speech communication pages jiwei minh thang luong dan hierarchical neural corr paragraphs documents jurafsky coder litvak litvak coling pages graph based document summarization extraction keyword luong thang luong ilya sutskever quoc oriol vinyals wojciech zaremba addressing rare word problem neural proceedings machine translation annual meeting association tional linguistics international joint conference natural language processing asian federation natural language processing pages mikolov tomas mikolov ilya sutskever kai chen greg corrado jeffrey dean distributed representations words phrases compositionality corr nallapati ramesh nallapati bing xiang sequence sequence iclr workshop bowen zhou rnns text summarization neto joel larocca neto alex alves itas celso kaestner automatic text summarization machine learning proach proceedings brazilian posium articial intelligence advances cial intelligence pages ricardo david martins matos joco neto anatole gershman jaime carbonell cardo ribeiro marujo self ment important passage retrieval national acm sigir conference research development information retrieval pages
