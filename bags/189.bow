summarizing event sequences serial episodes statistical model application soumyajit mitra sastry abstract paper address problem discovering small set frequent serial episodes sequential data adequately characterize summarize data discuss algorithm based minimum description length mdl principle algorithm slight modication earlier method called present novel generative model sequence data containing prominent pairs serial episodes provide statistical justication algorithm believe rst instance statistical justication mdl based algorithm summarizing event sequence data present novel application data mining algorithm text classication considering text documents temporal sequences words data mining algorithm set characteristic episodes training data words characteristic episodes considered relevant words dictionary resulting considerably reduced feature vector dimension simulation experiments benchmark data sets discovered frequent episodes achieve fold reduction dictionary size losing classication accuracy index terms frequent episodes mdl principle compressing frequent patterns hmm models episodes dictionary learning text classication introduction requent pattern mining important problem data mining applications diverse domains frequently occurring local patterns capture useful pects semantics data practice mined frequent patterns large number redundant nature makes difcult effectively use isolating small set non redundant informative frequent patterns best describes data interesting current research problem paper concerned mining sequential data framework frequent episodes address problem isolating small set redundant serial episodes best characterize data recent efforts extracting small subset non redundant characteristic patterns mainly families methods family methods retain patterns sense tistically signicant statistical signicance assessed suitable null model hypothesis testing framework tting generative model data source reduce number frequent patterns extent approach tackle redundancy discovered patterns prominent family methods deciding subset patterns best explains data based information theoretic approach called minimum scription length mdl principle context problem isolating best subset frequent patterns soumyajit mitra department electrical engineering indian institute science bangalore india currently samsung bangalore india mail com sastry department electrical engineering indian institute science bangalore india mail use mdl principle explained follows formulate mechanism given subset frequent patterns use model encode data subset results overall best level data compression considered subset best characterizes data view motivated mdl principle found effective frequent pattern mining algorithms mdl principle views learning data compression able discover important regularities data able use compress data view coding mechanism lossless original data exactly recoverable given encoded compressed representation krimp algorithm rst methods mdl principle identify small set vant patterns context frequent itemset mining mentioned earlier paper concerned sequential data sequential data unlike case transaction data temporal ordering data tuples important encoding mechanism able recover original data sequence correct order time stamps presents additional complications encoding sequential data frequent patterns discussion mdl motivated algorithms proposed characterizing sequence data subset frequent patterns different algorithms use different strategies coding data frequent patterns methods motivated mdl principle coding gies computation compression achieved given subset frequent patterns essentially arbitrary paper consider recently proposed algorithm called efcient method discover subset serial episodes best characterizes data event sequences uses novel pattern class consisting injective serial episodes xed inter event times similar pattern class recently ing association rules temporal data algorithm uses number distinct occurrences episode frequency extend case non overlapped occurrences episode frequency provide statistical justication algorithm based generative model main contribution paper hmm based generative model provides statistical tion algorithm mdl based approaches subset patterns selected based data pression achieve depends arbitrary coding scheme algorithm selected heuristically paper provide justication coding scheme algorithm based proposed statistical generative model rst time knowledge formal connection established mining episodes mdl principle generative model data source generative model markovian handle non overlapped occurrences based episode frequency extended use non overlapped occurrences episode frequency major contribution paper novel plication method discovering set characteristic episodes sequential data application text sication text classication methods represent document vector dictionary words called bag words representation dictionary taken words corpus propriate stemming dropping stop words dictionary sizes large resulting high dimensionality feature vectors representing individual documents text document viewed sequence events event types words method discover subset characteristic episodes best represent corpus document data use words event types subset discovered episodes form dictionary method need frequency threshold constitutes parameter unsupervised method feature selection problem empirical experiments method results signicant reduction dictionary size loss classication accuracy rest paper organized follows section describes episode mining algorithm section iii presents proposed generative model section explains method nding smaller sized dictionary text sication problems reports results obtained different text datasets conclusions presented section discovering best subset serial episodes episodes event sequences begin brief informal description episodes framework details data abstractly viewed event sequence denoted tuple event event type time occurrence event nite alphabet set example event sequence patterns interest called episodes paper concerned serial episodes represent serial episode event type ith event episode episode said injective event types episode distinct example node injective serial episode currence serial episode constituted events data sequence appropriate event types times occurrence correct order constitutes occurrence occur note events constituting occurrence need contiguous data data mining problem discover frequently occurring episodes frequent episodes framework different frequency measures dened based counting different subsets occurrences mention frequencies relevant paper efcient algorithms discovering serial episodes frequency measures details different frequencies algorithms discovering rial episodes occurrences serial episode said non overlapped event occurrence tween events non overlapped occurrences occurrence episode overlaps earlier ones non overlapped frequency episode dened maximum number non overlapped occurrences episode event sequence occurrences said distinct share event occurrences distinct mum number distinct occurrences frequency interest method use special class serial episodes called xed interval serial episodes xed val serial episode denoted prescribed gap times ith events occurrence example xed interval injective serial episode occurrence episode easy events constituting occurrence xed interval serial episode completely specied giving time occurrence rst event occurrences starting different times distinct episode injective event types episode different mining algorithm best subset interest discovering small set interval serial episodes best explains data use minimum description length mdl principle rank different subsets episodes total encoding length results use models encode data mdl encoding able recover original data completely considering sequential data means able recover data original sequence time stamps rst explain strategy coding data sequence episodes basic idea encode events constituting occurrence xed interval serial episode giving start times occurrence encoding strategy obtaining best subset episodes essentially use algorithm main difference use non overlapped frequency algorithm uses distinct occurrences frequency rst explain encoding scheme example briey explain algorithm details coding scheme algorithm table illustrates coding scheme encoding event sequence essentially arbitrarily lected episodes row species size description episode number occurrences episode start times occurrences rst row table species node episode occurrences starting time instants row codes events data constituting occurrences node episode similarly second row codes events specifying occurrences node episode row codes events specifying occurrence node episode suppose interested asking good subset episodes episodes specied table account events data coding mdl lossless row table occurrences node episode sure events data sequence covered easy given table recreate entire data sequence exactly table think rst columns coding model subset episodes columns coding data model length size table total encoded length subset episodes given subset episodes episodes rst rows table encoding like data adding occurrences node episodes needed fourth row table table event coded rst second episode table intuitively better data compression overlaps parts data encoded different episodes selected set minimized better compression data choose episodes high frequency cover large number events non redundant overlaps mentioned reduced intuitive reason coding scheme looking subset episodes achieves best compression data table encoding event sequence size episode occurrences episode list occurrences objective subset episodes encode data like best data compression poses counting length memory assume event types times occurrence integers integer takes unit memory let node episode frequency encoding row table need units unit represent size episode units represent event types episode units representing inter event gaps unit frequency units represent start times occurrences non overlapped distinct occurrences share events episode encodes events data need units memory want encode events data node episodes dene conclude useful candidate selecting improve encoding length comparison trivial encoding node episodes true utility assessed respect add compression given selected episodes let set episodes size greater given let denote total encoded length encode events occurrences episodes episodes encode remaining events episodes size given episodes let denote number events data covered occurrences data sequence dene overlap overlap score gives estimate extra ing efciency achieved selecting given set proved prop overlap means given current set episodes adding episode positive overlap score reduce total encoded length algorithm essentially greedy algorithm keeps adding episodes highest overlap score greedy selection best episode based overlap score set candidate episodes generated search lattice serial episodes candidate episode best episode paths search tree sake completeness pseudocode algorithm algorithm details algorithm algorithm find input event sequence maximum inter event gap maximum number selected episodes output set selected frequent episodes initialize nal set selected episodes data compression achievable generate candidate episodes calculate events shared occurrences pair candidate episodes repeat argmaxcoverlap overlap end overlap delete events corresponding currences selected episodes end episodes encode remaining events return run algorithm best episodes large value algorithm exits episodes size greater improves coding efciency algorithm needs frequency threshold given users score naturally prefer episodes higher frequency need threshold pick episodes based add coding efciency algorithm hyperparameters maximum allowable inter event gap critical calculating overlap score need decide type occurrences count frequency mentioned earlier uses distinct occurrences paper use non overlapped occurrences frequency episodes reason generative model present section non overlapped currences application text classication non overlapped occurrences natural choice frequency obtain sequence non overlapped occurrences distinct occurrences returned simple algorithm rst occurrence sequence distinct occurrence rst quence non overlapped occurrences onwards rst distinct occurrence starting overlapped occurrence sequence non overlapped occurrences pseudocode algorithm listed algorithm prove correctness algorithm sequence non overlapped occurrences maximal correct non overlapped frequency let denote set input occstarttime list start times distinct occurrences increasing order start times inter event gaps episode output occurrences list start times mal set non overlapping occurrences itr pointer rst element occstarttime occstarttime list return end itr starttime itr itr itr list itr starttime end return end itr itr overlapped occurrences returned algorithm currence thought tuple indices data sequence position events data constitute occurrence example data sequence occurrence episode constituted events represented tuple notation use denote time kth event episode occurrence set occurrences natural order occurrence earlier thj way occurrences selected algorithm following property easily seen hold property earliest distinct occurrence episode rst distinct occurrence starting distinct occurrence starts proposition maximal set non overlapped occurrences proof note xed interval injective serial episodes occurrences having different start times distinct sider set non overlapped occurrences episode let rst use induction prove let rst suppose inter event gaps xed means found distinct occurrence episode starts contradicts rst statement property earliest distinct occurrence suppose implies overlapped occurrences suppose true set contradicts fact property earliest distinct occurrence prove maximality set suppose assume inequality occurrence contradicts statement property distinct set occurrence non overlapped occurrences proves maximality set sum method nding subset serial episodes best characterizes data sequence use coding scheme described use greedy heuristic subset achieves best sion essentially algorithm use algorithm non overlapped occurrences episodes distinct occurrences use frequency selecting episodes best score section present interesting tive model provides statistical justication algorithm based selecting episode best score generative model episodes pairs section present class generative models specialized class hmms model motivated hmm based model single episodes proposed hmm contains markov chain state space states unobservable state symbol emitted nite symbol set according symbol ability distribution associated state stream symbols observable output sequence model case symbol set set event types observed output sequence sequence event types think event sequence event times explicitly specied occurrences frequencies general serial episodes inter event times specied time ordering event types data sequence important actual event times play role section consider serial episodes xed inter event times generative model state transition probability matrix markov chain parameterized single parameter called noise parameter pair serial episodes generative model small value noise parameter model output model event sequence containing non overlapped occurrences corresponding episodes occurrences episode non overlapped output event sequence occurrence episode arbitrarily interleaved occurrences episode good class generative model data source pair episodes form prominent frequent patterns frequency based non overlapped occurrences rst instance statistical generative model multiple episodes consider family models containing model possible pair episodes let denote model pair episodes given event sequence ask maximum likelihood estimate model class models essentially tell pair episodes best explains data sequence sense maximizing likelihood pair episodes necessarily frequent episodes data likelihood depends frequencies episodes number events data occurrences episodes share example better likelihood lower frequency overlap results present provide statistical justication coding scheme algorithm presented previous section hmm model hmm specied pij state transition probability matrix markov chain state space initial state probabilities denotes symbol probability distribution state let observed symbol output sequence joint probability output sequence state sequence given hmm bqt determine model maximum likelihood need data likelihood assessed evaluating joint probability likely state sequence argmax follow simplication employed ods hmm models assume referred assumption let denote model corresponding pair episodes description model sake simplicity consider episodes model depends episodes share event types consider separate cases necessary case common event types case common event types state space number states hmm state space partitioned parts episode states comprising states noise states comprising states episode states denoted noise states given state emission structure symbol probability distribution episode states delta function episode state emits symbol probability emits symbol probability noise state symbol probability distribution uniform alphabet set denote transition structure case share event types state transition probabilities episode states given fig case share event types transition probabilities episode states given state transition structure fig states mod mod mod transition probabilities given fig mod mod mod mod mod figure episode state transition structure mod mod mod mod mod mod mod mod figure transition structure mod mod noise states transition probability episode states structure shown fig noise state transit probability remain mod mod noted transition probabilities termined single parameter called noise parameter values individual transition probabilities xed intuitively simple manner state transitions noise state probability ing probability equally divided reachable episode states intuitively logic state transition structure recall state emit symbol emit event type emit event type allows arbitrary overlap occurrences similarly emitting event types constituting occurrence episode need contiguous episode states noise states cycle zero times coming episode states emitting event types event type emitted noise state transition structure given index incremented respect modulo share event types suppose share event type event type appears data occurrence possibilities accounted transition structure possibility occurrence occurrences share event transition structure given fig ensures generative model includes possibility initial states initial state probability initial state probability probability probability probability example consider model let alphabet set example state quences output sequences length emitted fig seen gure output sequence contains occurrences arbitrarily interleaved transitions episode states given fig episode states given fig special transition structure allows occurrences share event event type seen transition fig mod mod analysis figure noise state transition structure section derive expressions likelihood joint state output sequence hmm model use compare likelihoods models corresponding different pairs episodes expressions depend figure events sample output sequences pairs episodes share event types cases dealt separately analysis assume case episode states transition decomposing state sequence sub sequences corresponding episode noise states following observation equation transition probability state episode state bqt similarly corresponding state sequence non zero probability write joint probability denote lengths respective sequences length output event sequence written assumption monotonically increasing likely state sequence spends longest time episode states constraints imposed state transition structure state quence having non zero probability episode states corresponding particular episode occur sequence particular episode state revisited implies cycle types corresponding episode emitted suppose maximum possible number non overlapping occurrences respectively share event type episode state emit symbol likely state sequence number episode states sake simplicity assumption referred state sequence zero probability includes incomplete occurrence episodes assumption consider models assumption essentially implies given selected episode episode want select set episodes share event type selected choose frequent set case case episode states transition transition noted transition structure symbol emitted episode state transition occurrence episodes means event shared rences episodes hand symbol emitted episode state transition occurrence episode shared decompose episode states state sequence parts episode states corresponding event types shared form corresponding shared ones form state emits symbol joint probability output state sequence given let consider state sequence having non zero probability contains number occurrences episodes respectively let number events shared occurrences ensure assumption holds modifying model adding extra symbol end sequence marker end output sequence modifying symbol probability distribution noise states following trick hmms single episodes events covered occurrences episodes output sequence number events shared number events shared assumption state sequence joint ability increasing function occurrences episodes xed decreasing function number events shared occurrences let maximum possible number overlapped occurrences respectively likely state sequence emits number occurrences episode states state sequences shares minimum number events occurrences let number shared events corresponding episode higher data likelihood compared assumption case assumption easily seen easy check ply overlap overlap overlap overlap case scenario depending values overlaps metrics greater smaller consider sub cases case overlap overlap overlap overlap overlap overlap assumption case overlap overlap overlap overlap overlap overlap let assume write similar expression model assumption lihood higher pair episodes share lesser number events general relative likelihood depends frequencies difference overlaps better understand let dene metrics rate episode respect episode overlap overlap given episode values metric episode higher results presented provide statistical justication algorithm presented previous section select episodes based overlap score given suppose selected want choose second episode based choice depends sign gure merit motivated considerations coding efciency essentially difference overlap gure merit determines pair episodes maximize data likelihood application text classification section present novel application method nding good subset frequent episodes acterize data application domain text classication text classication techniques use words approach document data sample represented collection words belong dictionary dictionary usually considered set unique words present training corpus preliminary preprocessing makes size tionary large leading high dimensionality feature vector representation document vector space representation documents word averaging depends largely dictionary words think text document sequence events event types words training data unsupervised fashion use method best subset serial episodes represent data episodes likely contain specic words important document collection dictionary built words event types found subset discovered episodes likely useful explore let dictionary obtained unique words usual preprocessing training data collection termed dictionary run algorithm ering best subset serial episodes achieve best data compression entire training corpus form new dictionary set unique words types present non singleton episodes episodes size discovered algorithm smaller sized dictionary dictionary case represent documents vectors dictionaries investigate standard classiers naive bayes svm simulations standard benchmark datasets large dimensionality reduction loss accuracy classier typically training data text classication documents document short mining episodes achieve signicant compression ually document interesting episodes mainly sequence short string training data classes long document set frequent serial episodes achieve best compression algorithm cussed paper employ special symbols denote end training document modify mining algorithm occurrence episode span different documents single label text categorization cessed stemmed version datasets use class stemmed version dataset class dataset class dataset dictionary set unique words present stemmed training data apart movie review dataset prepared pang lee polarity dataset sentiment analysis dataset consist movie reviews preprocessing steps converted letters lower case removed words characters long stop words removed dictionary created preprocessed training data feature vectors compared text classication accuracies different models bag data sample converted feature vector dimension size corresponding dictionary feature represents frequency word data sample movie review dataset binary features denoting presence absence word corresponding ment idf cosine normalization feature vectors explained subsection average embedding vecavg produce word embeddings text represented average embeddings words present text case dictionary averaging word embeddings words tionary rest ignored case movie reviews newsgroup pretrained model googlenews vectors case datasets stemmed model trained gensim library parameters vector idf term frequency inverse document frequency idf merical statistic good quantifying importance word document collection let frequency number occurrences word document instead raw frequency feature value use modied word frequency dened odif ied idf inverse document frequency idf given idf log experimental results datasets total number documents number documents contain word use compared classication accuracies dard benchmarks newsgroup webkb downloaded publicly available repository datasets cachopo org datasets single label text categorization cornell edu people pabo movie review com mmihaltz googlenews vectors modied frequency word odif ied feature value feature vectors cosine normalized dataset webkb newsgroup dictionary dictionary results compare classication accuracies obtained ing proposed dictionary obtained dictionary bow vecavg representation present results linear svm bow naive results presented comparison accuracies reported literature movie review dataset present mean value corresponding fold cross validation original folds introduced dataset webkb newsgroup movie review number discovered episodes size dict size dict table dictionary sizes different datasets table shows sizes dictionaries different datasets number episodes reported table number non singleton episodes seen table size dictionary fourth dictionary case webkb datasets eighth tenth method results signicant reduction dictionary size feature vector dimension classication accuracies obtained different tionaries shown tables table shows accuracies measure linear svm classier vec avg representation table shows naive bayes linear svm classiers bow representation try nonlinear svm studies benchmark data sets reported accuracies linear svm easy accuracies measure scores dataset accuracy measure webkb newsgroup movie review dict dict dict dict table linear svm accuracy vecavg sentation dataset classier accuracy measure scores webkb newsgroup movie review svm svm svm svm dict dict dict dict table naive bayes linear svm classication accuracy bow representation table mean standard deviation classication accuracy naive bayes different dictionaries bow representation dataset webkb newsgroup dictionary dictionary table mean standard deviation classication accuracy linear svm different dictionaries bow representation scores bow vecavg representation achieved classier different dictionaries close conclude frequent episodes based method allows large tion dictionary size signicant change classication accuracy note accuracy dictionary table consistent bag words accuracy reported train test split given original datasets bow representation generated random splits datasets webkb newsgroup having train test distribution class original split results showing averages standard deviations presented tables results clearly signicant differences accuracies achieved tionaries bow representation document cation application method learning dictionary results signicant decrease feature vector sion method different generic mensionality reduction techniques pca pca dimensionality reduction choosing certain linear combinations earlier features original feature vector dimension tens thousands new features obtained linear combinations semantically interpretable data mining method essentially decides words retained rejected method essentially feature selection method dimensionality reduction method ality reduction achieved semantically interpretable feel data mining present sample words retained rejected method case movie review webkb datasets words shown hand picked set randomly selected words easy makes good semantic sense example movie review reject movie related words like stunts theater performances appear reviews carry information sentiment review hand retain words like hilarious boring surprised carry sentiment information similar comments apply webkb dataset selected words like prerequisite introductory project commonly found project course web page carry dataset movie review sentiment negative sentiment stunts theatre cinematographer moviestar directorship producers storyteller scripts spotlight audition auditorium backstage torrent reviewer performances entertainment enjoyable funny hilarious entertaining superb boring sleepy disappointed twists clever impressed surprised liked interested awful pleasing miserably dumber interesting impressive intelligent fantastic webkb chemistry cryptography probabilistic lagrangian arithmetic scholarship bibtex manuscript newsletter computer interdisciplinary mathematician biotechnology accuracy baseline neurocomputing gaussian syllabus internet introductory prerequisite research bibliography professor student quiz exercise credit query tutor project phd fellowship conference curriculum scientist magazine instructor theorem homework examination semester journal homepage rejected words selected words table sample words set rejected selected words dictionary tive information data mining method based nding episodes compressing data effective picking dictionary relevant text corpus conclusions paper considered problem discovering small set serial episodes characterize sequential data extended existing algorithm work non overlapped frequency main contribution novel hmm based ative model pairs episodes model generates general output sequences episodes prominent frequent episodes overlapped frequency model intuitive symbols emitted episode states constitute based occurrences episodes noise states emit symbol symbols emitted noise states thought distracting signal mask real episodes contribute spurious frequent episodes transition structure intuitively motivated state transitions noise state probability remaining probability equally divided reachable episode states model class showed episode pair model best likelihood data sequence determined frequencies episodes overlaps occurrences analytical expressions derived data likelihoods provide statistical justication algorithm selecting subset episodes algorithm motivated based mdl principle intuitively appealing coding scheme encode data episodes algorithm nds subset episodes maximize data compression achieved essentially incrementally picking episodes based called overlap score depends frequency episode extent overlap occurrences selected episodes hmm based model provides statistical justication strategy algorithm generative model sequential data capture teractions episodes justify mdl based algorithm frequent episodes novel contributions paper mentioned section algorithms motivated mdl philosophy succinctly characterizing data small set frequent patterns algorithms sequential data heuristic nature believe hmm model presented good rst step developing statistical theory mdl based algorithms good subset frequent episodes important contribution paper novel application frequent episodes mining text tion view text document sequence events event types words subset episodes best characterizes entire text corpus terms data compression words appearing subset frequent episodes likely gives formative words corpus use words form dictionary documents represented vectors method amounts learning context sensitive dictionary idea quent pattern mining data mining method need user specied hyperparameters true method dimensionality reduction best knowledge rst instance application frequent pattern methods dictionary learning showed extensive simulations method results fold decrease size dictionary compromising classication accuracy seen examples retained rejected words method effective learning good subset words hmm model presented pairs episodes principle extendable number episodes notationally complex good extension work presented direction extending analytical techniques arbitrary number episodes generative models general sessing statistical signicance frequency episode model introduced accounts interactions episodes usable questions observed frequencies episodes signicant given extent overlap occurrences useful direction work presented extended references aggarwal han frequent pattern mining springer vreeken van leeuwen siebes krimp mining itemsets compress data mining knowledge discovery vol tatti vreeken long short summarising event sequences serial episodes proceedings acm sigkdd international conference knowledge discovery data mining acm mampaey tatti vreeken tell need know succinctly summarizing data itemsets proceedings acm sigkdd international conference knowledge discovery data mining acm lam orchen fradkin calders mining pressing sequential patterns statistical analysis data mining vol ibrahim sastry sastry discovering compressing serial episodes event sequences knowledge information systems vol bhattacharyya vreeken efciently summarizing event sequences rich interleaving patterns proceedings siam international conference data mining siam fan zhang tan discovering thy themes sequenced data step computational journalism ieee transactions knowledge data engineering vol mannila toivonen inkeri verkamo discovery frequent episodes event sequences data mining knowledge discovery vol gwadera atallah szpankowski reliable tion episodes event sequences knowledge information systems vol tatti signicance episodes based minimal windows data mining ninth ieee international conference ieee gwadera crestani ranking sequential patterns respect signicance advances knowledge discovery data mining low kam rassi kaytoue pei mining cally signicant sequential patterns data mining icdm ieee international conference ieee laxman sastry unnikrishnan discovering frequent episodes learning hidden markov models formal connection ieee transactions knowledge data engineering vol grunwald minimum description length principle vol cambridge mit press matthijs van leeuwen mining sets patterns compression frequent pattern mining aggarwal han eds springer luo wang zhuang mining positioning episode rules event sequences ieee transactions knowledge data engineering vol achar laxman sastry unied view apriori based algorithms frequent episode discovery edge information systems vol laxman tankasali white stream prediction generative model based frequent episodes event sequences proceedings acm sigkdd international conference knowledge discovery data mining acm socher perelygin chuang manning potts recursive deep models semantic ity sentiment treebank proceedings conference empirical methods natural language processing pang lee sentimental education sentiment analysis subjectivity proceedings acl cardoso cachopo improving methods single label text categorization pdd thesis instituto superior tecnico dade tecnica lisboa wang manning baselines bigrams simple good sentiment topic classication proceedings annual meeting association computational linguistics short volume association computational linguistics
