multi modal summarization for video containing documents xiyan jun zhenglu of computer science nankai university china university china fuxiyan nankai edu cn edu cn p e s l c s c v v i x r a abstract summarization of multimedia data becomes increasingly signicant as it is the basis for many real world applications such as question answering web search and so forth most isting multi modal summarization works ever have used visual complementary tures extracted from images rather than videos thereby losing abundant information hence we propose a novel multi modal tion task to summarize from a document and its associated video in this work we also build a baseline general model with effective strategies i e bi hop attention and improved late fusion mechanisms to bridge the gap tween different modalities and a bi stream summarization strategy to employ text and video summarization simultaneously prehensive experiments show that the posed model is benecial for multi modal marization and superior to existing methods moreover we collect a novel dataset and it vides a new resource for future study that sults from documents and videos introduction multi modal summarization is conducted to rene salient information from different modalities cluding text image audio and video and to sent key information through one or more ties evangelopoulos et al li et al given the rapid increase of multimedia data ination over the internet this task has been widely exploited in recent years early works on multi modal summarization has dealt with sports video summarization goro et al meeting recordings tion erol et al li et al or dia micro blog summarization bian et al most of these approaches concentrate on rizing data that include synchronous information you are interested in our dataset please contact authors figure illustration of the proposed multi modal marization task which generates summaries from a document and its related video among different modalities however due to the lack of accurate alignment summarizing the large volume of multi modal data of the same topic is impractical to address this issue neural networks based methods have been introduced that search for the corresponding relations from different formation resources for example li et al learn the joint representations of texts and images to identify the text that is relevant to an image zhu et al and chen and zhuge propose an attention model to summarize a document and its accompanying images although the aforementioned models have shown reasonable qualitative results they still fer from the following drawbacks most existing applications extract visual information from the companying images but they ignore related videos we contend that videos contain abundant contents and have temporal characteristics where events are represented chronologically which are crucial for text summarization to the best of our knowledge the only work li et al that considers video information however neglects temporal sion and simply treating videos as a bag of images the authors use video frames for sentence ing in outputs rather than combining visual mation with text processing although attention mechanism bahdanau et al and early sion snoek et al are used extensively it adversely introduces noise as it is unsuitable for multi modal data without alignment which is acterized by a large gap that requires intensive munication various multi modal summarization works have focused on a single task such as text or video summarization with added information from other modalities zhu et al wei et al we observe that both summarization tasks share the same target of rening original long materials and as such they can be performed jointly due to common characteristics in this work we propose a novel multi modal summarization task which is depicted in fig to remove the noise among different modalities and effectively integrate complementary information we introduce a bi hop attention mechanism to align features and induce an improved late fusion method for feature fusion moreover we apply a bi stream summarization strategy for training by sharing the ability to rene signicant information from long materials in text and video summarization given the lack of relevant datasets for experiments we create a novel multi modal dataset from the daily and cnn websites collecting articles and their corresponding summaries videos images the main contributions are as follows we introduce a novel task that automatically generates a textual summary with signicant images from the multi modal data associated with an article and its corresponding video we propose the bi hop attention and improved late fusion mechanism to rene information from multi modal data besides we introduce a bi stream summarization strategy that taneously summarizes articles and videos we prepare a content rich multi modal dataset comprehensive experiments demonstrate that complementary information from multiple modalities is benecial and our general line model can exploit them more effectively than the existing approaches dailymail co uk index html cnn related work text summarization selects salient information from long documents to form a concise summary existing approaches can be roughly categorized into two groups extractive based methods which extract key sentences and objects without cation guarantee basic level grammaticality and accuracy for example jadhav and rajan modeled the interaction between keywords and salient sentences by using a new two level pointer network the proposal of narayan et al wu and hu extracted summary via deep forced learning abstractive based methods which paraphrase the signicant contents after hending the original document construct cated summaries with newly generated words and coherent expressions tan et al proposed a graph based attention mechanism in a to sequence framework the work of cao et al retrieved proper existing summaries as didate soft templates and extended the framework to jointly perform template reranking and summary generation see et al presented a generator network which can both copy words from articles and generate novel words recently the sheer volume of ne tuning approaches liu and lapata zhang et al dong et al boosted the quality of the generated summaries based on the pre trained language models which have advanced a wide range of nlp tasks video summarization is conducted to facilitate large scale video browsing by producing concise summaries early works in video summarization have focused on videos of certain genres they generated summaries by leveraging genre specic information i e salient objects of sports in ekin et al and signicant regions of egocentric videos in lu and grauman furthermore summarizing videos with auxiliary resources such as web images and videos has attracted able attention song et al developed a archetypal analysis technique that learns canonical visual concepts shared between a video and images given that the above mentioned non deep rization methods are time consuming zhou et al modeled video summarization via a deep rnn to capture long term dependencies in video frames and proposed a reinforcement based framework to train the network end to end recently a novel query focused video tion task was introduced in xiao et al multi modal summarization generates a densed multimedia summary from multi modal original inputs such as text images videos and uzzaman et al introduced an idea of illustrating complex sentences as multimodal maries that combine pictures structure and plied compressed text et al palaskar et al studied abstractive text marization for open domain videos besides li et al developed an extractive multi modal summarization method that automatically ates a textual summary based on a set of documents images audios and videos related to a specic topic zhu et al combined image selection and output to alleviate the modality bias chen and zhuge employed hierarchical decoder model to align sentences and images cently aspect aware model was introduced in li et al to incorporate visual information for producing summary for e commerce products model we introduce a novel multi modal summarization model to automatically generate modal summary from an article and its ing video fig illustrates the idea our proposed model consists of four modules feature extraction alignment fusion and bi stream summarization which are presented in fig feature extraction text feature we utilize a hierarchical framework based on word and sentence levels to read the kens of an article and induce a representation of each sentence the bi directional long short term memory bilstm which constitutes a forward and a backward lstm is employed as recurrent unit the rst layer of the encoder which extracts ne grained information of a sentence runs at the word level the hidden state of the jth word in the ith sentence is represented by the bilstm i e hj hj i moreover the encoder consists of a i second layer conducted at the sentence level which accepts the average pooled concatenated hidden states from the word level encoder as inputs the hidden state of the ith sentence is dened as hj i s s api i where ni is the number of words in the ith sentence and represents lstm figure utilizes multi modal information for sentence extraction in summarization the probability of the mismatched sentence calculated solely based on text is improved with the assistance of video video feature videos that accompany articles ten capture news highlights and usually provide abundant complementary information from the spective that is different from the article itself we choose he et al to extract frame features for its excellent performance thermore we train a bilstm to model the quential pattern in video frames where the tinctions between videos and bag of images are exhibited the image feature of the ith frame is a dimensional vector vi which is obtained using vi ramei correspondingly the representation of a frame is dened as mi where represents bilstm recently inspired by the success of the language model bert jacob et al which pre trains deep bidirectional representations on unlabeled texts a few visual linguistic joint models have been proposed such as videobert sun et al vilbert lu et al and vlp zhou et al these models can induce superior features for summarization our model is devised from a new perspective to extract ne grained tures which are orthogonal to the aforementioned bert based ones in a complementary manner feature alignment existing multi modal models with high alignment can not be used for capturing the correct common features among different modalities due to the chronism between modalities hence we introduce two multi modal attention mechanisms which cus on diverse parts of video frames in accordance to a current article sentence in order to search for aligned information figure overview of the multi modal summarization model and respectively represent the hadamard product and outer product and stands for vector concatenation and are the special versions of g mentioned in feature fusion section with different parameters single step attention we fulll the above stated goal by adopting the attention mechanism which was introduced in bahdanau et al cally ej i v t wmmj battn where ej i is the attention value between the ith tence and jth image and v ws wm and battn are learnable parameters although lots of modal summarization models chen and zhuge zhu et al li et al follow this mechanism we argue that it is unsuitable for a multi modal task because one modality might inate the summary resulting in substantial loss of information from other modalities therefore we generalize over bilinear attention kim et al and propose improvements such as single feature projection considering that features of different modalities are independent from each other the projection can be separately calculated such that neither component dominates another the feature ping of each modality can be modied as follows qj bm ri bs where wm ws bm and bs are parameters dual feature interaction for close communication of different ties we further propose a dual interaction ture calculated using hadamard product the concatenation feature cf represents the feature of each modality while dual interaction feature avoids insufcient interaction among modalities the attention value is formulated as ej i v t qj ri qj ri cf after calculating the softmax of the attention value to obtain weight aj i of each image state mj and computing the weighted sum of these states text value can be represented as follows ci aj i mj i sof aj i n m where nm is the number of ltered frames bi hop attention given that the transcript tracted from a video shares the same modality with an article and accurately aligns with a video we induce it as a bridge to deepen the relationship tween two modalities we introduce bi hop tion to produce a context by simultaneously bining text sentence with a transcript and video frames since a transcript is similar to an article text we use the bilstm to extract its features tn t where nt is the number of words in transcripts moreover the context vector of script m is obtained by replacing si with ti in eq for a new attention value bk i and the nal context vector of article m is improved as t by replacing mj with in eq for k bk i n t n m kmj similarly bi hop attention can be reversed to obtain an article context for video summarization feature fusion in terms of combining complementary information from multiple modalities we develop a method that not only smoothly suppresses the effect of modality in unfavorable situations but also tures synergies between the modalities that share reliable complementary information ically we use two common strategies for taking cross modal correlations early fusion which catenates various features directly has been plored in multi modal tasks extensively zhu et al chen and zhuge the fusion ture of ith sentence of text summarization can be represented as pi ts tensor fusion zadeh et al disentangles unimodal and modal dynamics by modeling each of them itly for intra modality and inter modality dynamics pi ts is denoted as fusion feature where indicates the outer product further the label of each sentence can be predicted by pi ts both the above mentioned strategies are based on a strong assumption that each modality is curately aligned yet multi modal summarization always contains asynchronous information hence we consider that late fusion which uses unimodal decision values and fuses them with a fusion mechanism suits multi modal summarization well f is the fusion process where g is a conventional feed forward network and f can be a function such as averaging ing or other learned models we use a feedforward network in this work given that each modality is not useful all the time e frames of accompanied interview video may contribute only to a small tent to visual features we restrain the noise from a certain modality by following the ideas in liu et al and induce fusion its fusion process is improved as f wsf and ws wc are calculated as follows ws wc where is a smoothing coefcient determining the penalty intensity for example if a frame comes from an interview video and its visual features are irrelevant for classication becomes small which bases the prediction mainly on article bi stream summarization training given that text and video summarization aim to extract salient information from the original dant content we propose bi stream summarization training strategy it indicates that they are learned jointly and simultaneously which improve the eralization performance by holding similar tives and sharing complementary information we consider extractive based text tion as a sentence classication task in which the binary label of each sentence is imperative given that most corpora only contain abstractive maries written by humans we construct the label of each sentence following the methods in ati et al sentences are selected to maximize the rouge with respect to the gold summary by a greedy approach furthermore we use cross tropy for training lts log yn yn log yn n s n s where and yn represent the true and predicted labels respectively video summarization can also be considered as a classication task we use unsupervised learning by rl methods proposed in zhou et al its loss can be separated into diversity reward rdiv and representativeness reward rrep the former measures the dissimilarity among selected frames in the feature space whereas the latter measures how well the generated summary can represent the original video the calculations are as follows d rdiv jm rrep exp n m n m min where m is the set of selected video frames and is the dissimilarity function which employed as one minus cosine similarity in this work for bi stream summarization we mix the loss from dual summarization tasks whose contribution relies on their weight ts and vs l tslts rrep experiments dataset and evaluation metrics there is no existing dataset that contains articles corresponding videos and references for modal summarization we construct a new corpus called mm avs to obtain high quality summaries we collect data from daily mail and cnn websites same as in hermann et al we also serve the related titles images and their captions baseline methods results for multi modal research samples which miss any elements mentioned above or which video duration is less than seconds are removed the detailed statistics of the corpus are shown in table notice that the videos in cnn are much longer than that in daily mail we collect less samples from the former and we mainly use them during testing rouge lin with standard options is used for text summary evaluation we apply the and rouge l r l score to evaluate the overlap of uni grams bi grams and the longest common subsequences between the decoded summary and the reference to evaluate the quality of the generated video maries images we employ resnet to construct image feature vectors and calculate the cosine age similarity cos between image references and the extracted frames from videos multi modal based vistanet truong and lauw prioritizes sual information as alignment to point out the portant sentences of an article to detect the ment of a document mm atg zhu et al is a multi modal tention model generating text and selecting the vant image from the article and alternative images hori et al applies multi modal video features including video frames transcripts and dialog context for dialog generation tfn zadeh et al learns both intra and inter modalities by modeling a new multi modal tensor fusion approach hnnattti chen and zhuge aligns the tences and accompanying images by attention pure video summarization random extracts the key video frames randomly uniform samples videos uniformly vsumm de avila et al extracts color tures from video frames via k means clustering dr dsn zhou et al proposes a ment learning framework equipped with a reward function for diverse and representative summaries pure text summarization picks the rst three sentences as summary summarunner nallapati et al is a pretable rnn based sequence model and it can be trained in both extractive and abstractive manners nn se cheng and lapata consists of a hierarchical document encoder and an based extractor that can extract sentences or words article number avg article length avg article sent num avg summary length avg video duration avg transcript length daily mail cnn table corpus statistics experimental settings the dataset is split by for train validation and test sets respectively the hidden dimension of bi lstm is beta in fusion is and the proportion of each modality in loss is to remove the redundancy of videos one of ve consecutive frames is randomly selected the last layer of with dimension is used for image feature extraction we perform training by adagrad with learning rate we use early stop to avoid overtting quantitative analysis as shown in table outperforms the best performing baseline for both text and video maries on the daily mail dataset these ments are achieved thanks to the bi hop attention mechanism improved fusion and bi stream summarization strategy which are all jointly porated in table also shows that vistanet underperforms other models by a large margin we consider that the learning strategy of vistanet wherein image information is considered prior to text information may bring noise in addition table shows that the results on cnn are inferior to those on daily mail we sider this may be attributed to the longer length of original materials especially of videos as shown in table given that it is difcult to extract visual features from long videos summarizing the articles in the cnn dataset is more challenging besides we ignore image evaluation in cnn due to the lack of reference from the original website and explore video summaries by comparing with pure video summarization in section ablation study to verify the effectiveness of each component of our model we conduct ablation experiments we construct a hierarchical framework which trates on word and sentence level to generate maries as our baseline based on the hierarchical text component three constituents of are model vistanet mm atg tfn hnnattti daily mail r l cnn r l table comparisons of proposed model with the multi modal baselines all of the text encoder parts in multi modal approaches are modied as hierarchical framework for fair comparison model vsumm random uniform dr dsn text video frames transcripts bi stream r l table ablation study to evaluate the effects of ent components of table comparisons of model with video marization baselines model summarunner nn se table text summary comparisons of proposed model with the text summarization baseline r l added in turn video frames indicates adding the frames of the related videos to provide tary information transcripts extracts transcripts of the videos to deepen the relationship between different modalities and bi stream uses the stream summarization training strategy to learn the similarity shared by summarization tasks as shown in table each component of contributes to improving the performance by ing video with text information this validates our assumption that videos possess abundant mentary information to texts and this information facilitates capturing the core ideas of materials prehensively and inducing effective summaries evaluation on single modalities to assess the gains of the proposed method coming from the multi modal information we compare it with the popular single modal approaches on daily mail dataset including the video only tion and text only summarization the video summary comparisons are shown in table we collect the related images as ences in an online manner because manually ing each video frame is labor intensive and consuming as shown in table performs better than the video only methods it can be figure comparison of four feature fusion methods each way is conducted with three training strategies tributed to its capability to derive comprehensive insights from multi modal materials in terms of the text summary as shown in table is also competitive although it does not achieve as much large margin as in the case of the multi modal summary comparisons we speculate this may be due to the noisy information using an effective information lter is a promising way which worth in depth exploration in the future sub module evaluation we evaluate the effectiveness of each module of the proposed model fig illustrates the mance of four feature fusion approaches which are early tensor late and fusion all these ture fusion methods are trained under three gies cross entropy minimizing the binary cross entropy video loss adding video summarization loss for optimization and weighted that gives weights to each task in consideration of their ent contributions as demonstrated in fig early fusion which is most commonly used in practice is inferior to other fusion approaches due to the asynchrony of multi modal feature and the various signicance of each modality simple feature catenation in early fusion may introduce noise and no attention concat product bilinear attention table comparison of three feature alignment gies on the basis of early fusion r l article reference cnn daily mail cnn daily mail r l table words overlap statistics of video transcripts with articles and references figure evaluation of the smoothed value in fusion and ts vs of each modality in loss lose its effectiveness for handling complex cases moreover fig depicts that weighted achieves the best results we consider that it successfully avoids unfavorable inuence of irrelevant ties in addition three attention mechanisms are tested no attention which uses the last state as video feature directly concat product which uses the common mechanism introduced in bahdanau et al and bilinear attention which we pose in this paper table presents the results and indicates a stimulating phenomenon that product attention mechanism performs poorly even worse than models without attention we speculate that conventional attention which lacks cation between modalities will mislead the model as it focuses on irrelevant parts and brings noise balance between modalities a particular characteristic of multi modal rization is that the related data are complementary and thus different modalities contribute differently to summarization hence we restrain the noises from irrelevant modalities through the improved late fusion and balance the loss function in ent tasks the left graph in fig illustrates the penalty of irrelevant information which shows that yields the best results the other graph depicts the proportion of each modality in a loss function and illustrates that the text tion gives approximately attention to the text modality however this graph also reveals that nearly of useful information is searched and complemented by other modalities effectiveness of transcript incorporates video transcripts to bridge videos and texts with appropriate alignments this is one of the critical factors for proper article video inform satis table manual summary quality evaluation rization as demonstrated in table to further investigate the nature of transcripts we tively evaluate their relationships with the articles and references as shown in table the results demonstrate that video transcripts are distinct from articles with low overlaps indicating that they are not repeating of articles but provide extra and ful information table also illustrates that they poorly correlate with references which suggests that transcripts assist summary generation by turing the key information of videos however they are not enough for nal summaries manual evaluation examples with text and video summarization results were selected and graduate students were volunteered to evaluate them based on ness inform and satisfaction satis each sample was graded on the scale from to where a higher score is better we calculate the average score of each evaluation table shows it further strates the proposed method conclusion in this work we have proposed a multi modal summarization task that generates summaries from documents and the related videos we have also constructed a content rich video containing dataset for future study a comprehensive evaluation has demonstrated the effectiveness of the proposed model and individual introduced strategies our work can be extended in some ways for example acoustic features could be extracted from acoustic signals and incorporated into our model to provide additional complementary information i e sentiment and tone in another example to further advance user satisfaction it would be worthwhile to explore generation techniques such as generating a small video accompanied with text description as a summary references dzmitry bahdanau kyunghyun cho and yoshua gio neural machine translation by jointly learning to align and translate in iclr jingwen bian yang yang and tat seng chua multimedia summarization for trending topics in croblogs in cikm pages ziqiang cao wenjie li sujian li and furu wei retrieve rerank and rewrite soft template based neural summarization in acl pages jingqiang chen and hai zhuge abstractive image summarization using multi modal attentional hierarchical rnn in emnlp pages jianpeng cheng and mirella lapata neural marization by extracting sentences and words in acl pages sandra eliza fontes de avila ana paula brandao lopes antonio da luz jr and arnaldo de querque araujo vsumm a mechanism designed to produce static video summaries and a novel evaluation method pattern recognition ters li dong nan yang wenhui wang furu wei aodong liu yu wang jianfeng gao ming zhou and hsiao wuen hon unied language model pre training for natural language in neurips pages ing and generation ahmet ekin a murat tekalp and rajiv mehrotra automatic soccer video analysis and ieee transactions on image processing rization berna erol d s lee and jonathan hull timodal summarization of meeting recordings in icme volume georgios evangelopoulos athanasia zlatintsi dros potamianos petros maragos konstantinos pantzikos georgios skoumas and yannis avrithis multimodal saliency and fusion for movie summarization based on aural visual and ieee transactions on multimedia tual attention kaiming he xiangyu zhang shaoqing ren and jian sun deep residual learning for image nition in cvpr pages karl moritz hermann tomas kocisky edward stette lasse espeholt will kay mustafa suleyman and phil blunsom teaching machines to read and comprehend in neurips pages chiori hori huda alamri jue wang gordon ern takaaki hori anoop cherian tim k marks vincent cartillier raphael gontijo lopes hishek das al end to end audio sual scene aware dialog using multimodal based video features in icassp pages devlin jacob chang ming wei lee kenton and toutanova kristina bert pre training of deep bidirectional transformers for language ing in naacl pages aishwarya jadhav and vaibhav rajan tive summarization with swap net sentences and in acl words from alternating pointer networks pages jin hwa kim kyoung woon on woosang lim jeonghee kim jung woo ha and byoung tak zhang hadamard product for low rank ear pooling in iclr haoran li peng yuan song xu youzheng wu aodong he and bowen zhou aspect aware multimodal summarization for chinese e commerce products in aaai page in press haoran li junnan zhu tianshang liu jiajun zhang and chengqing zong multi modal sentence summarization with modality attention and image tering in ijcai pages haoran li junnan zhu cong ma jiajun zhang and chengqing zong multi modal tion for asynchronous collection of text image dio and video in emnlp pages manling li lingyu zhang heng ji and richard j radke keep meeting summaries on topic abstractive multi modal meeting summarization in acl pages jindrich shruti palaskar spandana gella and florian metze multimodal abstractive summarization of opendomain videos in neurips workshop on vigil chin yew lin rouge a package for matic evaluation of summaries text summarization branches out kuan liu yanen li ning xu and prem natarajan learn to combine modalities in multimodal deep learning arxiv preprint yang liu and mirella lapata text in emnlp pages tion with pretrained encoders jiasen lu dhruv batra devi parikh and stefan lee vilbert pretraining task agnostic olinguistic representations for vision and language tasks in nips pages zheng lu and kristen grauman story driven in cvpr summarization for egocentric video pages ramesh nallapati feifei zhai and bowen zhou summarunner a recurrent neural network based quence model for extractive summarization of ments in aaai pages shashi narayan shay b cohen and mirella lapata ranking sentences for extractive tion with reinforcement learning in naacl pages kaiyang zhou yu qiao and tao xiang deep reinforcement learning for unsupervised video marization with diversity representativeness reward in aaai pages shruti palaskar jindrich libovicky spandana gella and florian metze multimodal abstractive in acl pages summarization for videos luowei zhou hamid palangi lei zhang houdong hu jason j corso and jianfeng gao ed vision language pre training for image ing and in aaai page in press junnan zhu haoran li tianshang liu yu zhou jun zhang and chengqing zong msmo timodal summarization with multimodal output in emnlp pages junnan zhu yu zhou jiajun zhang haoran li chengqing zong and changliang li modal summarization with guidance of multimodal reference in aaai page in press abigail see peter j liu and christopher d manning get to the point summarization with generator networks in acl pages cees gm snoek marcel worring and arnold wm smeulders early versus late fusion in tic video analysis in acm mm pages yale song jordi vallmitjana amanda stent and jandro jaimes tvsum summarizing web videos using titles in cvpr pages chen sun austin myers carl vondrick kevin phy and cordelia schmid videobert a joint model for video and language representation ing in iccv pages jiwei tan xiaojun wan and jianguo xiao abstractive document summarization with a based attentional neural model in acl pages dian tjondronegoro xiaohui tao johannes sasongko and cher han lau multi modal tion of key events and top players in sports ment videos in wacv pages quoc tuan truong and hady w lauw vistanet visual aspect attention network for multimodal timent analysis in aaai pages naushad uzzaman jeffrey p bigham and james f allen multimodal summarization of complex sentences in iui pages huawei wei bingbing ni yichao yan huanyu yu aokang yang and chen yao video in aaai rization via semantic attended networks pages yuxiang wu and baotian hu learning to extract coherent summary via deep reinforcement learning in aaai pages shuwen xiao zhou zhao zijian zhang xiaohui yan and min yang convolutional hierarchical tention network for query focused video tion arxiv preprint amir zadeh minghai chen soujanya poria erik bria and louis philippe morency tensor sion network for multimodal sentiment analysis in emnlp pages xingxing zhang furu wei and ming zhou bert document level pre training of hierarchical bidirectional transformers for document tion in acl pages
