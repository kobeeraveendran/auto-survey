deep keyphrase generation rui meng sanqiang zhao shuguang han daqing peter brusilovsky yu chi school computing information university pittsburgh pittsburgh pa rui meng daqing peterb edu p e s l c s c v v x r abstract keyphrase provides highly summative information effectively understanding organizing ing text content previous studies provided workable solutions automated keyphrase extraction commonly divided summarized content multiple text chunks ranked selected meaningful approaches ones identify keyphrases appear text capture real semantic meaning text propose generative model keyphrase prediction encoder decoder framework effectively overcome drawbacks deep keyphrase generation attempts capture deep semantic meaning content deep learning method empirical sis datasets demonstrates proposed model achieves nicant performance boost extracting keyphrases appear source text generate absent keyphrases based semantic meaning code dataset available text com memray keyphrase introduction keyphrase keyword piece short mative content expresses main semantic meaning longer text typical use keyphrase keyword scientic publications provide core information paper use corresponding author term keyphrase interchangeably word rest paper terms implication contain tiple words high quality keyphrases tate understanding organizing accessing document content result studies focused ways automatically extracting keyphrases textual content liu et al medelyan et al witten et al public accessibility scientic publication datasets test beds keyphrase extraction algorithms study focuses extracting keyphrases scientic publications automatically extracting keyphrases document called keypharase extraction widely applications information retrieval jones staveley text summarization zhang et al text categorization hulth megyesi opinion mining berend existing keyphrase extraction algorithms addressed problem steps liu et al tomokiyo hurst rst step acquire list keyphrase dates researchers tried use n grams noun phrases certain speech patterns identifying potential candidates hulth le et al liu et al wang et al second step rank candidates importance document pervised unsupervised machine learning ods set manually dened features frank et al liu et al kelleher luz matsuo ishizuka mihalcea tarau song et al witten et al major drawbacks keyphrase extraction approaches methods extract keyphrases pear source text fail predicting meaningful keyphrases slightly different quential order use synonyms authors scientic publications commonly assign keyphrases based semantic ing instead following written content publication paper denote phrases match contiguous subsequence source text absent keyphrases ones fully match text present keyphrases table shows proportion present absent keyphrases ment abstract commonly datasets observe large portions sent keyphrases datasets absent keyphrases extracted previous approaches prompts ment powerful keyphrase prediction model second ranking phrase candidates vious approaches adopted machine learning features tf idf pagerank features target detect importance word document based tics word occurrence co occurrence unable reveal semantics underlie document content table proportion present keyphrases absent keyphrases public datasets dataset inspec krapivin nus semeval keyphrase present absent overcome limitations previous ies examine process keyphrase diction focus real human annotators assign keyphrases given document man annotators rst read text sic understanding content try digest essential content summarize keyphrases generation keyphrases relies understanding content necessarily use exact words occur source text example human tors latent dirichlet allocation text write topic modeling text mining possible keyphrases addition semantic understanding human annotators pick tant parts based syntactic features ple phrases following propose apply use important text result better keyphrase prediction model understand semantic meaning content ture contextual features effectively capture semantic syntactic features use recurrent neural works rnn cho et al gers huber compress semantic tion given text dense vector e mantic understanding furthermore rate copying mechanism gu et al low model nd important parts based positional information model erate keyphrases based understanding text regardless presence absence keyphrases text time lose important text information contribution paper fold propose apply rnn based erative model keyphrase prediction incorporate copying mechanism rnn enables model successfully dict phrases rarely occur second rst work concerns problem sent keyphrase prediction scientic tions model recalls absent keyphrases conducted sive comparison important baselines broad range datasets results proposed model signicantly outperforms existing supervised unsupervised extraction methods remainder paper rst review related work section elaborate proposed model section present experiment setting section results section followed discussion section section concludes paper related work automatic keyphrase extraction keyphrase provides succinct accurate way describing subject subtopic document number extraction algorithms proposed process extracting keyphrases typically broken steps rst step generate list phrase didates heuristic methods dates prepared ltering erable number candidates produced step increase possibility correct keyphrases kept primary ways extracting candidates include retaining word quences match certain speech tag terns e nouns adjectives liu et al wang et al le et al extracting important n grams noun phrases hulth medelyan et al second step score candidate phrase likelihood keyphrase given document ranked candidates returned keyphrases supervised supervised machine learning methods widely employed supervised methods task solved binary classication problem types learning methods features explored frank et al witten et al hulth medelyan et al lopez romary gollapalli caragea unsupervised approaches primary ideas include nding central nodes text graph mihalcea tarau grineva et al detecting representative phrases cal clusters liu et al aside commonly adopted step process previous studies realized keyphrase extraction entirely different ways tomokiyo hurst applied language models measure phraseness tiveness phrases liu et al share similar ideas work word alignment model learns translation documents keyphrases approach alleviates problem vocabulary gaps source target certain degree translation model unable handle tic meaning additionally model trained target title summary enlarge number training samples diverge real objective generating keyphrases zhang et al proposed joint layer rent neural network model extract keyphrases tweets application deep neural networks context keyphrase traction work focused quence labeling able dict absent keyphrases encoder decoder model rnn encoder decoder model referred sequence sequence learning end end approach rst introduced cho et al sutskever et al solve translation problems provides erful tool modeling variable length sequences end end fashion ts natural guage processing tasks rapidly achieve great successes rush et al vinyals et al serban et al different strategies explored prove performance encoder decoder model attention mechanism bahdanau et al soft alignment approach allows model automatically locate relevant input components order use tant information source text ies sought ways copy certain parts content source text paste target text allamanis et al gu et al zeng et al discrepancy exists optimizing objective training rics evaluation studies attempted eliminate discrepancy incorporating new training algorithms marcaurelio ranzato et al modifying optimizing jectives shen et al methodology section introduce proposed deep keyphrase generation method detail task keyphrase generation dened lowed overview apply rnn encoder decoder model details work copying mechanism introduced sections problem denition consists n given keyphrase dataset th data sample data samples contains source text mi keyphrases mi source text keyphrase j sequences words lxi j j j j l j length word quence j respectively data sample contains source text sequence multiple target phrase sequences apply rnn encoder decoder model data need converted text keyphrase pairs contain source sequence target sequence adopt simple way splits data sample mi pairs mi encoder decoder model ready applied learn mapping source sequence target sequence purpose simplicity y denote data pair rest section word sequence source text y word sequence keyphrase encoder decoder model basic idea keyphrase generation model compress content source text den representation encoder generate corresponding keyphrases decoder based representation encoder coder implemented recurrent neural works rnn encoder rnn converts variable length input sequence xt set hidden representation ht iterating following equations time t ht xt f non linear function text vector c acting representation input non linear function q ht decoder rnn decompresses context vector generates variable length sequence y yt word word conditional language model st c st c st hidden state decoder rnn time t non linear function g softmax classier outputs probabilities words vocabulary yt predicted word time t taking word largest ity g encoder decoder networks trained jointly maximize conditional probability target sequence given source sequence ter training use beam search generate phrases max heap maintained predicted word sequences highest bilities details encoder decoder bidirectional gated recurrent unit gru plied encoder replace simple rent neural network previous studies bahdanau et al cho et al indicate generally provide better performance language modeling simple rnn simpler ture long short term memory works hochreiter schmidhuber result non linear function replaced gru function cho et al forward gru decoder addition attention mechanism adopted improve performance attention mechanism rstly introduced bahdanau et al model dynamically focus tant parts input context vector c puted weighted sum hidden representation h ht ci ijhj t ij hj hk hj soft alignment function measures similarity hj degree inputs tion j output position match copying mechanism ensure quality learned representation reduce size vocabulary typically rnn model considers certain number quent words e words cho et al large long tail words simply ignored rnn able recall keyphrase contains vocabulary words actually important phrases identied positional syntactic information contexts act meanings known copying nism gu et al feasible solution enables rnn predict vocabulary words selecting appropriate words source text incorporating copying mechanism probability predicting new word sists parts rst term probability generating term equation second probability copying source text similar attention mechanism copying mechanism weights importance word source text measure positional tion unlike generative rnn dicts word words lary copying siders words source text consequently hand rnn copying mechanism able predict words ulary source text hand model potentially preference pearing words caters fact keyphrases tend appear source text y z j yt j set unique words source text non linear function wc r learned parameter matrix z sum scores normalization gu et al details experiment settings section begins discussing signed evaluation experiments followed description training testing datasets introduce evaluation metrics baselines training dataset publicly available datasets evaluating keyphrase generation largest came krapivin et al tains scientic publications data unable train robust rent neural network model fact lions scientic papers available online contains keyphrases assigned authors collected large high quality scientic metadata computer science domain online ital libraries including acm digital library encedirect wiley web science han et al rui et al total tained dataset articles ing duplicates overlaps testing datasets times larger krapivin et al note model trained articles publications randomly held articles building new test dataset articles served validation dataset check convergence model training dataset supervised lines testing datasets evaluating proposed model hensively widely adopted scientic tion datasets addition datasets contain sand publications contribute new testing dataset larger number entic articles title abstract source text dataset described detail inspec hulth dataset provides paper abstracts adopt ing papers corresponding trolled keyphrases evaluation maining papers training supervised baseline models krapivin krapivin et al dataset provides papers text author assigned keyphrases author mention split ing data selected rst papers alphabetical order testing data remaining papers train pervised baselines nus nguyen kan use author assigned reader assigned keyphrases treat papers testing data nus dataset specically mention ways ting training testing data results supervised baseline models obtained ve fold cross validation kim et al ticles collected acm digital library articles testing rest training supervised baselines built new testing dataset contains titles abstracts keyphrases scientic articles computer ence randomly selected obtained articles ory limits implementation able train supervised baselines training set articles validation set train pervised baselines worth noting examined performance ing training dataset articles signicant improvement observed implementation details total pairs training text refers nation title abstract publication keyphrase indicates author assigned word text pre processing steps including kenization lowercasing replacing digits symbol applied decoder models trained tention mechanism rnn tention copying mechanism enabled rnn models choose frequently occurred words vocabulary dimension embedding set mension hidden layers set word embeddings randomly initialized uniform distribution models timized adam kingma ba initial learning rate gradient clipping dropout rate max depth beam search set beam size set training stopped convergence termined validation dataset stopping cross entropy loss stops dropping iterations generation keyphrases nd model tends assign higher probabilities shorter keyphrases keyphrases tain words resolve problem apply simple heuristic preserving rst single word phrase highest ing probability removing rest baseline models unsupervised algorithms tf idf trank mihalcea tarau ank wan xiao expandrank wan xiao supervised algorithms kea witten et al maui medelyan et al adopted baselines set unsupervised methods following mal settings hasan ng supervised methods following default setting specied papers evaluation metric evaluation metrics macro averaged cision recall f measure employed measuring algorithm s performance lowing standard denition precision dened number correctly predicted keyphrases number predicted keyphrases recall computed number predicted keyphrases total number data records note determining match keyphrases use porter stemmer processing results analysis conduct empirical study different tasks evaluate model predicting present keyphrases keyphrase extraction task prior studies analyze proposed model performs commonly dened task fair comparison sider present keyphrases evaluation task table provides performances baseline models proposed models e rnn copyrnn method table lists f measure dictions ve datasets best scores highlighted bold underlines indicate second best performances results unsupervised models tf idf texttank singlerank pandrank robust performance ferent datasets expandrank fails return result dataset high time complexity measures nus meval higher ones reported hasan ng kim et al probably utilized paper abstract instead text training method tf idf textrank singlerank expandrank maui kea rnn copyrnn inspec nus krapivin semeval n n table performance predicting present keyphrases models ve benchmark datasets lter noisy information mance supervised models e maui kea unstable datasets maui achieved best performances datasets baseline models proposed keyphrase prediction proaches rnn model attention anism perform expected rnn model cerned nding hidden semantics text tend generate keyphrases words general sarily refer source text addition serve keyphrases dataset contain vocabulary words rnn model able recall rnn model generate results words vocabulary indicates pure generative model t traction task need link language usage source text copyrnn model considering contextual information signicantly outperforms rnn model baselines ing best baselines erage result demonstrates importance source text extraction task nearly correct predictions contained vocabulary words example figure shows result predicted present keyphrases rnn rnn article video search models generate phrases relate topic information retrieval video rnn predictions high level minologies general selected keyphrases copyrnn hand predicts detailed phrases like video data integrated ranking interesting bad case rich content coordinates keyphrase video metadata copyrnn mistakenly puts prediction predicting absent keyphrases stated important motivation work interested proposed model s capability predicting absent keyphrases based understanding content worth noting prediction challenging task best knowledge existing methods handle task provide rnn copyrnn performances discussion results task evaluate performance recall results absent keyphrases correctly predicted use absent keyphrases testing datasets uation dataset inspec krapivin nus semeval rnn copyrnn table absent keyphrases prediction mance rnn copyrnn ve datasets table presents recall results predicted keyphrases rnn copyrnn models observe copyrnn average recall keyphrases predictions indicates extent models capture hidden semantics tual content reasonable predictions addition advantage features source text copyrnn model outperforms rnn model condition improvement present keyphrase extraction task example shown figure absent keyphrases video retrieval video ing correctly recalled models note term indexing appear text models detect information index videos rst sentence paraphrase target phrase copyrnn fully predicts keyphrases capturing detailed information text highlighted text segments transferring model news domain rnn copyrnn supervised models trained data specic domain writing style sufcient training large scale dataset expect models able learn universal language features effective corpora task test model type text model work transferred different environment use popular news article dataset wan xiao analysis dataset consists news articles manually annotated keyphrases result analysis shown table copyrnn extract portion rect keyphrases unfamiliar text port baseline performance included hasan ng performance copyrnn better textrank mihalcea tarau keycluster liu et al lags baselines worth noting hyperparameters baseline models number recalled keyphrases tf idf glerank carefully tuned drastically affect results copyrnn ply report score predicted phrases transferred corpus completely different type domain model encounters unknown words rely positional syntactic features experiment copyrnn recalls text keyphrases contain vocabulary words names persons places correctly predicted model tf idf textrank singlerank model expandrank keycluster copyrnn table keyphrase prediction performance copyrnn model trained scientic publication evaluated news discussion experimental results demonstrate copyrnn model performs dicting present keyphrases ity generate topically relevant keyphrases broader sense absent text model attempts map long text e paper stract representative short text chunks e keyphrases potentially applied improve information retrieval performance generating high quality index terms sisting user browsing summarizing long ments short readable phrases far tested model entic publications news articles demonstrated model ability ture universal language patterns extract key formation unfamiliar texts believe model greater potential ized domains types like books online reviews trained larger data pus directly applied model trained publication dataset ing keyphrases news articles tive training believe proper training news data model provement additionally work mainly studies lem discovering core content textual rials encoder decoder framework plied model language work extended locate core mation data resources ing content images videos figure example predicted keyphrase rnn copyrnn phrases shown bold correct predictions conclusions future work paper proposed rnn based erative model predicting keyphrases tic text best knowledge rst application encoder decoder model keyphrase prediction task model marizes phrases based deep semantic meaning text able handle rarely occurred phrases incorporating copying mechanism comprehensive empirical studies demonstrate effectiveness proposed model ing present absent keyphrases ent types text future work include following directions work evaluated mance proposed model conducting line experiments future terested comparing model human notators human judges evaluate quality predicted phrases current model fully consider correlation target keyphrases interesting explore multiple output optimization aspects model acknowledgments like thank jiatao gu miltiadis allamanis sharing source code ing helpful advice thank wei lu yong huang qikai cheng irlab members wuhan university assistance dataset development work partially supported national science foundation grant erratum mistakenly reported micro averaged scores models instead macro averaged ones updated scores macro averaged difference micro averaged macro averaged score marginal mistake nt affect conclusions drew mitted version sincerely apologize mistake thank wang chen nese university hong kong pointing references m allamanis h peng c sutton volutional attention network extreme rization source code arxiv e prints dzmitry bahdanau kyunghyun cho yoshua gio neural machine translation jointly arxiv preprint learning align translate gabor berend opinion expression mining ijcnlp exploiting keyphrase extraction seer pages kyunghyun cho bart van merrienboer caglar cehre dzmitry bahdanau fethi bougares holger learning schwenk yoshua bengio phrase representations rnn encoder decoder statistical machine translation arxiv preprint eibe frank gordon w paynter ian h witten carl gutwin craig g nevill manning domain specic keyphrase extraction felix gers e schmidhuber lstm rent networks learn simple context free sensitive languages ieee transactions neural networks sujatha das gollapalli cornelia caragea extracting keyphrases research papers ing citation networks eighth aaai conference articial ligence aaai press pages acm org citation proceedings maria grineva maxim grinev dmitry lizorkin extracting key terms noisy proceedings theme documents ternational conference world wide web acm new york ny usa www pages jiatao gu zhengdong lu hang li victor ok incorporating copying mechanism arxiv preprint li sequence sequence learning shuguang han daqing jiepu jiang zhen yue supporting exploratory people search study factor transparency user control ceedings acm international conference information knowledge management acm pages kazi saidul hasan vincent ng conundrums unsupervised keyphrase extraction making sense state art proceedings ternational conference computational tics posters association computational guistics pages sepp hochreiter jurgen schmidhuber neural computation long short term memory anette hulth improved automatic keyword traction given linguistic knowledge ceedings conference empirical ods natural language processing association computational linguistics pages anette hulth beata b megyesi study automatically extracted keywords text proceedings international rization conference computational linguistics annual meeting association tational linguistics association computational linguistics pages steve jones mark s staveley phrasier system interactive document retrieval keyphrases proceedings annual ternational acm sigir conference research development information retrieval acm pages daniel kelleher saturnino luz automatic proceedings hypertext keyphrase detection international joint conference articial intelligence morgan kaufmann publishers inc san francisco usa pages acm org citation su nam kim olena medelyan min yen kan timothy baldwin task tomatic keyphrase extraction scientic articles proceedings international workshop semantic evaluation association tional linguistics pages diederik kingma jimmy ba adam method stochastic optimization arxiv preprint mikalai krapivin aliaksandr autayeu izio marchese large dataset keyphrases extraction technical report disi trento italy tho thi ngoc le minh le nguyen akira mazu unsupervised keyphrase extraction introducing new kinds words keyphrases springer international publishing cham pages zhiyuan liu xinxiong chen yabin zheng maosong sun automatic keyphrase tion bridging vocabulary gap proceedings fifteenth conference computational natural language learning association computational linguistics pages zhiyuan liu wenyi huang yabin zheng maosong sun automatic keyphrase proceedings tion topic decomposition conference empirical methods ural language processing association tational linguistics pages zhiyuan liu peng li yabin zheng maosong sun clustering nd exemplar terms proceedings keyphrase extraction conference empirical methods natural guage processing volume volume association computational linguistics pages patrice lopez laurent romary humb automatic key term extraction scientic articles grobid international workshop semantic evaluation association computational linguistics burg pa usa semeval pages acm org citation proceedings sumit chopra marcaurelio ranzato michael auli wojciech zaremba sequence level ing recurrent neural networks iclr san juan puerto rico yutaka matsuo mitsuru ishizuka word extraction single document word co occurrence statistical information international journal articial intelligence tools ilya sutskever oriol vinyals quoc v le sequence sequence learning neural works advances neural information ing systems pages proceedings approach keyphrase takashi tomokiyo matthew hurst language model acl traction workshop multiword expressions volume sis acquisition treatment linguistics association stroudsburg pa usa mwe pages computational oriol vinyals ukasz kaiser terry koo slav petrov ilya sutskever geoffrey hinton mar foreign language advances neural information processing systems pages xiaojun wan jianguo xiao single ument keyphrase extraction neighborhood knowledge minmei wang bo zhao yihua huang ptr phrase based topical ranking automatic keyphrase extraction scientic publications springer international publishing cham pages ian h witten gordon w paynter eibe frank carl gutwin craig g nevill manning kea practical automatic keyphrase extraction ceedings fourth acm conference digital libraries acm pages wenyuan zeng wenjie luo sanja fidler raquel efcient summarization arxiv preprint urtasun read copy mechanism qi zhang yang wang yeyun gong xuanjing huang keyphrase extraction deep current neural networks twitter ings conference empirical ods natural language processing association computational linguistics austin texas pages org anthology yongzheng zhang nur zincir heywood los milios world wide web site tion web intelligence agent systems national journal olena medelyan eibe frank ian h witten human competitive tagging automatic proceedings keyphrase extraction conference empirical methods natural guage processing volume volume association computational linguistics pages olena medelyan eibe frank ian h witten human competitive tagging proceedings matic keyphrase extraction conference empirical methods natural language processing volume volume association computational linguistics burg pa usa emnlp pages acm org citation olena medelyan ian h witten david milne proceedings topic indexing wikipedia aaai wikiai workshop volume pages rada mihalcea paul tarau textrank ing order texts association computational linguistics thuy dung nguyen min yen kan keyphrase extraction scientic publications international conference asian digital braries springer pages meng rui han shuguang huang yun daqing brusilovsky peter knowledge based content ieee wic acm international conference web intelligence institute electrical ics engineers pages linking online textbooks alexander m rush sumit chopra jason ston neural attention model proceedings tive sentence summarization conference empirical methods ural language processing emnlp lisbon portugal september pages org anthology d pdf iulian v serban alessandro sordoni yoshua bengio aaron courville joelle pineau building end end dialogue systems generative archical neural network models proceedings aaai conference articial intelligence shiqi shen yong cheng zhongjun wei hua wu maosong sun yang liu mum risk training neural machine translation proceedings annual meeting association computational linguistics volume long papers association computational linguistics berlin germany pages aclweb org anthology min song il yeol song xiaohua hu exible information gain based kpspotter keyphrase extraction system proceedings acm international workshop web tion data management acm pages
