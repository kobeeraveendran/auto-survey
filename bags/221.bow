neural text summarization a critical evaluation wojciech krysci nski nitish shirish keskar bryan mccann caiming xiong richard socher salesforce research kryscinski nkeskar bmccann cxiong com g u a l c s c v v i x r a abstract text summarization aims at compressing long documents into a shorter form that conveys the most important parts of the original ment despite increased interest in the munity and notable research effort progress on benchmark datasets has stagnated we ically evaluate key ingredients of the current research setup datasets evaluation metrics and models and highlight three primary comings automatically collected datasets leave the task underconstrained and may tain noise detrimental to training and tion current evaluation protocol is weakly correlated with human judgment and does not account for important characteristics such as factual correctness models overt to layout biases of current datasets and offer limited versity in their outputs introduction text summarization aims at compressing long tual documents into a short human readable form that contains the most important information from the source two strategies of generating maries are extractive dorr et al nallapati et al where salient fragments of the source document are identied and directly copied into the summary and abstractive rush et al see et al where the salient parts are tected and paraphrased to form the nal output in network the number of summarization models duced every year has been increasing rapidly advancements neural tures sutskever et al bahdanau et al vinyals et al vaswani et al and the availability of large scale data sandhaus nallapati et al grusky et al enabled the transition from systems based on expert knowledge and heuristics to data driven approaches powered by end to end deep neural current approaches to text models rization utilize advanced attention and copying mechanisms see et al tan et al cohan et al multi task and multi reward training techniques guo et al pasunuru and bansal kryscinski et al forcement learning strategies paulus et al narayan et al dong et al wu and hu and hybrid extractive abstractive models liu et al hsu et al gehrmann et al chen and bansal many of the introduced models are trained on the cnn dailymail nallapati et al news corpus a popular benchmark for the eld and are evaluated based on n gram overlap between the generated and target summaries with the rouge package lin despite substantial research effort the progress on these benchmarks has stagnated state of art models only slightly outperform the baseline which generates summaries by ing the rst three sentences of the source ument we argue that this stagnation can be partially attributed to the current research setup which involves uncurated automatically collected datasets and non informative evaluations cols we critically evaluate our hypothesis and support our claims by analyzing three key nents of the experimental setting datasets tion metrics and model outputs our motivation is to shift the focus of the research community into developing a more robust research setup for text summarization related work datasets to accommodate the requirements of ern data driven approaches several large scale datasets have been proposed the majority of available corpora come from the news domain gaword graff and cieri is a set of cles and corresponding titles that was originally used for headline generation takase et al but it has also been adapted to single sentence summarization rush et al chopra et al nyt sandhaus is a collection of ticles from the new york times magazine with stracts written by library scientists it has been marily used for extractive summarization hong and nenkova li et al and importance prediction yang and nenkova nye and nenkova the cnn dailymail nallapati et al dataset consists of ticles with summaries composed of highlights from the article written by the authors themselves it is commonly used for both abstractive see et al paulus et al kryscinski et al and extractive dong et al wu and hu zhou et al neural tion the collection was originally introduced as a cloze style qa dataset by hermann et al xsum narayan et al is a lection of articles associated with one sentence summary targeted at abstractive models newsroom grusky et al is a diverse lection of articles sourced from major online news outlets this dataset was released together with a leaderboard and held out testing split outside of the news domain several datasets were collected from open discussion boards and other portals offering structure information dit tifu kim et al is a collection of posts scraped from reddit where users post their daily stories and each post is required to contain a too long did nt read summary how koupaee and wang is a collection of articles from the wikihow knowledge base where each article contains instructions for performing procedural multi step tasks covering various eas including arts nance travel and health evaluation metrics manual and semi automatic nenkova and sonneau passonneau et al evaluation of large scale summarization models is costly and cumbersome much effort has been made to velop automatic metrics that would allow for fast and cheap evaluation of models the rouge package lin offers a set of automatic metrics based on the lexical lap between candidate and reference summaries overlap can be computed between consecutive n grams and non consecutive skip grams sequences of tokens rouge scores are based on exact token matches meaning that computing overlap between synonymous phrases is not ported many approaches have extended rouge with support for synonyms and paraphrasing val zhou et al uses a three step son strategy where the rst two steps perform timal and greedy paraphrase matching based on paraphrase tables before reverting to exact token overlap rouge we ng and abrecht places exact lexical matches with a soft tic similarity measure approximated with the sine distances between distributed representations of tokens rouge ganesan leverages synonym dictionaries such as wordnet and siders all synonyms of matched words when puting token overlap rouge g shaeibavani et al combines lexical and semantic ing by applying graph analysis algorithms to the wordnet semantic network despite being a step in the direction of a more comprehensive ation protocol none of these metrics gained cient traction in the research community ing rouge as the default automatic evaluation toolkit for text summarization models existing summarization models fall into three egories abstractive extractive and hybrid extractive models select spans of text from the input and copy them directly into the mary non neural approaches neto et al dorr et al filippova and altun menares et al utilized domain expertise to develop heuristics for summary content tion whereas more recent neural techniques low for end to end training in the most common case models are trained as or level classiers that predict whether a fragment should be included in the summary nallapati et al narayan et al liu et al xu and durrett other approaches apply reinforcement learning training strategies to directly optimize the model on task specic differentiable reward functions narayan et al dong et al wu and hu abstractive models paraphrase the source uments and create summaries with novel phrases not present in the source document a mon approach in abstractive summarization is to use attention and copying mechanisms see et al tan et al cohan et al other approaches include using multi task and reward training paulus et al jiang and bansal guo et al pasunuru and bansal kryscinski et al and pervised training strategies chu and liu schumann hybrid models hsu et al liu et al gehrmann et al chen and bansal include both extractive and abstractive modules and allow to separate the summarization process into two phases content selection and paraphrasing for the sake of brevity we do not describe tails of different models we refer interested ers to the original papers analysis and critique most summarization research revolves around new architectures and training strategies that prove the state of the art on benchmark problems however it is also important to analyze and tion the current methods and research settings zhang et al conducted a quantitative study of the level of abstraction in abstractive summarization models and showed that level copy only extractive models achieve parable results to fully abstractive models in the measured dimension kedzie et al offered a thorough analysis of how neural models perform content selection across different data domains and exposed data biases that dominate the ing signal in the news domain and architectural limitations of current approaches in learning bust sentence level representations liu and liu examine the correlation between rouge scores and human judgments when evaluating meeting summarization data and show that the correlation strength is low but can be improved by leveraging unique meeting characteristics such as available speaker information owczarzak et al inspect how inconsistencies in man annotator judgments affect the ranking of summaries and correlations with automatic uation metrics the results showed that level rankings considering all summaries were stable despite inconsistencies in judgments ever summary level rankings and automatic ric correlations benet from improving annotator consistency graham compare the tness of the bleu metric papineni et al and a number of different rouge variants for ing summarization outputs the study reveals perior variants of rouge that are different from the commonly used recommendations and shows that the bleu metric achieves strong correlations with human assessments of generated summaries schulman et al study the problems related to using rouge as an evaluation metric with respect to nding optimal solutions and provide proof of np hardness of global optimization with respect to rouge similar lines of research where the authors put under scrutiny existing methodologies datasets or models were conducted by callison burch et al tan et al post in chine translation gkatzia and mahamood reiter and belz reiter in ral language generation lee et al chen et al kaushik and lipton in ing comprehension gururangan et al liak et al glockner et al in ral language inference goyal et al in sual question answering and xian et al in zero shot image classication comments on the general state of scholarship in the eld of chine learning were presented by sculley et al lipton and steinhardt and ences therein datasets underconstrained task the task of summarization is to compress long documents by identifying and extracting the most important information from the source documents however assessing the importance of information is a difcult task in itself that highly depends on the expectations and prior knowledge of the target reader we show that the current setting in which els are simply given a document with one ated reference summary and no additional mation leaves the task of summarization constrained and thus too ambiguous to be solved by end to end models to quantify this effect we conducted a human study which measured the agreement between ferent annotators in selecting important sentences article the glowing blue letters that once lit the bronx from above yankee stadium failed to nd a buyer at an auction at sotheby s on wednesday while the letters were expected to bring in anywhere from to the only person who raised a paddle for was a sotheby s employee trying to jump start the bidding the current owner of the signage is yankee hall of famer reggie jackson who purchased the feet tall letters for an undisclosed amount after the stadium saw its nal game in no love letters that hung over yankee stadium were estimated to bring in anywhere from to but received no bids at a sotheby s auction wednesday the year old yankee said he wanted a new generation to own and enjoy this icon of the yankees and of new york city the letters had beamed from atop yankee stadium near grand concourse in the bronx since the year before jackson joined the team summary questions when was the auction at sotheby who is the owner of the signage when had the letters been installed on the stadium glowing letters that had been hanging above the yankee dium from to were placed for auction at sotheby s on wednesday but were not sold the current owner of the sign is reggie jackson a yankee hall of famer constrained summary a unconstrained summary a constrained summary b unconstrained summary b an auction for the lights from yankee stadium failed to duce any bids on wednesday at sotheby s the lights rently owned by former yankees player reggie jackson lit the stadium from until there was not a single buyer at the auction at sotheby s on wednesday for the glowing blue letters that once lit the bronx s yankee stadium not a single non employee raised their paddle to bid jackson the owner of the letters was prised by the lack of results the venue is also auctioning off other items like mets memorabilia the once iconic and attractive pack of letters that was placed at the yankee stadium in and later removed in was unexpectedly not favorably considered at the sotheby s auction when the year old owner of the letters attempted to transfer its ownership to a member the younger populace thus when the minimum estimate of was not met a further attempt was made by a former player of the yankees to personally visit the new owner as an table example summaries collected from human annotators in the constrained left and unconstrained right task in the unconstrained setting annotators were given a news article and asked to write a summary covering the parts they considered most important in the constrained setting annotators were given a news article with three associated questions and asked to write a summary that contained the answers to the given questions from a fragment of text we asked workers to write summaries of news articles and highlight tences from the source documents that they based their summaries on the experiment was ducted in two settings unconstrained where the annotators were instructed to summarize the tent that they considered most important and strained where annotators were instructed to write summaries that would contain answers to three questions associated with each article this is ilar to the construction of the tac ion summarization task the questions ated with each article where collected from human workers through a separate assignment ments were conducted on randomly sampled articles further details of the human study can be found in appendix a table shows the average number of sentences per article that annotators agreed were important nist summarization op summ guidelines html the rows show how the average changes with the human vote threshold needed to reach consensus about the importance of any sentence for ple if we require that three or more human votes are necessary to consider a sentence important notators agreed on average on the importance of and sentences per article in the constrained and constrained settings respectively the average length in sentences of sampled cles was with a standard deviation of the study demonstrates the difculty and ity of content selection in text summarization we also conducted a qualitative study of maries written by annotators examples ing summaries written in the constrained and constrained setting are shown in table we ticed that in both cases the annotators correctly identied the main topic and important fragments of the source article however constrained maries were more succinct and targeted out sacricing the natural ow of sentences human vote threshold sent per article considered important unconstrained constrained table average number of sentences per article which annotators agreed were important the human vote threshold investigates how the average agreement changes with the threshold of human votes required to consider any sentence important rows and correspond to the set intersection and union of selected sentences accordingly constrained writers tended to write more verbose summaries that did not add information the study also highlights the abstractive nature of human written summaries in that similar content can be described in unique ways layout bias in news data figure the distribution of important sentences over the length of the article according to human annotators blue and its cumulative distribution red news articles adhere to a writing structure known in journalism as the inverted mid purdueowl in this form initial paragraphs contain the most newsworthy tion which is followed by details and background information to quantify how strongly articles in the cnn dm corpus follow this pattern we conducted a human study that measured the importance of different sections of the article annotators read news articles and selected sentences they found most important experiments were conducted on randomly sampled articles further details of the human study are described in appendix a figure presents how annotator selections were distributed over the length of the article the distribution is skewed towards the rst quarter of the length of articles the cumulative plot shows that nearly of the important information was present in the rst third of the article and mately and of selections pointing to the second and last third respectively it has become standard practice to exploit such biases during training to increase performance of models see et al paulus et al kryscinski et al gehrmann et al jiang and bansal pasunuru and bansal but the importance of these heuristics has been accepted without being quantied these same heuristics would not apply to books or legal documents which lack the inverted pyramid out so common in the news domain so it is tant that these heuristics be part of ablation ies rather than accepted as default pre processing step noise in scraped datasets given the data requirements of deep neural works and the vast amounts of diverse resources available online automatically scraping web tent is a convenient way of collecting data for new corpora however adapting scraped content to the needs of end to end models is problematic given that manual inspection of data is infeasible and man annotators are expensive data curation is ally limited to removing any markup structure and applying simple heuristics to discard obviously awed examples this in turn makes the quality of the datasets heavily dependent on how well the scraped content adheres to the assumptions made by the authors about its underlying structure this issue suggests that available tion datasets would be lled with noisy examples manual inspection of the data particularly the erence summaries revealed easily detectable sistent patterns of awed examples many such amples can be isolated using simple regular pressions and heuristics which allows mation of how widespread these aws are in the dataset we investigated this issue in two large marization corpora scraped from the internet cnn dm links to other articles michael carrick has helped manchester united win their last six games carrick should be selected alongside gary cahill for england carrick has been overlooked too many times by his country read carrick and man united team mates enjoy second christmas party newsroom links to news sources the latest breaking news get washington dc virginia maryland and national featuring national news get security read news headlines from the nation and from the washington post visit www washingtonpost com nation today science and courts table examples of noisy reference summaries found in the cnn dm and newsroom datasets article quick thinking brady olson a teacher at north thurston high took down a gunman on monday a washington high school teacher is being hailed a hero for tackling a old student to the ground after he opened re on monday morning summary factually incorrect brady olson a washington high school teacher at north thurston high opened re on monday morning no one was injured after the boy shot twice toward the ceiling in the school commons before classes began at north thurston high school in lacey table example of a factually incorrect summary generated by an abstractive model top ground truth article bottom summary generated by model cnn dm nallapati et al and the room grusky et al the problem of noisy data affects and of the ing validation and test split of the cnn dm dataset and and of the spective splits of the newsroom dataset ples of noisy summaries are shown in table flawed examples contained links to other cles and news sources placeholder texts unparsed html code and non informative passages in the reference summaries evaluation metrics weak correlation with human judgment the effectiveness of rouge was previously uated lin graham through cal correlations with human judgment on the duc datasets over and yen ever their setting was substantially different from the current environment in which summarization models are developed and evaluated to investigate the robustness of rouge in the setting in which it is currently used we evaluate how its scores correlate with the judgment of an average english speaker using examples from the cnn dm dataset following the human ation protocol from gehrmann et al we asked annotators to rate summaries across four mensions relevance selection of important tent from the source consistency factual ment between the summary and the source ency quality of individual sentences and ence collective quality of all sentences each summary was rated by distinct judges with the nal score obtained by averaging the individual scores experiments were conducted on domly sampled articles with the outputs of summarization systems provided by the original authors correlations were computed between all pairs of rouge scores for all tems additional summaries were collected from annotators to inspect the effect of using multiple ground truth labels on the correlation with matic metrics further details of the human study can be found in appendix a results are shown in table the left section of the table presents pearson s correlation cients and the right section presents kendall rank correlation coefcients in terms of pearsons s efcients the study showed minimal correlation with any of the annotated dimensions for both stractive and extractive models together and for abstractive models individually weak tion was discovered for extractive models ily with the uency and coherence dimensions we hypothesized that the noise contained in the ne grained scores generated by both human notators and rouge might have affected the relation scores we evaluated the relation on a higher level of granularity by means of correlation between rankings of models that were obtained from the ne grained scores the study showed weak correlation with all measured dimensions when evaluated for both abstractive and extractive models together and for abstractive models vidually moderate correlation was found for tractive models across all dimensions a ing result was that correlations grew weaker with the increase of ground truth references our results align with the observations from liu and liu who also evaluated rouge side of its original setting the study highlights the limited utility in measuring progress of the eld reference r l pearson correlation references r l references r l reference r l kendall rank correlation references r l references r l relevance consistency fluency coherence relevance consistency fluency coherence relevance consistency fluency coherence all models abstractive models extractive models table correlations between human annotators and rouge scores along different dimensions and multiple reference set sizes left pearson s correlation coefcients right kendall s rank correlation coefcients solely by means of rouge scores insufcient evaluation protocol the goal of text summarization is to cally generate succinct uent relevant and tually consistent summaries the current tion protocol depends primarily on the exact cal overlap between reference and candidate maries measured by rouge in certain cases rouge scores are complemented with human studies where annotators rate the relevance and uency of generated summaries neither of the methods explicitly examines the factual tency of summaries leaving this important sion unchecked to evaluate the factual consistency of existing models we manually inspected randomly pled articles with summaries coming from domly chosen abstractive models we focused exclusively on factual incorrectness and ignored any other issues such as low uency out of article summary pairs that were reviewed ally we found that contained tency issues table shows examples of covered inconsistencies some of the discovered inconsistencies despite being factually incorrect could be rationalized by humans however in many cases the errors were substantial and could have severe repercussions if presented as is to get readers models layout bias in news data we revisit the problem of layout bias in news data from the perspective of models kedzie et al showed that in the case of news articles the layout bias dominates the learning signal for neural models in this section we approximate the degree with which generated summaries rely on the leading sentences of news articles we computed rouge scores for collected models in two settings rst using the cnn dm reference summaries as the ground truth and ond where the leading three sentences of the source article were used as the ground truth i e the baseline we present the results in ble for all examined models we noticed a tial increase of overlap across all rouge variants results suggest that performance of current els is strongly affected by the layout bias of news corpora is a strong baseline that exploits the described layout bias however there is still a large gap between its performance and an upper bound for extractive models extractive oracle diversity of model outputs models analyzed in this paper are considerably different from each other in terms of architectures training strategies and underlying approaches we inspected how the diversity in approaches translates into the diversity of model outputs we computed and scores between pairs of model outputs to compare them by means of token and phrase overlap results are visualized in figure where the values above and below the diagonal are and scores accordingly and model names follow the der from table extractive oracle grusky et al baseline target reference r l reference abstractive models extractive models r l model hsu et al model gehrmann et al model jiang and bansal model chen and bansal model see et al model kryscinski et al model li et al model pasunuru and bansal model zhang et al model guo et al model dong et al model wu and hu model zhou et al table rouge scores computed for different models on the test set of the cnn dm dataset left scores computed with the original reference summaries right scores computed with used as the reference we notice that the scores vary siderably less than scores this gests that the models share a large part of the cabulary on the token level but differ on how they organize the tokens into longer phrases comparing results with the n gram overlap tween models and reference summaries table shows a substantially higher overlap between any model pair than between the models and reference summaries this might imply that the training data contains easy to pick up patterns that all models overt to or that the information in the training signal is too weak to connect the content of the source articles with the reference summaries conclusions this critique has highlighted the weak points of the current research setup in text summarization we showed that text summarization datasets quire additional constraints to have well formed summaries current state of the art methods learn to rely too heavily on layout bias associated with the particular domain of the text being rized and the current evaluation protocol reects human judgments only weakly while also failing to evaluate critical features e factual ness of text summarization we hope that this critique provides the rization community with practical insights for ture research directions that include the tion of datasets models less t to a particular figure pairwise similarities between model outputs computed using rouge above diagonal unigram overlap below diagonal gram overlap model order follows table main bias and evaluation that goes beyond current metrics to capture the most important features of summarization acknowledgements we thank all the authors listed in table for ing their model outputs and thus contributing to this work we also thank shaq rayhan joty for reviewing this manuscript and providing valuable feedback references dzmitry bahdanau kyunghyun cho and yoshua gio neural machine translation by jointly learning to align and translate in iclr chris callison burch cameron s fordyce philipp koehn christof monz and josh schroeder evaluation of machine translation in pages association for putational linguistics chris callison burch miles osborne and philipp koehn re evaluation the role of bleu in chine translation research in eacl the tion for computer linguistics danqi chen jason bolton and christopher d ning the cnn daily mail reading comprehension task in acl the association for computer linguistics a thorough examination of yen chun chen and mohit bansal fast tive summarization with reinforce selected sentence rewriting in acl pages association for computational linguistics sumit chopra michael auli and alexander m rush abstractive sentence summarization with in naacl hlt tentive recurrent neural networks the conference of the north american chapter of the association for computational guistics human language technologies san diego california usa june eric chu and peter j liu unsupervised neural multi document abstractive summarization corr arman cohan franck dernoncourt doo soon kim trung bui seokhwan kim walter chang and nazli goharian a discourse aware attention model for abstractive summarization of long documents in proceedings of the conference of the north american chapter of the association for tional linguistics human language technologies naacl hlt new orleans louisiana usa june volume short papers carlos a colmenares marina litvak amin mantrach and fabrizio silvestri heads headline eration as sequence prediction using an abstract feature rich space in hlt naacl pages yue dong yikang shen eric crawford herke van hoof and jackie chi kit cheung banditsum extractive summarization as a contextual bandit in proceedings of the conference on empirical methods in natural language processing brussels belgium october november bonnie dorr david zajic and richard schwartz hedge trimmer a parse and trim approach to line generation in hlt naacl katja filippova and yasemin altun ing the lack of parallel data in sentence compression in proceedings of emnlp pages seer kavita ganesan rouge updated and improved measures for evaluation of summarization tasks corr sebastian gehrmann yuntian deng and alexander m rush bottom up abstractive summarization in emnlp pages association for putational linguistics dimitra gkatzia and saad mahamood a in shot of nlg evaluation practices enlg proceedings of the european workshop on natural language generation september university of brighton brighton uk pages max glockner vered shwartz and yoav goldberg breaking nli systems with sentences that quire simple lexical inferences in acl pages association for computational tics yash goyal tejas khot douglas summers stay dhruv batra and devi parikh making the v in vqa matter elevating the role of image derstanding in visual question answering in cvpr pages ieee computer society david graff and c cieri english gigaword guistic data consortium yvette graham re evaluating automatic marization with bleu and shades of rouge in emnlp pages the association for computational linguistics max grusky mor naaman and yoav artzi newsroom a dataset of million summaries with diverse extractive strategies in proceedings of the conference of the north american chapter of the association for computational linguistics human language technologies naacl hlt new orleans louisiana usa june ume long papers han guo ramakanth pasunuru and mohit bansal soft layer specic multi task summarization in with entailment and question generation ceedings of the annual meeting of the tion for computational linguistics acl bourne australia july volume long papers suchin gururangan swabha swayamdipta omer levy roy schwartz samuel r bowman and noah a smith annotation artifacts in in naacl hlt ural language inference data pages association for computational guistics karl moritz hermann tomas kocisky edward grefenstette lasse espeholt will kay mustafa leyman and phil blunsom teaching chines to read and comprehend in nips kai hong and ani nenkova improving the estimation of word importance for news in proceedings of the document summarization conference of the european chapter of the association for computational linguistics eacl april gothenburg sweden wan ting hsu chieh kai lin ming ying lee kerui min jing tang and min sun a unied model for extractive and abstractive summarization using inconsistency loss in proceedings of the annual meeting of the association for tional linguistics acl melbourne australia july volume long papers yichen jiang and mohit bansal closed book training to improve summarization encoder memory in emnlp pages association for putational linguistics divyansh kaushik and zachary c lipton how much reading does reading comprehension require a critical investigation of popular benchmarks in emnlp pages association for tational linguistics chris kedzie kathleen r mckeown and hal daume iii content selection in deep learning models in emnlp pages of summarization association for computational linguistics byeongchang kim hyunwoo kim and gunhee kim abstractive summarization of reddit posts with multi level memory networks corr mahnaz koupaee and william yang wang ihow a large scale text summarization dataset corr wojciech kryscinski romain paulus caiming xiong and richard socher improving abstraction in text summarization in emnlp pages association for computational linguistics moontae lee xiaodong he wen tau yih jianfeng gao li deng and paul smolensky ing in vector space an exploratory study of tion answering in iclr junyi jessy li kapil thadani and amanda stent the role of discourse units in near extractive in proceedings of the sigdial summarization conference the annual meeting of the special interest group on discourse and dialogue september los angeles ca usa wei li xinyan xiao yajuan lyu and yuanzhuo improving neural abstractive wang ment summarization with structural regularization in emnlp pages association for putational linguistics chin yew lin rouge a package for automatic evaluation of summaries in proc acl workshop on text summarization branches out page zachary c lipton and jacob steinhardt bling trends in machine learning scholarship acm queue feifan liu and yang liu exploring correlation between rouge and human evaluation on meeting summaries ieee trans audio speech language processing jingyun liu jackie chi kit cheung and annie louis what comes next extractive corr marization by next sentence prediction peter j liu mohammad saleh etienne pot ben goodrich ryan sepassi lukasz kaiser and noam shazeer generating wikipedia by ing long sequences in international conference on learning representations iclr ver bc canada april may ence track proceedings ramesh nallapati feifei zhai and bowen zhou summarunner a recurrent neural network based quence model for extractive summarization of ments in aaai ramesh nallapati bowen zhou c aglar gulcehre bing xiang al abstractive text marization using sequence to sequence rnns and yond proceedings of signll conference on putational natural language learning ramesh nallapati bowen zhou and mingbo ma classify or select neural architectures for extractive document summarization corr shashi narayan shay b cohen and mirella lapata do nt give me the details just the summary topic aware convolutional neural networks for in proceedings of the treme summarization conference on empirical methods in natural guage processing brussels belgium shashi narayan shay b cohen and mirella lapata ranking sentences for extractive in rization with reinforcement learning hlt pages association for tional linguistics shashi narayan nikos papasarantopoulos mirella pata and shay b cohen neural tive summarization with side information corr ani nenkova and rebecca j passonneau ating content selection in summarization the mid method in human language technology ference of the north american chapter of the ciation for computational linguistics hlt naacl boston massachusetts usa may joel larocca neto alex a freitas and celso aa kaestner automatic text summarization ing a machine learning approach in brazilian posium on articial intelligence pages springer jun ping ng and viktoria abrecht better marization evaluation with word embeddings for rouge corr benjamin nye and ani nenkova identication and characterization of newsworthy verbs in world news in naacl hlt the conference of the north american chapter of the association for computational linguistics human language nologies denver colorado usa may june paul over and james yen an introduction to intrinsic evaluation of generic news text summarization systems paul over and james yen an introduction to intrinsic evaluation of generic news text summarization systems paul over and james yen an introduction to intrinsic evaluation of generic news text summarization systems karolina owczarzak peter a rankel hoa trang dang and john m conroy assessing the fect of inconsistent assessors on summarization uation in acl pages the association for computer linguistics kishore papineni salim roukos todd ward and jing zhu bleu a method for automatic uation of machine translation in acl pages acl rebecca j passonneau emily chen weiwei guo and dolores perin automated pyramid scoring of in acl summaries using distributional semantics pages the association for computer linguistics ramakanth pasunuru and mohit bansal reward reinforced summarization with saliency and entailment corr romain paulus caiming xiong and richard socher a deep reinforced model for abstractive marization in iclr adam poliak jason naradowsky aparajita haldar rachel rudinger and benjamin van durme hypothesis only baselines in natural language ence in hlt pages sociation for computational linguistics matt post a call for clarity in reporting bleu in wmt pages association for scores computational linguistics purdueowl journalism and journalistic ing the inverted pyramid structure accessed ehud reiter a structured review of the validity of bleu computational linguistics ehud reiter and anja belz an investigation into the validity of some metrics for automatically ating natural language generation systems tational linguistics alexander m rush sumit chopra and jason weston a neural attention model for abstractive tence summarization proceedings of emnlp evan sandhaus the new york times annotated corpus linguistic data consortium philadelphia john schulman nicolas heess theophane weber and pieter abbeel gradient estimation using stochastic computation graphs in nips raphael schumann unsupervised tive sentence summarization using length controlled variational autoencoder corr d sculley jasper snoek alexander b wiltschko and ali rahimi winner s curse on pace progress and empirical rigor in iclr workshop openreview net abigail see peter j liu and christopher d manning get to the point summarization with generator networks in acl elaheh shaeibavani mohammad ebrahimi mond k wong and fang chen a theoretic summary evaluation for rouge in emnlp pages association for computational guistics ilya sutskever oriol vinyals and quoc v le sequence to sequence learning with neural works in nips sho takase jun suzuki naoaki okazaki tsutomu rao and masaaki nagata neural headline generation on abstract meaning representation in proceedings of the conference on empirical methods in natural language processing emnlp austin texas usa november jiwei tan xiaojun wan and jianguo xiao abstractive document summarization with a in proceedings based attentional neural model of the annual meeting of the association for computational linguistics acl vancouver canada july august volume long papers liling tan jon dehdari and josef van genabith an awkward disparity between bleu ribes scores and human judgements in machine in wat pages workshop on asian tion translation ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan n gomez lukasz kaiser and illia polosukhin attention is all you need in advances in neural information cessing systems annual conference on neural information processing systems ber long beach ca usa pages oriol vinyals meire fortunato and navdeep jaitly pointer networks in nips yuxiang wu and baotian hu learning to extract coherent summary via deep reinforcement in proceedings of the thirty second aaai ing conference on articial intelligence the innovative applications of articial ligence and the aaai symposium on educational advances in articial intelligence new orleans louisiana usa february yongqin xian bernt schiele and zeynep akata zero shot learning the good the bad and the ugly in cvpr pages ieee computer ety jiacheng xu and greg durrett neural tive text summarization with syntactic compression corr yinfei yang and ani nenkova detecting information dense texts in multiple news domains in proceedings of the twenty eighth aaai ence on articial intelligence july quebec city quebec canada fangfang zhang jin ge yao and rui yan on the abstractiveness of neural document in emnlp pages association for tion computational linguistics liang zhou chin yew lin dragos stefan munteanu and eduard h hovy paraeval using phrases to evaluate summaries automatically in man language technology conference of the north american chapter of the association of tational linguistics proceedings june new york new york usa qingyu zhou nan yang furu wei shaohan huang ming zhou and tiejun zhao neural ment summarization by jointly learning to score and select sentences in proceedings of the annual meeting of the association for computational guistics acl melbourne australia july volume long papers a human study details reference summary a layout bias in news data human annotators were asked to read news ticles and highlight the sentences that contained the most important information the study was conducted on randomly sampled articles with each article annotated by unique annotators human studies were conducted through the zon mechanical turk platform prices of tasks were carefully calculated to ensure that workers would have an average compensation of per hour in all studies examples were sampled from the test split of the cnn dm dataset that contains a total of examples as with any human study there is a trade off between the number of examples annotated the breadth of the experiments and the quality of notations studies conducted for this paper were calibrated to primarily assure high quality of sults and the breadth of experiments a underconstrained task human annotators were asked to write summaries of news articles and highlight fragments of the source documents that they found useful for ing their summary the study was conducted on randomly sampled articles with each article annotated by unique annotators the same guration and articles were used in both the strained and unconstrained setting questions for the constrained setting were ten by human annotators in a separate assignment and curated before being used for to collect maries a rouge weak correlation with human judgment this study evaluated the quality of summaries erated by different neural models tive and extractive a list of evaluated models is available in table the study was conducted on randomly pled articles with each article annotated by unique annotators given the large number of evaluated models the experiment was split into groups two groups contained models one group contained models to prevent from lecting biased data models were assigned to periment groups on a per example basis thus domizing the context in which each model was evaluated to establish a common reference point between groups the reference summaries from the dataset were added to the pool of annotated els however annotators were not informed which of this fact the order in which summaries were displayed in the annotation interface was ized with the rst position always reserved for the
