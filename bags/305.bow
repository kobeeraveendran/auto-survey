automatic text summarization medical research articles bert bowen tan laboratory molecular genetics rockefeller university new york edu virapat kieuvongngam laboratory membrane biology biophysics rockefeller university new york edu yiming niu laboratory molecular neurobiology biophysics rockefeller university new york edu abstract pandemic growing urgency medical community accelerating growth new coronavirus related literature result open research dataset challenge released corpus scholarly articles calling machine learning approaches help bridging gap researchers rapidly growing publications advantage recent advances pre trained nlp models bert openai solve challenge performing text summarization dataset evaluate results rouge scores visual inspection model provides abstractive comprehensive information based keywords extracted original articles work help medical community providing succinct summaries articles abstract available introduction open research dataset chanllenge global health research community need way survey scientic literature come treatment measures response challenge white house leading research groups established open research dataset bring nlp expertise help nding answer literature bringing insights public large wang dataset consists scholarly articles including text related diseases text summarization automatic text summarization active area research focusing condensing large piece text smaller text retaining relevant information general approaches extractive summarization aiming extracting concatenating important span source text designed conducted pre processing quantitative assessment proposed idea designed experiment implemented prototype model trained model conducted quantitative assessment preprint work progress akin skimming text second approach focus generating new summaries paraphrase source text extractive approach shown maintain reasonable degree grammaticality accuracy contrary abstractive approach challenging fact model able represent semantic information source text use semantic representation generate paraphrase model gain ability creative use words ability inference source text existing body work approaches summarization progressed considerably thanks recent advances availability large pre trained nlp models use attention mechanism vaswani models include bidirectional encoder representations transformers bert devlin recently openai radford trained large dataset text entire corpus wikipedia able perform diverse nlp tasks including machine translation question answering multiple choice question text classication early text summarization models uses pre trained bert bertsum liu lapata bertsum extractive modied variant bert model trained general news cnn daily news summarization dataset model performs binary classication task predict sentence included summary bert built perform language generative task use abstractive summarization limited past years sequence sequence models based transformer decoder architecture widely abstractive summarization shi architecture point view encoder reads source text transform hidden states decoder takes hidden states output summary text mapping hidden representation output text gives architecture language generative capability recently unied text text framework train large language model multiple nlp tasks raffel basic idea train single model map input text tasks output text work similar spirit tuned pre trained perform mapping selected keywords summary text generating summary abstractly low resource challenge additional challenge task low availability domain specic corpus unlike general summarization like cnn daily mail dataset document summary pairs related literature april contain approximately text abstract pairs scientic terminology found peer reviewed literature esoteric mainstream text pre training performed low resource present considerable impediment tuning framework found useful expand approach overall outline project subdivided parts unsupervised extractive baseline performance novel abstractive unsupervised extractive summarization takes pre trained bert model perform sentence embedding individual sentence transformed high dimensional representation subsequently medoid clustering analysis performed high dimensional representation miller representing semantic centers text cluster centers selected extracted summary comparing extractive summarization abstractive summarization trained generate summary set keywords keywords extracted source text existing token classication tools nltk speech tagging packages tuned bert token classier speech tagging keywords tokens classied different groups verbs nouns verbs nouns following extraction keywords paired generated abstract gold summary abstract keyword summary pairs processed fed model illustrated gure training summary results generated stochastic sampling method described section results compared qualitative assess reading inspection quantitatively generated results compared gold summary rouge model architecture state art nlp models built transformer architecture vaswani relying attention mechanism convert input sequences output sequences kinds transformer architectures widely transformer encoder transformer decoder bert model unsupervised extractive summarization pre trained transformer encoder model sanh model attention heads transformer encoder layers output dimensional hidden state model use pytorch based distilbert implemented huggingface transformer wolf gpu resource constraint abstractive summarization model pre trained distil version token length attention heads transformer decoder layers use pytorch version implemented huggingface transformer package wolf training strategy abstractive summarization model trained tasks language modeling task multiple choice prediction task task model predicts word token given previous tokens context task given set keywords model choose correct gold summary summary choices tasks associated loss task projects hidden state word embedding ouput layer cross entropy loss applied target corresponding gold summary loss training label start end text special tokens enable model recognize summarization task special token separate keywords gold summary input padded padding token tokens input longer tokens truncated task hidden state token passed linear layer class likelihood score classication task cross entropy loss applied obtain loss create training dataset randomly select summaries unrelated keywords called distractors paired distractors keywords similar manners gold summary forming batch input items language modeling training labels token summary right shifted token auto regressive nature nth token output generated previous token inputs left multiple choice training label tensor numeric indicating ith item correct keyword gold summary pair total loss weighted sum losses ratio loss loss intuition training strategy intuition training strategy following model aims text generation designed auto regressive model takes backward context previous tokens predict nth token achieved masked self attention mechanism block information tokens right current position bert token classication adapted depends definition com named entity recognition training dataset kaggle entity annotated corpus calculated special token signify context subsequent tokens summary information special token model learn context clue tuning multi loss training hope able induce model map local semantic context keywords gold summary time model retains global contextual information keywords end text model able distinguish gold summary distractors multi loss training prominent recent language model training aims general language understanding raffel figure overview multi loss training example input shown items input true abstract distractors true abstract shown item beginning sequence end sequence padding summarization token denoted bos eos pad respectively language modeling label contains masked token masked gold summary tokens multiple choice label tensor indicates item correct keyword summary pair experiments results model training training carried google colab equiped nvidia tesla gpu total epochs performed training dataset consists training samples sample multiple choice options validation dataset consists samples multiple choices training parameters include learning rate batch size gradient accumulation steps linearly decreasing learning rate scheduler epoch training loss rst epoch shown gure validation loss loss indicating sign overtting visualizing results attention mechanism allows assess model performance attention broadly interpreted vector importance weights strongly tokens sequences correlated tokens vig visualize attention input sequence illustrated table plot attention matrix alignment gure visualize learned attention model comparing attention pre trained figure training results losses shown epoch training iterations language model loss blue shown exponentiated form cross entropy loss called perplexity score elmloss multiple choice loss orange calculated cross entropy loss multiple choices tuning tuning multi layer multi attention head architecture total unique structures select attentions exhibit interesting phenomena obvious pattern exihibited layer head tuning diagonal pattern interpreted representing self correlation observed strongly tuning second pattern left shifted diagonal line shown layer head layer head interpreted correlation keyword input summary attention learned tuning strongly epochs tuning indicates training strategy works expected thirdly vertical line pattern observed attention layer head tuning interpreted long range correlation phrases sentences sequence important maintaining coherence grammarticality sequence generation language modeling output tensor size sequence length vocab size tensor likelihood distribution words softmax generate text sequence output sample words distribution word word manner obtain ith word consider conditional probability previous words firstly sampling apply scaling factor called temperature likelihood reshape skews likelihood distribution softmax holtzman high temperature tends skews distribution favor low probability words low temperature skews distribution high probability words result tug war favoring generation accuracy cost decreasing word diversity secondly employs stochastic sampling method called sampling probability distribution words pick conditional previous words sequence rule sampling smallest set candidate words consider cumulative conditional probability greater additionally prevent model sampling low probability words limit number candidate words consider words empirically tested sampling parameters found temperature yields reasonable generations gold summary generated abstractive summary table example summary result keyword input epoch training inuenza virus quently reported viral cause rhabdomyolysis old child presented rhabdomyolysis associated parainuenza type virus cases rhabdomyolysis ciated parainuenza virus reported cations include electrolyte disturbances acute renal failure compartment syndrome rhabdomyolysis associated parainuenza virusinuenza virus cause rhabdomyolysis child domyolysis parainuenza type virus cases rhabdomyolysis parainuenza virus plications include electrolyte turbances renal failure ment syndrome epoch training rhabdomyolysis associated parainuenza virusinuenza virus cause rhabdomyolysis child domyolysis parainuenza type virus cases rhabdomyolysis parainuenza virus plications include electrolyte turbances renal failure partment syndrome inuenza virus mon cause respiratory domyolysis child believed ysis parainuenza type virus cases sis parainuenza type virus recent cases ysis parainuenza virus described complications include electrolyte disturbances kidney failure nal compartment syndrome rhabdomyolysis inuenza virus leading cause child cases domyolysis parainuenza virus reported plications include electrolyte turbances renal failure normal renal compartment drome analysis result table noteworthy model learns inuenza virus common cause rhabdomyolysis breakdown muscle tissue knowledge presented keyword respect model gains ability infer knowledge training corpus problematic wrongly infers falsehood example model outputs complication include gastrointestinal conpartment syndrome fact compartment syndrome gastrointestinal condition note attributed sampling method figure visualizing attention mechanism weights attentions layers mapping input output shown input sequence shown table passed pre trained model summarization tuned model axis represents input sequence axis represents aligned output notation denotes start summarization end token respectively keyword summary sequences labeled gure compares attention tuning selected attention layers heads plotted matrix heatmaps quantitative assessment rouge metric rouge recall oriented understudy gisting evaluation metric nlp evaluating text summarization metric compare model generated summary human generated reference summary rouge measures overlap grams texts rouge measures longest matching sequences words predened gram length require consecutive matches rouge measures longest matching sequences consecutive matching account lin recall version rouge reports ratio grams reference present generated summary precision version rouge reports ratio grams generated summary present reference summary score version rouge harmonic mean precision rouge recall rouge report use score version rouge extractive summarization applied kmeans clustering followed nearest neighbour extract sentences representing comprehensive semantic meanings abstracts extraction compared effects versus compression ratios rouge scores shown gure rouge scores extraction higher extraction irrespective training epochs additionally extractive summary produces reuslts higher rouge scores compared abstractive figure consistent assumption compression extraction preserves information compared original abstracts abstractive summarization abstractive summarization stage effect training time rstly investigated shown figure plotted weights attentions layers mapping input output epochs suggested concentrated distribution words model benet signicantly longer training process reected better relevance input output furthermore output summary shown table illustrates longer training help generate abstract summary compared gold summary model epoch training summarize special cases cases mixed infections reported explaining specic case model epoch training tries describe interestingly seemingly abstract results reected calculated rouge score shown figure difference rouge scores model epochs insignicant investigate effect keywords sentence generation compared keywords nouns verbs yielded versus extraction figure overall rouge scores abstractive group higher group indicates keywords input tokens result coverage information compared original abstracts generator compared different word classes inuence generated summaries compared original ones shown figure rst observation abstraction tend lower rouge scores group irrespective word classes second verbs keywords shows low rouge scores nouns tend similar rouge scores compared group verbs nouns suggest nouns generally weighted verbs summary generation nouns representing accurate information original abstracts convey exclude possibility advantage nouns larger percentage keywords nouns tend verbs sentences gure evaluate different word sampling methods greedy search versus sampling inuence results rouge scores groups similar cases greedy search group shows slightly higher scores readability abstractive meanings signicantly worse greedy search group compared group figure summary experiments hyper parameters comparison rouge scores model epochs training effect nouns verbs keywords percentages represent fraction specic category words included effect greedy search approach stands greedy search respectively conclusion future work abstractive summarization represents standing challenge deep learning nlp task applied domain specic corpus different pre training highly technical contains low training materials open research dataset challenge exemplify abovementioned difculty illustrated text text multi loss training strategy tune pre trained language model perform abstractive summarization result interpretable reasonable near human level performance think model benet training new coronavirus related research publication available model accurate ability infer conclusion keywords retrospect think keyword generation phrase exible signicantly improved investment keywords taken nouns verbs present text investigated delity gained adding information adjective keyword data augmentation performed randomly dropping adding words keyword sets effect create summary pairs existing ones finally think strategy extracting keywords experimented example tune token classication model selectively extract scientic keywords evaluation process abstractive summaries requires exploration data rouge scores represent direct information phrases overlapping actual meanings readability summaries clearly extractive models yield higher rouge scores abstractive ones intuitively extractions raw sentences result higher similarity approach favored better summarization fact generated abstractive summaries showing good readability succinct information coverage reected rouge scores strongly suggest scores evaluation systems aspects needed future possible idea combining regularization penalize local similarity reward global similarity finally think approach leveraged intensive computation resources available overall implementation limited computation power tesla gpu training version batch size memory limit gpu likely result greatly benet bigger version training permitted available computation power end hope text summarization approach help medical research community rapidly growing literature helps bring new insight ght pandemic code availability source codes models implemented study publicly available github com references dzmitry bahdanau kyunghyun cho yoshua bengio neural machine translation jointly learning align translate jacob devlin ming wei chang kenton lee kristina toutanova bert pre training deep bidirectional transformers language understanding ari holtzman jan buys maxwell forbes yejin choi curious case neural text degeneration jianquan xiaokang liu wenpeng yin min yang liqun empirical evaluation multi task learning deep neural networks natural language processing chin yew lin rouge package automatic evaluation summaries acl yang liu mirella lapata text summarization pretrained encoders derek miller leveraging bert extractive text summarization lectures alec radford jeffrey rewon child david luan dario amodei ilya sutskever language models unsupervised multitask learners url cloudfront net better language models language models pdf colin raffel noam shazeer adam roberts katherine lee sharan narang michael matena yanqi zhou wei peter liu exploring limits transfer learning unied text text transformer victor sanh lysandre debut julien chaumond thomas wolf distilbert distilled version bert smaller faster cheaper lighter tian shi yaser keneshloo naren ramakrishnan chandan reddy neural abstractive text summarization sequence sequence models survey ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan gomez lukasz kaiser illia polosukhin attention need jesse vig multiscale visualization attention transformer model arxiv preprint url org lucy wang kyle yoganand chandrasekhar russell reas jiangjiang yang darrin eide kathryn funk rodney kinney ziyang liu william merrill paul mooney dewey murdick devvret rishi jerry sheehan zhihong shen brandon stilson alex wade kuansan wang chris wilhelm boya xie douglas raymond daniel weld oren etzioni sebastian kohlmeier open research dataset thomas wolf lysandre debut victor sanh julien chaumond clement delangue anthony moi pierric cistac tim rault rmi louf morgan funtowicz jamie brew huggingface transformers state art natural language processing appendix generated sample gold summary publisher summary demyelination component viral diseases humans best known subacute sclerosing panencephalitis sspe progressive multifocal leukoencephalopathy pml number naturally occurring virus infections animals involve demyelination serve instructive models human demyelinating diseases addition naturally occurring diseases viruses shown capable producing demyelination experimental situations discussing virus associated demyelinating disease chapter reviews architecture functional organization cns considers known interaction viruses cns cells discusses immunology cns differs important aspects rest body experimental models viral induced demyelination considered viruses capable producing demyelinating disease common taxonomic features include dna rna viruses enveloped nonenveloped viruses chapter attempts summarize important factors inuencing viral demyelination common features possible mechanisms abstractive summary abstract demyelination component diseases humans experiencing sclerosing panencephalitis sspe leukoencephalopathy pml common causes demyelination animal human diseases virus infections animals involve demyelination situations demyelinating disease chapter reviews architecture cns considers interactions viruses cns different aspects rest body models demyelination explored viruses example identied important virus infections humans abstractive summary abstract demyelination important component diseases human suffering sclerosing panencephalitis sspe leukoencephalopathy pml common causes demyelination clinical conditions considered potential cause demyelination clinical conditions clinical epidemiological factors inuence demyelination clinical situations demyelination clinical situations result combination viral cellular factors viral rna viruses viruses chapter reviews architecture organization cns considers potential potential cause demyelination clinical situations discuss immunology cns generated sample gold summary rnai pathway antiviral defence mechanism employed insects result degradation viral rna curbing infection viruses including aviviruses encode viral suppressors rnai vsrs counteract antiviral rnai pathway till date vsr reported alphaviruses present study undertaken evaluate chikungunya virus chikv proteins rnai suppressor activity systematically analyzed chikv proteins rnai suppressor activity rnai sensor cell line based assay non structural proteins found exhibit rnai suppressor activity validated ndings natural hosts aedes mammalian cell lines emsa agrobacterium inltration gfp silenced transgenic tobacco plants domains responsible maximum rnai suppressor activity identied proteins rna binding motifs domains identied participation rnai suppression evaluated site directed mutagenesis sequence alignment motifs species known alphaviruses revealed conservation motifs emphasizing similar role action species alphaviruses validation rnai suppressor activity proteins awaits establishment specic virus infection models abstractive summary abstract chikungunya virus chikv proteins highly conserved highly conserved host cell line activity rnai understood evaluated chikungunya virus chikv proteins non structural rnai suppressor activity analyzed chikv proteins non structural rnai suppressor activity analyzed chikv proteins structural rnai suppressor activity rnai sensor cell line assay proteins found exhibit rnai activity ourndings host aedes cell lines emsa agrobacterium inltration gfp tobacco plants domains rnai suppressor activity consistent observed rnai suppression mutagenesis sequence alignment motifs species alphaviruses revealed conserved conservation motifs role action species abstractive summary abstract chikungunya virus chikv proteins novel promising rnai mediated antiviral protein antiviral activity chikungunya virus chikv proteins understood study able evaluate chikungunya virus chikv proteins novel rnai mediated antiviral protein detected chikv proteins novel rnai mediated antiviral protein suppress chikungunya virus infection targeting rnai sensor cell line assay proteins able demonstrate chikv proteins novel rnai mediated antiviral protein suppress chikungunya virus infection targeting rnai sensor cell line assay proteins
