noisy self knowledge distillation for text summarization yang liu sheng shen and mirella lapata university of edinburgh yang ac uk ed ac uk university of california berkeley sheng edu p e s l c s c v v i x r a abstract in this paper we apply self knowledge tion to text summarization which we argue can alleviate problems with maximum likelihood training on single reference and noisy datasets instead of relying on one hot annotation labels our student summarization model is trained with guidance from a teacher which generates smoothed labels to help regularize training furthermore to better model uncertainty ing training we introduce multiple noise nals for both teacher and student models we demonstrate experimentally on three marks that our framework boosts the mance of both pretrained and non pretrained summarizers achieving state of the art results introduction automatic summarization has enjoyed renewed thanks to the interest in recent years ity of neural network models and their ability to learn continuous representations without recourse to preprocessing tools or linguistic annotations the availability of large scale datasets sandhaus hermann et al grusky et al narayan et al containing hundreds of sands of document summary pairs has driven the development of neural architectures for rization several approaches have been proposed in the vast majority sequence to sequence models which are trained in an end to end fashion with a maximum likelihood estimation loss see et al celikyilmaz et al paulus et al gehrmann et al despite promising results the are specic acteristics of the summarization task which der it ill suited to standard sequence to sequence training for instance maximum likelihood ing on single reference datasets might not be timal for summarization which is subject to a great deal of human variation harman and over in the context of nenkova tive summarization different people select ent sentences to include in a summary rath et al and when writing abstracts disagreement exists both in terms of writing style and the specic content deemed important for the mary harman and over although marization models would naturally benet from multiple target references it is unrealistic to pect that multi reference summarization datasets can be created at scale for neural network ing in fact most popular benchmarks are collated opportunistically based on summaries which only loosely correspond to the source input for example narayan et al create a dataset by pairing the rst sentence of a news ticle with the rest of the document under the sumption that the introductory sentence expresses the gist of the article grusky et al pair ticles with metadata available in html pages der the assumption that html tags e tion summary like content in other work liu et al perez beltrachini et al multidocument summarization datasets are ated by viewing lead sections in wikipedia articles as summaries of documents cited therein the herent noise in the data collection process further hampers training with models often being prone to hallucination song et al maynez et al and struggling to identify which content units are salient tan et al in this paper we propose to alleviate these problems by turning to knowledge tion bucilu et al ba and caruana hinton et al kim and rush edge distillation transfers knowledge from a larger teacher network to a smaller student model by training the student to imitate the teacher s outputs in addition to learning from the training data set in born again networks furlanello et al the teacher and student have the same neural chitecture and model size and yet surprisingly the student is able to surpass the teacher s accuracy intuitively self knowledge distillation is effective because the teacher s output distribution provides a richer training signal capturing additional mation about training examples in the context of summarization the teacher can benet student training in two ways it provides a softened tribution over reference summaries thereby ing the single reference setting moreover the teacher s distribution is to a certain extent noised enabling the student to circumvent curacies in the training data we further talize on the idea that both the teacher and the student should be robust to noise and introduce several noise injection techniques which together with knowledge distillation improve model alization and performance covering we present experiments on several narayan et al marization benchmarks hermann et al perez beltrachini et al and multi document summarization settings as well as different types of summaries e verbose or more telegraphic across datasets we the proposed framework boosts the performance of pretrained and pretrained abstractive summarizers achieving new state of the art results background neural abstractive summarization neural approaches to abstractive summarization conceptualize the task as a sequence to sequence problem where the encoder maps the sequence of tokens in the source document xn to a sequence of continuous representations and the decoder autoregressively erates the target summary my ym token by token hence modeling the conditional probability xn rush et al and nallapati et al were among the rst to apply the neural encoder decoder architecture to text tion see et al enhance this model with a pointer generator network which allows to copy words from the source text and a coverage anism which keeps track of words that have been summarized other work develops tive models trained end to end with reinforcement learning based on multiple encoders and cal attention celikyilmaz et al or a erage mechanism where the decoder attends over previously generated words paulus et al gehrmann et al follow a bottom up proach where a content selector rst determines which phrases in a source document should be part of the summary and a copy mechanism is applied only to preselected phrases during decoding though the majority of summarization systems are composed of lstm units narayan et al and perez beltrachini et al propose stractive models based on convolutional neural works in gains abstractive pretrained language models have recently emerged as a key technology for achieving impressive tion liu and lapata lewis et al song et al these models rst pretrain a language model with self supervised tives on large corpora and then ne tune it liu and lapata on summarization datasets combine a pretrained encoder based on bert devlin et al with a randomly demonstrating substantial initialized decoder gains on summarization performance song et al pretrain an encoder decoder framework fragments within a to reconstruct sentence and then ne tune it on summarization datasets in the same vein lewis et al present bart an encoder decoder transformer vaswani et al pretrained by ing a text corrupted with several arbitrary noising functions bao et al design a transformer based neural network pretrained as a pseudo masked language model masked knowledge distillation knowledge distillation refers to a class of ods for training a new smaller student network by learning from a teacher network in addition to learning from the training data it is generally sumed that the teacher has been previously trained and the parameters for the student are estimated by matching the student s predictions to the teacher let t and s denote teacher and student models respectively also let ft and fs be functions of the teacher and student the models are typically neural networks and function can be in ple dened using the output of any network layer e a hidden or softmax layer knowledge tillation methods are commonly expressed as imizing an objective function over training set x lkd x xix xi where l is a loss function that penalizes the ference between the teacher and the student logits based hidden ba and caruana output on representations specic instantiations of this general work include minimizing the teacher student difference mediate attention maps and derivatives of the loss to the romero et al put zagoruyko and komodakis czarnecki et al other work integrates an ensemble of teachers in order to improve the student urban et al trains a succession of students furlanello et al introduces a teacher assistant for better knowledge transfer mirzadeh et al and regularizes parisotto et al teh et al task agents in reinforcement learning compared to direct training knowledge distillation provides a more stable training process which leads to better performing student models hinton et al phuong and lampert recent work furlanello et al hahn and choi also sheds light on leveraging knowledge distillation for training a high performing student model with the same size as the teacher see the discussion in the next section knowledge distillation has been also shown to improve results for various nlp tasks tan et al use it to transfer knowledge from bert to smaller models helping them approach or the quality of much larger pretrained neural networks aside from distilling large models into smaller ones kim and rush mou et al or ensembles of models into single els kuncoro et al liu et al edge distillation has been further used in task learning e to teach a multi task student from single task teachers clark et al self knowledge distillation for text summarization self knowledge distillation refers to the cial case where the teacher and student have identical neural network architectures prisingly perhaps it has been consistently served furlanello et al yang et al ahn et al that students trained with knowledge distillation outperform their teachers by signicant margins in several computer vision and language modeling tasks recent efforts have also focused on understanding why this happens e by observing that knowledge transferred by the teacher is localized mainly in higher layers and does not affect early feature extraction ers much gotmare et al by interpreting the teacher s knowledge as importance weighting furlanello et al by showing that stopping is crucial dong et al and by studying how self distillation modies tion mobahi et al for text summarization we argue that knowledge distillation can potentially alleviate problems in conventional maximum likelihood summarization models are typically training trained on single reference document summary pairs however considering a single summary as the only correct reference during maximum likelihood training can harm model ization elbayad et al and is there can be multiple valid intuitive harman and over maries for a source input nenkova and even the single ence summaries available are not entirely dard due to the inherent noise in the automatic construction of large scale summarization datasets kryscinski et al with self knowledge tillation teacher outputs provide softened tions of the reference summaries which can be viewed as an enrichment of the single reference setting and a reweighting of gold summaries to vent the student from becoming over condent in its predictions the standard objective for an abstractive marization model is negative log likelihood lnll t x where indicates the source document yt cates the t token in the target summary and are the rst t tokens in the target summary we further assume that the teacher is a fully trained neural model the student has the same ture with the teacher and access to the learned teacher s output distribution pt x lkd t x where pt and are model outputs from the teacher and student spectively where the input is usually a long document we design the following perturbation policies it is common practice to compensate for no rect access to the training data see equation by interpolating between the two losses in tions and so the nal objective for ing the student becomes lfinal where is a mixture parameter combining the hot distribution and the teacher distribution we further want our summarization systems to be robust to natural noise found in existing datasets injecting noise onto training samples has been proven useful for improving model ization et al we extend this idea for knowledge distillation and propose a novel work for introducing noise to both distillation nals and training data we design different noise mechanisms for the teacher and student and select the best noise conguration experimentally noisy teacher to inject noise into the lation signals we incorporate a teacher dropout mechanism et al where dropout is kept active while generating teacher predictions for training the student in this manner the teacher generates variable supervision labels for the dent with some degree of uncertainty alleviating the problem of overtting to the teacher tions meanwhile it can also be considered as proximating an average ensemble from many ral networks et al with dropout rate the knowledge distillation loss now becomes lkd t ps t x word drop a word in the source input is moved with probability pd word replacement for each word xi in the source input we calculate a candidate placement list by selecting k words most ilar to xi from the vocabulary the ity is calculated as the cosine distance tween the embedding of xi and embeddings of all other words in the vocabulary then a source word is replaced with a word domly selected from its candidate ment list with probability pr sentence drop a sentence in the source input is removed with probability ps gaussian noise a gaussian noise vector e is multiplied with the embeddings of input words e e n i these perturbation policies can be applied multaneously or successively as a pipeline we experimentally found the best combination for our task to be the sequential application of word drop followed by word replacement and sentence drop although gaussian noise has been effective in natural language understanding tasks zhang and yang we found it not to be helfpul in our summarization experiments the knowledge distillation loss with a student trained on noisy data becomes lkd t x t x where x indicates perturbed source input where p teacher model with active dropout t indicates the predictions from the experimental setup noisy student to inject noise into the ing data we propose various mechanisms to turb the source input random perturbation is effective in enforcing local smoothness for ing text generation models under the assumption that semantically similar inputs can be mapped to the same or similar targets a related approach has been shown to improve the performance of machine translation models in self training tings he et al for text summarization in this section we describe the summarization datasets used in our experiments and discuss ious implementation details summarization datasets on two evaluated our model we document summarization datasets namely the cnn dailymail news highlights hermann et al and xsum narayan et al and one multi document dataset i e wikicatsum perez beltrachini et al summarization without pretraining lead ptrnet transformerabs skd skd noisy t skd noisy t noisy s base size pretrained models massbase bertsumabs m m m m skd skd noisy t m skd noisy t noisy s m large size pretrained models unilmlarge bartlarge m m cnn dailymail rl rl rl xsum rl rl rl table rouge results on cnn dailymail and xsum test sets and are shorthands for unigram and bigram overlap rl is the longest common subsequence skd refers to a system trained with self knowledge distillation noisy t are skd models trained with noisy signals while noisy s are student models trained on noisy data results for comparison systems are taken from the authors respective papers or obtained on our data by running publicly released software represent different these datasets summary styles ranging from highlights to very brief one sentence summaries the summaries also vary with respect to the type of rewriting operations they exemplify e cnn dailymail showcases more cut and paste operations while xsum is genuinely abstractive two of these datasets xsum and wikicatsum were created assumptions automatically following various about the correspondence of purported summaries to the source input finally cnn dailymail contains news articles and sociated highlights i e a few bullet points ten by journalists which give a brief overview of the article we used the standard splits training of hermann et al tion and testing cnn ments and dailymail uments we did not anonymize entities tences were split with the stanford corenlp toolkit manning et al and the dataset was pre processed following see et al input documents were truncated to tokens for question what is this article about we used the splits of narayan et al for training tion and testing and lowed the pre processing introduced in their work input documents were also truncated to kens wikicatsum is a multi document tion dataset derived from wikisum liu et al the target summary is the lead tion of a wikipedia article and the source put are webpages related to this article catsum perez beltrachini et al represents three domains from the original wikisum dataset under the assumption that these vary in terms of the topics the summaries discuss and their tic characteristics aside from the summaries the dataset contains the input webpages whose length is truncated to the rst tokens contains samples for the company domain samples for the film domain and samples for the animal domain implementation details xsum contains news articles nied with a one sentence summary answering the for all datasets we evaluated our self knowledge in the distillation framework in two settings all animal without pretraining company rl cv cv tf skd skd noisy t skd noisy t noisy s rl rl rl rl rl skd skd noisy t skd noisy t noisy s film rl rl with pretraining table rouge results on wikicatsum test sets and are shorthands for unigram and bigram overlap rl is the longest common subsequence results are reported separately on three domains and in combination all skd refers to systems trained with self knowledge distillation noisy t are skd systems trained with noisy signals and noisy s are skd students trained on noisy data results for comparison systems are taken from the authors respective papers or obtained on our data by running publicly released software rst setting our models are non pretrained while in the second setting we take advantage of trained language models which have strated impressive improvements in tion lewis et al liu and lapata bao et al specically we adopt bao et al as the pretrained model is a transformer based neural network vaswani et al with transformer layers and attention heads it is pretrained as a pseudo masked guage model on a large corpus label smoothing is applied with smoothing factor we tuned our teacher models following the procedure in bao et al in the non pretrained setting we adopt a transformer encoder decoder model with layers hidden size and forward lter size label smoothing was also used with smoothing factor all teacher models in this setting were trained from randomly initialized parameters following liu and lapata in all knowledge distillation experiments dent models have the same neural network tecture with their teachers and are trained with the same hyperparameters as the teacher models the best teacher and student model are selected by evaluating perplexity on the development set for noisy distillation models word drop probability pd was set to the candidate length k for word replacement was and word replacement ability pr was sentence drop probability ps was during decoding we used beam search size and tuned for the length penalty wu et al between and on the validation set we decode until an end of sequence token is ted repeated trigrams are blocked paulus et al results automatic evaluation we evaluated summarization quality automatically using rouge lin we report unigram and bigram overlap and as a means of assessing informativeness and the longest common subsequence rouge l as a means of assessing uency examples of system output are shown in table table summarizes our results on the cnn dailymail and xsum single document datasets the rst block includes the results of non pretrained models we present the lead baseline which simply selects the rst three tences in a document for cnn dailymail and the rst sentence for we also report the results of see et al s pointer ator network ptrnet and an abstractive tem from liu and lapata based on formers transformerabs see section for tails the latter forms the backbone of our knowledge distillation models skd we present a variant without noise skd a variant with noise in the teacher training signal noisy t models transformerabs noisy skd noisy skd cnn dailymail xsum table factual correctness on cnn dailymail and xsum test set noisy skd are students trained on noisy signals and noisy data and a third variant where the student is ally trained on noisy data noisy s the second and third blocks in table include the results of pretrained models to make isons fairer we separate second block from base size third block pretrained models based on parameter size shown within ses with regard to large size models we port the results of three very strong summarization systems netuned with unilmlarge bao et al bartlarge lewis et al and raffel et al our base size models include bertsumbase liu and lapata a summarizer based on a base size bert encoder and a randomly initialized decoder massbase song et al and unilmbase which are both netuned with base size trained models as can be seen in table skd improves over teacher models in both pretrained size and non pretrained settings we also observe that injection of noise brings further improvements with noise in the training signal noisy t ing more effective compared to noisy data tation noisy s overall we obtain competitive results with skd and base size pretrained els and even manage to outperform unilmlarge and on the cnn dailymail dataset table presents experimental results on the kicatsum dataset the rst block in the table includes results for non pretrained models and cv perez beltrachini et al are convolutional encoder decoder models the former is a standard convolutional decoder while the latter adopts a hierarchical convolutional coder which rst generates target sentence tors and then generates target words based on sentence vectors tf is a standard former encoder decoder model trained on catsum perez beltrachini et al tf is the model used in our skd system and its noisy version noisy t noisy s the second block includes the results of a system using the size pretrained model unilmbase on its own and with skd results are reported per domain pany film and animal and across domains all under pretrained and non pretrained settings we skd boosts the performance of the teacher model unilmbase and tf respectively and that the injection of noise is benecial provements in performance vary across domains with film showing the least gains column all in table shows average rouge across domains although skd and noise injection improve sults we observe that non pretrained models et more factual consistency evaluation we also use besides rouge factcc kryscinski et al to evaluate the factual correctness of the generated factcc is a bert based classier maries trained to identify conicts between a source document and a generated summary given a document sentence pair as input it assigns a information mentioned positive label in a summary sentence is consistent with the document otherwise it assigns a negative label we view the percentage of positive labels assigned by factcc to all generated summaries as a factual correctness score for a summarization system if factual we performed experiments with the publicly released version of factcc our results on the cnn dailymail and xsum datasets are presented in table here we only focus on document summarization as there is no version of factcc trained on multi document datasets as can be seen the application of skd trained with noisy signals and on noisy data improves tual consistency for non pretrained and pretrained models on both datasets all noisy skd dents are signicantly p more factually correct compared to their teachers abs and using a paired student t test human evaluation in addition to automatic evaluation we also sessed system output by eliciting human ments we compared the quality of the summaries produced by a teacher model against its distilled student noisy skd for com salesforce factcc cnn daily mail gold lz granderson millennials say they ll marry if and when they want he says that s not the case they re happily single and happy granderson says marriage is about family not money lz granderson millennials say they do nt care what their generation thinks about marriage he says they ll get married if and when they want lz marriage is linked to economic well being but it s not clear if that s true noisy skd carol costello talk to any millennial and you can envision an america virtually marriage free in countries like sweden or denmark people do nt feel pressured to marry even if they have kids together xsum more than half of pupils in wales have passed their gcse exam for the third year running gold more than of pupils in wales have achieved the top grades in their gcse exams noisy skd two thirds of welsh pupils who took gcses got a to c grades according to this year s results animal gold the conception bank silver boa chilabothrus argentum is a species of boa described in may it is only known from the conception island bank in the bahamas it is the rst known discovery of a west indian boa species in years it is named for its unique silver color the conception bank silver boa chilabothrus argentum is a species of snake in the family boidae it is demic to the bahamas the species was discovered on conception island bank which comprises uninhabited islets noisy skd the conception bank silver boa chilabothrus argentum is a species of nonvenomous boa endemic to the bahamas it was discovered in on conception island bank an uninhabited islet in the bahamas table gold reference summaries and automatic summaries produced by and its distilled student on the cnn dailymail xsum and datasets cnn dailymail noisy skd succinct inform fluent xsum noisy skd succinct inform fluent noisy skd company film animal table human evaluation on cnn dailymail xsum and wikicatsum test sets noisy skd is trained with self knowledge distillation on noisy signals and noisy data all pairwise ences between systems are signicant p using a paired t test cnn dailymail and xsum human participants were presented with the output of two systems and the original document and asked to cide which one was better according to the lowing criteria succinctness does the summary avoid repetition informativeness does the mary capture the document s most important formation and fluency is the summary uent and grammatical evaluation was conducted on the amazon mechanical turk crowdsourcing platform we used the same test documents in total from liu and lapata for both cnn dailymail and xsum we elicited ve sponses per hit systems were rated along each dimension and assigned a score corresponding to the proportion of times a system was selected as better against another human evaluation results are shown in table upper part on both datasets participants ceive the student noisy skd as signicantly p more succinct and informative pared to the teacher however on fluency the student tends to be worse upon spection we found student summaries to be rather telegraphic and hypothesize that crowdworkers tend to penalize them in terms of uency even though they are grammatical human evaluation was performed slightly ferent for recall that this is a document dataset where input documents are continuous webpage fragments to allow pants to perform the experiment in a timely ion we used the gold summary as a proxy for the content of the input crowdworkers were presented with the output of two systems again and noisy skd and asked to decide which one was better according to the formation contained in the gold summary uation was conducted on amt we randomly lected samples from the test set and elicited three responses per hit for each domain we port the proportion of times a system was chosen as better human evaluation results are shown in table lower part amt crowdworkers prefer the maries produced by the student for the animal and film domains but not for company we found that the distilled model tends to generate too many tities in one sentence which render the summaries too dense for this domain conclusions in this paper we advocated the use of knowledge distillation for abstractive tion as a means to alleviate problems associated with maximum likelihood training for this task we also introduced several noise functions in the training signal and training data which help larize training and further boost performance periments on three benchmark datasets strate that our framework can improve both pretrained and pretrained summarizers in the ture we would like to investigate more thoroughly which aspects of pretrained models improve and how self knowledge distillation can be enhanced with more sophisticated noise functions references sungsoo ahn shell xu hu andreas c damianou neil d lawrence and zhenwen dai ational information distillation for knowledge in the ieee conference on computer vision fer and pattern recognition cvpr pages long beach california chine learning pages new york new york asli celikyilmaz antoine bosselut xiaodong he and yejin choi deep communicating agents for in proceedings of the abstractive summarization conference of the north american chapter of the association for computational linguistics man language technologies volume long pers pages new orleans louisiana kevin clark minh thang luong urvashi wal christopher d manning and quoc v le bam born again multi task networks for natural language understanding in proceedings of the annual meeting of the association for tional linguistics pages florence italy association for computational linguistics wojciech m czarnecki simon osindero max berg grzegorz swirszcz and razvan pascanu sobolev training for neural networks in i guyon u v luxburg s bengio h wallach r fergus s vishwanathan and r garnett editors advances in neural information processing systems pages curran associates inc jacob devlin ming wei chang kenton lee and kristina toutanova bert pre training of deep bidirectional transformers for language in proceedings of the conference standing of the north american chapter of the association for computational linguistics human language technologies volume long and short papers pages minneapolis minnesota ation for computational linguistics bin dong jikai hou yiping lu and zhihua zhang distillation early stopping harvesting dark knowledge utilizing anisotropic information in neurips workshop on machine trieval learning with guarantees vancouver canada jimmy ba and rich caruana do deep nets in advances in neural ally need to be deep formation processing systems pages curran associates inc maha elbayad laurent besacier and jakob verbeek token level and sequence level loss arxiv preprint ing for rnn language models hangbo bao li dong furu wei wenhui wang nan yang xiaodong liu yu wang songhao piao feng gao ming zhou and hsiao wuen hon pseudo masked language models for ed language model pre training arxiv preprint cristian bucilu rich caruana and alexandru niculescu mizil model compression in ceedings of the acm sigkdd international conference on knowledge discovery and data ing kdd page new york ny usa association for computing machinery samuel rota lorenzo porzi and peter in kontschieder dropout distillation ceedings of the international conference on tommaso furlanello zachary lipton michael nen laurent itti and anima anandkumar born again neural networks in international ference on machine learning pages stockholm sweden sebastian gehrmann yuntian deng and alexander rush bottom up abstractive summarization in proceedings of the conference on cal methods in natural language processing pages brussels belgium akhilesh gotmare nitish shirish keskar caiming xiong and richard socher a closer look at deep learning heuristics learning rate restarts warmup and distillation in international ference on learning representations iclr new orleans la usa may view net max grusky mor naaman and yoav artzi newsroom a dataset of million summaries with diverse extractive strategies in proceedings of the conference of the north american chapter of the association for computational linguistics man language technologies volume long pers pages new orleans louisiana sociation for computational linguistics sangchul hahn and heeyoul choi knowledge distillation in natural language ing in proceedings of the international conference on recent advances in natural language ing ranlp pages varna bulgaria incoma ltd donna harman and paul over the effects of human variation in duc summarization in text summarization branches out pages tion barcelona spain association for tional linguistics junxian he jiatao gu jiajun shen and marcaurelio revisiting self training for arxiv preprint sequence generation ranzato neural karl moritz hermann tomas kocisky edward stette lasse espeholt will kay mustafa suleyman and phil blunsom teaching machines to read and comprehend in c cortes n d lawrence d d lee m sugiyama and r garnett editors advances in neural information processing systems pages curran associates inc geoffrey hinton oriol vinyals and jeff dean distilling the knowledge in a neural network arxiv preprint yoon kim and alexander m rush level knowledge distillation in proceedings of the conference on empirical methods in ral language processing pages austin texas association for computational linguistics wojciech kryscinski nitish shirish keskar bryan cann caiming xiong and richard socher neural text summarization a critical evaluation arxiv preprint adhiguna kuncoro miguel ballesteros lingpeng kong chris dyer and noah a smith ing an ensemble of greedy dependency parsers into one mst parser in proceedings of the ference on empirical methods in natural language processing pages austin texas ciation for computational linguistics and comprehension in proceedings of the nual meeting of the association for computational linguistics pages online association for computational linguistics chin yew lin rouge a package for in text matic evaluation of summaries rization branches out proceedings of the workshop pages barcelona spain tion for computational linguistics peter j liu mohammad saleh etienne pot ben goodrich ryan sepassi lukasz kaiser and noam shazeer generating wikipedia by ing long sequences in proceedings of the national conference on learning representations vancouver canada xiaodong liu pengcheng he weizhu chen and feng gao multi task deep neural networks for natural language understanding in proceedings of the annual meeting of the association for putational linguistics pages florence italy association for computational linguistics yang liu and mirella lapata text in proceedings of tion with pretrained encoders the conference on empirical methods in ural language processing and the international joint conference on natural language processing emnlp ijcnlp pages hong kong china association for computational linguistics christopher manning mihai surdeanu john bauer jenny finkel steven bethard and david mcclosky the stanford corenlp natural language in proceedings of annual cessing toolkit meeting of the association for computational guistics system demonstrations pages timore maryland association for computational linguistics joshua maynez shashi narayan bernd bohnet and ryan mcdonald on faithfulness and ality in abstractive summarization in proceedings of the annual meeting of the association for computational linguistics pages line association for computational linguistics seyed iman mirzadeh mehrdad farajtabar ang li and hassan ghasemzadeh improved edge distillation via teacher assistant bridging the gap between student and teacher arxiv preprint hossein mobahi mehrdad farajtabar and peter l bartlett bartlett self distillation es regularization in hilbert space arxiv preprint mike lewis yinhan liu naman goyal jan ghazvininejad abdelrahman mohamed omer levy veselin stoyanov and luke zettlemoyer bart denoising sequence to sequence training for natural language generation translation lili mou ran jia yan xu ge li lu zhang and zhi jin distilling word embeddings an ing approach proceedings of the acm national on conference on information and edge management pages ramesh nallapati bowen zhou cicero dos santos caglar gulcehre and bing xiang tive text summarization using sequence to sequence in proceedings of the rnns and beyond signll conference on computational natural guage learning pages berlin germany association for computational linguistics shashi narayan shay b cohen and mirella lapata do nt give me the details just the summary topic aware convolutional neural networks for in proceedings of the treme summarization conference on empirical methods in natural guage processing pages brussels gium association for computational linguistics ani nenkova summarization evaluation for text and speech issues and approaches in ninth tional conference on spoken language processing emilio parisotto jimmy ba and ruslan salakhutdinov actor mimic deep multitask and transfer inforcement learning corr romain paulus caiming xiong and richard socher a deep reinforced model for abstractive marization in proceedings of the international conference on learning representations ver canada laura perez beltrachini yang liu and mirella lapata generating summaries with topic templates and structured convolutional decoders in ings of the annual meeting of the association for computational linguistics pages florence italy association for computational guistics mary phuong and christoph lampert towards in understanding knowledge distillation ings of the international conference on chine learning pages long beach ifornia colin raffel noam shazeer adam roberts katherine lee sharan narang michael matena yanqi zhou wei li and peter j liu exploring the limits of transfer learning with a unied text to text former arxiv e prints gj rath a resnick and tr savage the tion of abstracts by the selection of sentences part i sentence selection by men and machines american documentation adriana romero nicolas ballas samira ebrahimi hou antoine chassang carlo gatta and yoshua bengio fitnets hints for thin deep nets arxiv preprint alexander m rush sumit chopra and jason weston a neural attention model for abstractive in proceedings of the tence summarization conference on empirical methods in natural guage processing pages lisbon portugal association for computational linguistics evan sandhaus the new york times annotated corpus linguistic data consortium philadelphia abigail see peter j liu and christopher d manning get to the point summarization with generator networks in proceedings of the nual meeting of the association for computational linguistics volume long papers pages vancouver canada kaiqiang song lin zhao and fei liu infused copy mechanisms for abstractive tion in proceedings of the international ference on computational linguistics pages santa fe new mexico usa association for computational linguistics kaitao song xu tan tao qin jianfeng lu and yan liu mass masked sequence to quence pre training for language generation in ceedings of international conference on machine learning pages long beach nia jiwei tan xiaojun wan and jianguo xiao abstractive document summarization with a in proceedings based attentional neural model of the annual meeting of the association for computational linguistics volume long papers pages vancouver canada association for computational linguistics xu tan yi ren di he tao qin zhou zhao and tie yan liu multilingual neural machine corr translation with knowledge distillation yee teh victor bapst wojciech m czarnecki john quan james kirkpatrick raia hadsell nicolas heess and razvan pascanu distral robust multitask reinforcement learning in i guyon u v luxburg s bengio h wallach r fergus s wanathan and r garnett editors advances in ral information processing systems pages curran associates inc gregor urban krzysztof j geras samira ebrahimi kahou ozlem aslan shengjie wang rich ana abdelrahman mohamed matthai philipose and matt richardson do deep convolutional nets really need to be deep and convolutional arxiv preprint ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan n gomez ukasz kaiser and illia polosukhin attention is all you need in i guyon u v luxburg s bengio h wallach r fergus s vishwanathan and r nett editors advances in neural information cessing systems pages curran ciates inc yonghui wu mike schuster zhifeng chen quoc v le mohammad norouzi wolfgang macherey maxim krikun yuan cao qin gao klaus macherey et al google s neural machine translation system bridging the gap between in arxiv preprint man and machine translation qizhe xie eduard hovy minh thang luong and quoc v le self training with noisy student arxiv preprint improves imagenet classication chenglin yang lingxi xie siyuan qiao and alan yuille training deep neural networks in erations a more tolerant teacher educates better dents in proceedings of the aaai conference on articial intelligence volume pages honolulu hawaii sergey zagoruyko and nikos komodakis ing more attention to attention improving the formance of convolutional neural networks via tion transfer in proceedings of the international conference on learning represenatations toulon france dongxu zhang and zhichao yang word ding perturbation for sentence classication arxiv preprint
