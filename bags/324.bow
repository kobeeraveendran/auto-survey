noisy self knowledge distillation text summarization yang liu sheng shen mirella lapata university edinburgh yang university california berkeley sheng edu abstract paper apply self knowledge tion text summarization argue alleviate problems maximum likelihood training single reference noisy datasets instead relying hot annotation labels student summarization model trained guidance teacher generates smoothed labels help regularize training furthermore better model uncertainty ing training introduce multiple noise nals teacher student models demonstrate experimentally marks framework boosts mance pretrained non pretrained summarizers achieving state art results introduction automatic summarization enjoyed renewed thanks interest recent years ity neural network models ability learn continuous representations recourse preprocessing tools linguistic annotations availability large scale datasets sandhaus hermann grusky narayan containing hundreds sands document summary pairs driven development neural architectures rization approaches proposed vast majority sequence sequence models trained end end fashion maximum likelihood estimation loss celikyilmaz paulus gehrmann despite promising results specic acteristics summarization task der ill suited standard sequence sequence training instance maximum likelihood ing single reference datasets timal summarization subject great deal human variation harman context nenkova tive summarization different people select ent sentences include summary rath writing abstracts disagreement exists terms writing style specic content deemed important mary harman marization models naturally benet multiple target references unrealistic pect multi reference summarization datasets created scale neural network ing fact popular benchmarks collated opportunistically based summaries loosely correspond source input example narayan create dataset pairing rst sentence news ticle rest document sumption introductory sentence expresses gist article grusky pair ticles metadata available html pages der assumption html tags tion summary like content work liu perez beltrachini multidocument summarization datasets ated viewing lead sections wikipedia articles summaries documents cited herent noise data collection process hampers training models prone hallucination song maynez struggling identify content units salient tan paper propose alleviate problems turning knowledge tion bucilu caruana hinton kim rush edge distillation transfers knowledge larger teacher network smaller student model training student imitate teacher outputs addition learning training data set born networks furlanello teacher student neural chitecture model size surprisingly student able surpass teacher accuracy intuitively self knowledge distillation effective teacher output distribution provides richer training signal capturing additional mation training examples context summarization teacher benet student training ways provides softened tribution reference summaries ing single reference setting teacher distribution certain extent noised enabling student circumvent curacies training data talize idea teacher student robust noise introduce noise injection techniques knowledge distillation improve model alization performance covering present experiments narayan marization benchmarks hermann perez beltrachini multi document summarization settings different types summaries verbose telegraphic datasets proposed framework boosts performance pretrained pretrained abstractive summarizers achieving new state art results background neural abstractive summarization neural approaches abstractive summarization conceptualize task sequence sequence problem encoder maps sequence tokens source document sequence continuous representations decoder autoregressively erates target summary token token modeling conditional probability rush nallapati rst apply neural encoder decoder architecture text tion enhance model pointer generator network allows copy words source text coverage anism keeps track words summarized work develops tive models trained end end reinforcement learning based multiple encoders cal attention celikyilmaz erage mechanism decoder attends previously generated words paulus gehrmann follow proach content selector rst determines phrases source document summary copy mechanism applied preselected phrases decoding majority summarization systems composed lstm units narayan perez beltrachini propose stractive models based convolutional neural works gains abstractive pretrained language models recently emerged key technology achieving impressive tion liu lapata lewis song models rst pretrain language model self supervised tives large corpora tune liu lapata summarization datasets combine pretrained encoder based bert devlin randomly demonstrating substantial initialized decoder gains summarization performance song pretrain encoder decoder framework fragments reconstruct sentence tune summarization datasets vein lewis present bart encoder decoder transformer vaswani pretrained ing text corrupted arbitrary noising functions bao design transformer based neural network pretrained pseudo masked language model masked knowledge distillation knowledge distillation refers class ods training new smaller student network learning teacher network addition learning training data generally sumed teacher previously trained parameters student estimated matching student predictions teacher let denote teacher student models respectively let functions teacher student models typically neural networks function ple dened output network layer hidden softmax layer knowledge tillation methods commonly expressed imizing objective function training set lkd xix loss function penalizes ference teacher student logits based hidden caruana output representations specic instantiations general work include minimizing teacher student difference mediate attention maps derivatives loss romero zagoruyko komodakis czarnecki work integrates ensemble teachers order improve student urban trains succession students furlanello introduces teacher assistant better knowledge transfer mirzadeh regularizes parisotto teh task agents reinforcement learning compared direct training knowledge distillation provides stable training process leads better performing student models hinton phuong lampert recent work furlanello hahn choi sheds light leveraging knowledge distillation training high performing student model size teacher discussion section knowledge distillation shown improve results nlp tasks tan use transfer knowledge bert smaller models helping approach quality larger pretrained neural networks aside distilling large models smaller ones kim rush mou ensembles models single els kuncoro liu edge distillation task learning teach multi task student single task teachers clark self knowledge distillation text summarization self knowledge distillation refers cial case teacher student identical neural network architectures prisingly consistently served furlanello yang ahn students trained knowledge distillation outperform teachers signicant margins computer vision language modeling tasks recent efforts focused understanding happens observing knowledge transferred teacher localized mainly higher layers affect early feature extraction ers gotmare interpreting teacher knowledge importance weighting furlanello showing stopping crucial dong studying self distillation modies tion mobahi text summarization argue knowledge distillation potentially alleviate problems conventional maximum likelihood summarization models typically training trained single reference document summary pairs considering single summary correct reference maximum likelihood training harm model ization elbayad multiple valid intuitive harman maries source input nenkova single ence summaries available entirely dard inherent noise automatic construction large scale summarization datasets kryscinski self knowledge tillation teacher outputs provide softened tions reference summaries viewed enrichment single reference setting reweighting gold summaries vent student condent predictions standard objective abstractive marization model negative log likelihood lnll indicates source document cates token target summary rst tokens target summary assume teacher fully trained neural model student ture teacher access learned teacher output distribution lkd model outputs teacher student spectively input usually long document design following perturbation policies common practice compensate rect access training data equation interpolating losses tions nal objective ing student lfinal mixture parameter combining hot distribution teacher distribution want summarization systems robust natural noise found existing datasets injecting noise training samples proven useful improving model ization extend idea knowledge distillation propose novel work introducing noise distillation nals training data design different noise mechanisms teacher student select best noise conguration experimentally noisy teacher inject noise lation signals incorporate teacher dropout mechanism dropout kept active generating teacher predictions training student manner teacher generates variable supervision labels dent degree uncertainty alleviating problem overtting teacher tions considered proximating average ensemble ral networks dropout rate knowledge distillation loss lkd word drop word source input moved probability word replacement word source input calculate candidate placement list selecting words ilar vocabulary ity calculated cosine distance tween embedding embeddings words vocabulary source word replaced word domly selected candidate ment list probability sentence drop sentence source input removed probability gaussian noise gaussian noise vector multiplied embeddings input words perturbation policies applied multaneously successively pipeline experimentally found best combination task sequential application word drop followed word replacement sentence drop gaussian noise effective natural language understanding tasks zhang yang found helfpul summarization experiments knowledge distillation loss student trained noisy data lkd indicates perturbed source input teacher model active dropout indicates predictions experimental setup noisy student inject noise ing data propose mechanisms turb source input random perturbation effective enforcing local smoothness ing text generation models assumption semantically similar inputs mapped similar targets related approach shown improve performance machine translation models self training tings text summarization section describe summarization datasets experiments discuss ious implementation details summarization datasets evaluated model document summarization datasets cnn dailymail news highlights hermann xsum narayan multi document dataset wikicatsum perez beltrachini summarization pretraining lead ptrnet transformerabs skd skd noisy skd noisy noisy base size pretrained models massbase bertsumabs skd skd noisy skd noisy noisy large size pretrained models unilmlarge bartlarge cnn dailymail xsum table rouge results cnn dailymail xsum test sets shorthands unigram bigram overlap longest common subsequence skd refers system trained self knowledge distillation noisy skd models trained noisy signals noisy student models trained noisy data results comparison systems taken authors respective papers obtained data running publicly released software represent different datasets summary styles ranging highlights brief sentence summaries summaries vary respect type rewriting operations exemplify cnn dailymail showcases cut paste operations xsum genuinely abstractive datasets xsum wikicatsum created assumptions automatically following correspondence purported summaries source input finally cnn dailymail contains news articles sociated highlights bullet points journalists brief overview article standard splits training hermann tion testing cnn ments dailymail uments anonymize entities tences split stanford corenlp toolkit manning dataset pre processed following input documents truncated tokens question article splits narayan training tion testing lowed pre processing introduced work input documents truncated kens wikicatsum multi document tion dataset derived wikisum liu target summary lead tion wikipedia article source webpages related article catsum perez beltrachini represents domains original wikisum dataset assumption vary terms topics summaries discuss tic characteristics aside summaries dataset contains input webpages length truncated rst tokens contains samples company domain samples film domain samples animal domain implementation details xsum contains news articles nied sentence summary answering datasets evaluated self knowledge distillation framework settings animal pretraining company skd skd noisy skd noisy noisy skd skd noisy skd noisy noisy film pretraining table rouge results wikicatsum test sets shorthands unigram bigram overlap longest common subsequence results reported separately domains combination skd refers systems trained self knowledge distillation noisy skd systems trained noisy signals noisy skd students trained noisy data results comparison systems taken authors respective papers obtained data running publicly released software rst setting models non pretrained second setting advantage trained language models strated impressive improvements tion lewis liu lapata bao specically adopt bao pretrained model transformer based neural network vaswani transformer layers attention heads pretrained pseudo masked guage model large corpus label smoothing applied smoothing factor tuned teacher models following procedure bao non pretrained setting adopt transformer encoder decoder model layers hidden size forward lter size label smoothing smoothing factor teacher models setting trained randomly initialized parameters following liu lapata knowledge distillation experiments dent models neural network tecture teachers trained hyperparameters teacher models best teacher student model selected evaluating perplexity development set noisy distillation models word drop probability set candidate length word replacement word replacement ability sentence drop probability decoding beam search size tuned length penalty validation set decode end sequence token ted repeated trigrams blocked paulus results automatic evaluation evaluated summarization quality automatically rouge lin report unigram bigram overlap means assessing informativeness longest common subsequence rouge means assessing uency examples system output shown table table summarizes results cnn dailymail xsum single document datasets rst block includes results non pretrained models present lead baseline simply selects rst tences document cnn dailymail rst sentence report results pointer ator network ptrnet abstractive tem liu lapata based formers transformerabs section tails forms backbone knowledge distillation models skd present variant noise skd variant noise teacher training signal noisy models transformerabs noisy skd noisy skd cnn dailymail xsum table factual correctness cnn dailymail xsum test set noisy skd students trained noisy signals noisy data variant student ally trained noisy data noisy second blocks table include results pretrained models isons fairer separate second block base size block pretrained models based parameter size shown ses regard large size models port results strong summarization systems netuned unilmlarge bao bartlarge lewis raffel base size models include bertsumbase liu lapata summarizer based base size bert encoder randomly initialized decoder massbase song unilmbase netuned base size trained models seen table skd improves teacher models pretrained size non pretrained settings observe injection noise brings improvements noise training signal noisy ing effective compared noisy data tation noisy overall obtain competitive results skd base size pretrained els manage outperform unilmlarge cnn dailymail dataset table presents experimental results kicatsum dataset rst block table includes results non pretrained models perez beltrachini convolutional encoder decoder models standard convolutional decoder adopts hierarchical convolutional coder rst generates target sentence tors generates target words based sentence vectors standard encoder decoder model trained catsum perez beltrachini model skd system noisy version noisy noisy second block includes results system size pretrained model unilmbase skd results reported domain pany film animal domains pretrained non pretrained settings skd boosts performance teacher model unilmbase respectively injection noise benecial provements performance vary domains film showing gains column table shows average rouge domains skd noise injection improve sults observe non pretrained models factual consistency evaluation use rouge factcc kryscinski evaluate factual correctness generated factcc bert based classier maries trained identify conicts source document generated summary given document sentence pair input assigns information mentioned positive label summary sentence consistent document assigns negative label view percentage positive labels assigned factcc generated summaries factual correctness score summarization system factual performed experiments publicly released version factcc results cnn dailymail xsum datasets presented table focus document summarization version factcc trained multi document datasets seen application skd trained noisy signals noisy data improves tual consistency non pretrained pretrained models datasets noisy skd dents signicantly factually correct compared teachers abs paired student test human evaluation addition automatic evaluation sessed system output eliciting human ments compared quality summaries produced teacher model distilled student noisy skd com salesforce factcc cnn daily mail gold granderson millennials marry want says case happily single happy granderson says marriage family money granderson millennials care generation thinks marriage says married want marriage linked economic clear true noisy skd carol costello talk millennial envision america virtually marriage free countries like sweden denmark people feel pressured marry kids xsum half pupils wales passed gcse exam year running gold pupils wales achieved grades gcse exams noisy skd thirds welsh pupils took gcses got grades according year results animal gold conception bank silver boa chilabothrus argentum species boa described known conception island bank bahamas rst known discovery west indian boa species years named unique silver color conception bank silver boa chilabothrus argentum species snake family boidae demic bahamas species discovered conception island bank comprises uninhabited islets noisy skd conception bank silver boa chilabothrus argentum species nonvenomous boa endemic bahamas discovered conception island bank uninhabited islet bahamas table gold reference summaries automatic summaries produced distilled student cnn dailymail xsum datasets cnn dailymail noisy skd succinct inform fluent xsum noisy skd succinct inform fluent noisy skd company film animal table human evaluation cnn dailymail xsum wikicatsum test sets noisy skd trained self knowledge distillation noisy signals noisy data pairwise ences systems signicant paired test cnn dailymail xsum human participants presented output systems original document asked cide better according lowing criteria succinctness summary avoid repetition informativeness mary capture document important formation fluency summary uent grammatical evaluation conducted amazon mechanical turk crowdsourcing platform test documents total liu lapata cnn dailymail xsum elicited sponses hit systems rated dimension assigned score corresponding proportion times system selected better human evaluation results shown table upper datasets participants ceive student noisy skd signicantly succinct informative pared teacher fluency student tends worse spection found student summaries telegraphic hypothesize crowdworkers tend penalize terms uency grammatical human evaluation performed slightly ferent recall document dataset input documents continuous webpage fragments allow pants perform experiment timely ion gold summary proxy content input crowdworkers presented output systems noisy skd asked decide better according formation contained gold summary uation conducted amt randomly lected samples test set elicited responses hit domain port proportion times system chosen better human evaluation results shown table lower amt crowdworkers prefer maries produced student animal film domains company found distilled model tends generate tities sentence render summaries dense domain conclusions paper advocated use knowledge distillation abstractive tion means alleviate problems associated maximum likelihood training task introduced noise functions training signal training data help larize training boost performance periments benchmark datasets strate framework improve pretrained pretrained summarizers ture like investigate thoroughly aspects pretrained models improve self knowledge distillation enhanced sophisticated noise functions references sungsoo ahn shell andreas damianou neil lawrence zhenwen dai ational information distillation knowledge ieee conference computer vision fer pattern recognition cvpr pages long beach california chine learning pages new york new york asli celikyilmaz antoine bosselut xiaodong yejin choi deep communicating agents proceedings abstractive summarization conference north american chapter association computational linguistics man language technologies volume long pers pages new orleans louisiana kevin clark minh thang luong urvashi wal christopher manning quoc bam born multi task networks natural language understanding proceedings annual meeting association tional linguistics pages florence italy association computational linguistics wojciech czarnecki simon osindero max berg grzegorz swirszcz razvan pascanu sobolev training neural networks guyon luxburg bengio wallach fergus vishwanathan garnett editors advances neural information processing systems pages curran associates inc jacob devlin ming wei chang kenton lee kristina toutanova bert pre training deep bidirectional transformers language proceedings conference standing north american chapter association computational linguistics human language technologies volume long short papers pages minneapolis minnesota ation computational linguistics bin dong jikai hou yiping zhihua zhang distillation early stopping harvesting dark knowledge utilizing anisotropic information neurips workshop machine trieval learning guarantees vancouver canada jimmy rich caruana deep nets advances neural ally need deep formation processing systems pages curran associates inc maha elbayad laurent besacier jakob verbeek token level sequence level loss arxiv preprint ing rnn language models hangbo bao dong furu wei wenhui wang nan yang xiaodong liu wang songhao piao feng gao ming zhou hsiao wuen hon pseudo masked language models language model pre training arxiv preprint cristian bucilu rich caruana alexandru niculescu mizil model compression ceedings acm sigkdd international conference knowledge discovery data ing kdd page new york usa association computing machinery samuel rota lorenzo porzi peter kontschieder dropout distillation ceedings international conference tommaso furlanello zachary lipton michael nen laurent itti anima anandkumar born neural networks international ference machine learning pages stockholm sweden sebastian gehrmann yuntian deng alexander rush abstractive summarization proceedings conference cal methods natural language processing pages brussels belgium akhilesh gotmare nitish shirish keskar caiming xiong richard socher closer look deep learning heuristics learning rate restarts warmup distillation international ference learning representations iclr new orleans usa view net max grusky mor naaman yoav artzi newsroom dataset million summaries diverse extractive strategies proceedings conference north american chapter association computational linguistics man language technologies volume long pers pages new orleans louisiana sociation computational linguistics sangchul hahn heeyoul choi knowledge distillation natural language ing proceedings international conference recent advances natural language ing ranlp pages varna bulgaria incoma ltd donna harman paul effects human variation duc summarization text summarization branches pages tion barcelona spain association tional linguistics junxian jiatao jiajun shen marcaurelio revisiting self training arxiv preprint sequence generation ranzato neural karl moritz hermann tomas kocisky edward stette lasse espeholt kay mustafa suleyman phil blunsom teaching machines read comprehend cortes lawrence lee sugiyama garnett editors advances neural information processing systems pages curran associates inc geoffrey hinton oriol vinyals jeff dean distilling knowledge neural network arxiv preprint yoon kim alexander rush level knowledge distillation proceedings conference empirical methods ral language processing pages austin texas association computational linguistics wojciech kryscinski nitish shirish keskar bryan cann caiming xiong richard socher neural text summarization critical evaluation arxiv preprint adhiguna kuncoro miguel ballesteros lingpeng kong chris dyer noah smith ing ensemble greedy dependency parsers mst parser proceedings ference empirical methods natural language processing pages austin texas ciation computational linguistics comprehension proceedings nual meeting association computational linguistics pages online association computational linguistics chin yew lin rouge package text matic evaluation summaries rization branches proceedings workshop pages barcelona spain tion computational linguistics peter liu mohammad saleh etienne pot ben goodrich ryan sepassi lukasz kaiser noam shazeer generating wikipedia ing long sequences proceedings national conference learning representations vancouver canada xiaodong liu pengcheng weizhu chen feng gao multi task deep neural networks natural language understanding proceedings annual meeting association putational linguistics pages florence italy association computational linguistics yang liu mirella lapata text proceedings tion pretrained encoders conference empirical methods ural language processing international joint conference natural language processing emnlp ijcnlp pages hong kong china association computational linguistics christopher manning mihai surdeanu john bauer jenny finkel steven bethard david mcclosky stanford corenlp natural language proceedings annual cessing toolkit meeting association computational guistics system demonstrations pages timore maryland association computational linguistics joshua maynez shashi narayan bernd bohnet ryan mcdonald faithfulness ality abstractive summarization proceedings annual meeting association computational linguistics pages line association computational linguistics seyed iman mirzadeh mehrdad farajtabar ang hassan ghasemzadeh improved edge distillation teacher assistant bridging gap student teacher arxiv preprint hossein mobahi mehrdad farajtabar peter bartlett bartlett self distillation regularization hilbert space arxiv preprint mike lewis yinhan liu naman goyal jan ghazvininejad abdelrahman mohamed omer levy veselin stoyanov luke zettlemoyer bart denoising sequence sequence training natural language generation translation lili mou ran jia yan zhang zhi jin distilling word embeddings ing approach proceedings acm national conference information edge management pages ramesh nallapati bowen zhou cicero dos santos caglar gulcehre bing xiang tive text summarization sequence sequence proceedings rnns signll conference computational natural guage learning pages berlin germany association computational linguistics shashi narayan shay cohen mirella lapata details summary topic aware convolutional neural networks proceedings treme summarization conference empirical methods natural guage processing pages brussels gium association computational linguistics ani nenkova summarization evaluation text speech issues approaches ninth tional conference spoken language processing emilio parisotto jimmy ruslan salakhutdinov actor mimic deep multitask transfer inforcement learning corr romain paulus caiming xiong richard socher deep reinforced model abstractive marization proceedings international conference learning representations ver canada laura perez beltrachini yang liu mirella lapata generating summaries topic templates structured convolutional decoders ings annual meeting association computational linguistics pages florence italy association computational guistics mary phuong christoph lampert understanding knowledge distillation ings international conference chine learning pages long beach ifornia colin raffel noam shazeer adam roberts katherine lee sharan narang michael matena yanqi zhou wei peter liu exploring limits transfer learning unied text text arxiv prints rath resnick savage tion abstracts selection sentences sentence selection men machines american documentation adriana romero nicolas ballas samira ebrahimi hou antoine chassang carlo gatta yoshua bengio fitnets hints thin deep nets arxiv preprint alexander rush sumit chopra jason weston neural attention model abstractive proceedings tence summarization conference empirical methods natural guage processing pages lisbon portugal association computational linguistics evan sandhaus new york times annotated corpus linguistic data consortium philadelphia abigail peter liu christopher manning point summarization generator networks proceedings nual meeting association computational linguistics volume long papers pages vancouver canada kaiqiang song lin zhao fei liu infused copy mechanisms abstractive tion proceedings international ference computational linguistics pages santa new mexico usa association computational linguistics kaitao song tan tao qin jianfeng yan liu mass masked sequence quence pre training language generation ceedings international conference machine learning pages long beach nia jiwei tan xiaojun wan jianguo xiao abstractive document summarization proceedings based attentional neural model annual meeting association computational linguistics volume long papers pages vancouver canada association computational linguistics tan ren tao qin zhou zhao tie yan liu multilingual neural machine corr translation knowledge distillation yee teh victor bapst wojciech czarnecki john quan james kirkpatrick raia hadsell nicolas heess razvan pascanu distral robust multitask reinforcement learning guyon luxburg bengio wallach fergus wanathan garnett editors advances ral information processing systems pages curran associates inc gregor urban krzysztof geras samira ebrahimi kahou ozlem aslan shengjie wang rich ana abdelrahman mohamed matthai philipose matt richardson deep convolutional nets need deep convolutional arxiv preprint ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan gomez ukasz kaiser illia polosukhin attention need guyon luxburg bengio wallach fergus vishwanathan nett editors advances neural information cessing systems pages curran ciates inc yonghui mike schuster zhifeng chen quoc mohammad norouzi wolfgang macherey maxim krikun yuan cao qin gao klaus macherey google neural machine translation system bridging gap arxiv preprint man machine translation qizhe xie eduard hovy minh thang luong quoc self training noisy student arxiv preprint improves imagenet classication chenglin yang lingxi xie siyuan qiao alan yuille training deep neural networks erations tolerant teacher educates better dents proceedings aaai conference articial intelligence volume pages honolulu hawaii sergey zagoruyko nikos komodakis ing attention attention improving formance convolutional neural networks tion transfer proceedings international conference learning represenatations toulon france dongxu zhang zhichao yang word ding perturbation sentence classication arxiv preprint
