noisy self knowledge distillation text summarization yang liu sheng shen mirella lapata university edinburgh yang ac uk ed ac uk university california berkeley sheng edu p e s l c s c v v x r abstract paper apply self knowledge tion text summarization argue alleviate problems maximum likelihood training single reference noisy datasets instead relying hot annotation labels student summarization model trained guidance teacher generates smoothed labels help regularize training furthermore better model uncertainty ing training introduce multiple noise nals teacher student models demonstrate experimentally marks framework boosts mance pretrained non pretrained summarizers achieving state art results introduction automatic summarization enjoyed renewed thanks interest recent years ity neural network models ability learn continuous representations recourse preprocessing tools linguistic annotations availability large scale datasets sandhaus hermann et al grusky et al narayan et al containing hundreds sands document summary pairs driven development neural architectures rization approaches proposed vast majority sequence sequence models trained end end fashion maximum likelihood estimation loss et al celikyilmaz et al paulus et al gehrmann et al despite promising results specic acteristics summarization task der ill suited standard sequence sequence training instance maximum likelihood ing single reference datasets timal summarization subject great deal human variation harman context nenkova tive summarization different people select ent sentences include summary rath et al writing abstracts disagreement exists terms writing style specic content deemed important mary harman marization models naturally benet multiple target references unrealistic pect multi reference summarization datasets created scale neural network ing fact popular benchmarks collated opportunistically based summaries loosely correspond source input example narayan et al create dataset pairing rst sentence news ticle rest document sumption introductory sentence expresses gist article grusky et al pair ticles metadata available html pages der assumption html tags e tion summary like content work liu et al perez beltrachini et al multidocument summarization datasets ated viewing lead sections wikipedia articles summaries documents cited herent noise data collection process hampers training models prone hallucination song et al maynez et al struggling identify content units salient tan et al paper propose alleviate problems turning knowledge tion bucilu et al ba caruana hinton et al kim rush edge distillation transfers knowledge larger teacher network smaller student model training student imitate teacher s outputs addition learning training data set born networks furlanello et al teacher student neural chitecture model size surprisingly student able surpass teacher s accuracy intuitively self knowledge distillation effective teacher s output distribution provides richer training signal capturing additional mation training examples context summarization teacher benet student training ways provides softened tribution reference summaries ing single reference setting teacher s distribution certain extent noised enabling student circumvent curacies training data talize idea teacher student robust noise introduce noise injection techniques knowledge distillation improve model alization performance covering present experiments narayan et al marization benchmarks hermann et al perez beltrachini et al multi document summarization settings different types summaries e verbose telegraphic datasets proposed framework boosts performance pretrained pretrained abstractive summarizers achieving new state art results background neural abstractive summarization neural approaches abstractive summarization conceptualize task sequence sequence problem encoder maps sequence tokens source document xn sequence continuous representations decoder autoregressively erates target summary ym token token modeling conditional probability xn rush et al nallapati et al rst apply neural encoder decoder architecture text tion et al enhance model pointer generator network allows copy words source text coverage anism keeps track words summarized work develops tive models trained end end reinforcement learning based multiple encoders cal attention celikyilmaz et al erage mechanism decoder attends previously generated words paulus et al gehrmann et al follow proach content selector rst determines phrases source document summary copy mechanism applied preselected phrases decoding majority summarization systems composed lstm units narayan et al perez beltrachini et al propose stractive models based convolutional neural works gains abstractive pretrained language models recently emerged key technology achieving impressive tion liu lapata lewis et al song et al models rst pretrain language model self supervised tives large corpora ne tune liu lapata summarization datasets combine pretrained encoder based bert devlin et al randomly demonstrating substantial initialized decoder gains summarization performance song et al pretrain encoder decoder framework fragments reconstruct sentence ne tune summarization datasets vein lewis et al present bart encoder decoder transformer vaswani et al pretrained ing text corrupted arbitrary noising functions bao et al design transformer based neural network pretrained pseudo masked language model masked knowledge distillation knowledge distillation refers class ods training new smaller student network learning teacher network addition learning training data generally sumed teacher previously trained parameters student estimated matching student s predictions teacher let t s denote teacher student models respectively let ft fs functions teacher student models typically neural networks function ple dened output network layer e hidden softmax layer knowledge tillation methods commonly expressed imizing objective function training set x lkd x xix xi l loss function penalizes ference teacher student logits based hidden ba caruana output representations specic instantiations general work include minimizing teacher student difference mediate attention maps derivatives loss romero et al zagoruyko komodakis czarnecki et al work integrates ensemble teachers order improve student urban et al trains succession students furlanello et al introduces teacher assistant better knowledge transfer mirzadeh et al regularizes parisotto et al teh et al task agents reinforcement learning compared direct training knowledge distillation provides stable training process leads better performing student models hinton et al phuong lampert recent work furlanello et al hahn choi sheds light leveraging knowledge distillation training high performing student model size teacher discussion section knowledge distillation shown improve results nlp tasks tan et al use transfer knowledge bert smaller models helping approach quality larger pretrained neural networks aside distilling large models smaller ones kim rush mou et al ensembles models single els kuncoro et al liu et al edge distillation task learning e teach multi task student single task teachers clark et al self knowledge distillation text summarization self knowledge distillation refers cial case teacher student identical neural network architectures prisingly consistently served furlanello et al yang et al ahn et al students trained knowledge distillation outperform teachers signicant margins computer vision language modeling tasks recent efforts focused understanding happens e observing knowledge transferred teacher localized mainly higher layers affect early feature extraction ers gotmare et al interpreting teacher s knowledge importance weighting furlanello et al showing stopping crucial dong et al studying self distillation modies tion mobahi et al text summarization argue knowledge distillation potentially alleviate problems conventional maximum likelihood summarization models typically training trained single reference document summary pairs considering single summary correct reference maximum likelihood training harm model ization elbayad et al multiple valid intuitive harman maries source input nenkova single ence summaries available entirely dard inherent noise automatic construction large scale summarization datasets kryscinski et al self knowledge tillation teacher outputs provide softened tions reference summaries viewed enrichment single reference setting reweighting gold summaries vent student condent predictions standard objective abstractive marization model negative log likelihood lnll t x indicates source document yt cates t token target summary rst t tokens target summary assume teacher fully trained neural model student ture teacher access learned teacher s output distribution pt x lkd t x pt model outputs teacher student spectively input usually long document design following perturbation policies common practice compensate rect access training data equation interpolating losses tions nal objective ing student lfinal mixture parameter combining hot distribution teacher distribution want summarization systems robust natural noise found existing datasets injecting noise training samples proven useful improving model ization et al extend idea knowledge distillation propose novel work introducing noise distillation nals training data design different noise mechanisms teacher student select best noise conguration experimentally noisy teacher inject noise lation signals incorporate teacher dropout mechanism et al dropout kept active generating teacher predictions training student manner teacher generates variable supervision labels dent degree uncertainty alleviating problem overtting teacher tions considered proximating average ensemble ral networks et al dropout rate knowledge distillation loss lkd t ps t x word drop word source input moved probability pd word replacement word xi source input calculate candidate placement list selecting k words ilar xi vocabulary ity calculated cosine distance tween embedding xi embeddings words vocabulary source word replaced word domly selected candidate ment list probability pr sentence drop sentence source input removed probability ps gaussian noise gaussian noise vector e multiplied embeddings input words e e n perturbation policies applied multaneously successively pipeline experimentally found best combination task sequential application word drop followed word replacement sentence drop gaussian noise effective natural language understanding tasks zhang yang found helfpul summarization experiments knowledge distillation loss student trained noisy data lkd t x t x x indicates perturbed source input p teacher model active dropout t indicates predictions experimental setup noisy student inject noise ing data propose mechanisms turb source input random perturbation effective enforcing local smoothness ing text generation models assumption semantically similar inputs mapped similar targets related approach shown improve performance machine translation models self training tings et al text summarization section describe summarization datasets experiments discuss ious implementation details summarization datasets evaluated model document summarization datasets cnn dailymail news highlights hermann et al xsum narayan et al multi document dataset e wikicatsum perez beltrachini et al summarization pretraining lead ptrnet transformerabs skd skd noisy t skd noisy t noisy s base size pretrained models massbase bertsumabs m m m m skd skd noisy t m skd noisy t noisy s m large size pretrained models unilmlarge bartlarge m m cnn dailymail rl rl rl xsum rl rl rl table rouge results cnn dailymail xsum test sets shorthands unigram bigram overlap rl longest common subsequence skd refers system trained self knowledge distillation noisy t skd models trained noisy signals noisy s student models trained noisy data results comparison systems taken authors respective papers obtained data running publicly released software represent different datasets summary styles ranging highlights brief sentence summaries summaries vary respect type rewriting operations exemplify e cnn dailymail showcases cut paste operations xsum genuinely abstractive datasets xsum wikicatsum created assumptions automatically following correspondence purported summaries source input finally cnn dailymail contains news articles sociated highlights e bullet points journalists brief overview article standard splits training hermann et al tion testing cnn ments dailymail uments anonymize entities tences split stanford corenlp toolkit manning et al dataset pre processed following et al input documents truncated tokens question article splits narayan et al training tion testing lowed pre processing introduced work input documents truncated kens wikicatsum multi document tion dataset derived wikisum liu et al target summary lead tion wikipedia article source webpages related article catsum perez beltrachini et al represents domains original wikisum dataset assumption vary terms topics summaries discuss tic characteristics aside summaries dataset contains input webpages length truncated rst tokens contains samples company domain samples film domain samples animal domain implementation details xsum contains news articles nied sentence summary answering datasets evaluated self knowledge distillation framework settings animal pretraining company rl cv cv tf skd skd noisy t skd noisy t noisy s rl rl rl rl rl skd skd noisy t skd noisy t noisy s film rl rl pretraining table rouge results wikicatsum test sets shorthands unigram bigram overlap rl longest common subsequence results reported separately domains combination skd refers systems trained self knowledge distillation noisy t skd systems trained noisy signals noisy s skd students trained noisy data results comparison systems taken authors respective papers obtained data running publicly released software rst setting models non pretrained second setting advantage trained language models strated impressive improvements tion lewis et al liu lapata bao et al specically adopt bao et al pretrained model transformer based neural network vaswani et al transformer layers attention heads pretrained pseudo masked guage model large corpus label smoothing applied smoothing factor tuned teacher models following procedure bao et al non pretrained setting adopt transformer encoder decoder model layers hidden size forward lter size label smoothing smoothing factor teacher models setting trained randomly initialized parameters following liu lapata knowledge distillation experiments dent models neural network tecture teachers trained hyperparameters teacher models best teacher student model selected evaluating perplexity development set noisy distillation models word drop probability pd set candidate length k word replacement word replacement ability pr sentence drop probability ps decoding beam search size tuned length penalty wu et al validation set decode end sequence token ted repeated trigrams blocked paulus et al results automatic evaluation evaluated summarization quality automatically rouge lin report unigram bigram overlap means assessing informativeness longest common subsequence rouge l means assessing uency examples system output shown table table summarizes results cnn dailymail xsum single document datasets rst block includes results non pretrained models present lead baseline simply selects rst tences document cnn dailymail rst sentence report results et al s pointer ator network ptrnet abstractive tem liu lapata based formers transformerabs section tails forms backbone knowledge distillation models skd present variant noise skd variant noise teacher training signal noisy t models transformerabs noisy skd noisy skd cnn dailymail xsum table factual correctness cnn dailymail xsum test set noisy skd students trained noisy signals noisy data variant student ally trained noisy data noisy s second blocks table include results pretrained models isons fairer separate second block base size block pretrained models based parameter size shown ses regard large size models port results strong summarization systems netuned unilmlarge bao et al bartlarge lewis et al raffel et al base size models include bertsumbase liu lapata summarizer based base size bert encoder randomly initialized decoder massbase song et al unilmbase netuned base size trained models seen table skd improves teacher models pretrained size non pretrained settings observe injection noise brings improvements noise training signal noisy t ing effective compared noisy data tation noisy s overall obtain competitive results skd base size pretrained els manage outperform unilmlarge cnn dailymail dataset table presents experimental results kicatsum dataset rst block table includes results non pretrained models cv perez beltrachini et al convolutional encoder decoder models standard convolutional decoder adopts hierarchical convolutional coder rst generates target sentence tors generates target words based sentence vectors tf standard encoder decoder model trained catsum perez beltrachini et al tf model skd system noisy version noisy t noisy s second block includes results system size pretrained model unilmbase skd results reported domain pany film animal domains pretrained non pretrained settings skd boosts performance teacher model unilmbase tf respectively injection noise benecial provements performance vary domains film showing gains column table shows average rouge domains skd noise injection improve sults observe non pretrained models et factual consistency evaluation use rouge factcc kryscinski et al evaluate factual correctness generated factcc bert based classier maries trained identify conicts source document generated summary given document sentence pair input assigns information mentioned positive label summary sentence consistent document assigns negative label view percentage positive labels assigned factcc generated summaries factual correctness score summarization system factual performed experiments publicly released version factcc results cnn dailymail xsum datasets presented table focus document summarization version factcc trained multi document datasets seen application skd trained noisy signals noisy data improves tual consistency non pretrained pretrained models datasets noisy skd dents signicantly p factually correct compared teachers abs paired student t test human evaluation addition automatic evaluation sessed system output eliciting human ments compared quality summaries produced teacher model distilled student noisy skd com salesforce factcc cnn daily mail gold lz granderson millennials ll marry want says s case happily single happy granderson says marriage family money lz granderson millennials nt care generation thinks marriage says ll married want lz marriage linked economic s clear s true noisy skd carol costello talk millennial envision america virtually marriage free countries like sweden denmark people nt feel pressured marry kids xsum half pupils wales passed gcse exam year running gold pupils wales achieved grades gcse exams noisy skd thirds welsh pupils took gcses got c grades according year s results animal gold conception bank silver boa chilabothrus argentum species boa described known conception island bank bahamas rst known discovery west indian boa species years named unique silver color conception bank silver boa chilabothrus argentum species snake family boidae demic bahamas species discovered conception island bank comprises uninhabited islets noisy skd conception bank silver boa chilabothrus argentum species nonvenomous boa endemic bahamas discovered conception island bank uninhabited islet bahamas table gold reference summaries automatic summaries produced distilled student cnn dailymail xsum datasets cnn dailymail noisy skd succinct inform fluent xsum noisy skd succinct inform fluent noisy skd company film animal table human evaluation cnn dailymail xsum wikicatsum test sets noisy skd trained self knowledge distillation noisy signals noisy data pairwise ences systems signicant p paired t test cnn dailymail xsum human participants presented output systems original document asked cide better according lowing criteria succinctness summary avoid repetition informativeness mary capture document s important formation fluency summary uent grammatical evaluation conducted amazon mechanical turk crowdsourcing platform test documents total liu lapata cnn dailymail xsum elicited ve sponses hit systems rated dimension assigned score corresponding proportion times system selected better human evaluation results shown table upper datasets participants ceive student noisy skd signicantly p succinct informative pared teacher fluency student tends worse spection found student summaries telegraphic hypothesize crowdworkers tend penalize terms uency grammatical human evaluation performed slightly ferent recall document dataset input documents continuous webpage fragments allow pants perform experiment timely ion gold summary proxy content input crowdworkers presented output systems noisy skd asked decide better according formation contained gold summary uation conducted amt randomly lected samples test set elicited responses hit domain port proportion times system chosen better human evaluation results shown table lower amt crowdworkers prefer maries produced student animal film domains company found distilled model tends generate tities sentence render summaries dense domain conclusions paper advocated use knowledge distillation abstractive tion means alleviate problems associated maximum likelihood training task introduced noise functions training signal training data help larize training boost performance periments benchmark datasets strate framework improve pretrained pretrained summarizers ture like investigate thoroughly aspects pretrained models improve self knowledge distillation enhanced sophisticated noise functions references sungsoo ahn shell xu hu andreas c damianou neil d lawrence zhenwen dai ational information distillation knowledge ieee conference computer vision fer pattern recognition cvpr pages long beach california chine learning pages new york new york asli celikyilmaz antoine bosselut xiaodong yejin choi deep communicating agents proceedings abstractive summarization conference north american chapter association computational linguistics man language technologies volume long pers pages new orleans louisiana kevin clark minh thang luong urvashi wal christopher d manning quoc v le bam born multi task networks natural language understanding proceedings annual meeting association tional linguistics pages florence italy association computational linguistics wojciech m czarnecki simon osindero max berg grzegorz swirszcz razvan pascanu sobolev training neural networks guyon u v luxburg s bengio h wallach r fergus s vishwanathan r garnett editors advances neural information processing systems pages curran associates inc jacob devlin ming wei chang kenton lee kristina toutanova bert pre training deep bidirectional transformers language proceedings conference standing north american chapter association computational linguistics human language technologies volume long short papers pages minneapolis minnesota ation computational linguistics bin dong jikai hou yiping lu zhihua zhang distillation early stopping harvesting dark knowledge utilizing anisotropic information neurips workshop machine trieval learning guarantees vancouver canada jimmy ba rich caruana deep nets advances neural ally need deep formation processing systems pages curran associates inc maha elbayad laurent besacier jakob verbeek token level sequence level loss arxiv preprint ing rnn language models hangbo bao li dong furu wei wenhui wang nan yang xiaodong liu yu wang songhao piao feng gao ming zhou hsiao wuen hon pseudo masked language models ed language model pre training arxiv preprint cristian bucilu rich caruana alexandru niculescu mizil model compression ceedings acm sigkdd international conference knowledge discovery data ing kdd page new york ny usa association computing machinery samuel rota lorenzo porzi peter kontschieder dropout distillation ceedings international conference tommaso furlanello zachary lipton michael nen laurent itti anima anandkumar born neural networks international ference machine learning pages stockholm sweden sebastian gehrmann yuntian deng alexander rush abstractive summarization proceedings conference cal methods natural language processing pages brussels belgium akhilesh gotmare nitish shirish keskar caiming xiong richard socher closer look deep learning heuristics learning rate restarts warmup distillation international ference learning representations iclr new orleans la usa view net max grusky mor naaman yoav artzi newsroom dataset million summaries diverse extractive strategies proceedings conference north american chapter association computational linguistics man language technologies volume long pers pages new orleans louisiana sociation computational linguistics sangchul hahn heeyoul choi knowledge distillation natural language ing proceedings international conference recent advances natural language ing ranlp pages varna bulgaria incoma ltd donna harman paul effects human variation duc summarization text summarization branches pages tion barcelona spain association tional linguistics junxian jiatao gu jiajun shen marcaurelio revisiting self training arxiv preprint sequence generation ranzato neural karl moritz hermann tomas kocisky edward stette lasse espeholt kay mustafa suleyman phil blunsom teaching machines read comprehend c cortes n d lawrence d d lee m sugiyama r garnett editors advances neural information processing systems pages curran associates inc geoffrey hinton oriol vinyals jeff dean distilling knowledge neural network arxiv preprint yoon kim alexander m rush level knowledge distillation proceedings conference empirical methods ral language processing pages austin texas association computational linguistics wojciech kryscinski nitish shirish keskar bryan cann caiming xiong richard socher neural text summarization critical evaluation arxiv preprint adhiguna kuncoro miguel ballesteros lingpeng kong chris dyer noah smith ing ensemble greedy dependency parsers mst parser proceedings ference empirical methods natural language processing pages austin texas ciation computational linguistics comprehension proceedings nual meeting association computational linguistics pages online association computational linguistics chin yew lin rouge package text matic evaluation summaries rization branches proceedings workshop pages barcelona spain tion computational linguistics peter j liu mohammad saleh etienne pot ben goodrich ryan sepassi lukasz kaiser noam shazeer generating wikipedia ing long sequences proceedings national conference learning representations vancouver canada xiaodong liu pengcheng weizhu chen feng gao multi task deep neural networks natural language understanding proceedings annual meeting association putational linguistics pages florence italy association computational linguistics yang liu mirella lapata text proceedings tion pretrained encoders conference empirical methods ural language processing international joint conference natural language processing emnlp ijcnlp pages hong kong china association computational linguistics christopher manning mihai surdeanu john bauer jenny finkel steven bethard david mcclosky stanford corenlp natural language proceedings annual cessing toolkit meeting association computational guistics system demonstrations pages timore maryland association computational linguistics joshua maynez shashi narayan bernd bohnet ryan mcdonald faithfulness ality abstractive summarization proceedings annual meeting association computational linguistics pages line association computational linguistics seyed iman mirzadeh mehrdad farajtabar ang li hassan ghasemzadeh improved edge distillation teacher assistant bridging gap student teacher arxiv preprint hossein mobahi mehrdad farajtabar peter l bartlett bartlett self distillation es regularization hilbert space arxiv preprint mike lewis yinhan liu naman goyal jan ghazvininejad abdelrahman mohamed omer levy veselin stoyanov luke zettlemoyer bart denoising sequence sequence training natural language generation translation lili mou ran jia yan xu ge li lu zhang zhi jin distilling word embeddings ing approach proceedings acm national conference information edge management pages ramesh nallapati bowen zhou cicero dos santos caglar gulcehre bing xiang tive text summarization sequence sequence proceedings rnns signll conference computational natural guage learning pages berlin germany association computational linguistics shashi narayan shay b cohen mirella lapata nt details summary topic aware convolutional neural networks proceedings treme summarization conference empirical methods natural guage processing pages brussels gium association computational linguistics ani nenkova summarization evaluation text speech issues approaches ninth tional conference spoken language processing emilio parisotto jimmy ba ruslan salakhutdinov actor mimic deep multitask transfer inforcement learning corr romain paulus caiming xiong richard socher deep reinforced model abstractive marization proceedings international conference learning representations ver canada laura perez beltrachini yang liu mirella lapata generating summaries topic templates structured convolutional decoders ings annual meeting association computational linguistics pages florence italy association computational guistics mary phuong christoph lampert understanding knowledge distillation ings international conference chine learning pages long beach ifornia colin raffel noam shazeer adam roberts katherine lee sharan narang michael matena yanqi zhou wei li peter j liu exploring limits transfer learning unied text text arxiv e prints gj rath resnick tr savage tion abstracts selection sentences sentence selection men machines american documentation adriana romero nicolas ballas samira ebrahimi hou antoine chassang carlo gatta yoshua bengio fitnets hints thin deep nets arxiv preprint alexander m rush sumit chopra jason weston neural attention model abstractive proceedings tence summarization conference empirical methods natural guage processing pages lisbon portugal association computational linguistics evan sandhaus new york times annotated corpus linguistic data consortium philadelphia abigail peter j liu christopher d manning point summarization generator networks proceedings nual meeting association computational linguistics volume long papers pages vancouver canada kaiqiang song lin zhao fei liu infused copy mechanisms abstractive tion proceedings international ference computational linguistics pages santa fe new mexico usa association computational linguistics kaitao song xu tan tao qin jianfeng lu yan liu mass masked sequence quence pre training language generation ceedings international conference machine learning pages long beach nia jiwei tan xiaojun wan jianguo xiao abstractive document summarization proceedings based attentional neural model annual meeting association computational linguistics volume long papers pages vancouver canada association computational linguistics xu tan yi ren di tao qin zhou zhao tie yan liu multilingual neural machine corr translation knowledge distillation yee teh victor bapst wojciech m czarnecki john quan james kirkpatrick raia hadsell nicolas heess razvan pascanu distral robust multitask reinforcement learning guyon u v luxburg s bengio h wallach r fergus s wanathan r garnett editors advances ral information processing systems pages curran associates inc gregor urban krzysztof j geras samira ebrahimi kahou ozlem aslan shengjie wang rich ana abdelrahman mohamed matthai philipose matt richardson deep convolutional nets need deep convolutional arxiv preprint ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan n gomez ukasz kaiser illia polosukhin attention need guyon u v luxburg s bengio h wallach r fergus s vishwanathan r nett editors advances neural information cessing systems pages curran ciates inc yonghui wu mike schuster zhifeng chen quoc v le mohammad norouzi wolfgang macherey maxim krikun yuan cao qin gao klaus macherey et al google s neural machine translation system bridging gap arxiv preprint man machine translation qizhe xie eduard hovy minh thang luong quoc v le self training noisy student arxiv preprint improves imagenet classication chenglin yang lingxi xie siyuan qiao alan yuille training deep neural networks erations tolerant teacher educates better dents proceedings aaai conference articial intelligence volume pages honolulu hawaii sergey zagoruyko nikos komodakis ing attention attention improving formance convolutional neural networks tion transfer proceedings international conference learning represenatations toulon france dongxu zhang zhichao yang word ding perturbation sentence classication arxiv preprint
