contrastive attention mechanism abstractive sentence summarization xiangyu hongfei mingming min weihua yue institute articial intelligence soochow university suzhou china school computer science technology soochow university suzhou china alibaba damo academy hangzhou china school engineering westlake university china edu cn hfyu suda edu cn edu cn weihua inc com yue org abstract propose contrastive attention mechanism extend sequence sequence work abstractive sentence summarization task aims generate brief summary given source sentence proposed trastive attention mechanism accommodates categories attention tional attention attends relevant parts source sentence opponent attention attends irrelevant evant parts source sentence tentions trained opposite way contribution conventional tion encouraged contribution opponent attention discouraged novel softmax softmin functionality periments benchmark datasets proposed contrastive attention mechanism focused relevant parts summary conventional attention mechanism greatly advances art performance abstractive tence summarization task release code com travel abstractive text summarization introduction abstractive sentence summarization aims erating concise informative summaries based core meaning source sentences ous endeavors tackle problem dorr et al rule based methods tistical models trained relatively small scale training corpora banko et al following successful applications machine translation sutskever et al bahdanau et al sequence sequence framework applied abstractive sentence summarization task ing large scale sentence summary corpora rush et al chopra et al nallapati et al equal contribution obtaining better performance compared traditional methods central component state art quence sequence models use tion building connections source sequence target words formed decision generating word considering relevant parts source sequence bahdanau et al vaswani et al abstractive sentence summarization attention mechanisms useful selecting salient words short summary ltering negative ence redundant parts consider improving abstractive tion quality enhancing target source tion particular contrastive mechanism taken encouraging contribution conventional attention attends relevant parts source sentence time nalizing contribution opponent tion attends irrelevant relevant parts contrastive attention rst proposed puter vision song et al person identication attending person background regions contrastively edge rst use contrastive attention nlp deploy sequence sequence framework particular transformer vaswani et al baseline summarization model enhance proponent attention ule opponent attention module mer acts conventional attention mechanism regarded dual module similar weight calculation ture novel softmin function age contributions irrelevant relevant words knowledge rst investigate t c o l c s c v v x r transformer sequence sequence rizer results benchmark datasets gives highly competitive accuracies pared rnn cnn alternatives equipped proposed contrastive attention mechanism transformer model achieves best reported results data visualization attentions shows trastive attention mechanism attention focused relevant parts baseline lease code xxx related work automatic summarization investigated main paradigms extractive method abstractive method extracts important pieces source document catenates sequentially jing mckeown knight marcu neto et al grasps core meaning source text state short text tive summary banko et al rush et al paper focus abstractive marization especially abstractive sentence summarization previous work deals abstractive tence summarization task rule based methods dorr et al statistical methods utilizing source summary parallel pus train machine translation model banko et al syntax based transduction model cohn lapata woodsend et al recent years sequence sequence neural framework predominant task encoding long source texts decoding short summaries attention anism rnn commonly adopted extensively explored architecture chopra et al nallapati et al li et al cnn based architecture recently employed gehring et al plies cnn encoder decoder later wang et al build topic words embedding encoding train tem reinforcement learning related work contrastive tion mechanism eld computer vision song et al rst propose contrastive attention mechanism person identication work based pre provided person background segmentation regions contrastively attended comparison apply ily discriminated contrastive attention mechanism sentence level summarization contrastively attending evant parts irrelevant relevant parts furthermore propose novel softmax min functionality train attention mechanism different song et al use mean squared error loss attention training explorations respect teristics abstractive summarization task clude copying mechanism copies words source sequences composing summaries gu et al gulcehre et al song et al selection mechanism elaborately selects important parts source sentences zhou et al lin et al distraction anism avoids repeated attention area chen et al sequence level training avoids exposure bias teacher ing methods ayana et al li et al edunov et al methods built conventional attention orthogonal proposed contrastive attention mechanism approach use categories attention summary generation conventional attention attends relevant parts source sentence opponent attention contrarily tends irrelevant relevant parts gories attention output probability distributions summary words jointly optimized encouraging contribution tional attention discouraging contribution opponent attention figure illustrates overall networks use transformer architecture basis build contrastive attention mechanism left original transformer derive opponent attention conventional tention encoder decoder attention original transformer stack layers opponent attention shown right figure parts contribute summary generation producing probability tributions target vocabulary respectively left outputs conventional ity based conventional attention inal transformer right outputs opponent probability based opponent q k v denotes query vector key vectors value vectors respectively denotes mension vector k softmax function outputs attention weights distributed v k v vector weighted sum elements v represents current context information focus encoder decoder attention builds connection source target informing decoder area source text attended specically encoder decoder attention q single vector coming current position coder k v sequence vectors outcomes encoder source positions softmax function distributes tion weights source positions attentions transformer adopts head implementation head putes attention equation smaller q k v dimension h times original dimension respectively attentions h heads concatenated early projected compose nal attention way multi head attention provides view attention behavior benecial nal performance deep layers n plates figure stands stacked n identical layers source layer stacked n layers contains sublayers self attention mechanism fully nected feed forward network sublayer ploys residual connection adds input come sublayer layer normalization ployed outcome residual connection target summary layer contains additional sublayer encoder decoder tention self attention sublayer feed forward sublayer decoder softmax layer applied convert decoder output summary word generation probabilities contrastive attention mechanism opponent attention illustrated figure opponent attention derived conventional encoder decoder attention multi head attention ployed transformer nh heads tal conventional encoder decoder attention n denotes number layers h denotes figure overall networks left original transformer right takes opponent tention layer fulls contrastive attention mechanism attention probabilities figure jointly optimized novel way explained section transformer abstractive sentence summarization architecture attention network based transformer sequence sequence vaswani et al encodes source text hidden vectors decodes target text based source information target generation history comparison rnn based architecture cnn based architecture encoder decoder transformer adopt attention main function let x y denote source sentence summary respectively transformer trained maximize probability y given x x x conventional probability current summary word yi given source sentence mary generation history pc computed based attention mechanism stacked deep ers shown left figure attention mechanism scaled dot product attention applied k v softmax v qkt dk sourceembeddingtargetembeddinglinearsoftmaxfeedforwardadd normadd normselfattentionencoderdecoderattentionadd normfeedforwardadd normadd normselfattentionattentionindicesconventional probabilitiesnxnxfeedforwardadd normlinearsoftminopponentattentionnormopponent probabilitiestransformercontrastive attention mechanism innity value softmax function puts zero given negative innity value input maximum weight c set zero o opponent softmax functions way relevant source quence receives maximum attention conventional attention weights c masked neglected o instead remaining vant irrelevant parts extracted o following contrastive training decoding tried methods calculate o opponent attention weights c song et al o c aims o contrary c underperform masking nent function benchmark datasets present masking opponent ing sections o obtained equation ponent attention attentiono v head q computing c compared conventional attention attentionc summarizes current relevant context attentiono summarizes current irrelevant relevant context constitute trastive pair contribute nal summary word generation opponent probability opponent probability x puted stacking layers attentiono softmin layer end shown right figure lar x w matrix linear projection layer attentiono contributes po equation step step layernorm feedforward layers residual connection similar al directly let o extracting background features person identication computer vision add softmax function attention weights normalized sequence sequence framework figure heatmaps sampled heads conventional encoder decoder attention fth head layer fth head rst layer number heads layer heads exhibit diverse attention behaviors posing lenge determining head derive ponent attention attends irrelevant relevant parts figure illustrates attention weights sampled heads attention weights reect word level relevant relation source sentence target summary tention weights nd behavior characteristic head xed example head exhibits relevant lation different sentences different runs based depicting heatmaps heads sentences choose head corresponds relevant relation source target derive opponent attention specically let c denote conventional encoder decoder attention weights head deriving opponent attention c softmax qkt dk q k head c let o denote opponent attention weights obtained opponent function applied c followed softmax function o opponent function equation performs masking operation nds maximum weight c replaces negative manual alignments source target sampled sentence summary pairs select head lowest alignment error rate aer attention weights b original transformer novel softmin tion introduced end invert tion attentiono v w e input vector min function equation softmin normalizes v scores words summary cabulary sum bigger vi smaller po softmin functions contrarily softmax result try maximize x y gold summary word effectively search appropriate attentiono generate lowest vg g index y v means irrelevant attentiono summary lower vg obtained resulting higher po training decoding training jointly maximize tional probability pc opponent probability po j x x balanced weight conventional probability computed original basing sublayers feed forward linear projection softmax stacked conventional attention illustrated left figure opponent probability based similar sublayers stacked opponent tion softmin sublayer trated right figure contrary properties softmax softmin jointly maximizing pc po actually maximizes contribution conventional attention summary word generation time minimizes contribution opponent words ing objective let relevant attended tried replacing softmin equation max correspondingly setting training objective maximizing j method failed train po small training results negative innity value hampers training comparison softmin training objective equation problem enabling effective training proposed network attentionc contribute tion let irrelevant relevant parts attended attentiono contribute decoding aim nd maximum j equation beam search process experiments conduct experiments abstractive sentence summarization benchmark datasets demonstrate effectiveness proposed contrastive tion mechanism datasets paper evaluate proposed method abstractive text summarization benchmark datasets use annotated gigaword corpus preprocess identically rush et al results m ing samples k validation samples test samples evaluation source summary pairs formed pairing rst sentence article headline use english data set testing experiments contains documents containing human generated reference maries length summary capped bytes data set large corpus chinese short text summarization lcsts hu et al collected chinese microblogging website sina weibo follow data split original paper m summary pairs rst corpus training pairs high notation score testing experimental setup employ transformer basis layers stacked encoder coder dimensions embedding tors hidden vectors set inner layer feed forward sublayer sionality set heads head attention source embedding target embedding linear sublayer shared experiments byte pair encoding employed english experiment shared target vocabulary tokens sennrich et al contrastive attention mechanism opponent attention derived head com pytorch fairseq system abs rush et al rush et al ras elman chopra et al words nallapati et al seassbeam zhou et al rnnmrt ayana et al actor critic li et al structuredloss edunov et al drgd li et al gehring et al wang et al factaware cao et al transformer gigaword r l r l table rouge scores english evaluation sets gigaword gigaword length based rouge scores reported recall based rouge scores reported denotes score available work attention synchronous word alignments source summary pair experiments select fth head layer deriving opponent attention glish experiments select second head layer chinese experiments mensions contrastive architecture set equation tuned development set experiment training use adam optimizer initial learning rate inverse square root schedule applied initial warm ing vaswani et al training use dropout rate datasets evaluation employ rouge lin evaluation metric dard rouge package evaluate glish summarization systems follow method hu et al map chinese words numerical ids order evaluate mance chinese data set results english results experimental results english evaluation sets listed table report length scores rouge l r l evaluation set annotated gigaword report recall based scores r l evaluation set follow setting ous works results works shown tom table performances lated works reported upper ble comparison abs pioneer works neural models stractive text summarization ras elman extends abs attentive cnn encoder uses large vocabulary linguistic features pos ner tags rnnmrt actor critic structuredloss sequence level training methods overcome problem usual teacher forcing methods drgd uses current latent random model improve rization quality factaware generates summary words conditioned source text fact descriptions extracted openie dencies rnn based related works cnn based architectures included son table shows build strong line transformer obtains state art performance gigaword tion set obtains comparable performance state art troduce contrastive attention mechanism transformer signicantly improves mance transformer greatly advances state art gigaword evaluation set shown row attention chinese results table presents evaluation results sts upper rows list performances related works rows list system rnn context hu et al copynet gu et al rnnmrt ayana et al rnndistraction chen et al drgd li et al actor critic li et al global lin et al transformer r l table length based rouge scores chinese evaluation set lcsts mances transformer baseline gration contrastive attention mechanism transformer character sequences source summary pairs evaluate formance based reference characters strict comparison related works table shows transformer sets strong baseline lcsts surpasses formances previous works equipped proposed contrastive attention mechanism performance icantly improved drastically advances state art lcsts analysis discussion effect contrastive attention mechanism attentions figure shows attention weights contrastive attention mechanism depict averaged attention weights heads layer figure study contributes conventional ity computation depict opponent attention weights figure study contribution opponent probability select fth head layer derive opponent tion english experiment studies carried layer figure baseline transformer ure transformer tion transformer trastiveattention focused source parts relevant summary baseline transformer scatters attention weights summary word neighbors tional words contrastiveattention cancels tered attentions contrastive attention mechanism figure attention weight changes ing contrastive attention mechanism average attention weights layer baseline transformer c opponent attention derived fth head layer figure depicts opponent attention weights optimized training generate lowest score fed min highest opponent probability po irrelevant summary word ponent lower score obtained resulting higher po figure shows tentions formed irrelevant parts ied weights result maximizing po training effect opponent probability decoding study contribution opponent ability po dropping decoding hurts performance table shows dropping po signicantly harms performance transformer contrastiveatt mance difference model dropping po baseline transformer marginal ing adding opponent probability po key achieving performance improvement explorations deriving opponent attention masking attention weights deriving opponent attention c system mask maximum weight mask weights mask weights dynamically mask synchronous head non synchronous head averaged head transformer baseline gigaword r l r l table results explorations opponent attention derivation upper presents inuence masking attention weights deriving opponent attention middle presents results ing different head opponent attention derivation row presents result transformer gigaword transformer po transformer po r l r l table effect dropping po denoted decoding section mask salient word maximum weight c derive opponent attention subsection mented masking weights c ways masking k weights dynamically masking dynamically masking method order weights big small rst masking neighbors ratio threshold threshold based training tuning development set upper rows table presents formance comparison masking maximum weight masking weights shows masking maximum weight performs better cating masking salient weight leaves irrelevant relevant words pute opponent probability po reliable computed remaining words masking weights selecting non synchronous head averaged head deriving opponent attention explained section opponent tention derived head chronous word alignments source sentence summary denote chronous head explored deriving opponent attention fth head rst layer non synchronous word ments illustrated figure result sented non synchronous head row dition attention weights averaged heads layer derive opponent attention denote averaged head shown middle table non synchronous head averaged head underperform synchronous head synchronous head performs worst worse transformer baseline gigaword indicates better compose ponent attention irrelevant parts easily located synchronous head averaged head performs slightly worse synchronous head slower involved heads qualitative study contrast table shows qualitative results lights baseline transformer manifest incorrect areas extracted baseline tem highlights correct contents extracted contrastive system guish relevant parts irrelevant parts source attending correct areas easily conclusion proposed contrastive attention mechanism abstractive sentence summarization conventional attention attends evant parts source sentence novel opponent attention attends irrelevant relevant parts summary word ation categories attention constitute contrastive pair encourage contribution conventional attention penalize src press freedom algeria remains risk despite release wednesday prominent newspaper editor mohamed unk year prison sentence human rights organizations said ref algerian press freedom risk despite editor s release unk picture transformer press freedom remains risk algeria rights groups press freedom remains risk despite release algerian editor src denmark s poul erik hoyer completed hat trick men s singles badminton titles european championships winning nal saturday ref hoyer wins singles title transformer hoyer completes hat trick hoyer wins men s singles title src french bank credit agricole launched tuesday public cash offer buy percent emporiki bank bid valuing greek group billion euros billion dollars ref credit agricole announces euro bid greek bank emporiki transformer credit agricole launches public cash offer greek bank french bank credit agricole bids billion euros greek bank table example summaries generated baseline transformer tribution opponent attention joint training transformer strong baseline experiments benchmark data sets proposed contrastive attention mechanism signicantly improves performance ing state art performance task acknowledgments authors like thank anonymous reviewers helpful comments work supported national key program china grant national natural science foundation china grant references ayana shiqi shen yu zhao zhiyuan liu maosong sun neural headline generation sentence wise optimization computer search repository version dzmitry bahdanau kyunghyun cho yoshua gio neural machine translation jointly learning align translate international conference learning representations michele banko vibhu o mittal michael j brock headline generation based cal translation proceedings annual meeting association computational tics pages ziqiang cao furu wei wenjie li sujian li faithful original fact aware neural tive summarization thirty second aaai ence articial intelligence qian chen xiao dan zhu zhen hua ling si wei hui jiang distraction based neural proceedings works modeling document fifth international joint conference articial intelligence pages sumit chopra michael auli alexander m rush abstractive sentence summarization tentive recurrent neural networks proceedings conference north american ter association computational linguistics human language technologies pages trevor cohn mirella lapata sentence proceedings pression word deletion international conference tional linguistics volume pages bonnie dorr david zajic richard schwartz hedge trimmer parse trim approach headline generation proceedings naacl text summarization workshop volume pages sergey edunov myle ott michael auli david ier marcaurelio ranzato classical structured prediction losses sequence quence learning proceedings ference north american chapter ciation computational linguistics human guage technologies pages jonas gehring michael auli david grangier nis yarats yann n dauphin tional sequence sequence learning ings international conference chine learning pages jiatao gu zhengdong lu li hang victor o k incorporating copying mechanism li proceedings sequence sequence learning annual meeting association putational linguistics caglar gulcehre sungjin ahn ramesh nallapati bowen zhou yoshua bengio pointing unknown words proceedings nual meeting association computational linguistics baotian hu qingcai chen fangze zhu sts large scale chinese short text summarization dataset proceedings conference kaiqiang song lin zhao fei liu structure infused copy mechanisms abstractive proceedings summarization national conference computational linguistics pages ilya sutskever oriol vinyals quoc v le sequence sequence learning neural works advances neural information ing systems pages ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan n gomez ukasz kaiser illia polosukhin attention need advances neural information cessing systems pages li wang junlin yao yunzhe tao li zhong wei liu qiang du reinforced topic aware convolutional sequence sequence model stractive text summarization proceedings seventh international joint conference articial intelligence pages kristian woodsend yansong feng mirella ata title generation quasi synchronous grammar proceedings conference empirical methods natural language ing pages qingyu zhou yang nan furu wei zhou ming selective encoding abstractive sentence summarization proceedings annual meeting association computational guistics pages empirical methods natural language ing pages hongyan jing kathleen r mckeown cut paste based text summarization ings north american chapter sociation computational linguistics conference pages kevin knight daniel marcu based summarization step sentence proceedings seventeenth national sion conference articial intelligence twelfth conference innovative applications cial intelligence pages piji li lidong bing wai lam critic based training framework abstractive summarization computing research repository piji li wai lam lidong bing zihao wang deep recurrent generative decoder abstractive proceedings text summarization conference empirical methods natural guage processing pages chin yew lin rouge package automatic proc evaluation summaries workshop text summarization branches junyang lin sun xu shuming ma su qi global encoding abstractive summarization proceedings annual meeting sociation computational linguistics pages ramesh nallapati bowen zhou cicero nogueira dos santos caglar gulcehre xiang bing abstractive text summarization sequence proceedings sequence rnns signll conference computational ral language learning pages joel larocca neto alex freitas celso kaestner automatic text summarization ing machine learning approach brazilian posium articial intelligence pages alexander m rush sumit chopra jason weston neural attention model abstractive proceedings tence summarization conference empirical methods natural guage processing pages rico sennrich barry haddow alexandra birch neural machine translation rare words subword units proceedings annual meeting association computational guistics chunfeng song yan huang wanli ouyang liang wang mask guided contrastive attention proceedings model person identication ieee conference computer vision pattern recognition pages
