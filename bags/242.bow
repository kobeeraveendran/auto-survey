contrastive attention mechanism for abstractive sentence summarization xiangyu hongfei mingming min weihua yue institute of articial intelligence soochow university suzhou china school of computer science and technology soochow university suzhou china alibaba damo academy hangzhou china school of engineering westlake university china edu cn hfyu suda edu cn edu cn weihua inc com yue org abstract we propose a contrastive attention mechanism to extend the sequence to sequence work for abstractive sentence summarization task which aims to generate a brief summary of a given source sentence the proposed trastive attention mechanism accommodates two categories of attention one is the tional attention that attends to relevant parts of the source sentence the other is the opponent attention that attends to irrelevant or less evant parts of the source sentence both tentions are trained in an opposite way so that the contribution from the conventional tion is encouraged and the contribution from the opponent attention is discouraged through a novel softmax and softmin functionality periments on benchmark datasets show that the proposed contrastive attention mechanism is more focused on the relevant parts for the summary than the conventional attention mechanism and greatly advances the of the art performance on the abstractive tence summarization task we release the code at com travel abstractive text summarization introduction abstractive sentence summarization aims at erating concise and informative summaries based on the core meaning of source sentences ous endeavors tackle the problem through either dorr et al or rule based methods tistical models trained on relatively small scale training corpora banko et al following its successful applications on machine translation sutskever et al bahdanau et al the sequence to sequence framework is also applied on the abstractive sentence summarization task ing large scale sentence summary corpora rush et al chopra et al nallapati et al equal contribution obtaining better performance compared to the traditional methods one central component in state of the art quence to sequence models is the use of tion for building connections between the source sequence and target words so that a more formed decision can be made for generating a get word by considering the most relevant parts of the source sequence bahdanau et al vaswani et al for abstractive sentence summarization such attention mechanisms can be useful for selecting the most salient words for a short summary while ltering the negative ence of redundant parts we consider improving abstractive tion quality by enhancing target to source tion in particular a contrastive mechanism is taken by encouraging the contribution from the conventional attention that attends to relevant parts of the source sentence while at the same time nalizing the contribution from an opponent tion that attends to irrelevant or less relevant parts contrastive attention was rst proposed in puter vision song et al which is used for person re identication by attending to person and background regions contrastively to our edge we are the rst to use contrastive attention for nlp and deploy it in the sequence to sequence framework in particular we take transformer vaswani et al as the baseline summarization model and enhance it with a proponent attention ule and an opponent attention module the mer acts as the conventional attention mechanism while the latter can be regarded as a dual module to the former with similar weight calculation ture but using a novel softmin function to age contributions from irrelevant or less relevant words to our knowledge we are the rst to investigate t c o l c s c v v i x r a transformer as a sequence to sequence rizer results on three benchmark datasets show that it gives highly competitive accuracies pared with rnn and cnn alternatives when equipped with the proposed contrastive attention mechanism our transformer model achieves the best reported results on all data the visualization of attentions shows that through using the trastive attention mechanism our attention is more focused on relevant parts than the baseline we lease our code at xxx related work automatic summarization has been investigated in two main paradigms the extractive method and the abstractive method the former extracts important pieces of source document and catenates them sequentially jing and mckeown knight and marcu neto et al while the latter grasps the core meaning of the source text and re state it in short text as tive summary banko et al rush et al in this paper we focus on abstractive marization and especially on abstractive sentence summarization previous work deals with the abstractive tence summarization task by using either rule based methods dorr et al or statistical methods utilizing a source summary parallel pus to train a machine translation model banko et al or a syntax based transduction model cohn and lapata woodsend et al in recent years sequence to sequence neural framework becomes predominant on this task by encoding long source texts and decoding into short summaries together with the attention anism rnn is the most commonly adopted and extensively explored architecture chopra et al nallapati et al li et al a cnn based architecture is recently employed by gehring et al using which plies cnn on both encoder and decoder later wang et al build upon with topic words embedding and encoding and train the tem with reinforcement learning the most related work to our contrastive tion mechanism is in the eld of computer vision song et al rst propose the contrastive attention mechanism for person re identication in their work based on a pre provided person and background segmentation the two regions are contrastively attended so that they can be in comparison we apply the ily discriminated contrastive attention mechanism for sentence level summarization by contrastively attending to evant parts and irrelevant or less relevant parts furthermore we propose a novel softmax min functionality to train the attention mechanism which is different to song et al who use mean squared error loss for attention training other explorations with respect to the teristics of the abstractive summarization task clude copying mechanism that copies words from source sequences for composing summaries gu et al gulcehre et al song et al the selection mechanism that elaborately selects important parts of source sentences zhou et al lin et al the distraction anism that avoids repeated attention on the same area chen et al and the sequence level training that avoids exposure bias in teacher ing methods ayana et al li et al edunov et al such methods are built on conventional attention and are orthogonal to our proposed contrastive attention mechanism approach we use two categories of attention for summary generation one is the conventional attention that attends to relevant parts of source sentence the other is the opponent attention that contrarily tends to irrelevant or less relevant parts both gories of attention output probability distributions over summary words which are jointly optimized by encouraging the contribution from the tional attention and discouraging the contribution from the opponent attention figure illustrates the overall networks we use transformer architecture as our basis upon which we build the contrastive attention mechanism the left part is the original transformer we derive the opponent attention from the conventional tention which is the encoder decoder attention of the original transformer and stack several layers on top of the opponent attention as shown in the right part of figure both parts contribute to the summary generation by producing probability tributions over the target vocabulary respectively the left part outputs the conventional ity based on the conventional attention as the inal transformer does while the right part outputs the opponent probability based on the opponent where q k v denotes query vector key vectors and value vectors respectively denotes the mension of one vector of k softmax function outputs the attention weights distributed over v k v is a vector of weighted sum of elements of v and represents current context information we focus on the encoder decoder attention which builds the connection between source and target by informing the decoder which area of the source text should be attended to specically in the encoder decoder attention q is the single vector coming from the current position of the coder k and v are the same sequence of vectors that are the outcomes of the encoder at all source positions softmax function distributes the tion weights over the source positions the attentions in transformer adopts the head implementation in which each head putes attention as equation but with smaller q k v whose dimension is h times of their original dimension respectively the attentions from h heads are concatenated together and early projected to compose the nal attention in this way multi head attention provides a view of attention behavior benecial for the nal performance deep layers the n plates in figure stands for the stacked n identical layers on the source side each layer of the stacked n layers contains two sublayers the self attention mechanism and the fully nected feed forward network each sublayer ploys residual connection that adds input to come of sublayer then layer normalization is ployed on the outcome of the residual connection on the target summary side each layer contains an additional sublayer of the encoder decoder tention between the self attention sublayer and the feed forward sublayer at the top of the decoder the softmax layer is applied to convert the decoder output to summary word generation probabilities contrastive attention mechanism opponent attention as illustrated in figure the opponent attention is derived from the conventional encoder decoder attention since the multi head attention is ployed in transformer there are nh heads in tal in the conventional encoder decoder attention where n denotes the number of layers h denotes figure overall networks the left part is the original transformer the right part that takes the opponent tention as bottom layer fulls the contrastive attention mechanism attention the two probabilities in figure are jointly optimized in a novel way as explained in section transformer for abstractive sentence summarization architecture is an attention network based transformer sequence to sequence vaswani et al which encodes the source text into hidden vectors and decodes into the target text based on the source side information and the target generation history in comparison to the rnn based architecture and the cnn based architecture both the encoder and the decoder of transformer adopt attention as main function i let x and y denote the source sentence and its summary respectively transformer is trained to maximize the probability of y given x x where x is the conventional probability of the current summary word yi given the source sentence and the mary generation history pc is computed based on the attention mechanism and the stacked deep ers as shown in the left part of figure attention mechanism scaled dot product attention is applied in former k v softmax v qkt dk sourceembeddingtargetembeddinglinearsoftmaxfeedforwardadd normadd normselfattentionencoderdecoderattentionadd normfeedforwardadd normadd normselfattentionattentionindicesconventional probabilitiesnxnxfeedforwardadd normlinearsoftminopponentattentionnormopponent probabilitiestransformercontrastive attention mechanism innity value so that the softmax function puts zero given the negative innity value input then the maximum weight in c is set zero in o after the opponent and softmax functions in this way the most relevant part of the source quence which receives maximum attention in the conventional attention weights c is masked and neglected in o instead the remaining less vant or irrelevant parts are extracted into o for the following contrastive training and decoding we also tried other methods to calculate the such as o opponent attention weights c song et al or o c which aims to make o contrary to c but they underperform the masking nent function on all benchmark datasets so we present only the masking opponent in the ing sections after o is obtained via equation the ponent attention is attentiono where v is from the head same to that of q and in computing c to the compared conventional attention attentionc which summarizes current relevant context attentiono summarizes current irrelevant or less relevant context they constitute a trastive pair and contribute together for the nal summary word generation opponent probability the opponent probability x is puted by stacking several layers on top of attentiono and a softmin layer in the end as shown in the right part of figure in lar x where w is the matrix of the linear projection layer attentiono contributes to po via equation step by step the layernorm and feedforward layers with residual connection is similar to the al directly let o in extracting background features for person re identication in computer vision we have to add softmax function since the attention weights must be normalized to one in sequence to sequence framework figure heatmaps of two sampled heads from the a is of the conventional encoder decoder attention fth head of the third layer and is of the fth head of the rst layer the number of heads in each layer these heads exhibit diverse attention behaviors posing a lenge of determining which head to derive the ponent attention so that it attends to irrelevant or less relevant parts figure illustrates the attention weights of two sampled heads the attention weights in well reect the word level relevant relation between the source sentence and the target summary while tention weights in do not we nd that such behavior characteristic of each head is xed for example head a always exhibits the relevant lation across different sentences and different runs based on depicting heatmaps of all heads for a few sentences we choose the head that corresponds well to the relevant relation between source and target to derive the opponent attention specically let c denote the conventional encoder decoder attention weights of the head which is used for deriving the opponent attention c softmax qkt dk where q and k are from the head same to that of c let o denote the opponent attention weights it is obtained through the opponent function applied on c followed by the softmax function o the opponent function in equation performs a masking operation which nds the maximum weight in c and replaces it with the negative manual alignments between source and target of sampled sentence summary pairs we select the head that has the lowest alignment error rate aer of its attention weights b original transformer while a novel softmin tion is introduced in the end to invert the tion from attentiono where v w i e the input vector to the min function in equation softmin normalizes v so that scores of all words in the summary cabulary sum to one we can see that the bigger the vi the smaller the po i is softmin functions contrarily to softmax as a result when we try to maximize x where y is the gold summary word we effectively search for an appropriate attentiono to generate the lowest vg where g is the index of y in v it means that the more irrelevant is attentiono to the summary the lower the vg can be obtained resulting in higher po training and decoding during training we jointly maximize the tional probability pc and the opponent probability po j x x where is the balanced weight the conventional probability is computed as the original former does basing on sublayers of feed forward linear projection and softmax stacked over the conventional attention as illustrated in the left part of figure the opponent probability is based on similar sublayers stacked over the opponent tion but with softmin as the last sublayer as trated in the right part of figure due to the contrary properties of softmax and softmin jointly maximizing pc and po actually maximizes the contribution from the conventional attention for summary word generation while at the same time minimizes the contribution from the opponent in other words the ing objective is to let the relevant part attended also tried replacing softmin in equation with max and correspondingly setting the training objective as maximizing j but this method failed to train because po becomes too small during training and results in negative innity value of that hampers the training in comparison softmin and the training objective of equation do not have such problem enabling the effective training of the proposed network by attentionc contribute more to the tion while let the irrelevant or less relevant parts attended by attentiono contribute less during decoding we aim to nd maximum j of equation in the beam search process experiments we conduct experiments on abstractive sentence summarization benchmark datasets to demonstrate the effectiveness of the proposed contrastive tion mechanism datasets in this paper we evaluate our proposed method on three abstractive text summarization benchmark datasets first we use the annotated gigaword corpus and preprocess it identically to rush et al which results in around m ing samples k validation samples and test samples for evaluation the source summary pairs are formed through pairing the rst sentence of each article with its headline we use as another english data set only for testing in our experiments it contains documents each containing four human generated reference maries the length of the summary is capped at bytes the last data set we used is a large corpus of chinese short text summarization lcsts hu et al which is collected from the chinese microblogging website sina weibo we follow the data split of the original paper with m summary pairs from the rst part of the corpus for training pairs from the last part with high notation score for testing experimental setup we employ transformer as our basis six layers are stacked in both the encoder and coder and the dimensions of the embedding tors and all hidden vectors are set the inner layer of the feed forward sublayer has the sionality of we set eight heads in the head attention the source embedding the target embedding and the linear sublayer are shared in our experiments byte pair encoding is employed in the english experiment with a shared target vocabulary of about tokens sennrich et al regarding the contrastive attention mechanism the opponent attention is derived from the head com pytorch fairseq system abs rush et al rush et al ras elman chopra et al words nallapati et al seassbeam zhou et al rnnmrt ayana et al actor critic li et al structuredloss edunov et al drgd li et al gehring et al wang et al factaware cao et al transformer gigaword r l r l table rouge scores on the english evaluation sets of both gigaword and on gigaword the length based rouge scores are reported on the recall based rouge scores are reported denotes no score is available in that work whose attention is most synchronous to word alignments of the source summary pair in our experiments we select the fth head of the third layer for deriving the opponent attention in the glish experiments and select the second head of the third layer in the chinese experiments all mensions in the contrastive architecture are set the in equation is tuned on the development set in each experiment during training we use the adam optimizer with the initial learning rate is the inverse square root schedule is applied for initial warm up and ing vaswani et al during training we use a dropout rate of on all datasets during evaluation we employ rouge lin as our evaluation metric since dard rouge package is used to evaluate the glish summarization systems we also follow the method of hu et al to map chinese words into numerical ids in order to evaluate the mance on the chinese data set results english results the experimental results on the english evaluation sets are listed in table we report the full length scores of and rouge l r l on the evaluation set of the annotated gigaword while report the recall based scores of the and r l on the evaluation set of to follow the setting of the ous works the results of our works are shown at the tom of table the performances of the lated works are reported in the upper part of ble for comparison abs and are the pioneer works of using neural models for stractive text summarization ras elman extends abs with attentive cnn encoder uses large vocabulary and linguistic features such as pos and ner tags rnnmrt actor critic structuredloss are sequence level training methods to overcome the problem of the usual teacher forcing methods drgd uses current latent random model to improve rization quality factaware generates summary words conditioned on both the source text and the fact descriptions extracted from openie or dencies besides the above rnn based related works cnn based architectures of and are included for son table shows that we build a strong line using transformer alone which obtains the state of the art performance on gigaword tion set and obtains comparable performance to the state of the art on when we troduce the contrastive attention mechanism into transformer it signicantly improves the mance of transformer and greatly advances the state of the art on both gigaword evaluation set and as shown in the row of attention chinese results table presents the evaluation results on sts the upper rows list the performances of the related works the bottom rows list the system rnn context hu et al copynet gu et al rnnmrt ayana et al rnndistraction chen et al drgd li et al actor critic li et al global lin et al transformer r l table the full length based rouge scores on the chinese evaluation set of lcsts mances of our transformer baseline and the gration of the contrastive attention mechanism into transformer we only take character sequences as source summary pairs and evaluate the formance based on reference characters for strict comparison to the related works table shows that transformer also sets a strong baseline on lcsts that surpasses the formances of the previous works when former is equipped with our proposed contrastive attention mechanism the performance is icantly improved and drastically advances the state of the art on lcsts analysis and discussion effect of the contrastive attention mechanism on attentions figure shows the attention weights before and after using the contrastive attention mechanism we depict the averaged attention weights of all heads in one layer in figure and to study how it contributes to the conventional ity computation and depict the opponent attention weights in figure to study its contribution to the opponent probability since we select the fth head of the third layer to derive the opponent tion in english experiment the studies are carried out on the third layer figure is from the baseline transformer ure is from transformer tion we can see that transformer trastiveattention is more focused on the source parts that are most relevant to the summary than the baseline transformer which scatters attention weights on summary word neighbors or even tional words such as and the former contrastiveattention cancels such tered attentions by using the contrastive attention mechanism figure the attention weight changes by ing the contrastive attention mechanism is the average attention weights of the third layer of the baseline transformer is that of and c is the opponent attention derived from the fth head of the third layer figure depicts the opponent attention weights they are optimized during training to generate the lowest score which is fed into min to get the highest opponent probability po the more irrelevant to the summary word the ponent is the lower the score can be obtained thus resulting in higher po figure shows that the tentions are formed over irrelevant parts with ied weights as the result of maximizing po during training effect of the opponent probability in decoding we study the contribution of the opponent ability po by dropping it during decoding to see if it hurts the performance table shows that dropping po signicantly harms the performance of transformer contrastiveatt the mance difference between the model dropping po and the baseline transformer is marginal ing that adding the opponent probability po is key for achieving the performance improvement explorations on deriving the opponent attention masking more attention weights for deriving the opponent attention c system mask maximum weight mask weights mask weights dynamically mask synchronous head non synchronous head averaged head transformer baseline gigaword r l r l table results of explorations on the opponent attention derivation the upper part presents the inuence of masking more attention weights for deriving the opponent attention the middle part presents the results of ing different head for the opponent attention derivation the bottom row presents the result of transformer gigaword transformer po transformer po r l r l table the effect of dropping po denoted by from during decoding in section we mask the most salient word that has the maximum weight of c to derive the opponent attention in this subsection we mented with masking more weights of c by two ways masking top k weights dynamically masking in the dynamically masking method we order the weights from big to small at rst then go on masking two neighbors until the ratio between them is over a threshold the threshold is based on training and tuning on the development set the upper rows of table presents the formance comparison between masking maximum weight and masking more weights it shows that masking maximum weight performs better cating that masking the most salient weight leaves more irrelevant or less relevant words to pute the opponent probability po which is more reliable than that computed from less remaining words after masking more weights selecting non synchronous head or averaged head for deriving the opponent attention as explained in section the opponent tention is derived from the head that is most chronous to the word alignments between source sentence and summary we denote it chronous head we also explored deriving the opponent attention from the fth head of the rst layer which is non synchronous to the word ments as illustrated in figure its result is sented in the non synchronous head row in dition the attention weights averaged on all heads of the third layer are used to derive the opponent attention we denote it averaged head as shown in the middle part of table both non synchronous head and averaged head underperform synchronous head synchronous head performs worst and even worse than the transformer baseline on gigaword this indicates that it is better to compose the ponent attention from irrelevant parts that can be easily located in the synchronous head averaged head performs slightly worse than synchronous head and is also slower due to the involved all heads qualitative study in contrast table shows the qualitative results the lights in the baseline transformer manifest the incorrect areas extracted by the baseline tem the highlights in show that correct contents are extracted since the contrastive system guish relevant parts from irrelevant parts on the source side and made attending to correct areas more easily conclusion we proposed a contrastive attention mechanism for abstractive sentence summarization using both the conventional attention that attends to the evant parts of the source sentence and a novel opponent attention that attends to irrelevant or less relevant parts for the summary word ation both categories of the attention constitute a contrastive pair and we encourage contribution from the conventional attention and penalize src press freedom in algeria remains at risk despite the release on wednesday of prominent newspaper editor mohamed unk after a two year prison sentence human rights organizations said ref algerian press freedom at risk despite editor s release unk picture transformer press freedom remains at risk in algeria rights groups say press freedom remains at risk despite release of algerian editor src denmark s poul erik hoyer completed his hat trick of men s singles badminton titles at the european championships winning the nal here on saturday ref hoyer wins singles title transformer hoyer completes hat trick hoyer wins men s singles title src french bank credit agricole launched on tuesday a public cash offer to buy the percent of emporiki bank it does not already own in a bid valuing the greek group at billion euros billion dollars ref credit agricole announces euro bid for greek bank emporiki transformer credit agricole launches public cash offer for greek bank french bank credit agricole bids billion euros for greek bank table example summaries generated by the baseline transformer and tribution from the opponent attention through joint training using transformer as a strong baseline experiments on three benchmark data sets show that the proposed contrastive attention mechanism signicantly improves the performance ing the state of the art performance for the task acknowledgments the authors would like to thank the anonymous reviewers for the helpful comments this work was supported by national key program of china grant no national natural science foundation of china grant no references ayana shiqi shen yu zhao zhiyuan liu and maosong sun neural headline generation with sentence wise optimization computer search repository version dzmitry bahdanau kyunghyun cho and yoshua gio neural machine translation by jointly learning to align and translate in international conference on learning representations michele banko vibhu o mittal and michael j brock headline generation based on cal translation in proceedings of the annual meeting on association for computational tics pages ziqiang cao furu wei wenjie li and sujian li faithful to the original fact aware neural tive summarization in thirty second aaai ence on articial intelligence qian chen xiao dan zhu zhen hua ling si wei and hui jiang distraction based neural in proceedings of works for modeling document the twenty fifth international joint conference on articial intelligence pages sumit chopra michael auli and alexander m rush abstractive sentence summarization with tentive recurrent neural networks in proceedings of the conference of the north american ter of the association for computational linguistics human language technologies pages trevor cohn and mirella lapata sentence in proceedings pression beyond word deletion of the international conference on tional linguistics volume pages bonnie dorr david zajic and richard schwartz hedge trimmer a parse and trim approach to headline generation in proceedings of the naacl on text summarization workshop volume pages sergey edunov myle ott michael auli david ier and marcaurelio ranzato classical structured prediction losses for sequence to quence learning in proceedings of the ference of the north american chapter of the ciation for computational linguistics human guage technologies pages jonas gehring michael auli david grangier nis yarats and yann n dauphin in tional sequence to sequence learning ings of the international conference on chine learning pages jiatao gu zhengdong lu li hang and victor o k incorporating copying mechanism in li in proceedings of sequence to sequence learning the annual meeting of the association for putational linguistics caglar gulcehre sungjin ahn ramesh nallapati bowen zhou and yoshua bengio pointing the unknown words in proceedings of the nual meeting of the association for computational linguistics baotian hu qingcai chen and fangze zhu sts a large scale chinese short text summarization dataset in proceedings of the conference on kaiqiang song lin zhao and fei liu structure infused copy mechanisms for abstractive in proceedings of the summarization national conference on computational linguistics pages ilya sutskever oriol vinyals and quoc v le sequence to sequence learning with neural works in advances in neural information ing systems pages ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan n gomez ukasz kaiser and illia polosukhin attention is all you need in advances in neural information cessing systems pages li wang junlin yao yunzhe tao li zhong wei liu and qiang du a reinforced topic aware convolutional sequence to sequence model for stractive text summarization in proceedings of the twenty seventh international joint conference on articial intelligence pages kristian woodsend yansong feng and mirella ata title generation with quasi synchronous grammar in proceedings of the conference on empirical methods in natural language ing pages qingyu zhou yang nan furu wei and zhou ming selective encoding for abstractive sentence summarization in proceedings of the annual meeting of the association for computational guistics pages empirical methods in natural language ing pages hongyan jing and kathleen r mckeown cut in and paste based text summarization ings of the north american chapter of the sociation for computational linguistics conference pages kevin knight and daniel marcu based summarization step one sentence in proceedings of the seventeenth national sion conference on articial intelligence and twelfth conference on on innovative applications of cial intelligence pages piji li lidong bing and wai lam critic based training framework for abstractive summarization computing research repository piji li wai lam lidong bing and zihao wang deep recurrent generative decoder for abstractive in proceedings of the text summarization conference on empirical methods in natural guage processing pages chin yew lin rouge a package for automatic in proc of the evaluation of summaries workshop on text summarization branches out junyang lin sun xu shuming ma and su qi global encoding for abstractive summarization in proceedings of the annual meeting of the sociation for computational linguistics pages ramesh nallapati bowen zhou cicero nogueira dos santos caglar gulcehre and xiang bing abstractive text summarization using sequence in proceedings of the sequence rnns and beyond signll conference on computational ral language learning pages joel larocca neto alex a freitas and celso a a kaestner automatic text summarization ing a machine learning approach in brazilian posium on articial intelligence pages alexander m rush sumit chopra and jason weston a neural attention model for abstractive in proceedings of the tence summarization conference on empirical methods in natural guage processing pages rico sennrich barry haddow and alexandra birch neural machine translation of rare words with subword units in proceedings of the annual meeting of the association for computational guistics chunfeng song yan huang wanli ouyang and liang wang mask guided contrastive attention in proceedings model for person re identication of the ieee conference on computer vision and pattern recognition pages
