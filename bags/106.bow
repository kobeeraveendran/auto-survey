deep recurrent generative decoder abstractive text summarization piji li wai lam lidong bing zihao wang key laboratory high condence software technologies sub lab cuhk ministry education china department systems engineering engineering management chinese university hong kong ai lab tencent inc shenzhen china wlam cuhk edu hk com g u l c s c v v x r abstract propose new framework stractive text summarization based sequence sequence oriented decoder model equipped deep current generative decoder drgn tent structure information implied target summaries learned based current latent random model ing summarization quality neural variational inference employed dress intractable posterior inference recurrent latent variables abstractive summaries generated based generative latent variables criminative deterministic states extensive experiments benchmark datasets different languages drgn achieves improvements state art methods introduction automatic summarization process matically generating summary retains important content original text ument edmundson luhn nenkova mckeown different common extraction based compression based methods abstraction based methods aim constructing new sentences summaries require deeper understanding text ity generating new sentences provide obvious advantage improving focus summary reducing redundancy keeping good compression rate bing et al rush et al nallapati et al work described paper supported grant grant council hong kong special trative region china project code figure headlines stories channel technology cnn previous research works human written summaries abstractive jing mckeown vestigation reveals people naturally low inherent structures write abstractive summaries illustrate tion examples figure story summaries headlines channel technology cnn ing summaries carefully nd common structures happened action summary apple sues example comm nearly billion ized apple action sues summaries twitter comm xes botched account transfer pay million misleading drivers bipartisan bill aims reform visa system follow structure action summary cyber cold war matches gence structure summary st louis public library computers hacked follows similarly apple sues qualcomm nearly fixes botched account transfertrack trump s day promises silicon valley emergence cyber cold wartesla autopilot defective fatal crashtwitter meets modest diversity goalsuber pay million misleading driverstop stories structure happened intuitively incorporate latent structure information summaries stractive summarization model improve quality generated summaries existing works specically consider latent structure information summaries summarization models ular neural network based sequence sequence framework proposed tackle abstractive summarization problem lopyrev rush et al nallapati et al calculation internal decoding states tirely deterministic deterministic tions discriminative models lead itations representation ability latent structure information miao blunsom extended framework proposed generative model capture latent summary formation consider recurrent dependencies generative model leading limited representation ability tackle mentioned problems design new framework based sequence oriented encoder decoder model equipped latent structure modeling ponent employ variational auto encoders vaes kingma welling rezende et al base model tive framework handle inference problem associated complex generative modeling standard framework vaes designed sequence modeling inspired chung et al related tasks add historical dependencies latent variables vaes propose deep recurrent generative decoder drgd latent structure standard discriminative modeling deterministic decoder recurrent generative decoder integrated unied decoding framework target summaries decoded based discriminative deterministic variables generative latent structural information neural parameters learned propagation end end training paradigm main contributions framework summarized follows propose sequence sequence oriented encoder decoder model equipped deep recurrent generative decoder drgd model learn latent structure information implied target maries training data neural variational ference employed address intractable terior inference recurrent latent variables generative latent structural tion discriminative deterministic variables jointly considered generation process abstractive summaries experimental sults benchmark datasets different guages framework achieves better performance state art models related works automatic summarization process matically generating summary retains important content original text ument nenkova mckeown tionally summarization methods ed categories extraction based ods erkan radev goldstein et al wan et al min et al pati et al cheng lapata cao et al song et al compression based methods li et al wang et al li et al abstraction based ods fact previous investigations human written summaries abstractive barzilay mckeown bing et al abstraction based approaches generate new sentences based facts different source sentences barzilay mckeown ployed sentence fusion generate new sentence bing et al proposed ne grained fusion framework new sentences erated selecting merging salient phrases methods regarded kind direct abstractive summarization complicated constraints guarantee linguistic quality recently researchers employ neural work based framework tackle abstractive summarization problem rush et al posed neural network based model local attention modeling trained word corpus combined additional linear extractive summarization model crafted features gu et al integrated copying mechanism framework improve quality generated summaries chen et al proposed new attention anism considers important source segments distracts decoding step order better grasp overall meaning input documents nallapati et al utilized trick control vocabulary size improve training efciency calculations methods deterministic tion ability limited miao blunsom extended framework proposed generative model capture latent summary formation consider recurrent dependencies generative model leading limited representation ability research works employ topic models capture latent information source ments sentences wang et al proposed new bayesian sentence based topic model making use term document sentence associations improve performance sentence selection celikyilmaz tur estimated scores sentences based latent characteristics hierarchical topic model trained regression model tract sentences use latent topic information conduct sentence salience estimation extractive summarization trast purpose model learn latent structure information target summaries use enhance performance tive summarization framework description overview output framework shown figure basic framework approach neural network based decoder sequence sequence learning input variable length sequence x xm representing source text word embedding initialized domly learned optimization sequence y cess represents generated abstractive summaries gated recurrent unit gru cho et al employed sic sequence modeling component encoder decoder latent structure modeling add historical dependencies latent variables variational auto encoders vaes propose deep recurrent generative decoder drgd distill complex latent structures implied target summaries training data finally abstractive summaries decoded based discriminative deterministic variables h generative latent structural information z recurrent generative decoder assume obtained source text resentation rkh purpose decoder translate source code series hidden states hd n revert hidden states actual word sequence generate summary hd hd standard recurrent decoders time t rkh calculated step t hidden state hd ing dependent input symbol rkw previous hidden state hd t hd hd recurrent neural network vanilla rnn long short term memory lstm hochreiter schmidhuber gated recurrent unit gru cho et al ter use f common formation operation follows t hd wd hhhd bd h rkhkw wd hh rkhkh wd linear transformation matrices bd h bias kh dimension hidden layers kw dimension word embeddings g non linear activation function equation transformations ministic leads deterministic recurrent hidden state hd t investigations nd representational power istic variables limited complex latent structures target summaries high level syntactic features latent topics modeled effectively deterministic operations variables recently generative model called variational auto encoders vaes kingma welling rezende et al shows strong bility modeling latent random variables improves performance tasks different elds sentence generation bowman et al image generation gregor et al standard vaes designed modeling sequence directly inspired chung et al extend standard vaes figure deep recurrent generative decoder drgd latent structure modeling introducing historical latent variable dencies capable modeling quence data proposed latent structure eling framework viewed sequence generative model divided parts inference variational encoder ation variational decoder shown coder component figure input inal vaes contains observed variable yt variational encoder map latent variable z rkz reconstruct original input task summarization sequence decoder component previous latent structure information needs considered constructing effective representations generation state inference stage variational encoder map observed variable t vious latent structure information z terior probability distribution latent ture variable t z t obvious recurrent inference process zt contains historical dynamic latent structure formation compared variational ence process typical vaes model recurrent framework extract complex effective latent structure features implied sequence data generation process based tent structure variable zt target word yt time step t drawn conditional ity distribution target mize probability generated summary y yt based generation process according t purpose solving intractable integral marginal likelihood shown equation recognition model t z t introduced approximation intractable true rior t z t recognition model rameters generative model parameters learned jointly aim reduce kulllback leibler divergence kl t z t t z t t z t z t log z t z t z dz t z log denotes conditional variables y t bayes rule applied t z extract log expectation transfer expectation term t z t kl divergence rearrange terms consequently following holds log t t t z t z t z let represent terms right equation log t z t z rst kl divergence term equation non negative log t meaning y lower bound jective maximized marginal order differentiate optimize hood encoders encodervariational decoder lower bound y following core idea vaes use neural network framework probabilistic encoder t z t ter approximation abstractive summary generation design neural network based work conduct variational inference eration recurrent generative decoder ponent similar design previous works kingma welling rezende et al gregor et al encoder component decoder component integrated ed abstractive summarization framework sidering gru comparable performance parameters efcient putation employ gru basic recurrent model updates variables according following operations rt br zt bz gt bh ht zt zt gt rt reset gate zt update gate denotes element wise multiplication tanh hyperbolic tangent activation function shown left block figure coder designed based bidirectional recurrent neural networks let xt word embedding vector t th word source sequence gru maps previous hidden state current hidden state ht feed forward rection forward direction respectively gru xt gru xt nal hidden state nated hidden states tions shown middle block figure decoder consists nents discriminative deterministic decoding generative latent structure modeling t discriminative deterministic decoding improved attention modeling based recurrent quence decoder rst hidden state hd tialized average source input t e states hd t source hidden state t e input sequence length t t e deterministic decoder hidden state hd t culated layers grus rst layer hidden state calculated current input word embedding ous hidden state superscript denotes rst decoder gru layer attention weights time step t calculated based relationship t source hidden states t let ai j attention weight j calculated following formulation ai j e j ei j vt hhhe j ba hh rkhkh hh ba wd rkh rkh attention context tained weighted linear combination source hidden states ct e nal deterministic hidden state t output second decoder gru layer jointly considering word previous hidden state attention context ct t ct component recurrent generative model inspired ideas previous works kingma welling rezende et al gregor et al assume prior posterior latent variables gaussian e n t z n zt denote tional mean standard deviation respectively calculated multilayer tron precisely given word embedding previous latent structure variable previous deterministic hidden state hd rst project new hidden space hez hhhd h rkhkw wez wez hh rkhkh bez h rkh g sigmoid vation function ex zh rkhkz wez gaussian parameters t rkz t rkz obtained linear transformation based hez hhez t wez t whhez latent structure variable zt rkz culated reparameterization trick t bez t bez needs minimized formulated follows j n n t log t x n dkl t t t experimental setup n zt t t datesets rkz auxiliary noise variable process inference nding zt based neural networks teated variational encoding process generate summaries precisely rst tegrate recurrent generative decoding nent discriminative deterministic ing component map latent structure able zt deterministic decoding hidden state t new hidden variable t hdy zhzt wdz t bdy h given combined decoding state hdy t time t probability generating target word yt given follows yt hyhdy t bd hy hy rkykh bd hy rky wd softmax function finally use beam search algorithm koehn decoding ating best summary learning proposed model contains recurrent generative decoder framework fully differentiable shown section recurrent deterministic decoder recurrent generative decoder designed based neural networks parameters model optimized end end paradigm propagation use y n denote training source target sequence generally objective framework sists terms term negative likelihood generated summaries variational lower bound y mentioned equation variational lower bound y contains likelihood term merge likelihood term summaries nal objective function train evaluate framework ular datasets gigawords english sentence summarization dataset prepared based tated extracting rst sentence articles headline form summary pair directly download prepared dataset rush et al roughly tains m training pairs k validation pairs test pairs glish dataset testing ments contains documents document contains model summaries written experts length summary limited bytes lcsts large scale chinese short text rization dataset consisting pairs short text summary collected sina hu et al training set ii development set iii test set score range labeled human indicate relevant article summary reserve pairs scores size sets m respectively experiments chinese character sequence input performing word segmentation evaluation metrics use rouge score lin tion metric standard options basic idea rouge count number overlapping units generated summaries erence summaries overlapped n grams word sequences word pairs f measures rouge l l rouge r reported comparative methods compare model baselines state art methods datasets ldc upenn edu nist gov weibo com standard extract results papers baseline methods different datasets slightly different topiary zajic et al best compressive text marization combines system guistic based transformations pervised topic detection algorithm pressive text summarization rush et al uses based statistical machine translation system trained gigaword produce summaries augments phrase table tion rulesto improve baseline mance mert improve quality generated summaries abs rush et al neural network based models local attention modeling abstractive sentence summarization trained gaword corpus combined ditional log linear extractive summarization model handcrafted features rnn rnn context hu et al architectures rnn context tegrates attention mechanism model context copynet gu et al integrates copying mechanism sequence sequence framework rnn distract chen et al uses new attention mechanism distracting torical attention decoding steps ras lstm ras elman chopra et al consider words word sitions input use convolutional coders handle source information attention based sequence decoding cess ras elman selects elman rnn man decoder ras lstm lects long short term memory architecture hochreiter schmidhuber lenemb kikuchi et al uses anism control summary length sidering length embedding vector input miao blunsom uses generative model attention mechanism conduct sentence compression lem model rst draws latent summary sentence background language model subsequently draws observed sentence conditioned latent summary nallapati et al utilize trick control vocabulary size improve training efciency experimental settings experiments english dataset words set dimension word embeddings dimension hidden states tent variables maximum length uments summaries respectively batch size mini batch training maximum length summaries bytes dataset lcsts sion word embeddings set dimension hidden states latent variables maximum length documents maries respectively batch size beam size decoder set adadelta schmidhuber hyperparameter gradient based optimization ral network based framework implemented ing theano theano development team results discussions rouge evaluation table rouge validation sets dataset system stand giga drgd stand drgd lcsts r l rst depict performance model drgd comparing standard decoders stand implementation son results validation datasets gigawords lcsts shown table sults proposed generative coders drgd obtain obvious improvements abstractive summarization standard coders actually performance standard table rouge gigawords table rouge recall system abs ras lstm ras elman asc drgd system topiary abs ras elman ras lstm lenemb drgd r l r l table rouge lcsts system rnn rnn context copynet rnn distract drgd r l decoders similar mentioned popular baseline methods results english datasets words shown table table respectively model drgd achieves best summarization performance rouge metrics uses generative method model latent summary variables representation ability limited bring noticeable improvements worth noting methods nallapati et al utilize tic features parts speech tags entity tags tf idf statistics words fact document representation extracting features time consuming work especially large scale datasets gigawords end end style models complicated model practical applications results chinese dataset lcsts shown table model drgd achieves best performance copynet employs copying mechanism improve summary quality rnn distract considers attention formation diversity decoders model better methods demonstrating latent structure information learned target summaries plays role abstractive summarization believe integrating copying mechanism coverage diversity framework improve rization performance summary case analysis order analyze reasons improving performance compare generated maries drgd standard decoders stand works chopra et al source texts golden summaries generated summaries shown table cases observe drgd deed capture latent structures sistent golden summaries example result wuhan wins men s soccer tle chinese city games matches tion structure standard coder stand ignores latent structures erates loose sentences results results men s volleyball chinese city games catch main points reason recurrent variational auto encoders framework better representation ability capture effective complicated tent structures sequence data summaries generated drgd tent latent structures ground truth leading better rouge evaluation conclusions propose deep recurrent generative decoder drgd improve abstractive tion performance model sequence oriented encoder decoder framework equipped latent structure modeling nent abstractive summaries generated based latent variables tic states extensive experiments benchmark table examples generated summaries hosts wuhan won men s soccer title beating beijing shunyi chinese city games friday golden hosts wuhan wins men s soccer title chinese city games stand results men s volleyball chinese city games drgd wuhan wins men s soccer title chinese city games unk china meteorological administration tuesday signed agreement long short term cooperation projects involving meteorological satellites satellite meteorology golden unk china cooperate ogy stand weather forecast major chinese cities drgd china cooperate meteorological satellites rand gained ground dollar opening wednesday greenback close tuesday golden rand gains ground stand rand slightly higher dollar drgd rand gains ground dollar new zealand women having children country s birth rate reached highest level years statistics new zealand said wednesday golden new zealand birth rate reaches year high stand new zealand women having children birth rate hits highest level years drgd new zealand s birth rate hits year high datasets drgd achieves improvements state art methods references samuel r bowman luke vilnis oriol vinyals drew m dai rafal jozefowicz samy gio generating sentences continuous space conll pages ziqiang cao wenjie li sujian li furu wei ran li attsum joint learning focusing summarization neural attention coling pages asli celikyilmaz dilek hakkani tur brid hierarchical model multi document rization acl pages qian chen xiaodan zhu zhenhua ling si wei hui jiang distraction based neural works document summarization ijcai pages jianpeng cheng mirella lapata neural summarization extracting sentences words acl pages kyunghyun cho bart van merrienboer caglar cehre dzmitry bahdanau fethi bougares holger learning schwenk yoshua bengio phrase representations rnn encoder decoder statistical machine translation emnlp pages sumit chopra michael auli alexander m rush seas harvard abstractive sentence marization attentive recurrent neural networks naacl hlt pages junyoung chung kyle kastner laurent dinh kratarth goel aaron c courville yoshua bengio recurrent latent variable model sequential data nips pages harold p edmundson new methods journal acm jacm tomatic extracting jeffrey l elman finding structure time nitive science gunes erkan dragomir r radev lexrank graph based lexical centrality salience text journal articial intelligence summarization research jade goldstein vibhu mittal jaime carbonell mark kantrowitz multi document marization sentence extraction anlpworkshop pages regina barzilay kathleen r mckeown sentence fusion multidocument news rization computational linguistics karol gregor ivo danihelka alex graves danilo rezende daan wierstra draw rent neural network image generation icml pages lidong bing piji li yi liao wai lam weiwei guo rebecca passonneau abstractive document summarization phrase selection merging acl pages jiatao gu zhengdong lu hang li victor ok incorporating copying mechanism acl pages li sequence sequence learning sepp hochreiter jurgen schmidhuber neural computation long short term memory baotian hu qingcai chen fangze zhu sts large scale chinese short text summarization dataset emnlp pages hongyan jing kathleen r mckeown cut naacl paste based text summarization pages yuta kikuchi graham neubig ryohei sasano hiroya takamura manabu okumura ling output length neural encoder decoders emnlp pages diederik p kingma max welling arxiv preprint encoding variational bayes philipp koehn pharaoh beam search coder phrase based statistical machine conference association tion models machine translation americas pages springer ramesh nallapati bowen zhou caglar gulcehre bing xiang al abstractive text rization sequence sequence rnns yond arxiv preprint ani nenkova kathleen mckeown survey mining text text summarization techniques data pages springer danilo jimenez rezende shakir mohamed daan wierstra stochastic backpropagation proximate inference deep generative models icml pages alexander m rush sumit chopra jason weston neural attention model abstractive tence summarization emnlp pages jurgen schmidhuber deep learning neural networks overview neural networks hongya song zhaochun ren piji li shangsong liang jun ma maarten de rijke summarizing answers non factoid community question answering wsdm pages chen li fei liu fuliang weng yang liu document summarization guided sentence pression emnlp pages theano development team theano python framework fast computation mathematical pressions arxiv e prints xiaojun wan jianwu yang jianguo xiao manifold ranking based topic focused ijcai volume document summarization pages dingding wang shenghuo zhu tao li yihong gong multi document summarization ing sentence based topic models acl ijcnlp pages lu wang hema raghavan vittorio castelli radu rian claire cardie sentence pression based framework query focused acl pages document summarization david zajic bonnie dorr richard schwartz hlt naacl bbn umd topiary pages piji li lidong bing wai lam hang li yi liao reader aware multi document tion sparse coding ijcai pages piji li zihao wang wai lam zhaochun ren lidong bing salience estimation ational auto encoders multi document rization aaai pages chin yew lin rouge package matic evaluation summaries text tion branches proceedings shop volume konstantin lopyrev generating news lines recurrent neural networks arxiv preprint hans peter luhn automatic creation erature abstracts ibm journal research velopment yishu miao phil blunsom language latent variable discrete generative models tence compression emnlp pages ziheng lin min yen kan chew lim tan exploiting category specic information document summarization coling pages ramesh nallapati feifei zhai bowen zhou summarunner recurrent neural network based quence model extractive summarization ments aaai pages
