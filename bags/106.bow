deep recurrent generative decoder abstractive text summarization piji wai lam lidong bing zihao wang key laboratory high condence software technologies sub lab cuhk ministry education china department systems engineering engineering management chinese university hong kong lab tencent inc shenzhen china wlam cuhk edu com abstract propose new framework stractive text summarization based sequence sequence oriented decoder model equipped deep current generative decoder drgn tent structure information implied target summaries learned based current latent random model ing summarization quality neural variational inference employed dress intractable posterior inference recurrent latent variables abstractive summaries generated based generative latent variables criminative deterministic states extensive experiments benchmark datasets different languages drgn achieves improvements state art methods introduction automatic summarization process matically generating summary retains important content original text ument edmundson luhn nenkova mckeown different common extraction based compression based methods abstraction based methods aim constructing new sentences summaries require deeper understanding text ity generating new sentences provide obvious advantage improving focus summary reducing redundancy keeping good compression rate bing rush nallapati work described paper supported grant grant council hong kong special trative region china project code figure headlines stories channel technology cnn previous research works human written summaries abstractive jing mckeown vestigation reveals people naturally low inherent structures write abstractive summaries illustrate tion examples figure story summaries headlines channel technology cnn ing summaries carefully common structures happened action summary apple sues example comm nearly billion ized apple action sues summaries twitter comm xes botched account transfer pay million misleading drivers bipartisan bill aims reform visa system follow structure action summary cyber cold war matches gence structure summary louis public library computers hacked follows similarly apple sues qualcomm nearly fixes botched account transfertrack trump day promises silicon valley emergence cyber cold wartesla autopilot defective fatal crashtwitter meets modest diversity goalsuber pay million misleading driverstop stories structure happened intuitively incorporate latent structure information summaries stractive summarization model improve quality generated summaries existing works specically consider latent structure information summaries summarization models ular neural network based sequence sequence framework proposed tackle abstractive summarization problem lopyrev rush nallapati calculation internal decoding states tirely deterministic deterministic tions discriminative models lead itations representation ability latent structure information miao blunsom extended framework proposed generative model capture latent summary formation consider recurrent dependencies generative model leading limited representation ability tackle mentioned problems design new framework based sequence oriented encoder decoder model equipped latent structure modeling ponent employ variational auto encoders vaes kingma welling rezende base model tive framework handle inference problem associated complex generative modeling standard framework vaes designed sequence modeling inspired chung related tasks add historical dependencies latent variables vaes propose deep recurrent generative decoder drgd latent structure standard discriminative modeling deterministic decoder recurrent generative decoder integrated unied decoding framework target summaries decoded based discriminative deterministic variables generative latent structural information neural parameters learned propagation end end training paradigm main contributions framework summarized follows propose sequence sequence oriented encoder decoder model equipped deep recurrent generative decoder drgd model learn latent structure information implied target maries training data neural variational ference employed address intractable terior inference recurrent latent variables generative latent structural tion discriminative deterministic variables jointly considered generation process abstractive summaries experimental sults benchmark datasets different guages framework achieves better performance state art models related works automatic summarization process matically generating summary retains important content original text ument nenkova mckeown tionally summarization methods categories extraction based ods erkan radev goldstein wan min pati cheng lapata cao song compression based methods wang abstraction based ods fact previous investigations human written summaries abstractive barzilay mckeown bing abstraction based approaches generate new sentences based facts different source sentences barzilay mckeown ployed sentence fusion generate new sentence bing proposed grained fusion framework new sentences erated selecting merging salient phrases methods regarded kind direct abstractive summarization complicated constraints guarantee linguistic quality recently researchers employ neural work based framework tackle abstractive summarization problem rush posed neural network based model local attention modeling trained word corpus combined additional linear extractive summarization model crafted features integrated copying mechanism framework improve quality generated summaries chen proposed new attention anism considers important source segments distracts decoding step order better grasp overall meaning input documents nallapati utilized trick control vocabulary size improve training efciency calculations methods deterministic tion ability limited miao blunsom extended framework proposed generative model capture latent summary formation consider recurrent dependencies generative model leading limited representation ability research works employ topic models capture latent information source ments sentences wang proposed new bayesian sentence based topic model making use term document sentence associations improve performance sentence selection celikyilmaz tur estimated scores sentences based latent characteristics hierarchical topic model trained regression model tract sentences use latent topic information conduct sentence salience estimation extractive summarization trast purpose model learn latent structure information target summaries use enhance performance tive summarization framework description overview output framework shown figure basic framework approach neural network based decoder sequence sequence learning input variable length sequence representing source text word embedding initialized domly learned optimization sequence cess represents generated abstractive summaries gated recurrent unit gru cho employed sic sequence modeling component encoder decoder latent structure modeling add historical dependencies latent variables variational auto encoders vaes propose deep recurrent generative decoder drgd distill complex latent structures implied target summaries training data finally abstractive summaries decoded based discriminative deterministic variables generative latent structural information recurrent generative decoder assume obtained source text resentation rkh purpose decoder translate source code series hidden states revert hidden states actual word sequence generate summary standard recurrent decoders time rkh calculated step hidden state ing dependent input symbol rkw previous hidden state recurrent neural network vanilla rnn long short term memory lstm hochreiter schmidhuber gated recurrent unit gru cho ter use common formation operation follows hhhd rkhkw rkhkh linear transformation matrices bias dimension hidden layers dimension word embeddings non linear activation function equation transformations ministic leads deterministic recurrent hidden state investigations representational power istic variables limited complex latent structures target summaries high level syntactic features latent topics modeled effectively deterministic operations variables recently generative model called variational auto encoders vaes kingma welling rezende shows strong bility modeling latent random variables improves performance tasks different elds sentence generation bowman image generation gregor standard vaes designed modeling sequence directly inspired chung extend standard vaes figure deep recurrent generative decoder drgd latent structure modeling introducing historical latent variable dencies capable modeling quence data proposed latent structure eling framework viewed sequence generative model divided parts inference variational encoder ation variational decoder shown coder component figure input inal vaes contains observed variable variational encoder map latent variable rkz reconstruct original input task summarization sequence decoder component previous latent structure information needs considered constructing effective representations generation state inference stage variational encoder map observed variable vious latent structure information terior probability distribution latent ture variable obvious recurrent inference process contains historical dynamic latent structure formation compared variational ence process typical vaes model recurrent framework extract complex effective latent structure features implied sequence data generation process based tent structure variable target word time step drawn conditional ity distribution target mize probability generated summary based generation process according purpose solving intractable integral marginal likelihood shown equation recognition model introduced approximation intractable true rior recognition model rameters generative model parameters learned jointly aim reduce kulllback leibler divergence log log denotes conditional variables bayes rule applied extract log expectation transfer expectation term divergence rearrange terms consequently following holds log let represent terms right equation log rst divergence term equation non negative log meaning lower bound jective maximized marginal order differentiate optimize hood encoders encodervariational decoder lower bound following core idea vaes use neural network framework probabilistic encoder ter approximation abstractive summary generation design neural network based work conduct variational inference eration recurrent generative decoder ponent similar design previous works kingma welling rezende gregor encoder component decoder component integrated abstractive summarization framework sidering gru comparable performance parameters efcient putation employ gru basic recurrent model updates variables according following operations reset gate update gate denotes element wise multiplication tanh hyperbolic tangent activation function shown left block figure coder designed based bidirectional recurrent neural networks let word embedding vector word source sequence gru maps previous hidden state current hidden state feed forward rection forward direction respectively gru gru nal hidden state nated hidden states tions shown middle block figure decoder consists nents discriminative deterministic decoding generative latent structure modeling discriminative deterministic decoding improved attention modeling based recurrent quence decoder rst hidden state tialized average source input states source hidden state input sequence length deterministic decoder hidden state culated layers grus rst layer hidden state calculated current input word embedding ous hidden state superscript denotes rst decoder gru layer attention weights time step calculated based relationship source hidden states let attention weight calculated following formulation hhhe rkhkh rkh rkh attention context tained weighted linear combination source hidden states nal deterministic hidden state output second decoder gru layer jointly considering word previous hidden state attention context component recurrent generative model inspired ideas previous works kingma welling rezende gregor assume prior posterior latent variables gaussian denote tional mean standard deviation respectively calculated multilayer tron precisely given word embedding previous latent structure variable previous deterministic hidden state rst project new hidden space hez hhhd rkhkw wez wez rkhkh bez rkh sigmoid vation function rkhkz wez gaussian parameters rkz rkz obtained linear transformation based hez hhez wez whhez latent structure variable rkz culated reparameterization trick bez bez needs minimized formulated follows log dkl experimental setup datesets rkz auxiliary noise variable process inference nding based neural networks teated variational encoding process generate summaries precisely rst tegrate recurrent generative decoding nent discriminative deterministic ing component map latent structure able deterministic decoding hidden state new hidden variable hdy zhzt wdz bdy given combined decoding state hdy time probability generating target word given follows hyhdy rkykh rky softmax function finally use beam search algorithm koehn decoding ating best summary learning proposed model contains recurrent generative decoder framework fully differentiable shown section recurrent deterministic decoder recurrent generative decoder designed based neural networks parameters model optimized end end paradigm propagation use denote training source target sequence generally objective framework sists terms term negative likelihood generated summaries variational lower bound mentioned equation variational lower bound contains likelihood term merge likelihood term summaries nal objective function train evaluate framework ular datasets gigawords english sentence summarization dataset prepared based tated extracting rst sentence articles headline form summary pair directly download prepared dataset rush roughly tains training pairs validation pairs test pairs glish dataset testing ments contains documents document contains model summaries written experts length summary limited bytes lcsts large scale chinese short text rization dataset consisting pairs short text summary collected sina training set development set iii test set score range labeled human indicate relevant article summary reserve pairs scores size sets respectively experiments chinese character sequence input performing word segmentation evaluation metrics use rouge score lin tion metric standard options basic idea rouge count number overlapping units generated summaries erence summaries overlapped grams word sequences word pairs measures rouge rouge reported comparative methods compare model baselines state art methods datasets ldc upenn edu nist gov weibo com standard extract results papers baseline methods different datasets slightly different topiary zajic best compressive text marization combines system guistic based transformations pervised topic detection algorithm pressive text summarization rush uses based statistical machine translation system trained gigaword produce summaries augments phrase table tion rulesto improve baseline mance mert improve quality generated summaries abs rush neural network based models local attention modeling abstractive sentence summarization trained gaword corpus combined ditional log linear extractive summarization model handcrafted features rnn rnn context architectures rnn context tegrates attention mechanism model context copynet integrates copying mechanism sequence sequence framework rnn distract chen uses new attention mechanism distracting torical attention decoding steps ras lstm ras elman chopra consider words word sitions input use convolutional coders handle source information attention based sequence decoding cess ras elman selects elman rnn man decoder ras lstm lects long short term memory architecture hochreiter schmidhuber lenemb kikuchi uses anism control summary length sidering length embedding vector input miao blunsom uses generative model attention mechanism conduct sentence compression lem model rst draws latent summary sentence background language model subsequently draws observed sentence conditioned latent summary nallapati utilize trick control vocabulary size improve training efciency experimental settings experiments english dataset words set dimension word embeddings dimension hidden states tent variables maximum length uments summaries respectively batch size mini batch training maximum length summaries bytes dataset lcsts sion word embeddings set dimension hidden states latent variables maximum length documents maries respectively batch size beam size decoder set adadelta schmidhuber hyperparameter gradient based optimization ral network based framework implemented ing theano theano development team results discussions rouge evaluation table rouge validation sets dataset system stand giga drgd stand drgd lcsts rst depict performance model drgd comparing standard decoders stand implementation son results validation datasets gigawords lcsts shown table sults proposed generative coders drgd obtain obvious improvements abstractive summarization standard coders actually performance standard table rouge gigawords table rouge recall system abs ras lstm ras elman asc drgd system topiary abs ras elman ras lstm lenemb drgd table rouge lcsts system rnn rnn context copynet rnn distract drgd decoders similar mentioned popular baseline methods results english datasets words shown table table respectively model drgd achieves best summarization performance rouge metrics uses generative method model latent summary variables representation ability limited bring noticeable improvements worth noting methods nallapati utilize tic features parts speech tags entity tags idf statistics words fact document representation extracting features time consuming work especially large scale datasets gigawords end end style models complicated model practical applications results chinese dataset lcsts shown table model drgd achieves best performance copynet employs copying mechanism improve summary quality rnn distract considers attention formation diversity decoders model better methods demonstrating latent structure information learned target summaries plays role abstractive summarization believe integrating copying mechanism coverage diversity framework improve rization performance summary case analysis order analyze reasons improving performance compare generated maries drgd standard decoders stand works chopra source texts golden summaries generated summaries shown table cases observe drgd deed capture latent structures sistent golden summaries example result wuhan wins men soccer tle chinese city games matches tion structure standard coder stand ignores latent structures erates loose sentences results results men volleyball chinese city games catch main points reason recurrent variational auto encoders framework better representation ability capture effective complicated tent structures sequence data summaries generated drgd tent latent structures ground truth leading better rouge evaluation conclusions propose deep recurrent generative decoder drgd improve abstractive tion performance model sequence oriented encoder decoder framework equipped latent structure modeling nent abstractive summaries generated based latent variables tic states extensive experiments benchmark table examples generated summaries hosts wuhan won men soccer title beating beijing shunyi chinese city games friday golden hosts wuhan wins men soccer title chinese city games stand results men volleyball chinese city games drgd wuhan wins men soccer title chinese city games unk china meteorological administration tuesday signed agreement long short term cooperation projects involving meteorological satellites satellite meteorology golden unk china cooperate ogy stand weather forecast major chinese cities drgd china cooperate meteorological satellites rand gained ground dollar opening wednesday greenback close tuesday golden rand gains ground stand rand slightly higher dollar drgd rand gains ground dollar new zealand women having children country birth rate reached highest level years statistics new zealand said wednesday golden new zealand birth rate reaches year high stand new zealand women having children birth rate hits highest level years drgd new zealand birth rate hits year high datasets drgd achieves improvements state art methods references samuel bowman luke vilnis oriol vinyals drew dai rafal jozefowicz samy gio generating sentences continuous space conll pages ziqiang cao wenjie sujian furu wei ran attsum joint learning focusing summarization neural attention coling pages asli celikyilmaz dilek hakkani tur brid hierarchical model multi document rization acl pages qian chen xiaodan zhu zhenhua ling wei hui jiang distraction based neural works document summarization ijcai pages jianpeng cheng mirella lapata neural summarization extracting sentences words acl pages kyunghyun cho bart van merrienboer caglar cehre dzmitry bahdanau fethi bougares holger learning schwenk yoshua bengio phrase representations rnn encoder decoder statistical machine translation emnlp pages sumit chopra michael auli alexander rush seas harvard abstractive sentence marization attentive recurrent neural networks naacl hlt pages junyoung chung kyle kastner laurent dinh kratarth goel aaron courville yoshua bengio recurrent latent variable model sequential data nips pages harold edmundson new methods journal acm jacm tomatic extracting jeffrey elman finding structure time nitive science gunes erkan dragomir radev lexrank graph based lexical centrality salience text journal articial intelligence summarization research jade goldstein vibhu mittal jaime carbonell mark kantrowitz multi document marization sentence extraction anlpworkshop pages regina barzilay kathleen mckeown sentence fusion multidocument news rization computational linguistics karol gregor ivo danihelka alex graves danilo rezende daan wierstra draw rent neural network image generation icml pages lidong bing piji liao wai lam weiwei guo rebecca passonneau abstractive document summarization phrase selection merging acl pages jiatao zhengdong hang victor incorporating copying mechanism acl pages sequence sequence learning sepp hochreiter jurgen schmidhuber neural computation long short term memory baotian qingcai chen fangze zhu sts large scale chinese short text summarization dataset emnlp pages hongyan jing kathleen mckeown cut naacl paste based text summarization pages yuta kikuchi graham neubig ryohei sasano hiroya takamura manabu okumura ling output length neural encoder decoders emnlp pages diederik kingma max welling arxiv preprint encoding variational bayes philipp koehn pharaoh beam search coder phrase based statistical machine conference association tion models machine translation americas pages springer ramesh nallapati bowen zhou caglar gulcehre bing xiang abstractive text rization sequence sequence rnns yond arxiv preprint ani nenkova kathleen mckeown survey mining text text summarization techniques data pages springer danilo jimenez rezende shakir mohamed daan wierstra stochastic backpropagation proximate inference deep generative models icml pages alexander rush sumit chopra jason weston neural attention model abstractive tence summarization emnlp pages jurgen schmidhuber deep learning neural networks overview neural networks hongya song zhaochun ren piji shangsong liang jun maarten rijke summarizing answers non factoid community question answering wsdm pages chen fei liu fuliang weng yang liu document summarization guided sentence pression emnlp pages theano development team theano python framework fast computation mathematical pressions arxiv prints xiaojun wan jianwu yang jianguo xiao manifold ranking based topic focused ijcai volume document summarization pages dingding wang shenghuo zhu tao yihong gong multi document summarization ing sentence based topic models acl ijcnlp pages wang hema raghavan vittorio castelli radu rian claire cardie sentence pression based framework query focused acl pages document summarization david zajic bonnie dorr richard schwartz hlt naacl bbn umd topiary pages piji lidong bing wai lam hang liao reader aware multi document tion sparse coding ijcai pages piji zihao wang wai lam zhaochun ren lidong bing salience estimation ational auto encoders multi document rization aaai pages chin yew lin rouge package matic evaluation summaries text tion branches proceedings shop volume konstantin lopyrev generating news lines recurrent neural networks arxiv preprint hans peter luhn automatic creation erature abstracts ibm journal research velopment yishu miao phil blunsom language latent variable discrete generative models tence compression emnlp pages ziheng lin min yen kan chew lim tan exploiting category specic information document summarization coling pages ramesh nallapati feifei zhai bowen zhou summarunner recurrent neural network based quence model extractive summarization ments aaai pages
