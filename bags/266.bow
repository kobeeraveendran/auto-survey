structsum summarization structured representations vidhisha balachandran dheeraj rajagopal artidoro pagnoni jaime carbonell school computer science carnegie mellon university pittsburgh usa vbalacha apagnoni jaylee dheeraj jgc cmu edu jay yoon lee yulia tsvetkov e f l c s c v v x r abstract abstractive text summarization aims pressing information long source ument rephrased condensed summary despite advances modeling techniques stractive summarization models suffer key challenges layout bias overt style training corpora ii limited abstractiveness optimized copying n grams source generating novel abstractive summaries iii lack transparency work propose pretable work based document level structure tion summarization address lenges end propose incorporating latent explicit dependencies tences source document end end single document summarization models framework complements standard decoder summarization models ing rich structure aware document representations based implicitly learned latent structures externally derived guistic explicit structures summarization framework trained cnn dm dataset improves coverage content source documents generates abstractive summaries generating novel n grams incorporates pretable sentence level structures forming par standard baselines introduction text summarization aims identifying important information long source documents ing human readable summaries nent methods generating summaries tive dorr et al nallapati et al important sentences source article lected form summary abstractive rush data available vidhishanair al et al model restructures rephrases essential content paraphrased summary state art approaches abstractive rization employ neural encoder decoder methods encode source document sequence tokens producing latent document tions decode summary conditioned representations recent studies suggest models suffer key challenges standard training datasets derived news articles model outputs strongly affected layout bias articles models ing leading sentences source documents kryscinski et al kedzie et al ond aim generate paraphrased summaries abstractive summarization systems copy long sequences source causing outputs resemble extractive summaries lin ng gehrmann et al finally current methods lend easily interpretation intermediate structures lin ng useful identifying major bottlenecks summarization models address challenges introduce sum framework incorporates structured ument representations summarization els structsum complements standard decoder architecture novel components latent structure attention module adapts structured representations kim et al liu lapata summarization task explicit structure attention module porates external linguistic structure e erence links complementary components incorporated learned jointly coder decoder shown figure encoders induced latent structures shown benet tasks including ument classication natural language inference liu lapata cheng et al machine translation kim et al tent structure attention module builds liu lapata model dependencies tween sentences document uses variant kirchhoff s matrix tree theorem tutte model dependencies non projective tree explicit attention module linguistically motivated aims incorporate inter sentence links externally annotated ument structures incorporate coreference based dependency graph sentences combined output latent ture attention module produce hybrid aware sentence representation test framework cnn dm dataset hermann et al outperforms base pointer generator model et al l nd latent explicit structures complementary contributing nal performance improvement modules orthogonal choice underlying decoder architecture rendering exible incorporated advanced models quantitative qualitative analyses maries generated structsum baselines reveal structure aware summarization gates news corpora layout bias improving coverage source document sentences ally structsum reduces bias copying large sequences source inherently making summaries abstractive generating novel n grams competitive baseline examples learned interpretable sentence dependency structures motivating research structure aware modeling structsum framework consider source document consisting n tences s sentence composed sequence words document tion aims map source document target summary y m words y typical neural stractive summarization system attentional sequence sequence model encodes sequence continuous sequence kens w standard encoder hochreiter schmidhuber vaswani et al encoder produces set hidden representations h decoder maps previously generated token hidden state computes soft attention probability distribution encoder hidden states distribution p vocabulary computed time step t network trained negative log likelihood loss losst log structsum modies architecture lows aggregate token representations encoder form sentence representations hierarchical encoders yang et al use explicit structure attention ules augment sentence representations sentence dependency information leveraging learned latent structure external structure nlp modules attended vectors passed decoder produces abstractive summary rest section describe framework architecture shown figure detail sentence representations consider encoder takes sequence words sentence w input produces contextual hidden representation word hwik wik kth word ith sentence k q q number words sentence word hidden representations max pooled sentence level passed sentence encoder produces new hidden sentence representations sentence hsi sentence hidden representations passed inputs latent explicit structure attention modules latent structure ls attention model latent structure source document non projective dependency tree sentences force pairwise attention module cally induce tree denote marginal ability dependency edge aij zij latent variable representing edge sentence sentence j terize unnormalized pairwise scores sentences neural network use choff s matrix tree theorem tutte pute marginal probability dependency edge sentences specically decompose representation sentence si semantic vector gsi structure vector dsi hsi gsi structure vectors dsi dsj compute score fij sentence pairs j sentence figure structsum incorporates latent structure ls explicit structure es attention produce structure aware representations structsum augments pointer generator model methodology proposed general applied encoder decoder summarization systems parent node sentence j score ri sentence root node fij ri fp fc fr linear projection tions build representations parent child root nodes respectively wa weight bilinear transformation fij edge weight nodes j weighted cency graph f computed pairs sentences fij ri compute ized attention scores aij ar variant kirchhoff s matrix tree theorem aij marginal probability dependency edge tween sentences j ar probability sentence root probabilistic attention weights semantic vectors gs compute tended sentence representations n psi ajigsj ar groot n csi aijgsi lsi psi csi possible parents sentence csi context vector gathered possible children groot special embedding root node updated sentence representation lsi incorporates implicit structural information explicit structure es attention following durrett et al showed modeling coreference knowledge anaphora constraints leads improved clarity cality incorporate cross sentence coreference links source explicit structure use shelf coreference identify coreferring mentions build coreference based sentence graph adding link tences sj coreferring tions graph converted weighted graph incorporating weight edge sentences proportional number unique coreferring mentions normalize edge weights sentence effectively building weighted adjacency matrix k kij given psi context vector gathered com kij p zij mj mv mi denotes set unique mentions mj denotes set co referring tence si mi mentions sentences z latent variable representing link ence sentence graph smoothing hyperparameter given contextual sentence representations hs explicit coreference based weighted jacency matrix k learn explicit aware representation follows usi tsi kijusj p esi fu fe linear projections esi updated sentence representation incorporates explicit structural information finally combine structural sentations concatenate latent explicit sentence vectors hsi lsi esi form coder sentence representations source ument provide token representation context entire document ken representations concatenated corresponding structure aware sentence tation hwij hwij hsi si tence word wij belongs resulting structure aware token representations directly replace previous token representations input decoder experiments dataset evaluate approach cnn daily mail hermann et al nallapati et al use cessing steps shown et al cnn dm train val test samples respectively reference summaries average tokens sentences differing et al truncate source documents tokens instead training validation sets model longer documents sentences ments trained nvidia gtx titan x gpus base model structsum framework incorporated encoder decoder work structure aware representations experiments chose pointer generator model et al base model simplicity ubiquitous usage neural tive summarization model different domains liu et al krishna et al word sentence encoders bilstm coder bilstm pointer based copy anism implement base pointer generator model augment structsum modules described model directly compared baselines addition base model compare structsum following baselines tan et al graph based attention model closest spirit method present work graph attention module learn attention sentences easily induce interpretable ument structures attention scores constrained learn structure latent interpretable structured attention tences structsum introduces explicit structure component inject external document structure distinguishes tan et al gehrmann et al work introduces separate content selector tags words phrases copied diffmask variant end end variant like included baselines compare structsum diffmask experiment hyperparameters encoder uses den states directions layer stm single layer decoder use adagrad optimizer duchi et al learning rate initial tor value use dropout use best results gehrmann et al outperform diffmask experiment use inference time hard ing applied baselines exclude reinforcement learning rl based systems directly comparable approach introduced encoder decoder based rl system rate pretraining compare recent contextual representation based models liu lapata com atulkum pointer nyu summarizer model pointer generator et al pointer generator coverage et al graph attention tan et al pointer generator diffmask gehrmann et al rouge rouge rouge l pointer generator implementation pointer generator coverage implementation latent structure ls attention explicit structure es attention ls es attention table evaluation summarization models cnn dm dataset published abstractive summarization line scores section shows implementations et al structsum results incorporate latent explicit document structure base models structsum s utility par base models introducing additional benets better abstractiveness intrepretability shown gradient clipping maximum norm selected best model early stopping based rouge score validation dataset criteria coverage penalty ing inference shown gehrmann et al decoding use beam search beam width observe signicant ments higher beam widths evaluation standard rouge metric shed ingful light quality summaries important dimensions recall based metric suitable assessing abstractiveness summarization agnostic layout biases facilitate intrepretability model cisions adopt automatic metrics tailored evaluating separately aspects compare structsum base model generator network coverage et al reference automatic metrics rst conduct standard comparison ated summaries reference summaries l lin metric table shows results rst observe introducing latent structures explicit structures dently improves performance rouge l suggests modeling dependencies sentences helps model compose better long sequences compared baselines small improvements org project ing retrieve similar content words baseline compose better contiguous sequences es ls independently similar performance results ls tion induces good latent dependencies pure external coreference knowledge finally combined model uses tent explicit structure performs best improvement points rouge l points base pointer generator model statistically signicant samples wilson condence test shows latent explicit information plementary model jointly leverage produce better summaries additionally nd structural inductive bias helps model converge faster combined attention model converges k iterations son k iterations required generator network rouge popular metric uating summarization models limited evaluating n gram overlap ignoring tic correctness compared method baseline pointer generator model bertscore metric zhang et al observe model improves bertscore points pointer generator v s structsum showing model able erate semantically correct content abstractiveness despite abstractive model generator model tends copy long sequences words including sentences figure comparison novel n grams structsum pointer erence sent indicates novel sentences figure coverage source sentences summary axis sentence position source article y axis shows normalized count tences position copied summary source document observed gehrmann et al use metrics evaluate tiveness model copy length table shows comparison average length copy len ous copied sequences source document greater length observe generator baseline average copies ous tokens source shows tive nature model indicates pointer networks aimed combining advantages abstractive extractive methods allowing copy content input document tend skew copying particularly dataset sequence model fails interrupt copying desirable sequence length contrast modeling document structure structsum reduces length copied sequences words average reducing bias copying sentences entirely average closer reference words comparison ricing task performance structsum learns stop needed generating coherent maries novel n grams proportion novel grams generated literature measure degree abstractiveness rization models et al figure pares percentage novel n grams structsum compared baseline model model duces novel trigrams time copies sentences time parison pointer generator network novel trigrams copies entire sentences time shows structsum average generates novel n grams comparison pointer generator baseline coverage direct outcome copying shorter sequences able cover content source document given length constraints serve leads better summarization mance compute coverage computing number source sentences ous sequences greater length copied summary table shows comparison coverage source sentences summary tent baseline pointer generator model copies source sentences structsum copies content source sentences additionally average length summaries produced structsum remains unchanged words average compared baseline model indicates sum produces summaries draw wider selection sentences original article pared baseline models layout bias neural abstractive summarization methods applied news articles typically biased ing generating summaries based rst sentences articles stems structure news articles present salient information article rst sentences copy len coverage depth structsum reference structsum table distribution latent tree depth table results analysis copying coverage distribution source sentences cnn dm test set copy len denotes average length copied sequences coverage coverage source sentences coref ner precision recall table precision recall es ls shared edges expand subsequent ones result lead baseline selects sentences article widely ture strong baseline evaluate summarization models applied news domain narayan et al kryscinski et al observed current summarization models learn exploit layout biases current datasets offer limited diversity outputs analyze structsum holds layout biases compute distribution source sentence indices copying content copied sequences length considered figure shows distributions source sentences covered summaries coverage sentences reference summaries shows high proportion sentences article copied summary ally reference summaries smoother tail end distribution relevant sentences sitions copied shows smooth bution sentences desirable feature notice pointer generator framework stronger bias beginning cle high concentration copied sentences sentences article trast structsum improves coverage slightly having lower concentration sentences copies tail end sentences baselines modeling structure help model reasonable gap compared reference distribution area improvement direction future work analysis induced document structures similar liu lapata look quality intermediate structures learned model use chu liu edmonds gorithm chu liu edmonds extract maximum spanning tree tion score matrix sentence structure table shows frequency tree depths nd average tree depth average proportion leaf nodes consistent results tree induction document tion ferracane et al compare latent trees extracted structsum rected graphs based coreference ner constructed similarly explicit coreference based sentence graphs ing sentences overlapping coreference tions named entities measure similarity learned latent trees explicit graphs precision recall edges results shown table observe latent graphs low recall tic graphs showing latent graphs capture coreference named entity overlaps explicitly suggesting latent explicit structures capture complementary information figure shows qualitative examples induced structures summaries sum rst example shows tree sentence chosen root key sentence tioned reference examples tences lower level dependency tree contribute generated summary ilarly examples source sentences generate summaries tend closer root node rst summary source content tences summary root node depth root node second ample source sentences tree examples generated summaries diverged reference omitting certain sentences reference sentences lower section tree providing sights sentences preferred figure examples induced structures generated summaries summary generation example latent structures cluster sentences based main topic document sentences differ sentences topic discussed model clustered sets separately related work data driven neural summarization falls tive cheng et al zhang et al abstractive rush et al et al gehrmann et al chen bansal pointer generator et al learns generate novel vocabulary words copy source foundation work abstractive summarization gehrmann et al hsu et al song et al model extends incorporating latent explicit structure extensions applicable encoder decoder architecture ple follow study shown benets method multi document summarization chowdhury et al pre neural era document structure played critical role summarization leskovec et al litvak liu et al rett et al kikuchi et al cently song et al infuse source syntactic structure pointer generator level syntactic features augmenting decoder copy mechanism contrast model sentence dependencies latent structures plicit coreference structures use tics salient features li et al propose structural compression coverage regularizers incorporating structural bias target summaries model structure source ument frermann klementiev induce latent structures aspect based summarization cohan et al focus summarization scientic papers isonuma et al reviews supervised summarization mithun kosseim use discourse structures improve ence blog summarization ren et al use sentence relations multi document rization complementary directions work knowledge structsum rst jointly incorporate latent explicit document structure summarization framework conclusion future work work propose framework sum incorporating latent explicit document documentlatent leicester city rejected approaches striker tom lawrence astonishing clubs manchester united forward barely played leicester arriving old trafford summer manager nigel pearson wants options available battles odds leicester premier league lawrence poised international debut wales european championship qualifier israel saturday figured games leicester season substitute leicester city rejected approaches striker tom lawrence astonishing clubs championship promotion chasers bournemouth ipswich wolves asked lawrence blackburn charlton leeds bolton rotherham wigan contact looking options gasp bid bolster squad reference bournemouth ipswich wolves blackburn charlton leeds bolton rotherham wigan asked tom lawrence year old poised international debut wales leicester manager nigel pearson wants options available structsum leicester city rejected approaches tom lawrence lawrence poised debut wales european championship qualifier israel saturday leicester city looking options gasp bid bolster squad lawrence old trafford figured games leicester season substitute manchester united star andrew henderson celebrated landing london broncos coaching job permanent basis halifax beaten henderson given nod london hierarchy week mixed spell caretaker charge departure joey grima weakened fine crown appointment scoring tries daniel harrison matt garside iliess macani brad dwyer score winning iliess macani pictured year scored london broncos tries win halifax james saltonstall ben heaton mitch cahalane scored halifax henderson spoken earlier week felt broncos moving right direction narrow victory substance words win kingstone press championship having relegated super league end season andrew henderson won game broncos time coach daniel harrison matt garside iliess macani brad dwyer scored james saltonstall ben heaton mitch cahalane scored halifax structsum andrew henderson celebrated landing coaching job permanent basis henderson given nod london hierarchy week mixed spell win halifax weakened fine crown appointment felt broncos moving right direction win press championship having relegated structure neural abstractive summarization introduce novel explicit attention module incorporates external linguistic structures tiating coreference links framework improves abstractiveness coverage generated summaries helps igate layout biases associated prior models present extensive evaluation abstractiveness coverage layout titatively future work investigate role document structures pretrained language models lewis et al liu lapata acknowledgements anonymous authors grateful invaluable feedback reviewers sandeep subramanian waleed ammar kathryn mazaitis help support ious stages project material based work supported darpa semafor nnsa doe programs opinions ndings conclusions recommendations expressed material essarily reect views darpa nnsa like thank amazon providing gpu credits references yen chun chen mohit bansal fast tive summarization reinforce selected sentence rewriting proceedings annual ing association computational tics volume long papers pages bourne australia association computational linguistics jianpeng cheng li dong mirella lapata long short term memory networks machine reading proceedings conference empirical methods natural language processing pages austin texas association putational linguistics tanya chowdhury sachin kumar chakraborty rization structural attention tanmoy neural abstractive arxiv preprint yau chu tung kuan liu shortest arborescence directed graph science sinica arman cohan franck dernoncourt doo soon kim trung bui seokhwan kim walter chang zli goharian discourse aware attention model abstractive summarization long proceedings conference ments north american chapter association computational linguistics human language nologies volume short papers pages new orleans louisiana association tional linguistics bonnie dorr david zajic richard schwartz hedge trimmer parse trim approach headline generation proceedings naacl text summarization workshop volume pages association computational guistics john duchi elad hazan yoram singer adaptive subgradient methods online learning journal machine stochastic optimization learning research greg durrett taylor berg kirkpatrick dan klein learning based single document tion compression anaphoricity constraints proceedings annual meeting sociation computational linguistics volume long papers pages berlin germany association computational linguistics jack edmonds optimum branchings journal research national bureau standards b elisa ferracane greg durrett junyi jessy li trin erk evaluating discourse structured text representations acl lea frermann alexandre klementiev ducing document structure aspect based proceedings annual rization ing association computational linguistics pages sebastian gehrmann yuntian deng alexander m rush abstractive summarization emnlp karl moritz hermann tomas kocisky edward stette lasse espeholt kay mustafa suleyman phil blunsom teaching machines read comprehend advances neural information processing systems pages sepp hochreiter jurgen schmidhuber neural computation long short term memory wan ting hsu chieh kai lin ming ying lee kerui min jing tang min sun unied model extractive abstractive summarization inconsistency loss acl masaru isonuma junichiro mori ichiro sakata unsupervised neural single document marization reviews learning latent discourse structure ranking acl chris kedzie kathleen mckeown hal daume iii content selection deep learning models proceedings summarization ference empirical methods natural language processing pages brussels belgium association computational linguistics yuta kikuchi tsutomu hirao hiroya takamura abu okumura masaaki nagata single document summarization based nested tree ture proceedings annual meeting association computational linguistics ume short papers pages yoon kim carl denton luong hoang der m rush structured attention networks international conference learning sentations iclr toulon france april conference track proceedings kundan krishna sopan khosla jeffrey p bigham zachary c lipton generating soap notes doctor patient conversations arxiv preprint wojciech kryscinski nitish shirish keskar bryan cann caiming xiong richard socher neural text summarization critical evaluation proceedings conference empirical methods natural language processing international joint conference natural guage processing emnlp ijcnlp pages hong kong china association tional linguistics jure leskovec marko grobelnik natasa frayling learning sub structures ment semantic graphs document summarization linkkdd workshop pages mike lewis yinhan liu naman goyal jan ghazvininejad abdelrahman mohamed omer levy ves stoyanov luke zettlemoyer bart denoising sequence sequence pre training natural language generation translation comprehension arxiv wei li xinyan xiao yajuan lyu yuanzhuo wang improving neural abstractive document marization structural regularization ceedings conference empirical ods natural language processing pages chin yew lin rouge package matic evaluation summaries text summarization branches hui lin vincent ng abstractive rization survey state art ceedings aaai conference articial ligence volume pages multilingual information extraction rization pages association tional linguistics fei liu jeffrey flanigan sam thomson norman sadeh noah smith tive summarization semantic representations proceedings conference north american chapter association tional linguistics human language technologies pages denver colorado association computational linguistics yang liu mirella lapata learning tured text representations transactions ciation computational linguistics yang liu mirella lapata marization pretrained encoders text ijcnlp zhengyuan liu angela ng sheldon lee shao guang aiti aw nancy f chen topic aware pointer generator networks summarizing spoken conversations ieee automatic speech nition understanding workshop asru pages shamima mithun leila kosseim discourse structures reduce discourse incoherence blog summarization proceedings international conference recent advances natural language processing pages hissar bulgaria association computational linguistics ramesh nallapati feifei zhai bowen zhou summarunner recurrent neural network based quence model extractive summarization ments thirty aaai conference articial intelligence ramesh nallapati bowen zhou cicero dos santos c aglar bing xiang tive text summarization sequence sequence proceedings rnns signll conference computational natural guage learning pages berlin germany association computational linguistics shashi narayan shay b cohen mirella lapata nt details summary topic aware convolutional neural networks proceedings treme summarization conference empirical methods natural guage processing pages brussels gium association computational linguistics pengjie ren zhumin chen zhaochun ren furu wei liqiang nie jun ma maarten de rijke sentence relations extractive summarization deep neural networks acm transactions mation systems tois marina litvak mark graph based word extraction single document summarization proceedings workshop multi source alexander m rush sumit chopra jason weston neural attention model abstractive proceedings tence summarization conference empirical methods natural guage processing emnlp lisbon portugal september pages abigail peter j liu christopher d manning point summarization generator networks proceedings nual meeting association computational linguistics volume long papers pages vancouver canada association tional linguistics kaiqiang song lin zhao fei liu infused copy mechanisms abstractive tion coling jiwei tan xiaojun wan jianguo xiao abstractive document summarization proceedings based attentional neural model annual meeting association computational linguistics volume long papers pages william thomas tutte graph theory vol encyclopedia mathematics applications ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan n gomez ukasz kaiser illia polosukhin attention advances neural information need cessing systems pages zichao yang diyi yang chris dyer xiaodong alexander j smola eduard h hovy erarchical attention networks document cation hlt naacl tianyi zhang v kishore felix wu kilian q berger yoav artzi bertscore evaluating text generation bert iclr xingxing zhang mirella lapata furu wei ming zhou neural latent extractive document marization proceedings conference empirical methods natural language ing pages brussels belgium association computational linguistics
