structsum summarization via structured representations vidhisha balachandran dheeraj rajagopal artidoro pagnoni jaime carbonell school of computer science carnegie mellon university pittsburgh usa vbalacha apagnoni jaylee dheeraj jgc cmu edu jay yoon lee yulia tsvetkov e f l c s c v v i x r a abstract abstractive text summarization aims at pressing the information of a long source ument into a rephrased condensed summary despite advances in modeling techniques stractive summarization models still suffer from several key challenges i layout bias they overt to the style of training corpora ii limited abstractiveness they are optimized to copying n grams from the source rather than generating novel abstractive summaries iii lack of transparency they are not in this work we propose a pretable work based on document level structure tion for summarization to address these lenges to this end we propose incorporating latent and explicit dependencies across tences in the source document into end to end single document summarization models our framework complements standard decoder summarization models by ing them with rich structure aware document representations based on implicitly learned latent structures and externally derived guistic explicit structures we show that our summarization framework trained on the cnn dm dataset improves the coverage of content in the source documents generates more abstractive summaries by generating more novel n grams and incorporates pretable sentence level structures while forming on par with standard baselines introduction text summarization aims at identifying important information in long source documents and ing it in human readable summaries two nent methods of generating summaries are tive dorr et al nallapati et al where important sentences in the source article are lected to form a summary and abstractive rush and data available at vidhishanair al see et al where the model restructures and rephrases essential content into a paraphrased summary state of the art approaches to abstractive rization employ neural encoder decoder methods that encode the source document as a sequence of tokens producing latent document tions and decode the summary conditioned on the representations recent studies suggest that these models suffer from several key challenges first since standard training datasets are derived from news articles model outputs are strongly affected by the layout bias of the articles with models ing on the leading sentences of source documents kryscinski et al kedzie et al ond although they aim to generate paraphrased summaries abstractive summarization systems ten copy long sequences from the source causing their outputs to resemble extractive summaries lin and ng gehrmann et al finally current methods do not lend themselves easily to interpretation via intermediate structures lin and ng which could be useful for identifying major bottlenecks in summarization models to address these challenges we introduce sum a framework that incorporates structured ument representations into summarization els structsum complements a standard decoder architecture with two novel components a latent structure attention module that adapts structured representations kim et al liu and lapata for the summarization task and an explicit structure attention module that porates an external linguistic structure e erence links the two complementary components are incorporated and learned jointly with the coder and decoder as shown in figure encoders with induced latent structures have been shown to benet several tasks including ument classication natural language inference liu and lapata cheng et al and machine translation kim et al our tent structure attention module builds upon liu and lapata to model the dependencies tween sentences in a document it uses a variant of kirchhoff s matrix tree theorem tutte to model such dependencies as non projective tree the explicit attention module is linguistically motivated and aims to incorporate inter sentence links from externally annotated ument structures we incorporate a coreference based dependency graph across sentences which is then combined with the output of the latent ture attention module to produce a hybrid aware sentence representation we test our framework using the cnn dm dataset hermann et al and show in that it outperforms the base pointer generator model see et al by up to l we nd that the latent and explicit structures are complementary both contributing to the nal performance improvement our modules are also orthogonal to the choice of an underlying decoder architecture rendering them exible to be incorporated into other advanced models quantitative and qualitative analyses of maries generated by structsum and baselines reveal that structure aware summarization gates the news corpora layout bias by improving the coverage of source document sentences ally structsum reduces the bias of copying large sequences from the source inherently making the summaries more abstractive by generating more novel n grams than a competitive baseline we also show examples of the learned interpretable sentence dependency structures motivating further research for structure aware modeling structsum framework consider a source document consisting of n tences s where each sentence is composed of a sequence of words document tion aims to map the source document to a target summary y of m words y a typical neural stractive summarization system is an attentional sequence to sequence model that encodes the put sequence as a continuous sequence of kens w using a standard encoder hochreiter and schmidhuber vaswani et al the encoder produces a set of hidden representations h a decoder maps the previously generated token to a hidden state and computes a soft attention probability distribution over encoder hidden states a distribution p over the vocabulary is computed at every time step t and the network is trained using the negative log likelihood loss losst log structsum modies the above architecture as lows we aggregate the token representations from the encoder to form sentence representations as in hierarchical encoders yang et al we then use and explicit structure attention ules to augment the sentence representations with sentence dependency information leveraging both a learned latent structure and an external structure from other nlp modules the attended vectors are then passed to the decoder which produces the put abstractive summary in the rest of this section we describe our framework architecture shown in figure in detail sentence representations we consider an encoder which takes a sequence of words in a sentence w as input and produces contextual hidden representation for each word hwik where wik is the kth word of the ith sentence k q and q is the number of words in the sentence the word hidden representations are max pooled at the sentence level and passed through a sentence encoder which produces new hidden sentence representations for each sentence hsi the sentence hidden representations are then passed as inputs to the latent and explicit structure attention modules latent structure ls attention we model the latent structure of a source document as a non projective dependency tree of sentences and force a pairwise attention module to cally induce this tree we denote the marginal ability of a dependency edge as aij where zij is the latent variable representing the edge from sentence i to sentence j we terize the unnormalized pairwise scores between sentences with a neural network and use the choff s matrix tree theorem tutte to pute the marginal probability of a dependency edge between any two sentences specically we decompose the representation of a sentence si into a semantic vector gsi and structure vector dsi as hsi gsi using the structure vectors dsi dsj we compute a score fij between sentence pairs i j where sentence i is figure structsum incorporates latent structure ls and explicit structure es attention to produce structure aware representations here structsum augments the pointer generator model but the methodology that we proposed is general and it can be applied to other encoder decoder summarization systems the parent node of sentence j and a score ri where the sentence is the root node fij and ri where fp fc and fr are linear projection tions that build representations for the parent child and root nodes respectively and wa is the weight for bilinear transformation here fij is the edge weight between nodes i j in a weighted cency graph f and is computed for all pairs of sentences using fij and ri we compute ized attention scores aij and ar i using a variant of kirchhoff s matrix tree theorem where aij is the marginal probability of a dependency edge tween sentences i j and ar i is the probability of sentence i being the root using these probabilistic attention weights and the semantic vectors gs we compute the tended sentence representations as n psi ajigsj ar i groot n csi aijgsi lsi psi csi possible parents of sentence i csi is the context vector gathered from possible children and groot is a special embedding for the root node here the updated sentence representation lsi incorporates the implicit structural information explicit structure es attention following durrett et al who showed that modeling coreference knowledge through anaphora constraints leads to improved clarity or cality we incorporate cross sentence coreference links as the source of explicit structure first we use an off the shelf coreference to identify coreferring mentions we then build a coreference based sentence graph by adding a link between tences sj if they have any coreferring tions this graph is converted into a weighted graph by incorporating a weight on the edge between two sentences that is proportional to the number of unique coreferring mentions between them we normalize these edge weights for every sentence effectively building a weighted adjacency matrix k where kij is given by where psi is the context vector gathered from com kij p zij mj mv where mi denotes the set of unique mentions in mj denotes the set of co referring tence si mi mentions between the two sentences and z is a latent variable representing a link in the ence sentence graph is a smoothing hyperparameter given contextual sentence representations hs and our explicit coreference based weighted jacency matrix k we learn an explicit aware representation as follows usi tsi kijusj p esi where fu and fe are linear projections and esi is an updated sentence representation which incorporates explicit structural information finally to combine the two structural sentations we concatenate the latent and explicit sentence vectors as hsi lsi esi to form coder sentence representations of the source ument to provide every token representation with the context of the entire document the ken representations are concatenated with their corresponding structure aware sentence tation hwij hwij hsi where si is the tence to which the word wij belongs the resulting structure aware token representations can be used to directly replace previous token representations as input to the decoder experiments dataset we evaluate our approach on the cnn daily mail hermann et al nallapati et al and use the same cessing steps as shown in see et al the cnn dm has train val test samples respectively the reference summaries have an average of tokens and sentences differing from see et al we truncate source documents to tokens instead of in training and validation sets to model longer documents with more sentences all our ments were trained on nvidia gtx titan x gpus base model although structsum framework can be incorporated in any encoder decoder work with structure aware representations for our experiments we chose the pointer generator model see et al as the base model due to its simplicity and ubiquitous usage as a neural tive summarization model across different domains liu et al krishna et al the word and sentence encoders are bilstm and the coder is a bilstm with a pointer based copy anism we re implement the base pointer generator model and augment it with the structsum modules described in and hence our model can be directly compared to it baselines in addition to the base model we compare structsum with the following baselines tan et al this is a graph based attention model that is closest in spirit to the method we present in this work a graph attention module is used to learn attention between sentences but it can not be easily used to induce interpretable ument structures since its attention scores are not constrained to learn structure on top of latent and interpretable structured attention between tences structsum introduces an explicit structure component to inject external document structure which distinguishes it from tan et al gehrmann et al this work introduces a separate content selector which tags words and phrases to be copied the diffmask variant is an end to end variant like ours and hence is included in our baselines we compare structsum with the diffmask experiment hyperparameters our encoder uses den states for both directions in the one layer stm and for the single layer decoder we use the adagrad optimizer duchi et al with a learning rate of and an initial tor value of we do not use dropout and use best results from gehrmann et al outperform diffmask experiment but they use inference time hard ing which can be applied on ours our baselines also exclude reinforcement learning rl based systems as they are not directly comparable but our approach can be introduced in an encoder decoder based rl system since we do not rate any pretraining we do not compare with recent contextual representation based models liu and lapata com atulkum pointer nyu summarizer model pointer generator see et al pointer generator coverage see et al graph attention tan et al pointer generator diffmask gehrmann et al rouge rouge rouge l pointer generator re implementation pointer generator coverage re implementation latent structure ls attention explicit structure es attention ls es attention table evaluation of summarization models on the cnn dm dataset published abstractive summarization line scores are on top the bottom section shows re implementations of see et al and structsum results that incorporate latent and explicit document structure into the base models structsum s utility is on par with the base models while introducing additional benets of better abstractiveness and intrepretability shown in gradient clipping with a maximum norm of we selected the best model using early stopping based on the rouge score on the validation dataset as our criteria we also used the coverage penalty ing inference as shown in gehrmann et al for decoding we use beam search with a beam width of we did not observe signicant ments with higher beam widths evaluation a standard rouge metric does not shed ingful light into the quality of summaries across important dimensions as a recall based metric it is not suitable for assessing the abstractiveness of summarization it is also agnostic to layout biases and does not facilitate intrepretability of model cisions we thus adopt automatic metrics tailored to evaluating separately each of these aspects we compare structsum to our base model the generator network with coverage see et al and the reference automatic metrics we rst conduct a standard comparison of ated summaries with reference summaries using and l lin metric table shows the results we rst observe that introducing the latent structures and explicit structures dently improves our performance on rouge l it suggests that modeling dependencies between sentences helps the model compose better long sequences compared to baselines we see small improvements in and org project ing that we retrieve similar content words as the baseline but compose them into better contiguous sequences as both es and ls independently get similar performance the results show that ls tion induces good latent dependencies that make up for pure external coreference knowledge finally our combined model which uses both tent and explicit structure performs the best with an improvement of points in rouge l and points in over base pointer generator model statistically signicant for samples at using wilson condence test it shows that the latent and explicit information are plementary and a model can jointly leverage them to produce better summaries additionally we nd that structural inductive bias helps a model to converge faster the combined attention model converges in k iterations in son to k iterations required for the generator network while rouge is a popular metric used for uating summarization models it is limited to only evaluating n gram overlap while ignoring tic correctness hence we compared our method with the baseline pointer generator model using the bertscore metric zhang et al we observe that our model improves bertscore by points for pointer generator v s for structsum showing that our model is able to erate semantically correct content abstractiveness despite being an abstractive model the generator model tends to copy very long sequences of words including whole sentences from the figure comparison of novel n grams between structsum pointer and the erence here sent indicates full novel sentences figure coverage of source sentences in summary here the axis is the sentence position in the source article and y axis shows the normalized count of tences in that position copied to the summary source document also observed by gehrmann et al we use two metrics to evaluate the tiveness of the model copy length table shows a comparison of the average length copy len of ous copied sequences from the source document greater than length we observe that the generator baseline on average copies ous tokens from the source which shows the tive nature of the model this indicates that pointer networks aimed at combining advantages from abstractive and extractive methods by allowing to copy content from the input document tend to skew towards copying particularly in this dataset a sequence of this is that the model fails to interrupt copying at desirable sequence length in contrast modeling document structure through structsum reduces the length of copied sequences to words on average reducing the bias of copying sentences entirely this average is closer to the reference words in comparison without ricing task performance structsum learns to stop when needed while still generating coherent maries novel n grams the proportion of novel grams generated has been used in the literature to measure the degree of abstractiveness of rization models see et al figure pares the percentage of novel n grams in structsum as compared to the baseline model our model duces novel trigrams of the time and copies whole sentences only of the time in parison the pointer generator network has only novel trigrams and copies entire sentences of the time this shows that structsum on average generates more novel n grams in comparison to the pointer generator baseline coverage a direct outcome of copying shorter sequences is being able to cover more content from the source document within given length constraints we serve that this leads to better summarization mance we compute coverage by computing the number of source sentences from which ous sequences greater than length are copied in the summary table shows a comparison of the coverage of source sentences in the summary tent while the baseline pointer generator model only copies from of the source sentences structsum copies content from of the source sentences additionally the average length of the summaries produced by structsum remains mostly unchanged at words on average compared to of the baseline model this indicates that sum produces summaries that draw from a wider selection of sentences from the original article pared to the baseline models layout bias neural abstractive summarization methods applied to news articles are typically biased towards ing and generating summaries based on the rst few sentences of the articles this stems from the structure of news articles which present the salient information of the article in the rst few sentences copy len coverage depth structsum reference structsum table distribution of latent tree depth table results of analysis of copying and coverage distribution over the source sentences on cnn dm test set copy len denotes the average length of copied sequences coverage coverage of source sentences coref ner precision recall table precision and recall of es and ls shared edges and expand in the subsequent ones as a result the lead baseline which selects the top three sentences of an article is widely used in the ture as a strong baseline to evaluate summarization models applied to the news domain narayan et al kryscinski et al observed that the current summarization models learn to exploit the layout biases of current datasets and offer limited diversity in their outputs to analyze whether structsum also holds the same layout biases we compute a distribution of source sentence indices that are used for copying content copied sequences of length or more are considered figure shows the distributions of source sentences covered in the summaries the coverage of sentences in the reference summaries shows a high proportion of the top sentences of any article being copied to the summary ally the reference summaries have a smoother tail end distribution with relevant sentences in all sitions being copied it shows that a smooth bution over all sentences is a desirable feature we notice that the pointer generator framework have a stronger bias towards the beginning of the cle with a high concentration of copied sentences within the top sentences of the article in trast structsum improves coverage slightly having a lower concentration of top sentences and copies more tail end sentences than the baselines ever although the modeling of structure does help our model has a reasonable gap compared to the reference distribution we see this as an area of improvement and a direction for future work analysis of induced document structures similar to liu and lapata we also look at the quality of the intermediate structures learned by the model we use the chu liu edmonds gorithm chu and liu edmonds to extract the maximum spanning tree from the tion score matrix as our sentence structure table shows the frequency of various tree depths we nd that the average tree depth is and the average proportion of leaf nodes is consistent with results from tree induction in document tion ferracane et al further we compare latent trees extracted from structsum with rected graphs based on coreference on ner or on both these are constructed similarly to our explicit coreference based sentence graphs in by ing sentences with overlapping coreference tions or named entities we measure the similarity between the learned latent trees and the explicit graphs through precision and recall over edges the results are shown in table we observe that our latent graphs have low recall with the tic graphs showing that our latent graphs do not capture the coreference or named entity overlaps explicitly suggesting that the latent and explicit structures capture complementary information figure shows qualitative examples of induced structures along with summaries from the sum the rst example shows a tree with sentence chosen as root which was the key sentence tioned in the reference in both examples the tences in the lower level of the dependency tree contribute less to the generated summary ilarly in the examples source sentences used to generate summaries tend to be closer to the root node in the rst summary all source content tences used in the summary are either the root node or within depth of the root node in the second ample out of source sentences were at in the tree in both examples generated summaries diverged from the reference by omitting certain sentences used in the reference these sentences are in the lower section of the tree providing sights on which sentences were preferred for the figure examples of induced structures and generated summaries summary generation we also see in example that the latent structures cluster sentences based on the main topic of the document sentences differ from sentences in the topic discussed and our model clustered the two sets separately related work data driven neural summarization falls into tive cheng et al zhang et al or abstractive rush et al see et al gehrmann et al chen and bansal pointer generator see et al learns to either generate novel in vocabulary words or copy from the source it has been the foundation for much work on abstractive summarization gehrmann et al hsu et al song et al our model extends it by incorporating latent explicit structure but these extensions are applicable to any other encoder decoder architecture for ple a follow up study has already shown benets of our method in multi document summarization chowdhury et al in pre neural era document structure played a critical role in summarization leskovec et al litvak and last liu et al rett et al kikuchi et al more cently song et al infuse source syntactic structure into the pointer generator using level syntactic features and augmenting them to decoder copy mechanism in contrast we model sentence dependencies as latent structures and plicit coreference structures we do not use tics or salient features li et al propose structural compression and coverage regularizers incorporating structural bias of target summaries while we model the structure of the source ument frermann and klementiev induce latent structures for aspect based summarization cohan et al focus on summarization of scientic papers isonuma et al reviews supervised summarization mithun and kosseim use discourse structures to improve ence in blog summarization and ren et al use sentence relations for multi document rization these are complementary directions to our work to our knowledge structsum is the rst to jointly incorporate latent and explicit document structure in a summarization framework conclusion and future work in this work we propose the framework sum for incorporating latent and explicit document documentlatent leicester city have rejected approaches for striker tom lawrence from an astonishing nine clubs the former manchester united forward has barely played for leicester since arriving from old trafford in the summer but manager nigel pearson wants to have all options available as he battles against the odds to keep leicester in the premier league lawrence is poised to make his full international debut for wales in their european championship qualifier with israel on saturday but has only figured in four games for leicester this season and three as a substitute leicester city have rejected approaches for striker tom lawrence from an astonishing nine clubs championship promotion chasers bournemouth ipswich and wolves have all asked about lawrence blackburn charlton leeds bolton rotherham and wigan have also made contact however they are now looking at other options in a last gasp bid to bolster their squad reference bournemouth ipswich wolves blackburn charlton leeds bolton rotherham and wigan have all asked about tom lawrence the year old is poised to make his full international debut for wales leicester manager nigel pearson wants to have options available structsum leicester city have rejected approaches for tom lawrence lawrence is poised to make his debut for wales in their european championship qualifier with israel on saturday leicester city are looking at other options in last gasp bid to bolster their squad lawrence from old trafford has only figured in four games for leicester this season and three as a substitute the former manchester united star has andrew henderson celebrated landing the london broncos coaching job on a permanent basis as halifax were beaten henderson was given the nod by the london hierarchy this week after a mixed spell in caretaker charge since the departure of joey grima his weakened side put on a fine show to crown his appointment though scoring four tries through daniel harrison matt garside iliess macani and brad dwyer whose score was the winning one iliess macani pictured last year scored one of london broncos four tries in the win over halifax james saltonstall ben heaton and mitch cahalane scored for halifax henderson had spoken earlier in the week about how he felt broncos were moving in the right direction and their narrow victory put some substance to his words the win was just their third in six in the kingstone press championship having been relegated from super league at the end of last season andrew henderson won his first game as broncos full time coach daniel harrison matt garside iliess macani and brad dwyer all scored james saltonstall ben heaton and mitch cahalane scored for halifax structsum andrew henderson celebrated landing the coaching job on a permanent basis henderson was given the nod by london hierarchy this week after a mixed spell in the win over halifax his weakened side put on fine show to crown his appointment though he felt broncos were moving in the right direction the win was their third in six in the press championship having been relegated structure in neural abstractive summarization we introduce a novel explicit attention module which incorporates external linguistic structures tiating it with coreference links we show that our framework improves the abstractiveness and coverage of generated summaries and helps igate layout biases associated with prior models we present an extensive evaluation of along abstractiveness coverage and layout titatively future work will investigate the role of document structures in pretrained language models lewis et al liu and lapata acknowledgements their to the anonymous the authors are grateful invaluable feedback and reviewers for to sandeep subramanian waleed ammar and kathryn mazaitis for their help and support in ious stages of the project this material is based upon work supported by the darpa semafor and nnsa doe programs any opinions ndings and conclusions or recommendations expressed in this material are those of the and do not essarily reect the views of the darpa or nnsa we would also like to thank amazon for providing gpu credits references yen chun chen and mohit bansal fast tive summarization with reinforce selected sentence rewriting in proceedings of the annual ing of the association for computational tics volume long papers pages bourne australia association for computational linguistics jianpeng cheng li dong and mirella lapata long short term memory networks for machine reading in proceedings of the conference on empirical methods in natural language processing pages austin texas association for putational linguistics tanya chowdhury sachin kumar chakraborty rization with structural attention and tanmoy neural abstractive arxiv preprint yau chu and tung kuan liu on the shortest arborescence of a directed graph science sinica arman cohan franck dernoncourt doo soon kim trung bui seokhwan kim walter chang and zli goharian a discourse aware attention model for abstractive summarization of long in proceedings of the conference of ments the north american chapter of the association for computational linguistics human language nologies volume short papers pages new orleans louisiana association for tional linguistics bonnie dorr david zajic and richard schwartz hedge trimmer a parse and trim approach to headline generation in proceedings of the naacl on text summarization workshop volume pages association for computational guistics john duchi elad hazan and yoram singer adaptive subgradient methods for online learning journal of machine and stochastic optimization learning research greg durrett taylor berg kirkpatrick and dan klein learning based single document tion with compression and anaphoricity constraints in proceedings of the annual meeting of the sociation for computational linguistics volume long papers pages berlin germany association for computational linguistics jack edmonds optimum branchings journal of research of the national bureau of standards b elisa ferracane greg durrett junyi jessy li and trin erk evaluating discourse in structured text representations in acl lea frermann and alexandre klementiev ducing document structure for aspect based in proceedings of the annual rization ing of the association for computational linguistics pages sebastian gehrmann yuntian deng and alexander m rush bottom up abstractive summarization in emnlp karl moritz hermann tomas kocisky edward stette lasse espeholt will kay mustafa suleyman and phil blunsom teaching machines to read and comprehend in advances in neural information processing systems pages sepp hochreiter and jurgen schmidhuber neural computation long short term memory wan ting hsu chieh kai lin ming ying lee kerui min jing tang and min sun a unied model for extractive and abstractive summarization using inconsistency loss in acl masaru isonuma junichiro mori and ichiro sakata unsupervised neural single document marization of reviews via learning latent discourse structure and its ranking in acl chris kedzie kathleen mckeown and hal daume iii content selection in deep learning models of in proceedings of the summarization ference on empirical methods in natural language processing pages brussels belgium association for computational linguistics yuta kikuchi tsutomu hirao hiroya takamura abu okumura and masaaki nagata single document summarization based on nested tree ture in proceedings of the annual meeting of the association for computational linguistics ume short papers pages yoon kim carl denton luong hoang and der m rush structured attention networks in international conference on learning sentations iclr toulon france april conference track proceedings kundan krishna sopan khosla jeffrey p bigham and zachary c lipton generating soap notes from doctor patient conversations arxiv preprint wojciech kryscinski nitish shirish keskar bryan cann caiming xiong and richard socher neural text summarization a critical evaluation in proceedings of the conference on empirical methods in natural language processing and the international joint conference on natural guage processing emnlp ijcnlp pages hong kong china association for tional linguistics jure leskovec marko grobelnik and natasa frayling learning sub structures of ment semantic graphs for document summarization in linkkdd workshop pages mike lewis yinhan liu naman goyal jan ghazvininejad abdelrahman mohamed omer levy ves stoyanov and luke zettlemoyer bart denoising sequence to sequence pre training for natural language generation translation and comprehension arxiv wei li xinyan xiao yajuan lyu and yuanzhuo wang improving neural abstractive document in marization with structural regularization ceedings of the conference on empirical ods in natural language processing pages chin yew lin rouge a package for matic evaluation of summaries text summarization branches out hui lin and vincent ng abstractive in rization a survey of the state of the art ceedings of the aaai conference on articial ligence volume pages multilingual information extraction and rization pages association for tional linguistics fei liu jeffrey flanigan sam thomson norman sadeh and noah a smith toward tive summarization using semantic representations in proceedings of the conference of the north american chapter of the association for tional linguistics human language technologies pages denver colorado association for computational linguistics yang liu and mirella lapata learning tured text representations transactions of the ciation for computational linguistics yang liu and mirella lapata marization with pretrained encoders text ijcnlp zhengyuan liu angela ng sheldon lee shao guang aiti aw and nancy f chen topic aware pointer generator networks for summarizing spoken conversations ieee automatic speech nition and understanding workshop asru pages shamima mithun and leila kosseim discourse structures to reduce discourse incoherence in blog summarization in proceedings of the international conference recent advances in natural language processing pages hissar bulgaria association for computational linguistics ramesh nallapati feifei zhai and bowen zhou summarunner a recurrent neural network based quence model for extractive summarization of ments in thirty first aaai conference on articial intelligence ramesh nallapati bowen zhou cicero dos santos c aglar and bing xiang tive text summarization using sequence to sequence in proceedings of the rnns and beyond signll conference on computational natural guage learning pages berlin germany association for computational linguistics shashi narayan shay b cohen and mirella lapata do nt give me the details just the summary topic aware convolutional neural networks for in proceedings of the treme summarization conference on empirical methods in natural guage processing pages brussels gium association for computational linguistics pengjie ren zhumin chen zhaochun ren furu wei liqiang nie jun ma and maarten de rijke sentence relations for extractive summarization with deep neural networks acm transactions on mation systems tois marina litvak and mark last graph based word extraction for single document summarization in proceedings of the workshop on multi source alexander m rush sumit chopra and jason weston a neural attention model for abstractive in proceedings of the tence summarization conference on empirical methods in natural guage processing emnlp lisbon portugal september pages abigail see peter j liu and christopher d manning get to the point summarization with generator networks in proceedings of the nual meeting of the association for computational linguistics volume long papers pages vancouver canada association for tional linguistics kaiqiang song lin zhao and fei liu infused copy mechanisms for abstractive tion in coling jiwei tan xiaojun wan and jianguo xiao abstractive document summarization with a in proceedings based attentional neural model of the annual meeting of the association for computational linguistics volume long papers pages william thomas tutte graph theory vol of encyclopedia of mathematics and its applications ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan n gomez ukasz kaiser and illia polosukhin attention is all in advances in neural information you need cessing systems pages zichao yang diyi yang chris dyer xiaodong he alexander j smola and eduard h hovy erarchical attention networks for document cation in hlt naacl tianyi zhang v kishore felix wu kilian q berger and yoav artzi bertscore evaluating text generation with bert iclr xingxing zhang mirella lapata furu wei and ming zhou neural latent extractive document marization in proceedings of the conference on empirical methods in natural language ing pages brussels belgium association for computational linguistics
