survey neural network based summarization methods yue dong april r m l c s c v v x r introduction day enormous amounts text published online quick access major points documents critical decision making manually producing summaries large amounts documents timely manner longer feasible automatic text summarization automated process shortening text reserving main ideas consequently popular recently text summarization dominated unsupervised information retrieval models kageback et al demonstrated neural based continuous vector models promising text summarization marked beginning widespread use neural network based text summarization models superior performance compared traditional techniques aim literature review survey recent work neural based models automatic text summarization survey starts general background document summarization section including factors summarization tasks ed evaluation issues brief history traditional summarization techniques section examines detail neural based summarizers section discusses related techniques presents promising paths future research section concludes paper background summarization factors according jones et al text summarization tasks dened classied following factors input purpose output input factors single document vs multi document jones et al denes factor unit input parameter simply number input documents summarization system takes monolingual multilingual vs cross lingual monolingual summarizers produce summaries languages inputs multilingual systems handle input output pairs language dierent languages contrary cross lingual summarization systems operate input output pairs necessarily language purpose factors informative vs indicative indicative summary serves road map convey relevant contents original documents readers select documents align interests read indicative summary supposed substitute source documents hand purpose informative summary replace original documents far important contents concerned generic vs user oriented factor concerns coverage original documents conditioned potential readers summary generic systems create summaries consider information found documents contrast user oriented systems produce personalized summaries focus certain information source consistent user query general purpose vs domain specic general purpose summarizers little modication hand domain specic systems designed processing documents specic domain output factors extractive vs abstractive relation source summary extractive abstractive clear agreement denition literature review denition et al adopted extractive summarizer explicitly selects text snippets words phrases sentences source abstractive summarizer generates novel text snippets convey salient cepts prevalent source evaluation summarization systems evaluation critical developing summarization systems evaluation criteria assessing summarization systems remains unclear subjective aspect makes good summary general existing evaluation techniques split intrinsic extrinsic jones et al intrinsic methods directly evaluate outcome summarization system extrinsic methods evaluate summaries based performance stream tasks system summaries prevalent intrinsic evaluation compare system generated summaries system summaries human created gold summaries reference summaries allows use quantitative measures precisions recalls rouge lin problem rouge people usually disagree gold summary evaluation methods pyramid nenkova passonneau address problem assume single best gold summary exists pyramid expensive terms human involvement day single best summarization evaluation method exists researchers usually adopt cheap automated evaluation metric rouge coupled human ratings rouge lin recall oriented understudy gisting evaluation rouge set evaluation methods automatically determine quality system summary comparing created summaries rouge n rouge l rouge su commonly marization literatures rouge n computes percentage n gram overlapping system reference maries requires consecutive matches words n grams n needs dened xed best assumption rouge l computes sum longest sequence matches reference sentence system summary considers sentence level word orders automatically identify longest sequence word overlapping pre dened n rouge su measures percentage skip bigrams unigrams overlapping bigram consists words sentence arbitrary gaps sentence order plying skip bigrams constraint distance words usually produce spurious bigram matchings lin rouge su usually limited maximum skip distance rouge maximum skip distance pyramid nenkova passonneau instead matching exact phrase units lin pyramid tries score summaries based semantic matchings content units works assumption s single best summary multiple reference summaries necessary system given document n human created reference summaries rn pyramid score random summary s roughly computed follows human annotation rst required identify summarization content units scu rn s scus smallest content unit semantic meaning nenkova passonneau scu associated weight counting reference summaries cluster contain scu suppose summary s annotated k scus weights wk pyramid wi soptimal soptimal sum largest score s computed scus weights p summarization techniques scope review literature review primarily consider neural based extractive abstractive rization techniques following factors single document english informative generic general purpose far know related surveys investigate traditional models afantenos et al das martins nenkova et al little details neural based summarizers gambhir gupta brief history pre neural networks era extractive models early works single document extractive summarization employ statistical techniques based edmundsonian paradigm afantenos et al algorithms rank sentence based relation sentences pre dened formulas sum frequencies signicant words luhn overlapping rate document title correlation salient cepts topics latent semantic liu sum weighted similarities sentences textrank mihalcea tarau formulas usually nt contain hyper parameters training required later works text summarization address problem creating sentence representations documents utilizing machine learning algorithms models manually select appropriate features train supervised models classify include sentence summary example wong et al extracted surface content event relevance features sentence representation support vector machines svm nave addition sequential models hidden markov bayes models classication chains hmms conroy oleary proposed improve results considering sentence orders documents abstractive models core abstractive summarization techniques identify main ideas documents encode feature representations encoded tures passed natural language generation nlg systems proposed reiter dale summary generation early work abstractive summarization uses semi manual process identifying main ideas prior knowledge scripts templates usually produce summaries abstractive summary produced slot llings simple smoothing techniques dejong radev mckeown neural based summarization techniques bloom deep learning neural based summarizers attracted considerable tion automatic summarization compared traditional models neural based models achieve better performance human involvement training data abundant section ve extractive ve abstractive neural based models examined details neural based summarizers use following pipeline words transformed continuous vectors called word embeddings look table sentences documents encoded continuous vectors word embeddings sentence document tations word embeddings fed model selection extractive summarization generation abstractive summarization neural networks steps step use neural networks obtain pre learned look tables cw vectors glove step neural networks convolutional neural networks cnns recurrent neural encoders extracting sentence document features step neural network models regressors ranking selection extraction decoders generation abstraction cnns rnns cnns rnns commonly neural based summarizers cnns rnns serve purpose transform sequence word embeddings xt rd vector sentence representation s rh cnns achieve purpose h lters sliding input sequence lter performs local sub sequences input obtain set feature maps scalars global max pooling time performed tain scalar scalars h lters concatenated sequence representation vector s rh convolution operation basically element wise matrix multiplication followed tion rnns achieve purpose introducing time dependent neural networks time step t rnn computes hidden state vector ht obtained non linear transformation inputs previous hidden state current word input xt basic rnn called elman rnn ht xt ht popular rnns address problem long term dependencies adding extra parameters follows gated recurrent unit gru long short term memory lstm zt tanh xt zt zt ft ot c t tanh xt ct ft t ht ot denotes element wise matrix multiplication wi matrices responding dimensions hidden state ht usually sequence sentation s rh extractive models extractive summarizers selection based methods need solve following ical challenges represent sentences select appropriate sentences taking account coverage redundancy section review ve extractive neural based summarizers chronological order summarization system presented based sentence representation model sentence selection model end section techniques extractive neural based models summarized models performance compared continuous vector space models kageback et al sentence representation kageback et al proposes represent sentences tinuous vectors obtained adding word embeddings unfolding recursive auto encoder rae word embeddings rae basically combines text units recursive manner vector sentence representation left rae trained unsupervised manner backpropagation method reconstruction errors pre computed word embeddings collobert weston s model cw vectors mikolov et al s model vectors directly ne tuning sentence selection kageback et al formulates task choosing summary s optimization problem maximizes linear combination diversity sentences r coverage input text l tread o converge diversity according kageback et al optimization problem np hard exists fast scalable approximation algorithms theoretical guarantees objective tion submodular authors choose submodular functions computed based sentence similarities diversity function converge function respectively objective function submodular approximation optimization algorithm described kageback et al selecting sentences cnnlm yin pei sentence representation yin pei uses convolutional neural networks cnns similar basic cnn model described previously pre trained word embeddings obtain sentence representation learnable parameters including word embeddings cnn trained unsupervised learning noise contrastive estimation nce mnih teh cost function cost function model basically trained language learns discriminate true words noise words sentence selection similar kageback et al authors frame sentence selection direct optimization problem following objective function pimi jpj x js x matrix m obtained calculating pairwise cosine similarities learned sentence representations prestige vector p derived pagerank algorithm m goal nd summary s set sentences maximizes objective function fortunately equation submodular proof yin pei fore stated kageback et al near optimal solution exists presented yin pei priorsum cao et al sentence representation priorsum uses cnn learned features concatenated ument independent features sentence representation document independent tures sentence position averaged term frequency words sentence based document averaged term frequency words sentence based cluster multi document summarization similar yin pei cnns multiple lters capture sentence features priorsum employs deeper complicated cnn cnn priorsum multiple layers alternating convolution pooling operations lters convolution layers dierent window sizes stage max time pooling operations performed pooling layers parameters cnn updated applying diagonal variant adagrad mini batches described yin pei sentence selection unlike previous extractive neural based models priorsum supervised model requires gold standard summaries training priorsum follows traditional supervised extractive framework rst ranks sentence selects k ranked non redundant sentences nal summary function f called submodular set s s s b implies s condition called diminishing return property authors frame sentence ranking process regression problem training sentence document associated score stopwords removed respect gold standard summary linear regression model trained estimate scores updating regression weights testing non redundant sentences selected simple greedy algorithm greedy selection algorithm rst ranks sentences words descending order based estimated informative scores k sentences selected order long sentence redundant respect current summary sentence considered non redundant respect summary words appear summary nn se cheng lapata sentence representation cheng lapata sentence representations tained cnn followed rnn cnn extractor similar multiple feature maps dierent window sizes sentence representations st obtained cnn sentence extractor fed lstm encoder lstm s hidden states ht nal sentence sentations comparing st authors believe ht capture sentence dependency information better suited sentence representations sentence selection similar cao et al s work nn se supervised model rst scores sentences selects based estimated scores instead simple linear regressor cao et al nn se utilizes lstm decoder sigmoid layer equation scoring sentences training ground truth labels given sentences included reference summary decoder trained label sentences sequentially zeros ones given vectors st obtained cnn lstm encoder s hidden states ht decoder s hidden states ht computed lst probability decoder believes previous sentence included summary binary decision include sentence t modeled following sigmoid layer ht mlp multi layer neural network joint training large scale dataset nn se sequence sequence model encoder decoder encoder sentence representation model decoder sentence selection model jointly trained stochastic gradient descent sgd method objective minimizing negative log likelihood nll training sequence sequence summarizer requires large scale dataset extractive bels e documents sentences labeled summary worthy authors created large scale dataset dailymail dataset k training examples data instance contains extractive reference summary obtained labeling sentences based set rules sentence positions grams overlapping m x summarunner nallapati et al sentence representation summarunner employs layer bi directional rnn tences document representations rst layer rnn bi directional gru runs words level takes word embeddings sentence inputs produces set hidden states hidden states averaged vector sentence representation second layer rnn bi directional gru runs sentence level taking sentence representations obtained rst layer inputs hidden states second layer combined vector document representation non linear transformation sentence selection authors frame task sentence selection sequentially tence labeling problem similar settings cheng lapata dierent cheng lapata instead rnn decoder summarunner uses hidden states hm second layer encoder rnn directly binary decision modeled sigmoid function p yt st ht t includes information sentence s absolute relative position bias sj viewed soft summary representation computed running weighted sum sentence representations time t st hip yi sigmoid decision layer layer encoder rnn jointly trained sgd objective function similar cheng lapata p comparison extractive models performance table compares summarizes ve extractive models mentioned previously models evaluated dataset compare performance dataset table models continuous tor space models sentence selection sentence tation adding word beddings rae table comparison techniques extractive summarizers training sentence representation training adding rae trained supervise res unsupervised ing nce direct optimization submodular jectives cnn training training sentence selection training cnnlm priorsum cnn nn se unsupervised ing diagonal variant adagrad supervised co train decoder supervised learning scores supervised learning sgd nll summarunner rae recursive auto encoder res reconstruction errors nce noise contrastive estimation supervised co train decoder ranking supervised learning sgd nll direct optimization submodular jectives ranking sentence linear regression sentence ing sentence sigmoid table rouge scores extractive summarizers dataset models rougel rougesu continuous vector space models cnnlm priorsum nn se summarunner extra data training addition pre trained cw word embeddings pre trained word dings pre trained cw word dings gigaword cnn dailymail pre trained glove word beddings dailymail abstractive models abstractive summarizers focus capturing meaning representation document generate abstractive summary based meaning representation neural based abstractive summarizers generation based methods need following decisions represent document encoder generate words sequence decoder section review ve abstractive neural based summarizers chronological order summarization system presented based encoder decoder end section techniques abstractive neural based models summarized models performance compared abs rush et al encoder rush et al proposes encoder structures capture meaning resentation document common goal encoders transform sequence word embeddings wt vector d meaning representation document bag words encoder rst encoder basically computes summation word t xi word order preserved embeddings appeared sequence t bag words encoder p convolutional encoder encoder utilizes cnn model multiple alternating convolution element max pooling layers layer convolution operations extract sequence feature vectors ul number feature vectors reduced factor element max pooling l layers convolution max pooling max pooling time performed obtain document representation ul attention based encoder encoder produces document representation time step based previous c words context generated decoder time step t given inputs word embeddings x xm decoder s context c encoder produces document representation time step t follows pt x rm dt c decoder estimating probability distribution generates word time step t rush et al uses feed forward neural network based language model nnlm c dt ht c training rush et al encoder decoder trained jointly mini batches suppose j input summary pairs loss negative likelihood loss nll based parameters computed t j x j t x x training objective minimize nll achieved mini batch stochastic gradient descent ras lstm ras elman chopra et al encoder cnn based attentive encoder chopra et al similar attentive encoder proposed rush et al weights computed based aggregated vectors obtained cnn model time step t attention weights calculated aggregated vectors zt decoder s hidden state ht j t t ht attention weights combined inputs word embeddings form document representation dt dt t j p p decoder chopra et al replaces nnlm model rush et al recurrent neural network instead previously generated c words decoding nnlm rnn decoder s hidden state ht information words generated till time t authors propose decoder models based elman rnn addition previous generated word previous hidden state elman rnn lstm encoder s context vector dt document representation t additional input example elman rnn s hidden state computed ht decoder s hidden state ht computed combined document sentation dt decide word generate time step t decision modeled softmax function gives probability distribution words dictionary pt sof hierarchical attentive rnns nallapati et al encoder bidirectional gru represent document nallapati et al proposes feature rich hierarchical attentive encoder based feature rich inputs encoder takes input vector obtained concatenating word embedding additional linguistic features additional linguistic features model parts speech pos tags named entity ner tags term frequency tf inverse document frequency idf word continuous features tf idf rst discretized xed number bins encoded hot vectors discrete features hot vectors transformed continuous vectors embedding matrices continuous vectors concatenated single long vector fed encoder document representation time step t called encoder s context time step t commonly denoted ct literature elman rnn lstm work produce hidden states explained early section cnns rnns words generated dictionary hierarchical attention hierarchical encoder rnns similar structure nallapati et al runs word level runs sentence level hierarchical attention proposed authors basically weigh word attentions corresponding sentence level attention document representation dt obtained weighted sum feature rich input vectors decoder nallapati et al uses rnn decoder based uni directional gru works similar decoder chopra et al addition following mechanisms nallapati et al decoder large vocabulary trick lvt trick reduces computation time softmax layer limiting number words decoder generate training basically denes small dictionary mini batch training dictionary contains words source documents batch frequent k words global dictionary decoder pointer switch pointer network directly copy words source improve summaries quality including rare words source documents pointer network simply modeled based encoder s attention weights word largest weight word copying decision copy generate controlled switch modeled sigmoid function p ht dt pointer generator networks et al encoder encoder pointer generator network simply single layer bidirectional lstm computes document representation based attention weights encoder s hidden states exactly encoder chopra et al decoder basic building block et al decoder single layer uni directional lstm addition decoder pointer switch similar nallapati et al ing authors propose coverage mechanism penalizing repeated attentions attended words achieved coverage vector ct tracks attentions words dictionary received till time t ct coverage vector attention computation time step t objective function acted regularizer p lt t ct x t true label time step t hyperparameter controlling degree w coverage regularizer neural intra attention model paulus et al encoder et al uses bi directional lstm encoder modeling ument representation model similar encoder et al attention scores computed linear transformations softmax attention scores models reviewed computed sigmoid functions followed softmax function called intra attention mechanism authors dt computed based intra attentions encoder s hidden states decoder uni directional lstm decoder paulus et al addition authors employ intra attention mechanism decoder prevent generating repeated phrases decoder context vector ct computed based intra attentions generated sequence additional input softmax layer generating generator pointer switch similar ones nallapati et al et al employed decoder hybrid training objectives terms encoder decoder model et al et al similar novel paulus et al parameters model updated use stochastic gradient descent method reinforcement learning method update model parameters hybrid training objectives stochastic gradient descent method sgd abstractive summarization models minimize negative log likelihood ground truth values training plained previous models rush et al chopra et al nallapati et al et al denote nll objective lml sgd minimize lml shortcomings creates discrepancy training testing ground truth values testing optimizing objective correlate high score discrete evaluation metric rouge scores authors propose use objective based reinforcement learning method reinforce training lrl ys t x t ys obtained sampling decoding time step t y acts reinforce baseline obtained performing greedy selection sampling decoding time step reward score output sequence y usually obtained automated evaluation method rouge ys authors noticed optimizing lrl directly lead sequences high rouge scores ungrammatical mixed training objective hyperparameter balancing rouge score readability generated sequence lmixed lrl comparison abstractive models performance table compares summarizes ve abstractive models large scale datasets gigaword dataset cnn dailymail dataset commonly abstractive summarization benchmarks compare ve abstractive models performance datasets table table comparison techniques abstractive summarizers models abs ras lstm elman hierarchical rnns attentive pointer generator works neural model intra attention encoder bag words encoder cnn attention based encoder cnn attention decoder nnlm elman rnn lstm sgd training sgd feature rich bidirectional gru hierarchical attention bidirectional lstm attention bidirectional lstm intra attention gru lvt pointer switch sgd lstm pointer switch coverage mechanism lstm pointer switch intra attention sgd sgd reinforce table rouge scores abstractive summarizers datasets models rougesu abs ras lstm ras elman hierarchical attentive rnns pointer generator networks neural intra attention model g c g c rougel g c g c discussions promising paths future search related tasks techniques reinforcement learning methods sequence prediction bahdanau et al et al shows promising path applying reinforcement learning rl method abstractive summarization paulus et al applies reinforce unbiased estimator large variance sequence prediction summarization bahdanau et al authors apply actor critic algorithm biased estimator smaller variable machine translation addition policy network encoder decoder model introduce critic network trained predict values output tokens critic network based bidirectional gru trained supervisely ground truth labels key dierence reinforce algorithm actor critic algorithm rewards actor uses update parameters reinforce uses overall reward sequence performs update obtaining trajectory actor critic algorithm uses td errors bahdanau et al calculated based critic network update actor generating process compared force algorithm actor critic method lower variance faster convergence rate makes promising algorithm summarization text simplication xu et al zhang lapata goal text simplication rewrite complex documents simpler ones easier understand usually achieved operations splitting deletion paraphrasing xu et al text simplication help improve performance natural language processing nlp tasks example text simplication techniques transform long complex sentences ones easily processed automatic text summarizers challenge developing text simplication models lack datasets parallel complex simple sentence pairs xu et al created good quality simplication dataset called newsela dataset tasks text simplication analyses words distribution signicantly dierent complex simple texts addition distribution syntax patterns dierent ndings indicate text simplication model need consider semantic meaning words syntactic patterns sentences zhang lapata propose sequence sequence model attentions based lstms text simplication encoder decoder model called deep reinforcement tence simplication dress trained reinforcement learning method optimizes task specic discrete reward function discrete reward function encourages outputs simple grammatical semantically related inputs experiments datasets demonstrate model promising text simplication tasks discussions summarization critical issue represent semantic meanings sentences documents neural based models display superior performance automatically extracting feature representations deep neural network models transparent integrating prior knowledge analysis understanding neural based models needed exploiting models addition current neural based models following limitations unable deal sequences longer thousand words large memory requirement models unable work small scale datasets large parameters models slow train complexity models interesting promising directions future research text marization proposed directions review reinforcement learning approaches actor critic algorithm train neural based models exploiting techniques text simplication transform documents simpler ones summarizers process conclusion survey presented potential neural based techniques automatic text tion based examination state art extractive abstractive summarizers neural based models promising text summarization terms performance large scale datasets available training challenges neural based models remain unsolved future research directions adding reinforcement learning algorithms text simplication methods current neural based models provided researchers references afantenos et al afantenos s karkaletsis v stamatopoulos p summarization medical documents survey articial intelligence medicine bahdanau et al bahdanau d brakel p xu k goyal lowe r pineau j courville bengio y actor critic algorithm sequence prediction cao et al cao z wei f li s li w zhou m wang h learning summary prior representation extractive summarization acl cheng lapata cheng j lapata m neural summarization extracting sentences words proceedings annual meeting association computational linguistics volume long papers pages berlin germany association computational linguistics chopra et al chopra s auli m rush m abstractive sentence summarization attentive recurrent neural networks naacl hlt conference north american chapter association computational linguistics human language technologies san diego fornia usa june pages conroy oleary conroy j m oleary d p text summarization hidden markov models proceedings annual international acm sigir conference research development information retrieval pages acm das martins das d martins f survey automatic text summarization dejong dejong g f overview frump system lehnert w g ringle m h editors strategies natural language processing pages lawrence erlbaum gambhir gupta gambhir m gupta v recent automatic text summarization niques survey articial intelligence review gong liu gong y liu x generic text summarization relevance measure latent semantic analysis proceedings annual international acm sigir conference research development information retrieval pages acm jones et al jones k s al automatic summarizing factors directions advances automatic text summarization pages kageback et al kageback m mogren o tahmasebi n dubhashi d extractive marization continuous vector space models proceedings workshop continuous vector space models compositionality eacl pages lin lin c rouge package automatic evaluation summaries francine moens s s editor text summarization branches proceedings workshop pages barcelona spain association computational linguistics luhn luhn h p automatic creation literature abstracts ibm journal research development mihalcea tarau mihalcea r tarau p textrank bringing order text ceedings conference empirical methods natural language processing nallapati et al nallapati r zhai f zhou b summarunner recurrent neural network based sequence model extractive summarization documents proceedings thirty aaai conference articial intelligence february san francisco california usa pages nallapati et al nallapati r zhou b dos santos c n gulcehre c xiang b tive text summarization sequence sequence rnns proceedings signll conference computational natural language learning conll berlin germany august pages nenkova et al nenkova mckeown k al automatic summarization foundations trends information retrieval nenkova passonneau nenkova passonneau r j evaluating content selection summarization pyramid method human language technology conference north american chapter association computational linguistics hlt naacl boston massachusetts usa pages paulus et al paulus r xiong c socher r deep reinforced model abstractive summarization arxiv preprint radev mckeown radev d r mckeown k r generating natural language maries multiple line sources computational linguistics reiter dale reiter e dale r building applied natural language generation systems nat lang eng rush et al rush m chopra s weston j neural attention model abstractive sentence summarization proceedings conference empirical methods natural language processing emnlp lisbon portugal september pages et al liu p j manning c d point summarization pointer generator networks proceedings annual meeting association computational linguistics acl vancouver canada july august volume long papers pages wong et al wong k wu m li w extractive summarization supervised semi supervised learning proceedings international conference computational volume pages association computational linguistics xu et al xu w callison burch c napoles c problems current text simplication research new data help tacl xu xu x pyteaser yin pei yin w pei y optimizing sentence modeling selection document summarization proceedings fourth international joint conference articial intelligence ijcai buenos aires argentina july pages zhang lapata zhang x lapata m sentence simplication deep reinforcement proceedings conference empirical methods natural language processing learning emnlp copenhagen denmark september pages
