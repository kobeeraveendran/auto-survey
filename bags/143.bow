survey neural network based summarization methods yue dong april introduction day enormous amounts text published online quick access major points documents critical decision making manually producing summaries large amounts documents timely manner longer feasible automatic text summarization automated process shortening text reserving main ideas consequently popular recently text summarization dominated unsupervised information retrieval models kageback demonstrated neural based continuous vector models promising text summarization marked beginning widespread use neural network based text summarization models superior performance compared traditional techniques aim literature review survey recent work neural based models automatic text summarization survey starts general background document summarization section including factors summarization tasks evaluation issues brief history traditional summarization techniques section examines detail neural based summarizers section discusses related techniques presents promising paths future research section concludes paper background summarization factors according jones text summarization tasks dened classied following factors input purpose output input factors single document multi document jones denes factor unit input parameter simply number input documents summarization system takes monolingual multilingual cross lingual monolingual summarizers produce summaries languages inputs multilingual systems handle input output pairs language dierent languages contrary cross lingual summarization systems operate input output pairs necessarily language purpose factors informative indicative indicative summary serves road map convey relevant contents original documents readers select documents align interests read indicative summary supposed substitute source documents hand purpose informative summary replace original documents far important contents concerned generic user oriented factor concerns coverage original documents conditioned potential readers summary generic systems create summaries consider information found documents contrast user oriented systems produce personalized summaries focus certain information source consistent user query general purpose domain specic general purpose summarizers little modication hand domain specic systems designed processing documents specic domain output factors extractive abstractive relation source summary extractive abstractive clear agreement denition literature review denition adopted extractive summarizer explicitly selects text snippets words phrases sentences source abstractive summarizer generates novel text snippets convey salient cepts prevalent source evaluation summarization systems evaluation critical developing summarization systems evaluation criteria assessing summarization systems remains unclear subjective aspect makes good summary general existing evaluation techniques split intrinsic extrinsic jones intrinsic methods directly evaluate outcome summarization system extrinsic methods evaluate summaries based performance stream tasks system summaries prevalent intrinsic evaluation compare system generated summaries system summaries human created gold summaries reference summaries allows use quantitative measures precisions recalls rouge lin problem rouge people usually disagree gold summary evaluation methods pyramid nenkova passonneau address problem assume single best gold summary exists pyramid expensive terms human involvement day single best summarization evaluation method exists researchers usually adopt cheap automated evaluation metric rouge coupled human ratings rouge lin recall oriented understudy gisting evaluation rouge set evaluation methods automatically determine quality system summary comparing created summaries rouge rouge rouge commonly marization literatures rouge computes percentage gram overlapping system reference maries requires consecutive matches words grams needs dened xed best assumption rouge computes sum longest sequence matches reference sentence system summary considers sentence level word orders automatically identify longest sequence word overlapping pre dened rouge measures percentage skip bigrams unigrams overlapping bigram consists words sentence arbitrary gaps sentence order plying skip bigrams constraint distance words usually produce spurious bigram matchings lin rouge usually limited maximum skip distance rouge maximum skip distance pyramid nenkova passonneau instead matching exact phrase units lin pyramid tries score summaries based semantic matchings content units works assumption single best summary multiple reference summaries necessary system given document human created reference summaries pyramid score random summary roughly computed follows human annotation rst required identify summarization content units scu scus smallest content unit semantic meaning nenkova passonneau scu associated weight counting reference summaries cluster contain scu suppose summary annotated scus weights pyramid soptimal soptimal sum largest score computed scus weights summarization techniques scope review literature review primarily consider neural based extractive abstractive rization techniques following factors single document english informative generic general purpose far know related surveys investigate traditional models afantenos das martins nenkova little details neural based summarizers gambhir gupta brief history pre neural networks era extractive models early works single document extractive summarization employ statistical techniques based edmundsonian paradigm afantenos algorithms rank sentence based relation sentences pre dened formulas sum frequencies signicant words luhn overlapping rate document title correlation salient cepts topics latent semantic liu sum weighted similarities sentences textrank mihalcea tarau formulas usually contain hyper parameters training required later works text summarization address problem creating sentence representations documents utilizing machine learning algorithms models manually select appropriate features train supervised models classify include sentence summary example wong extracted surface content event relevance features sentence representation support vector machines svm nave addition sequential models hidden markov bayes models classication chains hmms conroy oleary proposed improve results considering sentence orders documents abstractive models core abstractive summarization techniques identify main ideas documents encode feature representations encoded tures passed natural language generation nlg systems proposed reiter dale summary generation early work abstractive summarization uses semi manual process identifying main ideas prior knowledge scripts templates usually produce summaries abstractive summary produced slot llings simple smoothing techniques dejong radev mckeown neural based summarization techniques bloom deep learning neural based summarizers attracted considerable tion automatic summarization compared traditional models neural based models achieve better performance human involvement training data abundant section extractive abstractive neural based models examined details neural based summarizers use following pipeline words transformed continuous vectors called word embeddings look table sentences documents encoded continuous vectors word embeddings sentence document tations word embeddings fed model selection extractive summarization generation abstractive summarization neural networks steps step use neural networks obtain pre learned look tables vectors glove step neural networks convolutional neural networks cnns recurrent neural encoders extracting sentence document features step neural network models regressors ranking selection extraction decoders generation abstraction cnns rnns cnns rnns commonly neural based summarizers cnns rnns serve purpose transform sequence word embeddings vector sentence representation cnns achieve purpose lters sliding input sequence lter performs local sub sequences input obtain set feature maps scalars global max pooling time performed tain scalar scalars lters concatenated sequence representation vector convolution operation basically element wise matrix multiplication followed tion rnns achieve purpose introducing time dependent neural networks time step rnn computes hidden state vector obtained non linear transformation inputs previous hidden state current word input basic rnn called elman rnn popular rnns address problem long term dependencies adding extra parameters follows gated recurrent unit gru long short term memory lstm tanh tanh denotes element wise matrix multiplication matrices responding dimensions hidden state usually sequence sentation extractive models extractive summarizers selection based methods need solve following ical challenges represent sentences select appropriate sentences taking account coverage redundancy section review extractive neural based summarizers chronological order summarization system presented based sentence representation model sentence selection model end section techniques extractive neural based models summarized models performance compared continuous vector space models kageback sentence representation kageback proposes represent sentences tinuous vectors obtained adding word embeddings unfolding recursive auto encoder rae word embeddings rae basically combines text units recursive manner vector sentence representation left rae trained unsupervised manner backpropagation method reconstruction errors pre computed word embeddings collobert weston model vectors mikolov model vectors directly tuning sentence selection kageback formulates task choosing summary optimization problem maximizes linear combination diversity sentences coverage input text tread converge diversity according kageback optimization problem hard exists fast scalable approximation algorithms theoretical guarantees objective tion submodular authors choose submodular functions computed based sentence similarities diversity function converge function respectively objective function submodular approximation optimization algorithm described kageback selecting sentences cnnlm yin pei sentence representation yin pei uses convolutional neural networks cnns similar basic cnn model described previously pre trained word embeddings obtain sentence representation learnable parameters including word embeddings cnn trained unsupervised learning noise contrastive estimation nce mnih teh cost function cost function model basically trained language learns discriminate true words noise words sentence selection similar kageback authors frame sentence selection direct optimization problem following objective function pimi jpj matrix obtained calculating pairwise cosine similarities learned sentence representations prestige vector derived pagerank algorithm goal summary set sentences maximizes objective function fortunately equation submodular proof yin pei fore stated kageback near optimal solution exists presented yin pei priorsum cao sentence representation priorsum uses cnn learned features concatenated ument independent features sentence representation document independent tures sentence position averaged term frequency words sentence based document averaged term frequency words sentence based cluster multi document summarization similar yin pei cnns multiple lters capture sentence features priorsum employs deeper complicated cnn cnn priorsum multiple layers alternating convolution pooling operations lters convolution layers dierent window sizes stage max time pooling operations performed pooling layers parameters cnn updated applying diagonal variant adagrad mini batches described yin pei sentence selection unlike previous extractive neural based models priorsum supervised model requires gold standard summaries training priorsum follows traditional supervised extractive framework rst ranks sentence selects ranked non redundant sentences nal summary function called submodular set implies condition called diminishing return property authors frame sentence ranking process regression problem training sentence document associated score stopwords removed respect gold standard summary linear regression model trained estimate scores updating regression weights testing non redundant sentences selected simple greedy algorithm greedy selection algorithm rst ranks sentences words descending order based estimated informative scores sentences selected order long sentence redundant respect current summary sentence considered non redundant respect summary words appear summary cheng lapata sentence representation cheng lapata sentence representations tained cnn followed rnn cnn extractor similar multiple feature maps dierent window sizes sentence representations obtained cnn sentence extractor fed lstm encoder lstm hidden states nal sentence sentations comparing authors believe capture sentence dependency information better suited sentence representations sentence selection similar cao work supervised model rst scores sentences selects based estimated scores instead simple linear regressor cao utilizes lstm decoder sigmoid layer equation scoring sentences training ground truth labels given sentences included reference summary decoder trained label sentences sequentially zeros ones given vectors obtained cnn lstm encoder hidden states decoder hidden states computed lst probability decoder believes previous sentence included summary binary decision include sentence modeled following sigmoid layer mlp multi layer neural network joint training large scale dataset sequence sequence model encoder decoder encoder sentence representation model decoder sentence selection model jointly trained stochastic gradient descent sgd method objective minimizing negative log likelihood nll training sequence sequence summarizer requires large scale dataset extractive bels documents sentences labeled summary worthy authors created large scale dataset dailymail dataset training examples data instance contains extractive reference summary obtained labeling sentences based set rules sentence positions grams overlapping summarunner nallapati sentence representation summarunner employs layer directional rnn tences document representations rst layer rnn directional gru runs words level takes word embeddings sentence inputs produces set hidden states hidden states averaged vector sentence representation second layer rnn directional gru runs sentence level taking sentence representations obtained rst layer inputs hidden states second layer combined vector document representation non linear transformation sentence selection authors frame task sentence selection sequentially tence labeling problem similar settings cheng lapata dierent cheng lapata instead rnn decoder summarunner uses hidden states second layer encoder rnn directly binary decision modeled sigmoid function includes information sentence absolute relative position bias viewed soft summary representation computed running weighted sum sentence representations time hip sigmoid decision layer layer encoder rnn jointly trained sgd objective function similar cheng lapata comparison extractive models performance table compares summarizes extractive models mentioned previously models evaluated dataset compare performance dataset table models continuous tor space models sentence selection sentence tation adding word beddings rae table comparison techniques extractive summarizers training sentence representation training adding rae trained supervise res unsupervised ing nce direct optimization submodular jectives cnn training training sentence selection training cnnlm priorsum cnn unsupervised ing diagonal variant adagrad supervised train decoder supervised learning scores supervised learning sgd nll summarunner rae recursive auto encoder res reconstruction errors nce noise contrastive estimation supervised train decoder ranking supervised learning sgd nll direct optimization submodular jectives ranking sentence linear regression sentence ing sentence sigmoid table rouge scores extractive summarizers dataset models rougel rougesu continuous vector space models cnnlm priorsum summarunner extra data training addition pre trained word embeddings pre trained word dings pre trained word dings gigaword cnn dailymail pre trained glove word beddings dailymail abstractive models abstractive summarizers focus capturing meaning representation document generate abstractive summary based meaning representation neural based abstractive summarizers generation based methods need following decisions represent document encoder generate words sequence decoder section review abstractive neural based summarizers chronological order summarization system presented based encoder decoder end section techniques abstractive neural based models summarized models performance compared abs rush encoder rush proposes encoder structures capture meaning resentation document common goal encoders transform sequence word embeddings vector meaning representation document bag words encoder rst encoder basically computes summation word word order preserved embeddings appeared sequence bag words encoder convolutional encoder encoder utilizes cnn model multiple alternating convolution element max pooling layers layer convolution operations extract sequence feature vectors number feature vectors reduced factor element max pooling layers convolution max pooling max pooling time performed obtain document representation attention based encoder encoder produces document representation time step based previous words context generated decoder time step given inputs word embeddings decoder context encoder produces document representation time step follows decoder estimating probability distribution generates word time step rush uses feed forward neural network based language model nnlm training rush encoder decoder trained jointly mini batches suppose input summary pairs loss negative likelihood loss nll based parameters computed training objective minimize nll achieved mini batch stochastic gradient descent ras lstm ras elman chopra encoder cnn based attentive encoder chopra similar attentive encoder proposed rush weights computed based aggregated vectors obtained cnn model time step attention weights calculated aggregated vectors decoder hidden state attention weights combined inputs word embeddings form document representation decoder chopra replaces nnlm model rush recurrent neural network instead previously generated words decoding nnlm rnn decoder hidden state information words generated till time authors propose decoder models based elman rnn addition previous generated word previous hidden state elman rnn lstm encoder context vector document representation additional input example elman rnn hidden state computed decoder hidden state computed combined document sentation decide word generate time step decision modeled softmax function gives probability distribution words dictionary sof hierarchical attentive rnns nallapati encoder bidirectional gru represent document nallapati proposes feature rich hierarchical attentive encoder based feature rich inputs encoder takes input vector obtained concatenating word embedding additional linguistic features additional linguistic features model parts speech pos tags named entity ner tags term frequency inverse document frequency idf word continuous features idf rst discretized xed number bins encoded hot vectors discrete features hot vectors transformed continuous vectors embedding matrices continuous vectors concatenated single long vector fed encoder document representation time step called encoder context time step commonly denoted literature elman rnn lstm work produce hidden states explained early section cnns rnns words generated dictionary hierarchical attention hierarchical encoder rnns similar structure nallapati runs word level runs sentence level hierarchical attention proposed authors basically weigh word attentions corresponding sentence level attention document representation obtained weighted sum feature rich input vectors decoder nallapati uses rnn decoder based uni directional gru works similar decoder chopra addition following mechanisms nallapati decoder large vocabulary trick lvt trick reduces computation time softmax layer limiting number words decoder generate training basically denes small dictionary mini batch training dictionary contains words source documents batch frequent words global dictionary decoder pointer switch pointer network directly copy words source improve summaries quality including rare words source documents pointer network simply modeled based encoder attention weights word largest weight word copying decision copy generate controlled switch modeled sigmoid function pointer generator networks encoder encoder pointer generator network simply single layer bidirectional lstm computes document representation based attention weights encoder hidden states exactly encoder chopra decoder basic building block decoder single layer uni directional lstm addition decoder pointer switch similar nallapati ing authors propose coverage mechanism penalizing repeated attentions attended words achieved coverage vector tracks attentions words dictionary received till time coverage vector attention computation time step objective function acted regularizer true label time step hyperparameter controlling degree coverage regularizer neural intra attention model paulus encoder uses directional lstm encoder modeling ument representation model similar encoder attention scores computed linear transformations softmax attention scores models reviewed computed sigmoid functions followed softmax function called intra attention mechanism authors computed based intra attentions encoder hidden states decoder uni directional lstm decoder paulus addition authors employ intra attention mechanism decoder prevent generating repeated phrases decoder context vector computed based intra attentions generated sequence additional input softmax layer generating generator pointer switch similar ones nallapati employed decoder hybrid training objectives terms encoder decoder model similar novel paulus parameters model updated use stochastic gradient descent method reinforcement learning method update model parameters hybrid training objectives stochastic gradient descent method sgd abstractive summarization models minimize negative log likelihood ground truth values training plained previous models rush chopra nallapati denote nll objective lml sgd minimize lml shortcomings creates discrepancy training testing ground truth values testing optimizing objective correlate high score discrete evaluation metric rouge scores authors propose use objective based reinforcement learning method reinforce training lrl obtained sampling decoding time step acts reinforce baseline obtained performing greedy selection sampling decoding time step reward score output sequence usually obtained automated evaluation method rouge authors noticed optimizing lrl directly lead sequences high rouge scores ungrammatical mixed training objective hyperparameter balancing rouge score readability generated sequence lmixed lrl comparison abstractive models performance table compares summarizes abstractive models large scale datasets gigaword dataset cnn dailymail dataset commonly abstractive summarization benchmarks compare abstractive models performance datasets table table comparison techniques abstractive summarizers models abs ras lstm elman hierarchical rnns attentive pointer generator works neural model intra attention encoder bag words encoder cnn attention based encoder cnn attention decoder nnlm elman rnn lstm sgd training sgd feature rich bidirectional gru hierarchical attention bidirectional lstm attention bidirectional lstm intra attention gru lvt pointer switch sgd lstm pointer switch coverage mechanism lstm pointer switch intra attention sgd sgd reinforce table rouge scores abstractive summarizers datasets models rougesu abs ras lstm ras elman hierarchical attentive rnns pointer generator networks neural intra attention model rougel discussions promising paths future search related tasks techniques reinforcement learning methods sequence prediction bahdanau shows promising path applying reinforcement learning method abstractive summarization paulus applies reinforce unbiased estimator large variance sequence prediction summarization bahdanau authors apply actor critic algorithm biased estimator smaller variable machine translation addition policy network encoder decoder model introduce critic network trained predict values output tokens critic network based bidirectional gru trained supervisely ground truth labels key dierence reinforce algorithm actor critic algorithm rewards actor uses update parameters reinforce uses overall reward sequence performs update obtaining trajectory actor critic algorithm uses errors bahdanau calculated based critic network update actor generating process compared force algorithm actor critic method lower variance faster convergence rate makes promising algorithm summarization text simplication zhang lapata goal text simplication rewrite complex documents simpler ones easier understand usually achieved operations splitting deletion paraphrasing text simplication help improve performance natural language processing nlp tasks example text simplication techniques transform long complex sentences ones easily processed automatic text summarizers challenge developing text simplication models lack datasets parallel complex simple sentence pairs created good quality simplication dataset called newsela dataset tasks text simplication analyses words distribution signicantly dierent complex simple texts addition distribution syntax patterns dierent ndings indicate text simplication model need consider semantic meaning words syntactic patterns sentences zhang lapata propose sequence sequence model attentions based lstms text simplication encoder decoder model called deep reinforcement tence simplication dress trained reinforcement learning method optimizes task specic discrete reward function discrete reward function encourages outputs simple grammatical semantically related inputs experiments datasets demonstrate model promising text simplication tasks discussions summarization critical issue represent semantic meanings sentences documents neural based models display superior performance automatically extracting feature representations deep neural network models transparent integrating prior knowledge analysis understanding neural based models needed exploiting models addition current neural based models following limitations unable deal sequences longer thousand words large memory requirement models unable work small scale datasets large parameters models slow train complexity models interesting promising directions future research text marization proposed directions review reinforcement learning approaches actor critic algorithm train neural based models exploiting techniques text simplication transform documents simpler ones summarizers process conclusion survey presented potential neural based techniques automatic text tion based examination state art extractive abstractive summarizers neural based models promising text summarization terms performance large scale datasets available training challenges neural based models remain unsolved future research directions adding reinforcement learning algorithms text simplication methods current neural based models provided researchers references afantenos afantenos karkaletsis stamatopoulos summarization medical documents survey articial intelligence medicine bahdanau bahdanau brakel goyal lowe pineau courville bengio actor critic algorithm sequence prediction cao cao wei zhou wang learning summary prior representation extractive summarization acl cheng lapata cheng lapata neural summarization extracting sentences words proceedings annual meeting association computational linguistics volume long papers pages berlin germany association computational linguistics chopra chopra auli rush abstractive sentence summarization attentive recurrent neural networks naacl hlt conference north american chapter association computational linguistics human language technologies san diego fornia usa june pages conroy oleary conroy oleary text summarization hidden markov models proceedings annual international acm sigir conference research development information retrieval pages acm das martins das martins survey automatic text summarization dejong dejong overview frump system lehnert ringle editors strategies natural language processing pages lawrence erlbaum gambhir gupta gambhir gupta recent automatic text summarization niques survey articial intelligence review gong liu gong liu generic text summarization relevance measure latent semantic analysis proceedings annual international acm sigir conference research development information retrieval pages acm jones jones automatic summarizing factors directions advances automatic text summarization pages kageback kageback mogren tahmasebi dubhashi extractive marization continuous vector space models proceedings workshop continuous vector space models compositionality eacl pages lin lin rouge package automatic evaluation summaries francine moens editor text summarization branches proceedings workshop pages barcelona spain association computational linguistics luhn luhn automatic creation literature abstracts ibm journal research development mihalcea tarau mihalcea tarau textrank bringing order text ceedings conference empirical methods natural language processing nallapati nallapati zhai zhou summarunner recurrent neural network based sequence model extractive summarization documents proceedings thirty aaai conference articial intelligence february san francisco california usa pages nallapati nallapati zhou dos santos gulcehre xiang tive text summarization sequence sequence rnns proceedings signll conference computational natural language learning conll berlin germany august pages nenkova nenkova mckeown automatic summarization foundations trends information retrieval nenkova passonneau nenkova passonneau evaluating content selection summarization pyramid method human language technology conference north american chapter association computational linguistics hlt naacl boston massachusetts usa pages paulus paulus xiong socher deep reinforced model abstractive summarization arxiv preprint radev mckeown radev mckeown generating natural language maries multiple line sources computational linguistics reiter dale reiter dale building applied natural language generation systems nat lang eng rush rush chopra weston neural attention model abstractive sentence summarization proceedings conference empirical methods natural language processing emnlp lisbon portugal september pages liu manning point summarization pointer generator networks proceedings annual meeting association computational linguistics acl vancouver canada july august volume long papers pages wong wong extractive summarization supervised semi supervised learning proceedings international conference computational volume pages association computational linguistics callison burch napoles problems current text simplication research new data help tacl pyteaser yin pei yin pei optimizing sentence modeling selection document summarization proceedings fourth international joint conference articial intelligence ijcai buenos aires argentina july pages zhang lapata zhang lapata sentence simplication deep reinforcement proceedings conference empirical methods natural language processing learning emnlp copenhagen denmark september pages
