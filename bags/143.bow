a survey on neural network based summarization methods yue dong april r a m l c s c v v i x r a introduction every day enormous amounts of text are published online and quick access to the major points of these documents is critical for decision making however manually producing summaries for such large amounts of documents in a timely manner is no longer feasible automatic text summarization the automated process of shortening a text while reserving the main ideas of the has consequently became popular up until recently text summarization was dominated by unsupervised information retrieval models in kageback et al demonstrated that the neural based continuous vector models are promising for text summarization this marked the beginning of the widespread use of neural network based text summarization models because of their superior performance compared to the traditional techniques the aim of this literature review is to survey the recent work on neural based models in automatic text summarization this survey starts with the general background on document summarization section including the factors by which summarization tasks may be ed evaluation issues and a brief history of the traditional summarization techniques section examines in detail ten neural based summarizers section discusses the related techniques and presents promising paths for future research and section concludes the paper background summarization factors according to jones et al text summarization tasks can be dened and classied by the following factors input purpose and output input factors single document vs multi document jones et al denes this factor as the unit input parameter which simply is the number of input documents that the summarization system takes monolingual multilingual vs cross lingual the monolingual summarizers produce summaries that are in the same languages as the inputs while the multilingual systems can handle the input output pairs in the same language across several dierent languages on the contrary the cross lingual summarization systems operate on input output pairs that are not necessarily in the same language purpose factors informative vs indicative an indicative summary serves as a road map to convey the relevant contents of the original documents so the readers can select documents that align with their interests to read further an indicative summary itself is not supposed to be a substitute for the source documents on the other hand the purpose of an informative summary is to replace the original documents as far as the important contents is concerned generic vs user oriented this factor concerns the coverage of the original documents conditioned on the potential readers of the summary generic systems create summaries which consider all the information found in the documents in contrast user oriented systems produce personalized summaries that focus on certain information from the source that are consistent with a user query general purpose vs domain specic general purpose summarizers can be used across any with little or no modication on the other hand domain specic systems are designed for processing documents in a specic domain output factors extractive vs abstractive in relation to the source a summary can either be extractive or abstractive there is no clear agreement on the denition of the two in this literature review the denition of see et al is adopted where an extractive summarizer explicitly selects text snippets words phrases sentences from the source while an abstractive summarizer generates novel text snippets to convey the most salient cepts prevalent in the source evaluation of summarization systems evaluation is critical for developing summarization systems however what evaluation criteria should be used for assessing summarization systems still remains unclear due to the subjective aspect of what makes for a good summary in general existing evaluation techniques can be split into either intrinsic or extrinsic jones et al intrinsic methods directly evaluate the outcome of a summarization system and extrinsic methods evaluate summaries based on the performance of the down stream tasks that the system summaries are used for the most prevalent intrinsic evaluation is to compare system generated summaries system summaries with human created gold summaries reference summaries this allows the use of quantitative measures such as the precisions and recalls in rouge lin however the problem with rouge is that people usually disagree on what a gold summary should be evaluation methods such as pyramid nenkova and passonneau address this problem and assume that no single best gold summary exists however pyramid is very expensive in terms of the human involvement up to this day no single best summarization evaluation method exists and researchers usually adopt the cheap automated evaluation metric rouge coupled with human ratings rouge lin recall oriented understudy for gisting evaluation rouge are a set of evaluation methods that automatically determine the quality of a system summary by comparing it to created summaries rouge n rouge l and rouge su are commonly used in the marization literatures rouge n computes the percentage of n gram overlapping of system and reference maries it requires the consecutive matches of words in n grams n needs to be dened and xed that is often not the best assumption rouge l computes the sum of the longest in sequence matches of each reference sentence to the system summary it considers the sentence level word orders and automatically identify the longest in sequence word overlapping without a pre dened n rouge su measures the percentage of skip bigrams and unigrams overlapping bigram consists two words from the sentence with arbitrary gaps in their sentence order plying skip bigrams without any constraint on the distance between the words usually produce spurious bigram matchings lin therefore rouge su is usually used with a limited maximum skip distance such as rouge with maximum skip distance of pyramid nenkova and passonneau instead of matching the exact phrase units as in lin pyramid tries to score summaries based on semantic matchings of the content units it works under the assumption that there s no single best summary and therefore multiple reference summaries are necessary for this system given a document with n human created reference summaries rn pyramid score of a random summary s is roughly computed as follows human annotation are rst required to identify all summarization content units scu in rn and in s where scus are the smallest content unit for some semantic meaning nenkova and passonneau each scu is then associated with a weight by counting how many reference summaries in the cluster contain this scu suppose summary s is annotated with k scus with weights wk the pyramid wi soptimal where soptimal is the sum of the largest score of s is computed as scus weights p summarization techniques scope of this review in this literature review we primarily consider neural based extractive and abstractive rization techniques with the following factors single document english informative generic and general purpose as far as i know related surveys either investigate the traditional models afantenos et al das and martins nenkova et al or give little details for neural based summarizers gambhir and gupta brief history of pre neural networks era extractive models most early works on single document extractive summarization employ statistical techniques based on the edmundsonian paradigm afantenos et al such algorithms rank each sentence based on its relation to the other sentences by using pre dened formulas such as the sum of frequencies of signicant words luhn the overlapping rate with the document title the correlation with salient cepts topics latent semantic and liu and sum of weighted similarities to other sentences textrank mihalcea and tarau formulas usually do nt contain hyper parameters and therefore training is not required later works on text summarization address the problem by creating sentence representations of the documents and utilizing machine learning algorithms these models manually select the appropriate features and train supervised models to classify whether to include the sentence in the summary for example wong et al extracted surface content event and relevance features for the sentence representation and used support vector machines svm and nave in addition sequential models such as hidden markov bayes models for the classication chains hmms conroy and oleary were proposed to improve the results by considering the sentence orders in the documents abstractive models the core of abstractive summarization techniques is to identify the main ideas in the documents and encode them into feature representations these encoded tures are then passed to natural language generation nlg systems such as the one proposed in reiter and dale for summary generation most of the early work on abstractive summarization uses semi manual process of identifying the main ideas of the prior knowledge such as scripts and templates are usually used to produce summaries thus the abstractive summary is produced through slot llings and simple smoothing techniques such as in dejong radev and mckeown neural based summarization techniques since the bloom of deep learning neural based summarizers have attracted considerable tion for automatic summarization compared to the traditional models neural based models achieve better performance with less human involvement if the training data is abundant in this section ve extractive and ve abstractive neural based models are examined in details most neural based summarizers use the following pipeline words are transformed to continuous vectors called word embeddings by a look up table sentences documents are encoded as continuous vectors using the word embeddings sentence document tations sometimes also word embeddings are then fed to a model for selection extractive summarization or generation abstractive summarization neural networks can be used in any of the above three steps in step we can use neural networks to obtain pre learned look up tables such as cw vectors and glove in step neural networks such as convolutional neural networks cnns or recurrent neural can be used as encoders for extracting sentence document features in step neural network models can be used as regressors for ranking selection extraction or decoders for generation abstraction cnns and rnns cnns and rnns are commonly used in neural based summarizers both cnns and rnns serve the same purpose transform a sequence of word embeddings xt rd to a vector sentence representation s rh cnns achieve this purpose by using h lters and sliding them over the input sequence each lter performs local on the sub sequences of the input to obtain a set of feature maps scalars then a global max pooling over time is performed to tain a scalar these scalars from the h lters are then concatenated into the sequence representation vector s rh convolution operation used here is basically element wise matrix multiplication followed by a tion rnns achieve this purpose by introducing time dependent neural networks at the time step t an rnn computes a hidden state vector ht which is obtained by a non linear transformation with two inputs the previous hidden state and the current word input xt the most basic rnn is called the elman rnn ht xt ht two other popular rnns which address the problem of long term dependencies by adding extra parameters are as follows gated recurrent unit gru long short term memory lstm zt tanh xt zt zt it ft ot c t tanh xt ct ft it t ht ot where denotes element wise matrix multiplication and wi are matrices with the responding dimensions the last hidden state ht is usually used as the sequence sentation s rh extractive models extractive summarizers which are selection based methods need to solve the following two ical challenges how to represent sentences how to select the most appropriate sentences taking into account of the coverage and the redundancy in this section we review ve extractive neural based summarizers in chronological order each summarization system is presented based on its sentence representation model and its sentence selection model at the end of this section the techniques used in the extractive neural based models are summarized and the models performance are compared continuous vector space models kageback et al sentence representation kageback et al proposes to represent sentences as tinuous vectors that are obtained by either adding the word embeddings or using an unfolding recursive auto encoder rae on word embeddings the rae basically combines two text units into one in a recursive manner until only one vector the sentence representation left the rae is trained in an unsupervised manner by the backpropagation method with the reconstruction errors the pre computed word embeddings from collobert and weston s model cw vectors or mikolov et al s model vectors are directly used without ne tuning sentence selection kageback et al formulates the task of choosing summary s as an optimization problem that maximizes the linear combination of the diversity of the sentences r and the coverage of the input text l where is the tread o between the converge and the diversity according to kageback et al this optimization problem is np hard however there exists fast scalable approximation algorithms with theoretical guarantees if the objective tion is submodular the authors choose two submodular functions which are computed based on sentence similarities as the diversity function and the converge function respectively the objective function is therefore submodular and an approximation optimization algorithm described in kageback et al is used for selecting the sentences cnnlm yin and pei sentence representation yin and pei uses convolutional neural networks cnns similar to the basic cnn model we described previously on pre trained word embeddings to obtain the sentence representation the learnable parameters including the word embeddings in the cnn are trained by unsupervised learning the noise contrastive estimation nce mnih and teh is used as the cost function with this cost function the model is basically trained as a language it learns to discriminate between true next words and noise words sentence selection similar as in kageback et al the authors frame the sentence selection as a direct optimization problem with the following objective function i pimi jpj is x i js x here the matrix m is obtained by calculating the pairwise cosine similarities of the learned sentence representations the prestige vector p is derived by using the pagerank algorithm on m the goal is to nd a summary s as set of sentences that maximizes the above objective function fortunately equation is also submodular proof in yin and pei fore as stated in kageback et al a near optimal solution exists and is presented in yin and pei priorsum cao et al sentence representation priorsum uses the cnn learned features concatenated with ument independent features as the sentence representation three document independent tures are used sentence position averaged term frequency of words in the sentence based on the document averaged term frequency of words in the sentence based on the cluster multi document summarization similar as in yin and pei cnns with multiple lters are used to capture sentence features however priorsum employs a deeper and more complicated cnn the cnn used in priorsum has multiple layers with alternating convolution and pooling operations the lters in the convolution layers have dierent window sizes and two stage max over time pooling operations are performed in the pooling layers the parameters in this cnn is updated by applying the diagonal variant of adagrad with mini batches as described in yin and pei sentence selection unlike the previous two extractive neural based models priorsum is a supervised model that requires the gold standard summaries during training priorsum follows the traditional supervised extractive framework it rst ranks each sentence and then selects the top k ranked non redundant sentences as the nal summary function f is called submodular on the set s if s s a b implies s this condition is also called as the diminishing return property the authors frame the sentence ranking process as a regression problem during training each sentence in the document is associated with the score stopwords removed with respect to the gold standard summary then a linear regression model is trained to estimate these scores by updating the regression weights during testing non redundant sentences are selected by a simple greedy algorithm the greedy selection algorithm rst ranks all sentences with more than words in descending order based on the estimated informative scores the top k sentences are then selected in order as long as the sentence is not redundant with respect to the current summary a sentence is considered non redundant with respect to a summary if more than of its words do not appear in the summary nn se cheng and lapata sentence representation in cheng and lapata sentence representations are tained by using a cnn followed by an rnn the cnn extractor which is similar to the one in has multiple feature maps with dierent window sizes once sentence representations st are obtained by using the cnn sentence extractor they are fed into an lstm encoder the lstm s hidden states ht are then used as the nal sentence sentations comparing to st the authors believe ht capture the sentence dependency information and are therefore better suited as sentence representations sentence selection similar to cao et al s work nn se is a supervised model that rst scores the sentences and then selects them based on the estimated scores instead of using a simple linear regressor as in cao et al nn se utilizes an lstm decoder with a sigmoid layer equation for scoring sentences during training the ground truth labels are given for sentences included in the reference summary and otherwise and the decoder is trained to label sentences sequentially by zeros and ones given vectors st obtained by the cnn and the lstm encoder s hidden states ht the decoder s hidden states ht are computed as lst where is the probability that the decoder believes the previous sentence should be included in the summary the binary decision of whether to include sentence t are modeled by the following sigmoid layer ht where mlp is a multi layer neural network joint training with a large scale dataset nn se is a sequence to sequence model with a encoder and a decoder the encoder sentence representation model and the decoder sentence selection model can be jointly trained by the stochastic gradient descent sgd method with the objective of minimizing the negative log likelihood nll training a sequence to sequence summarizer requires a large scale dataset with extractive bels i e documents with sentences labeled as summary worthy or not the authors created a large scale dataset the dailymail dataset with about k training examples each data instance contains an extractive reference summary that is obtained by labeling sentences based on a set of rules such as sentence positions and grams overlapping m x summarunner nallapati et al sentence representation summarunner employs a two layer bi directional rnn for tences and document representations the rst layer of the rnn is a bi directional gru that runs on words level it takes word embeddings in a sentence as the inputs and produces a set of hidden states these hidden states are averaged into a vector which is used as the sentence representation the second layer of the rnn is also a bi directional gru and it runs on the sentence level by taking the sentence representations obtained by the rst layer as inputs the hidden states of the second layer are then combined into a vector document representation through a non linear transformation sentence selection the authors frame the task of sentence selection as a sequentially tence labeling problem which is similar to the settings of cheng and lapata dierent from cheng and lapata instead of using another rnn as the decoder summarunner uses the hidden states hm from the second layer of the encoder rnn directly for the binary decision modeled by a sigmoid function p yt st ht t where includes the information of the sentence s absolute and relative position as well as the bias sj can be viewed as a soft summary representation that is computed as the running weighted sum of sentence representations until time t st hip yi the sigmoid decision layer and the two layer encoder rnn are jointly trained by sgd with the objective function similar to in cheng and lapata p comparison of the extractive models and their performance table compares and summarizes the ve extractive models mentioned previously almost all these models are evaluated on dataset and we therefore compare their performance on dataset in table models continuous tor space models sentence selection sentence tation adding word beddings or using rae table comparison of the techniques used in the extractive summarizers training of sentence representation for training no adding rae is trained in an supervise with res unsupervised ing with nce direct optimization on submodular jectives cnn no training training of sentence selection no training cnnlm priorsum cnn nn se unsupervised ing with diagonal variant of adagrad supervised co train with decoder supervised learning with scores supervised learning with sgd and nll summarunner rae recursive auto encoder res reconstruction errors nce noise contrastive estimation supervised co train with decoder ranking supervised learning with sgd and nll direct optimization on submodular jectives ranking sentence by linear regression from sentence ing sentence from sigmoid table rouge scores of the extractive summarizers on the dataset models rougel rougesu continuous vector space models cnnlm priorsum nn se summarunner extra data used for training in addition to pre trained and cw word embeddings pre trained word dings pre trained cw word dings gigaword for cnn dailymail pre trained glove word beddings dailymail abstractive models abstractive summarizers focus on capturing the meaning representation of the whole document and then generate an abstractive summary based on this meaning representation therefore neural based abstractive summarizers which are generation based methods need to make the following two decisions how to represent the whole document by an encoder how to generate the words sequence by a decoder in this section we review ve abstractive neural based summarizers in chronological order each summarization system is presented based on its encoder and its decoder at the end of this section the techniques used in the abstractive neural based models are summarized and the models performance are compared abs rush et al encoder rush et al proposes three encoder structures to capture the meaning resentation of a document the common goal of these encoders is to transform a sequence of word embeddings wt to a vector d which is used as the meaning representation of the document bag of words encoder the rst encoder basically computes the summation of the word t xi the word order is not preserved by embeddings appeared in the sequence t this bag of words encoder p convolutional encoder this encoder utilizes a cnn model with multiple alternating convolution and element max pooling layers in each layer the convolution operations extract a sequence of feature vectors ul and the number of these feature vectors are reduced by a factor of two with the element max pooling after l layers of convolution and max pooling a max pooling over time is performed to obtain the document representation ul attention based encoder this encoder produces a document representation at each time step based on the previous c words context generated by the decoder at time step t given the inputs word embeddings x xm and the decoder s context c the encoder produces a document representation for time step t as follows pt x where rm dt c decoder for estimating the probability distribution that generates the word at each time step t rush et al uses a feed forward neural network based language model nnlm c dt where ht c training in rush et al the encoder and decoder are trained jointly in mini batches suppose are j input summary pairs then the loss negative likelihood loss nll based on the parameters is computed as t j x j t x x the training objective is to minimize the nll and it is achieved by using mini batch stochastic gradient descent ras lstm and ras elman chopra et al encoder the cnn based attentive encoder used in chopra et al is similar to the attentive encoder proposed by rush et al except the weights i is computed based on the aggregated vectors obtained by a cnn model at time step t the attention weights are calculated by the aggregated vectors zt and decoder s hidden state ht j t t ht these attention weights are then combined with the inputs word embeddings to form the document representation dt dt t j p p decoder chopra et al replaces the nnlm model used in rush et al to a recurrent neural network instead of only using the previously generated c words for decoding as in nnlm the rnn decoder s hidden state ht can keep the information of all the words generated till time t the authors propose two decoder models based on the elman rnn and the in addition to the previous generated word and the previous hidden state the elman rnn and the lstm take encoder s context vector dt document representation at t as an additional input for example the elman rnn s hidden state is computed as ht once the decoder s hidden state ht is computed it is combined with the document sentation dt to decide which word to generate at the time step t the decision is modeled by a softmax function which gives the probability distribution over all the words in the dictionary pt sof hierarchical attentive rnns nallapati et al encoder on the bidirectional gru to represent the document nallapati et al proposes a feature rich hierarchical attentive encoder based feature rich inputs the encoder takes the input vector obtained by concatenating the word embedding with additional linguistic features the additional linguistic features used in their model are parts of speech pos tags named entity ner tags term frequency tf and inverse document frequency idf of the word the continuous features tf and idf are rst discretized into a xed number of bins and then encoded into one hot vectors as other discrete features all the one hot vectors are then transformed into continuous vectors by embedding matrices and these continuous vectors are concatenated into a single long vector which is then fed into the encoder the document representation at time step t is also called the encoder s context at time step t which is commonly denoted as ct in literature the elman rnn and lstm work to produce the hidden states are explained early in section cnns and rnns words are generated from a dictionary hierarchical attention the hierarchical encoder has two rnns with a similar structure as in nallapati et al one runs on the word level and one runs on the sentence level the hierarchical attention proposed by the authors basically re weigh the word attentions by the corresponding sentence level attention the document representation dt is then obtained by the weighted sum of the feature rich input vectors decoder nallapati et al uses a rnn decoder based on uni directional gru which works similar to the decoder in chopra et al in addition the following two mechanisms are used in nallapati et al decoder the large vocabulary trick lvt this trick reduces the computation time in the softmax layer by limiting the number of words the decoder can generate from during training basically it denes a small dictionary in each mini batch during training the dictionary only contains the words that are in the source documents of that batch and the most frequent k words in the global dictionary decoder pointer switch using a pointer network which directly copy words from the source can improve the summaries quality by including the rare words from the source documents a pointer network can simply be modeled based on the encoder s attention weights where the word with the largest weight is the word for copying the decision of whether to copy or generate is controlled by a switch which is modeled by a sigmoid function p ht dt pointer generator networks see et al encoder the encoder of the pointer generator network is simply a single layer bidirectional lstm it computes the document representation based on the attention weights and the encoder s hidden states which is exactly the same as the encoder in chopra et al decoder the basic building block of see et al decoder is a single layer uni directional lstm in addition a decoder pointer switch similar to nallapati et al is used for ing moreover the authors propose a coverage mechanism for penalizing repeated attentions on already attended words this is achieved by using a coverage vector ct which tracks the attentions that all the words in the dictionary has received till time t ct the coverage vector is then used for the attention computation at time step t as well as in the objective function acted as a regularizer at p lt t i ct i i x t is the true label at the time step t and is a hyperparameter controlling the degree here w of the coverage regularizer neural intra attention model paulus et al encoder et al also uses a bi directional lstm encoder for modeling the ument representation the model is similar to the encoder in see et al except the attention scores are computed by linear transformations and a softmax which is attention scores in all other models we reviewed are computed by sigmoid functions followed by a softmax function called the intra attention mechanism by the authors dt is then computed based on these intra attentions and the encoder s hidden states decoder a uni directional lstm is used as the decoder in paulus et al in addition the authors employ an intra attention mechanism on the decoder to prevent generating repeated phrases a decoder context vector ct is computed based on the intra attentions of the already generated sequence and then used as an additional input for the softmax layer of generating a generator pointer switch similar to the ones in nallapati et al and see et al is also employed in the decoder hybrid training objectives in terms of the encoder decoder model et al and see et al are very similar however what novel in paulus et al is how the parameters in their model are updated they use both stochastic gradient descent method and reinforcement learning method to update model parameters with a hybrid training objectives stochastic gradient descent method sgd is used in abstractive summarization models to minimize the negative log likelihood of the ground truth values during the training as plained in the previous models rush et al chopra et al nallapati et al see et al we denote this nll objective as lml using sgd to minimize lml has two shortcomings it creates a discrepancy during training and testing since there are no ground truth values during testing optimizing this objective does not always correlate to a high score on the discrete evaluation metric such as rouge scores therefore the authors propose to use another objective based on the reinforcement learning method reinforce for training lrl ys t x t where ys is obtained by sampling from the at each decoding time step t y acts as the reinforce baseline which is obtained by performing a greedy selection rather than sampling at each decoding time step is the reward score for an output sequence y which is usually obtained by an automated evaluation method such as rouge ys the authors noticed that optimizing lrl directly would lead to sequences with high rouge scores that are ungrammatical therefore a mixed training objective with hyperparameter is used for balancing the rouge score and the readability of the generated sequence lmixed lrl comparison of the abstractive models and their performance table compares and summarizes the above ve abstractive models two large scale datasets the gigaword dataset and the cnn dailymail dataset are commonly used as the abstractive summarization benchmarks we therefore compare the ve abstractive models performance on these two datasets as in table table comparison of the techniques used in the abstractive summarizers models abs ras lstm and elman hierarchical rnns attentive pointer generator works neural model intra attention encoder bag of words encoder cnn attention based encoder cnn attention decoder nnlm elman rnn or lstm sgd training sgd feature rich bidirectional gru hierarchical attention bidirectional lstm attention bidirectional lstm intra attention gru lvt pointer switch sgd lstm pointer switch coverage mechanism lstm pointer switch intra attention sgd sgd reinforce table rouge scores of the abstractive summarizers on the datasets models rougesu abs ras lstm and ras elman hierarchical attentive rnns pointer generator networks neural intra attention model g c g c rougel g c g c discussions and the promising paths for future search other related tasks and techniques reinforcement learning methods for sequence prediction bahdanau et al et al shows a promising path of applying the reinforcement learning rl method in abstractive summarization paulus et al applies reinforce which is an unbiased estimator with large variance for sequence prediction in summarization in bahdanau et al the authors apply the actor critic algorithm which is a biased estimator with smaller variable for machine translation in addition to the policy network an encoder decoder model they introduce a critic network that is trained to predict the values of output tokens this critic network is based on a bidirectional gru and is trained supervisely with the ground truth labels the key dierence in the reinforce algorithm and the actor critic algorithm is what rewards the actor uses to update its parameters reinforce uses the overall reward from the whole sequence and only performs the update after obtaining the whole trajectory the actor critic algorithm uses the td errors bahdanau et al calculated based on the critic network and can update the actor during the generating process compared to the force algorithm the actor critic method has lower variance and faster convergence rate which makes it a promising algorithm to be used in summarization text simplication xu et al zhang and lapata the goal of text simplication is to rewrite complex documents into simpler ones that are easier to understand this is usually achieved by three operations splitting deletion and paraphrasing xu et al text simplication can help improve the performance of many natural language processing nlp tasks for example text simplication techniques can transform long complex sentences into ones that are more easily processed by automatic text summarizers one challenge of developing text simplication models is the lack of datasets with parallel complex simple sentence pairs xu et al created a good quality simplication dataset called the newsela dataset for the tasks of text simplication from their analyses we could see that the words distribution are signicantly dierent in complex and simple texts in addition the distribution of syntax patterns are also very dierent these ndings indicate that a text simplication model need to consider both the semantic meaning of words and the syntactic patterns of sentences zhang and lapata propose a sequence to sequence model with attentions based on lstms for text simplication this encoder decoder model called deep reinforcement tence simplication dress is trained with the reinforcement learning method that optimizes a task specic discrete reward function this discrete reward function encourages the outputs to be simple grammatical and semantically related to the inputs experiments on three datasets demonstrate that their model is promising for text simplication tasks discussions in summarization one critical issue is to represent the semantic meanings of the sentences and documents neural based models display superior performance on automatically extracting these feature representations however deep neural network models are neither transparent enough nor integrating with the prior knowledge well more analysis and understanding of the neural based models are needed for further exploiting these models in addition the current neural based models have the following limitations they are unable to deal with sequences longer than a few thousand words due to the large memory requirement of these models they are unable to work well on small scale datasets due to the large amount of parameters these models have they are very slow to train due to the complexity of the models there are many very interesting and promising directions for future research on text marization we proposed two directions in this review using the reinforcement learning approaches such as the actor critic algorithm to train the neural based models exploiting techniques in text simplication to transform documents into simpler ones for summarizers to process conclusion this survey presented the potential of neural based techniques in automatic text tion based on the examination of the state of the art extractive and abstractive summarizers neural based models are promising for text summarization in terms of the performance when large scale datasets are available for training however many challenges with neural based models still remain unsolved future research directions such as adding the reinforcement learning algorithms and text simplication methods to the current neural based models are provided to the researchers references afantenos et al afantenos s karkaletsis v and stamatopoulos p summarization from medical documents a survey articial intelligence in medicine bahdanau et al bahdanau d brakel p xu k goyal a lowe r pineau j courville a and bengio y an actor critic algorithm for sequence prediction cao et al cao z wei f li s li w zhou m and wang h learning summary prior representation for extractive summarization in acl cheng and lapata cheng j and lapata m neural summarization by extracting sentences and words in proceedings of the annual meeting of the association for computational linguistics volume long papers pages berlin germany association for computational linguistics chopra et al chopra s auli m and rush a m abstractive sentence summarization with attentive recurrent neural networks in naacl hlt the conference of the north american chapter of the association for computational linguistics human language technologies san diego fornia usa june pages conroy and oleary conroy j m and oleary d p text summarization via hidden markov models in proceedings of the annual international acm sigir conference on research and development in information retrieval pages acm das and martins das d and martins a f a survey on automatic text summarization dejong dejong g f an overview of the frump system in lehnert w g and ringle m h editors strategies for natural language processing pages lawrence erlbaum gambhir and gupta gambhir m and gupta v recent automatic text summarization niques a survey articial intelligence review gong and liu gong y and liu x generic text summarization using relevance measure and latent semantic analysis in proceedings of the annual international acm sigir conference on research and development in information retrieval pages acm jones et al jones k s al automatic summarizing factors and directions advances in automatic text summarization pages kageback et al kageback m mogren o tahmasebi n and dubhashi d extractive marization using continuous vector space models in proceedings of the workshop on continuous vector space models and their compositionality eacl pages lin lin c rouge a package for automatic evaluation of summaries in francine moens s s editor text summarization branches out proceedings of the workshop pages barcelona spain association for computational linguistics luhn luhn h p the automatic creation of literature abstracts ibm journal of research and development mihalcea and tarau mihalcea r and tarau p textrank bringing order into text in ceedings of the conference on empirical methods in natural language processing nallapati et al nallapati r zhai f and zhou b summarunner a recurrent neural network based sequence model for extractive summarization of documents in proceedings of the thirty first aaai conference on articial intelligence february san francisco california usa pages nallapati et al nallapati r zhou b dos santos c n gulcehre c and xiang b tive text summarization using sequence to sequence rnns and beyond in proceedings of the signll conference on computational natural language learning conll berlin germany august pages nenkova et al nenkova a mckeown k al automatic summarization foundations and trends in information retrieval nenkova and passonneau nenkova a and passonneau r j evaluating content selection in summarization the pyramid method in human language technology conference of the north american chapter of the association for computational linguistics hlt naacl boston massachusetts usa may pages paulus et al paulus r xiong c and socher r a deep reinforced model for abstractive summarization arxiv preprint radev and mckeown radev d r and mckeown k r generating natural language maries from multiple on line sources computational linguistics reiter and dale reiter e and dale r building applied natural language generation systems nat lang eng rush et al rush a m chopra s and weston j a neural attention model for abstractive sentence summarization in proceedings of the conference on empirical methods in natural language processing emnlp lisbon portugal september pages see et al see a liu p j and manning c d get to the point summarization with pointer generator networks in proceedings of the annual meeting of the association for computational linguistics acl vancouver canada july august volume long papers pages wong et al wong k wu m and li w extractive summarization using supervised and semi supervised learning in proceedings of the international conference on computational volume pages association for computational linguistics xu et al xu w callison burch c and napoles c problems in current text simplication research new data can help tacl xu xu x pyteaser yin and pei yin w and pei y optimizing sentence modeling and selection for document summarization in proceedings of the twenty fourth international joint conference on articial intelligence ijcai buenos aires argentina july pages zhang and lapata zhang x and lapata m sentence simplication with deep reinforcement in proceedings of the conference on empirical methods in natural language processing learning emnlp copenhagen denmark september pages
