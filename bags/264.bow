e f l c s c v v x r abstractive summarization model satyaki chakraborty cmu edu xinya li cmu edu sayak chakraborty sayak com introduction like pointer generator networks extremely popular method text summarization recent works domain build baseline pointer generator augmenting content selection phase decomposing decoder contextual network language model work rst thoroughly investigate generator network unable generate novel words adding vocabulary oov penalty able improve novelty abstraction signicantly use normalized n gram novelty scores metric determining level straction report rouge scores model summarization models evaluated r l scores related work research text summarization largely divided approaches methodology pervised learning trained cross entropy loss rl based training directly tries optimize rouge score sequence seqeuence models pioneering work rst approach based original model encoder decoder attention rnn model encoder bidirectional gru rnn form condensed representation decoder unidirectional gru rnn generates summary proposes original way duce dimensionality softmax s output thresholding size vocabulary applies switching pointer generator work generate rare vocab oov words test time proposes model extractive summarization uniquely models extractive summarization sequence tion learns generate extractive summaries ground truths abstract maries pointer generator model project based shares idea generating oov words network sign network output hybrid summaries abstractive extractive type time avoid phrase tion incorporating coverage vector attention distribution dening auxiliary coverage loss penalizes phrase repetition rl based methods main papers applying rl methods text summarization include corporates standard encoder decoder neural network supervised word prediction inforcement learning policy self critical gradient poses extractive summarization problem sentence ranking task uses inforcement learning training network optimizing rouge metric objective function combines cross entropy loss rewards obtained policy gradient learning optimize rouge objective proposes hybrid summarization task tracts sentences rewrite abstractively hybrid network architecture icy based reinforcement learning rouge l gram ized novelty cnn dm dataset use gigaword survey material contains multiple summaries source text provides ample examples survey people s preferences ferent types discuss methods improve survey tion models focus improving rouge score generated summaries attempt actually focus improving level abstraction having separate contextual network encodes current state decoder separate language model adding novelty reward optimized policy dient encourage novel word generation contrary approach use external optimization reward novel word instead argue eration fundamental issue novelty proved external reward datasets commonly dataset text marization cnn daily mail dataset processed version project popular train s original pointer generator work original cnn daily mail dataset contains online news articles tokens age paired multi sentence summaries sentences tokens cessed version contains training pairs validation pairs test original pg network model evaluated length scores motivation research project provide better abstractive summarization tool vanilla pointer generator model aims learn hybrid summaries stractive extractive type summaries largely cling source text contain atively novel words low novelty score abstractiveness demonstrate tance conducted survey investigate ple s preferences abstractive tive ground truth summaries document understanding conference marization survey taker presented pieces source texts stractive extractive summaries ing told asked choose summary prefer total takers completed survey preferences total averaging preferences choices equal weight favor stractive summaries approach section rstly briey discuss tails original vanilla pointer generator work understand fundamentals shift limitations diagnosis nally overcome limitations additional loss results ed algorithm figure network architecture vanilla pointer generator vanilla model h t context vector dened model based pointer generator work model single layer bidirectional lstm layer encodes source text duces sequence encoder hidden states hi time step t unidirectional decoder receives hi current decoder state st generate attention distribution ability distribution source words makes generation oov words inside source text possible vocabulary tion pvocab vt wsst battn et sof v wh ws battn learnable eters attention distribution pattn given subsequently generate text vector h t probability distribution entire vocabulary pvocab given follows pvocab sof v st h h t ai thi nal probability distribution word eration weighted combination attention distribution vocabulary tribution respective domain words zero values words source text oov words respectively tion weights learned end end eration probability pgen pgen h h t wt s st wt xt bptr vectors wt h ws wx scalar bptr learnable parameters sigmoid tion pf inal pgen pvocab pgen pattn easily eq pgen urally acts like control switch decide model generate new word vocabulary distribution source word bution controlling generation probability source text words pgen control abstraction summaries observations novelty rst calculate unigram bigram elties summaries generated vanilla pointer generator reference summaries evaluate quality novelty score dened n xgen n n denotes function computes set unique n grams ment xgen denotes generated summary xsrc denotes source document number words piece text s novel word word given source text abstractive summaries dier extractive ones exactly use novel words novelty score eectively reects stractive summary compared source text novelty comparison table shown low table novelty score comparison pg ground truth n gram unigram bigram pointer generator ground truth table vanilla pointer generator network hardly produces novel words explore happens pgen distribution figure pgen distribution gure observe time pgen time explains novelty vanilla model pgen extremely low time nal probability distribution nicantly biased attention tion model ends copying words source text vocabulary distribution ignored pgen trend answer question following analysis pvocab vs pattn sampled words plot pvocab pattn word sampled nal distribution randomly selected summary scatter plot shown follows figure pvocab vs pattn vanilla model time increases left right pgen distribution randomly lected generated summaries shown follows observe word exists vocabulary contribution vocab bution non signicant pvocab hardly greater words occurrence word vocabulary pvocab pattn pattn pvocab tion higher value pf inal obtained pgen low explains pattern shown gure network biases attention distribution observe initial phase training network randomly initialized pattn pvocab low training proceeds pattn slowly increases nt change pvocab caused fact pattn distributed words maximum encoder sequence length pvocab distributed words easier network distribute probability mass words compared words preference pattn set pgen start increase favour pattn pvocab vocabulary distribution ignored modication assumption stems simple tion named entities foreign people place names appear source text words main reason separate copy tribution pointer generator network cause want option ating oov words summary ply copying source text non oov word argue prefer vocabulary distribution attention tribution able increase pability network generate novel words result abstract maries means case target word generated oov want pgen low e favour attention distribution target word generated non oov want pgen high formulate straint auxiliary loss computes negative log likelihood pgen yoov yoov target word oov loov pgen figure loov loov target word vocabulary lower value pgen ferred loov target word oov higher value pgen preferred noted penalized model preference vocabulary distribution non oov word use vocab distribution copy distribution debatable order prevent model solely relying vocabulary distribution non oov words train network phases rst vanilla pointer generator coverage loss second coverage loss auxiliary loss gives reasonable balance tween distributions observed following gure truth unsurprisingly scores highest higher novelty scores mixed pvocab pattn demonstrate crease abstractiveness summaries figure pvocab vs pattn model time increases left right results jump result report necessary note robust metric evaluate abstractive text summaries metrics like rouge metrics like rouge sess content selection account quality aspects factual accuracy uency metrics rely ical overlap evaluate content selection naturally puts abstractive summary advantage makes abstractive summary dierent summarizes source text perfect lexical overlaps address problem choose use normalized n gram novelty dened eq having decided evaluation metric fective abstractive summaries given low pgen distribution original generator model hypothesized ed model penalty score higher novelty comparison assess esis plot novelty score comparison tween vanilla pointer generator model model ground truth order single novelty score calculated normalizing randomly chosen samples source plot matches hypothesis summaries huge improvement elty score original model cases unigram bigram score midway original ground figure normalized novelties baseline vs vs ground truth report rouge scores original model s rouge score cluded summarization tools evaluation metric albeit ideal table rouge score comparison pg modied pg pointer generator modied pg r l future directions results mixed pattn pvocab strates denitive improvement summaries abstractiveness worth mentioning trade o rouge score novelty novelty increases rouge score decrease report time resource constraints unable measure correlation alyze number ne tuning iterations needed hit sweet spot better understanding trade o plan plot pareto frontier n gram novelty rouge n dierent ne tuning iterations references p j liu c d manning point summarization pointer generator networks arxiv preprint s gehrmann y deng m rush abstractive summarization arxiv preprint w kryscinski r paulus c xiong r socher improving abstraction text summarization arxiv preprint r nallapati b zhou c gulcehre b et al abstractive text tion sequence sequence rnns arxiv preprint d bahdanau k cho y bengio neural machine translation jointly learning align translate arxiv preprint r nallapati f zhai b zhou marunner recurrent neural network based sequence model extractive marization documents thirty aaai conference articial intelligence r paulus c xiong r socher preprint deep reinforced model summarization tive arxiv s narayan s b cohen m lapata ranking sentences extractive rization reinforcement learning arxiv preprint y chen m bansal fast summarization stractive selected sentence rewriting arxiv preprint k m hermann t kocisky e stette l espeholt w kay m suleyman p blunsom teaching machines read comprehend advances ral information processing systems pp summarization english summarization html annotated english gigaword catalog ldc upenn edu duc documents measures nist gov tasks html tasks duc documents measures nist gov tasks html tasks
