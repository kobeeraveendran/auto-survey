mast multimodal abstractive summarization with trimodal hierarchical attention aman khullar iiit hyderabad hyderabad india aman ac in udit arora new york university new york ny usa edu t c o l c s c v v i x r a abstract this paper presents mast a new model for multimodal abstractive text tion that utilizes information from all three modalities text audio and video in a timodal video prior work on multimodal stractive text summarization only utilized formation from the text and video modalities we examine the usefulness and challenges of deriving information from the audio modality and present a sequence to sequence trimodal hierarchical attention based model that comes these challenges by letting the model pay more attention to the text modality mast outperforms the current state of the art model video text by points in terms of content score and points in terms of rouge l score on the dataset for multimodal guage understanding introduction in recent years there has been a dramatic rise in information access through videos facilitated by a proportional increase in the number of sharing platforms this has led to an enormous amount of information accessible to help with our day to day activities the accompanying scripts or the automatic speech to text transcripts for these videos present the same information in the textual modality however all this information is often lengthy and sometimes incomprehensible because of verbosity these limitations in user experience and information access are improved upon by the recent advancements in the eld of multimodal text summarization multimodal text summarization is the task of condensing this information from the interacting indicates equal contribution aman khullar is presently at gram vaani this paper will appear at the rst emnlp workshop on nlp beyond text modalities into an output summary this ated output summary may be unimodal or modal zhu et al the textual summary may in turn be extractive or abstractive the task of extractive multimodal text summarization volves selection and concatenation of the most portant sentences in the input text without altering the sentences or their sequence in any way li et al made the selection of these tant sentences using visual and acoustic cues from the corresponding visual and auditory modalities on the other hand the task of abstractive modal text summarization involves identication of the theme of the input data and the generation of words based on the deeper understanding of the material this is a tougher problem to solve which has been alleviated with the advancements in the abstractive text summarization techniques rush et al see et al and liu and ata sanabria et al introduced the dataset for large scale multimodal language understanding and palaskar et al were able to produce state of the art results for multimodal abstractive text summarization on the dataset they utilized a sequence to sequence hierarchical tion based technique and helcl for combining textual and image features to duce the textual summary from the multimodal input moreover they used speech for generating the speech to text transcriptions using pre trained speech recognizers however it did not supplement the other modalities though the previous work in abstractive timodal text summarization has been promising it has not yet been able to capture the effects of combining the audio features our work improves upon this shortcoming by examining the benets and challenges of introducing the audio modality as part of our solution we hypothesize that the audio modality can impart additional useful information original text let s talk now about how to bait a tip up hook with a maggot typically you re going to be using this for pan not a real well known or common nique but on a given day it could be the difference between not catching sh and catching all you do you take your maggot you can use meal worms as well which are much bigger which are probably more well suited for this because this is a rather large hook you would just again put that hook right through the maggot with a big hook like this i would probably put ten of these on it just line the whole thing this is going to be more of a technique for pan such as perch and sunsh some of your smaller sh but if you had maggots like this or a meal worm or two on a hook like this this would be a fantastic setup for trout as well text only ice shing is used for ice shing learn about ice shing bait with tips from an experienced sherman artist in this free shing video video text learn about the ice shing bait in this ice shing lesson from an experienced sherman mast maggots are good for catching perch learn more about ice shing bait in this ice shing lesson from an experienced sherman table comparison of outputs by using different modality congurations for a test video example quently occurring words are highlighted in red which are easier for a simpler model to predict but do not tribute much in terms of useful content the summary generated by the mast model contains more content words as compared to the baselines for the text summarization task by letting the model pay more attention to words that are spoken with a certain tone or level of emphasis through our experiments we were able to prove that not all modalities contribute equally to the output we found a higher contribution of text followed by video and then by audio this formed the vation for our mast model which places higher importance on text input while generating the put summary mast is able to produce a more illustrative summary of the original text see table and achieves state of the art results in summary our primary contributions are introduction of audio modality for abstractive multimodal text summarization examining the challenges of utilizing audio formation and understanding its contribution in the generated summary proposition of a novel state of the art model mast for the task of multimodal abstractive text summarization methodology in this section we describe the dataset used the modalities and our mast model s ture the code for our model is available dataset we use the version of the sanabria et al of open domain videos the dataset consists of about hours of short tional videos spanning different domains such as cooking sports indoor outdoor activities music and more a human generated transcript panies each video and a to sentence summary is available for every video written to generate interest in a potential viewer the version is used instead of the version because the audio modality information is only available for the subset the dataset is divided into the training tion and test sets the training set consists of videos totaling hours the tion set consists of videos totaling hours and the test set consists of videos totaling hours a more detailed description of the dataset has been given by sanabria et al for our experiments we took videos for the ing set videos for the validation set and videos for the test set modalities we use the following three inputs corresponding to the three different modalities used audio we use the concatenation of dimensional kaldi povey et al lter bank features from raw audio using a time window of with frame shift and the dimensional pitch features extracted from the dataset to obtain the nal sequence of dimensional audio features text we use the transcripts corresponding to each video all texts are normalized and lower cased video we use a dimensional feature vector per group of frames which is tracted from the videos using a cnn trained to recognize different actions hara et al this results in a sequence of feature vectors per video multimodal abstractive summarization with trimodal hierarchical attention figure shows the architecture of our multimodal abstractive summarization with trimodal com amankhullar mast dently as in bahdanau et al ij a j att a a u k k ij ij ij j i text video j where si is the decoder hidden state at i decoder timestep is the encoder hidden state at j th encoder timestep nk is the number of encoder timesteps for the k th modality and is attention ij energy corresponding to them wa and ua are trainable projection matrices va is a weight vector and batt is the bias term we now look at two different strategies of bining information from the modalities the rst is a simple extension of the hierarchical attention combination the second is the strategy used in mast which combines modalities using three els of hierarchical attention to obtain our rst baseline model with level attention chy the context vectors for all three modalities are combined using a second layer of attention nism and its context vector is computed separately by using hierarchical attention combination as in and helcl u k b vt i i c i u k ci i i text video where is the hierarchical attention distribution over the modalities is the context vector of the k th modality encoder vb and wb are shared and u k parameters across modalities and u k are modality specic projection matrices i figure multimodal abstractive summarization with trimodal hierarchical attention mast architecture mast is a sequence to sequence model that uses mation from all three modalities audio text and video the modality information is encoded using modality encoders followed by a trimodal hierarchical tion layer which combines this information using a three level hierarchical attention approach it attends to two pairs of modalities audio text and text followed by the modality in each pair and followed by the individual features within each ity the decoder utilizes this combination of ities to generate the output over the vocabulary chical attention mast model the model sists of three components modality encoders modal hierarchical attention layer and the modal decoder modality encoders the text is embedded with an embedding layer and encoded using a bidirectional gru encoder the audio and video features are encoded using bidirectional lstm encoders this gives us the individual output encoding corresponding to all modalities at each encoder timestep the tokens corresponding to modality k are encoded using i the corresponding modality encoders and produce a sequence of hidden states for each encoder time step i i trimodal hierarchical attention layer we build upon the hierarchical attention approach proposed by and helcl to bine the modalities on each decoder timestep i the attention distribution and the context vector for the k th modality is rst computed mast to obtain our mast model the text vectors for audio text and text video are bined using a second layer of hierarchical attention mechanisms and and their context vectors are computed separately these context vectors are then combined using the third hierarchical attention mechanism audio text u k d vt i i e i u k i text i i video text u k vt i i g i u k i text i i where l audio text video text is the i text vector obtained for the corresponding pair wise modality combination finally these audio text and video text context vectors are combined using the third and nal tion layer with this trimodal hierarchical tion architecture we combine the textual modality twice with the other two modalities in a pair wise manner and this allows the model to pay more tention to the textual modality while incorporating the benets of the other two modalities h i u l vt i m cf i u l i text video text i where cf timestep i is the nal context vector at i decoder trimodal decoder we use a gru based conditional decoder firat and cho to generate the nal vocabulary distribution at each timestep at each timestep the decoder has the aggregate information from all the modalities the trimodal decoder focuses on the modality combination followed by the individual modality then focuses on the particular tion inside that modality finally it uses this formation along with information from previous timesteps which is passed on to two linear layers to generate the next word from the vocabulary experiments we train trimodal hierarchical attention mast and models on the version of the dataset using all three modalities we also train hierarchical attention models considering audio text and video text modalities as well as simple models with attention for each modality individually as baselines as observed by palaskar et al the pointer generator model see et al does not perform as well as models on this dataset hence we do not use that as a baseline in our experiments we consider another transformer based baseline for the text modality bertsumabs liu and lapata for all our experiments except for the abs baseline we use the nmtpytorch toolkit caglayan et al the source and the target vocabulary consists of words on which we train our word embeddings we use the nll loss and the adam optimizer kingma and ba with learning rate and trained the models for epochs we generate our summaries using beam search with a beam size of and then ate them using the rouge metric lin and the content metric palaskar et al in our experiments the text is embedded with an embedding layer of size and then encoded ing a bidirectional gru encoder cho et al with a hidden layer of size which gives us a dimensional output encoding corresponding to the text at each timestep the audio and video frames are encoded using bidirectional lstm coders hochreiter and schmidhuber with a hidden layer of size which gives a dimensional output encoding corresponding to the audio and video features at each timestep finally the gru based conditional decoder uses a hidden layer of size followed by two linear layers which transform the decoder output to generate the nal output vocabulary distribution to improve generalization of our model we use two dropout layers within the text encoder and one dropout layer on the output of the conditional decoder all with a probability of we also use implicit regularization by using early stopping mechanism on the validation loss with a patience of epochs challenges of using audio modality the rst challenge comes with obtaining a good representation of the audio modality that adds value beyond the text modality for the task of text summarization as found by mohamed dnn acoustic models prefer features that smoothly change both in time and frequency like the log mel frequency spectral coefcients mfsc to the decorrelated mel frequency cepstral coefcients mfcc mfsc features make it easier for dnns to discover linear relations as well as higher order causes of the input data leading to better overall system performance hence we do not consider mfcc features in our experiments and use the ter bank features instead the second challenge arises due to the larger number of parameters that a model needs when handling the audio information the number of parameters in the video text baseline is lion as compared to million when we add audio this is because of the high number of put timesteps in the audio modality encoder which makes learning trickier and more time consuming to demonstrate these challenges as an iment we group the audio features across input timesteps into bins with an average of utive timesteps and train our mast model this makes the number of audio timesteps comparable to the number of video and text timesteps while we observe an improvement in computational ciency it achieves a lower performance than the baseline video text model as described in table mast binned we also train audio only and audio text models which fail to beat the text only baseline we observe that the generated summaries of the audio only model are similar and repetitive indicating that the model failed to learn useful mation relevant to the task of text summarization results and discussion model name text only bertsumabs video only audio only audio text video text mast binned mast rouge l content table results for different congurations mast outperforms all baseline models in terms of rouge scores and obtains a higher content score than all baselines while obtaining a score close to the model preliminaries our results are given in table to demonstrate the contribution of various modalities towards the output summary we experiment with the three modalities taken individually as well as in tion text only video only and the audio only are attention based models bahdanau et al with their respective modality features taken as coder inputs to situate the efcacy of the decoder architecture for our task we use the sumabs liu and lapata as a bert based baseline for abstractive text summarization text and the video text are models with archical attention layer the video text model as presented by palaskar et al has been pared on the version instead of the version of the dataset because the audio modality is only available in the former model adds the audio modality in the second level of archical attention mast binned model groups the features of the audio modality for computational efciency these models show alternative methods for utilizing audio modality information we evaluate our models with the rouge metric lin and the content metric palaskar et al the content metric is the score of the content words in the summaries based on a monolingual alignment it is calculated ing the meteor toolkit denkowski and lavie by setting zero weight to function words equal weights to precision and recall and no cross over penalty for generated words tionally a set of catchphrases like the words in this free video learn how tips expert which appear in most summaries and act like function words instead of content words are removed from the reference and hypothesis summaries as a processing step it ignores the uency of the put but gives an estimate of the amount of useful content words the model is able to capture in the output discussion as observed from the scores for the text only model the text modality contains the most amount of information relevant to the nal summary lowed by the video and the audio modalities the scores obtained by combining the audio text and video text modalities also indicate the same the transformer based model bertsumabs fails to form well because of the smaller amount of text figure distribution of the duration of videos in onds in the test set data available to ne tune the model we also observe that combining the text and dio modalities leads to a lower rouge score than the text only model which indicates that the plain hierarchical attention model fails to learn well over the audio modality by itself this observation is in line with the result obtained by the model where we simply extend the hierarchical attention approach to three modalities usefulness of audio modality the mast and the models achieve a higher content score than the video text line indicating that the model learns to extract more useful content by utilizing information from the audio modality corresponding to the istics of speech in line with our initial hypothesis as illustrated in table however the model which simply adds the audio modality in the second level of erarchical attention fails to outperform the text baseline in terms of rouge scores our tecture lets the mast model choose between ing attention to a different combination of ties with the text modality this forces the model to pay more attention to the text modality thereby overcoming the shortcoming of the model and achieving better rouge scores while maintaining a similar content score when pared to attention distribution across modalities to understand the importance of individual ities and their combinations we plot their tion distribution at different levels of attention erarchy across the decoder timesteps figure corresponds to attention weights as calculated in figure distribution of rouge l scores of summaries produced for different video durations in seconds for mast and video text baseline the videos are binned into groups of seconds by duration and the tion of rouge l scores within each group is shown ing density plots the dotted lines inside each group show the quartile distribution equation while gures and correspond to the product of attention weights between equations and corresponding weight in equation for each decoder timestep the nal attention within each individual modality at each decoder timestep is calculated by multiplying the corresponding mulative attention weights obtained at level of attention hierarchy with the attention weights tained in equation gures to the attention weights assigned to the audio modality have been added across input timesteps group size of in order to obtain a more interpretable visualization through these visualizations we observe that the text modality dominates the generation of the output summary while giving lesser attention to the audio and video modalities the latter being more important these ndings support the extra importance being given to the text modality in the mast model during its interaction with the other modalities figures and highlight the modest gains through the audio modality and the challenge in its appropriate usage performance across video durations we also look at how our model performs for ent video durations in our test set figure shows the variation in the rouge l scores across different videos for mast and the video text baseline the gure shows videos binned into seven groups of seconds by duration we can observe from the quartile distribution that mast outperforms the baseline in ve out of the seven groups gives lar performance for videos with a duration between seconds and underperforms for videos with a duration between seconds however overall by looking at the distribution of the tion of videos in our test set figure we can observe that mast outperforms the baseline for a vast majority of videos across durations new ne tuning schedule for abstractive rization which adopted different optimizers for the encoder and the decoder to alleviate the mismatch between the two bert models typically require large amounts of annotated data to produce state the art results recent works like gan bert by croce et al focus on solving this problem related work abstractive text summarization abstractive summarization of documents was ditionally achieved by paraphrasing and fusing multiple sentences along with their grammatical rewriting woodsend and lapata this was later improved by taking inspiration from human comprehension capabilities when fang and teufel implemented the model of human hension and summarization proposed by kintsch and van dijk they did this by identifying these concepts in text through the application of co reference resolution named entity recognition and semantic similarity detection implemented as a two step competition the real stimulus to the eld of abstractive marization was provided by the application of neural encoder decoder architectures rush et al were among the rst to achieve state of art results on gigaword graff et al and the over et al datasets and lished the importance of end to end deep learning models for abstractive summarization their work was later improved upon by see et al where they used copying from the source text to remove the problem of incorrect generation of facts in the summary as well as a coverage mechanism to curb the problem of repetition of words in the generated summary pretrained language models another breakthrough for the eld of natural guage processing came with the use of pre trained language models for carrying out various language downstream tasks pre trained language models like bert devlin et al introduced masked language modelling which allowed models to learn interactions between left and right context words these models have signicantly changed the way word embeddings are generated by training textual embeddings rather than static embeddings liu and lapata presented how bert could be used for text summarization and proposed a advancements in speech recognition and computer vision parallel advancements in the eld of speech nition and computer vision have been able to give us successful methods to extract useful features of speech and images peddinti et al built a robust acoustic model for speech recognition using a time delay neural network they were able to achieve state of the art results in the iarpa pire challenge similarly with the advancements of convolutional neural networks the eld of puter vision has progressed signicantly he et al demonstrated the strength of deep residual networks which learned residual functions with erence to the layers and were able to achieve of the art results on the imagenet dataset hara et al showed that simple convolutional neural network cnn architectures outperform complex architectures and trained a cnn to recognize different human actions on the kinetics dataset kay et al summarization beyond text the advancements in these elds have in turn also facilitated text summarization rott and cerva used only the input audio to generate tual summaries while sah et al were among the rst to show the possibility of summarizing long videos and then annotating the summarized video to obtain a textual summary these els however were not able to capture the mation of other modalities to obtain the output textual summary and hence their limitations led to the increasing use of multimodal data a major drance in the eld of multimodal text tion was the lack of datasets li et al created an asynchronous benchmark dataset with annotated summaries for videos sanabria et al then released a large scale dataset for structional videos jn et al and zhu et al presented multimodal text summarization models using textual and visual modalities as input and multimodal outputs of summarized text and video palaskar et al used dataset c d e figure visualization of attention weights in the trimodal hierarchical attention layer for a sample video in the test set figures to show the varying attention distribution on different combinations of modalities across the decoder timesteps figures to show the attention distribution on the encoder timesteps for each modality across the decoder timesteps this shows the usefulness of each modality for the generation of the summary to present an abstractive summary of open domain videos these models however are not completely multimodal since they do not utilise the audio mation a major focus of our work is to highlight the importance of using audio data as input and incorporate it in a truly multimodal manner conclusion in this we presented mast a state of the art sequence to sequence based model that uses information from all three modalities audio text and video to generate abstractive multimodal text summaries it uses a trimodal hierarchical tion layer to utilize information from all modalities we explored the role played by adding the audio modality and compared mast with several line models demonstrating the effectiveness of our approach in the future we would like to extend this work by looking at alternate audio modality tations including using neural networks for audio feature extraction and also explore the use of formers for an end to end attention based learning we also aim to explore the application of mast to com amankhullar mast other multimodal tasks like translation references dzmitry bahdanau kyunghyun cho and yoshua gio neural machine translation by jointly arxiv preprint learning to align and translate ozan caglayan mercedes garca martnez adrien bardet walid aransa fethi bougares and loc rault nmtpy a exible toolkit for advanced neural machine translation systems the prague bulletin of mathematical linguistics kyunghyun cho bart van merrienboer caglar cehre dzmitry bahdanau fethi bougares holger schwenk and yoshua bengio learning phrase representations using rnn encoder decoder for statistical machine translation arxiv preprint danilo croce giuseppe castellucci and roberto basili gan bert generative adversarial ing for robust text classication with a bunch of beled examples in proceedings of the annual meeting of the association for computational guistics pages michael denkowski and alon lavie meteor automatic metric for reliable optimization and uation of machine translation systems in ings of the sixth workshop on statistical machine translation pages jacob devlin ming wei chang kenton lee and kristina toutanova bert pre training of deep bidirectional transformers for language ing arxiv preprint yimai fang and simone teufel a summariser based on human memory limitations and lexical competition in proceedings of the conference of the european chapter of the association for putational linguistics pages orhan firat and kyunghyn cho gated ditional mechanism tutorial blob master docs cgru pdf recurrent attention unit with com nyu dl david graff junbo kong ke chen and kazuaki maeda english gigaword linguistic data consortium philadelphia kensho hara hirokatsu kataoka and yutaka satoh can spatiotemporal cnns retrace the in proceedings of tory of cnns and imagenet the ieee conference on computer vision and tern recognition pages kaiming he xiangyu zhang shaoqing ren and jian sun deep residual learning for image in proceedings of the ieee conference on nition computer vision and pattern recognition pages sepp hochreiter and jurgen schmidhuber neural computation long short term memory chin yew lin rouge a package for automatic in text summarization evaluation of summaries branches out pages yang liu and mirella lapata text in proceedings of tion with pretrained encoders the conference on empirical methods in ural language processing and the international joint conference on natural language processing emnlp ijcnlp pages abdel rahman mohamed deep neural network acoustic models for asr paul over hoa dang and donna harman duc in context information processing management shruti palaskar jindrich spandana gella and florian metze multimodal abstractive arxiv preprint summarization for videos vijayaditya peddinti guoguo chen vimal manohar tom ko daniel povey and sanjeev khudanpur jhu aspire system robust lvcsr with tdnns ivector adaptation and rnn lms in ieee shop on automatic speech recognition and standing asru pages ieee daniel povey arnab ghoshal gilles boulianne lukas burget ondrej glembek nagendra goel mirko hannemann petr motlicek yanmin qian petr schwarz et al the kaldi speech tion toolkit in ieee workshop on automatic speech recognition and understanding conf ieee signal processing society zhu jn zhang jj li hr zong cq al modal summarization with guidance of multimodal reference association for computational tics michal rott and petr cerva speech to text marization using automatic phrase extraction from recognized text in international conference on text speech and dialogue pages springer chloe hillier will kay joao carreira karen simonyan brian sudheendra zhang narasimhan fabio viola tim green trevor back paul natsev al the kinetics human action video dataset arxiv preprint diederik p kingma and jimmy ba adam a method for stochastic optimization arxiv preprint walter kintsch and teun a van dijk toward a model of text comprehension and production chological review haoran li junnan zhu cong ma jiajun zhang chengqing zong al multi modal rization for asynchronous collection of text image audio and video jindrich and jindrich helcl attention strategies for multi source sequence to sequence learning in proceedings of the annual ing of the association for computational linguistics volume short papers pages alexander m rush sumit chopra and jason weston a neural attention model for abstractive in proceedings of the tence summarization conference on empirical methods in natural guage processing pages shagan sah sourabh kulhare allison gray hashini venugopalan emily prudhommeaux and raymond ptucha semantic text in ieee winter tion of long videos ence on applications of computer vision wacv pages ieee ramon sanabria ozan caglayan shruti palaskar desmond elliott loc barrault lucia specia and florian metze a large scale dataset arxiv for multimodal preprint language understanding abigail see peter j liu and christopher d to the point summarization arxiv preprint ning get with pointer generator networks kristian woodsend and mirella lapata multiple aspect summarization using integer linear ming in proceedings of the joint conference on empirical methods in natural language ing and computational natural language learning pages association for computational guistics junnan zhu haoran li tianshang liu yu zhou ajun zhang chengqing zong al msmo multimodal summarization with multimodal output
