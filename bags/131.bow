published conference paper iclr maskgan better text generation filling william fedus ian goodfellow andrew dai google brain liam com goodfellow com abstract neural text generation models autoregressive language models models models generate text sampling words sequentially word conditioned previous word state art machine translation summarization benchmarks benchmarks dened validation perplexity direct measure quality generated text additionally models typically trained mum likelihood teacher forcing methods suited optimizing perplexity result poor sample quality generating text requires tioning sequences words observed training time propose improve sample quality generative adversarial networks gans explicitly train generator produce high quality samples shown lot success image generation gans originally designed output differentiable values discrete language generation challenging claim validation perplexity indicative quality text generated model introduce actor critic conditional gan lls missing text conditioned surrounding context tively quantitatively evidence produces realistic conditional unconditional text samples compared maximum likelihood trained model introduction recurrent neural networks rnns graves common generative model sequences sequence labeling tasks shown impressive results language modeling mikolov machine translation text classication miyato text typically generated models sampling distribution conditioned previous word hidden state consists representation words generated far typically trained maximum likelihood approach known teacher forcing ground truth words fed model conditioned generating following parts sentence causes problems sample generation model forced condition sequences conditioned training time leads unpredictable dynamics hidden state rnn methods professor forcing lamb scheduled sampling bengio proposed solve issue approaches work indirectly causing hidden state dynamics predictable professor forcing randomly conditioning sampled words training time directly specify cost function output rnn encourages high sample quality proposed method generative adversarial networks gans goodfellow framework training generative models adversarial setup generator generating images trying fool discriminator trained discriminate real synthetic images gans lot success producing realistic images approaches seen limited use text sequences discrete nature text making infeasible propagate gradient discriminator generator standard gan training overcome reinforcement learning train generator discriminator trained maximum likelihood stochastic gradient descent gans commonly suffer issues published conference paper iclr training instability mode dropping exacerbated textual setting mode dropping occurs certain modalities training set rarely generated generator example leading generated images volcano multiple variants volcano signicant problem text generation complex modes data ranging bigrams short phrases longer idioms training stability issue unlike image generation text generated autoregressively loss discriminator observed complete sentence generated problem compounds generating longer longer sentences reduce impact problems training model text blank task similar task proposed bowman use robust setup task portions body text deleted redacted goal model inll missing portions text indistinguishable original data text model operates autoregressively tokens far lled standard language modeling conditioning true known context entire body text redacted reduces language modeling designing error attribution time step noted important prior natural language gan research text inlling task naturally achieves consideration discriminator evaluate token provide grained supervision signal generator consider instance generator produces sequence perfectly matching data distribution rst time steps produces outlier token despite entire sequence clearly synthetic result errant token discriminative model produces high loss signal outlier token likely yield informative error signal generator research opens inquiry conditional gan models context natural language following sections introduce text generation model trained maskgan consider actor critic architecture extremely large action spaces consider new evaluation metrics generation synthetic training data related work research reliably extending gan training discrete spaces discrete sequences highly active area gan training continuous setting allows fully differentiable computations permitting gradients passed discriminator generator discrete elements break differentiability leading researchers avoid issue reformulate problem work continuous domain consider methods seqgan trains language model policy gradients train generator fool cnn based discriminator discriminates real synthetic text generator discriminator pretrained real fake data phase training policy gradients training monte carlo rollouts order useful loss signal word follow work demonstrated text generation pretraining rnns press additionally zhang produced results rnn generator matching dimensional latent representations professor forcing lamb alternative training rnn teacher forcing discriminator discriminate hidden states generator rnn conditioned real synthetic samples discriminator operates hidden states gradients passed generator hidden state dynamics inference time follow training time gans applied dialogue generation showing improvements adversarial evaluation good results human evaluation compared maximum likelihood trained baseline method applies reinforce monte carlo sampling generator published conference paper iclr replacing non differentiable sampling operations efcient gradient approximators jang shown strong results discrete gans recent unbiased low variance gradient estimate techniques tucker prove effective wgan gulrajani avoids issue dealing backpropagating discrete nodes generating text shot manner convolutional network hjelm proposes algorithmic solution uses boundary seeking gan objective importance sampling generate text rajeswar discriminator operates directly continuous probabilistic output generator accomplish recast traditional autoregressive sampling text inputs rnn predetermined che instead optimize lower variance objective discriminator output standard gan objective reinforcement learning methods explored successfully natural language reinforce cross entropy hybrid mixer ranzato directly optimized bleu score demonstrated improvements baselines recently actor critic methods natural language explored bahdanau instead having rewards supplied discriminator adversarial setting rewards task specic scores bleu conditional text generation gan training explored rajeswar work distinct employ actor critic training procedure task designed provide rewards time step believe mitigate problem severe mode collapse task harder discriminator reduces risk generator contending near perfect discriminator critic method helps generator converge rapidly reducing high variance gradient updates extremely high action space environment operating word level natural language maskgan notation architecture let denote pairs input target tokens let denote masked token original token replaced hidden token let denote lled token finally lled token passed discriminator real fake task imputing missing tokens requires maskgan architecture condition information past future choose use sutskever architecture generator consists encoding module decoding module discrete sequence binary mask generated deterministically stochastically length selects tokens remain token time replaced special mask token mask remains unchanged mask encoder reads masked sequence denote mask applied element wise encoder provides access future context maskgan decoding standard language modeling decoder lls missing tokens auto regressively conditioned masked text lled point generator decomposes distribution sequence ordered conditional sequence discriminator identical architecture output scalar probability time point distribution vocabulary size nator given lled sequence generator importantly given original tried cnn based discriminators found lstms performed best published conference paper iclr figure generator architecture blue boxes represent known tokens purple boxes imputed tokens demonstrate sampling operation dotted line encoder reads masked sequence masked tokens denoted underscore decoder imputes missing tokens encoder hidden states example generator alphabetical ordering real context discriminator true context algorithm critical failure mode instance context discriminator given lled sequence director director guided series fail reliably identify director director bigram fake text despite bigram potentially appearing training corpus aside errant typo reason ambiguous currences director fake associate director guided series director expertly guided series potentially valid sequences context words real discriminator found assign equal probability words result course inaccurate learning signal generator correctly penalized producing bigrams prevent discriminator computes probability token real given true context masked sequence xreal formulation logarithm discriminator estimates regarded reward log network critic network implemented additional head inator critic estimates value function discounted total return lled sequence srs discount factor position sequence training model fully differentiable sampling operations generator probability distribution produce token train generator estimate gradient respect parameters policy gradients sutton reinforcement learning rst employed gans language modeling analogously generator seeks maximize cumulative total reward optimize parameters generator performing gradient ascent reinforce family algorithms unbiased estimator log variance gradient estimator reduced learned value function baseline produced critic results generator gradient contribution single token log nomenclature quantity interpreted estimate advantage action token chosen generator source code available com tensorflow models master research maskgan published conference paper iclr state current tokens produced point approach actor critic architecture determines policy baseline critic sutton barto degris task design rewards time step single sequence order aid credit assignment result token generated time step inuence rewards received time step subsequent time steps gradient generator include contributions token lled order maximize discounted total return generator gradient given equation extg extg srs intuitively shows gradient generator associated producing depend discounted future rewards assigned discriminator non zero discount factor generator penalized greedily selecting token earns high reward time step sequence sum generated words finally conventional gan training discriminator updated according gradient alternative approaches long sequences large vocabularies aside avenues explored highlight particular problems task plausible remedies task difcult long sequences large vocabularies address issue extended sequence length modify core algorithm dynamic task apply algorithm maximum sequence length satisfying convergence criterion increment maximum sequence length continue training allows model build ability capture dependencies shorter sequences moving longer dependencies form curriculum learning order alleviate issues variance reinforce methods large vocabulary size consider simple modication time step instead generating reward sampled token instead seek use information generator distribution sampling generator produces probability distribution tokens compute reward possible token conditioned generated incurs computational penalty discriminator predict tokens performed efciently potential reduction variance benecial method details prior training rst perform pretraining train language model standard maximum likelihood training use pretrained language model weights encoder decoder modules language models pretrain model task maximum likelihood particular attention parameters described luong select model producing lowest validation perplexity masked task hyperparameter sweep runs initial algorithms include critic found inclusion critic decreased variance gradient estimates order magnitude substantially improved training published conference paper iclr evaluation evaluation generative models continues open ended research question seek heuristic metrics believe correlated human evaluation bleu score papineni extensively machine translation compare quality candidate translations reference motivated metric compute number unique grams produced generator occur validation corpus small compute geometric average metrics unied view performance generator maximum likelihood trained benchmark able gan hyperparameter congurations led small decreases validation perplexity found models yield considerable improvements sample quality abandoned trying reduce validation perplexity biggest advantages gan trained nlp models generator produce alternative realistic language samples unfairly penalized producing high likelihood single correct sequence generator explores manifold free running mode alternative options valid maximize probability underlying sequence choose focus architectures hyperparameter congurations led small reductions validation perplexity searched improved heuristic evaluation metrics experiments present conditional unconditional samples generated ptb imdb data sets word level maskgan refers gan trained variant maskmle refers likelihood trained variant additional samples supplied appendix penn treebank ptb penn treebank dataset marcus vocabulary unique words training set contains words validation set contains words test set contains words experiments train training partition rst pretrain commonly variational lstm language model parameter dimensions common maskgan following gal ghahramani validation perplexity loading weights language model maskgan generator pretrain masking rate half text blanked validation perplexity finally pretrain discriminator samples produced current generator real training text conditional samples produce samples conditioned surrounding text table underlined sections text missing lled maskgan maskmle algorithm ground truth day interactive telephone technology taken new leap unk television programmers maskgan day interactive telephone technology taken new leap retail business eos day interactive telephone technology taken new leap complicate case table conditional samples ptb maskgan maskmle models published conference paper iclr language model unconditional samples run maskgan unconditional mode entire context blanked making equivalent language model present language model sample table additional samples included appendix oct end year resignations approved eos march unk table language model unconditional sample ptb maskgan imdb movie dataset imdb dataset maas consists movie reviews taken imdb review contain sentences dataset divided labeled training instances labeled test instances unlabeled training instances label indicates sentiment review positive negative use rst words review training set train models leads dataset million words identical training process ptb pretrain language model validation perplexity loading weights language model maskgan generator pretrain masking rate half text blanked validation perplexity finally pretrain discriminator samples produced current generator real training text conditional samples compare maskgan maskmle conditional language generation ability imdb dataset ground truth pitch black complete shock rst saw previous years maskgan pitch black complete shock rst saw looking forward black complete shock rst saw live new zealand table conditional samples imdb maskgan maskmle models language model unconditional samples case ptb generate imdb samples unconditionally equivalent language model present sample table additional samples included appendix maskgan positive follow good earth movie linked vacation comedy credited modern day era yarns helpful modern day best interesting drama based story famed table language model unconditional sample imdb maskgan perplexity generated samples date gan training achieved state art word level validation perplexity penn treebank dataset performing models maximum likelihood trained published conference paper iclr model perplexity imdb samples pretrained maskgan table perplexity calculated pre trained language model equivalent decoder terms architecture size maskmle maskgan models language model initialize models models recent architectures found neural architecture search zoph extensive hyperparameter search maskgan supported gan training improve validation perplexity results set state art models instead seek understand quality sample generation highlighted earlier fundamental problem generating free running mode potentially leads manifold sequences result poor sample quality teacher forced models seek quantitatively evaluate dynamic present sampling commonly bleu shown bleu necessarily correlated sample quality believe correlation task potential valid bleu penalize valid ones instead calculate perplexity generated samples maskgan maskmle language model initialize maskgan maskmle maskgan produce samples autoregressively free running mode building previously sampled tokens produce distribution maskgan model produces samples likely initial model maskmle model maskmle model generates improbable sentences assessed initial language model inference compounding sampling errors result recurrent hidden states seen teacher forcing lamb conversely maskgan model operates free running mode training supports robust sampling perturbations mode collapse contrast image generation mode collapse measured directly calculating certain gram statistics instance measure mode collapse percentage unique grams set generated imdb movie reviews unconditionally generate sample consisting words results total tri quad grams model unique bigrams unique trigrams unique quadgrams maskgan table diversity statistics unconditional samples ptb news snippets words results table maskgan mode collapse evidenced reduced number unique quadgrams complete samples taken sequence models unique observed training initial small drop perplexity ground truth validation set steady increase perplexity training progressed despite sample quality remained relatively consistent nal samples generated model perplexity ground truth hypothesize mode dropping occurring near tail end sequences generated samples unlikely generate previous words correctly order properly model distribution words tail theis shows validation perplexity necessarily correlate sample quality published conference paper iclr human evaluation ultimately evaluation generative models best measured unbiased human evaluation evaluate quality generated samples initial language model maskmle model maskgan model blind heads comparison amazon mechanical turk note models number parameters inference time pay raters compare quality extracts axes grammaticality topicality overall quality asked rst extract second extract higher quality preferred model grammaticality topicality overall table mechanical turk blind heads evaluation pairs models trained imdb reviews reviews words long model unconditionally sampled randomized raters asked sample preferred pair ratings obtained model pair comparison preferred model grammaticality topicality overall maskgan maskgan real samples real samples maskgan maskgan maskgan seqgan seqgan maskgan table mechanical turk blind heads evaluation pairs models trained ptb news snippets words long model unconditionally sampled randomized raters asked sample preferred pair ratings obtained model pair comparison mechanical turk results maskgan generates superior human looking samples maskmle imdb dataset smaller ptb dataset word instead word samples results closer results seqgan trained network size vocabulary size maskgan maskgan produces superior samples seqgan published conference paper iclr discussion work supports case matching training inference procedures order produce higher quality language samples maskgan algorithm directly achieves gan training improved generated samples assessed human evaluators experiments generally found training contiguous blocks words masked produced better samples conjecture allows generator opportunity explore longer sequences free running mode comparison random mask generally shorter sequences blanks gain gan training substantial found policy gradient methods effective conjunction learned critic highly active research training discrete nodes present stable training procedures found use attention important words sufciently conditioned input context attention reasonable subsequences implausible context adjacent surrounding words given suspect promising avenue consider gan training attention models vaswani general think proposed contiguous task good approach reduce mode collapse help training stability textual gans maskgan samples larger dataset imdb reviews signicantly better corresponding tuned maskmle model shown human evaluation produce high quality samples despite maskgan model having higher perplexity ground truth test set acknowledgements like thank george tucker jascha sohl dickstein jon shlens ryan sepassi jasmine collins irwan bello barret zoph gabe pereyra eric jang google brain team particularly rst year residents humored listening commenting conceivable variation core idea published conference paper iclr references dzmitry bahdanau philemon brakel kelvin anirudh goyal ryan lowe joelle pineau aaron courville yoshua bengio actor critic algorithm sequence prediction international conference learning representations samy bengio oriol vinyals navdeep jaitly noam shazeer scheduled sampling sequence prediction recurrent neural networks advances neural information processing systems yoshua bengio rejean ducharme pascal vincent christian jauvin neural probabilistic language model journal machine learning research samuel bowman luke vilnis oriol vinyals andrew dai rafal jozefowicz samy bengio generating sentences continuous space signll conference computational natural language learning conll tong che yanran ruixiang zhang devon hjelm wenjie yangqiu song yoshua bengio maximum likelihood augmented discrete generative adversarial networks arxiv preprint thomas degris patrick pilarski richard sutton model free reinforcement learning continuous action practice american control conference acc ieee yarin gal zoubin ghahramani theoretically grounded application dropout recurrent neural networks advances neural information processing systems ian goodfellow jean pouget abadie mehdi mirza bing david warde farley sherjil ozair aaron courville yoshua bengio generative adversarial nets advances neural tion processing systems alex graves supervised sequence labelling recurrent neural networks volume springer ishaan gulrajani faruk ahmed martin arjovsky vincent dumoulin aaron courville improved training wasserstein gans arxiv preprint devon hjelm athul paul jacob tong che kyunghyun cho yoshua bengio seeking generative adversarial networks arxiv preprint hakan inan khashayar khosravi richard socher tying word vectors word classiers loss framework language modeling international conference learning representations eric jang shixiang ben poole categorical reparameterization gumbel softmax international conference learning representations diederik kingma jimmy adam method stochastic optimization international conference learning representations alex lamb anirudh goyal ying zhang saizheng zhang aaron courville yoshua bengio advances neural professor forcing new algorithm training recurrent networks information processing systems jiwei monroe tianlin shi alan ritter dan jurafsky adversarial learning neural dialogue generation conference empirical methods natural language processing minh thang luong hieu pham christopher manning effective approaches attention based neural machine translation conference empirical methods natural language processing published conference paper iclr andrew maas raymond daly peter pham dan huang andrew christopher potts learning word vectors sentiment analysis proceedings annual meeting association computational linguistics human language technologies volume association computational linguistics mitchell marcus mary ann marcinkiewicz beatrice santorini building large annotated corpus english penn treebank computational linguistics tomas mikolov martin karaat lukas burget jan sanjeev khudanpur recurrent neural network based language model interspeech volume takeru miyato andrew dai ian goodfellow virtual adversarial training semi supervised text classication international conference learning representations volume kishore papineni salim roukos todd ward wei jing zhu bleu method automatic evaluation machine translation proceedings annual meeting association computational linguistics association computational linguistics press lior wolf output embedding improve language models conference european chapter association computational linguistics press amir bar ben bogin jonathan berant lior wolf language generation recurrent generative adversarial networks pre training arxiv preprint sai rajeswar sandeep subramanian francis dutil christopher pal aaron courville adversarial generation natural language workshop representation learning nlp marcaurelio ranzato sumit chopra michael auli wojciech zaremba sequence level training recurrent neural networks arxiv preprint ilya sutskever oriol vinyals quoc sequence sequence learning neural networks advances neural information processing systems richard sutton andrew barto reinforcement learning introduction volume mit press cambridge richard sutton david mcallester satinder singh yishay mansour policy gradient ods reinforcement learning function approximation advances neural information processing systems lucas theis aaron van den oord matthias bethge note evaluation generative models international conference learning representations george tucker andriy mnih chris maddison dieterich lawson jascha sohl dickstein rebar low variance unbiased gradient estimates discrete latent variable models conference neural information processing systems nips ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan gomez lukasz kaiser illia polosukhin attention need conference neural information processing systems nips yonghui mike schuster zhifeng chen quoc mohammad norouzi wolfgang macherey maxim krikun yuan cao qin gao klaus macherey jeff klingner apurva shah melvin johnson xiaobing liu lukasz kaiser stephan gouws yoshikiyo kato taku kudo hideto kazawa keith stevens george kurian nishant patil wei wang cliff young jason smith jason riesa alex rudnick oriol vinyals greg corrado macduff hughes jeffrey dean google neural machine translation system bridging gap human machine translation corr url org lantao weinan zhang jun wang yong seqgan sequence generative adversarial nets policy gradient association advancement articial intelligence published conference paper iclr yizhe zhang zhe gan kai fan zhi chen ricardo henao dinghan shen lawrence carin adversarial feature matching text generation arxiv preprint barret zoph quoc neural architecture search reinforcement learning international conference learning representations published conference paper iclr training details model trained adam method stochastic optimization kingma default tensorow exponential decay rates model uses layers unit lstms generator discriminator dimensional word embeddings variational dropout bayesian hyperparameter tuning tune variational dropout rate learning rates generator discriminator critic perform gradient descent steps discriminator step generator critic share embedding softmax weights generator proposed bengio press wolf inan furthermore improve convergence speed share embeddings generator discriminator additionally noted architectural section critic shares discriminator parameters exception separate output head estimate value generator discriminator use variational recurrent dropout gal ghahramani additional samples penn treebank ptb present additional samples ptb conditional samples ground truth day interactive telephone technology taken new leap unk television programmers maskgan day interactive telephone technology taken new leap retail business eos day interactive telephone technology long dominated unk nation largest economic day interactive telephone technology exercised stake france day interactive telephone technology taken new leap complicate case day interactive telephone technology unk number clients estimates mountain bike day interactive telephone technology instituted week unk unk unk wis auto consider lling non continguous masks ground truth president united states ronald reagan delivered unk address nation president reagan addressed sues maskgan president united states congress delivered unk address nation reagan addressed issues president united states delivered unk address nation eos reagan addressed issues language model unconditional samples present additional language model unconditional samples ptb modied seqgan train generate ptb samples size architecture generator maskgan generator present samples maskgan samples published conference paper iclr maskgan seqgan unk basis despite huge tax interest income unk million eos west germany world corrupt organizations act multibillion dollar unk atmosphere metropolitan zone historic array removed eos takeover target lin directors attempted october unk british airways allowed funds cineplex odeon corp shares fresh group purchase revised class unk british unk unk unk unk seed eos use pcs unk performance imdb movie dataset present additional samples imdb conditional samples ground truth pitch black complete shock rst saw previous years maskgan pitch black complete shock rst saw looking forward pitch black complete shock rst saw promos pitch black complete shock rst saw days black complete shock rst saw live new zealand pitch black complete shock rst saw funny interiors pitch black complete shock rst saw day language model unconditional samples present additional language model unconditional samples maskgan imdb positive follow good earth movie linked vacation comedy credited modern day era yarns helpful modern day best interesting drama based story famed negative understand movie falls like seeing sorry reason watched casting emperor expecting negative time time persevered cast good way didn realize book story manhattan allies failure modes explore failure modes maskgan model certain bad hyperparameter settings published conference paper iclr mode collapse widely witnessed gan training common failure mode collapse gram levels mode collapse extreme collapse gram level ddddddd described gulrajani manifest grammatical albeit inanely repetitive phrases example funny funny funny movie charming course discriminator discern distribution sample certain failure modes observed generator common modes frequently present text matching syntax boundaries notice maskgan architecture struggles produce syntactically correct sequences hard boundary end relatively challenging task humans lled text contextual match syntactically boundary blank text present xed number words cartoon lms rst saw noted failure mode intersection lled text present text non grammatical loss global context similar failure modes present gan image generation produced samples lose global coherence despite sensible locally expect larger capacity model mitigate issues movie terrible plot ludicrous title interesting original great movie lord rings great movie john travolta brilliant published conference paper iclr gram metrics misleading proxies absence global scalar objective optimize training monitor gram language statistics assess performance crude proxies quality produced samples gram perplexity figure particular failure mode succeeding optimization gram metric extreme expense validation perplexity resulting samples shown instance maskgan models led improvements particular gram metric extreme expense validation perplexity seen figure devolve generator low sample diversity produce samples particular model despite dramatically improved gram metric lost diversity great movie tragic story man working home great great premise funny silly best movie seen series story simple clever capturing complexities natural language metrics clearly insufcient
