c e d l c s c v v i x r a generating summaries tailored to target characteristics kushal chawla hrituraj singh arijit pramanik mithlesh kumar balaji vasan srinivasan adobe research bangalore india indian institute of technology roorkee indian institute of technology bombay indian institute of technology kanpur com com abstract recently research eorts have gained pace to cater to varied user preferences while generating text summaries while there have been attempts to incorporate a few handpicked characteristics such as length or entities a holistic view around these preferences is missing and crucial insights on why certain characteristics should be incorporated in a cic manner are absent with this objective we provide a categorization around these characteristics relevant to the task of text summarization one focusing on what content needs to be generated and second focusing on the stylistic aspects of the output summaries we use our insights to provide guidelines on appropriate methods to incorporate various classes characteristics in sequence to sequence summarization framework our experiments with incorporating topics readability and simplicity cate the viability of the proposed prescriptions introduction automatic text summarization is the task of generating a summary of an input document while retaining the key aspects such a summary helps in senting the important content from a long input text in a succinct form for quick information consumption traditional methods for summarization tract key sentences from the source text to construct an extractive summary recent eorts towards abstractive summarization have geared towards ating human like paraphrased summaries from the input article while these algorithms allow for the generation of a single summary it is often desirable to generate summary variants tailored towards specic teristics for instance readers may be interested in summaries of dierent lengths or might want to focus on specic entities topics from the input text pending on dierent age groups of the readers they might prefer formal informal variants of the summary irrespective of the application context it has been shown that incorporating these characteristics at the time of generation can yield more contextual summaries recent works have proposed dierent ways to incorporate target teristics at the time of summary generation introducing modications to the architecture learning objectives or the decoder probabilities however all these attempts handpick a few characteristics and propose ways to incorporate them in the absence of appropriate insights it is unclear as to why these methodologies work in tuning the summaries towards the chosen acteristics and why the same can not be extended for other characteristics in this work our objective is to gain a holistic understanding around these additional constraints centering on the task of text summarization taking a step in this direction we propose a categorization of these acteristics into content specic which primarily focus on what content is presented pivoting on the semantics or information presented in the output mary and style specic focusing on the stylistic expressions pivoting on the linguistic presentation in the output summary through a comparative tion of various existing and proposed methods we further prescribe guidelines to help choosing the right framework for tailoring these categories of characteristics our primary contribution is providing a categorization of target characteristics as content and style specic towards a holistic understanding of tailored mary generation additionally we propose an attention boosting approach to improve tailoring of content specic characteristics and a policy gradient based algorithm to incorporate stylistic characteristics in summaries related work traditional methods for summarization extract key sentences from the source text to construct an extractive summary features like descriptiveness of words and word frequencies have been explored to choose the summary sentences ever humans summarize by understanding the content and paraphrasing the understood content into a summary extractive summarization is hence unable to produce human like summaries this has led to eorts towards abstractive summarization which paraphrase summaries from input article content early attempts at abstractive summarization created summary sentences ther based on templates or used ilp based sentence compression niques with the advent of deep sequence to sequence models attention based neural models have been proposed for summarizing long tences these approaches were further improved by incorporating stract meaning representations and using hierarchical encoding networks more recent approaches have focused on large scale datasets for rization such as cnn dailymail corpus gulcehre et al introduced the ability to copy out of vocabulary words from the article to incorporate rarely seen words like names in the generated text tu et al included the concept of coverage to prevent the models from repeating the same phrases while erating a sentence see et al proposed a pointer generator framework which incorporates these improvements and also learns to switch between generating new words and copying words from the source article although research has primarily focused on unconditioned abstractive text summarization there have been some recent eorts to incorporate a variety of additional constraints into the generation algorithm fan et al use explicit indicators to control the length of the output summary imposing a constraint on how detailed the output needs to be they extend the same technique to also control the entities which must be focused on while generating the summary krishna et al generated topic oriented summaries by using an indicator topic vector along with the input representation each of these approaches aim to control the information presented in the generated summary and therefore modies the attention distribution to focus on appropriate parts of the text as dictated by the target characteristics we group these approaches as specic characteristic tuning another direction of eorts have attempted to incorporate aspects like ments or tense using generative models like variational auto encoders or ing adversarial training focusing on politeness sennrich et al propose modications to neural machine translation setup to generate polite variants propose the use of a conditional language model to generate ficler et al text with variations such as descriptiveness personal and sentiment ously more recently generating text with varying levels of formality was studied in machine translation oraby et al attempt to control personality dimensions in generation namely agreeable disagreeable conscientious scientious and extravert by using indicator tokens and stylistic encodings ishna et al modify the decoding algorithm to produce readable and simple summaries each of these exploration focus primarily of tuning the linguistic presentation of the generated text and we group these as style specic teristic tuning policy learning based approaches have shown promise to control several qualitative characteristics explicitly and can potentially be used for both content and style specic characteristics while it has been successfully deployed for metrics like rouge we show its applicability to style specic characteristics by proposing a policy gradient framework for readability and simplicity content vs style the notion of style and the associated nomenclature is quite convoluted in the literature approaches in style transfer try to obtain independent latent representations for style and semantics of the content using this interpretation we see the output of a text generation system as a combination of the semantics of the information which is being presented and the style associated with that content however unlike these approaches that learn the notion of style implicitly from the available corpora we fragment style across a set of dimensions such as readability simplicity these are referred to as various aspects of style aligning with ad approaches towards style transfer as described in pointer generator framework we base all our explorations on the pointer generator network however all our ndings and insights are generic and can be extended to any other works without loss of generality we describe the pointer generator framework here for the sake of completion and please refer to for more details on the framework the pointer generator network consists of an encoder and a coder both based on lstm architecture given an input article the encoder takes the embedding vectors of each word in the source text a and computes the encoder hidden states hn the nal hidden state is passed to a coder which computes a hidden state st at each decoding step and calculates an attention distribution at over all words as at where vt wsst batt et t n where v wh ws and batt are model parameters to be trained the attention is a probability distribution over words in the source text which aids the decoder in generating the next word in the summary using words in the source text with higher attention the context vector h at ihi is a weighted sum of the i the attention on the ith input word at encoder hidden states weighted on at tth step and is used to determine the next word to be generated the attention distribution allows the network to focus on specic parts of the input as the output summary is generated to tailor a summary to various content specic characteristics it is important to modify this attention distribution to focus on the appropriate parts of the input text as required by the characteristics tuned at each decoding step the decoder also gets the last word yt in the summary generated so far and computes a scalar pgen denoting the probability of ating a new word from the vocabulary pgen y yt bgen where wh ws wy bgen are trained vectors the network probabilistically decides based on pgen whether to generate a new word from the vocabulary or copy a word from the source text using the attention distribution for each word w in the vocabulary the model calculates the probability of the word getting generated next for each word in the input article its total attention received yields its probability of being copied since some words occur in the vocabulary and also the input article they will have non zero probabilities of being newly generated as well as being copied hence the total probability of w being the next word generated in the by p is given by s st wt t wt h h pgen at i i wi w the second term allows the framework to choose a word to copy from the put text using the attention distribution the pointer generator network further employs a coverage mechanism to encourage diversity in attention distributions over time steps the training loss is set to be the average negative log likelihood of the ground truth summaries the model is trained using back propagation and the adagrad gradient descent algorithm since the stylistic characteristics deal with specic expressions of the output text it can be tailored by modifying to incorporate the corresponding stylistic preferences for more complex characteristics as we show it is possible to dene a reinforcement learning based loss appended to the training loss to tailor the specic characteristics content specic characteristics content specic characteristics primarily govern what content needs to be sented in the output summary for the rest of the paper we illustrate the needs and modeling for content based characteristics with topical tailoring however the proposed approach can be extended to other content characteristics like entity centric tailoring often the whole content of the article may not be relevant to the readers and may prefer specic elements of the input to be summarized for instance a sports enthusiast may only be interested in content concerning that domain or a surgeon may only be interested in health related content this calls for a need to generate multiple summary variants taking this information into account table shows a particular instance from our dataset which talks about both politics and military if a reader is interested only in politics the baseline summary generated by pointer generator pgen model does not refer to politics and hence fails to meet the needs article bernie sanders my vermont senator and indeed a friend of many years is now running for president he noted at his announcement with a familiar note of wise irony people should not underestimate me to most americans of course sen bernie sanders is only a name if that he is barely known to the general public which makes him a very long shot indeed to win election to the highest oce in the nation cnn he was impressively polite and bright in the eyes of his boyhood teachers an encourager of his college friends he was a docile captured killer in the care of paramedics tending to his gunshot wounds dzhokhar jahar tsarnaev s defense team is seeking to spare him from a death sentence for his part two years ago in the boston marathon bombings and murder of an mit police ocer pgen dzhokhar jahar tsarnaev s defense team is seeking to spare him from a death sentence he was a docile captured killer in the care of paramedics tending to his gunshot wounds tsarnaev was convicted april on all counts including that carry a possible death penalty token based mixed attention boosting politics sen bernie sanders is running for president he is barely known to the general public which makes him a very long shot indeed he is barely known to the general public which makes him a very long shot indeed token based mixed attention boosting military dzhokhar jahar tsarnaev s defense team is seeking to spare him from a death sentence he was convicted april on all counts including that carry a possible death penalty a paramedic testied wednesday that it was common for patients in shock to become agitated table sample output topic tailored summaries generated by token based proach trained on cnn dm mixed dataset we show just the top few sentences in the input article in the interest of space sequence to sequence learning models have been shown to understand where to look in the input through attention mechanisms which is then used for output generation tailoring content specic characteristics would require this attention to be tuned to focus on the relevant parts of the input e the relevant parts of the input talking about a topic of interest to generate the desired output this requires the model to be taught either explicitly or implicitly where to attend in the input article to tailor the summary appropriately one possibility is to maintain explicit indicators for each category of the characteristic e for each topic allowing the model to learn where to pay more attention directly from the data fan et al propose to use such dicator tokens to tune characteristics such as length or desired entities when training on an article summary pair belonging to a particular bin a token indicating the characteristic topic in this case represented by the summary is added to the beginning of the input article while decoding an unseen article the framework can generate multiple summary variants based on what token is prepended to the input word sequence internally the model uses the token to learn a conditioned space of parameters ensuring appropriate attention tuning to generate the summary with corresponding tailoring we refer to this approach as token based in our experiments a key requirement to make the model learn these intricacies is that the training data should contain sucient samples under each category however there could be a skew in the dataset which calls for alternate approach to tackle these characteristics to deal with this problem krishna et al create a separate dataset where the model sees multiple summary variants for the same input article by mixing multi topic articles given such a dataset the vocabulary tokens can be used to guide the learning process towards a topic specic attention with a skewed dataset this method is called as token based mixed which is trained on such an interspersed dataset while and use token based approaches implicitly teach the networks by taking advantage of the diversity in the training data we propose an alternative to explicitly boost the attention distributions referred to as attn boost restricting the model to focus on some parts of the input more than others more formally we modify eq from the pointer generator as i ivt wsst batt where v wh ws and batt are trainable model parameters as before and use this ei to compute the attention at i is a word specic attention boosting parameter this explicitly teaches the model to pay more attention towards specic words than others we leverage topic specic word lists curated by and select the top in our experiments sentences from the input article which are most related to the target topic we explicitly boost is of all the words in these sentences using the topic condence measures as used by to draw more insights on each of these approaches we evaluate it on the task of topic based summarization on the cnn dailymail cnn dm dataset the dataset consists of training validation and test instances the articles have an average length of tokens and sentence summaries with average length of tokens we use the vanilla pointer generator pgen as the baseline for all our experiments retaining the we train the token based model for topics parameters by see et al by categorizing the ground truth summaries into topics business education entertainment health military politics social sports and technology extending the setup by and prepending the topic to the input article while training following we also have a setup where we intersperse articles from dierent topics in cnn dm resulting in multiple topic specic ground truth summaries for the same article using the topics as before the model now sees multiple summaries for the same article there are article summary topic tuples for training tuples for validation and tuples in the test dataset to evaluate the generation quality of all the approaches we compare them on rouge and l score note that generating summaries for all the topics may not make sense for the same input article particularly when the article does not talk about the target topic hence for each article in the test set we generate the summary corresponding to the target topic dened by the ground truth summary then we use the topic specic word lists to get the and topics in the decoded summaries the fraction of times the target topic lies in and topics of the decoded summaries denes the and accuracies of the various setups method rouge f score accuracy l pgen token based attn boost proposed token based attn boost token based mixed attn boost table performance of proposed methodologies for generating topic tuned summaries on the cnn dm mixed test dataset table summarizes the results of our methods for generating topic oriented summaries we observe that boosting attention values explicitly shows ment in topic percentage accuracies but it suers a decline in quality based on rouge scores this is expected since the explicit topic attention would make the model attend to parts of documents that are dierent from the ground truth summary on the other hand token based approach improves on rouge with a lesser topical accuracy a combined framework of token based and attention boosting yields the best performance across both rouge and topical accuracy metrics this is perhaps because in the combined setup the model learns more intricacies implicit in the data along with explicit attention to the topics thus getting the best of both frameworks when we train the same setup with the mixed dataset by both rouge and the topical accuracies improve ing the importance of the diversity in data for the network to implicitly learn attention patterns we show generated summaries from the token based approach in table for a particular instance from the testing dataset the article was created by bining two articles from politics and military domain the proposed approach appended with token based framework on cnn dm mixed dataset is able to generate topic specic variants while the pgen approach fails to meet the quirement towards both the topics figure shows the average attention on the most attended parts of the input for the same instance by the token based model trained on cnn dm mixed when generating a politics oriented summary the attention is on words like president bernie sanders and general public showing the bias towards political phrases on the other hand when the target topic is military the focus attention shifts to death sentence and defence team politics military fig attention distribution over the source article for dierent target ics our evaluations show that tuning content based characteristics can be achieved by modifying the attention of the network either implicitly or explicitly plicit attention modication uses a token based approach but relies on the diversity of the characteristics in the data where not available feeding o of an interspersed dataset to articially infuse diversity is benecial by bining an interspersed dataset with explicitly attention boosting framework the model is able to tune the characteristics better by learning where to attend and where not to attend while tuning the content based characteristics stylistic characteristics next we focus on incorporating stylistic characteristics in the generated mary style specic preferences ensure that the content is served appropriately to the target audience in this direction prior work has focused on incorporating dimensions such as sentiment descriptiveness formality and many more across various tasks in text generation we describe our methodology below to incorporate such characteristics into abstractive text summarization the way these stylistic aspects are incorporated into the sequence to sequence summarization framework depends on how these aspects are dened for ample simplicity of text can be dened at a lexical level based on the frequency in a simple corpus as dened in krishna et al extend this towards generating simple summary by modifying the decoder probability in eq they incorporate simplicity by modifying the beam search decoder to choose contextual replacement with words that are simpler dening simplicity as m m where is the frequency of the ith word in the subtlex ishna et al then use a word to word replacement probabilities to achieve the tailoring however not all aspects can be dened at a lexical level and hence it is not always straightforward to modify the decoder probability for example ability can be quantied via the flesch reading ease score given by total words total sentences total syllables total words the flesch reading ease score quanties the diculty in understanding a passage written in english higher scores indicate easier to read passages it posits that the readability is inversely related to the average number of words in a sentence and to the average number of syllables in a word using a partial form of this denition propose to use shorter words lesser syllables as a surrogate to reading ease and use it to modify decoder probabilities however ignoring the rst component of the reading ease makes this an incomplete tailoring accounting for the rst component would require the whole sentence to be generated before providing any feedback to the model on its generated ity we propose to use a reinforcement learning based framework to incorporate such complex objectives requiring the generation of the complete partial put for feedback generation recently reinforcement learning frameworks have been successfully used to optimize text generation for content based metrics e rouge scores for summarization in cider scores for image captioning in we extend these to propose a reinforcement loss for stylistic elements as an additional term along with cross entropy using the self critical sequence training scst algorithm given an input article word sequence and a corresponding ground truth t the pointer generator framework optimizes the summary y y y negative log likelihood objective function is given by y lnll t y t for providing explicit feedback on the stylistic characteristics two output quences are generated at the time of training sampled sequence ys and baseline sequence yb we generate ys by sampling from the bution at each time step and by greedily choosing the word with maximum probability from the output distribution at each time step the scst algorithm denes a loss term lnll with a reward for the target style characteristics ys ys lrl ys t where is the reward function based on the target style characteristic to be optimized optimizing lrl improves the expected reward of the generated output the nal loss is a linear combination of lnll and lrl given by l lnll lrl where governs the strength of rl based loss term reinforcement learning allows the loss function to include any non dierentiable metric in the form of rewards which can be leveraged to optimize on our complex stylistic aspects directly the self critical sequence training approach also helps in dealing with a exposure bias a limitation in teacher forcing algorithm for training recurrent neural networks by using sampled sequences the model is exposed to its own distribution learning to generate in accordance with such global meta properties to evaluate this methodology towards incorporating such characteristics we use our setup to improve on the readability and simplicity of the generated maries and incorporate the corresponding metrics into the learning algorithm rectly as a reward function using the reinforcement learning based loss function lrl to gain more insights on appropriate methods of tailoring stylistic acteristics we compare our rl based approach against the pointer generator method from the use of vocabulary tokens adapted from and with fying word to word anity probabilities adapted from to adapt token based approach for readability we dene two tokens not readable and readable based on whether the readability of the ground truth summary was less or more as compared to the median value of in the ing dataset we also evaluate against the lexical level modications suggested by and use their voting method to modify the generation probabilities by moting the generation of shorter words over their longer synonyms finally for our rl based approach we observe that training using the reinforcement loss is extremely slow owing to the computation of sampled and greedy sequences along with the teacher forced outputs hence we use the pgen model pre trained on cnn dm dataset as the initialization point i e training with then we train for more iterations with a xed similarly for simplicity we leverage the work by to measure simplicity based on eq and establish the baselines similar to readability above our based method directly uses this simplicity score as the reward function for the token based approach we divide the ground truth summaries into two classes not simple and simple by thresholding at the median observed to be in the training dataset similar to content specic characteristics we use rouge rouge and rouge l f scores to evaluate the overlap between the generated and truth summaries to evaluate whether the models are able to capture our ability denition we report the average flesch reading ease score for the ated summaries for simplicity we report the corresponding average simplicity score note that our objective is to understand whether the methods are able to capture a given denition for style specic characteristics therefore we have evaluated the tailoring based on the dened target metrics itself table summarizes our experiments for readability and simplicity we serve a trade o between the use of simpler readable words from the vocabulary and the generation quality as captured by rouge metric primarily because of the deviation towards more simpler or readable words from the ones in erence summaries the proposed rl based approach is better able to capture readability achieving higher average scores over all other approaches however it is not the best model for simplicity where the lexical modications at the coder beats the rl method this suggests that where the entire sequence needs to be generated to measure the stylistic aspect like readability it is useful to resort to rl based frameworks however when the stylistic aspect can be sured lexically decoder modications perform better there exist works in the reinforcement learning literature that have explored actor critic methods to provide intermediate feedback to the model even before generating the plete output sequences via partial rewards exploring such techniques to tackle simpler to complex denitions for style specic constraints is a topic for future work also note that token based frameworks have a mixed results since it heavily relies on the diversity of training data without any explicit signal hence might not be suited for stylistic aspects unless the training data contains sucient diversity it is possible to train a joint model which can be trained using the feedback on both ground truth summaries in the data and the sequences sampled from the output distributions in a token based framework which is a subject of further research readability simplicity method rouge f score readability rouge f score simplicity pgen token based voting rl based l l table performance of the proposed approach in improving the readability and simplicity of generated summaries table shows the generated output summaries for the rl based approach and pgen baseline model on an instance from cnn dm dataset where our rl based method achieves better readability scores using shorter sentence structs such a framework can be used to teach the model on sentence level characteristics required to achieve a target style similarly the summaries ated by voting based approach is shown in table the generated summaries uses simpler words in the summaries such as big in place of major and hurt in place of injured in a similar manner careful modications of the generated probabilities while decoding can be used to incorporate various other stylistic aspects dened at a lexical level the killing of an employee at wayne community college in goldsboro north carolina may have been a hate crime authorities said tuesday investigators are looking into the possibility said goldsboro police sgt jeremy sutton he did not explain what may have made it a hate crime the victim ron lane whom ocials said was a longtime employee and the school s print shop operator was white as is the suspect lane s relatives said he was gay cnn aliate wncn reported the suspect kenneth morgan stancil iii worked with lane as part of a work study program but was let go from the program in early march due to poor attendance college president kay albertson said tuesday on monday stancil walked into the print shop on the third oor of a campus building aimed a pistol grip shotgun and red once killing lane according to sutton stancil has tattoos on his face relatives of wayne community college shooting victim say he was gay local media report the suspect had worked for the victim but was let go college president says the suspect kenneth morgan stancil iii was found sleeping on a orida beach and arrested wayne community college north carolina may have been a hate crime authorities say investigators are looking into the possibility said goldsboro police sgt jeremy sutton investigators are looking into the possibility said goldsboro police sgt jeremy sutton rl the killing of an employee at wayne community college may have been a hate crime the suspect kenneth morgan stancil iii worked with lane as part of a work study program he has no previous criminal record authorities say table sample output summary generated by incorporating readability as a reward function along with baseline and reference summaries on an instance from cnn dm mixed dataset the numbers in brackets refer to the sponding readability scores we show just the top few sentences in the input article in the interest of space conclusions in this work we study a variety of constraints which may be imposed while generating abstractive summaries of a given input article by categorizing these constraints as either content specic which govern what content needs to be generated and style specic which govern various stylistic expressions in these outputs our experiments indicate that the content based characteristics can be tailored in the summary via explicitly or implicitly tuning the attention to focus on relevant parts of the network approach to tailor stylistic constraints depends on the nature of denition characteristics dened at lexical level can be tuned better by modifying decoder probabilities during beam search more complicated metrics can be tuned by using reinforced rewards in the loss function references nenkova a mckeown k automatic summarization foundations and trends in information retrieval article hong kong cnn six people were hurt after an explosion at a troversial chemical plant in china s southeastern fujian province sparked a huge re provincial authorities told state media the plant located in zhangzhou city produces paraxylene px a reportedly carcinogenic chemical used in the production of polyester lms and fabrics the blast occurred at an oil storage facility monday night after an oil leak though local media has not reported any toxic chemical spill summary ve out of six people were by broken glass and have been sent to the hospital for treatment article cnn debates on climate change can break down fairly fast there are those who believe that mankind s activities are changing the planet s climate and those who do nt but a new way to talk about climate change is emerging which shifts focus from impersonal discussions about greenhouse gas emissions and power plants to a very personal one your health summary it s easy to brush aside debates involving international corporations but who would nt stop to think and perhaps do something about their own health table sample simplied summaries generated by the proposed approach words in bold show the use of simpler summaries generated by our approach while the words in italics are those picked up by the baseline model nallapati r zhai f zhou b summarunner a recurrent neural network based in aaai sequence model for extractive summarization of documents see a liu p j manning c d get to the point summarization with in annual meeting of the association for computational generator networks linguistics acl fan a grangier d auli m controllable abstractive summarization arxiv preprint krishna k srinivasan b v generating topic oriented summaries using neural attention in conference of the north american chapter of the association for computational wang l yao j tao y zhong l liu w du q a reinforced aware convolutional sequence to sequence model for abstractive text tion arxiv preprint niu x martindale m carpuat m a study of style in machine translation controlling the formality of machine translation output in proceedings of the conference on empirical methods in natural language processing krishna k murhekar a sharma s srinivasan b v vocabulary tailored summary generation in international conference on computational linguistics coling paulus r xiong c socher r a deep reinforced model for abstractive marization arxiv wang l cardie c domain independent abstract generation for focused meeting summarization in acl genest p e lapalme g framework for abstractive summarization using to text generation in workshop on monolingual text to text generation filippova k multi sentence compression finding shortest paths in word graphs in international conference on computational linguistics coling berg kirkpatrick t gillick d klein d jointly learning to extract and press in annual meeting of the association for computational linguistics acl banerjee s mitra p sugiyama k multi document abstractive summarization using ilp based multi sentence compression in ijcai sutskever i vinyals o le q v sequence to sequence learning with neural in advances in neural information processing systems networks rush a m chopra s weston j a neural attention model for abstractive tence summarization in conference on empirical methods in natural language processing chopra s auli m rush a m harvard s abstractive sentence summarization with attentive recurrent neural networks in hlt naacl takase s suzuki j okazaki n hirao t nagata m neural headline ation on abstract meaning representation in proceedings of the conference on empirical methods in natural language processing nallapati r zhou b dos santos c gulcehre c xiang b abstractive in signll text summarization using sequence to sequence rnns and beyond conference on computational natural language learning hermann k m kocisky t grefenstette e espeholt l kay w suleyman m blunsom p teaching machines to read and comprehend in advances in neural information processing systems gulcehre c ahn s nallapati r zhou b bengio y pointing the known words in annual meeting of the association for computational linguistics acl tu z lu z liu y liu x li h modeling coverage for neural machine lation in annual meeting of the association for computational linguistics volume long papers hu z yang z liang x salakhutdinov r xing e p toward controlled generation of text in international conference on machine learning shen t lei t barzilay r jaakkola t style transfer from non parallel text by cross alignment in advances in neural information processing systems sennrich r haddow b birch a controlling politeness in neural machine translation via side constraints in proceedings of the conference of the north american chapter of the association for computational linguistics human language technologies ficler j goldberg y controlling linguistic style aspects in neural language generation in proceedings of the workshop on stylistic variation niu x rao s carpuat m multi task neural models for translating between styles within and across languages arxiv preprint oraby s reed l tandon s sharath t lukin s walker m controlling personality based stylistic variation with neural natural language generators arxiv preprint tikhonov a yamshchikov i p what is wrong with style transfer for texts arxiv preprint artetxe m labaka g agirre e cho k unsupervised neural machine lation arxiv preprint han m wu o niu z unsupervised automatic text style transfer using lstm in national ccf conference on natural language processing and chinese puting springer xu j sun x zeng q ren x zhang x wang h li w unpaired sentiment to sentiment translation a cycled reinforcement learning approach arxiv preprint prabhumoye s tsvetkov y salakhutdinov r black a w style transfer through back translation arxiv preprint zhang y ding n soricut r shaped shared private encoder decoder for text style adaptation in proceedings of the conference of the north can chapter of the association for computational linguistics human language technologies volume long papers volume duchi j hazan e singer y adaptive subgradient methods for online learning and stochastic optimization journal of machine learning research paetzold g specia l lexenstein a framework for lexical simplication ceedings of acl ijcnlp system demonstrations brysbaert m new b moving beyond kucera and francis a critical evaluation of current word frequency norms and the introduction of a new and improved word frequency measure for american english behavior research methods flesch r f how to write plain english a book for lawyers and consumers harpercollins rennie s j marcheret e mroueh y ross j goel v self critical sequence training for image captioning in cvpr bahdanau d brakel p xu k goyal a lowe r pineau j courville a bengio y an actor critic algorithm for sequence prediction arxiv preprint
