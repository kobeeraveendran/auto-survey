encode tag realize high precision text editing eric malmi google research com sebastian krause google research com sascha rothe google research com daniil mirylenka google research com aliaksei severyn google research com p e s l c s c v v x r abstract propose lasertagger sequence ging approach casts text generation text editing task target texts structed inputs main edit operations keeping token deleting adding phrase token dict edit operations propose novel model combines bert encoder autoregressive transformer decoder approach evaluated english text tasks sentence fusion sentence splitting stractive summarization grammar tion lasertagger achieves new state art results tasks performs comparably set strong lines large number training ples outperforms number examples limited furthermore inference time tagging orders magnitude faster ble models making attractive running live environment introduction neural sequence sequence models provide powerful framework learning late source texts target texts rst application machine translation mt sutskever et al proach virtually text generation task cluding summarization tan et al image captioning xu et al text style transfer rao tetreault nikolov hahnloser jin et al grammatical error correction chollampatt ng grundkiewicz et al observe text generation tasks recently introduced sentence splitting sentence fusion tasks output texts highly lap inputs setting learning model generate output text scratch figure lasertagger applied sentence fusion intuitively wasteful copy mechanisms gu et al et al allow choosing copying source tokens generating bitrary tokens hybrid models help vocabulary words quire large training sets depend output vocabularies large standard approaches contrast propose learning text editing model applies set edit operations input sequence reconstruct output use relatively small set output tags representing text deletion rephrasing word reordering able reproduce large percentage targets training data results learning problem smaller vocabulary size output length xed number words source text turn greatly reduces number training examples quired train accurate models larly important applications small number human labeled data available tagging approach lasertagger consists steps fig encode builds tation input sequence tag assigns edit tags pre computed output vocabulary input tokens realize applies simple set rules convert tags output text tokens experimental evaluation lasertagger encodetagturing born turing died turing born died realizekeep hedelete delete different text generation tasks shows yields comparable results models tens thousands training examples clearly outperforms number examples smaller contributions following demonstrate text generation tasks overlapping inputs outputs effectively treated text editing tasks propose lasertagger sequence tagging based model text editing method generating tag vocabulary training data describe versions lasertaggerff tagger ging model devlin et al based bert ii lasertaggerar novel tagging model combining bert encoder sive transformer decoder improves results bert tagger evaluate lasertagger strong baseline models based bert chitecture baseline models outperform ously reported state art results tasks demonstrate lasertaggerar achieves state art comparable results examined tasks lasertaggerff faster inference time performance comparable state art els furthermore models require training data compared els d controllable interpretable models small vocabulary edit operations e prone typical model errors hallucination code available lasertagger page link code related work recent work discusses difculties learning neural decoders text generation man et al prabhakaran et al tional approaches require large amounts training data hard control constrain desirable outputs time nlp tasks appear text generation tasks natural testbeds simpler methods section briey review tasks text simplication paraphrasing task known benet modeling edit operations simple instance type sentence pression systems apply drop operation token phrase level filippova strube et al intricate systems apply splitting reordering lexical tution zhu et al simplication attempted systems developed based mt xu et al neural encoder decoder models zhang lapata independent work dong et al recently proposed text editing model similar text simplication main differences work introduce interpreter module acts language model far realized text generate added tokens vocabulary optimized set frequently added phrases allows model generate diverse output negatively effect inference time precision data efciency model recent model similar called levenshtein transformer gu et al text editing performing sequence deletion insertion actions single document summarization task requires systems shorten texts preserving way approached deletion based methods token level al sentence level narayan et al liu papers neural encoder decoder methods tan et al rush et al paulus et al stractive summarization allows edits yond mere deletion motivated work jing mckeown identied small number fundamental high level editing operations useful producing summaries reduction combination syntactic transformation lexical paraphrasing generalization specication reordering et al extended ral encoder decoder model copy mechanism allow model easily reproduce input tokens generation available summarization datasets noncourt et al nd toutanova et al particularly interesting specically targets abstractive summarization tems lengths texts dataset short paragraphs suited text editing analysis showed dataset covers different summarization operations grammatical error correction ng et al system presented input texts written usually language learner tasked detecting xing grammatical mistakes approaches task incorporate task specic knowledge e designing ers specic error types knight chander rozovskaya et al trained manually labeled data adapting statistical machine translation methods dowmunt grundkiewicz methods sub problem error detection similar spirit sentence compression systems implemented word based neural sequence belers rei rei et al neural decoder methods commonly applied error correction task ge et al patt ng zhao et al suffer lack training data specic tricks need applied kasewa et al junczys dowmunt et al text editing tagging problem approach text editing cast tagging problem describe main ponents tagging operations convert plain text training targets tagging format realization step convert tags nal output text tagging operations tagger assigns tag input token tag composed parts base tag added phrase base tag delete indicates retain token output added phrase p enforces p added corresponding token p belongs vocabulary v denes set words phrases inserted input sequence transform output combination base tag b added phrase p treated single tag denoted p b total number unique tags equal number base tags times size phrase vocabulary unique tags additional task specic tags employed sentence fusion section input consists sentences need swapped introduce custom tag swap applied period rst sentence fig tag instructs realize step swap order input sentences realizing rest tags tasks different supplementary tags useful e allow replacing tity mentions appropriate pronouns introduce pronominalize tag given access knowledge base includes entity gender information look rect pronoun realization step instead having rely model predicting correct tag shedelete hedelete theydelete optimizing phrase vocabulary phrase vocabulary consists phrases added source words hand wish minimize number phrases output tag vocabulary small hand like maximize centage target texts reconstructed source available tagging ations leads following combinatorial optimization problem problem given collection phrase sets ai p p set candidate phrases select phrase lary v p phrases e number covered phrase sets imized phrase set ai covered ai v problem closely related mum k union problem np hard vinterbo problem asks set k phrase sets cardinality union minimum able solve problem polynomial time solve mum k union problem polynomial time simply nding smallest phrase vocabulary size number covered phrase sets reduction minimum k union problem gives following result theorem problem np hard identify candidate phrases included vocabulary rst align source text s training data target text t achieved computing longest common sequence lcs word sequences dynamic programming time n grams target text lcs phrases need included phrase vocabulary able construct t s source tags realization dylan american musician won nobel prize dylan delete swap commadelete won nobel prize dylan american musician commadelete figure example sentence fusion obtained tagging swap tag swaps order source sentences practice phrase vocabulary expected consist phrases frequently added target adopt following simple proach construct phrase vocabulary sort phrases number phrase sets occur pick frequent phrases found produce meaningful phrase vocabularies based manual inspection shown section e phrases sentence fusions include discourse connectives considered greedy approach structs vocabulary phrase time selecting phrase largest incremental coverage approach ideal use case frequent phrases strongly coupled selecting close zero incremental coverage cover examples converting training targets tags phrase vocabulary determined convert target texts training data tag sequences given phrase vocabulary need compute lcs leverage efcient approach iterates words input greedily attempts match words target case match phrases vocabulary v np time np length longest phrase v shown algorithm training targets require adding phrase vocabulary v converted tag sequence ltered making training smaller effectively lter low quality targets percentage converted examples different datasets reported later section note target reconstructed inputs output tag vocabulary proach produce reasonable outputs available phrases e target require use infrequent token vocabulary model instead choose predict common token algorithm converting target string tags input source text s target text t phrase vocabulary v imum added phrase length np output tag sequence length ns length conversion possible ns initialize tags current source word index current target word index conversion infeasibile p j np ns return delete nt return break pkeep added phrase word sequence p p v target consumed return tags realization obtaining predicted tag sequence vert text realization step classic works text generation distinction tween planning realization end end neural approaches typically ignore distinction exception works moryossef et al puduppully et al basic tagging operations keeping deleting adding realization ward process additionally adjust tion sentence boundaries realization involved introduce special tags pronominalize mentioned section tag need look gender tagged entity knowledge base having separate realization step benecial decide pronominalize condent appropriate pronoun leave entity mention untouched advantage having separate consuming embedding previously dicted label activations encoder ways decoder communicate encoder attention sequence encoder activations similar conventional architectures ii directly consuming encoder activation current step preliminary experiments found option perform better converge faster require learning ditional encoder decoder attention weights experiment decoder variants forward autoregressive nd toregressive decoder outperforms previously feedforward decoder rest paper tagging model autoregressive decoder referred lasertaggerar model feedforward decoder lasertaggerff experiments evaluate method conducting experiments different text editing tasks sentence sion split rephrase abstractive tion grammatical error correction baselines addition reporting previously published results task train set strong baselines based transformer encoder decoder replicate base architecture devlin et al fair comparison similar initialize tagger encoder pretrained bert checkpoint use initialization transformer encoder produces strong line results new state art metrics tasks sentence fusion sentence fusion problem fusing sentences single coherent sentence data use balanced wikipedia tion geva et al discofuse dataset experiments henceforth dfwiki m fusion examples dataset require reordering input cope duce swap tag enables model ip order input sentences construct phrase vocabulary described sec validation set k examples phrases shown rst column table evaluation metrics following geva et al use evaluation metrics exact score figure architecture lasertaggerar tion step specic loss patterns dressed adding specialized realization rules instance rule applying tag hisdelete entity mention followed s realizer delete possessive s regardless predicted tag tagging model architecture tagger composed components encoder generates activation vectors element input sequence decoder converts encoder activations tag labels encoder choose bert transformer model devlin et al encoder demonstrated state art results ber sentence encoding tasks use bert base architecture consists self attention layers refer reader devlin et al detailed description model architecture input representation ize encoder publicly available checkpoint pretrained case sensitive bert base model original bert paper ple decoding mechanism sequence output tags generated single ging feed forward pass applying argmax encoder logits way output tag predicted independently modelling pendencies tags sequence simple decoder demonstrated state art sults named entity recognition task applied bert encoder decoder better model dependencies output tag labels propose powerful toregressive decoder specically run layer transformer decoder bert encoder fig step decoder com google research bert layer dfwiki wikisplit model exact sari s nt gec table frequently added phrases datasets studied work order decreasing quency marks sentence boundary short abstractive summarization grammatical error correction figure performance model lasertaggerar dfwiki dataset conditioned vocabulary size gold score e percentage examples reconstructed text edit operations percentage exactly correctly dicted fusions sari xu et al computes average scores added kept deleted n grams vocabulary size understand impact number phrases include vocabulary trained models different vocabulary sizes lasertaggerar results shown figure increasing vocabulary size phrases exact score reaches plateau set vocabulary size ing experiments paper gold curve fig shows vocabulary size sufcient use implementation available git setting deletion geva et al smaller datasets smaller vocabulary size yield better results simplicity optimize size separately dataset transformer geva et al lasertaggerar swap lasertaggerff lasertaggerar table sentence fusion results dfwiki cover training examples gives upper bound exact score comparison baselines table lists results dfwiki dataset obtain new sota results lasertaggerar ing previous sota layer transformer model geva et al exact score sari score nd pretrained model yields nearly good formance demonstrating effectiveness supervised pretraining generation tasks performance tagger impaired signicantly leaving swap tag model s inability reconstruct training set impact dataset size study fect training data size creating creasingly smaller subsets dfwiki fig data size drops examples lasertagger performs surprisingly clearly outperforming baseline split rephrase reverse task sentence fusion rephrase task requires rewriting long sentence coherent short sentences data use wikisplit dataset botha et al consists m human editor ated examples sentence splits follow dataset split suggested authors phrase vocabulary size yields age targets training set phrases shown table lower coverage compared dfwiki suggests higher noise wikipedia author edits unrelated splitting results botha et al report results ing layer bi directional lstm cell size attention copying mechanism simplicity use phrase vocabulary size computed validation set k examples experiments note subsampled training sets contain k examples vocabulary taggers unfair advantage baselines tagger predict phrase seen training data vocabulary scoregoldlasertaggerar sentence fusion dfwiki split rephrase wikisplit figure sari score function training data size models tens thousands training examples tagging approach clearly outperforms baseline model bleu exact sari model exact sari rouge l botha et al lasertaggerff lasertaggerar table results wikisplit dataset filippova et al clarke lapata cohn lapata rush et al lasertaggerff lasertaggerar table results summarization et al results shown table lasertaggerar yield similar performance form model copying mechanism botha et al studied impact training data size subsampling training set ure similar previous experiment lasertagger methods degrade gracefully reducing training data size start perform baseline going circa examples smallest training set lasertaggerar contains merely examples remarkably model able learn thing useful generalizes unseen test ples reaching sari score dicting targets exactly correctly following example prediction model source delhi public library national depository library delhi india branches state prediction delhi public library national depository brary delhi india branches state model picked right comma place period sentence separator et al report bleu kindly shared model s predictions allowing pute exact sari score method similar work nltk bleu computation abstractive summarization task summarization reduce length text preserving meaning dataset use dataset toutanova et al contains short input texts sentences written summaries human experts restricted deleting words generating summary allowed insert new words reorder parts sentence makes dataset particularly suited tive summarization models set size phrase vocabulary tasks extract phrases training partition size able cover training data evaluation metrics addition metrics previous sections report rouge l lin metric commonly summarization literature rouge l recall oriented measure computed longest common sub sequence reference summary candidate summary results table compares taggers baselines systems ture filippova et al clarke pata proposed deletion based approaches extracted toutanova et al training training uses network mulates summarization optimization lem solved integer linear programming cohn lapata proposed early proach abstractive summarization tree transducer rush et al developed neural model abstractive tion line results subsampled sion splitting datasets figure tagger nicantly outperforms baselines shows text editing approach suited extreme summarization examples plete paraphrase zero lexical overlap tice limited paraphrasing capability reach good empirical performance note low absolute values exact metric expected large number acceptable summaries grammatical error correction gec gec requires systems identify x ical errors given input text data use recent benchmark shared task building educational applications workshop specically low resource bryant et al publicly able set ill formed sentences gold error corrections split training validation partition create phrase vocabulary frequently added phrases training partition gives coverage training data evaluation metrics results report precision recall task s main metric gives weight precision corrections recall table compares taggers lines tagging approach clearly performs bert based model seven times accurate diction corrections accounted model s richer generation capacity model properly tune task hand given small training data tagging approach hand naturally suited kind problem report best performing method grundkiewicz et al shared task informational purposes train transformer cl cam ac uk research nl model grundkiewicz et al lasertaggerff lasertaggerar p r table results grammatical error correction note grundkiewicz et al augment training dataset examples million synthetic amples million wikipedia edits batch size lasertaggerff lasertaggerar table inference time ms batch sizes gpu nvidia tesla averaged runs random inputs model dataset augmented million synthetic examples million wikipedia edits use sentences provided training dataset inference time getting state art results requires ing larger complex models ning model production cares accuracy inference time ble reports latency numbers lasertagger models accurate baseline baseline practical run production batch size hand batch size lasertaggerar faster comparable accuracy line difference model layer decoder instead layers encoder decoder cross attention tried training layer decoder performed poorly terms accuracy nally lasertaggerff faster accuracy points best reported results qualitative evaluation assess qualitative difference outputs lasertagger analyzed texts generated models test sets tasks inspected tive worst predictions model according bleu identied seven main error patterns error type lasertagger example imaginary words repeated phrases affected affected premature end sentence affected hallucinations affected coreference issues misleading rephrasing lazy sentence splitting affected affected affected affected affected affected affected affected affected affected zenica cyrillic zenica cyrillic gratulation m employee serve company m company serve company way favorite football team manchester united way favorite football team tobacco smokers experience anthropology smokers experience daughter alistair crane secretly built daughter alistair crane secretly built postal service way responsible postal service responsible home world marglotta located sagittarius arm home world marglotta located sagittarius arm table main error patterns observed output tagging models test sets tasks specic model specic lasertagger illustrates lasertagger prone errors compared standard proach restricted exibility model certain types errors imaginary words repeated phrases virtually impossible tagger likelihood hallucination abrupt sentence ending greatly reduced table list error classes refer appendix details observations conclusions proposed text editing approach generation tasks high overlap input output texts compared models typically applied setting approach sults simpler sequence tagging problem smaller output tag vocabulary strated approach comparable mance trained medium large datasets clearly outperforms strong baseline number training examples limited qualitative analysis model outputs suggests tagging approach affected common errors models lucination abrupt sentence ending demonstrated tagging speed inference orders magnitude making attractive production applications limitations arbitrary word reordering feasible approach limited ordering achieved deletion tion operations custom tags swap section enable exible reordering possible apply techniques developed phrase based machine translation limitation approach straightforward apply languages morphologically richer english sophisticated realizer needed adjust e cases words future work like experiment light weight tagging architectures dor et al better understand trade inference time model accuracy acknowledgments like thank enrique alfonseca idan szpektor orgad keller useful discussions references daniel andor chris alberti david weiss aliaksei severyn alessandro presta kuzman ganchev slav petrov michael collins globally ized transition based neural networks ings annual meeting association computational linguistics volume long pers pages jan botha manaal faruqui john alex jason baldridge dipanjan das learning split rephrase wikipedia edit history proceedings conference empirical methods natural language processing christopher bryant mariano felice istein e dersen ted briscoe shared task grammatical error correction ceedings fourteenth workshop innovative use nlp building educational applications pages shamil chollampatt hwee tou ng neural quality estimation grammatical error correction proceedings conference cal methods natural language processing pages james clarke mirella lapata global ference sentence compression integer j artif intell res ear programming approach trevor cohn mirella lapata sentence proceedings pression word deletion international conference tional linguistics coling pages franck dernoncourt mohammad ghassemi ter chang repository corpora marization proceedings eleventh tional conference language resources uation jacob devlin ming wei chang kenton lee kristina toutanova bert pre training deep bidirectional transformers language standing proceedings conference north american chapter association computational linguistics human language nologies volume long short papers pages yue dong zichao li mehdi rezagholizadeh jackie chi kit cheung editnts neural programmer interpreter model sentence proceedings cation explicit editing annual meeting association putational linguistics katja filippova enrique alfonseca carlos menares lukasz kaiser oriol vinyals sentence compression deletion lstms proceedings conference empirical methods natural language processing pages katja filippova michael strube dency tree based sentence compression ings fifth international natural language generation conference pages tao ge furu wei ming zhou fluency boost learning inference neural cal error correction proceedings nual meeting association computational linguistics volume long papers pages jiatao gu zhengdong lu hang li victor ok incorporating copying mechanism li proceedings sequence sequence learning annual meeting association putational linguistics volume long papers pages jiatao gu changhan wang levenshtein transformer jake zhao arxiv preprint zhijing jin di jin jonas mueller nicholas matthews enrico santus unsupervised text style transfer iterative matching translation arxiv preprint hongyan jing kathleen mckeown cut meeting paste based text summarization north american chapter association computational linguistics marcin junczys dowmunt roman grundkiewicz amu system shared task grammatical error correction intensive feature rich statistical machine proceedings eighteenth lation ence computational natural language learning shared task pages marcin junczys dowmunt roman grundkiewicz shubha guha kenneth heaeld proaching neural grammatical error correction low resource machine translation task ings conference north american chapter association computational guistics human language technologies volume long papers pages sudhanshu kasewa pontus stenetorp sebastian riedel wronging right generating ter errors improve grammatical error detection proceedings conference cal methods natural language processing pages kevin knight ishwar chander automated proceedings postediting documents national conference articial intelligence seattle wa usa july august volume pages mor geva eric malmi idan szpektor jonathan berant discofuse large scale dataset discourse based sentence fusion ings conference north american chapter association computational guistics human language technologies volume long short papers pages chin yew lin rouge package text matic evaluation summaries rization branches proceedings workshop pages yang liu fine tune bert extractive rization arxiv preprint roman grundkiewicz marcin junczys dowmunt kenneth heaeld neural grammatical error correction systems unsupervised pre training synthetic data proceedings fourteenth workshop innovative use nlp building ucational applications pages amit moryossef yoav goldberg ido dagan step step separating planning realization neural data text generation proceedings conference north american ter association computational linguistics human language technologies volume long short papers pages shashi narayan shay b cohen mirella lapata ranking sentences extractive tion reinforcement learning proceedings conference north american ter association computational linguistics human language technologies volume long pers pages hwee tou ng siew mei wu ted briscoe christian hadiwinoto raymond hendy susanto pher bryant shared task grammatical error correction proceedings eighteenth conference computational ral language learning shared task pages hwee tou ng siew mei wu yuanbin wu christian hadiwinoto joel tetreault shared task grammatical error correction proceedings seventeenth conference computational natural language learning shared task pages nikola nikolov richard hr hahnloser large scale hierarchical alignment author style transfer arxiv preprint romain paulus caiming xiong richard socher deep reinforced model abstractive marization arxiv preprint vinodkumar prabhakaran camilla grifths hang su prateek verma nelson morgan jennifer l hardt dan jurafsky detecting tional dialog acts police trafc stops tions association computational tics ratish puduppully li dong mirella lapata data text generation content selection planning arxiv preprint sudha rao joel tetreault dear sir madam introduce gyafc dataset pus benchmarks metrics formality style transfer proceedings conference north american chapter association computational linguistics human language nologies volume long papers pages marek rei semi supervised multitask proceedings ing sequence labeling annual meeting association tational linguistics volume long papers pages marek rei mariano felice zheng yuan ted briscoe articial error generation chine translation syntactic patterns ings workshop innovative use nlp building educational applications pages alla rozovskaya kai wei chang mark sammons dan roth nizar habash columbia system shared task proceedings eighteenth conference computational natural language learning shared task pages alexander m rush sumit chopra jason weston neural attention model abstractive proceedings tence summarization conference empirical methods natural guage processing pages abigail peter j liu christopher d manning point summarization generator networks proceedings nual meeting association computational linguistics volume long papers pages ilya sutskever oriol vinyals quoc v le sequence sequence learning neural networks advances neural information processing tems pages jiwei tan xiaojun wan jianguo xiao stractive document summarization proceedings based attentional neural model annual meeting association computational linguistics volume long papers pages kristina toutanova chris brockett ke m tran saleema amershi dataset evaluation metrics abstractive compression sentences short paragraphs proceedings ference empirical methods natural language processing pages staal vinterbo note hardness k ambiguity problem technical report dsg t sam wiseman stuart m shieber alexander m rush learning neural templates text eration arxiv preprint kelvin xu jimmy ba ryan kiros kyunghyun cho aaron courville ruslan salakhudinov rich zemel yoshua bengio attend tell neural image caption generation visual proceedings international tion conference machine learning volume proceedings machine learning research pages wei xu courtney napoles ellie pavlick quanze chen chris callison burch optimizing statistical machine translation text simplication transactions association computational linguistics wei xu courtney napoles ellie pavlick quanze chen chris callison burch optimizing statistical machine translation text simplication transactions association computational linguistics xingxing zhang mirella lapata sentence simplication deep reinforcement learning proceedings conference empirical methods natural language processing pages wei zhao liang wang kewei shen ruoyu jia jingming liu improving grammatical error correction pre training copy augmented tecture unlabeled data proceedings conference north american chapter association computational linguistics man language technologies volume long short papers pages zhemin zhu delphine bernhard iryna gurevych monolingual tree based translation model proceedings sentence simplication international conference computational linguistics coling pages appendix examples qualitative analysis table present cherry picked examples error patterns introduced section imaginary words operating sub word level model capable ing nonexistent words concatenating unrelated wordpieces encountered ples words outputs model tasks typically pens model encounters rare word input text lasertagger trained word level immune specic type error premature end sentence model produces sequences arbitrary length ing end sentence eos seen model generate eos prematurely resulting abrupt sentence ending extreme cases especially abstractive summarization task model generated eos start effectively producing output lasertagger type error technically possible unlikely seen practice tagger need generate long sequence delete tags seen training data repeated phrases type error repetition cic model information single words entire phrases sentence splitting task model repeat parts sentence twice fore splitting symbol grammar correction summarization tasks model replace rare word word input sentence repeating word twice lasertagger add words phrases limited vocabulary unlikely cause repetition observed model cially likely repeat large fragments tence splitting task cases ous good way split sentence interestingly cases lasertagger split sentence inserting splitting symbol examples present training data cases lasertagger produced lazy split discussed hallucination known problem neural works lasertagger susceptible lesser degree lasertagger lucinate inserting unexpected word short phrase vocabulary seen insertions tagger output dered sentence ungrammatical simply odd model likely produce tly misleading hallucinations misrepresent input text looking uent credible seen examples model changing factual details dangerous error scenarios coreference problems type errors tagger models typical instance model inserts rect pronoun cases model makes incorrect coreference resolution inserting wrong proper noun model susceptible second type error ability copy proper nouns input tence lasertagger typically attempt resolve coreference leave pronoun intact misleading rephrasing deletion want highlight lasertagger limited set operations insert arbitrary phrases immune semantic rors misrepresenting input text seen examples deletion word spans sufcient completely alter meaning sentence model naturally prone type errors lazy splitting type error specic lasertagger sentence splitting task occurs input sentence split arbitrarily middle modications resulting parts splits usually reasonable point input sentence e yielding valid grammatical clauses complete sentence imaginary words introduced sentence splitting task input zenica cyrillic industrial city largest sarajevo banja luka bosnia herzegovina capital zenica doboj canton federation bosnia herzegovina entity output zenica cyrillic gratulation industrial city largest sarajevo banja luka bosnia herzegovina capital zenica doboj canton federation bosnia herzegovina entity imaginary words indroduced sentence fusion task input output carboxysomes aid carbon xotrophically mixotrophically grown cells carboxysomes found lithoautotrophically mixotrophically grown cells carboxysomes aid carbon xation abrupt sentence end model grammar correction task input way favorite football team manchester united brilliant amazing football players awesome output way favorite football team abrupt sentence end model summarization task input suppose likely people little money think money hungry dwell thoughts food thirsty drink output redundant repetition model sentence splitting task input output bethany noel mota born november american video blogger los banos california bethany noel mota born november american video blogger los banos california mota american video blogger los banos california redundant repetition model sentence fusion task input general fazlollah zahedi prime minister general fazlollah zahedi led military coup output general fazlollah zahedi led military coup prime minister prime minister redundant repetition model grammar correction task input output m employee serve company m company serve company hallucinations produced tagger grammar correction task input output interesedt nd clock near cinema entrance play nd clock near cinema entrance hallucinations produced tagger sentence fusion task input carried bow nerva output receiving bow nerva hallucinations produced model sentence splitting task input output native block size bytes inodes bytes disk native block size bytes inodes bytes disk hallucinations produced model sentence fusion task input tobacco smokers experience powerful effects psilocybin tobacco smoke exposure decreases activity mao brain peripheral organs anthropology smokers experience powerful effects psilocybin output coreference error model sentence fusion task input tries help yuushi catching sayuki s eye encourages idea baseball club output tries help yuushi catching sayuki s eye yuushi encourages idea baseball club pronominalization error tagger sentence fusion task input output sharmistha mukherjee indian kathak dancer choreographer politician indian sharmistha mukherjee indian kathak dancer choreographer politician indian national congress national congress lazy splitting tagger sentence splitting task input home world marglotta located sagittarius arm output home world marglotta located sagittarius arm lazy splitting tagger sentence splitting task input output jesse son named sahvere duaghter named jaylen jesse son named sahvere duaghter named jaylen misleading rephrasing deletion tagger sentence splitting task input output ex lover winnie mann son wilson adopted chinese girl jang yin ex lover winnie mann son wilson adopted chinese girl jang yin misleading rephrasing deletion tagger sentence splitting task input article proudly notes postal service way responsible crash valujet flight florida everglades output article notes postal service responsible crash valujet flight table illustration typical errors produced models cherry picked examples
