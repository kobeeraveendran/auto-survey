encode tag realize high precision text editing eric malmi google research com sebastian krause google research com sascha rothe google research com daniil mirylenka google research com aliaksei severyn google research com p e s l c s c v v i x r a abstract we propose lasertagger a sequence ging approach that casts text generation as a text editing task target texts are structed from the inputs using three main edit operations keeping a token deleting it and adding a phrase before the token to dict the edit operations we propose a novel model which combines a bert encoder with an autoregressive transformer decoder this approach is evaluated on english text on four tasks sentence fusion sentence splitting stractive summarization and grammar tion lasertagger achieves new state the art results on three of these tasks performs comparably to a set of strong lines with a large number of training ples and outperforms them when the number of examples is limited furthermore we show that at inference time tagging can be more than two orders of magnitude faster than ble models making it more attractive for running in a live environment introduction neural sequence to sequence models provide a powerful framework for learning to late source texts into target texts since their rst application to machine translation mt sutskever et al they have become the proach for virtually every text generation task cluding summarization tan et al image captioning xu et al text style transfer rao and tetreault nikolov and hahnloser jin et al and grammatical error correction chollampatt and ng grundkiewicz et al we observe that in some text generation tasks such as the recently introduced sentence splitting and sentence fusion tasks output texts highly lap with inputs in this setting learning a model to generate the output text from scratch figure lasertagger applied to sentence fusion seems intuitively wasteful copy mechanisms gu et al see et al allow for choosing between copying source tokens and generating bitrary tokens but although such hybrid models help with out of vocabulary words they still quire large training sets as they depend on output vocabularies as large as those used by the standard approaches in contrast we propose learning a text editing model that applies a set of edit operations on the input sequence to reconstruct the output we show that it is often enough to use a relatively small set of output tags representing text deletion rephrasing and word reordering to be able to reproduce a large percentage of the targets in the training data this results in a learning problem with a much smaller vocabulary size and the output length xed to the number of words in the source text this in turn greatly reduces the number training examples quired to train accurate models which is larly important in applications where only a small number of human labeled data is available our tagging approach lasertagger consists of three steps fig i encode builds a tation of the input sequence tag assigns edit tags from a pre computed output vocabulary to the input tokens and realize applies a simple set of rules to convert tags into the output text tokens an experimental evaluation of lasertagger encodetagturing was born in turing died in turing was born in and he died in realizekeep keep keep keep keep and hedelete delete keep keep keep keep on four different text generation tasks shows that it yields comparable results to models when we have tens of thousands of training examples and clearly outperforms them when the number of examples is smaller our contributions are the following we demonstrate that many text generation tasks with overlapping inputs and outputs can be effectively treated as text editing tasks we propose lasertagger a sequence tagging based model for text editing together with a method for generating the tag vocabulary from the training data we describe two versions of the i lasertaggerff a tagger ging model devlin et al and based on bert ii lasertaggerar a novel tagging model combining the bert encoder with an sive transformer decoder which further improves the results over the bert tagger we evaluate lasertagger against strong baseline models based on the bert chitecture our baseline models outperform ously reported state of the art results on two tasks we demonstrate that a lasertaggerar achieves state of the art or comparable results on out of examined tasks lasertaggerff is up to faster at inference time with performance comparable to the state of the art els furthermore both models require much less training data compared to the els d are more controllable and interpretable than models due to the small vocabulary of edit operations e are less prone to typical model errors such as hallucination the code will be available at lasertagger page link code related work recent work discusses some of the difculties of learning neural decoders for text generation man et al prabhakaran et al tional approaches require large amounts of training data are hard to control and to constrain to desirable outputs at the same time many nlp tasks that appear to be text generation tasks are natural testbeds for simpler methods in this section we briey review some of these tasks text simplication is a paraphrasing task that is known to benet from modeling edit operations a simple instance of this type are sentence pression systems that apply a drop operation at the token phrase level filippova and strube et al while more intricate systems also apply splitting reordering and lexical tution zhu et al simplication has also been attempted with systems developed for based mt xu et al as well as with neural encoder decoder models zhang and lapata independent of this work dong et al recently proposed a text editing model similar to ours for text simplication the main differences to our work are they introduce an interpreter module which acts as a language model for the so far realized text and they generate added tokens one by one from a full vocabulary rather than from an optimized set of frequently added phrases the latter allows their model to generate more diverse output but it may negatively effect the inference time precision and the data efciency of their model another recent model similar to ours is called levenshtein transformer gu et al which does text editing by performing a sequence of deletion and insertion actions single document summarization is a task that requires systems to shorten texts in a preserving way it has been approached with deletion based methods on the token level al and the sentence level narayan et al liu other papers have used neural encoder decoder methods tan et al rush et al paulus et al to do stractive summarization which allows edits yond mere deletion this can be motivated by the work of jing and mckeown who identied a small number of fundamental high level editing operations that are useful for producing summaries reduction combination syntactic transformation lexical paraphrasing generalization specication and reordering see et al extended a ral encoder decoder model with a copy mechanism to allow the model to more easily reproduce input tokens during generation out of available summarization datasets noncourt et al we nd the one by toutanova et al particularly interesting because it specically targets abstractive summarization tems the lengths of texts in this dataset short paragraphs seem well suited for text editing and an analysis showed that the dataset covers many different summarization operations in grammatical error correction ng et al a system is presented with input texts written usually by a language learner and is tasked with detecting and xing grammatical and other mistakes approaches to this task often incorporate task specic knowledge e by designing ers for specic error types knight and chander rozovskaya et al that can be trained without manually labeled data or by adapting statistical machine translation methods dowmunt and grundkiewicz methods for the sub problem of error detection are similar in spirit to sentence compression systems in that they are implemented as word based neural sequence belers rei rei et al neural decoder methods are also commonly applied to the error correction task ge et al patt and ng zhao et al but suffer from a lack of training data which is why specic tricks need to be applied kasewa et al junczys dowmunt et al text editing as a tagging problem our approach to text editing is to cast it into a tagging problem here we describe its main ponents the tagging operations how to convert plain text training targets into a tagging format as well as the realization step to convert tags into the nal output text tagging operations our tagger assigns a tag to each input token a tag is composed of two parts a base tag and an added phrase the base tag is either keep or delete which indicates whether to retain the token in the output the added phrase p which can be empty enforces that p is added before the corresponding token p belongs to a vocabulary v that denes a set of words and phrases that can be inserted into the input sequence to transform it into the output the combination of the base tag b and the added phrase p is treated as a single tag and denoted by p b the total number of unique tags is equal to the number of base tags times the size of the phrase vocabulary hence there are unique tags additional task specic tags can be employed too for sentence fusion section the input consists of two sentences which sometimes need to be swapped therefore we introduce a custom tag swap which can only be applied to the last period of the rst sentence see fig this tag instructs the realize step to swap the order of the input sentences before realizing the rest of the tags for other tasks different supplementary tags may be useful e to allow for replacing tity mentions with the appropriate pronouns we could introduce a pronominalize tag given an access to a knowledge base that includes entity gender information we could then look up the rect pronoun during the realization step instead of having to rely on the model predicting the correct tag shedelete hedelete theydelete optimizing phrase vocabulary the phrase vocabulary consists of phrases that can be added between the source words on the one hand we wish to minimize the number of phrases to keep the output tag vocabulary small on the other hand we would like to maximize the centage of target texts that can be reconstructed from the source using the available tagging ations this leads to the following combinatorial optimization problem problem given a collection of phrase sets am where ai p and p is the set of all candidate phrases select a phrase lary v p of at most phrases i e so that the number of covered phrase sets is imized a phrase set ai is covered if and only if ai v this problem is closely related to the mum k union problem which is np hard vinterbo the latter problem asks for a set of k phrase sets such that the cardinality of their union is the minimum if we were able to solve problem in polynomial time we could solve also the mum k union problem in polynomial time simply by nding the smallest phrase vocabulary size such that the number of covered phrase sets is at least this reduction from the minimum k union problem gives us the following result theorem problem is np hard to identify candidate phrases to be included in the vocabulary we rst align each source text s from the training data with its target text t this is achieved by computing the longest common sequence lcs between the two word sequences which can be done using dynamic programming in time the n grams in the target text that are not part of the lcs are the phrases that would need to be included in the phrase vocabulary to be able to construct t from s source tags realization dylan an american musician won nobel prize dylan delete keep keep keep swap keep commadelete keep won nobel prize dylan an is american musician keep keep commadelete figure an example sentence fusion obtained by tagging using the swap tag which swaps the order of the two source sentences in practice the phrase vocabulary is expected to consist of phrases that are frequently added to the target thus we adopt the following simple proach to construct the phrase vocabulary sort the phrases by the number of phrase sets in which they occur and pick most frequent phrases this was found to produce meaningful phrase vocabularies based on manual inspection as shown in section e the top phrases for sentence fusions include many discourse connectives we also considered a greedy approach that structs the vocabulary one phrase at a time always selecting the phrase that has the largest incremental coverage this approach is not however ideal for our use case since some frequent phrases such as and are strongly coupled selecting alone has close to zero incremental coverage but together with they can cover many examples converting training targets into tags once the phrase vocabulary is determined we can convert the target texts in our training data into tag sequences given the phrase vocabulary we do not need to compute the lcs but can leverage a more efcient approach which iterates over words in the input and greedily attempts to match them against the words in the target and in case there is no match against the phrases in the vocabulary v this can be done in np time where np is the length of the longest phrase in v as shown in algorithm the training targets that would require adding a phrase that is not in our vocabulary v will not get converted into a tag sequence but are ltered out while making the training smaller this may effectively also lter out low quality targets the percentage of converted examples for different datasets is reported later in section note that even when the target can not be reconstructed from the inputs using our output tag vocabulary our proach might still produce reasonable outputs with the available phrases e a target may require the use of the infrequent token which is not in our vocabulary but a model could instead choose to predict a more common token algorithm converting a target string to tags input source text s target text t phrase vocabulary v and the imum added phrase length np output tag sequence of length ns or of length if conversion is not possible i ns initialize tags current source word index current target word index conversion infeasibile if then keep it it p for j np do if is ns then return else delete is it while it nt do return is is break if then pkeep it it added phrase word sequence p if and p v then target has been consumed so return tags realization after obtaining a predicted tag sequence we vert it to text realization step while classic works on text generation make a distinction tween planning and realization end to end neural approaches typically ignore this distinction with the exception of few works moryossef et al puduppully et al for the basic tagging operations of keeping deleting and adding realization is a ward process additionally we adjust tion at sentence boundaries realization becomes more involved if we introduce special tags such as pronominalize mentioned in section for this tag we would need to look up the gender of the tagged entity from a knowledge base having a separate realization step is benecial since we can decide to pronominalize only when condent about the appropriate pronoun and can otherwise leave the entity mention untouched another advantage of having a separate consuming the embedding of the previously dicted label and the activations from the encoder there are several ways in which the decoder can communicate with the encoder i through a full attention over the sequence of encoder activations similar to conventional architectures and ii by directly consuming the encoder activation at the current step in our preliminary experiments we found the latter option to perform better and converge faster as it does not require learning ditional encoder decoder attention weights we experiment with both decoder variants forward and autoregressive and nd that the toregressive decoder outperforms the previously used feedforward decoder in the rest of this paper the tagging model with an autoregressive decoder is referred to as lasertaggerar and the model with feedforward decoder as lasertaggerff experiments we evaluate our method by conducting experiments on four different text editing tasks sentence sion split and rephrase abstractive tion and grammatical error correction baselines in addition to reporting previously published results for each task we also train a set of strong baselines based on transformer where both the encoder and decoder replicate the base architecture devlin et al to have a fair comparison similar to how we initialize a tagger encoder with a pretrained bert checkpoint we use the same initialization for the transformer encoder this produces a very strong line which already results in new state of the art metrics on two out of four tasks sentence fusion sentence fusion is the problem of fusing sentences into a single coherent sentence data we use the balanced wikipedia tion of geva et al discofuse dataset for our experiments henceforth dfwiki out of the m fusion examples in the dataset require reordering of the input to cope with this we duce the swap tag which enables the model to ip the order of two input sentences we construct the phrase vocabulary as described in sec using the validation set of k examples the top phrases are shown in the rst column of table evaluation metrics following geva et al we use two evaluation metrics exact score figure the architecture of lasertaggerar tion step is that specic loss patterns can be dressed by adding specialized realization rules for instance one could have a rule that when applying tag hisdelete to an entity mention followed by s the realizer must always delete the possessive s regardless of its predicted tag tagging model architecture our tagger is composed of two components an encoder which generates activation vectors for each element in the input sequence and a decoder which converts encoder activations into tag labels encoder we choose the bert transformer model devlin et al as our encoder as it demonstrated state of the art results on a ber of sentence encoding tasks we use the bert base architecture which consists of self attention layers we refer the reader to devlin et al for a detailed description of the model architecture and its input representation we ize the encoder with a publicly available checkpoint of the pretrained case sensitive bert base model in the original bert paper a ple decoding mechanism is used for sequence the output tags are generated in a single ging feed forward pass by applying an argmax over the encoder logits in this way each output tag is predicted independently without modelling the pendencies between the tags in the sequence such a simple decoder demonstrated state of the art sults on the named entity recognition task when applied on top of the bert encoder decoder to better model the dependencies between the output tag labels we propose a more powerful toregressive decoder specically we run a layer transformer decoder on top of the bert encoder see fig at each step the decoder is com google research bert layer dfwiki wikisplit model exact sari and however but he because although but and although his while it which she he it the and was is she it is a they however he as the a and is in s with for of nt an gec the a to in of on at for have is was and that table the most frequently added phrases in the datasets studied in this work in order of decreasing quency marks a sentence boundary is short for abstractive summarization grammatical error correction figure performance of model lasertaggerar on the dfwiki dataset conditioned on the vocabulary size and the gold score i e the percentage of examples that can be reconstructed via text edit operations which is the percentage of exactly correctly dicted fusions and sari xu et al which computes the average scores of the added kept and deleted n grams vocabulary size to understand the impact of the number of phrases we include in the vocabulary we trained models for different vocabulary sizes only lasertaggerar the results are shown in figure after increasing the vocabulary size to phrases exact score reaches a plateau so we set the vocabulary size to in all the ing experiments of this paper the gold curve in fig shows that this vocabulary size is sufcient use the implementation available at git setting for deletion geva et al smaller datasets a smaller vocabulary size may yield better results but for simplicity we do not optimize the size separately for each dataset transformer geva et al lasertaggerar no swap lasertaggerff lasertaggerar table sentence fusion results on dfwiki to cover of the training examples which gives us an upper bound for the exact score comparison against baselines table lists the results for the dfwiki dataset we obtain new sota results with lasertaggerar ing the previous sota layer transformer model from geva et al by exact score and sari score we also nd that the pretrained model yields nearly as good formance demonstrating the effectiveness of supervised pretraining for generation tasks the performance of the tagger is impaired signicantly when leaving out the swap tag due to the model s inability to reconstruct of the training set impact of dataset size we also study the fect of the training data size by creating four creasingly smaller subsets of dfwiki see fig when data size drops to or examples lasertagger still performs surprisingly well clearly outperforming the baseline split and rephrase the reverse task of sentence fusion is the and rephrase task which requires rewriting a long sentence into two or more coherent short sentences data we use the wikisplit dataset botha et al which consists of m human editor ated examples of sentence splits and follow the dataset split suggested by the authors using the phrase vocabulary of size yields a age of the targets from the training set top phrases shown in table the lower coverage compared to dfwiki suggests a higher amount of noise due to wikipedia author edits unrelated to splitting results botha et al report results ing a one layer bi directional lstm cell size with attention and a copying mechanism see simplicity we use the same phrase vocabulary of size computed using the validation set of k examples for all experiments note that even though some subsampled training sets contain less than k examples using the same vocabulary does not give the taggers an unfair advantage over the baselines because the tagger will never predict a phrase it has not seen in the training data vocabulary scoregoldlasertaggerar sentence fusion on dfwiki split and rephrase on wikisplit figure sari score as a function of the training data size for three models unless we have tens of thousands of training examples the tagging approach clearly outperforms the baseline model bleu exact sari model exact sari rouge l botha et al lasertaggerff lasertaggerar table results on the wikisplit dataset filippova et al clarke and lapata cohn and lapata rush et al lasertaggerff lasertaggerar table results on summarization et al the results are shown in table and lasertaggerar yield similar performance with each other and they both form the model with a copying mechanism from botha et al we again studied the impact of training data size by subsampling the training set see ure similar to the previous experiment the lasertagger methods degrade more gracefully when reducing training data size and start to perform the baseline once going below circa examples the smallest training set for lasertaggerar contains merely examples remarkably the model is still able to learn thing useful that generalizes to unseen test ples reaching a sari score of and dicting of the targets exactly correctly the following is an example prediction by the model source delhi public library is a national depository library in delhi india it has over branches across the state prediction delhi public library is a national depository brary in delhi india it has over branches across the state here the model has picked the right comma to place with a period and a sentence separator et al report only bleu but they kindly shared with us their model s predictions allowing us to pute the exact and sari score for their method similar to their work we used nltk for the bleu computation abstractive summarization the task of summarization is to reduce the length of a text while preserving its meaning dataset we use the dataset from toutanova et al which contains short input texts one or two sentences and one or more written summaries the human experts were not restricted to just deleting words when generating a summary but were allowed to also insert new words and reorder parts of the sentence which makes this dataset particularly suited for tive summarization models we set the size of the phrase vocabulary to as for the other tasks and extract the phrases from the training partition with a size of we are able to cover of the training data evaluation metrics in addition to the metrics from the previous sections we report rouge l lin as this is a metric that is commonly used in the summarization literature rouge l is a recall oriented measure computed as the longest common sub sequence between a reference summary and a candidate summary results table compares our taggers against baselines and systems from the ture filippova et al and clarke and pata proposed deletion based approaches are extracted from toutanova et al of training of training the former uses a network the latter mulates summarization as an optimization lem that is solved via integer linear programming cohn and lapata proposed an early proach to abstractive summarization via a tree transducer rush et al developed a neural model for abstractive tion in line with the results on the subsampled sion splitting datasets figure the tagger nicantly outperforms all baselines this shows that even though a text editing approach is not suited for extreme summarization examples a plete paraphrase with zero lexical overlap in tice already a limited paraphrasing capability is enough to reach good empirical performance note that the low absolute values for the exact metric are expected since there is a very large number of acceptable summaries grammatical error correction gec gec requires systems to identify and x ical errors in a given input text data we use a recent benchmark from a shared task of the building educational applications workshop specically from the low resource bryant et al the publicly able set has ill formed sentences together with gold error corrections which we split into a training and validation partition we again create the phrase vocabulary from the most frequently added phrases in the training partition which gives us a coverage of of the training data evaluation metrics and results we report precision and recall and the task s main metric which gives more weight to the precision of the corrections than to their recall table compares our taggers against two lines again the tagging approach clearly performs the bert based model here by being more than seven times as accurate in the diction of corrections this can be accounted to the model s much richer generation capacity which the model can not properly tune to the task at hand given the small amount of training data the tagging approach on the other hand is naturally suited to this kind of problem we also report the best performing method by grundkiewicz et al from the shared task for informational purposes they train a transformer cl cam ac uk research nl model grundkiewicz et al lasertaggerff lasertaggerar p r table results on grammatical error correction note that grundkiewicz et al augment the training dataset of examples by million synthetic amples and million wikipedia edits batch size lasertaggerff lasertaggerar table inference time in ms across various batch sizes on gpu nvidia tesla averaged across runs with random inputs model using a dataset which is augmented by million synthetic examples and million wikipedia edits whereas we only use sentences from the provided training dataset inference time getting state of the art results often requires ing larger and more complex models when ning a model in production one cares not only about the accuracy but also the inference time ble reports latency numbers for lasertagger models and our most accurate baseline as one can see the baseline is practical to run in production even for the batch size on the other hand for a batch size lasertaggerar is already faster than comparable in accuracy line this difference is due to the former model using a layer decoder instead of layers and no encoder decoder cross attention we also tried training with a layer decoder but it performed very poorly in terms of accuracy nally lasertaggerff is more than faster while being only a few accuracy points below our best reported results qualitative evaluation to assess the qualitative difference between the outputs of lasertagger and we analyzed the texts generated by the models on the test sets of the four tasks we inspected the tive worst predictions from each model according to bleu and identied seven main error patterns error type lasertagger example imaginary words repeated phrases not affected not affected premature end of sentence less affected hallucinations less affected coreference issues misleading rephrasing lazy sentence splitting affected affected affected affected affected affected affected affected affected not affected zenica cyrillic is zenica cyrillic gratulation is i m your employee to serve on your company i m your company to serve on your company by the way my favorite football team is manchester united they in out in out in out by the way my favorite football team is tobacco smokers may also experience in anthropology smokers may also experience out in she is the daughter of alistair crane who secretly built out she is the daughter of alistair crane she secretly built postal service was in no way responsible in postal service was responsible out in home world of the marglotta located in the sagittarius arm out home world of the marglotta located in the sagittarius arm table main error patterns observed in the output of the tagging and models on their test sets all tasks two of which are specic to the model and one being specic to lasertagger this illustrates that lasertagger is less prone to errors compared to the standard proach due to the restricted exibility of its model certain types of errors namely imaginary words and repeated phrases are virtually impossible for the tagger to make the likelihood of others such hallucination and abrupt sentence ending is at least greatly reduced in table we list the error classes and refer to appendix a for more details on our observations conclusions we proposed a text editing approach to generation tasks with high overlap between input and output texts compared to the models typically applied in this setting our approach sults in a simpler sequence tagging problem with a much smaller output tag vocabulary we strated that this approach has comparable mance when trained on medium to large datasets and clearly outperforms a strong baseline when the number of training examples is limited qualitative analysis of the model outputs suggests that our tagging approach is less affected by the common errors of the models such as lucination and abrupt sentence ending we further demonstrated that tagging can speed up inference by more than two orders of magnitude making it more attractive for production applications limitations arbitrary word reordering is not feasible with our approach although limited ordering can be achieved with deletion and tion operations as well as custom tags such as swap see section to enable more exible reordering it might be possible to apply techniques developed for phrase based machine translation another limitation is that our approach may not be straightforward to apply to languages that are morphologically richer than english where a more sophisticated realizer might be needed to adjust e the cases of the words in future work we would like to experiment with more light weight tagging architectures dor et al to better understand the trade off between inference time and model accuracy acknowledgments we would like to thank enrique alfonseca idan szpektor and orgad keller for useful discussions references daniel andor chris alberti david weiss aliaksei severyn alessandro presta kuzman ganchev slav petrov and michael collins globally in ized transition based neural networks ings of the annual meeting of the association for computational linguistics volume long pers pages jan a botha manaal faruqui john alex jason baldridge and dipanjan das learning to split and rephrase from wikipedia edit history in proceedings of the conference on empirical methods in natural language processing christopher bryant mariano felice istein e dersen and ted briscoe the shared task on grammatical error correction in ceedings of the fourteenth workshop on innovative use of nlp for building educational applications pages shamil chollampatt and hwee tou ng neural quality estimation of grammatical error correction in proceedings of the conference on cal methods in natural language processing pages james clarke and mirella lapata global ference for sentence compression an integer j artif intell res ear programming approach trevor cohn and mirella lapata sentence in proceedings pression beyond word deletion of the international conference on tional linguistics coling pages franck dernoncourt mohammad ghassemi and ter chang a repository of corpora for marization in proceedings of the eleventh tional conference on language resources and uation jacob devlin ming wei chang kenton lee and kristina toutanova bert pre training of deep bidirectional transformers for language standing in proceedings of the conference of the north american chapter of the association for computational linguistics human language nologies volume long and short papers pages yue dong zichao li mehdi rezagholizadeh and jackie chi kit cheung editnts an neural programmer interpreter model for sentence in proceedings of cation through explicit editing the annual meeting of the association for putational linguistics katja filippova enrique alfonseca carlos a menares lukasz kaiser and oriol vinyals sentence compression by deletion with lstms in proceedings of the conference on empirical methods in natural language processing pages katja filippova and michael strube dency tree based sentence compression in ings of the fifth international natural language generation conference pages tao ge furu wei and ming zhou fluency boost learning and inference for neural cal error correction in proceedings of the nual meeting of the association for computational linguistics volume long papers pages jiatao gu zhengdong lu hang li and victor ok incorporating copying mechanism in li in proceedings of sequence to sequence learning the annual meeting of the association for putational linguistics volume long papers pages jiatao gu changhan wang levenshtein transformer and jake zhao arxiv preprint zhijing jin di jin jonas mueller nicholas matthews and enrico santus unsupervised text style transfer via iterative matching and translation arxiv preprint hongyan jing and kathleen mckeown cut and in meeting of paste based text summarization the north american chapter of the association for computational linguistics marcin junczys dowmunt and roman grundkiewicz the amu system in the shared task grammatical error correction by intensive and feature rich statistical machine in proceedings of the eighteenth lation ence on computational natural language learning shared task pages marcin junczys dowmunt roman grundkiewicz shubha guha and kenneth heaeld proaching neural grammatical error correction as a low resource machine translation task in ings of the conference of the north american chapter of the association for computational guistics human language technologies volume long papers pages sudhanshu kasewa pontus stenetorp and sebastian riedel wronging a right generating ter errors to improve grammatical error detection in proceedings of the conference on cal methods in natural language processing pages kevin knight and ishwar chander automated in proceedings of the postediting of documents national conference on articial intelligence seattle wa usa july august volume pages mor geva eric malmi idan szpektor and jonathan berant discofuse a large scale dataset in for discourse based sentence fusion ings of the conference of the north american chapter of the association for computational guistics human language technologies volume long and short papers pages chin yew lin rouge a package for in text matic evaluation of summaries rization branches out proceedings of the workshop pages yang liu fine tune bert for extractive rization arxiv preprint roman grundkiewicz marcin junczys dowmunt and kenneth heaeld neural grammatical error correction systems with unsupervised pre training on synthetic data in proceedings of the fourteenth workshop on innovative use of nlp for building ucational applications pages amit moryossef yoav goldberg and ido dagan step by step separating planning from realization in neural data to text generation in proceedings of the conference of the north american ter of the association for computational linguistics human language technologies volume long and short papers pages shashi narayan shay b cohen and mirella lapata ranking sentences for extractive tion with reinforcement learning in proceedings of the conference of the north american ter of the association for computational linguistics human language technologies volume long pers pages hwee tou ng siew mei wu ted briscoe christian hadiwinoto raymond hendy susanto and pher bryant the shared task on grammatical error correction in proceedings of the eighteenth conference on computational ral language learning shared task pages hwee tou ng siew mei wu yuanbin wu christian hadiwinoto and joel tetreault the shared task on grammatical error correction in proceedings of the seventeenth conference on computational natural language learning shared task pages nikola i nikolov and richard hr hahnloser large scale hierarchical alignment for author style transfer arxiv preprint romain paulus caiming xiong and richard socher a deep reinforced model for abstractive marization arxiv preprint vinodkumar prabhakaran camilla grifths hang su prateek verma nelson morgan jennifer l hardt and dan jurafsky detecting tional dialog acts in police trafc stops tions of the association for computational tics ratish puduppully li dong and mirella lapata data to text generation with content selection and planning arxiv preprint sudha rao and joel tetreault dear sir or madam may i introduce the gyafc dataset pus benchmarks and metrics for formality style transfer in proceedings of the conference of the north american chapter of the association for computational linguistics human language nologies volume long papers pages marek rei semi supervised multitask in proceedings of the ing for sequence labeling annual meeting of the association for tational linguistics volume long papers pages marek rei mariano felice zheng yuan and ted briscoe articial error generation with chine translation and syntactic patterns in ings of the workshop on innovative use of nlp for building educational applications pages alla rozovskaya kai wei chang mark sammons dan roth and nizar habash the columbia system in the shared task in proceedings of the eighteenth conference on computational natural language learning shared task pages alexander m rush sumit chopra and jason weston a neural attention model for abstractive in proceedings of the tence summarization conference on empirical methods in natural guage processing pages abigail see peter j liu and christopher d manning get to the point summarization with generator networks in proceedings of the nual meeting of the association for computational linguistics volume long papers pages ilya sutskever oriol vinyals and quoc v le sequence to sequence learning with neural networks in advances in neural information processing tems pages jiwei tan xiaojun wan and jianguo xiao stractive document summarization with a in proceedings based attentional neural model of the annual meeting of the association for computational linguistics volume long papers pages kristina toutanova chris brockett ke m tran and saleema amershi a dataset and evaluation metrics for abstractive compression of sentences and short paragraphs in proceedings of the ference on empirical methods in natural language processing pages staal a vinterbo a note on the hardness of the k ambiguity problem technical report dsg t sam wiseman stuart m shieber and alexander m rush learning neural templates for text eration arxiv preprint kelvin xu jimmy ba ryan kiros kyunghyun cho aaron courville ruslan salakhudinov rich zemel and yoshua bengio show attend and tell neural image caption generation with visual in proceedings of the international tion conference on machine learning volume of proceedings of machine learning research pages wei xu courtney napoles ellie pavlick quanze chen and chris callison burch optimizing statistical machine translation for text simplication transactions of the association for computational linguistics wei xu courtney napoles ellie pavlick quanze chen and chris callison burch optimizing statistical machine translation for text simplication transactions of the association for computational linguistics xingxing zhang and mirella lapata sentence simplication with deep reinforcement learning in proceedings of the conference on empirical methods in natural language processing pages wei zhao liang wang kewei shen ruoyu jia and jingming liu improving grammatical error correction via pre training a copy augmented tecture with unlabeled data in proceedings of the conference of the north american chapter of the association for computational linguistics man language technologies volume long and short papers pages zhemin zhu delphine bernhard and iryna gurevych a monolingual tree based translation model in proceedings of the for sentence simplication international conference on computational linguistics coling pages appendix a examples from qualitative analysis in table we present cherry picked examples for the error patterns introduced in section imaginary words operating on a sub word level the model is capable of ing nonexistent words by concatenating unrelated wordpieces indeed we have encountered ples of such made up words in the outputs of the model on all tasks typically this pens when the model encounters a rare word in the input text lasertagger which is trained on the word level is immune to this specic type of error premature end of sentence a model produces sequences of arbitrary length by ing the end of sentence eos we have seen the model generate eos prematurely resulting in an abrupt sentence ending in extreme cases and especially on the abstractive summarization task the model generated eos at the start effectively producing an empty output for lasertagger this type of error is technically possible but very unlikely and we have not seen it in practice the tagger would need to generate a long sequence of delete tags something it has not seen in the training data repeated phrases another type of error is repetition of cic to the model information either single words or entire phrases in the sentence splitting task the model would often repeat parts of the sentence twice fore and after the splitting symbol in the grammar correction and summarization tasks the model would often replace a rare word with another word from the input sentence thus repeating that word twice lasertagger can only add words or phrases from its limited vocabulary which is unlikely to cause repetition we observed that the model was cially likely to repeat large fragments in the tence splitting task in cases when there is no ous good way to split the sentence interestingly in most of these cases lasertagger did not split the sentence at all by not inserting the splitting symbol even though such examples were not present in its training data in other cases lasertagger produced a lazy split discussed below hallucination is a known problem for neural works but lasertagger is susceptible to it to a much lesser degree lasertagger can lucinate only by inserting an unexpected word or a short phrase from its vocabulary we have seen such insertions in the tagger output that dered the sentence ungrammatical or simply odd a model is more likely to produce tly misleading hallucinations that misrepresent the input text while looking uent and credible we have seen examples of the model changing the factual details which is a more dangerous error to make in some scenarios coreference problems this type of errors is ten made by both and tagger models in the most typical instance a model inserts an rect pronoun in other cases the model makes an incorrect coreference resolution by inserting the wrong proper noun the model is more susceptible to the second type of error because of its ability to copy proper nouns from the input tence lasertagger will typically not attempt to resolve the coreference and just leave the pronoun intact misleading rephrasing through deletion we want to highlight that although lasertagger has a limited set of operations and can not insert arbitrary phrases it is not immune to semantic rors and misrepresenting the input text we have seen examples where deletion of word spans was sufcient to completely alter the meaning of the sentence a model is naturally also prone to this type of errors lazy splitting this type of error is specic to lasertagger and the sentence splitting task it occurs when the input sentence is split arbitrarily in the middle without any modications made to the resulting two parts such splits were usually made at a reasonable point in the input sentence i e yielding two valid grammatical clauses at least one of which however was not a complete sentence imaginary words introduced by a sentence splitting task input zenica cyrillic is an industrial city the third largest after sarajevo and banja luka in bosnia and herzegovina and the capital of the zenica doboj canton of the federation of bosnia and herzegovina entity output zenica cyrillic gratulation is an industrial city the third largest after sarajevo and banja luka in bosnia and herzegovina it is the capital of the zenica doboj canton of the federation of bosnia and herzegovina entity imaginary words indroduced by a sentence fusion task input output carboxysomes which aid carbon xotrophically and mixotrophically grown cells carboxysomes are found in lithoautotrophically and mixotrophically grown cells carboxysomes aid carbon xation abrupt sentence end by the model grammar correction task input by the way my favorite football team is manchester united they are brilliant they have an amazing football players and they are awesome output by the way my favorite football team is abrupt sentence end by the model summarization task input it is i suppose likely that people with little money must think of money more than the well off do and that the hungry will dwell on thoughts of food and the thirsty on drink it output redundant repetition by the model sentence splitting task input output bethany noel mota born november is an american video blogger from los banos california bethany noel mota born november is an american video blogger from los banos california mota is an american video blogger from los banos california redundant repetition by the model sentence fusion task input general fazlollah zahedi became prime minister general fazlollah zahedi led the military coup output general fazlollah zahedi who led the military coup became prime minister and became prime minister redundant repetition by the model grammar correction task input output i m your employee to serve on your company i m your company to serve on your company hallucinations produced by the tagger grammar correction task input output if you are interesedt nd me at clock near the cinema entrance if you are play nd me at clock near the cinema entrance hallucinations produced by the tagger sentence fusion task input he carried a bow he was nerva output receiving a bow he was nerva hallucinations produced by the model sentence splitting task input output the native block size is bytes inodes are bytes on disk the native block size is bytes inodes are bytes on disk hallucinations produced by the model sentence fusion task input tobacco smokers may also experience more powerful effects with psilocybin tobacco smoke exposure decreases the activity of mao in the brain and peripheral organs anthropology smokers may also experience more powerful effects with psilocybin output coreference error made by the model sentence fusion task input he tries to help yuushi with catching sayuki s eye and encourages the idea of a baseball club output he tries to help yuushi with catching sayuki s eye yuushi encourages the idea of a baseball club pronominalization error made by the tagger sentence fusion task input output sharmistha mukherjee is an indian kathak dancer choreographer and politician he is of the indian sharmistha mukherjee is an indian kathak dancer choreographer and politician of the indian national congress national congress lazy splitting made by the tagger sentence splitting task input home world of the marglotta located in the sagittarius arm output home world of the marglotta located in the sagittarius arm lazy splitting made by the tagger sentence splitting task input output jesse has a son named sahvere and a duaghter named jaylen jesse has a son named sahvere and a duaghter named jaylen misleading rephrasing through deletion made by the tagger sentence splitting task input output she has an ex lover winnie mann a son wilson she was adopted chinese girl jang yin she has an ex lover winnie mann with whom she has a son wilson and an adopted a chinese girl jang yin misleading rephrasing through deletion made by the tagger sentence splitting task input the article proudly notes that the postal service was in no way responsible for the crash of valujet flight in the florida everglades output the article notes postal service was responsible for the crash of valujet flight table illustration of the typical errors produced by the models cherry picked examples
