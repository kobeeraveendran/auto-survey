v o n l c s c v v x r generative adversarial network abstractive text summarization linqing yao min qiang jia hongyan institutes advanced technology chinese academy sciences machine intelligence institute computer science south china normal university key laboratory machine perception peking university likicode min com ac scnu edu pku edu abstract paper propose adversarial process tive text summarization simultaneously train generative model g discriminative model d ticular build generator g agent ment learning takes raw text input predicts abstractive summarization build discriminator attempts distinguish generated summary ground truth summary extensive experiments strate model achieves competitive rouge scores state art methods cnn daily mail dataset qualitatively model able generate abstractive readable diverse introduction abstractive text summarization task generating short concise summary captures salient ideas source text generated summaries tially contain new phrases sentences pear source text past decades urry studies conducted abstractive text rization nallapati et al liu manning paulus xiong socher despite remarkable progress previous studies abstractive summarization challenged neural sequence sequence models tend generate trivial generic summary involving high frequency phrases generated summaries limited grammaticality readability previous work standard sequence sequence models trained predict word summary likelihood estimation mle objective function strategy major shortcomings ation metric different training loss second input decoder time step true summary training testing phase input time step previous word work partially supported cas pioneer talents program moe key laboratory machine ception peking university grant number q qu corresponding author copyright association advancement articial intelligence www aaai org rights reserved material com generated decoder exposure bias leads error accumulation test time address challenge paper propose adversarial framework jointly train generative model g discriminative model d specically generator g takes original text input generate summary use reinforcement learning e policy gradient mize g highly rewarded summary effectively bypasses exposure bias non differentiable task metrics issues implement discriminator d text classier learns classify generated summaries machine human generated generator g tor d optimized minimax player game discriminator d tries distinguish ground truth maries generated summaries generator g training procedure generator g maximize probability d making mistake ial process eventually adjust g generate plausible high quality abstractive summaries training model similar strategy standard goodfellow et al simultaneously train models adversarial manner generative model g discriminative model d rst pre train generative model generating summaries given source text pre train discriminator providing positive examples human generated summaries negative examples produced pre trained generator pre training generator discriminator trained alternatively generative model generator takes source text input predicts summary y ym n length source text m length predicted summary use text directional lstm encoder convert input sequence hidden states hn lowing liu manning time step t attention based lstm decoder compute hidden state st decoder context vector reader refer supplement paper liu manning implementation tails parameters generator g collectively resented context vector ct concatenated decoder state st fed fully connected layer softmax layer produce probability predicting word target vocabulary time step t pvocab yt sof v st v v learnable parameters similar work liu manning incorporate switching pointer generator network use word erator xed vocabulary pointer copying rare seen input sequence finally nal probability p yt token yt summary discriminative model discriminator binary classier aims guishing input sequence originally generated mans synthesized machines encode input quence cnn shows great effectiveness text classication kim use multiple lters ing window sizes obtain different features apply max time pooling operation features pooled features passed fully connected softmax layer output probability original updating model parameters adversarial process discriminator ward function improve generator iteratively dynamically updating discriminator obtain realistic high quality summaries generated erator g train discriminator min ey pdata ey g discriminator d obtained xed ready update generator g loss function generator g consists parts loss computed policy gradient denoted jpg maximum likelihood loss denoted jml formally objective function g j jpg scaling factor balance magnitude difference jpg jml cording policy gradient theorem sutton et al compute gradient jpg w t parameters t t x x yt t t x jpg r g d x x eyt g rg d x log x rg d x yt action value function rg d x t t length text update parameters stochastic gradient descent t generated summary time step t x source text condensed experiments cnn daily mail corpus dataset dataset nallapati et al widely abstractive summarization comprises news stories cnn daily methods abs pgc deeprl pretrain rouge l human table quantitative evaluation results mail websites paired multi sentence human generated abstractive summaries contains training pairs validation pairs test pairs experimental results compare approach including abstractive model pointer generator liu manning deeprl methods abs nallapati et al pgc erage abstractive deep reinforced model paulus xiong socher version networks rstly compare model pre trained erator adversarial training rouge l increase absolute points respectively addition model exhibits competitive rouge scores state art methods ically approach achieves best scores perform human evaluation evaluate ability quality summaries randomly select test examples dataset example human evaluators asked rank summary generated models based readability indicates lowest level readability indicates highest level observe table model contributes nicantly improving readability summaries evaluate proposed model qualitatively port generated summaries supplementary les conclusion paper proposed adversarial process tive text summarization experimental results showed model generate abstractive readable verse summaries references goodfellow et al goodfellow pouget abadie j mirza m xu b warde farley d ozair s courville bengio y generative adversarial nets nips kim kim y convolutional neural networks sentence classication arxiv preprint nallapati et al nallapati r zhou b gulcehre c xiang b et al abstractive text summarization ing sequence sequence rnns arxiv preprint paulus xiong socher paulus r xiong c socher r deep reinforced model abstractive summarization arxiv preprint liu manning liu p j point arxiv preprint ning c d tion pointer generator networks sutton et al sutton r s mcallester d singh s p mansour y policy gradient methods inforcement learning function approximation nips
