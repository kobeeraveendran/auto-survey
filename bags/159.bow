adapting neural encoder decoder framework single multi document summarization logan lebanoff kaiqiang song fei liu department computer science university central florida orlando fl usa loganlebanoff ucf edu ucf edu g u l c s c v v x r abstract generating text abstract set ments remains challenging task neural encoder decoder framework recently exploited summarize single documents success attributed ability large parallel data automatically quired web contrast parallel data multi document summarization scarce costly obtain pressing need adapt encoder decoder model trained single document summarization data work multiple document input paper present initial investigation novel adaptation method exploits maximal marginal relevance method select tative sentences multi document input leverages abstractive encoder decoder model fuse disparate sentences stractive summary adaptation method robust requires training data system compares favorably state art extractive abstractive approaches judged automatic metrics human assessors introduction neural abstractive summarization primarily focused summarizing short texts written gle authors example sentence summarization seeks reduce rst sentence news article title like summary rush et al pati et al takase et al song et al single document summarization sds cuses condensing news article handful bullet points paulus et al et al summarization studies ered large parallel datasets automatically vested online news outlets including word rush et al cnn daily mail mann et al nyt sandhaus newsroom grusky et al date multi document summarization mds fully beneted development dataset source summary pairs gigaword rush et al cnn daily mail hermann et al tac dang et al duc yen news article rst sentence news article words title like words multi sent news articles words related topic multi sent news articles words related topic multi sent million k table comparison datasets available sent marization gigaword single doc cnn dm multi doc summarization duc tac labelled data multi doc summarization neural encoder decoder models mds seeks condense set documents likely written tiple authors short informative summary practical applications summarizing product reviews gerani et al student sponses post class questionnaires luo man luo et al sets news cles discussing certain topics hong et al state art mds systems tive nenkova mckeown despite promising results systems perform text abstraction e paraphrasing generalization sentence fusion jing mckeown annotated mds datasets scarce containing hundreds training pairs ble cost create ground truth summaries multiple document inputs prohibitive mds datasets small train neural encoder decoder models millions parameters overtting promising route generating abstractive summary multi document input apply neural encoder decoder model trained document summarization mega document created concatenating documents set test time nonetheless model scale reasons identifying portant text pieces mega document challenging encoder decoder model trained single document summarization data summary worthy content tained rst sentences article case mega document second dundant text pieces mega document peatedly summary generation current framework attention mechanism encoder decoder model bahdanau et al position based lacks awareness mantics text piece attended ing summary generation unlikely attention value assigned similar text piece different position fected content repeatedly summary generation issues alleviated improving encoder decoder architecture attention mechanism cheng lapata tan et al cases model trained large scale mds datasets available current stage increasing need lightweight adaptation encoder decoder model trained sds datasets work document inputs test time paper present novel adaptation method named pg mmr generate abstracts multi document inputs method bust requires mds training data bines recent neural encoder decoder model pg pointer generator networks et al generates abstractive summaries document inputs strong extractive rization algorithm mmr maximal marginal relevance carbonell goldstein identies important source sentences document inputs pg mmr algorithm tively performs following identies ful important sentences document attention weights pg model directly modied focus important sentences generating summary sentence system identies number tant sentences likelihood choosing tain sentences reduced based ity partially generated summary ducing redundancy research contributions clude following present investigation novel tion method encoder decoder framework multi document summarization best knowledge rst tempt couple maximal marginal relevance algorithm pointer generator networks multi document summarization demonstrate effectiveness posed method extensive experiments standard mds datasets system compares favorably state art extractive stractive summarization systems measured automatic metrics human judgments related work popular methods multi document tion extractive important sentences extracted set source documents tionally compressed form summary daume iii marcu zajic et al gillick favre galanis androutsopoulos berg kirkpatrick et al li et al thadani mckeown wang et al yogatama et al filippova et al rett et al recent years neural networks exploited learn word sentence resentations multi document marization cheng lapata cao et al isonuma et al yasunaga et al narayan et al approaches remain extractive despite encouraging results marizing large quantity texts requires phisticated abstraction capabilities alization paraphrasing sentence fusion prior deep learning abstractive tion investigated barzilay et al carenini cheung ganesan et al gerani et al fabbrizio et al pighin et al bing et al liu et al liao et al approaches construct domain templates text planner open tem employ natural language generator surface realization limited availability labelled data experiments performed small domain specic datasets neural abstractive summarization utilizing encoder decoder architecture shown ing results studies focus primarily document summarization nallapati et al kikuchi et al chen et al miao blunsom tan et al zeng et al zhou et al paulus et al et al gehrmann et al ing mechanism gulcehre et al gu et al allows summarization system copy words source text generate new words vocabulary reinforcement learning exploited directly optimize tion metrics paulus et al kryscinski et al chen bansal studies cus summarizing single documents cause training data abundant work baumel et al zhang et al related particular baumel et al propose extend abstractive marization system generate query focused maries zhang et al add document set coder hierarchical summarization work exceptions little research dedicated investigate feasibility extending encoder decoder framework erate abstractive summaries multi document inputs available training data scarce paper presents rst steps goal extending encoder decoder model multi document setting introduce tation method combining pointer generator pg networks et al maximal marginal relevance mmr algorithm carbonell goldstein pg model trained sds data detailed section capable generating document abstracts performing text abstraction sentence fusion model applied test time rize multi document inputs tions pg mmr algorithm presented tion teaches pg model effectively nize important content input documents improving quality abstractive maries requiring training document inputs limits encoder decoder model encoder decoder architecture standard neural abstractive rization rush et al encoder bidirectional lstm hochreiter ber converting input text set den states input word indexed decoder unidirectional lstm generates summary predicting word time decoder hidden states represented hd t indexed t sentence document summarization nallapati et al paulus et al et al input text treated sequence words model expected capture source syntax inherently t et t attention weight t measures tant th input word generating t th output word eq following et al t calculated measuring strength interaction decoder hidden state t encoder hidden state hd tive attention eq denotes lative attention th input word receives time step large value indicates th input word prior time t unlikely generating t th output word context vector ct constructed eq summarize semantic meaning input weighted sum encoder hidden states context vector decoder hidden state hd t compute vocabulary probability measuring likelihood vocabulary word w selected t word eq ct t t encoder decoder models switch estimated pgen indicate system chosen select word cabulary copy word input text eq switch computed ward layer activation hd embedding output word time attention weights t compute copy probability eq word w appears input text copy probability wi w t sum attention weights occurrences nal probability p w weighted combination vocabulary probability copy bility cross entropy loss function train model end end pgen t p t wi w thoroughly understand aforementioned encoder decoder model divide model rameters groups include parameters encoder decoder wz bz calculating switch eq represents concatenation vectors pointer generator networks et al use linear layers produce vocabulary distribution use wy denote parameters layers figure system framework pg mmr system uses k highest scored source sentences case guide pg model generate summary sentence source sentences muted process best viewed color wy calculating eq v attention weights eq training encoder decoder model document summarization sds data containing large collection news articles paired maries hermann et al model eters effectively learned test time wish model generate abstractive summaries document inputs brings issues parameters ineffective identifying salient content multi document inputs humans good identifying representative sentences set documents fusing abstract capability supported encoder decoder model second tion mechanism based input word positions semantics lead redundant content multi document input edly summary generation ture aspects addressed ducing external model selects tative sentences multi document inputs dynamically adjusts sentence importance duce summary redundancy external model integrated encoder decoder model erate abstractive summaries selected sentative sentences following section present adaptation method multi document summarization method maximal marginal relevance adaptation method incorporates maximal marginal vance algorithm mmr carbonell goldstein pointer generator networks pg et al adjusting network s attention ues mmr successful extractive approaches despite straightforwardness performs par state art systems luo litman yogatama et al iteration mmr selects sentence ument d includes summary s length threshold reached selected tence important remaining sentences content overlap current summary equation d measures similarity sentence document serves proxy sentence importance important sentences usually similarity centroid ument maxsj s sj measures imum similarity sentence si summary sentences acting proxy dancy balancing factor argmax d importance sjs sj redundancy pg mmr describes iterative framework summarizing multi document input mary consisting multiple sentences eration pg mmr follows mmr principle select k highest scored source sentences serve basis pg generate summary sentence scores source tences updated based importance redundancy sentences highly similar partial summary receive lower scores ing k sentences mmr algorithm helps pg system effectively identify salient source content included summary muting allow pg system effectively utilize k source sentences retraining neural model dynamically adjust pg attention weights t test time let sk sent encoderneural decoderdocument scoressumm sent sent resent selected sentence attention weights words belonging calculated eq words tences forced receive zero attention weights t t renormalized eq new t t means remaining sentences muted process variant sentence tance affect original attention weights muting alternative setting sentence salience multiplied word salience ized eq pg uses reweighted alpha ues predict summary word new t t sentence importance estimate sentence portance d introduce supervised regression model work importantly model trained single document tion datasets training data abundant test time model applied identify portant sentences multi document input model determines sentence importance based indicators inspired humans identify important sentences document set clude sentence length absolute tive position document c sentence quality close sentence main topic document set features considered important indicators previous extractive summarization framework galanis sopoulos hong et al n sentence quality c age pg model build sentence tation use bidirectional lstm encoder encode source sentence vector concatenation sentation hidden states forward backward passes document vector average sentence vectors use document vector cosine similarity document sentence vectors indicator d support tor regression model trained sentence score pairs training data obtained cnn daily mail dataset target importance score rouge l recall sentence pared ground truth summary model chitecture leverages neural representations algorithm pg mmr algorithm rizing multi document inputs input sds data mds source sentences si train pg model sds data importance dundancy scores source sentence si source sentences source sentences summary t index summary words t lmax find highest mmr scores t based compute new eq run pg decoder step wt summary summary wt wt period symbol end summary end tences documents data driven restricted particular domain sentence redundancy calculate dancy sentence maxsj s sj compute rouge l precision sures longest common subsequence source sentence partial summary ing sentences generated far pg model divided length source tence source sentence yielding high l precision deemed signicant content overlap partial summary receive low mmr score likely serve basis generating future summary sentences alg provides overview pg mmr gorithm fig graphical illustration mmr scores source sentences updated ter summary sentence generated pg model different set highest scored tences guide pg model generate summary sentence muting ing source sentences important helps pg model focus attention nicant source content code model publicly available mds research experimental setup datasets investigate effectiveness pg mmr method testing standard document summarization datasets yen com ucfnlp multidoc summarization dang owczarzak include containing topics respectively summarization system tasked ating concise uent summary words set documents discussing topic documents set chronologically ordered concatenated form mega document ing input pg mmr system sentences start quotation mark end period excluded wong et al system summary compared human stracts created nist assessors following vention report results datasets standard test sets validation set parameter tuning pg model trained single document summarization cnn daily mail mann et al dataset containing single news articles paired summaries human written ticle highlights training set contains articles article contains tokens age summary contains tokens tences training use eters provided et al test time maximum minimum decoding steps set words respectively corresponding max min lengths pg mmr summaries cause focus work multi document summarization mds report results cnn daily mail dataset baselines compare pg mmr broad spectrum baselines including state art extractive abstractive systems described ext sumbasic vanderwende et al extractive approach assuming words occurring frequently ment set likely included summary ext kl sum haghighi vanderwende greedily adds source sentences summary leads crease kl divergence ext lexrank erkan radev uses graph based approach compute sentence importance based vector centrality graph representation ext centroid hong et al computes importance source sentence based cosine similarity document centroid ext icsisumm gillick et al leverages ilp framework identify globally optimal set sentences covering important concepts document set hyperparameters pg mmr variants bestsummrec grateful hong et al providing summaries generated centroid icsisumm dpp systems available dataset system sumbasic vanderwende et al klsumm haghighi et al lexrank erkan radev centroid hong et al icsisumm gillick favre dpp taskar song et al opinosis ganesan et al pg original et al pg mmr summrec pg mmr sentattn pg mmr cosine default pg mmr bestsummrec r table rouge results dataset system r sumbasic vanderwende et al klsumm haghighi et al lexrank erkan radev song et al opinosis ganesan et al pg original et al pg mmr summrec pg mmr sentattn pg mmr cosine default pg mmr bestsummrec table rouge results dataset ext dpp taskar selects optimal set sentences determinantal point processes balance erage important information sentence diversity abs opinosis ganesan et al generates abstractive summaries searching salient paths word occurrence graph created source documents abs song et al recent proach scores sentences lexrank generates title like summary sentence decoder model trained gigaword data abs pg original et al introduces decoder model encourages system copy words source text pointing retaining ity produce novel words generator results having described experimental setup compare pg mmr method lines standard mds datasets evaluated automatic metrics human assessors rouge lin automatic metric sures overlap unigrams bigrams skip bigrams maximum distance words r system summary set reference summaries rouge scores systems presented table spectively datasets explore variants pg mmr method differ importances source tences estimated sentence tance affects word attention weights cosine computes sentence importance cosine similarity score sentence ment vectors represented sparse tf idf vectors vector space model rec estimates sentence importance predicted r l recall score sentence summary support vector regression model trained sentences cnn daily mail datasets k applied duc tac sentences test time rec obtains best estimate sentence tance calculating r l recall score sentence reference summaries serves upper bound performance summrec variants sentence tance scores normalized range sentattn adjusts attention weights eq words important sentences likely generate summary weights computed eq seen table pg mmr method surpasses unsupervised extractive baselines cluding sumbasic klsumm lexrank dataset icsisumm dpp good performance systems trained directly mds datasets utilized pg mmr method pg mmr exhibits perior performance compared existing outperforms opinosis tive systems original large margin terms f scores particular pg original original pointer generator networks document inputs test time compared mmr effective identifying worthy content input cosine default pg mmr shows ter results summrec suggests sentence document representations tained encoder decoder model trained cnn dm suboptimal possibly vocabulary mismatch certain words duc tac datasets appear cnn dm embeddings learned ing finally observe bestsummrec yields highest performance datasets nding suggests great potential improvements pg mmr method extractive abstractive components separately optimized figure median location summary n grams multi document input lower higher quartiles n grams come summary tence location source sentence index system grams grams grams sent pg original pg mmr human abst table percentages summary n grams entire tences appear multi document input location summary content ested understanding pg mmr forms pg original identifying summary content multi document input ask tion source documents system tend look generating maries ndings indicate pg original gravitates early source sentences pg mmr searches rst sentences figure median location rst occurrences summary n grams n grams come summary sentence pg original summaries n grams summary sentence frequently come source sentences corresponding lower higher quartiles source sentence dices similarly n grams summary tence come source sentences pg mmr summaries patterns ent n grams summary tences come source sentences range respectively ndings gest pg original tends treat input single document identies summary worthy content beginning input pg mmr successfuly search broader range input summary content capability crucial multi document input tant content come article set degree extractiveness table shows multi document inputpg originalpg summ summ summ summ summ sent linguistic quality system fluency inform nonred lexrank pg original pg mmr rankings table linguistic quality rankings system summaries human abstract pg original summary boeing plane people board crashed tain west sulawesi province indonesia monday january killing passengers possible survivors plane adam air ight departing pm surabaya java bound manado northeast sulawesi plane crashed mountainous region polewali west lawesi province americans board survived know cause crash known time possible bad weather factor summary plane people board crashes americans board plane indonesia rescue team arrives indonesia plane crash plane crashes west sulawesi killing word fate boeing plane carrying passengers loses contact makassar plane crashes indonesia killing indonesian navy sends planes carry bodies ve indonesian plane carrying missing indonesian lawmaker criticises slow deployment plane hundreds kilometers plane crash adam air boeing crashed monday vanishing air trafc control radar screens indonesian islands java sulawesi people thought survived rescue teams racing crash site near polewali west sulawesi metres north south sulawesi provincial capital makassar worst air disaster sept mandala line s boeing crashed shortly taking north sumatra s airport killing people earlier friday ferry carrying people sank java coast pg mmr summary adam air boeing crashed monday afternoon search rescue teams discovered wreckage early tuesday indonesian rescue team arrived mountainous area west sulawesi province passenger plane people onboard crashed mountain polewali west sulawesi province air force rear commander eddy suyanto told shinta radio station plane operated local carrier adam air crashed mountainous region polewali province monday word fate remaining people board boeing table example system summaries human written abstract sentences manually de tokenized readability percentages summary n grams entire tences appearing multi document input pg original pg mmr summaries high degree extractiveness similar ings revealed et al pg mmr relies handful resentative source sentences mutes rest appears marginally extractive pg original systems encourage generating summary sentences stitching source sentences mary sentences appear source n grams summaries generated rewriting selected source sentences title like summary sentences exhibits high degree abstraction close human abstracts linguistic quality assess linguistic quality system summaries employ amazon mechanical turk human evaluators judge summary quality including pg mmr lexrank pg original turker asked rate system summary scale worst best based evaluation ria informativeness extent ing expressed ground truth text preserved summary uency summary matical formed non redundancy summary successfully avoid repeating information human summaries ground truth turkers asked provide overall ranking system summaries results presented table observe lexrank summaries highest rated ency lexrank extractive approach summary sentences directly taken input pg mmr rated best informativeness non redundancy garding overall system rankings pg mmr maries frequently ranked best summaries outperforming example summaries table present example summaries generated tems pg original effectively identify portant content multi document input tends generate short title like sentences informative carry stantial redundancy system trained gigaword dataset rush et al target summary length words mmr generates summaries effectively dense important source content conclusion describe novel adaptation method erate abstractive summaries multi document inputs method combines extractive marization algorithm mmr sentence tion recent abstractive model pg fusing source sentences pg mmr system strates competitive results outperforming strong extractive abstractive baselines references dzmitry bahdanau kyunghyun cho yoshua neural machine translation corr bengio jointly learning align translate regina barzilay kathleen r mckeown michael elhadad information fusion context multi document summarization proceedings annual meeting association tional linguistics acl tal baumel matan eyal michael elhadad query focused abstractive summarization rating query relevance multi document coverage summary length constraints els arxiv preprint taylor berg kirkpatrick dan gillick dan klein jointly learning extract compress proceedings annual meeting tion computational linguistics acl lidong bing piji li yi liao wai lam weiwei guo rebecca j passonneau abstractive multi document summarization phrase selection merging proceedings acl ziqiang cao wenjie li sujian li furu wei improving multi document summarization text classication proceedings association advancement articial intelligence aaai jaime carbonell jade goldstein use mmr diversity based reranking reordering documents producing summaries ings international acm sigir conference research development information retrieval sigir giuseppe carenini jackie chi kit cheung extractive vs nlg based abstractive tion evaluative text effect versiality proceedings fifth international natural language generation conference inlg qian chen xiaodan zhu zhen hua ling si wei hui jiang distraction based neural networks document summarization proceedings fifth international joint conference ticial intelligence ijcai yen chun chen mohit bansal fast stractive summarization reinforce selected tence rewriting proceedings annual ing association computational linguistics acl jianpeng cheng mirella lapata neural summarization extracting sentences words proceedings acl hoa trang dang karolina owczarzak overview tac update summarization proceedings text analysis conference task tac hal daume iii daniel marcu channel model document compression ceedings annual meeting association computational linguistics acl greg durrett taylor berg kirkpatrick dan klein learning based single document tion compression anaphoricity constraints proceedings association computational linguistics acl gunes erkan dragomir r radev lexrank graph based lexical centrality salience text journal articial intelligence summarization research giuseppe di fabbrizio amanda j stent robert gaizauskas hybrid approach document summarization opinions reviews proceedings international natural guage generation conference inlg katja filippova enrique alfonseca carlos menares lukasz kaiser oriol vinyals sentence compression deletion lstms proceedings conference empirical ods natural language processing emnlp dimitrios galanis ion androutsopoulos extractive supervised stage method sentence compression proceedings naacl hlt kavita ganesan chengxiang zhai jiawei han opinosis graph based approach stractive summarization highly redundant proceedings international ions ence computational linguistics coling sebastian gehrmann yuntian deng alexander rush abstractive proceedings conference tion pirical methods natural language processing emnlp shima gerani yashar mehdad giuseppe carenini raymond t ng bita nejat abstractive summarization product reviews discourse structure proceedings conference pirical methods natural language processing emnlp dan gillick benoit favre scalable global proceedings model summarization naacl workshop integer linear programming natural langauge processing dan gillick benoit favre dilek hakkani tur berndt bohnet yang liu shasha xie icsi utd summarization system tac proceedings tac max grusky mor naaman yoav artzi newsroom dataset million summaries diverse extractive strategies proceedings north american chapter association computational linguistics naacl jiatao gu zhengdong lu hang li victor o k incorporating copying mechanism proceedings li sequence sequence learning acl caglar gulcehre sungjin ahn ramesh nallapati bowen zhou yoshua bengio pointing unknown words proceedings nual meeting association computational linguistics acl aria haghighi lucy vanderwende ing content models multi document tion proceedings north american ter association computational linguistics naacl karl moritz hermann tomas kocisky edward grefenstette lasse espeholt kay mustafa leyman phil blunsom teaching chines read comprehend proceedings neural information processing systems nips sepp hochreiter jurgen schmidhuber long short term memory neural computation kai hong john m conroy benoit favre alex kulesza hui lin ani nenkova itory state art competitive baseline maries generic news summarization ings ninth international conference guage resources evaluation lrec masaru isonuma toru fujino junichiro mori yutaka matsuo ichiro sakata extractive marization multi task learning document classication proceedings conference empirical methods natural language processing emnlp hongyan jing kathleen mckeown composition human written summary sentences proceedings international acm sigir conference research development mation retrieval sigir yuta kikuchi graham neubig ryohei sasano hiroya takamura manabu okumura ling output length neural encoder decoders proceedings emnlp wojciech kryscinski romain paulus caiming xiong improving abstraction richard socher text summarization proceedings ference empirical methods natural language processing emnlp chen li fei liu fuliang weng yang liu document summarization guided sentence pression proceedings conference empirical methods natural language processing emnlp kexin liao logan lebanoff fei liu stract meaning representation multi document summarization proceedings international conference computational linguistics ing chin yew lin rouge package proceedings tomatic evaluation summaries acl workshop text summarization branches fei liu jeffrey flanigan sam thomson norman sadeh noah smith tive summarization semantic representations proceedings north american chapter association computational linguistics man language technologies naacl wencan luo diane litman summarizing student responses reection prompts ings conference empirical methods ural language processing emnlp wencan luo fei liu zitao liu diane litman automatic summarization student course proceedings north american feedback chapter association computational guistics human language technologies naacl yishu miao phil blunsom language latent variable discrete generative models proceedings tence compression ence empirical methods natural language processing emnlp ramesh nallapati bowen zhou cicero dos santos caglar gulcehre bing xiang stractive text summarization sequence sequence rnns proceedings signll conference computational natural language learning conll shashi narayan shay b cohen mirella lapata ranking sentences extractive tion reinforcement learning proceedings annual conference north american chapter association computational guistics human language technologies hlt ani nenkova kathleen mckeown matic summarization foundations trends information retrieval paul james yen introduction national institute standards technology romain paulus caiming xiong richard socher deep reinforced model abstractive proceedings conference marization empirical methods natural language processing emnlp daniele pighin marco cornolti enrique alfonseca katja filippova modelling events memory based open patterns proceedings annual tive summarization meeting association computational guistics acl lu wang hema raghavan vittorio castelli radu rian claire cardie sentence pression based framework query focused document summarization proceedings acl kam fai wong mingli wu wenjie li tractive summarization supervised proceedings supervised learning national conference computational linguistics coling michihiro yasunaga rui zhang kshitijh meelu ayush pareek krishnan srinivasan dragomir radev graph based neural multi document summarization ence computational natural language learning conll proceedings alexander m rush sumit chopra jason weston neural attention model sentence proceedings conference marization empirical methods natural language processing emnlp dani yogatama fei liu noah smith extractive summarization maximizing semantic volume proceedings conference pirical methods natural language processing emnlp david zajic bonnie j dorr jimmy lin richard schwartz multi candidate reduction tence compression tool document rization tasks information processing ment wenyuan zeng wenjie luo sanja fidler raquel urtasun efcient summarization copy mechanism proceedings international conference learning tions iclr jianmin zhang jiwei tan xiaojun wan neural network approach tive multi document summarization arxiv preprint qingyu zhou nan yang furu wei ming zhou selective encoding abstractive sentence summarization proceedings annual ing association computational linguistics acl evan sandhaus new york times annotated corpus linguistic data consortium abigail peter j liu christopher d manning point summarization proceedings annual generator networks meeting association computational guistics acl kaiqiang song lin zhao fei liu structure infused copy mechanisms abstractive summarization proceedings international conference computational linguistics ing sho takase jun suzuki naoaki okazaki tsutomu rao masaaki nagata neural headline generation abstract meaning representation proceedings conference empirical ods natural language processing emnlp jiwei tan xiaojun wan jianguo xiao abstractive document summarization proceedings based attentional neural model annual meeting association tional linguistics acl alex kuleszaand ben taskar determinantal point processes machine learning lishers inc kapil thadani kathleen mckeown tence compression joint structural inference proceedings conll lucy vanderwende hisami suzuki chris brockett ani nenkova sumbasic focused summarization sentence simplication lexical expansion information processing management
