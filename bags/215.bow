l u j l c s c v v x r macnet transferring knowledge machine comprehension sequence sequence models boyuan pan yazheng yang hao li zhou zhao yueting zhuang deng xiaofei state key lab zhejiang university college computer science zhejiang university zhejiang university joint institute frontier technologies inc hangzhou china panby haolics zhaozhou yzhuang edu cn ai abstract machine comprehension mc core problems natural language cessing requiring understanding natural language knowledge world rapid progress release benchmark datasets recently state art models surpass human performance known squad evaluation paper transfer knowledge learned machine comprehension sequence sequence tasks deepen understanding text propose macnet novel encoder decoder plementary architecture widely attention based sequence sequence models experiments neural machine translation nmt abstractive text summarization proposed framework signicantly improve performance baseline models method abstractive text marization achieves state art results gigaword dataset introduction machine comprehension mc gained signicant popularity past years coveted goal eld natural language understanding task teach machine understand content given passage answer related question requires deep comprehension accurate information extraction text release high quality benchmark datasets hermann et al rajpurkar et al joshi et al end end neural networks wang et al xiong et al cui et al achieved promising results mc tasks outperform humans squad rajpurkar et al popular machine comprehension tests table shows simple example squad dataset sequence sequence models sutskever et al attention mechanism bahdanau et al encoder compresses source text decoder attention nism generates target words shown great capability handle natural language generation tasks machine translation luong et al xia et al text summarization rush et al nallapati et al dialogue systems williams et al encoder decoder networks directly map source input xed target sentence learn relationship natural language texts makes hard capture lot deep intrinsic details understand potential implication li et al shi et al corresponding author conference neural information processing systems neurips montral canada passage rst super bowl feature quarterback teams pick draft classes manning selection nfl draft newton picked rst matchup pits picks draft newton carolina von miller denver question considered rst choice nfl draft answer manning table example squad dataset inspired recent success approaches machine comprehension tasks focus exploring mc knowledge help attention based models deeply comprehend text machine comprehension requires encode words passage question rstly methods seo et al wang et al xiong et al employ attention mechanism rnn based modeling layer capture interaction passage words conditioned question nally use mlp classier pointer networks vinyals et al predict answer span mc encoder mentioned common component models rnn based modeling layer input attention vectors supposed augment performance outputs models intuitively mc knowledge improve models measuring relevance generated sentence input source quesiton answering text generation different training data distributions benet sharing model s high level semantic components guo pasunuru bansal paper propose macnet machine comprehension augmented encoder decoder mentary architecture applied variety sequence generation tasks begin pre training mc model contains rnn based encoding layer modeling layer transferring source sequence sequence model encoding concatenate outputs original encoder transferred mc encoder decoding rst input attentional vectors model transferred mc modeling layer combine outputs attentional vectors formulate predictive vectors solve class imbalance resulted high frequency phrases adopt focal loss lin et al reshapes standard cross entropy improve weights loss distribution verify effectiveness approach conduct experiments representative sequence generation tasks neural machine translation transfer knowledge machine comprehension model attention based neural machine translation nmt model experimental results method signicantly improves performance large scale mt datasets abstractive text summarization modify pointer generator networks recently proposed et al evaluate model cnn daily mail hermann et al gigaword rush et al datasets model obtains rouge l scores english gigaword dataset improvement previous state art results literature related work machine comprehension teaching machines read process comprehend text answer questions called machine comprehension key problems articial intelligence recently rajpurkar et al released stanford question answering dataset squad high quality large scale benchmark inspired signicant works xiong et al pan et al cui et al seo et al wang et al xiong et al shen et al wang et al state art works attention based neural network models seo et al propose bi directional attention ow achieve query aware context representation wang et al employ gated self matching attention obtain relation question passage model rst surpass human performance squad paper pre trained mc architecture transferred nlp tasks sequence sequence model existing sequence sequence models attention focused generating target sequence aligning generated output token token input sequence approach proven successful nlp tasks neural machine translation bahdanau et al text summarization rush et al dialogue systems williams et al adapted applications including speech recognition chan et al image caption generation xu et al general models encode input sequence set vector representations recurrent neural network rnn second rnn decodes output sequence step step conditioned encodings work augment natural language understanding encoder decoder framework transferring knowledge supervised task transfer learning nlp transfer learning aims build learning machines generalize different domains following different probability distributions widely applied natural language processing tasks collobert et al glorot et al min seo hajishirzi mccann et al pan et al collobert et al propose unied neural network architecture learned unsupervised learning applied natural language processing tasks including speech tagging chunking named entity recognition semantic role labelling glorot et al propose deep learning approach learns extract meaningful representation review unsupervised fashion mccann et al propose transfer pre trained encoder neural machine translation nmt text classication question answering tasks pan et al propose transfer encoder pre trained discourse marker prediction model natural language inference model unlike previous works focus encoding unsupervised knowledge source extract multiple layers neural networks machine comprehension model insert sequence sequence model approach makes transfer directly compatible subsequent rnns augments text understanding attention mechanism machine comprehension model task description machine comprehension task given question q qm passage p pn m n length question passage goal predict correct answer ac subspan p framework state art mc models structures popular works essentially combination encoding layer attention mechanism rnn based modeling layer output et al seo et al pan et al xiong et al describe mc model follows encoding layer use pre trained word vectors glove pennington et al character level embeddings transfer words vectors applies cnn characters word proved helpful handling vocab words kim use bi directional lstm concatenation model temporal interactions words ui m hj j n genc bi directional lstm concatenation word character embedding vectors word contextual figure overview macnet framework comprising machine comprehension upper pre training sequence sequence model learned knowledge transferred representations question q passage p attention layer attention mechanisms commonly machine comprehenion model document representation emphasize key information capture long distance dependencies g attention function fatt represents series normalized linear logical operations follow seo et al use bi directional attention ow bidaf passage question interacted alignment matrix g query aware context representation modeling layer step use stacking lstm g capture interaction passage words conditioned question mj j n gmodel layers uni directional lstm mj expected represent contexual information j th word passage question use simple mlp classier combination g locate start end positions answer training dene training loss sum negative log probability true positions predicted distributions macnet architecture section shown figure introduce macnet transfers knowledge mc model model sequence sequence models typically implemented recurrent neural network encoder decoder framework framework directly models probability p target sentence y yty conditioned source sentence tx ty length sentence encoder model encoder reads source sentence word word generates hidden representation word xs attention mechanismembeddingencoding layerpredicted answermodeling layerpassagequestiontransferringembeddingpqaembeddingsource sentencexintegrationdecoder attentionmechanismytarget sentenceencoderbilstmtransferringlstmmachine comprehensionsequence sequencemodellstmbilstmbilstm fenc recurrent unit long short term memory lstm sutskever et al unit gated recurrent unit gru cho et al embedding vector xs hs hidden state paper use bi directional lstm recurrent unit consistent encoding layer mc model described section augment performance encoding use simple method exploit word representations learned mc task source sentence use bi directional lstm equation encoder obtain es hidden state represents word xs perspective mc model instead conventional models directly send results equation decoder concatenate es hs feed integration layer hs fint uni directional lstm means concatenation contextual tations sentence contain information machine comprehension knowledge decoder attention mechanism initialized representations obtained encoder decoder attention mechanism receives word embedding previous word training previous word reference sentence testing previous generated word step generates word decoder states computed fdec unidirectional lstm t th generated word hs hidden state attentional models attention steps summarized equations ts ct ts s ht ba ct source context vector attention vector derive softmax logit loss wa ba trainable parameters function ga forms score referred content based function usually implemented feed forward network hidden layer common models attention vector fed softmax layer produce predictive distribution formulated p t bp macnet additionally send attention vector modeling layer pre trained mc model equation deeply capture interaction source target states rt rt attention state augmentation machine comprehension knowledge combine results attention vectors equation p t wqrt bp wp wq bp trainable parameters modeling layer helps deeply understand interaction contextual information output sequence different encoding layer inputs independent source sentences training denote parameters learned framework d training dataset contains source target sequence pairs training process aims seeking optimal paramaters encodes source sequence provides output sentence close target sentence formula form popular objective maximum log likelihood estimation bahdanau et al xia et al arg max p ty arg max logp t results high frequency commonly expressions nt know output sentences nature class imbalance corpus inspired focal loss lin et al recently proposed solve foreground background class imbalance task object detection add modulating factor cross entropy loss simplifying p t pt modify equation arg max ty tunable focusing parameter case focusing parameter smoothly adjusts rate high frequency phrases weighted experiments machine comprehension use stanford question answering dataset et al training questions posed crowd workers wikipedia articles hidden state size lstm set select glove word embeddings use dimensional lters cnn character level embedding width dropout ratio use adadelta zeiler optimizer initial learning rate mc model achieves exact match em score squad development dataset application neural machine translation rst evaluate method neural machine translation nmt task requires encode source language sentence predict target language sentence use architecture luong et al baseline framework gnmt wu et al attention parallelize decoder s computation datasets evaluation wmt translation tasks english german directions translation performances reported case sensitive bleu papineni et al implementation details training nmt systems split data subword units bpe sennrich et al train layer lstms units bidirectional encoder embedding dimension use fully connected layer transform input vector size transferred neural networks model trained stochastic gradient descent learning rate began train k steps k steps start halving learning rate k step batch size set dropout rate focal loss set squad dataset referred github io squad statmt org translation task html statmt org translation task html nmt systems ende deen ende deen baseline baseline encoding layer baseline modeling layer baseline encoding layer modeling layer baseline random initalized framework baseline macnet table bleu scores ofcial test sets wmt english german performance baseline model medium present ablation experiments effectiveness macnet em bleu mc attention context query attention query context attention bidaf bidaf self attention bidaf memory network results shown table line nmt model datasets performs better help macnet work medium conduct tion experiment evaluate individual tribution component model encoding layer modeling layer demonstrates effectiveness ablate modules add focal loss bleu scores test sets rise point shows signicance transferred knowledge finally add architecture ing layer modeling layer baseline model initialize randomly rnn layers observe performance drops indicates machine comprehension knowledge deep connections machine translation tasks ablation experiments found improvement modeling layer architecture bit modest believe transferring high level networks e modeling layer help lot suitable structure networks contains deeper semantic knowledge abstractive information compared lower level layers e encoding layer table performance different pre trained chine comprehension models nmt model deen em means exact match score represents performance mc model squad dev set bleu results nmt model table explore different choices attention architectures fatt equation usually discrimination different mc models mc models impact performance method rst follow seo et al rate directions attention bidaf use place original attention nism respectively performance machine comprehension task drops lot fect results nmt models add self attention proposed fuse context widely mc ods wang et al weissenborn et al fortunately result nmt model fails pace performance pre train mc model finally apply memory network popular mc models pan et al hu et al performance squad rises lot nmt result similar original model series experiments denote model s performance macnet positive correlation improvement figure performance different values summarization models words et al et al et al et al rl intra et al pointer et al pointer generator encoding layer pointer generator modeing layer pointer generator macnet cnn daily mail rg l gigaword rg l table rouge evaluation results cnn daily mail test set english gigaword test set rg table denotes rouge results mark taken corresponding papers table shows performance macnet ablation results mc architecture conjecture depend potential factors complexity extracted parts heterogeneity different tasks figure present models different focal loss affects performance models increase enlarges arrives performance gets worse raise means modulating factor close zero benet limited application text summarization verify effectiveness macnet abstractive text summarization typical application sequence sequence model use pointer generator et al baseline model applies encoder decoder architecture state art models text summarization evaluation metric reported scores rouge l lin evaluate method high quality datasets cnn daily mail hermann et al gigaword rush et al cnn daily mail dataset use supplied et al pre process data contains training pairs validation pairs test pairs english gigaword dataset use released rush et al pre process obtain m training pairs development set testing implementation details training hyperparameters similar pointer generator networks experiments important details follows input output vocabulary size hidden state size word embedding size use fully connected layer transform input vector size transferred neural networks train adagrad duchi et al learning rate initial accumulator value set results table shows performance methods competing approaches datasets compared original pointer generator model results macnet architecture outperform kinds rouge scores especially approach achieves state art results metrics gigaword cnn daily mail dataset similar nmt task encoding layer contributes improvement modeling layer stable gains evaluations table present summaries produced model original pointer generator model rst example summary given pointer generator model nt sense perspective logic model accurately summarizes article provides com abisee cnn dailymail com facebook namas article israeli warplanes raided hezbollah targets south lebanon guerrillas killed militiamen wounded seven troops wednesday police said reference israeli warplanes raid south lebanon pg macnet israeli warplanes attack hezbollah targets south lebanon pg hezbollah targets hezbollah targets south lebanon article dollar racked clear gains wednesday london forex market operators waited outcome talks white house congress raising national debt ceiling cutting american budget decit reference dollar gains market eyes debt budget talks pg macnet dollar racked clear gains pg london forex market racked gains table examples summaries english gigaword pg denotes pointer generator model details second example original pg model produces logical sentence output sentence expresses completely different meanings information article method correctly comprehends article provides high quality summary sentence paper propose macnet supplementary framework sequence sequence tasks transfer knowledge machine comprehension task variety tasks augment text understanding models experimental evaluation shows method signicantly improves performance baseline models benchmark datasets different nlp tasks hope work encourage research transfer learning multi layer neural networks future works involve choice transfer learning sources transfer learning different domains nlp cv work supported national nature science foundation china grant nos national youth notch talent support program experiments supported chengwei yao experiment center college computer science technology zhejiang university conclusion acknowledgments references translate iclr bahdanau d cho k bengio y et al neural machine translation jointly learning align chan w jaitly n le q vinyals o listen attend spell neural network large vocabulary conversational speech recognition acoustics speech signal processing icassp ieee international conference cho k van merrienboer b gulcehre c bahdanau d bougares f schwenk h bengio y learning phrase representations rnn encoder decoder statistical machine translation emnlp collobert r weston j bottou l karlen m kavukcuoglu k kuksa p natural language processing scratch journal machine learning research cui y chen z wei s wang s liu t hu g attention attention neural networks reading comprehension acl volume duchi j hazan e singer y et al adaptive subgradient methods online learning stochastic optimization journal machine learning research gehring j auli m grangier d yarats d dauphin y n convolutional sequence sequence learning arxiv preprint glorot x bordes bengio y et al domain adaptation large scale sentiment classication deep learning approach icml guo h pasunuru r bansal m soft layer specic multi task summarization entailment question generation arxiv preprint hermann k m kocisky t grefenstette e espeholt l kay w suleyman m blunsom p teaching machines read comprehend nips hu m peng y qiu x et al reinforced mnemonic reader machine comprehension corr joshi m choi e weld d zettlemoyer l triviaqa large scale distantly supervised challenge dataset reading comprehension acl kim y convolutional neural networks sentence classication emnlp li j xiong d tu z zhu m zhang m zhou g modeling source syntax neural machine translation acl volume lin t goyal p girshick r k dollar p focal loss dense object detection iccv lin c rouge package automatic evaluation summaries text summarization branches luong t pham h manning c d et al effective approaches attention based neural machine translation emnlp mccann b bradbury j xiong c socher r learned translation contextualized word vectors nips supervision data acl min s seo m hajishirzi h question answering transfer learning large ne grained nallapati r zhou b dos santos c gulcehre c xiang b abstractive text summarization sequence sequence rnns proceedings signll conference computational natural language learning nallapati r zhai f zhou b et al summarunner recurrent neural network based sequence model extractive summarization documents aaai pan b li h zhao z cao b cai d x memen multi layer embedding memory networks machine comprehension arxiv preprint pan b yang y zhao z zhuang y cai d x discourse marker augmented network reinforcement learning natural language inference proceedings annual meeting association computational linguistics volume long papers volume papineni k roukos s ward t zhu w bleu method automatic evaluation machine translation acl association computational linguistics paulus r c socher r et al deep reinforced model abstractive summarization arxiv preprint emnlp text emnlp emnlp acl volume acl pennington j socher r manning c d et al glove global vectors word representation rajpurkar p zhang j lopyrev k liang p squad questions machine comprehension rush m chopra s weston j et al neural attention model abstractive sentence summarization liu p j manning c d et al point summarization pointer generator networks sennrich r haddow b birch et al neural machine translation rare words subword units seo m kembhavi farhadi hajishirzi h bidirectional attention ow machine comprehension iclr sion kdd acm shen y huang p gao j chen w reasonet learning stop reading machine shi x padhi knight k et al string based neural mt learn source syntax acl sutskever vinyals o le q v et al sequence sequence learning neural networks nips vinyals o fortunato m jaitly n et al pointer networks nips wang z mi h hamza w florian r multi perspective context matching machine comprehension arxiv preprint wang w yang n wei f chang b zhou m gated self matching networks reading comprehension question answering acl weissenborn d wiese g seiffe l et al making neural qa simple possible simpler conll williams j d asadi k zweig g et al hybrid code networks practical efcient end end dialog control supervised reinforcement learning acl volume wu y schuster m chen z le q v norouzi m macherey w krikun m cao y gao q macherey k klingner j shah johnson m liu x ukasz kaiser gouws s kato y kudo t kazawa h stevens k kurian g patil n wang w young c smith j riesa j rudnick vinyals o corrado g hughes m dean j google s neural machine translation system bridging gap human machine translation corr xia y tian f wu l lin j qin t yu n liu t deliberation networks sequence generation pass decoding nips xiong c zhong v socher r et al dynamic coattention networks question answering iclr answering iclr xiong c zhong v socher r et al mixed objective deep residual coattention question xu k ba j kiros r cho k courville salakhudinov r zemel r bengio y attend tell neural image caption generation visual attention international conference machine learning zeiler m d adadelta adaptive learning rate method arxiv preprint zhou q yang n wei f zhou m selective encoding abstractive sentence summarization acl
