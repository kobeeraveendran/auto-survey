l u j l c s c v v i x r a macnet transferring knowledge from machine comprehension to sequence to sequence models boyuan pan yazheng yang hao li zhou zhao yueting zhuang deng xiaofei state key lab of zhejiang university college of computer science zhejiang university zhejiang university joint institute of frontier technologies inc hangzhou china panby haolics zhaozhou yzhuang edu cn ai abstract machine comprehension mc is one of the core problems in natural language cessing requiring both understanding of the natural language and knowledge about the world rapid progress has been made since the release of several benchmark datasets and recently the state of the art models even surpass human performance on the well known squad evaluation in this paper we transfer knowledge learned from machine comprehension to the sequence to sequence tasks to deepen the understanding of the text we propose macnet a novel encoder decoder plementary architecture to the widely used attention based sequence to sequence models experiments on neural machine translation nmt and abstractive text summarization show that our proposed framework can signicantly improve the performance of the baseline models and our method for the abstractive text marization achieves the state of the art results on the gigaword dataset introduction machine comprehension mc has gained signicant popularity over the past few years and it is a coveted goal in the eld of natural language understanding its task is to teach the machine to understand the content of a given passage and then answer a related question which requires deep comprehension and accurate information extraction towards the text with the release of several high quality benchmark datasets hermann et al rajpurkar et al joshi et al end to end neural networks wang et al xiong et al cui et al have achieved promising results on the mc tasks and some even outperform humans on the squad rajpurkar et al which is one of the most popular machine comprehension tests table shows a simple example from the squad dataset sequence to sequence models sutskever et al with attention mechanism bahdanau et al in which an encoder compresses the source text and a decoder with an attention nism generates target words have shown great capability to handle many natural language generation tasks such as machine translation luong et al xia et al text summarization rush et al nallapati et al and dialogue systems williams et al however these encoder decoder networks directly map the source input to a xed target sentence to learn the relationship between the natural language texts which makes them hard to capture a lot of deep intrinsic details and understand the potential implication of them li et al shi et al corresponding author conference on neural information processing systems neurips montral canada passage this was the rst super bowl to feature a quarterback on both teams who was the pick in their draft classes manning was the selection of the nfl draft while newton was picked rst in the matchup also pits the top two picks of the draft against each other newton for carolina and von miller for denver question who was considered to be the rst choice in the nfl draft of answer manning table an example from the squad dataset inspired by the recent success of the approaches for the machine comprehension tasks we focus on exploring whether mc knowledge can further help the attention based models deeply comprehend the text machine comprehension requires to encode words from the passage and the question rstly then many methods seo et al wang et al xiong et al employ attention mechanism with an rnn based modeling layer to capture the interaction among the passage words conditioned on the question and nally use an mlp classier or pointer networks vinyals et al to predict the answer span the mc encoder mentioned above is a common component in the models while the rnn based modeling layer whose input is the attention vectors is also supposed to augment the performance of the outputs of the models intuitively mc knowledge could improve models through measuring the relevance between the generated sentence and the input source moreover while quesiton answering and text generation have different training data distributions they can still benet from sharing their model s high level semantic components guo pasunuru and bansal in this paper we propose macnet a machine comprehension augmented encoder decoder mentary architecture that can be applied to a variety of sequence generation tasks we begin by pre training an mc model that contains both the rnn based encoding layer and modeling layer as the transferring source in the sequence to sequence model for encoding we concatenate the outputs of the original encoder and the transferred mc encoder for decoding we rst input the attentional vectors from the model into the transferred mc modeling layer and then combine its outputs with the attentional vectors to formulate the predictive vectors moreover to solve the class imbalance resulted by the high frequency phrases we adopt the focal loss lin et al which reshapes the standard cross entropy to improve the weights of the loss distribution to verify the effectiveness of our approach we conduct experiments on two representative sequence generation tasks neural machine translation we transfer the knowledge from the machine comprehension model to the attention based neural machine translation nmt model experimental results show that our method signicantly improves the performance on several large scale mt datasets abstractive text summarization we modify the pointer generator networks recently proposed by see et al we evaluate this model on the cnn daily mail hermann et al and gigaword rush et al datasets our model obtains and rouge l scores on the english gigaword dataset which is an improvement over previous state of the art results in the literature related work machine comprehension teaching machines to read process and comprehend text and then answer questions which is called machine comprehension is one of the key problems in articial intelligence recently rajpurkar et al released the stanford question answering dataset squad which is a high quality and large scale benchmark thus inspired many signicant works xiong et al pan et al cui et al seo et al wang et al xiong et al shen et al wang et al most of the state of the art works are attention based neural network models seo et al propose a bi directional attention ow to achieve a query aware context representation wang et al employ gated self matching attention to obtain the relation between the question and passage and their model is the rst one to surpass the human performance on the squad in this paper we show that the pre trained mc architecture can be transferred well to other nlp tasks sequence to sequence model existing sequence to sequence models with attention have focused on generating the target sequence by aligning each generated output token to another token in the input sequence this approach has proven successful in many nlp tasks such as neural machine translation bahdanau et al text summarization rush et al and dialogue systems williams et al and has also been adapted to other applications including speech recognition chan et al and image caption generation xu et al in general these models encode the input sequence as a set of vector representations using a recurrent neural network rnn a second rnn then decodes the output sequence step by step conditioned on the encodings in this work we augment the natural language understanding of this encoder decoder framework via transferring knowledge from another supervised task transfer learning in nlp transfer learning which aims to build learning machines that generalize across different domains following different probability distributions has been widely applied in natural language processing tasks collobert et al glorot et al min seo and hajishirzi mccann et al pan et al collobert et al propose a unied neural network architecture and learned from unsupervised learning that can be applied to various natural language processing tasks including part of speech tagging chunking named entity recognition and semantic role labelling glorot et al propose a deep learning approach which learns to extract a meaningful representation for each review in an unsupervised fashion mccann et al propose to transfer the pre trained encoder from the neural machine translation nmt to the text classication and question answering tasks pan et al propose to transfer the encoder of a pre trained discourse marker prediction model to the natural language inference model unlike previous works that only focus on the encoding part or unsupervised knowledge source we extract multiple layers of the neural networks from the machine comprehension model and insert them into the sequence to sequence model our approach not only makes the transfer more directly compatible with subsequent rnns but also augments the text understanding of the attention mechanism machine comprehension model task description in the machine comprehension task we are given a question q qm and a passage p pn where m and n are the length of the question and the passage the goal is to predict the correct answer ac which is a subspan of p framework the state of the art mc models are various in structures but many popular works are essentially the combination of the encoding layer the attention mechanism with and an rnn based modeling layer and the output et al seo et al pan et al xiong et al now we describe our mc model as follows encoding layer we use pre trained word vectors glove pennington et al and character level embeddings to transfer the words into vectors where the latter one applies cnn over the characters of each word and is proved to be helpful in handling out of vocab words kim we then use a bi directional lstm on top of the concatenation of them to model the temporal interactions between words ui m hj j n where genc is the bi directional lstm is the concatenation of the word and character embedding vectors of the word are the contextual and figure overview of our macnet framework comprising the part of machine comprehension upper for pre training and sequence to sequence model bottom to which the learned knowledge will be transferred representations of the question q and the passage p attention layer attention mechanisms are commonly used in machine comprehenion to model the document so that its representation can emphasize the key information and capture long distance dependencies g here the attention function fatt represents a series of normalized linear and logical operations we follow seo et al to use a bi directional attention ow bidaf where the passage and the question are interacted each other with an alignment matrix g is the query aware context representation modeling layer in this step we use the stacking lstm on g to further capture the interaction among the passage words conditioned on the question mj j n where gmodel is two layers of uni directional lstm each mj is expected to represent the contexual information of the j th word in the passage to the whole question we use a simple mlp classier on the combination of and g to locate the start and end positions of the answer for training we dene the training loss as the sum of the negative log probability of the true positions by the predicted distributions macnet architecture in this section as shown in the figure we introduce how our macnet transfers the knowledge from the mc model to the model the sequence to sequence models are typically implemented with a recurrent neural network encoder decoder framework such a framework directly models the probability p of a target sentence y yty conditioned on the source sentence where tx and ty are the length of the sentence and encoder for the model the encoder reads the source sentence word by word and generates a hidden representation of each word xs attention mechanismembeddingencoding layerpredicted answermodeling layerpassagequestiontransferringembeddingpqaembeddingsource sentencexintegrationdecoder attentionmechanismytarget sentenceencoderbilstmtransferringlstmmachine comprehensionsequence to sequencemodellstmbilstmbilstm where fenc is the recurrent unit such as long short term memory lstm sutskever et al unit or gated recurrent unit gru cho et al is the embedding vector of xs hs is the hidden state in this paper we use the bi directional lstm as the recurrent unit to be consistent with the encoding layer of the mc model described in section to augment the performance of the encoding part we use a simple method to exploit the word representations that learned from the mc task for the source sentence we use the bi directional lstm of the equation as another encoder and obtain where es is the hidden state which represents the word xs from the perspective of the mc model instead of the conventional models that directly send the results of the equation to the decoder we concatenate es and hs and feed them into an integration layer hs where fint is a uni directional lstm means concatenation are the contextual tations of the sentence which contain the information of the machine comprehension knowledge as well decoder attention mechanism initialized by the representations obtained from the encoder the decoder with an attention mechanism receives the word embedding of the previous word while training it is the previous word of the reference sentence while testing it is the previous generated word at each step and generates next word the decoder states are computed via where fdec is a unidirectional lstm is the t th generated word hs is the hidden state for most attentional models the attention steps can be summarized by the equations below ts ct ts s at ht ba here ct is the source side context vector the attention vector at is used to derive the softmax logit and loss wa and ba are trainable parameters the function ga can also take other forms score is referred as a content based function usually implemented as a feed forward network with one hidden layer for the common models the attention vector at is then fed through the softmax layer to produce the predictive distribution formulated as p t bp in our macnet however we additionally send the attention vector at into the modeling layer of the pre trained mc model in the equation to deeply capture the interaction of the source and the target states rt where rt is another attention state with the augmentation of machine comprehension knowledge we combine the results of the two attention vectors and the equation becomes p t wqrt bp where wp wq and bp are all trainable parameters the modeling layer helps deeply understand the interaction of the contextual information of the output sequence which is different from the encoding layer whose inputs are independent source sentences training denote as all the parameters to be learned in the framework d as the training dataset that contains source target sequence pairs the training process aims at seeking the optimal paramaters that encodes the source sequence and provides an output sentence as close as the target sentence for the formula form the most popular objective is the maximum log likelihood estimation bahdanau et al xia et al arg max p ty arg max logp t however this results in the high frequency of some commonly used expressions such as i do nt know in the output sentences because of the nature of the class imbalance in the corpus inspired by the focal loss lin et al which is recently proposed to solve the foreground background class imbalance in the task of object detection we add a modulating factor to the above cross entropy loss simplifying p t as pt we modify the equation as arg max ty where is a tunable focusing parameter in this case the focusing parameter smoothly adjusts the rate at which high frequency phrases are down weighted experiments machine comprehension we use the stanford question answering dataset et al as our training which has questions posed by crowd workers on wikipedia articles the hidden state size of the lstm is set as and we select the glove as the word embeddings we use one dimensional lters for cnn in the character level embedding with width of for each one the dropout ratio is we use the adadelta zeiler optimizer with an initial learning rate as our mc model achieves of exact match em and of score on the squad development dataset application to neural machine translation we rst evaluate our method on the neural machine translation nmt task which requires to encode a source language sentence and predict a target language sentence we use the architecture from luong et al as our baseline framework with the gnmt wu et al attention to parallelize the decoder s computation the datasets for our evaluation are the wmt translation tasks between english and german in both directions translation performances are reported in case sensitive bleu papineni et al on and implementation details when training our nmt systems we split the data into subword units using bpe sennrich et al we train layer lstms of units with bidirectional encoder embedding dimension is we use a fully connected layer to transform the input vector size for the transferred neural networks the model is trained with stochastic gradient descent with a learning rate that began at we train for k steps after k steps we start halving learning rate every k step our batch size is set as the dropout rate is for the focal loss the is set to be squad dataset is referred at github io squad statmt org translation task html statmt org translation task html nmt systems ende deen ende deen baseline baseline encoding layer baseline modeling layer baseline encoding layer modeling layer baseline random initalized framework baseline macnet table bleu scores on ofcial test sets wmt english german for and in the top part we show the performance of our baseline model in the medium part we present the ablation experiments in the bottom part we show the effectiveness of our macnet em bleu mc attention context to query attention query to context attention bidaf bidaf self attention bidaf memory network results as shown in the table the line nmt model on all of the datasets performs much better with the help of our macnet work in the medium part we conduct an tion experiment to evaluate the individual tribution of each component of our model both of the encoding layer and the modeling layer demonstrates their effectiveness when we ablate other modules when we add both of them still without the focal loss the bleu scores on all the test sets rise at least point which shows the signicance of the transferred knowledge finally we add the architecture of the ing layer and the modeling layer to the baseline model but initialize them randomly as its other rnn layers we observe that the performance drops around which indicates that the machine comprehension knowledge has deep connections with the machine translation tasks from the ablation experiments we found that the improvement of the modeling layer in our architecture is a bit modest but we believe transferring high level networks e the modeling layer can help a lot with a more suitable structure because those networks contains deeper semantic knowledge and more abstractive information compared with the lower level layers e encoding layer table performance with different pre trained chine comprehension models for our nmt model on deen of em means the exact match score which represents the performance of the mc model on the squad dev set bleu is the results of our nmt model in the table we explore how different choices of the attention architectures fatt in the equation which is usually the discrimination of different mc models of the mc models impact the performance of our method we rst follow seo et al to rate the two directions of the attention in bidaf and use them to take place of the original attention nism respectively their performance on the machine comprehension task drops a lot and it seems to fect the results of the nmt models as well we then add the self attention which is proposed to fuse the context into itself is widely used by many mc ods wang et al weissenborn et al fortunately the result of the nmt model fails to keep pace with the performance of its pre train mc model finally we apply memory network which is also very popular among mc models pan et al hu et al the performance on the squad rises a lot but the nmt result is similar to the original model this series of experiments denote that the model s performance with our macnet is not always in positive correlation to the improvement of the figure performance on the with different values summarization models words et al et al et al et al rl with intra et al pointer et al pointer generator encoding layer pointer generator modeing layer pointer generator macnet cnn daily mail rg l gigaword rg l table rouge evaluation results on the cnn daily mail test set and the english gigaword test set rg in the table denotes rouge results with mark are taken from the corresponding papers the bottom part of the table shows the performance of our macnet and the ablation results mc architecture we conjecture that it might depend on many potential factors such as the complexity of the extracted parts the heterogeneity of different tasks in the figure we present the models on the with different to show how the focal loss affects the performance as we can see the models increase as the enlarges until it arrives or afterwards the performance gets worse when we raise the which means the modulating factor is close to zero so that its benet is limited application to text summarization we then verify the effectiveness of our macnet on the abstractive text summarization which is also a typical application of the sequence to sequence model we use the pointer generator et al as our baseline model which applies the encoder decoder architecture and is one of the state of the art models for the text summarization the evaluation metric is reported with the scores for and rouge l lin we evaluate our method on two high quality datasets cnn daily mail hermann et al and gigaword rush et al for the cnn daily mail dataset we use supplied by see et al to pre process the data which contains training pairs validation pairs and test pairs for the english gigaword dataset we use the released by rush et al to pre process and obtain m training pairs development set for testing implementation details our training hyperparameters are similar to the pointer generator networks experiments while some important details are as follows the input and output vocabulary size is the hidden state size is the word embedding size is and we use a fully connected layer to transform the input vector size for the transferred neural networks we train using adagrad duchi et al with learning rate and an initial accumulator value of the is set as results table shows the performance of our methods and the competing approaches on both datasets compared to the original pointer generator model the results with our macnet architecture outperform around on all kinds of the rouge scores especially our approach achieves the state of the art results on all the metrics on gigaword and the on cnn daily mail dataset similar to the nmt task the encoding layer contributes most of the improvement while the modeling layer also has stable gains in each evaluations in the table we present some summaries produced by our model and the original pointer generator model in the rst example the summary given by the pointer generator model does nt make sense from the perspective of logic while our model accurately summarizes the article and even provides com abisee cnn dailymail com facebook namas article israeli warplanes raided hezbollah targets in south lebanon after guerrillas killed two militiamen and wounded seven other troops on wednesday police said reference israeli warplanes raid south lebanon pg macnet israeli warplanes attack hezbollah targets in south lebanon pg hezbollah targets hezbollah targets in south lebanon article the dollar racked up some clear gains on wednesday on the london forex market as operators waited for the outcome of talks between the white house and congress on raising the national debt ceiling and on cutting the american budget decit reference dollar gains as market eyes us debt and budget talks pg macnet dollar racked up some clear gains pg london forex market racked gains table examples of summaries on english gigaword pg denotes the pointer generator model with more details in the second example although the original pg model produces a logical sentence the output sentence expresses completely different meanings from the information in the article our method however correctly comprehends the article and provides with a high quality summary sentence in this paper we propose macnet which is a supplementary framework for the sequence to sequence tasks we transfer the knowledge from the machine comprehension task to a variety of tasks to augment the text understanding of the models the experimental evaluation shows that our method signicantly improves the performance of the baseline models on several benchmark datasets for different nlp tasks we hope this work can encourage further research into the transfer learning of multi layer neural networks and the future works involve the choice of other transfer learning sources and the transfer learning between different domains such as nlp cv this work was supported in part by the national nature science foundation of china grant nos and and in part by the national youth top notch talent support program the experiments are supported by chengwei yao in the experiment center of the college of computer science and technology zhejiang university conclusion acknowledgments references translate in iclr bahdanau d cho k bengio y et al neural machine translation by jointly learning to align and chan w jaitly n le q and vinyals o listen attend and spell a neural network for large vocabulary conversational speech recognition in acoustics speech and signal processing icassp ieee international conference on cho k van merrienboer b gulcehre c bahdanau d bougares f schwenk h and bengio y learning phrase representations using rnn encoder decoder for statistical machine translation in emnlp collobert r weston j bottou l karlen m kavukcuoglu k and kuksa p natural language processing almost from scratch journal of machine learning research cui y chen z wei s wang s liu t and hu g attention over attention neural networks for reading comprehension in acl volume duchi j hazan e singer y et al adaptive subgradient methods for online learning and stochastic optimization journal of machine learning research gehring j auli m grangier d yarats d and dauphin y n convolutional sequence to sequence learning arxiv preprint glorot x bordes a bengio y et al domain adaptation for large scale sentiment classication a deep learning approach in icml guo h pasunuru r and bansal m soft layer specic multi task summarization with entailment and question generation arxiv preprint hermann k m kocisky t grefenstette e espeholt l kay w suleyman m and blunsom p teaching machines to read and comprehend in nips hu m peng y qiu x et al reinforced mnemonic reader for machine comprehension corr joshi m choi e weld d and zettlemoyer l triviaqa a large scale distantly supervised challenge dataset for reading comprehension in acl kim y convolutional neural networks for sentence classication in emnlp li j xiong d tu z zhu m zhang m and zhou g modeling source syntax for neural machine translation in acl volume lin t goyal p girshick r he k and dollar p focal loss for dense object detection in iccv lin c rouge a package for automatic evaluation of summaries text summarization branches out luong t pham h manning c d et al effective approaches to attention based neural machine translation in emnlp mccann b bradbury j xiong c and socher r learned in translation contextualized word vectors in nips supervision data in acl min s seo m and hajishirzi h question answering through transfer learning from large ne grained nallapati r zhou b dos santos c gulcehre c and xiang b abstractive text summarization using sequence to sequence rnns and beyond in proceedings of the signll conference on computational natural language learning nallapati r zhai f zhou b et al summarunner a recurrent neural network based sequence model for extractive summarization of documents in aaai pan b li h zhao z cao b cai d and he x memen multi layer embedding with memory networks for machine comprehension arxiv preprint pan b yang y zhao z zhuang y cai d and he x discourse marker augmented network with reinforcement learning for natural language inference in proceedings of the annual meeting of the association for computational linguistics volume long papers volume papineni k roukos s ward t and zhu w bleu a method for automatic evaluation of machine translation in acl association for computational linguistics paulus r c socher r et al a deep reinforced model for abstractive summarization arxiv preprint emnlp of text in emnlp in emnlp in acl volume in acl pennington j socher r manning c d et al glove global vectors for word representation in rajpurkar p zhang j lopyrev k and liang p squad questions for machine comprehension rush a m chopra s weston j et al a neural attention model for abstractive sentence summarization see a liu p j manning c d et al get to the point summarization with pointer generator networks sennrich r haddow b birch a et al neural machine translation of rare words with subword units seo m kembhavi a farhadi a and hajishirzi h bidirectional attention ow for machine comprehension in iclr sion in kdd acm shen y huang p gao j and chen w reasonet learning to stop reading in machine shi x padhi i knight k et al does string based neural mt learn source syntax in acl sutskever i vinyals o le q v et al sequence to sequence learning with neural networks in nips vinyals o fortunato m jaitly n et al pointer networks in nips wang z mi h hamza w and florian r multi perspective context matching for machine comprehension arxiv preprint wang w yang n wei f chang b and zhou m gated self matching networks for reading comprehension and question answering in acl weissenborn d wiese g seiffe l et al making neural qa as simple as possible but not simpler in conll williams j d asadi k zweig g et al hybrid code networks practical and efcient end to end dialog control with supervised and reinforcement learning in acl volume wu y schuster m chen z le q v norouzi m macherey w krikun m cao y gao q macherey k klingner j shah a johnson m liu x ukasz kaiser gouws s kato y kudo t kazawa h stevens k kurian g patil n wang w young c smith j riesa j rudnick a vinyals o corrado g hughes m and dean j google s neural machine translation system bridging the gap between human and machine translation corr xia y tian f wu l lin j qin t yu n and liu t deliberation networks sequence generation beyond one pass decoding in nips xiong c zhong v socher r et al dynamic coattention networks for question answering iclr answering in iclr xiong c zhong v socher r et al mixed objective and deep residual coattention for question xu k ba j kiros r cho k courville a salakhudinov r zemel r and bengio y show attend and tell neural image caption generation with visual attention in international conference on machine learning zeiler m d adadelta an adaptive learning rate method arxiv preprint zhou q yang n wei f and zhou m selective encoding for abstractive sentence summarization in acl
