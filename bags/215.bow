macnet transferring knowledge machine comprehension sequence sequence models boyuan pan yazheng yang hao zhou zhao yueting zhuang deng xiaofei state key lab zhejiang university college computer science zhejiang university zhejiang university joint institute frontier technologies inc hangzhou china panby haolics zhaozhou yzhuang edu abstract machine comprehension core problems natural language cessing requiring understanding natural language knowledge world rapid progress release benchmark datasets recently state art models surpass human performance known squad evaluation paper transfer knowledge learned machine comprehension sequence sequence tasks deepen understanding text propose macnet novel encoder decoder plementary architecture widely attention based sequence sequence models experiments neural machine translation nmt abstractive text summarization proposed framework signicantly improve performance baseline models method abstractive text marization achieves state art results gigaword dataset introduction machine comprehension gained signicant popularity past years coveted goal eld natural language understanding task teach machine understand content given passage answer related question requires deep comprehension accurate information extraction text release high quality benchmark datasets hermann rajpurkar joshi end end neural networks wang xiong cui achieved promising results tasks outperform humans squad rajpurkar popular machine comprehension tests table shows simple example squad dataset sequence sequence models sutskever attention mechanism bahdanau encoder compresses source text decoder attention nism generates target words shown great capability handle natural language generation tasks machine translation luong xia text summarization rush nallapati dialogue systems williams encoder decoder networks directly map source input xed target sentence learn relationship natural language texts makes hard capture lot deep intrinsic details understand potential implication shi corresponding author conference neural information processing systems neurips montral canada passage rst super bowl feature quarterback teams pick draft classes manning selection nfl draft newton picked rst matchup pits picks draft newton carolina von miller denver question considered rst choice nfl draft answer manning table example squad dataset inspired recent success approaches machine comprehension tasks focus exploring knowledge help attention based models deeply comprehend text machine comprehension requires encode words passage question rstly methods seo wang xiong employ attention mechanism rnn based modeling layer capture interaction passage words conditioned question nally use mlp classier pointer networks vinyals predict answer span encoder mentioned common component models rnn based modeling layer input attention vectors supposed augment performance outputs models intuitively knowledge improve models measuring relevance generated sentence input source quesiton answering text generation different training data distributions benet sharing model high level semantic components guo pasunuru bansal paper propose macnet machine comprehension augmented encoder decoder mentary architecture applied variety sequence generation tasks begin pre training model contains rnn based encoding layer modeling layer transferring source sequence sequence model encoding concatenate outputs original encoder transferred encoder decoding rst input attentional vectors model transferred modeling layer combine outputs attentional vectors formulate predictive vectors solve class imbalance resulted high frequency phrases adopt focal loss lin reshapes standard cross entropy improve weights loss distribution verify effectiveness approach conduct experiments representative sequence generation tasks neural machine translation transfer knowledge machine comprehension model attention based neural machine translation nmt model experimental results method signicantly improves performance large scale datasets abstractive text summarization modify pointer generator networks recently proposed evaluate model cnn daily mail hermann gigaword rush datasets model obtains rouge scores english gigaword dataset improvement previous state art results literature related work machine comprehension teaching machines read process comprehend text answer questions called machine comprehension key problems articial intelligence recently rajpurkar released stanford question answering dataset squad high quality large scale benchmark inspired signicant works xiong pan cui seo wang xiong shen wang state art works attention based neural network models seo propose directional attention achieve query aware context representation wang employ gated self matching attention obtain relation question passage model rst surpass human performance squad paper pre trained architecture transferred nlp tasks sequence sequence model existing sequence sequence models attention focused generating target sequence aligning generated output token token input sequence approach proven successful nlp tasks neural machine translation bahdanau text summarization rush dialogue systems williams adapted applications including speech recognition chan image caption generation general models encode input sequence set vector representations recurrent neural network rnn second rnn decodes output sequence step step conditioned encodings work augment natural language understanding encoder decoder framework transferring knowledge supervised task transfer learning nlp transfer learning aims build learning machines generalize different domains following different probability distributions widely applied natural language processing tasks collobert glorot min seo hajishirzi mccann pan collobert propose unied neural network architecture learned unsupervised learning applied natural language processing tasks including speech tagging chunking named entity recognition semantic role labelling glorot propose deep learning approach learns extract meaningful representation review unsupervised fashion mccann propose transfer pre trained encoder neural machine translation nmt text classication question answering tasks pan propose transfer encoder pre trained discourse marker prediction model natural language inference model unlike previous works focus encoding unsupervised knowledge source extract multiple layers neural networks machine comprehension model insert sequence sequence model approach makes transfer directly compatible subsequent rnns augments text understanding attention mechanism machine comprehension model task description machine comprehension task given question passage length question passage goal predict correct answer subspan framework state art models structures popular works essentially combination encoding layer attention mechanism rnn based modeling layer output seo pan xiong describe model follows encoding layer use pre trained word vectors glove pennington character level embeddings transfer words vectors applies cnn characters word proved helpful handling vocab words kim use directional lstm concatenation model temporal interactions words genc directional lstm concatenation word character embedding vectors word contextual figure overview macnet framework comprising machine comprehension upper pre training sequence sequence model learned knowledge transferred representations question passage attention layer attention mechanisms commonly machine comprehenion model document representation emphasize key information capture long distance dependencies attention function fatt represents series normalized linear logical operations follow seo use directional attention bidaf passage question interacted alignment matrix query aware context representation modeling layer step use stacking lstm capture interaction passage words conditioned question gmodel layers uni directional lstm expected represent contexual information word passage question use simple mlp classier combination locate start end positions answer training dene training loss sum negative log probability true positions predicted distributions macnet architecture section shown figure introduce macnet transfers knowledge model model sequence sequence models typically implemented recurrent neural network encoder decoder framework framework directly models probability target sentence yty conditioned source sentence length sentence encoder model encoder reads source sentence word word generates hidden representation word attention mechanismembeddingencoding layerpredicted answermodeling layerpassagequestiontransferringembeddingpqaembeddingsource sentencexintegrationdecoder attentionmechanismytarget sentenceencoderbilstmtransferringlstmmachine comprehensionsequence sequencemodellstmbilstmbilstm fenc recurrent unit long short term memory lstm sutskever unit gated recurrent unit gru cho embedding vector hidden state paper use directional lstm recurrent unit consistent encoding layer model described section augment performance encoding use simple method exploit word representations learned task source sentence use directional lstm equation encoder obtain hidden state represents word perspective model instead conventional models directly send results equation decoder concatenate feed integration layer fint uni directional lstm means concatenation contextual tations sentence contain information machine comprehension knowledge decoder attention mechanism initialized representations obtained encoder decoder attention mechanism receives word embedding previous word training previous word reference sentence testing previous generated word step generates word decoder states computed fdec unidirectional lstm generated word hidden state attentional models attention steps summarized equations source context vector attention vector derive softmax logit loss trainable parameters function forms score referred content based function usually implemented feed forward network hidden layer common models attention vector fed softmax layer produce predictive distribution formulated macnet additionally send attention vector modeling layer pre trained model equation deeply capture interaction source target states attention state augmentation machine comprehension knowledge combine results attention vectors equation wqrt trainable parameters modeling layer helps deeply understand interaction contextual information output sequence different encoding layer inputs independent source sentences training denote parameters learned framework training dataset contains source target sequence pairs training process aims seeking optimal paramaters encodes source sequence provides output sentence close target sentence formula form popular objective maximum log likelihood estimation bahdanau xia arg max arg max logp results high frequency commonly expressions know output sentences nature class imbalance corpus inspired focal loss lin recently proposed solve foreground background class imbalance task object detection add modulating factor cross entropy loss simplifying modify equation arg max tunable focusing parameter case focusing parameter smoothly adjusts rate high frequency phrases weighted experiments machine comprehension use stanford question answering dataset training questions posed crowd workers wikipedia articles hidden state size lstm set select glove word embeddings use dimensional lters cnn character level embedding width dropout ratio use adadelta zeiler optimizer initial learning rate model achieves exact match score squad development dataset application neural machine translation rst evaluate method neural machine translation nmt task requires encode source language sentence predict target language sentence use architecture luong baseline framework gnmt attention parallelize decoder computation datasets evaluation wmt translation tasks english german directions translation performances reported case sensitive bleu papineni implementation details training nmt systems split data subword units bpe sennrich train layer lstms units bidirectional encoder embedding dimension use fully connected layer transform input vector size transferred neural networks model trained stochastic gradient descent learning rate began train steps steps start halving learning rate step batch size set dropout rate focal loss set squad dataset referred github squad statmt org translation task html statmt org translation task html nmt systems ende deen ende deen baseline baseline encoding layer baseline modeling layer baseline encoding layer modeling layer baseline random initalized framework baseline macnet table bleu scores ofcial test sets wmt english german performance baseline model medium present ablation experiments effectiveness macnet bleu attention context query attention query context attention bidaf bidaf self attention bidaf memory network results shown table line nmt model datasets performs better help macnet work medium conduct tion experiment evaluate individual tribution component model encoding layer modeling layer demonstrates effectiveness ablate modules add focal loss bleu scores test sets rise point shows signicance transferred knowledge finally add architecture ing layer modeling layer baseline model initialize randomly rnn layers observe performance drops indicates machine comprehension knowledge deep connections machine translation tasks ablation experiments found improvement modeling layer architecture bit modest believe transferring high level networks modeling layer help lot suitable structure networks contains deeper semantic knowledge abstractive information compared lower level layers encoding layer table performance different pre trained chine comprehension models nmt model deen means exact match score represents performance model squad dev set bleu results nmt model table explore different choices attention architectures fatt equation usually discrimination different models models impact performance method rst follow seo rate directions attention bidaf use place original attention nism respectively performance machine comprehension task drops lot fect results nmt models add self attention proposed fuse context widely ods wang weissenborn fortunately result nmt model fails pace performance pre train model finally apply memory network popular models pan performance squad rises lot nmt result similar original model series experiments denote model performance macnet positive correlation improvement figure performance different values summarization models words intra pointer pointer generator encoding layer pointer generator modeing layer pointer generator macnet cnn daily mail gigaword table rouge evaluation results cnn daily mail test set english gigaword test set table denotes rouge results mark taken corresponding papers table shows performance macnet ablation results architecture conjecture depend potential factors complexity extracted parts heterogeneity different tasks figure present models different focal loss affects performance models increase enlarges arrives performance gets worse raise means modulating factor close zero benet limited application text summarization verify effectiveness macnet abstractive text summarization typical application sequence sequence model use pointer generator baseline model applies encoder decoder architecture state art models text summarization evaluation metric reported scores rouge lin evaluate method high quality datasets cnn daily mail hermann gigaword rush cnn daily mail dataset use supplied pre process data contains training pairs validation pairs test pairs english gigaword dataset use released rush pre process obtain training pairs development set testing implementation details training hyperparameters similar pointer generator networks experiments important details follows input output vocabulary size hidden state size word embedding size use fully connected layer transform input vector size transferred neural networks train adagrad duchi learning rate initial accumulator value set results table shows performance methods competing approaches datasets compared original pointer generator model results macnet architecture outperform kinds rouge scores especially approach achieves state art results metrics gigaword cnn daily mail dataset similar nmt task encoding layer contributes improvement modeling layer stable gains evaluations table present summaries produced model original pointer generator model rst example summary given pointer generator model sense perspective logic model accurately summarizes article provides com abisee cnn dailymail com facebook namas article israeli warplanes raided hezbollah targets south lebanon guerrillas killed militiamen wounded seven troops wednesday police said reference israeli warplanes raid south lebanon macnet israeli warplanes attack hezbollah targets south lebanon hezbollah targets hezbollah targets south lebanon article dollar racked clear gains wednesday london forex market operators waited outcome talks white house congress raising national debt ceiling cutting american budget decit reference dollar gains market eyes debt budget talks macnet dollar racked clear gains london forex market racked gains table examples summaries english gigaword denotes pointer generator model details second example original model produces logical sentence output sentence expresses completely different meanings information article method correctly comprehends article provides high quality summary sentence paper propose macnet supplementary framework sequence sequence tasks transfer knowledge machine comprehension task variety tasks augment text understanding models experimental evaluation shows method signicantly improves performance baseline models benchmark datasets different nlp tasks hope work encourage research transfer learning multi layer neural networks future works involve choice transfer learning sources transfer learning different domains nlp work supported national nature science foundation china grant nos national youth notch talent support program experiments supported chengwei yao experiment center college computer science technology zhejiang university conclusion acknowledgments references translate iclr bahdanau cho bengio neural machine translation jointly learning align chan jaitly vinyals listen attend spell neural network large vocabulary conversational speech recognition acoustics speech signal processing icassp ieee international conference cho van merrienboer gulcehre bahdanau bougares schwenk bengio learning phrase representations rnn encoder decoder statistical machine translation emnlp collobert weston bottou karlen kavukcuoglu kuksa natural language processing scratch journal machine learning research cui chen wei wang liu attention attention neural networks reading comprehension acl volume duchi hazan singer adaptive subgradient methods online learning stochastic optimization journal machine learning research gehring auli grangier yarats dauphin convolutional sequence sequence learning arxiv preprint glorot bordes bengio domain adaptation large scale sentiment classication deep learning approach icml guo pasunuru bansal soft layer specic multi task summarization entailment question generation arxiv preprint hermann kocisky grefenstette espeholt kay suleyman blunsom teaching machines read comprehend nips peng qiu reinforced mnemonic reader machine comprehension corr joshi choi weld zettlemoyer triviaqa large scale distantly supervised challenge dataset reading comprehension acl kim convolutional neural networks sentence classication emnlp xiong zhu zhang zhou modeling source syntax neural machine translation acl volume lin goyal girshick dollar focal loss dense object detection iccv lin rouge package automatic evaluation summaries text summarization branches luong pham manning effective approaches attention based neural machine translation emnlp mccann bradbury xiong socher learned translation contextualized word vectors nips supervision data acl min seo hajishirzi question answering transfer learning large grained nallapati zhou dos santos gulcehre xiang abstractive text summarization sequence sequence rnns proceedings signll conference computational natural language learning nallapati zhai zhou summarunner recurrent neural network based sequence model extractive summarization documents aaai pan zhao cao cai memen multi layer embedding memory networks machine comprehension arxiv preprint pan yang zhao zhuang cai discourse marker augmented network reinforcement learning natural language inference proceedings annual meeting association computational linguistics volume long papers volume papineni roukos ward zhu bleu method automatic evaluation machine translation acl association computational linguistics paulus socher deep reinforced model abstractive summarization arxiv preprint emnlp text emnlp emnlp acl volume acl pennington socher manning glove global vectors word representation rajpurkar zhang lopyrev liang squad questions machine comprehension rush chopra weston neural attention model abstractive sentence summarization liu manning point summarization pointer generator networks sennrich haddow birch neural machine translation rare words subword units seo kembhavi farhadi hajishirzi bidirectional attention machine comprehension iclr sion kdd acm shen huang gao chen reasonet learning stop reading machine shi padhi knight string based neural learn source syntax acl sutskever vinyals sequence sequence learning neural networks nips vinyals fortunato jaitly pointer networks nips wang hamza florian multi perspective context matching machine comprehension arxiv preprint wang yang wei chang zhou gated self matching networks reading comprehension question answering acl weissenborn wiese seiffe making neural simple possible simpler conll williams asadi zweig hybrid code networks practical efcient end end dialog control supervised reinforcement learning acl volume schuster chen norouzi macherey krikun cao gao macherey klingner shah johnson liu ukasz kaiser gouws kato kudo kazawa stevens kurian patil wang young smith riesa rudnick vinyals corrado hughes dean google neural machine translation system bridging gap human machine translation corr xia tian lin qin liu deliberation networks sequence generation pass decoding nips xiong zhong socher dynamic coattention networks question answering iclr answering iclr xiong zhong socher mixed objective deep residual coattention question kiros cho courville salakhudinov zemel bengio attend tell neural image caption generation visual attention international conference machine learning zeiler adadelta adaptive learning rate method arxiv preprint zhou yang wei zhou selective encoding abstractive sentence summarization acl
