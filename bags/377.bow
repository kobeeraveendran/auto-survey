train agent read write li mengge guanghui mingkui qi school software engineering south china university technology pazhou laboratory university adelaide key laboratory big data intelligent robot ministry education seliushiya semenggehe scut edu cn edu cn qi edu au n j l c s c v v x r abstract reading writing research papers leged abilities qualied researcher master difcult new researchers e students fully grasp ability fascinating train intelligent agent help people read summarize pers discover exploit potential knowledge clues write novel papers existing works focusing summarizing e reading knowledge given text generating e writing text based given knowledge ability ously reading writing development cally requires agent fully understand knowledge given text materials generate correct ent novel paragraphs challenging practice paper propose deep reader writer draw network consists reader extract edge graphs kgs input paragraphs discover tial knowledge graph text writer generates novel paragraph reviewer reviews generated graph different aspects extensive experiments draw network outperforms considered lines state art methods agenda m agenda datasets code supplementary leased com menggehe draw introduction currently hundreds papers published online day small topics study wang et al shows scientists read papers year average researchers exhausted following sharply increased numbers papers standing research coming new ideas write novel papers gopen ja buenz practice writing novel papers requires abilities reading reasoning ability creative thinking nontrivial fresh researchers xiao et al fantastic agent help people especially authors contributed equally corresponding author copyright association advancement articial intelligence www aaai org rights reserved figure intuitive understanding draw network draw network reads multiple related works discovers potential knowledge writes new paragraph based knowledge graph reviews output uses feedback rewards improve quality writing new researchers read write building agent encounters challenges understand multiple related works agent needs capture complex logic related works trivial knowledge extraction methods min et al gerber chai yoshikawa et al achieve identifying entities texts extracting ships entities representing knowledge graph kg trouble covering potential connections entities hampers comprehensive understanding related works second generating kg agent required decode uent novel paragraph kg practice evaluate quality generated texts accurately open problem existing methods kedziorski et al wang et al adopt forcing scheme aims match tokens generated texts tokens target texts methods related workswriteknowledge graphnew paragraphscores focus token level matching ignoring level graph level evaluation generated texts paper propose method named deep writer draw draw network able read multiple texts discover potential knowledge write novel paragraph figure draw network consists e reader writer reviewer modules cally reader rst extracts kgs research texts discovers potential knowledge enrich kgs reader considers multi hop neighborhood predict new links conceptual nodes writer writes novel paragraph describe main idea enriched kgs graph attention network aggregates global local graph information inspired review process research papers propose reviewer module evaluate quality generated paragraphs return rewards feedback signals rene writer specic given generated paragraph reviewer output feedback signals including quality reward reects metric scores generated paragraph adversarial reward denotes probability generated paragraph passing turing test alignment reward represents matching score tween generated paragraphs enriched kgs way writer able write better paragraphs clearly represent key idea enriched kgs summary main contributions threefold propose deep reader writer draw network reads multiple research texts discovers potential knowledge write novel paragraph covering key idea source inputs propose feedback mechanism review generated paragraph consistent enriched kg generated paragraph human written greatly improving quality paragraph tion extensive experiments writer reviewer leads signicant improvements kgs text eration task outperforms state art methods related work automatic writing paperrobot wang et al forms automatic research assistant incrementally write chemical related research datasets enriches kgs predicting links input papers kgs according given title selects entities related title enriched kgs generate texts robot neglects consider multi hop neighborhood predict links important capturing potential relationships addition generated texts closely align kgs address use graph attention network consider multi hop neighborhood capturing complex hidden information inherently plicit neighborhood design reviewer measure quality generated text different mensions effectively align kgs particular draw network different multi document mary ling hui compresses lengthy document content relatively short paragraphs extract important knowledge discover potential knowledge multiple paragraphs predicting links writing novel paragraph link prediction translation based approaches al zhen et al lin et al widely link prediction result poor representation ability recently cnn based models dettmers et al nguyen et al proposed relation prediction methods focus entity neighborhood considering relationships nodes methods kipf welling schlichtkrull et al relationships entities hop neighbors consideration omit information multi hop neighborhood instead pose reader module capture semantic information multi hop neighborhood kg graph text task graph text active research area works generate texts based structured edge trisedya et al xu et al feng et al neural graph text models use different coders based gnn ribeiro gardent gurevych zhijiang et al huang et al vaswani et al architectures learn graph representations koncel kedziorski et al proposes novel graph transformer encoder leverages topological structure kgs generate texts ignores global graph information important text tion solve ribeiro et al introduce novel architecture aggregates global local graph information generate texts encoder decoder framework presents problems word repetition lack diversity solve issues propose reviewer module review generated paragraphs rene quality paragraphs feedback rewards reviewer consists modules review evaluate generated paragraphs real align given kgs order improve text generation ability proposed method paper focus generating novel paragraphs reading multiple ai related paragraphs end pose deep reader writer draw network consists modules reader writer reviewer shown figure understand sort textual logic given paragraphs reader rst reads extracts knowledge graphs kgs considering multi hop neighborhood reader predicts new links conceptual nodes potential knowledge enrich kgs writer adopts graph encoder encode rich semantic information kgs delivers text encoder generate novel paragraph inspired adversarial learning cao et al wang et al cao et al chen et al devise reviewer evaluate quality generated paragraph serves feedback signal rene writer relate details modules following sections figure overview deep reader writer draw network draw network consists modules reader writer reviewer given multiple related works reader rst extracts knowledge construct initial knowledge graphs kgs performs link prediction enrich kgs based enriched kgs writer captures global local topology information graph encoder generates novel paragraph text decoder particular reviewer employs feedback modules measure quality generated paragraph reader text graph extract textual logic given related paragraphs use standard sciie luan et al science domain information extraction system constrcu edge graphs specically output sciie system list triplets triplet consists entities corresponding relation knowledge graph noted gi v r v node set r edge set n represents number nodes v r represent extracted entities relations respectively initial knowledge graph gi exploit potential knowledge address perform link prediction predict new links entities based initial kgs link prediction given kgs gi obtain entity bedding rd relation embedding rij rd separated embedding layers feature sion formally given entity embedding vj relation embedding rij triplet represented rij aggregate information introduce auxiliary edges entity n hop hood entity n hop neighborhood sum embeddings relations path auxiliary relation embedding apply linear formation update entity representation rd rij trainable parameter denotes catenation operation particular entity vi involved multiple triplets neighborhood denoted denotes k th neighborhood th entity learn importance triplet entity apply self attention calculate attention weights follows exp exp help attention weights update feature vi rd fusing information neighborhood e vi k trainable parameter sigmoid function based original relation feature rij apply ear transformation obtain updated relation embedding rij rd updating node relation embeddings need determine relationship given entities intuitive way calculate ability triple following convkb nguyen et al train scoring function perform relation prediction follows sm rm denotes convolution operation set convolution lters fc linear transformation layer following nathani et al assign score sm triplet vi rm vj eqn indicates probability triplet holds entity rst traverse entities relationships construct triples select triplet highest score new link way reader capture potential relations different nodes derive new graph gp finally denote enriched knowledge graph g gi gp writer graph text based enriched graph g n entities propose writer generate novel paragraphs consists bcedafhggraph encoderwriter related workstext decoderbcdaafefhgknowledge graphreaderrewardrevieweralignmentmoduleadversarialmodulequalitymodulenew paragraphreading writing researchpapers recently consider task paper propose anew paper consider graph encoder text decoder specically writer rst uses graph encoder extract knowledge resentations writes new paragraph text decoder vaswani et al graph encoder comprehensive understanding kg g rst step generate desired paragraph difcult directly capture rich semantic information knowledge graph g address extract edge representations sub encoders e graph encoder local graph encoder following cge lw ribeiro et al integrate global context information local topology information generate new paragraphs aggregate global context information rst nate node features v feed graph encoder follows vn standard transformer encoder vaswani et al contains multi head self attention layers feed forward networks global graph encoder treat knowledge graphs g fully connected graph labeled edges based self attention mechanism global graph encoder suitable discovering global correlation nodes node rd ability capture nodes information better represent interaction nodes need build local relations node hood global graph encoder explicitly consider graph topology information address use local graph encoder model local relations node rst calculate attention weights adjacent nodes different types relationships considerable discrepancies impact fusing tion based attention weights obtain hidden node features aij rij jni jni aij denotes model parameters denotes hidden features encode local interaction th node neighborhood ni denotes bourhood th node perform multi head attention operation learn structural information ferent perspectives employ gru cho et al merge local information different layers follows hi nal node representation hi rd text decoder based node representations h use standard transformer decoder vaswani et al generate novel paragraph t words auto regression manner step t text decoder consumes previously generated tokens additional input outputs probability pt candidate vocabularies train writer supervised learning follows lsl t yt ground truth hot vector step t generates words selecting element highest score step practice text decoder use sequence generation models lstm hochreiter schmidhuber reviewer feedback rewards encoder decoder framework great progress sequence generation tasks including text rization image captioning suffers problems training sample framework tends use word ground truth generation step candidate words reasonable leads lack diversity generated text language complex requires evaluate quality generated paragraph different dimensions grammatical correctness topic relevance language logic coherence inspired review process research paper propose reviewer module review generated paragraph different dimensions output reviewer auxiliary training signal optimize writer similar researchers polishing paper based reviews specically design feedback rewards viewer use metric scores generated graph reward meet rules metrics second train turing test discriminator determine paragraph generated agent written human draws idea adversarial training requires paragraph conform natural language cation design alignment module align generated paragraphs corresponding enriched edge graphs ensures correctness completeness generated texts different teacher forcing ods reviewer focuses sentence level graph level alignment given generated paragraph evaluation processes non differentiable discussed seqgan yu et al discrete signals limited passing gradient update reviewer writer address denote outputs reviewer rewards r maximize expectation rewards e reinforcement learning formally goal reviewer represented max ep denotes trainable rameters model paragraph generated writer based generation probability p w t specically reward function denoted ar m r correspond modules reviewer ar m r control contribution corresponding reward following policy gradient ods williams schulman et al solve problem batch training follows lrl ep logp logp b b b b training batch size introduce reward modules detail quality reward given generated paragraph calculate quantitative metrics et al meteor denkowski lavie cider vedantam zitnick parikh directly metrics training reward boost sentence generation quality paper simply adopt bleu score reward bleu score popular automated inexpensive metrics practice bleu replaced metric needs optimized adversarial reward based paragraph ule acts discriminator determine manual annotation real generated machine fake ing yu et al use convolutional neural network cnn extract text features capture sequence information shown exhibited high performance complicated sequence classication task zhang cun specically given generated paragraph rst concatenate token embedding text tion use different numbers kernels different window sizes extract different features text sentation produce new feature map applying max pooling operation perform fully connected layer sigmoid activation output probability notes probability input text real calculation formulated inspired adversarial training cao et al module aims minimize performance gap humans writer alignment reward paragraph supposed align enriched kg g generated writer according g sense propose compute similarity g based attention mechanism given abstract t words rst use long short term memory lstm extract text representation c rd t t following attngan xu et al obtain hidden representation follows qt softmax wv h d wq wk wv trainable parameters d scaling factor h rdn node features obtained writer help self attention nism vaswani et al hidden feature qt rd fuses text representations merges graph formation calculate cosine similarity ing score follows t t ct far obtain rewards reviewer modules finally train draw network dene overall training loss follows l lsl rl lrl rl trade parameter lsl trains draw network supervised learning lrl allows draw network explore diverse generation ment learning evaluate generation multiple entations experiments datasets agenda dataset agenda popular kgs text datasets concludes pair samples collected proceedings ai conferences sample consists title abstract sponding kg extracted sciie system kg composed recognized scientic terms tionships particular types scientic terms include task metric method material types tionships include conjunction feature compare evaluate hyponym m agenda dataset demonstrate tiveness draw network create new dataset called m agenda specically rst calculate sine similarity abstract agenda dataset select related instances combine new data example m agenda dataset experimental settings implementation details draw network consists design modules e reader writer reviewer rst train reader writer reviewer agenda dataset use trained reader writer model m agenda generate novel paragraphs speed convergence early training adopt different ing strategies module reader rst use bordes et al train entity relation dings aggregate information passed hop neighborhood update embedding node ing nathani al use adam optimization initial learning rate writer pre train epochs early stopping following ribeiro et al use adam optimization initial learning rate ensure generation effect set maximum generation length reviewer pre train adversarial module sgd optimization initialize learning rate pre training graph encoder alignment module use model rameters writer addition systematically adjust values ar m r conduct ablation studies nd experimental results different coefcient combinations uctuate causing little effect results writer reviewer obtains best results ar m r set trade parameter rl implement method pytorch model graphwriter graformer cge lw writer reviewer bleu meteor cider model writer writer reviewer bleu meteor cider table quantitative evaluations generation systems agenda dataset higher better table ablation study modules reviewer agenda dataset paragraph turing test results human machine written human written draw table quantitative results turing test model paperrobot cge lw draw grammar coherence informativeness table automatic evaluations results higher better evaluation metrics demonstrate quality erated paragraphs report quantitative results human study results divide evaluation parts kgs text evaluation overall performance evaluation kgs text evaluations adopt general titative evaluation metrics e bleu papineni et al meteor denkowski lavie cider tam zitnick parikh evaluate reviewer addition demonstrate realness paragraphs generated model set ing test specically randomly select abstracts shufe nd evaluation set half stracts written authors rest generated writer reviewer test turkers amazon mechanical turk amt determine paragraphs evaluation set written humans overall performance evaluation set human study rate abstracts generated draw network cge lw paperrobot model randomly select generated paragraphs score terms grammar informativeness coherence amazon mechanical turk amt specically metric grammar measures paragraphs written formed english metric informativeness denotes paragraphs use appropriate scientic terms metric herence denotes generated text conforms general specications example complete abstract clude brief introduction task describe solution analyze discuss results metric scribed contains levels rankings bad good following relation prediction task nathani et al evaluate link prediction method reader proportion correct entities n ranks kgs text evaluation agenda dataset verify model kgs text task compare writer reviewer state art models cluding graphwriter koncel kedziorski et al graformer schmitt al cge lw ribeiro et al agenda dataset results report results method compared models respect quantitative evaluation metrics table shown table writer reviewer achieves better performance compared models quantitative evaluation metrics specically reviewer outperforms state art method cge lw points bleu points meteor points cider results demonstrate superiority writer reviewer kgs text task addition carry human evaluation strate effectiveness writer reviewer specic paragraph evaluation set ask human choose paragraphs written authors results table nearly half paragraphs generated writer reviewer reviewed written humans critically paragraphs written humans chosen written ai system results demonstrate writer reviewer erate realistic paragraphs similar written humans ablation studies reviewer investigate effect different modules reviewer conduct ablation study shown table writer combined ules reviewer arbitrarily obtains better performance writer demonstrates effectiveness modules reviewer writer combined modules viewer writer reviewer achieves best performance evaluation m agenda dataset effectiveness draw network duct experiments m agenda dataset agenda dataset provide ground truth conduct human study instead quantitative evaluations specically metric human study average scores paragraphs rated humans nal score results draw report experimental results draw network compared methods table results draw network achieves best performance terms grammar coherence formativeness specically paperrobot wang et al initial kgs paperrobot entities relations global scene level contextual information spatial context recurrent convnet model wikipedia multilingual ner systems local image de scriptors tion spatial congurations paper propose novel approach multilingual named entity recognition tasks proposed method based semantic similarity measure improve word retrieval performance wikipedia type words text documents build efcient query language model allows users similar information entities clusters different domains speech tags generated user s document representation knowledge base system evaluated state art approaches trained object covering entities cge lw draw paper propose spatial context recurrent convnet model incorporate global scene level contextual information spatial context recurrent convnet model object retrieval contextual information candidate boxes object retrieval positional language model captures contextual information candidate boxes object retrieval proposed system evaluated tac kbp data experimental results proposed system signicantly improve entity linking performance covering entities paper propose novel approach entity based statistical language model based information exploits local contexts global world improve entity performance propose spatial context recurrent convnet model integrate global context features local image de spatial congurations global scene level contextual spatial context recurrent convnet model recurrent network local global information guide search candidate boxes object retrieval covering entities table example outputs models better visualize generated text omit information irrelevant comparisons repetitive words represented red entities included kgs represented orange potential knowledge represented blue corresponding superscript method paperrobot table accuracy link prediction m agenda dataset values percentage obtains poor performance neglect logical structure entities cge lw ribeiro et al takes advantage graph information effectively achieves points terms metrics ignores fact generated graphs supposed match kgs different methods draw network performs link prediction multi hop information reader matches graphs generated paragraphs achieves best performance ablation experiments reader found supplementary material results reader shown table report perimental results link prediction method reader paperrobot method achieves scores outperforming paperrobot points respectively demonstrates effectiveness link prediction method visualization analysis shown table visualize generated paragraph draw network ization results found supplementary material draw network ability cover entities represented orange paperrobot mentions entities given kg addition cge lw tends repeat unrelated entities sentences represented red help reviewer generated text draw network uent grammatically correct draw work able discover potential relationships entities represented blue superscript conclusions future work paper propose deep reader writer draw work reads multiple ai related abstracts writes new paragraph represent enriched knowledge combining potential knowledge covering topics mentioned source abstracts inspired review process propose reviewer rate quality generated texts ent dimensions serve feedback signals rene draw network ablation experiments demonstrate tiveness method writer reviewer achieves state art results kgs text generation task terms human study generations draw work successfully pass turing test confuse turkers future study extend draw network write complete paper iterative manner develop niques discover novel ideas creating new entities acknowledgments work partially supported key area search development program guangdong province national natural science foundation china nsfc key project program dong introducing innovative entrepreneurial teams international cooperation open project state key laboratory subtropical building science south china university technology fundamental research funds central universities references b repulsive bayesian sampling diversied attention modeling workshop neurips bordes usunier n garcia duran weston j yakhnenko o translating embeddings modeling multi relational data neurips buenz e j essential elements high impact tic writing nature doi cao j guo y wu q shen c huang j tan m adversarial learning local coordinate coding icml cao j guo y wu q shen c huang j tan m improving generative adversarial networks local coordinate coding ieee transactions pattern analysis machine intelligence cao j mo l zhang y jia k shen c tan m multi marginal wasserstein gan neurips chen p zhang y tan m xiao h huang d gan c generating visually aligned sound videos ieee trans image process cho k merrienboer b v aglar glehre bahdanau d bougares f schwenk h bengio y learning phrase representations rnn encoder decoder statistical machine translation emnlp denkowski m j lavie meteor universal language specic translation evaluation target language acl dettmers t minervini p stenetorp p riedel s convolutional knowledge graph embeddings aaai feng n jinpeng w jin ge y rong p chin yew l operation guided neural networks high fidelity data text generation emnlp gerber m chai j nombank study implicit arguments nominal predicates acl gopen g d ja s science scientic writing american scientist hochreiter s schmidhuber j long short term memory neural computation huang d chen p zeng r du q tan m gan c location aware graph convolutional networks video question answering aaai kipf t welling m semi supervised tion graph convolutional networks iclr koncel kedziorski r bekal d luan y lapata m hajishirzi h text generation knowledge graphs graph transformers naacl hlt lin y liu z sun m liu y zhu x ing entity relation embeddings knowledge graph completion aaai ling f u hui z multi document summary lda spectral clustering computer engineering applications luan y l ostendorf m hajishirzi h multi task identication entities relations erence scientic knowledge graph construction emnlp min z jie z jian s guodong z ite kernel extract relations entities flat structured features acl nathani d chauhan j sharma c kaul m learning attention based embeddings relation tion knowledge graphs acl nguyen d q nguyen t nguyen d q phung d q novel embedding model knowledge base completion based convolutional neural network naacl hlt papineni k roukos s ward t zhu w bleu method automatic evaluation machine translation acl ribeiro l gardent c gurevych enhancing amr text generation dual graph representations emnlp ijcnlp ribeiro l f r zhang y gardent c gurevych modeling global local node contexts text generation knowledge graphs transactions association computational linguistics schlichtkrull m kipf t bloem p berg r titov welling m modeling relational data graph convolutional networks eswc schmitt m ribeiro l f r dufter p gurevych schutze h modeling graph structure relative position better text generation knowledge graphs arxiv schulman j wolski f dhariwal p radford klimov o proximal policy optimization algorithms arxiv trisedya b jianzhong q rui z wei w lstm triple encoder sentence generation rdf data acl vaswani shazeer n parmar n uszkoreit j jones l gomez n kaiser l u polosukhin attention need neurips vedantam r zitnick c l parikh d cider consensus based image description evaluation cvpr wang h wang j wang j zhao m zhang w zhang f xie x guo m graphgan graph tion learning generative adversarial nets aaai wang q huang l jiang z knight k ji h bansal m luan y paperrobot incremental draft eration scientic ideas acl williams r j simple statistical gradient following algorithms connectionist reinforcement learning machine learning xiao l wang l h jin y copy rewrite hybrid summarization hierarchical ment learning aaai xu k wu l guo wang z yu m chen l sheinin v sql text generation graph sequence model emnlp xu t zhang p huang q zhang h gan z huang x x fine grained text age generation attentional generative adversarial works cvpr yoshikawa k riedel s hirao t asahara m sumoto y coreference based event argument relation extraction biomedical text journal biomedical tics yu l zhang w wang j yu y seqgan sequence generative adversarial nets policy gradient aaai zhang x lecun y text understanding scratch arxiv zhen w jianwen z jianlin f zheng c knowledge graph embedding translating planes aaai zhijiang g yan z zhiyang t wei l densely connected graph convolutional networks graph sequence learning transactions association computational linguistics references b repulsive bayesian sampling diversied attention modeling workshop neurips bordes usunier n garcia duran weston j yakhnenko o translating embeddings modeling multi relational data neurips buenz e j essential elements high impact tic writing nature doi cao j guo y wu q shen c huang j tan m adversarial learning local coordinate coding icml cao j guo y wu q shen c huang j tan m improving generative adversarial networks local coordinate coding ieee transactions pattern analysis machine intelligence cao j mo l zhang y jia k shen c tan m multi marginal wasserstein gan neurips chen p zhang y tan m xiao h huang d gan c generating visually aligned sound videos ieee trans image process cho k merrienboer b v aglar glehre bahdanau d bougares f schwenk h bengio y learning phrase representations rnn encoder decoder statistical machine translation emnlp denkowski m j lavie meteor universal language specic translation evaluation target language acl dettmers t minervini p stenetorp p riedel s convolutional knowledge graph embeddings aaai feng n jinpeng w jin ge y rong p chin yew l operation guided neural networks high fidelity data text generation emnlp gerber m chai j nombank study implicit arguments nominal predicates acl gopen g d ja s science scientic writing american scientist hochreiter s schmidhuber j long short term memory neural computation huang d chen p zeng r du q tan m gan c location aware graph convolutional networks video question answering aaai kipf t welling m semi supervised tion graph convolutional networks iclr koncel kedziorski r bekal d luan y lapata m hajishirzi h text generation knowledge graphs graph transformers naacl hlt lin y liu z sun m liu y zhu x ing entity relation embeddings knowledge graph completion aaai ling f u hui z multi document summary lda spectral clustering computer engineering applications luan y l ostendorf m hajishirzi h multi task identication entities relations erence scientic knowledge graph construction emnlp min z jie z jian s guodong z ite kernel extract relations entities flat structured features acl nathani d chauhan j sharma c kaul m learning attention based embeddings relation tion knowledge graphs acl nguyen d q nguyen t nguyen d q phung d q novel embedding model knowledge base completion based convolutional neural network naacl hlt papineni k roukos s ward t zhu w bleu method automatic evaluation machine translation acl zhen w jianwen z jianlin f zheng c knowledge graph embedding translating planes aaai zhijiang g yan z zhiyang t wei l densely connected graph convolutional networks graph sequence learning transactions association computational linguistics ribeiro l gardent c gurevych enhancing amr text generation dual graph representations emnlp ijcnlp ribeiro l f r zhang y gardent c gurevych modeling global local node contexts text generation knowledge graphs transactions association computational linguistics schlichtkrull m kipf t bloem p berg r titov welling m modeling relational data graph convolutional networks eswc schmitt m ribeiro l f r dufter p gurevych schutze h modeling graph structure relative position better text generation knowledge graphs arxiv schulman j wolski f dhariwal p radford klimov o proximal policy optimization algorithms arxiv trisedya b jianzhong q rui z wei w lstm triple encoder sentence generation rdf data acl vaswani shazeer n parmar n uszkoreit j jones l gomez n kaiser l u polosukhin attention need neurips vedantam r zitnick c l parikh d cider consensus based image description evaluation cvpr wang h wang j wang j zhao m zhang w zhang f xie x guo m graphgan graph tion learning generative adversarial nets aaai wang q huang l jiang z knight k ji h bansal m luan y paperrobot incremental draft eration scientic ideas acl williams r j simple statistical gradient following algorithms connectionist reinforcement learning machine learning xiao l wang l h jin y copy rewrite hybrid summarization hierarchical ment learning aaai xu k wu l guo wang z yu m chen l sheinin v sql text generation graph sequence model emnlp xu t zhang p huang q zhang h gan z huang x x fine grained text age generation attentional generative adversarial works cvpr yoshikawa k riedel s hirao t asahara m sumoto y coreference based event argument relation extraction biomedical text journal biomedical tics yu l zhang w wang j yu y seqgan sequence generative adversarial nets policy gradient aaai zhang x lecun y text understanding scratch arxiv
