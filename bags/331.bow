multi fact correction abstractive text summarization yue shuohang zhe jackie chi kit jingjing mcgill university dynamics research yue mcgill shuowa zhe gan cheng jingjl com abstract pre trained neural abstractive summarization systems dominated extractive gies news summarization performance terms rouge generated abstractive summaries face pitfall factual inconsistency generating correct facts respect source text address challenge propose fact suite factual correction models leverages knowledge learned tion answering models corrections system generated summaries span tion models employ single masking strategies iteratively regressively replace entities order ensure source text semantic consistency retaining syntactic structure maries generated abstractive tion models experiments els signicantly boost factual consistency system generated summaries cing summary quality terms matic metrics human evaluation introduction informative text summarization aims shorten long piece text preserving main message existing systems divided main types extractive abstractive tractive strategies directly copy text snippets source form summaries abstractive strategies generate summaries containing novel sentences found source despite fact extractive strategies simpler pensive generate summaries grammatically semantically correct tive strategies increasingly popular thanks exibility coherency vocabulary diversity zhang work rst author intern microsoft cnn quarter million tralian homes businesses power decade storm battered ney nearby areas people isolated ood waters roads cut able reach days quarter million australian homes businesses power decade quarter million australian homes businesses power decade storm victims including killed injured identied senior high school students second senior high school ruzhou city central china henan province local police said friday killed injured central china school shooting killed injured central china school shooting clare catholic primary school ingham met equality leaders city council discuss complaint pupil family council supporting school ensure policies appropriate muslim school accused ing equality act refusing wear scarves catholic school accused ing equality act refusing wear scarves cnndm source summary corrected spanfact gigaword source generator summary corrected spanfact xsum source bertabs summary corrected spanfact table examples factual error correction ent summarization datasets factual errors marked red corrections proposed spanfact models marked orange recently advent transformer based models vaswani pre trained self supervised objectives large text corpora devlin radford lewis raffel abstractive marization models surpassing extractive ones automatic evaluation metrics rouge lin studies falke goodrich kryscinski wang durmus maynez observe despite high rouge scores system generated abstractive summaries factually inconsistent spect source text factual inconsistency known problem conditional text tion requires models generate readable text faithful input document quently sequence sequence generation models need learn balance signals source faithfulness learned language eling prior uency kryscinski dual objectives render abstractive tion models highly prone hallucinating content factually inconsistent source ments maynez prior work pushed frontier teeing factual consistency abstractive rization systems focus proposing tion metrics specic factual consistency multiple human evaluations shown rouge bertscore zhang relates poorly faithfulness kryscinski maynez evaluation models range fact triples goodrich textual entailment predictions falke adversarially pre trained classiers kryscinski question answering systems wang durmus worth noting based evaluation metrics surprisingly high correlations human judgment factuality wang indicating models robust capturing facts benet summarization tasks hand work focuses model design incorporate factual triples cao zhu textual entailment falke boost factual consistency generated summaries models efcient boosting factual scores expense signicantly lowering rouge scores generated summaries happens models struggle generating pivotal content retaining true facts eventual propensity sacricing mativeness sake correctness mary addition models inherit bone generative models suffer cination despite regularization complex knowledge graphs text entailment signals work propose spanfact suite neural based factual correctors improve summary factual correctness sacricing informativeness ensure retention tic meaning original documents ing syntactic structures generated advanced summarization models focus factual edits entities major source hallucinated rors abstractive summarization systems tice kryscinski maynez proposed model inspired observation fact checking model reliable medium assessing entity included summary fact wang mus knowledge rst adapt knowledge enhance tive summarization compared sequential eration models incorporate complex edge graph nli mechanisms boost ity approach lightweight ily applied system generated summaries retraining model empirical results multiple summarization datasets proposed approach signicantly improves rization quality multiple factuality measures sacricing rouge scores contributions summarized follows propose spanfact new factual correction framework focuses correcting erroneous facts generated summaries generalizable summarization system propose ods solve multi fact correction problem single multi span selection iterative auto regressive manner respectively imental results multiple summarization marks demonstrate approach cantly improve multiple factuality measurements huge drop rouge scores related work general neural based encoder decoder ture abstractive summarization rst posed rush later work proves structure better encoders lstms chopra grus pati able capture range dependencies reinforcement learning methods directly optimize rization evaluation scores paulus drawback earlier neural based rization models inability produce figure training example created span prediction model upper right auto regressive fact correction model right vocabulary words model ate words based xed vocabulary proposes pointer generator work copy words directly source pointer network vinyals addition traditional sequence sequence generation model abstractive summarization starts shine advent self supervised algorithms low deeper complicated neural networks transformers vaswani learn diverse language priors large scale pora models bert devlin gpt radford bart lewis achieved new state art performances abstractive summarization liu lapata lewis zhang shi fabbri models netune pre trained transformers supervised summarization datasets tain pairs source summary inconsistency encoder decoder architectures widely abstractive summarization systems inherently difcult control prone lucination vinyals koehn knowles lee leads factual system generated summary uent unfaithful source cao studies shown system generated abstractive summaries factual errors falke kryscinski discovered rouge scores recent studies proposed new ods ensure factual consistency tion cao zhu pose rnn based transformer based decoders attend source extracted edge triples respectively pose entailment reward augmented likelihood training objective falke proposes rerank beam results based entailment scores source fact correction models inherently ferent models focus correcting summaries generated model models trained objective dicting masked entities identied fact tion figure learn entity masks system generated summaries single multi span selection mechanism figure similar work proposed concurrently meng tune bart lewis model distant vision examples use post editing model factual error correction multi fact correction models section describe models proposed factual error correction span fact rection model auto regressive fact rection model methods rely span selection different masking prediction strategies spanfact collectively problem formulation let document summary pair source sequence tokens target sequence tokens abstractive marization model aims model conditional figure model architecture left span fact correction model right auto regressive fact correction model likelihood factorized product denote preceding tokens tion conditional maximum likelihood jective ideally requires summarization models optimize informativeness rectness reality fails models high propensity leaning informativeness correctness suppose summarization system generates sequence tokens form summary factual correction models aim edit informative incorrect summary metric measuring factual consistency source system summary span selection dataset fact correction models inspired span selection task ing comprehension tasks question ing figure shows examples span lection datasets created training span auto regressive fact correction models respectively query reference summary masked passage corresponding source document marized entity appears multiple times source document rank based fuzzy string matching scores variation enshtein distance query sentence work use spacy ner tagger honnibal montani identify entities data construction source sentence containing entity models explicitly learn predict span masked entity pointing specic ken pointer network vinyals original tokens replaced tokens different lengths span fact correction model iteratively mask replace entity time auto regressive model masks entities taneously replace auto regressive fashion left right figure shows overview models comparing els span fact correction model works ter errors exist draft mary prediction mask relatively independent hand auto regressive fact correction model starts skeleton summary entities masked robust summaries tain factual errors span fact correction model iterative setting model aims conduct entity correction answering query tains mask time suppose system summary entities time step mask entity use masked sequence query span model prediction placed masked slot query ate updated system summary step given source text masked query iterative rection model aims predict answer span modeling start end span selection use model adds separate non linear ers transformers pointers start end token position answer ize fact correction model pre trained bert model devlin perform netuning span selection datasets ated summarization datasets figure input bert model tion segments masked query source separated special delimiter markers sep token quence assigned embeddings token embedding position embedding tion embedding embeddings summed single vector fed multi layer transformer model input vectors represents depth stacked layers mhatt layer normalization multi head attention operations vaswani layer provides hidden states input tokens rich textual information start end answer span predicted astart aend number encoder hidden states trainable rameters nal span selected based argmax eqn constraint pstart pend pend pstart auto regressive fact correction model disadvantage style span prediction strategy sequence contains factual errors masking entity time lead highly erroneous skeleton summary com huggingface transformers segmentation embedding distinguish query special tokens cls sep source models start model making tions wrong entities later quence masking entity time essentially greedy local method prone error mulation alleviate issue propose new sequential fact correction model handle errors global manner beam search cally mask entities simultaneously use novel auto regressive span selection coder predict llers multiple masks quentially assume dependency masks earlier predicted entities corrected context better tions later steps given source text draft summary model rst masks entities masks leaves skeleton summary query concatenate query source ilar section inputs encoder inputs fed bert obtain contextual den representations maskt select encoder hidden states masks partial input auto regressive transformer based decoder unlike generation tasks require eos ken indicate end decoding decoder runs steps predict answer spans masks step rst fuse hidden sentation mask ken previously predicted entity representation sent sent sent sentation cls token denotes vector concatenation input fed transformer coder eqn generate coder hidden state time step based use pointer network predict start end positions answer entity source encoder hidden states achieved cross attention encoder den states similar eqn tion results distributions encoder hidden states start end span positions nal prediction start end positions mask obtained taking argmax selecting start end indexes pointer position distributions pstart arg pend arg astart aend constraint pstart pend pend pstart based start end positions dicted entity obtain predicted entity representation time step mean span encoder hidden states sent mean hpend input step worth noting decoding argmax operations eqn differentiable model trained based start end positions ground truth answer start end logits eqn makes gradient propagates encoder encoder hidden states compose sent eqn carry gradients inference beam search best sequence predicted spans source replace masks compared conventional pointer network vinyals points token time sequential span selection decoder exibility replace mask number entity tokens required summary factual correction experiment section present results spanfact multiple summarization datasets experimental setup training data fact correction models generated described section cnn dailymail hermann xsum narayan gigaword graff rush statistics dataset provided table ing entity corresponding span source point answer span cls token inference swer span predicted cls token place original masked entity answer span softmax computing loss propagation datasets docs train val test doc len summ len mask cnn dailymail xsum gigaword table comparison summarization datasets train validation test set splits average document summary length numbers words report average number entity masks reference summary dataset fact correction models implemented huggingface transformers library wolf pytorch paszke initialize encoder models point uncased large bert model trained english data squad periments source target texts kenized bert sub words tokenizer max sequence length set encoder use shallow transformer decoder auto regressive span selection decoder pre trained bert large encoder robust selecting right spans single span tion task pointers section transformer decoder hidden units feed forward intermediate size layers models netuned span tion data epochs batch size adamw optimizer loshchilov hutter initial learning rate training learning rate schedule follows linear decay scheduler ing inference use beam search constraint distance start end pointer best model checkpoints chosen based performance validation set experiments conducted quadro rtx gpus memory evaluation metrics use automatic evaluation metrics uate models rst rouge lin standard summarization quality metric high correlation summary ness news domain kryscinski rouge criticized poor correlation factual consistency kryscinski wang use ditional automatic metrics specically focus factual consistency factcc kryscinski qgqa factcc rouge qgqa factcc rouge datasets split encoders span auto regressive split encoders span auto regressive split encoders span auto regressive transformerabs split encoders span auto regressive sent table factual correctness scores rouge scores cnn dailymail test set qags wang factcc pre trained binary classier evaluates tuality system generated summary ing consistent inconsistent source classier trained sarial examples obtained heuristically injecting noise reference summaries addition recent work proposed based models factuality evaluation wang durmus maynez wang showed evaluation models higher correlation human judgements factuality compared factcc kryscinski include implementation question eration question answering model qgqa following wang evaluation metric factuality model generates set questions based system generated summary answers questions ing source summary obtain sets answers answers compared answer similarity ric token level averaged similarity metric questions qgqa able obtain evaluation model code wang durmus maynez authors stage making code public pre trained unilm model question generation ing model question answering model tuned newsqa trischler answer conditional task wang model pre trained squad rajpurkar datasets split encoders span auto regressive split encoders span auto regressive transformerabs split encoders span auto regressive sent table factual correctness scores rouge scores xsum test set score answers generated highly faithful system summary similar ated source baselines following abstractive compare summarization baselines cnndm xsum use bertsumabs transformerabs liu lapata tion compare gehrmann gigaword use generator base parse models song comparison factual correction baseline compare encoder pointer split encoder shah employs similar setting masking entities source uses dual encoders copy generate source masked query fact update compared span tion models mask ber tokens models aim regenerate mask query based source words decoder regenerates sequence ken token pointer generator herits backbone generative models fer hallucination experimental results tables summarize results cnn dailymail xsum gigaword datasets respectively block tables compares original summarization model output com split qgqa factcc rouge datasets genparse base split encoders span auto regressive genparse split encoders span auto regressive pointer generator split encoders span auto regressive sent bertabs better worse span original auto regressive original span auto regressive transformerabs better worse span original auto regressive original span auto regressive better worse span original auto regressive original span auto regressive table factual correctness scores rouge scores gigaword test set table human evaluation results pairwise ison factual correctness randomly sampled articles corrected outputs obtained baseline proposed models cnn dailymail table correction models signicantly boost factual consistency measures qgqa factcc large margins small drops rouge shows models ability improve correctness system generated summaries ing informativeness comparing proposed models observe span model performs better auto regressive model expected cnn dailymail erence summaries tend extractive summarization models tend errors summary narayan iterative procedure span model robust high precision correct context query minimum negative inuence current errors reected high scores qgqa factcc models tested qgqa factcc based comparing system generated summary source text high score means high semantic larity system summary source xsum table gigaword table correction models boost factual sistency measures large margins slight drop rouge average encouraging abstractive summarization models use complex factual controlling ponents generation drops rouge points zhu notice qgqa factcc scores summarization models lower cnn dailymail scores especially low xsum likely data struction protocol xsum rst tence source document mary remainder article source result entities appear reference summary appear source cause abstractive summarization els hallucinate severely factual errors maynez system summaries contain errors span model relies answering single mask query wrong context condition step negatively affects performance model contrast strategy masking entities provide auto regressive model better query entity replacement serve table auto regressive model forms better span model xsum human evaluation provide qualitative analysis proposed models conduct human evaluation wise comparison cnn dailymail summaries enhanced different correction strategies select state art abstractive tion models backbones collect sets pairwise summaries setting original span corrected original auto regressive corrected span rected auto regressive corrected sets randomly selected samples total ples labeled amt tuckers pair anonymized order annotators amazon mechanical turk amt asked judge factually correct based factcc dataset factcc score qaqg human eval corr span auto regressive reviewers valuable comments cial thanks yuwei fang members microsoft dynamics research team feedback suggestions table test results human annotated dataset provided factcc kryscinski performance comparisons original summaries summaries corrected spanfact references source document shown table summaries models chosen frequently factually correct compared nal correction models erences comparable addition test fact tion models factcc test set provided kryscinski manually checked outputs table shows results inal summaries summaries corrected models terms automatic fact evaluation manual evaluation generated summary sentences incorrect span model able correct right auto regressive model able correct sentences labeled correct annotators kryscinski models wrong changes entities keeping entities unchanged changed equivalent entities conclusion present spanfact suite factual rection models use span selection mechanisms replace multiple entity masks time spanfact fact correction stractive summaries empirical results models improve factuality summaries generated state art abstractive rization systems huge drop rouge scores future work plan apply method type spans noun phrases verbs clauses acknowledgments research supported microsoft dynamics research canada far chair program like thank ziqiang cao furu wei wenjie sujian faithful original fact aware neural tive summarization thirty second aaai ence articial intelligence sumit chopra michael auli alexander rush abstractive sentence summarization tentive recurrent neural networks proceedings conference north american ter association computational linguistics human language technologies pages jacob devlin ming wei chang kenton lee kristina toutanova bert pre training deep bidirectional transformers language standing proceedings conference north american chapter association computational linguistics human language nologies volume long short papers pages esin durmus mona diab feqa question answering evaluation framework fulness assessment abstractive summarization proceedings annual meeting sociation computational linguistics alexander fabbri irene tianwei suyi dragomir radev multi news large scale multi document summarization dataset tive hierarchical model proceedings annual meeting association tional linguistics pages florence italy association computational linguistics tobias falke leonardo ribeiro prasetya ajie utama ido dagan iryna gurevych ranking generated summaries correctness interesting challenging application natural language inference proceedings nual meeting association computational linguistics pages sebastian gehrmann yuntian deng alexander rush abstractive summarization proceedings conference cal methods natural language processing pages ben goodrich vinay rao peter liu mad saleh assessing factual accuracy generated text proceedings acm sigkdd international conference knowledge discovery data mining pages excludes cases model change person break uency spacy ner errors david graff junbo kong chen kazuaki maeda english gigaword linguistic data consortium philadelphia karl moritz hermann tomas kocisky edward grefenstette lasse espeholt kay mustafa leyman phil blunsom teaching advances chines read comprehend neural information processing systems pages matthew honnibal ines montani spacy natural language understanding bloom dings convolutional neural networks tal parsing appear philipp koehn rebecca knowles lenges neural machine translation ceedings workshop neural machine translation pages conference empirical methods ural language processing emnlp ramesh nallapati bowen zhou cicero dos santos caglar gulcehre bing xiang stractive text summarization sequence sequence rnns proceedings signll conference computational ral language learning pages shashi narayan shay cohen mirella lapata details summary topic aware convolutional neural networks proceedings treme summarization conference empirical methods natural guage processing pages wojciech kryscinski bryan mccann caiming xiong richard socher evaluating factual consistency abstractive text summarization arxiv preprint adam paszke sam gross soumith chintala gory chanan edward yang zachary devito ing lin alban desmaison luca antiga adam lerer automatic differentiation pytorch katherine lee orhan firat ashish agarwal clara fannjiang david sussillo hallucinations neural machine translation mike lewis yinhan liu naman goyal jan ghazvininejad abdelrahman mohamed omer levy veselin stoyanov luke zettlemoyer bart denoising sequence sequence training natural language generation translation comprehension proceedings nual meeting association computational linguistics haoran junnan zhu jiajun zhang chengqing zong ensure correctness mary incorporate entailment knowledge stractive sentence summarization proceedings international conference computational linguistics pages chin yew lin rouge package matic evaluation summaries text tion branches pages yang liu mirella lapata text proceedings tion pretrained encoders conference empirical methods ural language processing international joint conference natural language processing emnlp ijcnlp pages ilya loshchilov frank hutter pled weight decay regularization arxiv preprint joshua maynez shashi narayan bernd bohnet ryan mcdonald faithfulness ality abstractive summarization proceedings annual meeting association computational linguistics cao meng yue cheung dong jiapeng jackie chi kit factual error correction stractive summarization models proceedings romain paulus caiming xiong richard socher deep reinforced model abstractive marization international conference ing representations alec radford karthik narasimhan tim salimans improving language ilya sutskever standing generative pre training colin raffel noam shazeer adam roberts katherine lee sharan narang michael matena yanqi zhou wei peter liu exploring limits transfer learning unied text text journal machine learning research pranav rajpurkar robin jia percy liang know know unanswerable tions squad proceedings annual meeting association computational guistics volume short papers pages alexander rush sumit chopra jason weston neural attention model abstractive proceedings tence summarization conference empirical methods natural guage processing pages abigail peter liu christopher manning point summarization generator networks proceedings nual meeting association computational linguistics volume long papers pages darsh shah tal schuster regina barzilay automatic fact guided sentence modication proceedings thirty fourth aaai conference articial intelligence tian shi ping wang chandan reddy leafnats open source toolkit live demo system neural abstractive text summarization proceedings conference north american chapter association tational linguistics demonstrations pages minneapolis minnesota association tional linguistics kaiqiang song logan lebanoff qipeng guo xipeng qiu xiangyang xue chen dong fei liu joint parsing generation tive summarization proceedings fourth aaai conference articial intelligence pages adam trischler tong wang xingdi yuan justin ris alessandro sordoni philip bachman heer suleman newsqa machine proceedings hension dataset shop representation learning nlp pages ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan gomez ukasz kaiser illia polosukhin attention need advances neural information cessing systems pages oriol vinyals meire fortunato navdeep jaitly pointer networks advances neural formation processing systems pages oriol vinyals quoc neural tional model arxiv preprint alex wang kyunghyun cho mike lewis asking answering questions evaluate proceedings tual consistency summaries annual meeting association putational linguistics thomas wolf lysandre debut victor sanh julien chaumond clement delangue anthony moi ric cistac tim rault remi louf morgan icz jamie brew huggingface formers state art natural language ing arxiv jingqing zhang yao zhao mohammad saleh ter liu pegasus pre training tracted gap sentences abstractive summarization thirty seventh international conference chine learning icml tianyi zhang varsha kishore felix kilian weinberger yoav artzi bertscore evaluating text generation bert tional conference learning representations chenguang zhu william hinthorn ruochen qingkai zeng michael zeng xuedong huang meng jiang boosting factual correctness abstractive summarization knowledge graph arxiv preprint jerusalem ame remembrance burns jerusalem song memory haunts valerie braham year israel memorial day ration bereaved family members braham truly understand lost loved braham said husband philippe braham people killed january terror attacks paris kosher supermarket gunman stormed killing people jewish france memorial day commemoration bereaved family members braham valerie braham people killed january terror attacks paris israel memorial day commemoration bereaved family members braham philippe braham people killed january terror attacks paris describe relationship word matched america alienating closest allies iran deal iran picking new ones bolstering relations old ones growing dependent irans power rising iran alienating closest allies iran deal iran picking new ones america alienating closest allies iran deal iran picking new ones north pacic gray whale earned spot record books completing longest migration mammal recorded whale named varvara swam nearly miles kilometers according release oregon state university scientists helped conduct whale tracking study varvara russian barbara left primary feeding ground russias sakhalin island cross pacic ocean west coast united states baja mexico north pacic gray whale swam nearly miles oregon state university north pacic gray whale swam nearly miles russias sakhalin island sanaa yemen airstrikes yemen resumed days saudi arabia announced end air campaign airstrikes thursday targeted rebel houthi militant positions parts sanaa yemeni defense ministry ofcials said attacks lasted hours saudi led coalition said new initiative underway operation renewal hope focused political process hours later rebel forces attacked yemeni military brigade airstrikes resumed security sources taiz said attacks lasted hours days rebel forces attacked yemeni military troops attacks lasted hours hours rebel forces attacked yemeni military troops boston bomb went steve woolfenden thought standing lay ground holding handles son stroller pulled stroller cover saw son leo conscious bleeding left head woolfenden checked leo injuries thought let steve woolfenden conscious bleeding left head leo conscious bleeding left head cnndm source system summary corrected spanfact cnndm source system summary corrected spanfact cnndm source system summary corrected spanfact cnndm source system summary corrected spanfact cnndm source system summary corrected spanfact table examples factual error correction factcc dataset human annotated subset cnndm obtained kryscinski factual errors abstractive summarization system marked red corrections proposed spanfact models marked orange
