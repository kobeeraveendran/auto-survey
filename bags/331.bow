multi fact correction in abstractive text summarization yue shuohang zhe yu jackie chi kit jingjing mcgill university dynamics ai research yue mcgill ca shuowa zhe gan yu cheng jingjl com t c o l c s c v v i x r a abstract pre trained neural abstractive summarization systems have dominated extractive gies on news summarization performance at least in terms of rouge however generated abstractive summaries often face the pitfall of factual inconsistency generating correct facts with respect to the source text to address this challenge we propose fact a suite of two factual correction models that leverages knowledge learned from tion answering models to make corrections in system generated summaries via span tion our models employ single or masking strategies to either iteratively or regressively replace entities in order to ensure the source text semantic consistency w t while retaining the syntactic structure of maries generated by abstractive tion models experiments show that our els signicantly boost the factual consistency of system generated summaries without cing summary quality in terms of both matic metrics and human evaluation introduction informative text summarization aims to shorten a long piece of text while preserving its main message existing systems can be divided into two main types extractive and abstractive tractive strategies directly copy text snippets from the source to form summaries while abstractive strategies generate summaries containing novel sentences not found in the source despite the fact that extractive strategies are simpler and less pensive and can generate summaries that are more grammatically and semantically correct tive strategies are becoming increasingly popular thanks to its exibility coherency and vocabulary diversity zhang et al of this work was done when the rst author was an intern at microsoft cnn about a quarter of a million tralian homes and businesses have no power after a once in a decade storm battered ney and nearby areas about people have been isolated by ood waters as the roads are cut off and we wo nt be able to reach them for a few days a quarter of a million australian homes and businesses have no power after a decade about a quarter of a million australian homes and businesses have no power after a once in a decade storm all the victims including killed and injured have been identied as senior high school students of the second senior high school of ruzhou city central china s henan province local police said friday killed injured in central china school shooting killed injured in central china school shooting st clare s catholic primary school in ingham has met with equality leaders at the city council to discuss a complaint from the pupil s family the council is supporting the school to ensure its policies are appropriate a muslim school has been accused of ing the equality act by refusing to wear scarves a catholic school has been accused of ing the equality act by refusing to wear scarves cnndm source bottom up summary corrected by spanfact gigaword source generator summary corrected by spanfact xsum source bertabs summary corrected by spanfact table examples of factual error correction on ent summarization datasets factual errors are marked in red corrections made by the proposed spanfact models are marked in orange recently with the advent of transformer based models vaswani et al pre trained using self supervised objectives on large text corpora devlin et al radford et al lewis et al raffel et al abstractive marization models are surpassing extractive ones on automatic evaluation metrics such as rouge lin however several studies falke et al goodrich et al kryscinski et al wang et al durmus et al maynez et al observe that despite high rouge scores system generated abstractive summaries are often factually inconsistent with spect to the source text factual inconsistency is a well known problem for conditional text tion which requires models to generate readable text that is faithful to the input document quently sequence to sequence generation models need to learn to balance signals between the source for faithfulness and the learned language eling prior for uency kryscinski et al the dual objectives render abstractive tion models highly prone to hallucinating content that is factually inconsistent with the source ments maynez et al prior work has pushed the frontier of teeing factual consistency in abstractive rization systems most focus on proposing tion metrics that are specic to factual consistency as multiple human evaluations have shown that rouge or bertscore zhang et al relates poorly with faithfulness kryscinski et al maynez et al these evaluation models range from using fact triples goodrich et al textual entailment predictions falke et al adversarially pre trained classiers kryscinski et al to question answering qa systems wang et al durmus et al it is worth noting that qa based evaluation metrics show surprisingly high correlations with human judgment on factuality wang et al indicating that qa models are robust in capturing facts that can benet summarization tasks on the other hand some work focuses on model design to incorporate factual triples cao et al zhu et al or textual entailment li et al falke et al to boost factual consistency in generated summaries such models are efcient in boosting factual scores but often at the expense of signicantly lowering rouge scores of the generated summaries this happens because the models struggle between generating pivotal content while retaining true facts often with an eventual propensity to sacricing mativeness for the sake of correctness of the mary in addition these models inherit the bone of generative models that suffer from cination despite the regularization from complex knowledge graphs or text entailment signals in this work we propose spanfact a suite of two neural based factual correctors that improve summary factual correctness without sacricing informativeness to ensure the retention of tic meaning in the original documents while ing the syntactic structures generated by advanced summarization models we focus on factual edits on entities only a major source of hallucinated rors in abstractive summarization systems in tice kryscinski et al maynez et al the proposed model is inspired by the observation that fact checking qa model is a reliable medium in assessing whether an entity should be included in a summary as a fact wang et al mus et al to our knowledge we are the rst to adapt qa knowledge to enhance tive summarization compared to sequential eration models that incorporate complex edge graph and nli mechanisms to boost ity our approach is lightweight and can be ily applied to any system generated summaries without retraining the model empirical results on multiple summarization datasets show that the proposed approach signicantly improves rization quality over multiple factuality measures without sacricing rouge scores our contributions are summarized as follows i we propose spanfact a new factual correction framework that focuses on correcting erroneous facts in generated summaries generalizable to any summarization system we propose two ods to solve multi fact correction problem with single or multi span selection in an iterative or auto regressive manner respectively imental results on multiple summarization marks demonstrate that our approach can cantly improve multiple factuality measurements without a huge drop on rouge scores related work the general neural based encoder decoder ture for abstractive summarization is rst posed by rush et al later work proves this structure with better encoders such as lstms chopra et al and grus pati et al that are able to capture range dependencies as well as with reinforcement learning methods that directly optimize rization evaluation scores paulus et al one drawback of the earlier neural based rization models is the inability to produce out figure training example created for the qa span prediction model upper right and the auto regressive fact correction model bottom right vocabulary words as the model can only ate whole words based on a xed vocabulary see et al proposes a pointer generator work that can copy words directly from the source through a pointer network vinyals et al in addition to the traditional sequence to sequence generation model abstractive summarization starts to shine with the advent of self supervised algorithms which low deeper and more complicated neural networks such as transformers vaswani et al to learn diverse language priors from large scale pora models such as bert devlin et al gpt radford et al and bart lewis et al have achieved new state of the art performances on abstractive summarization liu and lapata lewis et al zhang et al shi et al fabbri et al these models often netune pre trained transformers with supervised summarization datasets that tain pairs of source and summary inconsistency however encoder decoder architectures widely used in abstractive summarization systems are inherently difcult to control and prone to lucination vinyals and le koehn and knowles lee et al and often leads to factual the system generated summary is uent but unfaithful to the source cao et al studies have shown that to system generated abstractive summaries have factual errors falke et al kryscinski et al that can not be discovered by rouge scores recent studies have proposed new ods to ensure factual consistency in tion cao et al zhu et al pose rnn based and transformer based decoders that attend to both source and extracted edge triples respectively li et al pose an entailment reward augmented likelihood training objective and falke et al proposes to rerank beam results based on entailment scores to the source our fact correction models are inherently ferent from these models as we focus on correcting summaries generated by any model our models are trained with the objective of dicting masked entities identied for fact tion figure and learn to ll in the entity masks of any system generated summaries with single or multi span selection mechanism figure the most similar work to ours is proposed concurrently by meng et al where they ne tune a bart lewis et al model on distant vision examples and use it as a post editing model for factual error correction multi fact correction models in this section we describe two models proposed for factual error correction qa span fact rection model and auto regressive fact rection model as both methods rely on span selection with different masking and prediction strategies we call them spanfact collectively problem formulation let y be a document summary pair where xm is the source sequence with m tokens and y yn is the target sequence with n tokens an abstractive marization model aims to model the conditional figure model architecture left qa span fact correction model right auto regressive fact correction model likelihood which can be factorized into a product where denote the preceding tokens before tion t the conditional maximum likelihood jective ideally requires summarization models to not only optimize for informativeness but also rectness however in reality this often fails as the models have a high propensity for leaning towards informativeness than correctness li et al suppose a summarization system generates a sequence of tokens n to form a summary our factual correction models aim to edit an informative yet incorrect summary into k such that where f is a metric measuring factual consistency between the source and system summary span selection dataset our fact correction models are inspired by the span selection task which is often used in ing comprehension tasks such as question ing figure shows examples of the span lection datasets we created for training our span and auto regressive fact correction models respectively the query is a reference summary masked with one or all and the passage is the corresponding source document to be marized if an entity appears multiple times in the source document we rank them based on the fuzzy string matching scores a variation of enshtein distance between the query sentence and this work we use spacy ner tagger honnibal and montani to identify entities for data construction the source sentence containing the entity our models explicitly learn to predict the span of the masked entity rather than pointing to a specic ken as in pointer network vinyals et al because the original tokens and replaced tokens often have different lengths our qa span fact correction model iteratively mask and replace one entity at a time while the auto regressive model masks all the entities taneously and replace them in an auto regressive fashion from left to right figure shows an overview of our models comparing the two els the qa span fact correction model works ter when only a few errors exist in the draft mary as the prediction of each mask is relatively independent of each other on the other hand the auto regressive fact correction model starts with a skeleton summary that has all the entities masked which is often more robust when summaries tain many factual errors qa span fact correction model in the iterative setting our model aims to conduct entity correction by answering a query that tains only one mask at a time suppose a system summary has t entities at time step i we mask the i th entity and use this masked sequence as the query to our qa span model the prediction is placed into the masked slot in the query to ate an updated system summary to be used in the next step given the source text and a masked query q m our iterative rection model aims to predict the answer span via modeling start and end for span selection we use the model which adds two separate non linear ers on top of transformers as pointers to the start and end token position for the answer we ize the fact correction model from a pre trained bert model devlin et al and perform netuning with the span selection datasets we ated from the summarization datasets figure the input to the bert model is a tion of two segments the masked query q and the source separated by special delimiter markers as q sep each token in the quence is assigned with three embeddings token embedding position embedding and tion embedding these embeddings are summed into a single vector and fed to the multi layer transformer model l h hl l l where are the input vectors and l represents the depth of stacked layers ln and mhatt are layer normalization and multi head attention operations vaswani et al the top layer provides the hidden states for the input tokens with rich textual information the start s and end e of the answer span are predicted as astart i aend i e i j j qs i s hi bs e hi be where h is the number of encoder s hidden states ws we rd and bs be r are trainable rameters the nal span is selected based on the argmax of eqn and with the constraint of pstart pend and pend pstart auto regressive fact correction model one disadvantage of the qa style span prediction strategy is that if the sequence contains too many factual errors masking out one entity at a time may lead to highly erroneous skeleton summary com huggingface transformers segmentation embedding is used to distinguish the query with two special tokens cls and sep and the source in our models to start with the model might be making tions on top of wrong entities from later in the quence masking one entity at a time is essentially a greedy local method that is prone to error mulation to alleviate this issue we propose a new sequential fact correction model to handle errors in a more global manner with beam search cally we mask out all the entities simultaneously and use a novel auto regressive span selection coder to predict llers for the multiple masks quentially by doing this we assume dependency between the masks the earlier predicted entities will be used as corrected context for better tions in the later steps given a source text xn and a draft summary m our model rst masks out all the entities with t masks and leaves a skeleton summary as the query q m then we concatenate the query q with the source ilar to section as inputs to the encoder the inputs are fed into bert to obtain contextual den representations maskt we then select the encoder s hidden states for the t masks as partial input to an auto regressive transformer based decoder unlike generation tasks that require an eos ken to indicate the end of decoding our decoder runs t steps to predict the answer spans for these t masks at step t we rst fuse the hidden sentation rd of the t th mask ken and previously predicted entity representation rd sent zt sent where w sent the sentation of cls token and denotes vector concatenation the input zt is then fed to the transformer coder as in eqn and to generate the coder s hidden state t at time step t based on t we use a two pointer network to predict the start and end positions of the answer entity in the source encoder s hidden states this is achieved with cross attention of t w t the encoder s den states similar to eqn and this tion results in two distributions over the encoder s hidden states for the start and end span positions the nal prediction of the start and end positions for mask t is obtained by taking the over argmax is used for selecting the start and end indexes the pointer position distributions pstart arg pend arg astart aend m m under the constraint that pstart pend and pend pstart based on the start and end positions for the dicted entity we can obtain the predicted entity representation at time step t as the mean over the in span encoder s hidden states sent t mean hpend which is used as the input for the next step of it is worth noting that although the decoding argmax operations in eqn and are differentiable the model is trained based on the start and end positions of the ground truth answer w t the start and end logits in eqn and which makes the gradient back propagates to the encoder meanwhile the encoder s hidden states used to compose sent in eqn also carry the gradients during inference beam search is used to nd the best sequence of predicted spans in the source to replace the masks i compared to the conventional pointer network vinyals et al see et al that only points to one token at a time our sequential span selection decoder has the exibility to replace a mask by any number of entity tokens which is ten required in summary factual correction experiment in this section we present our results on using spanfact for multiple summarization datasets experimental setup training data for our fact correction models are generated as described in section on cnn dailymail hermann et al xsum narayan et al and gigaword graff et al rush et al the statistics of these three dataset are provided in table during ing if an entity does not have a corresponding span in the source we point the answer span to the cls token during inference if the swer span predicted is the cls token we place back the original masked entity for the answer span and the softmax is used for computing the loss for back propagation datasets docs train val test doc len summ len mask cnn dailymail xsum gigaword table comparison of summarization datasets on train validation test set splits average document and summary length numbers of words we also report the average number of entity masks on the reference summary for each dataset our fact correction models are implemented via the huggingface transformers library wolf et al in pytorch paszke et al we initialize all encoder models with the point of an uncased large bert model trained on english data and squad for all periments both source and target texts were kenized with bert s sub words tokenizer the max sequence length is set to for the encoder we use a shallow transformer decoder for the auto regressive span selection decoder as the pre trained bert large encoder is already robust for selecting right spans in the single span tion task with only two pointers section the transformer decoder has hidden units and the feed forward intermediate size for all layers is all models were netuned on our span tion data for epochs with batch size adamw optimizer loshchilov and hutter with and an initial learning rate is used for training our learning rate schedule follows a linear decay scheduler with ing inference we use beam search with and k constraint for the distance between the start and end pointer the best model checkpoints are chosen based on performance on the validation set experiments are conducted using quadro rtx gpus with gb of memory evaluation metrics we use three automatic evaluation metrics to uate our models the rst is rouge lin the standard summarization quality metric which has high correlation with summary ness in the news domain kryscinski et al since rouge has been criticized for its poor correlation with factual consistency kryscinski et al wang et al we use two ditional automatic metrics that specically focus on factual consistency factcc kryscinski et al qgqa factcc rouge qgqa factcc rouge datasets bottom up split encoders qa span auto regressive split encoders qa span auto regressive split encoders qa span auto regressive transformerabs split encoders qa span auto regressive sent l table factual correctness scores and rouge scores on cnn dailymail test set and qags wang et al factcc is a pre trained binary classier that evaluates the tuality of a system generated summary by ing whether it is consistent or inconsistent w t the source this classier was trained on sarial examples obtained by heuristically injecting noise into reference summaries in addition very recent work proposed based models for factuality evaluation wang et al durmus et al maynez et al and wang et al showed that their evaluation models have higher correlation with human judgements on factuality when compared with factcc kryscinski et al we thus include our re implementation of a question eration and question answering model qgqa following wang et al as an evaluation metric for factuality this model generates a set of questions based on the system generated summary and then answers these questions ing either the source or the summary to obtain two sets of answers the answers are compared against each other using an answer similarity ric token level and the averaged similarity metric over all questions is used as the qgqa were not able to obtain any of the qa evaluation model or code from wang et al durmus et al maynez et al as the authors are still in the stage of making the code public we used pre trained unilm model for question generation qg and ing model for question answering qa the qg model is ne tuned on newsqa trischler et al with answer conditional task wang et al and the qa model is pre trained on squad rajpurkar et al datasets split encoders qa span auto regressive split encoders qa span auto regressive transformerabs split encoders qa span auto regressive sent l table factual correctness scores and rouge scores on xsum test set score answers generated from a highly faithful system summary should be similar to those ated from the source baselines the following abstractive we compare against summarization baselines on cnndm and xsum we use bertsumabs and transformerabs liu and lapata in tion we also compare with bottom up gehrmann et al on gigaword we use the generator see et al base and full parse models song et al for comparison for the factual correction baseline we compare with the two encoder pointer split encoder shah et al which employs a similar setting to ours for masking entities w t the source and uses dual encoders to copy and generate from both the source and the masked query for fact update compared to our span tion models that can ll in the mask with any ber of tokens their models aim to regenerate the mask query based on the source in other words their decoder regenerates the whole sequence ken by token with a pointer generator which herits the backbone of generative models that fer from hallucination experimental results tables and summarize the results on the cnn dailymail xsum and gigaword datasets respectively each block in the tables compares the original summarization model s output with com split qgqa factcc rouge datasets genparse base split encoders qa span auto regressive genparse full split encoders qa span auto regressive pointer generator split encoders qa span auto regressive sent l bertabs better worse same qa span vs original auto regressive vs original qa span vs auto regressive transformerabs better worse same qa span vs original auto regressive vs original qa span vs auto regressive bottom up better worse same qa span vs original auto regressive vs original qa span vs auto regressive table factual correctness scores and rouge scores on gigaword test set table human evaluation results on pairwise ison of factual correctness on randomly sampled articles the corrected outputs obtained by our baseline and proposed models on cnn dailymail table our correction models signicantly boost factual consistency measures qgqa and factcc by large margins with only small drops on rouge this shows our models have the ability to improve the correctness of system generated summaries without ing informativeness when comparing our two proposed models we observe that the qa span model performs better than the auto regressive model this is expected as cnn dailymail erence summaries tend to be more extractive see et al and summarization models tend to make few errors per summary narayan et al thus the iterative procedure of the span model is more robust with high precision as it has more correct context from the query with only minimum negative inuence from other current errors this is also reected in the high scores of qgqa and factcc across all the models we tested since qgqa and factcc are based on the comparing system generated summary w t source text high score means high semantic larity between system summary to the source on xsum table and gigaword table both of our correction models boost factual sistency measures by large margins with a slight drop in rouge to on average this is still encouraging as abstractive summarization models that use complex factual controlling ponents for generation often have drops of rouge points zhu et al we also notice that the qgqa and factcc scores of all summarization models are lower than that on cnn dailymail the scores are especially low on xsum this is likely due to the data struction protocol of xsum where the rst tence of a source document is used as the mary and the remainder of the article is used as the source as a result many entities that appear in the reference summary never appear in the source which may cause abstractive summarization els to hallucinate severely with many factual errors maynez et al as the system summaries often contain many errors our qa span model that relies on answering a single mask query often has the wrong context to condition on at each step which negatively affects the performance of this model in contrast the strategy of masking all the entities would provide the auto regressive model a better query for entity replacement we can serve in table that the auto regressive model forms better than the qa span model on xsum human evaluation to provide qualitative analysis of the proposed models we conduct human evaluation on wise comparison of cnn dailymail summaries enhanced by different correction strategies we select three state of the art abstractive tion models as the backbones and collect three sets of pairwise summaries for each setting i original vs qa span corrected original vs auto regressive corrected qa span rected vs auto regressive corrected nine sets of randomly selected samples total ples are labeled by amt tuckers for each pair in anonymized order three annotators from amazon mechanical turk amt are asked to judge which is more factually correct based on the factcc dataset factcc score qaqg human eval before corr qa span auto regressive the reviewers for their valuable comments and cial thanks to yuwei fang and other members of the microsoft dynamics ai research team for the feedback and suggestions table test results on the human annotated dataset provided by factcc kryscinski et al we show the performance comparisons of the original summaries and the summaries corrected by spanfact references source document as shown in table summaries from our two models are chosen more frequently as the factually correct one compared to the nal between the two correction models the erences are comparable in addition we also test our fact tion models on the factcc test set provided by kryscinski et al and manually checked the outputs table shows the results of the inal summaries and the summaries corrected by our models in terms of automatic fact evaluation and our manual evaluation among generated summary sentences were incorrect the qa span model was able to correct out of right and the auto regressive model was able to correct out of among the sentences that are labeled as correct by the annotators in kryscinski et al our two models made and wrong changes in the entities while keeping most of the entities unchanged or changed with equivalent entities conclusion we present spanfact a suite of two factual rection models that use span selection mechanisms to replace one or multiple entity masks at a time spanfact can be used for fact correction on any stractive summaries empirical results show that our models improve the factuality of summaries generated by state of the art abstractive rization systems without a huge drop on rouge scores for future work we plan to apply our method for other type of spans such as noun phrases verbs and clauses acknowledgments this research was supported in part by microsoft dynamics ai research and the canada far ai chair program we would like to thank ziqiang cao furu wei wenjie li and sujian li faithful to the original fact aware neural tive summarization in thirty second aaai ence on articial intelligence sumit chopra michael auli and alexander m rush abstractive sentence summarization with tentive recurrent neural networks in proceedings of the conference of the north american ter of the association for computational linguistics human language technologies pages jacob devlin ming wei chang kenton lee and kristina toutanova bert pre training of deep bidirectional transformers for language standing in proceedings of the conference of the north american chapter of the association for computational linguistics human language nologies volume long and short papers pages esin durmus he he and mona diab feqa a question answering evaluation framework for fulness assessment in abstractive summarization in proceedings of the annual meeting of the sociation for computational linguistics alexander fabbri irene li tianwei she suyi li and dragomir radev multi news a large scale multi document summarization dataset and tive hierarchical model in proceedings of the annual meeting of the association for tional linguistics pages florence italy association for computational linguistics tobias falke leonardo fr ribeiro prasetya ajie utama ido dagan and iryna gurevych ranking generated summaries by correctness an interesting but challenging application for natural language inference in proceedings of the nual meeting of the association for computational linguistics pages sebastian gehrmann yuntian deng and alexander m rush bottom up abstractive summarization in proceedings of the conference on cal methods in natural language processing pages ben goodrich vinay rao peter j liu and mad saleh assessing the factual accuracy of generated text in proceedings of the acm sigkdd international conference on knowledge discovery data mining pages excludes the cases where the model would change a person s full name by last name or break the uency due to spacy ner errors david graff junbo kong ke chen and kazuaki maeda english gigaword linguistic data consortium philadelphia karl moritz hermann tomas kocisky edward grefenstette lasse espeholt will kay mustafa leyman and phil blunsom teaching in advances in chines to read and comprehend neural information processing systems pages matthew honnibal and ines montani spacy natural language understanding with bloom dings convolutional neural networks and tal parsing to appear philipp koehn and rebecca knowles six in lenges for neural machine translation ceedings of the first workshop on neural machine translation pages the conference on empirical methods in ural language processing emnlp ramesh nallapati bowen zhou cicero dos santos caglar gulcehre and bing xiang stractive text summarization using sequence sequence rnns and beyond in proceedings of the signll conference on computational ral language learning pages shashi narayan shay b cohen and mirella lapata do nt give me the details just the summary topic aware convolutional neural networks for in proceedings of the treme summarization conference on empirical methods in natural guage processing pages wojciech kryscinski bryan mccann caiming xiong and richard socher evaluating the factual consistency of abstractive text summarization arxiv preprint adam paszke sam gross soumith chintala gory chanan edward yang zachary devito ing lin alban desmaison luca antiga and adam lerer automatic differentiation in pytorch katherine lee orhan firat ashish agarwal clara fannjiang and david sussillo hallucinations in neural machine translation mike lewis yinhan liu naman goyal jan ghazvininejad abdelrahman mohamed omer levy veselin stoyanov and luke zettlemoyer bart denoising sequence to sequence training for natural language generation translation and comprehension in proceedings of the nual meeting of the association for computational linguistics haoran li junnan zhu jiajun zhang and chengqing zong ensure the correctness of the mary incorporate entailment knowledge into stractive sentence summarization in proceedings of the international conference on computational linguistics pages chin yew lin rouge a package for matic evaluation of summaries in text tion branches out pages yang liu and mirella lapata text in proceedings of tion with pretrained encoders the conference on empirical methods in ural language processing and the international joint conference on natural language processing emnlp ijcnlp pages ilya loshchilov and frank hutter pled weight decay regularization arxiv preprint joshua maynez shashi narayan bernd bohnet and ryan mcdonald on faithfulness and ality in abstractive summarization in proceedings of the annual meeting of the association for computational linguistics cao meng yue cheung dong jiapeng wu and jackie chi kit factual error correction for stractive summarization models in proceedings of romain paulus caiming xiong and richard socher a deep reinforced model for abstractive marization in international conference on ing representations alec radford karthik narasimhan tim salimans and improving language ilya sutskever standing by generative pre training colin raffel noam shazeer adam roberts katherine lee sharan narang michael matena yanqi zhou wei li and peter j liu exploring the limits of transfer learning with a unied text to text journal of machine learning research former pranav rajpurkar robin jia and percy liang know what you do nt know unanswerable tions for squad in proceedings of the annual meeting of the association for computational guistics volume short papers pages alexander m rush sumit chopra and jason weston a neural attention model for abstractive in proceedings of the tence summarization conference on empirical methods in natural guage processing pages abigail see peter j liu and christopher d manning get to the point summarization with generator networks in proceedings of the nual meeting of the association for computational linguistics volume long papers pages darsh j shah tal schuster and regina barzilay in automatic fact guided sentence modication proceedings of the thirty fourth aaai conference on articial intelligence tian shi ping wang and chandan k reddy leafnats an open source toolkit and live demo system for neural abstractive text summarization in proceedings of the conference of the north american chapter of the association for tational linguistics demonstrations pages minneapolis minnesota association for tional linguistics kaiqiang song logan lebanoff qipeng guo xipeng qiu xiangyang xue chen li dong yu and fei liu joint parsing and generation for tive summarization in proceedings of the fourth aaai conference on articial intelligence pages adam trischler tong wang xingdi yuan justin ris alessandro sordoni philip bachman and heer suleman newsqa a machine in proceedings of the hension dataset shop on representation learning for nlp pages ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan n gomez ukasz kaiser and illia polosukhin attention is all you need in advances in neural information cessing systems pages oriol vinyals meire fortunato and navdeep jaitly pointer networks in advances in neural formation processing systems pages oriol vinyals and quoc le a neural tional model arxiv preprint alex wang kyunghyun cho and mike lewis asking and answering questions to evaluate the in proceedings of tual consistency of summaries the annual meeting of the association for putational linguistics thomas wolf lysandre debut victor sanh julien chaumond clement delangue anthony moi ric cistac tim rault remi louf morgan icz and jamie brew huggingface s formers state of the art natural language ing arxiv jingqing zhang yao zhao mohammad saleh and ter j liu pegasus pre training with tracted gap sentences for abstractive summarization in thirty seventh international conference on chine learning icml tianyi zhang varsha kishore felix wu kilian q weinberger and yoav artzi bertscore in evaluating text generation with bert tional conference on learning representations chenguang zhu william hinthorn ruochen xu qingkai zeng michael zeng xuedong huang and meng jiang boosting factual correctness of abstractive summarization with knowledge graph arxiv preprint jerusalem ame of remembrance burns in jerusalem and a song of memory haunts valerie braham as it never has before this year israel s memorial day ration is for bereaved family members such as braham now i truly understand everyone who has lost a loved one braham said her husband philippe braham was one of people killed in january s terror attacks in paris he was in a kosher supermarket when a gunman stormed in killing four people all of them jewish france s memorial day commemoration is for bereaved family members as braham valerie braham was one of people killed in january s terror attacks in paris israel s memorial day commemoration is for bereaved family members as braham philippe braham was one of people killed in january s terror attacks in paris i had to describe the u s relationship in one word it would be matched america is alienating some of our closest allies because of the iran deal and iran is picking up new ones and bolstering relations with old ones who are growing more dependent because they see irans power rising iran is alienating some of our closest allies because of the iran deal and iran is picking up new ones america is alienating some of our closest allies because of the iran deal and iran is picking up new ones north pacic gray whale has earned a spot in the record books after completing the longest migration of a mammal ever recorded the whale named varvara swam nearly miles kilometers according to a release from oregon state university whose scientists helped conduct the whale tracking study varvara which is russian for barbara left her primary feeding ground off russias sakhalin island to cross the pacic ocean and down the west coast of the united states to baja mexico a north pacic gray whale swam nearly miles from oregon state university a north pacic gray whale swam nearly miles from russias sakhalin island sanaa yemen airstrikes over yemen have resumed once again two days after saudi arabia announced the end of its air campaign the airstrikes thursday targeted rebel houthi militant positions in three parts of sanaa two yemeni defense ministry ofcials said the attacks lasted four hours the saudi led coalition said a new initiative was underway operation renewal of hope focused on the political process but less than hours later after rebel forces attacked a yemeni military brigade the airstrikes resumed security sources in taiz said the attacks lasted four hours two days after rebel forces attacked yemeni military troops the attacks lasted four hours less than hours after rebel forces attacked yemeni military troops boston the bomb went off steve woolfenden thought he was still standing that was because as he lay on the ground he was still holding the handles of his son s stroller he pulled back the stroller s cover and saw that his son leo was conscious but bleeding from the left side of his head woolfenden checked leo for other injuries and thought let s get out of here steve woolfenden was conscious but bleeding from the left side of his head leo was conscious but bleeding from the left side of his head cnndm source system summary corrected by spanfact cnndm source system summary corrected by spanfact cnndm source system summary corrected by spanfact cnndm source system summary corrected by spanfact cnndm source system summary corrected by spanfact table examples of factual error correction on factcc dataset a human annotated subset from cnndm obtained by kryscinski et al factual errors by abstractive summarization system are marked in red corrections made by the proposed spanfact models are marked in orange
