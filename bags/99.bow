text summarization using abstract meaning representation shibhansh dohare cse department iit kanpur iitk ac in harish karnick cse department iit kanpur iitk ac in vivek gupta microsoft research bangalore t com l u j l c s c v v i x r a abstract with an ever increasing size of text present on the internet automatic summary eration remains an important problem for in this natural language understanding work we explore a novel pipeline for text summarization with an termediate step of abstract meaning resentation amr the pipeline proposed by us rst generates an amr graph of an input story through which it extracts a summary graph and nally generate mary sentences from this summary graph our proposed method achieves state the art results compared to the other text summarization routines based on amr we also point out some signicant lems in the existing evaluation methods which make them unsuitable for ing summary quality introduction summarization of large texts is still an open lem in language processing people nowadays have lesser time and patience to go through large pieces of text which make automatic tion important automatic summarization has nicant applications in summarizing large texts like stories journal papers news articles and even larger texts like books existing methods for summarization can be broadly categorized into two categories tive and abstractive extractive methods picks up words and sometimes directly sentences from the text these methods are inherently limited in the sense that they can never generate human level summaries for large and complicated documents which require rephrasing sentences and rating information from full text to generate maries most of the work done on summarization in past has been extractive on the other hand most abstractive methods take advantages of the recent developments in deep learning specically the recent success of the sequence to sequence learning models where recurrent networks read the text encodes it and then generate target text though these methods have recently shown to be competitive with the tractive methods they are still far away from ing human level quality in summary generation the work on summarization using amr was started by liu et al abstract meaning representation amr was as introduced by narescu et al amr focuses on capturing the meaning of the text by giving a specic ing representation to the text amr tries to ture the who is doing what to whom in a tence the formalism aims to give same tation to sentences which have the same ing meaning for example he likes apple and apples are liked by him should be assigned the same amr liu et al s approach aimed to produce a summary for a story by extracting a summary subgraph from the story graph and nally ate a summary from this extracted graph but cause of the unavailability of amr to text tor at that time their work was limited till ing the summary graph this method extracts a single summary graph from the story graph tracting a single summary graph assumes that all of the important information from the graph can be extracted from a single subgraph but it can be difcult in cases where the information is spread out in the graph thus the method compromises between size of the summary sub graph and the amount of information it can extract this can be easily solved if instead of a single sub graph we extract multiple subgraphs each focusing on background amr parsing and generation amr was introduced by banarescu et al with the aim to induce work on statistical natural language understanding and generation amr represents meaning using graphs amr graphs are rooted directed edge and vertex labeled graphs figure shows the graphical representation of the amr graph of the sentence i looked carefully all around me generated by jamr parser gan et al the graphical representation was produced using amrica saphra and lopez the nodes in the amr are labeled with concepts as in figure around represents a cept edges contains the information regarding the relations between the concepts in figure tion is the relation between the concepts and around amr relies on propbank for tic relations edge labels concepts can also be of the form where the index represents the rst sense of the word run further details about the amr can be found in the amr guidelines narescu et al a lot of work has been done on parsing tences to their amr graphs there are three main approaches to parsing there is alignment based parsing flanigan et al parser zhou et al which uses graph based algorithms for concept and relation tication second grammar based parsers like wang et al camr generate output by performing shift reduce transformations on output of a dependency parser neural parsing konstas et al peng et al is based on using models for parsing the main problem for neural methods is the absence of a huge corpus of human generated amrs peng et al duced the vocabulary size to tackle this problem while konstas et al used larger external corpus of external sentences recently some work has been done on ducing meaningful sentences form amr graphs flanigan et al used a number of tree to string conversion rules for generating sentences song et al reformed the problem as a eling salesman problem konstas et al used learning methods datasets we used two datasets for the task amr bank knight et al and cnn dailymail figure the graphical representation of the amr graph of the sentence i looked carefully all around me using amrica mation in a different part of the story we propose a two step process for extracting multiple summary graphs first step is to select few sentences from the story we use the idea that there are only few sentences that are tant from the point of view of summary i e most of the information contained in the summary is present in very few sentences and they can be used to generate the summary second step is to extract important information from the selected sentences by extracting a sub graph from the selected tences our main contributions in this work are three folds we propose a pipeline for text summarization providing strong baseline for future work on summarization using amr present a novel approach for extracting ple summary graphs that outperforms the vious methods based on a single sub graph extraction expose some problems with existing uation methods and datasets for abstractive summarization the rest of the paper is organized as follows section contains introduction to amr section and contains the datasets and the algorithm used for summary generation respectively tion has a detailed step by step evaluation of the pipeline and in section we discuss the problems with the current dataset and evaluation metric mann al nallapati et al we use the proxy report section of the amr bank as it is the only one that is relevant for the task cause it contains the gold standard human ated amr graphs for news articles and the maries in the training set the stories and maries contain sentences and sentences on an average respectively the training and test sets contain and summary document pairs respectively cnn dailymail corpus is better suited for marization as the average summary size is around or sentences this dataset has around document summary pairs with stories having sentences on average the dataset comes in sions one is the anonymized version which has been preprocessed to replace named entities e the times of india with a unique identier for ample second is the non anonymized which has the original text we use the anonymized version of the dataset as it is more suitable for amr parsing as most of the parsers have been trained on non anonymized text the dataset does not have gold standard amr graphs we use automatic parsers to get the amr graphs but they are not gold standard and will effect the quality of nal summary to get an idea of the error introduced by using automatic parsers we compare the results after using gold standard and automatically generated amr graphs on the standard dataset pipeline for summary generation the pipeline consists of three steps rst convert all the given story sentences to there amr graphs followed by extracting summary graphs from the story sentence graphs and nally generating tences from these extracted summary graphs in the following subsections we explain each of the methods in greater detail step story to amr as the rst step we convert the story sentences to their abstract meaning representations we use jamr parser version flanigan et al as its openly available and has a performance close to the state of the art parsers for parsing the dailymail corpus for the amr bank we have the gold standard amr parses but we still parse the input stories with jamr parser to study the effect of using graphs produced by jamr parser instead figure graph of the best recall scores for summaries around sentences in the cnn dailymail corpus y axis is the rogue score and x axis is the cumulative percentage of sentence with the corresponding score of the gold standard amr graphs step story amr to summary amr after parsing step we have the amr graphs for the story sentences in this step we extract the amr graphs of the summary sentences using story sentence amrs we divide this task in two parts first is nding the important sentences from the story and then extracting the key information from those sentences using their amr graphs selecting important sentences our algorithm is based on the idea that only few sentences are important from the point of view of summary i e there are only a few sentences which contain most of the important information and from these sentences we can generate the mary hypothesis most of the information sponding to a summary sentence can be found in only one sentence from the story to test this hypothesis for each summary tence we nd the sentence from the story that tains maximum information of this summary tence we use lin recall scores measures the ratio of number of words in the get summary that are contained in the predicted summary to the total number of words in the target summary as the metric for the information tained in the story sentence we consider the story sentence as the predicted summary and the mary sentence as the target summary the results that we obtained for randomly chosen ment summary pairs from the cnn dailymail pus are given in gure the average recall score that we obtained is the score will be fectly when the summary sentence is directly picked up from a story sentence upon manual inspection of the summary sentence and the sponding best sentence from the story we realized when this score is more than or almost ways the information in the summary sentence is contained in this chosen story sentence the score for in these cases is not perfectly because of stop words and different verb forms used in story and summary sentence around of summary tences have score above so our hypothesis seems to be correct for most of the summary tences this also suggests the highly extractive ture of the summary in the corpus now the task in hand is to select few tant sentences methods that use sentence tion for summary generation can be used for the task it is very common in summarization tasks specically in news articles that a lot of mation is contained in the initial few sentences choosing initial few sentences as the summary produces very strong baselines which the state the art methods beat only marginally ex on the cnn dailymail corpus the state of the art tive method beats initial sentences only by as reported by nallapati et al using this idea of picking important sentences from the beginning we propose two methods rst is to simply pick initial few sentences we call this rst n method where n stands for the ber of sentences we pick initial sentences for the cnn dailymail corpus i e and only the rst sentence for the proxy report section amr bank i e as they produce the best scores on the rogue metric compared to any other rst n second we try to capture the relation between the two most important entities we dene importance by the number of occurrences of the entity in the story of the document for this we simply nd the rst sentence which contains both these entities we call this the rst co occurrence based sentence selection we also select the rst sentence along with rst co occurrence based sentence selection note that methods rst n and rst co are by default followed by the summary graph extraction step and they are not just sentence selection methods as the important sentences we call this the rst co based sentence selection extracting summary graph as the datasets under consideration are news ticles the most important information in them is about an entity and a verb associated with it so to extract important information from the tence we try to nd the entity being talked about in the sentence we consider the most referred tity one that occurs most frequently in the text now for the main verb associated with the entity in the sentence we nd the verb closest to this entity in the amr graph we dene the closest verb as the one which lies rst in the path from the entity to the root we start by nding the position of the most ferred entity in the graph then we nd the closest verb to the entity and nally select the subtree hanging from that verb as the summary amr step summary generation to generate sentences from the extracted amr graphs we can use already available generators we use neural amr konstas et al as it provides state of the art results in sentence ation we also use flanigan et al generator in one of the experiments in the next section generators signicantly effect the results we will analyze the effectiveness of generator in the next section results and analysis baselines for the cnn dailymail dataset in this section we present the baseline models and analysis method used for each step of our pipeline the model is considered a strong baseline both the stractive paulus et al and extractive lapati et al state of the art methods on this dataset beat this baseline only marginally the model simply produces the leading three sentences of the document as its summary the key step in our pipeline is i e mary graph extraction directly comparing the baseline with amr based pipeline to evaluate the effectiveness of is an unfair comparison because of the errors introduced by imperfect parser and tor in the amr pipeline thus to evaluate the fectiveness of against baseline we table comparison with previous methods and baselines this table reports rogue scores on the proxy report section using alignment based generator recall precision method liu et al amr st co occurrence rst table table for analyzing the effect of using jamr parser in this table has rogue scores after using neural amr for sentence generation i e first half contains scores by using standard amr graphs second half has amr graphs generated by jamr parser method recall precision rogue l amr rst co occurrence rst using gold standard amr in using jamr parser for amr rst co occurrence rst need to nullify the effect of errors introduce by amr parser and generator we achieve this by trying to introduce similar errors in the leading thre sentences of each document we generate the amr graphs of the leading three sentences and then generate the sentences using these amr graph we use parser and generator that were used in our pipeline we consider these generated tences as the new baseline summary we shall now refer to it amr baseline in the remaining of the paper for the proxy report section of the amr bank we consider the amr model as the line for this dataset we already have the standard amr graphs of the sentences therefore we only need to nullify the error introduced by the generator procedure to analyze and evaluate each step for the evaluation of summaries we use the dard rogue metric for comparison with vious amr based summarization methods we report the recall precision and scores for since most of the literature on marization uses scores for and rogue l for comparison we also report scores for and rogue l for our method recall and precision are sured for uni gram overlap between the reference and the predicted summary on the other hand uses bi gram overlap while l uses the longest common sequence between the target and the predicted summaries for evaluation in rest of this section we provide methods to lyze and evaluate our pipeline at each step amr parsing to understand the fects of using an amr parser on the results we compare the nal scores after the following two rst when we use the gold standard amr graphs and second when we used the amr graphs generated by jamr parser in the pipeline tion contains a comparison between the two summary graph extraction for uating the effectiveness of the summary graph traction step we compare the nal scores with the lead n amr baselines described in section in order to compare our summary graph traction step with the previous work liu et al we generate the nal summary using the same generation method as used by them their method uses a simple module based on alignments for generating summary after the ments simply map the words in the original tence with the node or edge in the amr graph to generate the summary we nd the words aligned with the sentence in the selected graph and put them in no particular order as the predicted summary though this does not generate matically correct sentences we can still use the metric similar to liu et al as it is based on comparing uni grams between the target and predicted summaries generation for evaluating the ity of the sentences generated by our method we compare the summaries generated by the model and amr model on the standard dataset however when we looked at the scores given by rogue we decided to do get the above summaries evaluated by humans this duced interesting results which are given in more detail in section results on the proxy report section in table we report the results of using the pipeline with generation using the alignment based ation module dened in section on the proxy report section of the amr bank all of our ods out perform liu et al s method we obtain best scores using the rst model for important sentences this also out perform our amr baseline by points effects of using jamr parser in this subsection we analyze the effect of ing jamr parser for instead of the standard amr graphs first part of table has scores after using the gold standard amr graphs in the second part of table we have included the scores of using the jamr parser for amr graph generation we have used the same neural amr for sentence generation in all methods scores of all methods including the amr baseline have dropped signicantly the usage of jamr parser has affected the scores of rst co and more than that for the amr the drop in rogue score when we use rst is around two rogue points more than when amr this is a surprising result and we believe that it is worthy of further research effectiveness of the generator in this subsection we evaluate the effectiveness of the sentence generation step for fair comparison at the generation step we use the gold standard amrs and do nt perform any extraction in instead we use full amrs this allows to move any errors that might have been generated in and in order to compare the ity of sentences generated by the amr we need a gold standard for sentence generation step for this we simply use the original sentence as standard for sentence generation thus we pare the quality of summary generated by and amr the scores using the rogue metric are given in bottom two rows of table the results show that there is signicant drop in amr when compared to we perform human evaluation to check whether the drop in rogue scores is because of drop in information contained and human readability or is it because of the inability of the rogue ric to judge to perform this evaluation we domly select ten test examples from the three test cases of the proxy report section for each example we show the summaries generated by four different models side by side to the man evaluators the human evaluator does not know which summaries come from which model a score from to is then assigned to each summary on the basis of readability and mation contained of summary where sponds to the lower level and to the highest in table we compare the scores of these four cases as given by rogue along with human uation the parser generator pairs for the four cases are gold neural gold neural and the original sentence spectively here gold parser means that we have used the gold standard amr graphs the scores given by the humans do not relate with rogue human evaluators gives most similarly scores to summary generated by the and amr with amr tually performing better on readability though it dropped some information as clear from the scores on information contained on the other hand rogue gives very high score to while models and get almost same scores the similar scores of model and shows that tors are actually producing meaningful sentences thus the drop in rogue scores is mainly due to the inability of the rogue to evaluate abstractive summaries moreover the rogue gives model higher score compared to model while human evaluators give the opposite scores on information table comparion of the scores given by rogue and human evaluators on different models scores suggest that rogue do nt co relate with the human evaluators parser generator jamr gold neural jamr gold neural original sentence information contained readability r l table results on cnn dailymail corpus table has parts first part contains baselines our method and the state of the art on the non anonymized dataset second part has scores on the anonymized dataset method amr baseline non anonymized see et al pointer generator coverage see et al anonymized nallapati et al rl with intra attention paulus et al recall rogue precision l contained in the sentence a possible reason for the inability of the rogue metric to properly evaluate the maries generated by our method might be due to its inability to evaluate restructured sentences amr formalism tries to assign the same amr graphs to the sentences that have same meaning so there exists a one to many mapping from amr graphs to sentences this means that the automatic generators that we are using might not be trying to generate the original sentence instead it is ing to generate some other sentence that has the same underlying meaning this also helps in plaining the low and rogue l scores if the sentences might be getting rephrased they would loose most of the and tri grams from the original sentence resulting in low and rogue l scores analyzing the effectiveness of amr extraction the aim of extracting summary graphs from the amr graphs of the sentence is to drop the not so important information from the sentences if we are able to achieve this perfectly the recall scores that we are getting should remain almost the same since we are not add any new information and the precision should go up as we have thrown out some useless formation thus effectively improving the overall score in the rst two rows of ble we have the scores after using the full amr and extracted amr for generation respectively it is safe to say that extracting the amr results in improved precision whereas recall reduces only slightly resulting in an overall improved results on the cnn dailymail corpus in table we report the results on the dailymail corpus we present scores by using the model the rst row contains the amr baseline the results we achieve are petitive with the amr baseline the rest of the table contains scores of baseline followed by the state of the art method on the anonymized and non anonymized versions of the dataset the drop in the scores from the anonymized to amr is signicant and is largely because of the error introduced by parser and generator related work and discussion related work dang and owczarzak showed that most of the work in text summarization has been tive where sentences are selected from the text which are then concatenated to form a summary vanderwende et al transformed the input to nodes then used the pagerank algorithm to score nodes and nally grow the nodes from value to low value using some heuristics some of the approaches combine this with sentence pression so more sentences can be packed in the summary mcdonald martins and smith almeida and martins and gillick and favre among others used ilps and proximations for encoding compression and traction recently some abstractive approaches have also been proposed most of which used sequence to sequence learning models for the task rush nallapati chopra et al et al et al see et al used standard encoder decoder models along with their variants takase et al to generate summaries incorporated the amr information in the dard encoder decoder models to improve results our work in similar to other graph based tive summarization methods penn and cheung and gerani et al penn and ung used dependency parse trees to produce summaries on the other hand our work takes vantage of semantic graphs need of an new dataset and evaluation metric rogue metric by it is design has lots of ties that make it unsuitable for evaluating tive summaries for example rogue matches exact words and not the stems of the words it also considers stop words for evaluation one of the reasons why rogue like metrics might never become suitable for evaluating abstractive maries is its incapabilities of knowing if the tences have been restructured a good evaluation metric should be one where we compare the ing of the sentence and not the exact words as we showed section rogue is not suitable for evaluating summaries generated by the amr pipeline we now show why the cnn dailymail corpus is not suitable for abstractive summarization the nature of summary points in the corpus is highly extractive section for details where most of the summary points are simply picked up from some sentences in the story tough this is a good enough reason to start searching for better dataset it is not the biggest problem with the dataset the dataset has the property that a lot of important formation is in the rst few sentences and most of the summary points are directly pick from these sentences the extractive methods based on tence selection like summarunner are not ally performing well the results they have got are only slightly better than the baseline the work does nt show how much of the selected tences are among the rst few and it might be the case that the sentences selected by the extractive methods are mostly among the rst few sentences the same can be the problem with the abstractive methods where most the output might be getting copied from the initial few sentences these problems with this corpus evoke the need to have another corpus where we do nt have so much concentration of important information at any location but rather the information is more spread out and the summaries are more abstractive in nature possible future directions as this proposed algorithm is a step by step cess we can focus on improving each step to duce better results the most exciting ments can be done in the summary graph tion method not a lot of work has been done to extract amr graphs for summaries in order to make this pipeline generalizable for any sort of text we need to get rid of the hypothesis that the summary is being extracted exactly from one tence so the natural direction seems to be joining amr graphs of multiple sentences that are similar and then extracting the summary amr from that large graph it will be like clustering similar tences and then extracting a summary graph from each of these cluster another idea is to use amr graphs for important sentence selection conclusion in this work we have explored a pipeline using amr for summarization for the rst time we propose a new method for ing summary graph which outperformed previous methods overall we provide strong baseline for text summarization using amr for possible future works we also showed that rogue ca nt be used for evaluating the abstractive summaries generated by our amr pipeline references miguel b almeida and andre f t martins fast and robust compressive summarization with dual composition and multi task learning in ings of acl laura banarescu claire bonial shu cai madalina georgescu kira griftt ulf jakob kevin knight philipp koehn martha palmer and nathan schneider guidelines com amrisi guidelines blob master amr laura banarescu claire bonial shu cai madalina georgescu kira griftt ulf hermjakob kevin knight martha palmer philipp koehn and abstract nathan schneider ing ceedings of linguistic annotation workshop aclweb org anthology representation sembanking for sumit chopra michael auli and alexander m rush abstractive sentence summarization with tentive recurrent neural networks hoa trang dang and karolina owczarzak overview of the tac update summarization task in proceedings of text analysis conference tac jeffrey flanigan jaime carbonell chris dyer generation from and noah a smith tree using abstract meaning in proceedings of the transducers ference of the north american chapter of the association for computational linguistics org anthology representation jeffrey flanigan sam thomson jaime carbonell chris dyer and noah a smith a inative graph based parser for the abstract ing representation in proceedings of the annual meeting of the association for tional linguistics association for computational linguistics baltimore maryland pages aclweb org anthology shima gerani yashar mehdad giuseppe carenini raymond t ng and bita nejat stractive summarization of product reviews using discourse structure in proceedings of emnlp org papers pdf pdf dan gillick and benoit favre a scalable global model for summarization in ings of the naacl workshop on integer ear programming for natural langauge processing acm org citation kevin knight laura baranescu claire bonial madalina georgescu kira griftt ulf hermjakob daniel marcu martha palmer and nathan der deft phase amr annotation philadelphia linguistic data tium abstract meaning representation amr notation release web download philadelphia linguistic data consortium ioannis konstas srinivasan iyer mark yatskar yejin choi and luke zettlemoyer neural amr sequence to sequence models for parsing and eration in proceedings of the annual ing of the association for computational guistics association for computational linguistics org c lin rouge a package for automatic tion of summaries text summarization branches out post conference workshop of acl barcelona spain fei liu flanigan jeffrey thomson sam sadeh man and smith noah a toward abstractive in summarization using semantic representations proceedings of the conference of the north american chapter of the association for tational linguistics association for computational linguistics denver colorado pages aclweb org anthology andre f t martins and noah a smith summarization with a joint model in tence extraction and compression ings of the acl workshop on integer linear programming for natural language processing aclweb org anthology for ryan mcdonald a study of global inference gorithms in multi document summarization in ceedings of ecir ramesh nallapati feifei zhai and bowen zhou summarunner a recurrent neural network based quence model for extractive summarization of uments proceedings of the thirty first aaai conference on articial intelligence org anthology ramesh nallapati bowen zhou cicero a glar g ulehre abstractive text santos sequence to sequence rnns and beyond computational natural aclweb org anthology dos and bing xiang summarization using in learning language romain paulus caiming xiong and richard socher a deep reinforced model for abstractive marization org karl moritz hermann tomas kocisky ward grefenstette lasse espeholt will kay and phil blunsom mustafa suleyman teaching machines to read and comprehend org pdf xiaochang peng chuan wang daniel gildea and anwen xue addressing the data sparsity sue in neural amr parsing in proceedings of the conference of the european chapter of the ation for computational linguistics association for computational linguistics valencia spain pages aclweb org anthology gerald penn and jackie chi kit cheung for automatic in proceedings of emnlp supervised sentence enhancement summarization aclweb org anthology alexander m rush sumit chopra and jason weston a neural attention model for sentence marization org naomi saphra and adam lopez rica an amr inspector for cross language the system demonstrations of ments conference of the north american chapter of the association for computational linguistics aclweb org anthology j liu abigail see peter get manning marization with pointer generator org anthology and christopher d networks to the point linfeng song yue zhang xiaochang peng zhiguo wang and daniel gildea amr to text eration as a traveling salesman problem ings of the conference on empirical ods in natural language processing association for computational linguistics austin texas pages org anthology sho takase jun suzuki naoaki okazaki tomu hirao and masaaki nagata ral headline generation on abstract meaning representation in proceedings of emnlp org anthology lucy vanderwende michele banko and arul menezes event centric summary generation in proceedings of duc chuan wang sameer pradhan xiaoman pan heng ji and nianwen xue camr at task an extended transition based amr parser in proceedings of the international workshop on semantic evaluation association for compuational linguistics aclweb org anthology junsheng zhou feiyu xu hans uszkoreit weiguang qu ran li and yanhui gu amr parsing with an incremental joint model in proceedings of the conference on empirical methods in ural language processing association for tational linguistics austin texas pages org anthology
