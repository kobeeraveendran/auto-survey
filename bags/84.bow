improving multi document summarization text classication ziqiang wenjie sujian furu computing hong kong polytechnic university hong kong kong polytechnic university shenzhen research institute china laboratory computational linguistics peking university moe china research beijing china cszqcao polyu edu hk edu cn com v o n l c s c v v x r abstract developed far multi document summarization reached bottleneck lack sufcient ing data diverse categories documents text sication makes deciencies paper propose novel summarization system called tcsum leverages plentiful text classication data improve performance multi document summarization tcsum projects documents tributed representations act bridge text classication summarization utilizes classication results produce summaries different styles extensive experiments duc generic document summarization datasets tcsum achieve state art performance ing hand crafted features capability catch variations summary styles respect different text categories introduction increasing online information necessitated velopment effective automatic multi document rization systems long term research based summarization approaches grown dominant literature far prominent issue hinders improvement supervised approaches lack sufcient human summaries ing cao et al instance widely generic multi document summarization benchmark datasets contain human reference summaries total writing summaries extremely labor intensive consuming process limitation training data learning based summarization system forced heavily rely designed features simple models like support vector regression achieve state art performance extensive linguistic statistical tures hong nenkova break tleneck insufcient summarization training data taking advantage rich data sources good idea worth considering copyright association advancement articial intelligence www aaai org rights reserved nist existing summarization approaches basically apply uniform model generate summaries uments different text categories according observe summary styles different categories vary large degree common gories duc datasets e natural disaster phy example summarize natural disaster like hurricane people tend present moving path loss brings contrast biography summary pected include personal prole main butions person apparently summaries focus different aspects topics belong sponding categories document category given kedzie mckeown diaz nds tion category specic language models largely promotes summarization performance experiments wan et al summarization model good overall performance produces low quality summaries certain document sets summary style issue previously mentioned partly explain phenomena suggest possible way improve summarization performance compared summarization text classication datasets richer note summarization text classication require models understand tics documents better text representations learned classication data help train effective rization models know category document chance explore proper summary styles end propose novel rization system called tcsum leverages text cation data improve performance summarization distributed representations documents strated advantages summarization e kobayashi noguchi yatsuka text classication e lai et al tcsum projects documents distributed representations shared tasks text classication document embeddings followed classier learn association categories summarization document embeddings transformed match meaning reference summaries transformed embeddings hold information summary styles utilize cation result develop category specic transformation process model adopts recent hot topic neural work based transfer learning e syntactic parsing discourse parsing li li hovy noted model totally data driven e abstract tures learned automatically verify effectiveness tcsum duc generic summarization benchmark datasets tcsum able pete state art summarization systems ally heavily depends hand crafted features serve tcsum catches variations mary styles different text categories contributions paper listed follows leverage text classication datasets learn better ument representations summarization explore variations summary styles respect different text categories develop competitive summarization system need hand crafted features method let d denote document composed set sentences n text classication use c stand entire set categories assume d belongs c e cd cd sents actual category document d text classication model trained predict category d supervised sentence ranking required learning based summarization sentence holds saliency score ally measured respect human summaries reference summaries summarization model expected learn rank sentences accord actual sentence saliency section describe summarization tem called tcsum ranks sentences help text classication overall framework tcsum trated fig rst text classication model trained convolutional neural network model projects document distributed representation adds softmax classier predict category document summarization model shares projection cess generate document embeddings given mantic analysis understanding documents tial classication summarization transforms document embedding summary bedding tries maximize match meaning reference summaries transformed mary embedding sensitive different summary styles tcsum learns category specic transformation matrices cording predicted categories finally sentences ranked according saliency scores calculated based similarity sentence embedding summary embedding rest section describes details model text classication model convolutional neural networks cnns learn stract representations n grams effectively tackle sentences variable lengths naturally models ing cnns achieved excellent performance text figure overview tcsum classication lai et al summarization yin pei paper develop simple cnn based classication model specically use cnn project sentence s distributed representation rm e s cnn basic cnn contains convolution operation word embeddings followed pooling eration let rk refer k dimensional word embedding corresponding ith word sentence let concatenation word embeddings convolution operation involves lter w rmhk applied window h words produce abstract features gh rm gh w non linear function use tanh common practice simple bias term left lter applied possible window words sentence produce feature map subsequently pooling operation applied feature map obtain nal features gh rm lter use max time pooling collobert et al gh gh primary purpose pooling capture important features feature map gh output cnn e embedding sentence document represented average pooling sentence embeddings like lai et al sd learn association document embedding categories document embedding followed softmax classier w weight matrix predicted probability distribution gories summarization model previously mentioned summarization model sum shares convolution pooling operations classication model generating document embedding tcsum transforms match meaning reference summary e rm transformed embedding called summary embedding w rmm tion matrix note dene dimension document summary embeddings setting simplies sentence ranking process explained later like summary embedding hold information summary styles inspired work dong et al develop category specic formation matrix w according predicted category introduce sub matrices directly corresponding text category based predicted category derived eq transformation matrix w computed weighted sum matrices way w automatically biased sub matrix predicted text category vi summary embedding expected match meaning reference summaries ability properly judge sentence saliency sistent reference summaries following kobayashi noguchi yatsuka use cosine similarity summary embedding sentence embedding predict sentence saliency rs rs vt s d document summary embeddings dimensionality training use pre trained word embeddings date avoid types weight matrices models e w w formation sub matrices text classication dataset larger summarization dataset w w learned classication data transformation matrices trained summarization data text classication adopt cross entropy cost function e vi equals iff actual category der cost function gradient softmax similar linear function fastens training process summarization apply pairwise ranking egy collobert et al tune weights specically time randomly select sentence high actual saliency score low actual saliency score denoted s respectively eq obtain predicted saliency scores pairwise ranking criterion tcsum higher score comparison s cost function dened follows rs margin threshold cost functions apply diagonal variant adagrad mini batches duchi hazan singer update model parameters adagrad adapts learning rate different parameters different steps sensitive initial parameters tic gradient descent experiments datasets summarization commonly evaluation pora summarization ones published ment understanding conferences duc text analytics conferences work focus generic multi document summarization task carried duc documents news domain collection documents related topic grouped cluster ter accompanied reference summaries written human experts summarization model compiles uments cluster single document table shows size datasets summary length itation task duc datasets come wide range categories manually categorize duc documents categories e biography culture ness health politics law society natural disaster ence sports international category distribution duc illustrated fig categories natural disaster politics biography account documents dataset duc duc duc cluster doc ref limitation words words bytes table statistics summarization datasets text classication order benet text tion need classication dataset large cover categories discovered duc datasets build dataset new york times nyt annotated corpus nyt corpus contains million articles published annotated new york times notably new york times important data provider duc nyt documents rich data utilize types metadata types material nist gov ldc upenn edu ing employs simple greedy algorithm similar vious work cao et al select summary sentences baseline methods compare tcsum best peer systems participating duc evaluations named peer plus ids addition include cao et al art supervised summarization model based neural networks applies recursive neural network learn combination hand crafted features notably heavily depends hand crafted features contrast sum fully data driven e features learned matically implement widely learning based tion method support vector regression svr li et al extracts number manually compiled features sentence tf frequency word cluster number documents containing word cluster number sentence contains number design neural network based baselines named notc singlet emsim rst verify value text classication notc use classication data applies marization model tcsum designed check summarization model work singlet ignores predicted text category uses single transformation matrix explores effect summary styles emsim aims test need learn summary embedding uses cosine similarity sentence embedding document bedding rank sentences emsim unsupervised summarization model similar kobayashi noguchi yatsuka baselines employ tence selection process model summarization performance conduct fold validation model trained years data tested remaining year rouge scores models compared presented table draw lines table distinguish els hand crafted features seen models completely dependent automatically learned features tcsum achieves est performance datasets poor mance emsim denotes directly use document embeddings learned text classication measure sentence saliency summarization note notc achieves competitive performance svr summarization models hand crafted features doable singlet greatly outperforms notc veries text classication help rization model learn better document representations tcsum greatly surpass singlet terms rouges section usually captures different summary styles figure category distribution duc taxonomic classiers online descriptors pick documents categories notice numbers documents different categories extremely imbalanced example category business contains documents documents category natural disaster conduct sampling process ensure category contains documents classication dataset times larger summarization dataset cross validation shows learned tion model tcsum achieves accuracy dataset classication focus paper ignore detailed performance evaluation classication model evaluation metric summarization evaluation use lin regarded standard automatic evaluation metric rouge measures summary quality ing overlapping units n grams word sequences word pairs candidate summary ence summary following common practice recall scores main metrics comparison measure gram bi gram similarities respectively training actual saliency sentence eq evaluated model settings cnn introduce word embedding set trained large english news corpus tokens mikolov et al dimension word embeddings set previous papers e collobert et al set dimension tence document embeddings equivalent dimension word embeddings window size sistent evaluation empirically set margin threshold pairwise ranking initial learning rate batch size summary obliged offer informative redundant content tcsum focuses sentence options parameter length constraint duc duc use integer linear programming select better sentences consider result greedy selection fair comparison year model peer t svr notc emsim singlet tcsum peer svr notc emsim singlet tcsum peer svr notc emsim singlet tcsum table rouge scores different methods compared models tcsum largely forms svr peer systems time superior state art method sidering tcsum supplemented hand crafted features performance promising taking closer look feature weights learned svr nd important feature measure sentence saliency cf treat documents topic cluster gle document feature lost current tion model important aspect impedes excellent performance tcsum discussion summary style learning examine ability tcsum learn summary styles ways rst speculate similar tion matrices tend generate summaries similar styles calculate similarity tion matrices atten matrix vector use cosine similarity measure similarity scores different transformation matrices presented fig ease reference results common categories ducs e raphy politics natural disaster seen ilarity relations categories vary greatly matches intuition large difference mary styles exists categories biography nd transformation matrix similar categories business culture politics international lation possible reason summaries biography necessarily tell career related information person duc prefers choosing biographies artists nessmen politicians reasonable summary style biography associated categories contrast natural disaster present obvious ity category observe summaries ural disaster contain series times sites bers categories seldom need details politics nd similar international ship law understandable use number terms politics describing tional relationships caused news content documents category concerned political scandals lead lawsuits ingly obvious negative similarity tics culture wordings politics thought documents culture usually lated entertainment inspect style change summaries ated according different categories end ually assign category document cluster culate sentence saliency based summarization model salient sentences respect different egories shown table limit space display ranked summary sentences styles common text categories hurricane natural disaster introduces founder wall mart biography describes resignation prime minister itics seen salient sentences calculated rect categories properly represent main idea document cluster lated politics sentences selected corresponding transformation matrix contain terms politics shown biography sentences contain ther words describing careers killer mayor founder evaluative words better boldly career personal prole description main contributions person usually involves evaluative words corresponding transformation matrix catch types needs biography summaries read documents carefully nd sentence exactly matching natural disaster surprising sentences selected natural aster clusters somewhat strange sentences contain date site mation absolutely consistent style summary natural disaster expected money value word bombing describe loss disaster appears mation matrix natural disaster works topic natural disaster diligence complete task related work work extractive summarization spans large range proaches starting unsupervised methods widely known approaches maximum marginal relevance mmr carbonell goldstein greedy approach select sentences considered trade saliency redundancy good results achieved reformulating integer linear cluster category natural disaster biography politics biography natural disaster politics politics biography sentence storm packing winds mph raged charleston thursday night dangerous killer hurricane likes people lived lives charleston experienced warned mayor joseph p riley jr gov joe frank harris declared state emergency counties sam walton founder wal mart chain discount supermarkets died cancer april negotiated pitfalls better chain s sales risen nearly dollars making world s largest retailer terms revenues walton family probably america s richest bud senior vice president board member wal mart flamboyant defense minister hazeltine s challenge prime minister garet thatcher leadership conservative party caused political tion britain persian gulf crisis boldly joined george bush sending troops middle east natural disaster western allies ronald reagan s supporting u s bombing libya table salient sentences selected different categories sentences correct categories displayed rst trained language model based convolutional ral networks project sentences distributed tations cheng lapata treated single document summarization sequence labeling task modeled recurrent neural networks like kobayashi noguchi yatsuka simply sum trained word beddings represent sentences documents addition extractive summarization deep learning technologies applied compressive abstractive rization filippova et al rush chopra weston conclusion future work paper propose novel summarization system called tcsum leverages text classication prove performance summarization extensive iments duc generic summarization benchmark datasets tcsum achieves state art performance hand crafted features serve tcsum catches variations summary styles different text categories believe model summarization tasks including focused summarization guided summarization tion plan let model distinguish documents topic cluster better adapted multi document summarization acknowledgments work described paper supported search grants council hong kong polyu national natural science foundation china hong kong polytechnic university b correspondence authors paper wenjie li sujian li references cao et al cao z wei f dong l li s zhou m ranking recursive neural networks figure similarity transformation matrices set self similarity scores ming ilp problem able nd global timal solution mcdonald gillick favre graph based models manifold wan xiao played important role extractive summarization cause ability reect sentence relationships contrast unsupervised methods successful learning based summarization approaches different classiers explored including tional random field galley support vector sion li et al logistic regression li qian liu recently application deep neural network niques attracted interest rization research genest gotti bengio unsupervised auto encoders represent manual system summaries summary evaluation method surpass rouge cao et al cao et al tried use neural networks complement sentence ranking features models achieved state art performance relied hand crafted features researches explored directly measure ilarity based distributed representations yin pei lai et al lai s xu l liu k zhao j recurrent convolutional neural networks text tion proceedings aaai li et al li s ouyang y wang w sun b multi document summarization support vector regression proceedings duc li li hovy li j li r hovy e h recursive deep models discourse parsing ings emnlp li qian liu li c qian x liu y supervised bigram based ilp extractive rization proceedings acl lin lin c rouge package automatic evaluation summaries proceedings acl shop mcdonald mcdonald r study global inference algorithms multi document summarization springer mikolov et al mikolov t chen k corrado g dean j efcient estimation word tions vector space arxiv preprint rush chopra weston rush m chopra s weston j neural attention model proceedings emnlp tive sentence summarization wan xiao wan x xiao j based multi modality learning topic focused document summarization proceedings ijcai wan et al wan x cao z wei f li s zhou m multi document summarization discriminative summary reranking arxiv preprint yin pei yin w pei y optimizing sentence modeling selection document tion proceedings ijcai application multi document summarization proceedings aaai cao et al cao z wei f li s li w zhou m wang h learning summary prior tation extractive summarization proceedings acl short papers cao et al cao z chen c li w li s wei f zhou m tgsum build tweet guided document summarization dataset proceedings aaai cao et al cao z li w li s wei f attsum joint learning focusing summarization neural attention proceedings coling carbonell goldstein carbonell j goldstein j use mmr diversity based reranking ordering documents producing summaries ings sigir cheng lapata cheng j lapata m neural summarization extracting sentences words arxiv preprint collobert et al collobert r weston j bottou l karlen m kavukcuoglu k kuksa p natural language processing scratch journal machine learning research dong et al dong l wei f zhou m xu k adaptive multi compositionality recursive neural models applications sentiment analysis ings aaai duchi hazan singer duchi j hazan e singer y adaptive subgradient methods online learning stochastic optimization journal chine learning research filippova et al filippova k alfonseca e menares c kaiser l vinyals o tence compression deletion lstms proceedings emnlp galley galley m skip chain conditional random eld ranking meeting utterances importance proceedings emnlp genest gotti bengio genest p gotti f bengio y deep learning automatic mary scoring proceedings workshop automatic text summarization gillick favre gillick d favre b scalable global model summarization proceedings workshop ilp nlp hong nenkova hong k nenkova improving estimation word importance news multi document summarization proceedings eacl kedzie mckeown diaz kedzie c mckeown k diaz f predicting salient updates disaster summarization proceedings acl kobayashi noguchi yatsuka kobayashi h noguchi m yatsuka t summarization based proceedings emnlp embedding distributions
