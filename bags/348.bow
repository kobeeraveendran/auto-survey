scisummpip unsupervised scientic paper summarization pipeline jiaxin ming liu longxiang shirui information technology monash university australia vic information technology deakin university australia vic monash edu m liu longxiang edu au shirui edu abstract scholarly document processing sdp workshop encourage efforts ural language understanding scientic task contains shared tasks pate longsumm shared task paper describe text summarization system scisummpip inspired summpip zhao et al unsupervised text summarization system multi document news domain scisummpip includes transformer based language model scibert beltagy et al contextual sentence representation content selection ank page et al sentence graph struction deep linguistic mation sentence graph clustering graph summary generation work differs previous method content tion summary length constraint plied adapt scientic domain experiment results training dataset blind test dataset effectiveness method empirically verify ness modules scisummpip bertscore zhang et al introduction text summarization aims automatically ating uent coherent summary mainly contains salient information source main categories typically involved text summarization task tractive approach luo et al xu rett directly extracts salient sentences input text summary abstractive approach sutskever et al et al sharma et al imitates man behaviour produce new sentences based extracted information given document order meet requirements modern data driven methods large datasets presented majority datasets generic domain available corpora task specic domains existing state art summarization systems liu pata zhou et al wang et al target news simple documents adequate summarizing scientic work length complexity summarization systems provide sufcient information veyed scientic paper general domain paid tention attention scientic domain far address point arly document processing sdp workshop drasekaran et al held accelerate tic discovery research community appeal researchers designing summarization tem generate relatively long summary scientic work release transformer vaswani et al bert devlin et al search carried involving system liu modied input quence embedding built specic layers extractive summarization ilarly liu lapata present novel document level encoder based bert devlin et al extractive summarization abstractive summarization model structure lower transformer represents adjacent sentences higher layer self attention mechanism represents multi sentence discourse works leverage advantage deep neural work taking account linguistic mation contrast zhao et al construct semantic clusters sentence graphs document summarization involves linguistic information discourse markers paper com summpip t c o l c s c v v x r followed framework zhao et al construct unsupervised text summarization system model different previous work modify pipeline structure multi document summarization eld news single document summarizer marizing scholarly documents introduce new steps control length generated summary remove irrelevant sentences contributions work rized following aspects highlight importance sentence bedding scientic work variety works focus facilitating process taining sentence representation trained language model generic domain attention paid specic domains compare performances pagerank page et al maximal marginal relevance mmr carbonell goldstein content selection ule knowledge previous work pares performances scientic long document summarization task deep ral representation experimentally verify ness proposed model achieve better rouge results original model training dataset blind test dataset model evaluated bertscore metric zhang et al results indicate model robust erate high quality summary related work text summarization system recent text summarization systems leverage tages deep neural networks decoder structures use recurrent neural works cheng lapata nallapati et al transformer encoders zhang et al khandelwal et al benet sequence sequence structure great progress extractive abstractive document rization achieved abstractive rization potentials generate tions human like fashion found repeatedly produces phrase sentence suzuki nagata greatly reduces comprehensibility readability contrast extractive summarization performs better uency aspect grammatical curately represent source text potential issue extractive summarization information extracted sentence tant leads redundancy generated summary work zhao et al apply graph structure consider discourse ship sentences decoder structure text compression mented nal stage reduce redundancy generated sentences model designed multi document summarization news domain extend summpip document settings scientic long articles sentence embedding method term quency inverse document frequency tf idf widely traditional nlp capture semantic information contextual lationship sentences mikolov et al summpip zhao et al capture contextualized relationship embedding method solve polysemous problem recently bert devlin et al achieved better performance nlp downstream tasks difcult derive sentence embeddings solve limitation single sentences passed bert common ways extract sentence representation widely averaging outputs output cls token et al zhang et al xiao develops repository bert accelerates process ing token sentence embeddings bert devlin et al lately order nd better way derive semantically similar sentence language models reimers gurevych present sbert works help cilitate workload generic domain task specic domain content selection graph intuitive structure utilizing relation information tences work mihalcea tarau erkan radev focuses selecting salient sentences leveraging graph based com hanxiao bert characteristics range corpus size sentences median value corpus size sentences range sentence length words median value sentence length words extractive abstractive sci p ref s sci p ref s test dataset sci p table elementary data statistics longsumm shared task scholarly document processing emnlp sci p ref s represent scientic paper reference summary respectively ing methods inspired pagerank algorithm page et al consider document graph sentences vertices edges resent relations sentences shortly researchers carbonell stein kurmi jain mao et al involved query biased strategy imal marginal relevance mmr carbonell goldstein summarizers mmr tries balance relevance diversity ling trade parameter rst formula controls query relevance second controls diversity m m r argmax q sic argmax sj sj s c set candidate sentences s set extracted sentences q query ding si sj sentence embeddings candidate sentences j respectively sim indicates cosine similarity embeddings approach proved outperforms generic summarization approaches information retrieval task knowledge previous work compared ank algorithm scientic long document rization task work incorporates deep neural representations pagerank algorithm mmr strategy shows comparison methods eld scientic work extractive abstractive summarization dataset pre processing training dataset provided longsumm shared task consists scientic papers extractive method abstractive method reference tive summaries generated talksumm lev et al extracts sentences appeared associated conference videos tive summaries collected blogs written researchers download paper download training pus given urls abstractive script extractive paper parsing papers parsed pdf form json structure outputs json le pdf contains title abstract text metadata text section paper text processing concatenate section text paper text sentences mented nltk library tence tokenized table reports result statistics analysis training dataset test dataset number sentences reference summaries far required length generated summary words lead bias evaluation system overview adopt summpip zhao et al baseline model modify pipeline tecture summarizing scholarly documents new steps introduce adapting scientic main remove irrelevant sentences control length generated summary following subsections specify component scisummpip embedding method pretrained language model paper apply publicly available large scale language model scibert beltagy et al pretrained based bert devlin et al extends idea word embeddings learning com allenai science parse contextual representations large scale tic corpora implemented pytorch transformers established wolf et al sentence embedding accurate tence embeddings improve performance summarization system language understanding scisummpip average output scibert second layer layer addition experiment embedding methods results accurate way represent scientic sentences sentence graph construction content selection sentences involved summary include content selection step constructing sentence graph build matrix store similarity sentences pagerank page et al algorithm implemented rank tences sentences lower score deleted candidate list introduce new step control ratio removed sentences graph construction construct sentence graph node represents sentence nodes connected meet linguistic requirements identify structure row components previous work zhao et al specically pipeline consists discovering deverbal noun reference nding entity continuation recognizing discourse markers calculating sentence similarity ing cosine similarity text generation spectral clustering identifying pairwise sentence connection involve new step termining number clusters control length generated summary mary varies length original paper compression module multi sentences boudin morin generate single summary sentence sentence cluster sentences similar semantic information compressed building word graph ering key phrases discourse structure reconstructed sentence higher score select sentence highest score summary sentence combine reconstructed summary sentences generated summary experiment setup implementation details extractive summarization task use ert sentence embedding pipeline extractive text summarization task directly use scibert xed length range words abstractive summarization task ment pipeline scisummpip abstractive marization task compare performances pagerank algorithm mmr strategy content selection module pagerank gorithm set cutoff ratio new duced parameter removing irrelevant sentences andthe empirical results setting achieves better performance mmr egy set trade parameter experiment respectively control erated summary length introduce new parameter extended ratio modify number clusters based number ranking sentences pipeline set comparison systems extractive task compare model following unsupervised summarization models textrank barrios al textrank halcea tarau applies variation pagerank algorithm page et al graph based structure produces list ranked elements graph need training corpus textrank implemented paper produced barrios al change similarity function okapi performance better original trank model set output summary xed length words lexrank erkan radev lar textrank mihalcea tarau lexrank applies pagerank algorithm leverages graph structure summarization ferently textrank calculate similarity based number words sentences common lexrank uses cosine similarity tf idf vectors extractive summarizer org project com huggingface transformers extractive f r f r rl f rl r extractive dataset scibert summarizer textrank lextrank mmrsci abstractive dataset r scisummpipp r scisummpipm m scisummpipm m scisummpipm m blind test dataset scibert summarizer scisummpip summpip table rouge scores reported training dataset blind test dataset best results boldface reference extractive summary abstractive summary generated talksumm lev et al collected online blogs respectively mmrsci indicates implement mmr algorithm sentence embeddings derived et al scisummpipp r scisummpipmm r model different content selection modules number follow mmr setting trade parameter summpip effectively run large scale corpora long document add content selection module shown r mmr carbonell goldstein mmr query biased summarization approach tries balance relevance diversity ling trade parameter previous works similarity usually calculate based idf implementation use sentence embeddings derived output scibert beltagy et al addition set ument title query xed length generated summary set words abstractive task apply different sentence embedding methods scisummpip scibert beltagy et al ment common strategies sentence beddings derived scibert model eraging output second layer cls token embedding summpip zhao et al use embedding method original pipeline compare performance sbert reimers gurevych modication bert network ing siamese triplet networks order nd semantically similar sentences vector space empirical results indicate method better common embedding strategies incorporate scisummpip comparison evaluation results experiment result training dataset extractive summaries training dataset extractive method consists papers paper parsed evaluate papers rouge hovy experiments displayed table scibert summarizer achieves better rouge scores pared systems implement mmr algorithm sentence embedding derived averaging scibert beltagy et al output performs better lexrank erkan radev worse textrank model barrios al okapi ity function verify ank ranking algorithm performers better mmr strategy extractive task abstractive summaries abstractive ments collect summaries total paper parsed science parse sentence embedding avg scibert embeddings special token embedding sbert f f rl f precision recall scisummpip scisummpipm m r r sbert score table rouge scores scisummpip ferent sentence embedding methods special token bedding method extracting cls token embedding scibert beltagy et al output table bertscore reported abstractive training dataset investigate text generation ability model sbert means use use sbert sentence bedding method scisummpip sentence embedding avg scibert embeddings special token embedding sbert r r rl r table rouge recall results scisummpip different sentence embedding methods implement scisummpip different rameter settings nd best ber words sentence set observe summary words sentence achieves best mance incorporate pagerank algorithm page et al mmr algorithm carbonell goldstein scisummpip content tion module respectively displayed table surprising scisummpip pagerank algorithm outperforms settings scisummpip mmr algorithm performance textrank better mmr extractive task experiment result test dataset blind test dataset consists scientic declare blind test data extractive summarizer abstractive summarizer implement scibert summarizer scisummpip comparing summpip zhao et al experiment results verify new pipeline architecture signicantly prove performance addition try different number words generated sentence nd setting closes median value scientic papers gain higher score extractive model gains est rouge score scisummpip competitive dataset com guyfe longsumm different sentence embedding methods nd accurate method ing scientic sentences incorporate different embedding strategies scisummpip mances reported table table indicate model ranks highest averaging output scibert beltagy et al method sbert reimers gurevych shows petitive performance designed generic domain fact utilizing sbert cantly reduce workload extracting sentence embedding sufcient resenting scientic sentence bertscore evaluation evaluate models bertscore zhang et al automatic evaluation metric text generation investigate ability writing stractive summary bertscore calculates ilarity score token candidate tence token reference sentence leveraging contextual embeddings seen table scisummpip achieves highest cision score sbert gains highest recall proves summary generated model informative tive bertscore utilizes bert devlin et al calculate similarity score max length input sequence tokens limits performance relatively long summary investigate distribution score bertscore evaluation shown gure models achieve similar formance score distribution scisummpip obviously stable scisummpip achieve highest frequency range means near generated summaries gain score model robust summarizing entic work abstractive task figure histogram distribution score evaluated bertscore metric model reported table x axis indicates data range score y axis indicates frequency data bin order ensure bin data range distribution set data range bin parameter bins set range f extractive reference summary analysis emotions texts important task nlp traditional studies treat task pipeline separated sub tasks emotion classication emotion cause detection identies category emotion detects cause emotion separated framework makes sub task exible deal neglects relevance sub tasks paper use human labeled emotion corpus provided cheng et al tal data cheng emotion corpus cheng emotion corpus considered collection subtweets emotion subtweet emotion keywords ing emotion selected class cause emotion annotated scibert summarizer analysis emotions texts important task nlp cheng emotion corpus considered collection subtweets given instance pair emotion keyword clause subtweet ecause assigns binary label instance indicates presence causal relation input text ecause instance sequences words emotion keyword e emokw current clause e causecl context emokw causecl bilstm layer focuses extraction sequence features attention layer focuses learning word importance weights table example generated extractive summary compared reference summary generated talksumm lev et al text color indicates content describe length constraint omit generated summary shown human analysis manually inspect generated mary explore model capture salient information given document table ble display example generated summary compared corresponding reference mary training dataset abstractive abstractive reference summary paper proposes stage synthesis network perform transfer learning task machine hension problem following domain ds labelled dataset question answer pairs domain dt labelled dataset use data domain ds train synnet use generate synthetic answer pairs domain dt train machine comprehension model m ds netune thetic data dt synnet works stages answer synthesis given text paragraph generate answer word vector append word candidate answer append feed bi lstm network encoder decoder decoder conditions representation generated encoder question tokens generated far scisummpip ability quickly use mc model trained main answer questions paragraphs annotated data recent work generated synthetic data generated questions leads improved performance use model answer synthesis tion types generate answer rst answers usually key semantic concepts questions transfer mc model trained domain ensemble bidaf model fs use stage synnet generate data tuples directly boost performance boost unlike machine translation tasks like mc need synthesize question answers given context paragraph rst stage model answer synthesis module uses bi directional lstm predict iob tags input paragraph mark key semantic concepts likely answers table example generated abstractive mary compared reference summary lected researcher s blog text color dicates content describe similar length constraint omit generated mary shown erence summary collected online blog written researcher difcult capture similar description generated score bertscore evaluation summary shown table model successfully write similar context nal output notwithstanding ability grammatically generated mary need improved blind test dataset inspect tractive summary abstractive summary paper nd scibert summarizer tends extract sentence appeared early paper generated summary ally lack logicality consistency contrast summary produced scisummpip ical contains salient information methodology experiment scibert summarizer gains higher rouge score blind test dataset summary generated model consistent purpose longsumm shared task conclusion limitation paper presented modied pervised pipeline architecture scisummpip leverages transformer based language model summarizing scientic papers add content lection module steps remove irrelevant sentences control length generated summary linguistic knowledge incorporated process multi sentences compression summarizing scientic work experiment results automatic evaluation prove new pipeline signicantly improves overall performance training blind test dataset manual inspection nd model capture salient mation given source document admit readability generated summary needs improved incorporated deep neural representation mmr carbonell goldstein egy pagerank page et al algorithm mmr strategy performs better formation retrieval task empirically veried sufcient model summarize scientic work mmr query biased approach chose title query tation potential reason worse mance query chose effective investigate sentence embedding method sufciently summarizing scholarly document compared performances ding strategies evaluated mances rouge metric bertscore metric averaging output scibert beltagy et al achieves better performance workload extract sentence dings heavier directly sbert reimers gurevych work generic domain attention paid task specic domain far fore appeal researchers making efforts task specic domain search future work future evaluate pipeline larger scientic datasets effectiveness robustness like conduct analysis faithfulness level straction generated summary acknowledgments like thank anonymous helpful comments suggestions references federico barrios federico lopez luis argerich rosa wachenchauzer variations larity function textrank automated tion arxiv preprint iz beltagy arman cohan kyle lo scibert pretrained contextualized embeddings scientic text arxiv preprint florian boudin emmanuel morin keyphrase extraction n best multi sentence compression reranking jaime carbonell jade goldstein use mmr diversity based reranking reordering proceedings uments producing summaries annual international acm sigir ence research development information retrieval pages maroli krishnayya chandrasekaran guy blat hovy eduard anirudh ravichander michal shmueli scheuer anita de waard overview insights scientic document summarization shared tasks cl scisumm summ longsumm proceedings workshop scholarly document processing sdp jianpeng cheng mirella lapata neural marization extracting sentences words arxiv preprint jacob devlin ming wei chang kenton lee kristina toutanova bert pre training deep bidirectional transformers language ing arxiv preprint ramesh nallapati bowen zhou caglar gulcehre bing xiang al abstractive text tion sequence sequence rnns arxiv preprint gunes erkan dragomir r radev lexrank graph based lexical centrality salience text summarization journal articial intelligence search urvashi khandelwal kevin clark dan jurafsky lukasz kaiser sample efcient text marization single pre trained transformer arxiv preprint lawrence page sergey brin rajeev motwani terry winograd pagerank citation ing bringing order web technical report stanford infolab nils reimers iryna gurevych bert sentence embeddings siamese networks arxiv preprint rashmi kurmi pranita jain text tion enhanced mmr technique national conference computer communication informatics pages ieee abigail peter j liu christopher d point summarization arxiv preprint ning pointer generator networks guy lev michal shmueli scheuer jonathan herzig achiya jerbi david konopnicki summ dataset scalable annotation method scientic paper summarization based conference talks arxiv preprint chin yew lin eduard hovy matic evaluation summaries n gram occurrence statistics proceedings man language technology conference north american chapter association tional linguistics pages yang liu fine tune bert extractive rization arxiv preprint yang liu mirella lapata text rization pretrained encoders arxiv preprint ling luo xiang ao yan song feiyang pan min yang qing reading like human reading inspired extractive summarization ceedings conference empirical ods natural language processing ternational joint conference natural language processing emnlp ijcnlp pages yuning mao yanru qu yiqing xie xiang ren jiawei han multi document summarization maximal marginal relevance guided ment learning arxiv preprint chandler alex wang shikha bordia samuel r bowman rachel rudinger arxiv suring social biases sentence encoders preprint rada mihalcea paul tarau textrank ing order text proceedings ference empirical methods natural language processing pages tomas mikolov ilya sutskever kai chen greg s rado jeff dean distributed tions words phrases advances neural information processing ity systems pages eva sharma luyang huang zhe hu lu wang entity driven framework abstractive summarization arxiv preprint ilya sutskever oriol vinyals quoc v le sequence sequence learning neural networks advances neural information processing tems pages jun suzuki masaaki nagata cutting dundant repeating generations neural abstractive summarization arxiv preprint ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan n gomez ukasz kaiser illia polosukhin attention advances neural information need cessing systems pages danqing wang pengfei liu yining zheng xipeng qiu xuanjing huang heterogeneous graph neural networks extractive document marization arxiv preprint thomas wolf lysandre debut victor sanh julien chaumond clement delangue anthony moi ric cistac tim rault remi louf morgan icz joe davison sam shleifer patrick von platen clara ma yacine jernite julien plu canwen xu teven le scao sylvain gugger mariama drame quentin lhoest alexander m rush huggingface s transformers state art natural language processing arxiv han xiao bert service com hanxiao bert service jiacheng xu greg durrett neural tive text summarization syntactic compression arxiv preprint tianyi zhang varsha kishore felix wu kilian q weinberger yoav artzi bertscore arxiv preprint uating text generation bert xingxing zhang furu wei ming zhou hibert document level pre training hierarchical bidirectional transformers document tion arxiv preprint jinming zhao ming liu longxiang gao yuan jin lan du zhao zhang gholamreza haffari summpip unsupervised document summarization sentence graph pression proceedings international acm sigir conference research ment information retrieval pages qingyu zhou furu wei ming zhou level extract empirical study extractive document summarization arxiv preprint
