c e d l c s c v v i x r a abstractive headline generation for spoken content by attentive recurrent neural networks with asr error modeling lang chi hung yi and lin shan institute of communication engineering national taiwan university institute of computer science and information engineering national taiwan university edu tw sinica edu tw abstract headline generation for spoken content is important since spoken content is difcult to be shown on the screen and browsed by the user it is a special type of abstractive marization for which the summaries are generated word by word from scratch without using any part of the original tent many deep learning approaches for headline generation from text document have been proposed recently all requiring huge quantities of training data which is difcult for spoken document summarization in this paper we propose an asr error modeling approach to learn the underlying structure of asr error patterns and incorporate this model in an attentive recurrent neural network arnn architecture in this way the model for abstractive headline generation for spoken tent can be learned from abundant text data and the asr data for some recognizers experiments showed very encouraging results and veried that the proposed asr error model works well even when the input spoken content is recognized by a recognizer very different from the one the model learned from index terms abstractive summarization headline eration asr error modeling attention mechanism decoder architecture introduction document summarization is to generate a concise version of a given document while preserving the core information this is important for both written and spoken content because both of them usually include redundant noisy or less mative parts causing interference to users who wish to grasp copyright ieee published in the ieee workshop on spoken language technology slt scheduled for december in san juan puerto rico personal use of this material is permitted however permission to reprint republish this material for advertising or promotional purposes or for creating new collective works for resale or redistribution to servers or lists or to reuse any copyrighted component of this work in other works must be obtained from the ieee contact manager copyrights and permissions ieee service center hoes lane p o box away nj usa telephone intl the key information quickly it is much more crucial for ken content than for written content since spoken content is difcult to be shown on the screen and browsed by the user while summaries of spoken content are very helpful in ing there are two categories for the summarization task in extractive approaches the important parts of the original data are extracted and put together to form the summary in contrast in abstractive approaches the summary is ated word by word from scratch without using any part of the original content when the abstractive summarization sult includes only one sentence this is usually referred to as sentence summarization headline generation is an example of abstractive sentence summarization and is tremely important for spoken content because with the lines the users do not have to go through the lengthy part of the spoken content which they are not interested we focus on abstractive headline generation for spoken content in this paper abstractive summarization for text content has been cessful with deep neural network dnn techniques for example those using dnn models with attention nism and using rnn models with encoder decoder architecture useful in neural machine translation and dialogue model improved training techniques were also developed for example scheduled sampling was applied to abstractive summarization to bridge the gap between training and inference stage due to the differences in the input tokens to the decoder the els can also be learned to directly optimize some evaluation metrics however all the above works focused on text content while such neural network based approaches for spoken tent summarization were rarely seen probably due to the difculties in acquiring enough quantities of spoken content including the reference summaries to train such models for example in the previous works for text summarization the training datasets were english gigaword corpus for glish and lcsts corpus for chinese which included respectively million and million document headline pairs to collect speech corpora including the reference maries in the quantities of this order of magnitude is probably difcult it is certainly possible to directly apply the scriptions of audio data to the summarization models trained on text corpora but the asr errors would inevitably degrade the summarization performance since these models never learned how to generate the abstract summaries from content with asr errors in this paper we solve this problem by developing an asr error model learned from the asr data for some recognizer and incorporate this model with an attentive rnn arnn encoder decoder architecture in order to learn from written content to generate headlines from spoken content this per is organized as follows in section we dene the task introduce the previously proposed architectures and present the model proposed in this paper we then describe the imental setup in section and present the results in section and concluding remarks in section models task denition our summarization task is dened as below given an input sequence x xm which is a sequence of m kens from a xed known dictionary vx the model is to nd y yn which is another sequence of n tokens from another xed known dictionary vy here x is the input text or spoken documents expressed as a sequence and y is the abstractive headline expressing the meaning of x in the most concise way for example in our experiments below vy is the set of all allowed chinese characters and vx can be the same character set as vy or a set of initials and finals of mandarin initial is the initial consonant of a mandarin lable while final is the vowel part including optional medials and nasal ending this is because the input spoken content can be expressed as a sequence of phonetic symbols also since very often recognition errors are caused by incorrectly recognized phonetic units expressing the input as a sequence of phonetic units may be helpful in asr error modeling as will be clear below the task here can then be considered with a conditional probability p y for all possible y such that the desired output y arg maxy p y this probability p y is usually parameterized by a set of neural ters as p y and y is usually obtained sequentially by predicting every token in y based on the previous token p y p x n which can be modeled with rnn encoder decoder tures described in the following fig rnn encoder decoder architecture rnn encoder decoder architecture an rnn encoder decoder architecture is shown in fig it consists of two parts the encoder rnn and the decoder rnn the encoder reads the input x one token at a time and updates its hidden state hj according to the current input xj and the previous hidden state hj j m where rnne is a nonlinear function after the encoder reads the last token xm it outputs a context vector c hm rn as the learned representation of the whole input sequence x the decoder then predicts y one token at a time given the context vector c and all previous predicted tokens based on the conditional probability in can be expressed as p x c hm c hm where rnnd and dec are certain nonlinear tions i n bos a special token for beginning of sentence and rn is the decoder rnn den state at i th output step the i th output token yi is then the one which maximizes the probability in which is then used in for decoding the next output token ing training the previous output token can be the labeled reference output attentive rnn arnn architecture in the above architecture the next token yi of y is predicted based on the conditional probability in which is mined by the context vector c containing information of all the tokens in x nonetheless not all tokens in x are equally informative for the decoding process and some of the input token may be noisy an improved attentive rnn ture was then proposed as in fig in fig the context vector c in and is modied as the weighted sum of the encoder hidden states at all input steps ci aijhj m now we can dene f as f x xm where is a distribution p k is a distinct token item in vx over all allowed distinct token item in vx so for a given sequence x the confused sequence f x j in can be any token in vx with some probability m is stochastic because each asr error modeling for arnn there can be at least two possible approaches for error eling with the attentive rnn arnn as explained below nave approach we can simply apply f in to the input data x to obtain many confused data f x and use these confused data and their headlines to train the models in section and we call this method nave approach proposed approach in this approach we modify the attention mechanism in tion with the function f x in in order to explore the underlying structure of asr error patterns first we dene a function p p xm xm which is a vector of dimensionality m the lengths of the put sequence x xm the th element in is the probability that xj is correct so this vector gives the likelihood whether or not a token in x is unaffected by asr we apply to ci ejaijhj m in this way the decoder pays more attention to those tokens that are more likely to be correct to estimate the elements in we train a sequential error estimation model see ej p xj xj where see is a neural network whose training target can be easily obtained by comparing x and any confused version of it note that an asr error may lead to a signicant change in semantics which is why ej can be estimated sequentially as in this neural network and the attentive rnn can be jointly trained in practice see is the direct output of the encoder rnn which is unused in the architectures in tion and as shown in fig and fig the complete attentive rnn with weighted attention in by error eling is in fig the training process includes the following steps fig attentive rnn arnn encoder decoder architecture the weight aij is dened as aij where mij is the cosine similarity between the decoder hidden state si and encoder hidden state this implies that the input tokens better matched to the output token being decoded are given higher weights with the new context vector ci in the decoding cess in are modied accordingly p x in this way different input tokens are weighted differently for different output tokens i e the decoder pays more attention to those input tokens more useful for the output token it is currently decoding in fig we only show the decoding process of asr error confusion function the asr error modeling can be started with a ed confusion function asr errors can be considered as a transformation called confusion here f x where x xm is the correct input sequence and m the asr results both of which are quences of tokens from the dictionary vx in we may approximate f with a simplied context independent sion matrix trained with the output samples from a speech recognizer we wish to model we rst align the pairs of correct and asr transcriptions with minimum levenshtein distance using dynamic programming with the alignment we compute the confusion probability as p q p k p where p q k are distinct token items in vx and q p is the number of token q in asr results aligned to token p in correct transcriptions the summation in the denominator is over all allowed distinct token items in vx finals of mandarin after the preprocessing we paired the rst sentence of each news story with its headline to form a story headline pair and removed those pairs whose headlines contained over of unk symbols the whole corpus was used on the training set which consisted of about million story headline pairs and about k distinct characters the dataset used to obtain the confusion matrix for asr error modeling and evaluation of the headline generation was the matbn mandarin chinese broadcast news it contained a total of hours of broadcast news from the public television service foundation of taiwan with corresponding transcriptions including human generated headlines we partitioned the corpus into two parts k utterances for confusion matrix construction and the rest utterances for headline generation evaluation for the part for evaluation we paired the asr results of each story with its corresponding headlines to form a story headline pair there are about audio stories for the evaluation we used two different recognizers in the experiments here the kaldi toolkit and the online asr recognizer wit ai for the recognizer with kaldi toolkit we used a tri gram language model trained on m words of yahoo news and a set of acoustic models with gaussian tures per state and states per model trained on a training corpus of hours of mandarin broadcast news ent from matbn the character error rates cer for the matbn corpus with kaldi and wit ai were and respectively the confusion matrix used for asr error modeling was obtained from the kaldi toolkit while the evaluation part was transcribed by both the kaldi toolkit and wit ai with the error modeling based on kaldi toolkit performed on wit ai transcriptions we wish to evaluate the robustness of the error modeling approach with respect to mismatched recognizers implementation we implemented the models with lstm networks mized by minimizing the negative log likelihood between the predicted and the human generated headlines with mini batch stochastic gradient descent the training setting summarized below were adjusted based on the validation set the encoder and the decoder both had two hidden ers of dimensions the lstm network parameters were initialized from a uniform distribution between the initial learning rate was and divided by if the log likelihood in validation set did not improve for every epoch the training dropout rate was gradient ping was adopted with a gradient norm threshold of the models were trained at most epochs during the training process we adopted the scheduled sampling nism with decay schedule of inverse sigmoid decay for k fig the proposed arnn with error modeling we apply f in to each input training sequence x to generate many different samples of f x since f x is stochastic the encoder reads all these confused sequences f x into hidden states and predicts the ness ej for each input token xj the decoder predicts y one token at a time based on the encoder hidden states and the weighted attention sidering as in in the testing process step above is skipped since the input is the asr data so fig actually shows the testing process for training process the input xj should be replaced by f xj experimental setup here we describe the corpora used and some implementation details datasets the arnn model was trained on the chinese gigaword corpus this corpus consists of several years of news articles from central news agency of taiwan and xinhua news agency of china the following preprocessing steps were performed on this corpus all chinese characters were rst converted into the traditional version of characters if they were not next we removed articles from the period of the matbn corpus this corpus was used to train the asr recognizer replaced characters that occurred less than ve times in the whole corpus with a special ter unk and replaced arabic numerals with note that the basic processing unit for the work here was the character so there was no need to segment the character sequences into word sequences in order to be able to take initial final sequences as the input we also converted the articles from character sequences to initials final sequences with a pronunciation dictionary which contained a total of right context dependent initials and context independent experimental results we evaluated the results with and rouge l scores as mentioned in section the input spoken content can be either character sequences or initial final sequences the models in sections and rnn and arnn without asr error modeling were taken as the baseline models the nave approach of directly training the baseline models with confused input sequences described in subsection and the proposed approach described in subsection were compared oracle results manual transcriptions input first we tested the baseline models rnn and arnn on manual transcriptions of news stories without asr errors which can be considered as the upper bound of the task ble shows the results the upper half of the table are for character sequence input char while the lower half for tial final sequence input i f from table we observed the slight improvement obtained by including the attention anism arnn vs rnn also character sequence input formed signicantly better than the initial final sequence put in all cases this is natural because in chinese language there exist large number of homonym characters sharing the same pronunciation so the pronunciation sequences carry much less information than character sequences table oracle results baseline models for manual scriptions input char i f rnn arnn rnn arnn rouge l asr transcriptions input the results for asr transcriptions input obtained with kaldi and wit ai are respectively in table and the upper half of table is for character sequence input the baseline els bsl in the rows refer to the same models as in table but in table asr errors are not considered the nave models na in rows refer to the nave approaches proposed in section i e baseline models but directly trained with confused data and the proposed approach in row e is arnn with error modeling the lower half is the same but with initial final sequence input table is exactly the same but with recognizer wit ai from rows of table we see the baseline arnn was actually slightly worse than baseline rnn rows vs table results for asr transcriptions input obtained with kaldi char i f bsl na rnn arnn c rnn arnn bsl na e proposed rnn g arnn h rnn i arnn j proposed rouge l a probably due to the wrong attention caused by asr rors in other words the model paid attention to some kens which were actually recognition errors this situation is reversed in nave approach rows d vs c probably cause the model may have learned to avoid to pay attention to incorrectly recognized errors but the overall performance of nave approach was worse than baseline rows vs probably because the baseline models rows were trained with correct manual transcriptions while the nave models rows were trained with confused scriptions and were therefore weaker having the error eling telling the model which input tokens were more likely to be correct in the proposed approach row e as explained in section not only the wrong attention could be avoided but the model learned how to take care of the errors when erating the headlines to a certain degree so the performance of the model was much better rows e vs the lower half of table for initial final sequence input offered lower performance just as in table but with similar trend as discussed above the only difference was that here baseline arnn was slightly better than baseline rnn rows g vs because many homonym characters share the same pronunciation the character error rate was much higher than the initial final error rate so the lower initial final error rate led to much less wrong attention on recognition errors table results for asr transcriptions input obtained with wit ai char i f bsl na rnn arnn c rnn arnn bsl na e proposed rnn g arnn h rnn i arnn j proposed rouge l the results using wit ai as the recognizer are listed in ble in which the confusion matrix used for error modeling was obtained by the transcriptions of kaldi toolkit pared with the results in table we see the scores in ble are in general lower than those in table not only because of the mismatched recognizers and recognition ror patterns but because the character error rate of wit ai was much higher than that of kaldi vs the specially low performance of baseline arnn row was obviously because the very high character error rate caused too much wrong attention and disturbed the model the very low performance of initial final sequence input lower half of table indicated that the phonetic sequences with low curacy carried too little information to be used for headline generation however we found that the proposed approach still performed very well row e for character sequence put even with the low asr accuracy and the mismatched ognizers conclusion in this paper we propose a novel attentive rnn arnn chitecture with asr error modeling for headline generation for spoken content which can be trained without a large pus of speech headline pairs experimental results show that the proposed model is able to learn the recognition error terns and avoid the errors when paying attention to important tokens in generating the headlines the model is even sonably robust with respect to the mismatched condition that the input spoken content is recognized by a recognizer very different from the one the model learned from references michele banko vibhu o mittal and michael j brock headline generation based on statistical lation in proceedings of the annual meeting on association for computational linguistics association for computational linguistics pp bonnie dorr david zajic and richard schwartz hedge trimmer a parse and trim approach to line generation in proceedings of the hlt naacl on text summarization workshop volume association for computational linguistics pp songhua xu shaohui yang and francis chi moon lau keyword extraction and headline generation ing novel word features in aaai alexander m rush sumit chopra and jason weston a neural attention model for abstractive sentence marization in emnlp dzmitry bahdanau kyunghyun cho and yoshua neural machine translation by jointly learning gio to align and translate in international conference on learning representations konstantin lopyrev generating news headlines with recurrent neural networks corr katja filippova enrique alfonseca carlos colmenares lukasz kaiser and oriol vinyals sentence sion by deletion with lstms in proceedings of emnlp pp sumit chopra michael auli alexander m rush and seas harvard abstractive sentence summarization with attentive recurrent neural networks in naacl jiatao gu zhengdong lu hang li and victor ok li incorporating copying mechanism in sequence sequence learning in association for computational linguistics jianpeng cheng and mirella lapata neural rization by extracting sentences and words corr caglar gulcehre sungjin ahn ramesh nallapati bowen zhou and yoshua bengio pointing the known words corr kyunghyun cho bart van merrienboer caglar cehre dzmitry bahdanau fethi bougares holger schwenk and yoshua bengio learning phrase sentations using rnn encoder decoder for statistical chine translation in conference on empirical methods in natural language processing lifeng shang zhengdong lu and hang li ral responding machine for short text conversation in emnlp samy bengio oriol vinyals navdeep jaitly and noam shazeer scheduled sampling for sequence prediction with recurrent neural networks in advances in neural information processing systems pp marcaurelio ranzato sumit chopra michael auli and wojciech zaremba sequence level training with recurrent neural networks in international conference on learning representations shiqi shen zhiyuan liu maosong sun al neural headline generation with minimum risk training corr david graff junbo kong ke chen and kazuaki maeda english gigaword linguistic data tium philadelphia baotian hu qingcai chen and fangze zhu lcsts a large scale chinese short text summarization dataset corr ilya sutskever oriol vinyals and quoc v le quence to sequence learning with neural networks in advances in neural information processing systems pp oriol vinyals meire fortunato and navdeep jaitly pointer networks in advances in neural information processing systems pp oriol vinyals and quoc le a neural conversational model in international conference on machine ing deep learning workshop david graff and ke chen chinese gigaword ldc catalog no isbn vol pp sebastien jean kyunghyun cho roland memisevic and yoshua bengio on using very large target ulary for neural machine translation in proceedings of acl ijcnlp pp hsin min wang berlin chen jen wei kuo shih sian cheng al matbn a mandarin chinese broadcast news corpus international journal of computational linguistics and chinese language processing vol no pp daniel povey arnab ghoshal gilles boulianne lukas burget ondrej glembek nagendra goel mirko nemann petr motlicek yanmin qian petr schwarz et al the kaldi speech recognition toolkit in ieee workshop on automatic speech recognition and understanding ieee signal processing society number epfl wit ai sepp hochreiter and jurgen schmidhuber long term memory neural computation vol no pp razvan pascanu tomas mikolov and yoshua gio on the difculty of training recurrent neural works icml vol pp chin yew lin rouge a package for automatic uation of summaries in text summarization branches out proceedings of the workshop barcelona spain vol
