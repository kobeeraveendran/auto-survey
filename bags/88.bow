c e d l c s c v v x r abstractive headline generation spoken content attentive recurrent neural networks asr error modeling lang chi hung yi lin shan institute communication engineering national taiwan university institute computer science information engineering national taiwan university edu tw sinica edu tw abstract headline generation spoken content important spoken content difcult shown screen browsed user special type abstractive marization summaries generated word word scratch original tent deep learning approaches headline generation text document proposed recently requiring huge quantities training data difcult spoken document summarization paper propose asr error modeling approach learn underlying structure asr error patterns incorporate model attentive recurrent neural network arnn architecture way model abstractive headline generation spoken tent learned abundant text data asr data recognizers experiments showed encouraging results veried proposed asr error model works input spoken content recognized recognizer different model learned index terms abstractive summarization headline eration asr error modeling attention mechanism decoder architecture introduction document summarization generate concise version given document preserving core information important written spoken content usually include redundant noisy mative parts causing interference users wish grasp copyright ieee published ieee workshop spoken language technology slt scheduled december san juan puerto rico personal use material permitted permission reprint republish material advertising promotional purposes creating new collective works resale redistribution servers lists reuse copyrighted component work works obtained ieee contact manager copyrights permissions ieee service center hoes lane p o box away nj usa telephone intl key information quickly crucial ken content written content spoken content difcult shown screen browsed user summaries spoken content helpful ing categories summarization task extractive approaches important parts original data extracted form summary contrast abstractive approaches summary ated word word scratch original content abstractive summarization sult includes sentence usually referred sentence summarization headline generation example abstractive sentence summarization tremely important spoken content lines users lengthy spoken content interested focus abstractive headline generation spoken content paper abstractive summarization text content cessful deep neural network dnn techniques example dnn models attention nism rnn models encoder decoder architecture useful neural machine translation dialogue model improved training techniques developed example scheduled sampling applied abstractive summarization bridge gap training inference stage differences input tokens decoder els learned directly optimize evaluation metrics works focused text content neural network based approaches spoken tent summarization rarely seen probably difculties acquiring quantities spoken content including reference summaries train models example previous works text summarization training datasets english gigaword corpus glish lcsts corpus chinese included respectively million million document headline pairs collect speech corpora including reference maries quantities order magnitude probably difcult certainly possible directly apply scriptions audio data summarization models trained text corpora asr errors inevitably degrade summarization performance models learned generate abstract summaries content asr errors paper solve problem developing asr error model learned asr data recognizer incorporate model attentive rnn arnn encoder decoder architecture order learn written content generate headlines spoken content organized follows section dene task introduce previously proposed architectures present model proposed paper describe imental setup section present results section concluding remarks section models task denition summarization task dened given input sequence x xm sequence m kens xed known dictionary vx model nd y yn sequence n tokens xed known dictionary vy x input text spoken documents expressed sequence y abstractive headline expressing meaning x concise way example experiments vy set allowed chinese characters vx character set vy set initials finals mandarin initial initial consonant mandarin lable final vowel including optional medials nasal ending input spoken content expressed sequence phonetic symbols recognition errors caused incorrectly recognized phonetic units expressing input sequence phonetic units helpful asr error modeling clear task considered conditional probability p y possible y desired output y arg maxy p y probability p y usually parameterized set neural ters p y y usually obtained sequentially predicting token y based previous token p y p x n modeled rnn encoder decoder tures described following fig rnn encoder decoder architecture rnn encoder decoder architecture rnn encoder decoder architecture shown fig consists parts encoder rnn decoder rnn encoder reads input x token time updates hidden state hj according current input xj previous hidden state hj j m rnne nonlinear function encoder reads token xm outputs context vector c hm rn learned representation input sequence x decoder predicts y token time given context vector c previous predicted tokens based conditional probability expressed p x c hm c hm rnnd dec certain nonlinear tions n bos special token beginning sentence rn decoder rnn den state th output step th output token yi maximizes probability decoding output token ing training previous output token labeled reference output attentive rnn arnn architecture architecture token yi y predicted based conditional probability mined context vector c containing information tokens x nonetheless tokens x equally informative decoding process input token noisy improved attentive rnn ture proposed fig fig context vector c modied weighted sum encoder hidden states input steps ci aijhj m dene f f x xm distribution p k distinct token item vx allowed distinct token item vx given sequence x confused sequence f x j token vx probability m stochastic asr error modeling arnn possible approaches error eling attentive rnn arnn explained nave approach simply apply f input data x obtain confused data f x use confused data headlines train models section method nave approach proposed approach approach modify attention mechanism tion function f x order explore underlying structure asr error patterns dene function p p xm xm vector dimensionality m lengths sequence x xm th element probability xj correct vector gives likelihood token x unaffected asr apply ci ejaijhj m way decoder pays attention tokens likely correct estimate elements train sequential error estimation model ej p xj xj neural network training target easily obtained comparing x confused version note asr error lead signicant change semantics ej estimated sequentially neural network attentive rnn jointly trained practice direct output encoder rnn unused architectures tion shown fig fig complete attentive rnn weighted attention error eling fig training process includes following steps fig attentive rnn arnn encoder decoder architecture weight aij dened aij mij cosine similarity decoder hidden state si encoder hidden state implies input tokens better matched output token decoded given higher weights new context vector ci decoding cess modied accordingly p x way different input tokens weighted differently different output tokens e decoder pays attention input tokens useful output token currently decoding fig decoding process asr error confusion function asr error modeling started ed confusion function asr errors considered transformation called confusion f x x xm correct input sequence m asr results quences tokens dictionary vx approximate f simplied context independent sion matrix trained output samples speech recognizer wish model rst align pairs correct asr transcriptions minimum levenshtein distance dynamic programming alignment compute confusion probability p q p k p p q k distinct token items vx q p number token q asr results aligned token p correct transcriptions summation denominator allowed distinct token items vx finals mandarin preprocessing paired rst sentence news story headline form story headline pair removed pairs headlines contained unk symbols corpus training set consisted million story headline pairs k distinct characters dataset obtain confusion matrix asr error modeling evaluation headline generation matbn mandarin chinese broadcast news contained total hours broadcast news public television service foundation taiwan corresponding transcriptions including human generated headlines partitioned corpus parts k utterances confusion matrix construction rest utterances headline generation evaluation evaluation paired asr results story corresponding headlines form story headline pair audio stories evaluation different recognizers experiments kaldi toolkit online asr recognizer wit ai recognizer kaldi toolkit tri gram language model trained m words yahoo news set acoustic models gaussian tures state states model trained training corpus hours mandarin broadcast news ent matbn character error rates cer matbn corpus kaldi wit ai respectively confusion matrix asr error modeling obtained kaldi toolkit evaluation transcribed kaldi toolkit wit ai error modeling based kaldi toolkit performed wit ai transcriptions wish evaluate robustness error modeling approach respect mismatched recognizers implementation implemented models lstm networks mized minimizing negative log likelihood predicted human generated headlines mini batch stochastic gradient descent training setting summarized adjusted based validation set encoder decoder hidden ers dimensions lstm network parameters initialized uniform distribution initial learning rate divided log likelihood validation set improve epoch training dropout rate gradient ping adopted gradient norm threshold models trained epochs training process adopted scheduled sampling nism decay schedule inverse sigmoid decay k fig proposed arnn error modeling apply f input training sequence x generate different samples f x f x stochastic encoder reads confused sequences f x hidden states predicts ness ej input token xj decoder predicts y token time based encoder hidden states weighted attention sidering testing process step skipped input asr data fig actually shows testing process training process input xj replaced f xj experimental setup describe corpora implementation details datasets arnn model trained chinese gigaword corpus corpus consists years news articles central news agency taiwan xinhua news agency china following preprocessing steps performed corpus chinese characters rst converted traditional version characters removed articles period matbn corpus corpus train asr recognizer replaced characters occurred ve times corpus special ter unk replaced arabic numerals note basic processing unit work character need segment character sequences word sequences order able initial final sequences input converted articles character sequences initials final sequences pronunciation dictionary contained total right context dependent initials context independent experimental results evaluated results rouge l scores mentioned section input spoken content character sequences initial final sequences models sections rnn arnn asr error modeling taken baseline models nave approach directly training baseline models confused input sequences described subsection proposed approach described subsection compared oracle results manual transcriptions input tested baseline models rnn arnn manual transcriptions news stories asr errors considered upper bound task ble shows results upper half table character sequence input char lower half tial final sequence input f table observed slight improvement obtained including attention anism arnn vs rnn character sequence input formed signicantly better initial final sequence cases natural chinese language exist large number homonym characters sharing pronunciation pronunciation sequences carry information character sequences table oracle results baseline models manual scriptions input char f rnn arnn rnn arnn rouge l asr transcriptions input results asr transcriptions input obtained kaldi wit ai respectively table upper half table character sequence input baseline els bsl rows refer models table table asr errors considered nave models na rows refer nave approaches proposed section e baseline models directly trained confused data proposed approach row e arnn error modeling lower half initial final sequence input table exactly recognizer wit ai rows table baseline arnn actually slightly worse baseline rnn rows vs table results asr transcriptions input obtained kaldi char f bsl na rnn arnn c rnn arnn bsl na e proposed rnn g arnn h rnn arnn j proposed rouge l probably wrong attention caused asr rors words model paid attention kens actually recognition errors situation reversed nave approach rows d vs c probably cause model learned avoid pay attention incorrectly recognized errors overall performance nave approach worse baseline rows vs probably baseline models rows trained correct manual transcriptions nave models rows trained confused scriptions weaker having error eling telling model input tokens likely correct proposed approach row e explained section wrong attention avoided model learned care errors erating headlines certain degree performance model better rows e vs lower half table initial final sequence input offered lower performance table similar trend discussed difference baseline arnn slightly better baseline rnn rows g vs homonym characters share pronunciation character error rate higher initial final error rate lower initial final error rate led wrong attention recognition errors table results asr transcriptions input obtained wit ai char f bsl na rnn arnn c rnn arnn bsl na e proposed rnn g arnn h rnn arnn j proposed rouge l results wit ai recognizer listed ble confusion matrix error modeling obtained transcriptions kaldi toolkit pared results table scores ble general lower table mismatched recognizers recognition ror patterns character error rate wit ai higher kaldi vs specially low performance baseline arnn row obviously high character error rate caused wrong attention disturbed model low performance initial final sequence input lower half table indicated phonetic sequences low curacy carried little information headline generation found proposed approach performed row e character sequence low asr accuracy mismatched ognizers conclusion paper propose novel attentive rnn arnn chitecture asr error modeling headline generation spoken content trained large pus speech headline pairs experimental results proposed model able learn recognition error terns avoid errors paying attention important tokens generating headlines model sonably robust respect mismatched condition input spoken content recognized recognizer different model learned references michele banko vibhu o mittal michael j brock headline generation based statistical lation proceedings annual meeting association computational linguistics association computational linguistics pp bonnie dorr david zajic richard schwartz hedge trimmer parse trim approach line generation proceedings hlt naacl text summarization workshop volume association computational linguistics pp songhua xu shaohui yang francis chi moon lau keyword extraction headline generation ing novel word features aaai alexander m rush sumit chopra jason weston neural attention model abstractive sentence marization emnlp dzmitry bahdanau kyunghyun cho yoshua neural machine translation jointly learning gio align translate international conference learning representations konstantin lopyrev generating news headlines recurrent neural networks corr katja filippova enrique alfonseca carlos colmenares lukasz kaiser oriol vinyals sentence sion deletion lstms proceedings emnlp pp sumit chopra michael auli alexander m rush seas harvard abstractive sentence summarization attentive recurrent neural networks naacl jiatao gu zhengdong lu hang li victor ok li incorporating copying mechanism sequence sequence learning association computational linguistics jianpeng cheng mirella lapata neural rization extracting sentences words corr caglar gulcehre sungjin ahn ramesh nallapati bowen zhou yoshua bengio pointing known words corr kyunghyun cho bart van merrienboer caglar cehre dzmitry bahdanau fethi bougares holger schwenk yoshua bengio learning phrase sentations rnn encoder decoder statistical chine translation conference empirical methods natural language processing lifeng shang zhengdong lu hang li ral responding machine short text conversation emnlp samy bengio oriol vinyals navdeep jaitly noam shazeer scheduled sampling sequence prediction recurrent neural networks advances neural information processing systems pp marcaurelio ranzato sumit chopra michael auli wojciech zaremba sequence level training recurrent neural networks international conference learning representations shiqi shen zhiyuan liu maosong sun al neural headline generation minimum risk training corr david graff junbo kong ke chen kazuaki maeda english gigaword linguistic data tium philadelphia baotian hu qingcai chen fangze zhu lcsts large scale chinese short text summarization dataset corr ilya sutskever oriol vinyals quoc v le quence sequence learning neural networks advances neural information processing systems pp oriol vinyals meire fortunato navdeep jaitly pointer networks advances neural information processing systems pp oriol vinyals quoc le neural conversational model international conference machine ing deep learning workshop david graff ke chen chinese gigaword ldc catalog isbn vol pp sebastien jean kyunghyun cho roland memisevic yoshua bengio large target ulary neural machine translation proceedings acl ijcnlp pp hsin min wang berlin chen jen wei kuo shih sian cheng al matbn mandarin chinese broadcast news corpus international journal computational linguistics chinese language processing vol pp daniel povey arnab ghoshal gilles boulianne lukas burget ondrej glembek nagendra goel mirko nemann petr motlicek yanmin qian petr schwarz et al kaldi speech recognition toolkit ieee workshop automatic speech recognition understanding ieee signal processing society number epfl wit ai sepp hochreiter jurgen schmidhuber long term memory neural computation vol pp razvan pascanu tomas mikolov yoshua gio difculty training recurrent neural works icml vol pp chin yew lin rouge package automatic uation summaries text summarization branches proceedings workshop barcelona spain vol
