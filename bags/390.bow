extractive summarization of call transcripts pratik k biswas and aleksandr iakubovich ai data science global network and technology verizon communications new jersey usa abstract text summarization is the process of extracting the most important information from the text and presenting it concisely in fewer sentences call transcript is a text that involves textual description of a phone conversation between a customer caller and customer representatives this paper presents an indigenously developed method that combines topic modeling and sentence selection with punctuation restoration in condensing ill punctuated or un punctuated call transcripts to produce summaries that are more readable extensive testing evaluation and comparisons have demonstrated the efficacy of this summarizer for call transcript summarization index terms extractive summarization topic models transformers embedding punctuation restoration i introduction in recent years there is an abundance of multi sourced information available for public consumption fueled by the growth of the internet in many cases this volume of readily available text needs effective summarization to be useful for different purposes it is very difficult for human beings to summarize large quantities of text manually hence automatic text summarization has become a very desirable tool in today s information age it has the task of producing concise fluent and readable summaries from larger bodies of text while still preserving the original information content and meaning such summarization can be very useful when applied to various domains e news articles emails call transcripts medical history mobile text messages many such summarizers are available online on the internet like microsoft for news articles mead swesum for biomedical information wikisummarizer for wikipedia articles extractive numerous approaches have been developed for automatic text summarization and can be broadly classified into two groups abstractive summarization extractive summarization extracts important sentences from the original text and reproduce them verbatim in the summary while abstractive summarization generates new sentences summarization and call transcripts are written texts originally presented in a different medium and so call transcription is defined as the process of converting a voice or video call audio track into written words through speech to text conversion to be stored as plain text in a conversational language in this paper however we will be confining ourselves to textual descriptions of audio recordings of voice calls between customer caller and customer representatives of a phone company transcripts summarization of call automatic in our consideration pose certain unique challenges as follows they are not continuous texts but include conversation between customers and agents they are often very long and are embedded with small talks and can include a large number of sentences that are irrelevant and even meaningless they include several ill formed grammatically incorrect sentences they are either un punctuated or are improperly punctuated based on pauses in the conversation as perceived by annotators and so are often unreadable and existing open source too well with call summarization transcripts tools do nt perform into customer and agent in this paper we have presented a novel extractive summarization technique that combines channel separation topic separation modeling sentence selection with punctuation restoration to produce properly punctuated fixed length and readable customer and agent summaries from the original call transcripts that can adequately summarize customer concerns and agent resolutions transcripts ii related work related research can be broadly grouped into two categories extractive summarization and abstractive summarization research under the first category is most relevant to our work radef et al defined summary as a text that is produced from one or more texts that conveys important information in the original and that is no longer than half of the original and usually significantly less than that automatic text summarization gained attraction as early as the different methods text summarization have been provided in surveys of automatic extensive and luhn al introduced a method to extract salient sentences from the text using features such as word and phrase frequency they proposed to weight the sentences of a document as a function of high frequency words ignoring very high frequency common words edmundson al described a based on key phrases where they used four different methods to determine the sentence weight the trainable document kupiec al developed summarizer which performed the sentence extracting task based on a number of weighting heuristics bookstein et al built clusters of index terms phrases and other sub parts of documents for extractive text summarization brandow al that automatically condensed domain independent electronic news data conroy al mittendorf al used hidden models for text summarization chen al text extraction system the anes launched copyright verizon all rights reserved information contained herein is provided as is and subject to change without notice all trademarks used herein are property of their respective owners this work has now been submitted to the ieee for possible publication copyright may be transferred without notice after which this version may no longer be accessible et to al discussed a sentence selection based approach text summarization while gong al and wang al described how multiple documents could be summarized using topic models neto al introduced machine learning approaches to automatic text summarization and kaikhah discussed how neural networks could be useful for summarizing news articles suanmali et al proposed fuzzy logic based extractive text summarization to improve the quality of the summaries created by the general statistical method nallapati al presented a recurrent neural network rnn based sequence model for extractive summarization of documents narayan conceptualized extractive summarization as a sentence ranking task and proposed a novel training algorithm for optimizing the rouge evaluation metric through a reinforcement learning objective xu et al constructed a neural model for single document summarization based jointly on extraction and syntactic compression verma al applied a restricted boltzmann machine to enhance the summaries of factual reports created through extractive summarization miller used bidirectional encoder representations from transformers bert for summarization of lecture notes liu described bertsum a simple variant of bert for extractive summarization liu et al displayed how bert could be generally applied in text summarization and proposed a general framework for both extractive and abstractive models zhong et al formulated the extractive summarization task as a semantic text matching problem while wu et al instituted a new text to graph task of predicting summarized knowledge graphs from long documents lemberger et al recounted the applicability of deep learning models for automatic text summarization lin al surveyed the state of the art in abstractive summarization while khan al reviewed various abstractive summarization methods nallapati al paulus et al and see et al employed recurrent neural network deep reinforcement learning and pointer generator network for abstractive summarization iii major contributions in this paper we have proposed an innovative extractive summarization technique for call transcript summarization our main contributions and advantages can be summarized as follows we have integrated topic modeling and embedding based sentence selection with transformer based punctuation restoration for extractive summarization through a novel step sequential procedure method our method splits the original call transcript into customer and agent transcripts by the associated channel identifiers and then summarizes each transcript separately for more coherent results our method restores full punctuation in the summaries of un punctuated or ill punctuated call transcripts we have uniquely modified and retrained the bert transformer model architecture for punctuation restoration by adding a classification layer above bert s layers our method creates compares and evaluates the performances of multiple different types of topic models for the transcripts before selecting the most optimal one for extractive summarization it also provides the option to specify the topic model type to be used for extractive summarization and allows the summarizer to use different to generate customer and agent topic model summaries types we have introduced a new metric for measuring the effectiveness of punctuation restoration in the punctuated summaries iv preliminaries concepts and terminologies in this section we clarify key concepts and terminologies and explain certain technologies which provide the foundation for our work the automatic summarization of text is a well defined task in the field of natural language processing nlp automatic text summarization attempts to convert a larger document into a shorter version preserving its information content and overall meaning a good summary should reflect the diverse topics of the document while keeping redundancy to a minimum in general there are two different approaches to automatic text summarization namely extractive and abstractive extractive summarization methods identify the relevant sections in the original text extract the most important paragraphs sentences phrases from there and concatenate them into shorter form in contrast abstractive summarization methods attempt to convey the most important information from the original text by generating new sentences in other words they interpret examine and analyze the original text using advanced natural language techniques to get a better understanding of the content and then describe it through shorter and more focused text comprising of new sentences purely extractive summaries often give better results than automatic abstractive summaries this is because of the that abstractive summarization methods cope with fact problems like semantic representation inference and natural language generation which are relatively harder than data driven approaches such as sentence extraction most abstractive summarization techniques specifically the ones extractive using deep summarization to extract the summaries for the training samples from which they train to generate new text in this paper we focus only on extractive summarization as it is relevant to our work also depend upon learning a extractive summarization extractive summarization techniques generate summaries by selecting the most important sentences paragraphs from the original text the importance of sentences is decided based on statistical and linguistic features of sentences input can be either single or multiple documents or sources of text extractive summarization consists of three main steps namely intermediate representation of input text scoring of sentences based on the intermediate representation selection of sentences for summary generation there are two approaches namely topic based and indicator based which are used for intermediate representation copyright verizon all rights reserved information contained herein is provided as is and subject to change without notice all trademarks used herein are property of their respective owners this work has now been submitted to the ieee for possible publication copyright may be transferred without notice after which this version may no longer be accessible of original text topic representation based approaches transform input text into constituent topics they are further subdivided into frequency driven topic word based cluster based latent semantic analysis dependent and bayesian topic model based methods indicator representation based approaches characterize sentences in the input text through features such as sentence length position in the document having certain phrases they can be further subdivided into graph theoretic fuzzy logic driven machine learning based and neural network based methods b latent semantic analysis latent semantic analysis lsa also known as latent semantic indexing lsi is an unsupervised method for extracting a representation of text semantics based on observed words it tries to bring out latent relationships within a collection of documents on to a lower dimensional space lsa is based on the principle that words that are close in meaning will occur in similar pieces of text the distributional hypothesis it uses a value mathematical decomposition svd to identify patterns in the relationships between in unstructured texts it was introduced by deerwester et al in gong et al proposed a lsa based method to select highly ranked sentences for single and multi document news summarization the terms and concepts contained technique singular called c bayesian topic models topic modeling can be described as a type of a statistical method for finding a group of words i e topic from a collection of documents that best represents the information in the collection bayesian topic models are unsupervised probabilistic models that uncover and represent the topics of documents or source texts they have gained huge popularity in recent years their advantage in describing and representing topics in detail enables the development of summarizer systems which can use them to determine the similarities and differences between documents to be used in summarization there are many techniques which are used to obtain probabilistic topic models latent dirichlet allocation lda is one such widely used topic modelling technique that represents the documents as a random mixture of latent topics where each topic is a probability distribution of words it has been used in recent times for multi document summarization wang et al introduced a bayesian sentence based topic model for summarization which using both term document and term sentence associations achieved significant performance improvement and outperformed many other summarization methods hierarchical dirichlet process hdp is another topic modeling technique which is an extension of lda it is a a nonparametric bayesian mixed membership model for unsupervised analysis of grouped data unlike lda its s finite counterpart hdp infers the number of topics from the data approach which uses d transformers transformers in nlp provide general purpose architectures for natural language understanding nlu and natural language generation nlg with over pre trained models the earlier deep they were first introduced in transformers are deep learning models that transform sequential inputs to sequential outputs however they are based solely on attention recurrence and mechanisms dispensing entirely with learning architectures convolutions of transformers do not require that the sequential data be processed in order which allows for much more parallelization than recurrent neural networks rnns and therefore reduced training times since their introduction transformers have become the model of choice for tackling many problems in nlp replacing older recurrent neural network models such as the long short term memory lstm transformer models can train on much larger datasets than before as they can support more parallelization during training this has resulted in the as bidirectional development of pre trained encoder representations from transformers bert which have been trained with huge general language datasets and can be fine tuned to specific linguistic tasks bert is a bidirectional transformer pre trained using a combination of language modeling objective and next sentence masked prediction on a large comprising the toronto book corpus and wikipedia by jointly conditioning on both left and right contexts in all layers consequently a pre trained bert model can be fine tuned with just one additional output layer to create state of the art models for a wide range of nlp tasks systems such the input transformers employ a layered encoder decoder architecture that comprises a stack of encoding layers that layer after another iteratively one processes and another stack of decoding layers that does the same thing to the output of the encoder the encoders are all identical in structure each one is broken down into two sub layers namely self attention and feed forward neural network the decoder has one more layer between them which is an attention layer that helps it to focus on relevant parts of the input sentence similar to what attention does in models therefore when we pass a sentence into a transformer it is embedded and passed into a stack of encoders the output from the final encoder is passed into each decoder block in the decoder stack which then generates the output e embeddings embeddings are mathematical functions that map entities to a latent space with complex and meaningful dimensions words or sentences or paragraphs can be mapped into a shared latent space such that the meaning of the word sentence paragraph can be represented geometrically machine learning approaches towards nlp require words to be expressed in vector form word embeddings proposed in is a feature engineering technique in which words are mapped into a vector of real numbers in a pre defined vector space it is a learned representation for text where words that have the same meaning have a similar representation the idea of using a dense distributed representation for each word is a key to the approach glove provide pre trained word embedding models in a type of transfer learning embedding techniques initially focused on words but the attention soon started to shift to other types of copyright verizon all rights reserved information contained herein is provided as is and subject to change without notice all trademarks used herein are property of their respective owners this work has now been submitted to the ieee for possible publication copyright may be transferred without notice after which this version may no longer be accessible textual content such as n grams sentences and documents the universal sentence encoder use encodes text into high dimensional vectors that can be used for text classification semantic similarity clustering and other natural language tasks the model is trained and optimized for sentences phrases or short paragraphs from a variety of data sources with the aim of dynamically accommodating a wide variety of natural language understanding tasks the model maps variable length input english text into an output of a dimensional vector v extractive summarization of call transcripts restoration this section provides a description of an extractive summarization technique that we are proposing for the summarization of call transcripts this extractive summarization technique uniquely integrates channel speaker separation topic modeling and similarity based sentence selection with punctuation sequential through procedure method the procedure is highly parameterized the following are its ten self contained steps each with its brief description call transcript channel speaker separation separate each call transcript into customer and agent transcripts based on its channel or speaker identifier by iterating through all transcripts a step partial punctuation restoration preprocess transcripts customer agent to remove existing punctuations and use a transformer based model to restore punctuations partially i e restore only periods as delimiters so that sentences can be separated in each transcript by iterating through all transcripts document preparation preprocess transcripts and generate documents from customer and agent transcripts by iterating through all transcripts i e one document from each transcript where each document is a list of words from that transcript obtained through nlp pipeline based preprocessing topic modeling build and optimize different types of topic models using the vocabularies corpus and documents from all of customer and agent transcripts and then pick the best customer and agent topic models based on their coherence scores i build different types of customer agent topic models e lda lsi hdp by varying their topic number values hyper parameter e within pre specified ranges and evaluate the models using their coherence scores select the most optimal or near about topic models for customer and agent transcripts the topic model type is a parameter of this procedure and so if it is provided during invocation then model optimization is confined to only that topic model type in step i for best model selection ii dominant topic identification get the most dominant from the aforesaid topic models with the associated keywords for each of customer and agent documents in every pair by iterating through all transcripts significant term selection get the most relevant keywords terms from each pair of customer and agent transcripts by doing term based similarity analysis between the keywords of the corresponding dominant topics using one of the following two approaches by iterating through all transcripts i ii global extraction extract terms from the keywords associated with each pair of dominant topics which need not be necessarily present in the transcripts customer agent themselves local extraction extract terms from local to the customer and agent transcripts that are similar to the corresponding dominant topic keywords and are also similar to themselves the choice for the term extraction method has also been parameterized for the procedure summary generation fixed length user specified number customer and agent transcript summaries by iterating through all pairs of customer and agent transcripts generate ii i identify the most unique sentences in each of customer and agent transcripts in every pair if necessary based on similarity analysis among all sentences of the corresponding transcript using embeddings extract a fixed number user specified of most relevant sentences from each of customer and agent transcripts through sentence based similarity analysis between the every corresponding transcript and the string document created out of the most significant terms for that pair of transcripts step using embeddings the desired length of the summary number of sentences sentence of is a parameter of the procedure punctuation restoration remove existing periods from each pair of customer and agent summaries restore partial and full punctuation using a transformer based model and post process to make them more readable by iterating through all of them summary tabulation save summaries of all transcripts in a table for future use summarization efficacy determination evaluate summaries on content information and readability punctuation restoration by iterating through every pair of transcripts their corresponding summaries i ii transcripts against original summary evaluation evaluate the goodness of summarization by comparing customer and agent summaries or manually generated summaries to generate average rouge scores punctuation restoration evaluation evaluate the correctness of punctuation restoration by matching the number of punctuation symbols periods their partially between the extracted and copyright verizon all rights reserved information contained herein is provided as is and subject to change without notice all trademarks used herein are property of their respective owners this work has now been submitted to the ieee for possible publication copyright may be transferred without notice after which this version may no longer be accessible punctuated summaries for both customer and agent to generate the average accuracy scores the full punctuation restored summaries from step are the outputs from this procedure next we take a deeper look at some key steps of this procedure discuss their implementations in detail and provide algorithms where necessary a channel separation call transcripts include conversations dialogs between customer and one or more agents and so the resultant summaries can often get mixed up separation of a transcript into customer and agent transcripts can make each summary more coherent customer summaries can give better ideas of the problems while the agent summaries can give a better understanding of the causes or the solutions the call transcripts are generally available to us as json formatted objects hence channel separation involves extraction of the transcript string from json formatted object channel identification and then decomposition of the transcript into customer and agent transcripts using the associated channel identifiers if the channel identifiers do not clearly identify the speakers then we can use a pre trained bert transformer model with a linear classifier from pytorch nn module as an additional layer on top of bert s layers to classify each dialog of the transcript into one of the classes i e customer and agent and then combine each type of dialogs to create customer and agent transcripts b document preparation a document is a list of keywords extracted from each transcript and is used as input to the topic model for document preparation we have built a custom nlp preprocessing pipeline comprising of tokenization punctuation extended stop words small words length removal regular expression matching lowercasing contraction mapping bigrams and trigrams creation lemmatization parts of speech tagging allowable tag selection this has been implemented by combining modules functionalities available from python packages namely re spacy nltk and genism c topic model optimization optimal model selection if the topic model type is specified at the invocation of the procedure then we create multiple topic models of the desired type for both customer and agent using the documents and vocabulary from the corresponding call transcripts by varying the hyper parameter e topic number values within the pre defined ranges by the pre defined steps compute their coherence scores and identify the topic models and associated hyper parameter values that produce the best scores otherwise by default we perform the above mentioned activity for all different topic model types namely lda lsi and hdp in parallel and identify the topic models and associated hyper parameter values that produce the best scores amongst topic models of all types fig displays the steps of this algorithm for topic modeling we have exclusively used the python based genism package fig topic model optimization selection d punctuation restoration here we describe the punctuation restoration algorithm used in steps and of the previously mentioned proposed procedure in detail we have used the bertformaskedlm class of the pytorch bert model for punctuation restoration and added an additional linear layer pytorch nn module above the bert layers the output of original bert layers is a vector with the size of all vocabulary the additional linear layer takes this as input and gives as output one of four classes i e o other comma period and question for each encoded word we retrained this modified bert model using ted transcripts consisting of two million words different variations of punctuation restoration with bert model have been presented earlier but the retraining with the proposed architecture is a unique approach for punctuation restoration the steps of this algorithm are as follows preprocess either transcript to remove duplicate words phrases and expressions or punctuations inserted as delimiters based on annotator s perceptions of the pauses in the conversation or a summary to remove periods the output from this step is a continuous string representing the cleaned unpunctuated text of the transcript summary instantiate the pre trained bert punctuation model and initialize it on gpus to classify each encoded word in text to one of classes namely other comma period and question tokenize numeric berttokenizer transcript summary and encode format identifier id token tokens using to the copyright verizon all rights reserved information contained herein is provided as is and subject to change without notice all trademarks used herein are property of their respective owners this work has now been submitted to the ieee for possible publication copyright may be transferred without notice after which this version may no longer be accessible create segments of surrounding token ids for each encoded word token id from the text and insert as a placeholder halfway through each segment the segment size is a parameter of the algorithm use the placeholders from the above step to predict punctuation class identifiers ids for all token ids in the the modified bert model segments classifications through to sets restore map class ids to words symbols and merge words if needed punctuated transcripts summaries one with just periods partial punctuation restoration and the other with all punctuations full punctuation restoration partial punctuation restoration is used in step while both partial and full punctuation restorations are used in step of the main procedure of fig displays the algorithm through block diagrams we have found that the bert model for punctuation restoration gives more accurate results than the lstm based model we have implemented the punctuation restoration algorithm using bert transformer bertpunc and nn modules available from pytorch the algorithm are a pair of customer and agent transcripts and the corresponding optimized topic models documents from all transcripts use selected topic models customer and agent to identify from each of customer agent documents for every pair and produce two lists of associated keywords one for each of customer agent transcripts the number of dominant topics to be identified per transcript is a parameter of the procedure use the keywords terms associated with customer and agent dominant topics to extract the most significant inter related terms for each pair of transcripts this has been achieved using word based similarity analysis as mentioned before in the main procedure we provide alternatives to keyword term extraction where the choice is parameterized if a global extraction is desired then use the two lists of keywords associated with the corresponding customer and agent dominant topics in every pair and identify terms that are most similar to each other i e where the degree of similarity is above a certain parameterized threshold the otherwise if a local extraction is desired then first find a set of terms from each of customer and agent documents in every pair that is most similar above parameterized the to threshold corresponding dominant transcript secondly identify terms from these two sets of terms extracted locally from customer and agent documents that are most inter related i e where degree of similarity is above the parameterized threshold associated with that keywords for construct a string or document with the extracted significant terms for each pair of customer and agent transcripts identify the most unique sentences in each of the customer and agent transcripts in every pair using embeddings and eliminate the redundant sentences to condense the original transcripts this is achieved by generating a correlation matrix with the embeddings for all the sentences in the original transcript and removing those sentences whose correlations are above a certain pre specified threshold the uniqueness threshold for sentence elimination is a parameter to the procedure select a certain specified number parameter to the procedure of most important sentences from each of the condensed customer and agent transcripts in every pair that are most similar to the string constructed step of the current procedure using sentence based embeddings list and concatenate them in order subsequently present the results as the summaries for the corresponding transcripts fig illustrates the crux of this algorithm for any given pair of term based through block diagrams for similarity analysis we have used algorithm based spacy s while sentence based similarity analysis and summary generation we have used the universal sentence encoder use from tensorflow hub along with the python based pandas and numpy packages transcripts for fig punctuation restoration e summary generation through sentence selection next we present the algorithm for generating separate customer and agent summaries from each pair of transcripts through sentence selection starting with their corresponding topic models in other words the following algorithm implements steps to of the proposed procedure the inputs to copyright verizon all rights reserved information contained herein is provided as is and subject to change without notice all trademarks used herein are property of their respective owners this work has now been submitted to the ieee for possible publication copyright may be transferred without notice after which this version may no longer be accessible pre processing punctuations removalbert punctuation model instantiation token id based segment creations placeholder insertions punctuation class id predictions merging restorationinput transcript summarypunctuated output transcript summarytokenization encoding produce lists one each for extracted and partially punctuated summaries for each of customer and agent count the number of matches between the two extracted lists of periods from the summaries of both customer agent and compute the above defined accuracy scores for both in each case the number reflects the of periods from the partially punctuated periods only summary found in the extracted summary we have computed the customer and agent rouge punctuation restoration accuracy scores for every pair of customer agent summaries by iterating through all transcripts calculated their averages from their respective individual scores vi performance evaluation user satisfaction effectiveness quality of summaries correctness of punctuations efficiency summarization time flexibility and performance comparison with another open source off the shelf extractive summarizer are some of the considerations that helped us evaluate the performance of our summarizer for call transcript summarization a experimental setup we set up a spark cluster consisting of a driver node and dynamically allocated multiple executor nodes for data collection preprocessing and summarization the nvidia cuda deep neural network accelerated our training process for punctuation restoration we re trained and tested the modified bert transformer model on nvidia tesla gb gpu based nodes the driver node used anywhere between to gpus we have tested our extractive summarizer on separate samples from different use cases with of these samples consisting of around call transcripts and another larger one consisting of call transcripts the evaluation of results have been both manual and automated b manual evaluation the summaries generated by the proposed method have been manually verified and validated for content and readability by different user groups for the four different use cases spread across multiple business units the goal was to see if the summaries were deemed generally useful for the purposes they were used in the specific use cases the process was informal in nature and the evaluation was subjective we relied on user our customer feedback and let them manage and control their own evaluation process and satisfaction levels four different user groups customers three internal and one external have now manually evaluated close to pairs of customer and agent summaries for the four different use cases for user evaluations we were mainly looking for answers to questions which were both generic and specific to the use cases the following are some examples of the two kinds generic did the customer and agent summaries in general give a fair description of the main problems complaints and the resolutions based on the original unseparated call fig summary generation through sentence selection f summarization evaluation we have determined the effectiveness of the summarizer by measuring both the goodness quality of summarization and the correctness accuracy of the punctuation restoration reflecting the readability of the summaries for the quality of the information content of the generated summaries we have used the metric rouge or bleu scores as a measurement of their goodness we have compared the customer and agent summaries against the corresponding transcripts or manually generated summaries if available and computed their individual rouge l or bleu scores using the python packages rouge rouge and nltk nltk translate for the correctness of the punctuation restoration we have defined as punctuation restoration accuracy score to measure the accuracy of the algorithm following named metric the definition punctuation restoration accuracy score represents the number of matches of punctuation symbols text periods transcript summary text transcript summary expressed as a percentage original extracted punctuated the and between the we have used the function from python s sklearn metrics package to implement this metric we have evaluated the effectiveness of the punctuation restoration algorithm through the following two steps extract periods from both the extracted customer and agent summaries step of the main procedure together with their period only summaries step of the same procedure and copyright verizon all rights reserved information contained herein is provided as is and subject to change without notice all trademarks used herein are property of their respective owners this work has now been submitted to the ieee for possible publication copyright may be transferred without notice after which this version may no longer be accessible dominant identification with associated keywordssignificant terms selection unique sentences identificationimportant sentences selectioncustomer agent transcripts anddocuments corpus optimized topic modelsfixed length summariesstring document construction results and summarizer comparisons fig and fig a show short sentences summaries from two call transcripts after channel separations describing customer complaints about internet and phone service disruptions and agents confirmations about impending technicians visits table i compares the effectiveness efficiency of the proposed summarizer for shorter summaries with those from the open source bert extractive summarizer using the four different samples on two different evaluation metrics bert extractive summarizer generated the summaries using the period restored customer and agent transcripts from step of the proposed procedure it s ratio parameter was automatically adjusted using the number of words in the transcript to ensure that its summaries were of comparable shorter lengths this is important as we found that longer the summary the more similar it is to the original transcript and so higher the rouge score for the larger sample sizes where manual summaries were not available we compared the generated summaries with the period restored original transcripts from step of our procedure for computing their corresponding rouge scores this was done for the summaries from both the proposed method as well as the bert extractive summarizer to ensure that there was consistency and similarity in the comparisons the results in table i establish that the proposed summarizer is more effective and efficient than bert extractive summarizer for call transcripts the punctuation restoration accuracy scores for customer and agent summaries have also varied between in all cases it may also be noted here that the proposed summarizer is highly parameterized and provides more options than the bert extractive summarizer transcripts if so then what percentage of customer and agent summaries accurately summarized the content did the summaries help users better comprehend the information content of the transcripts than the original transcripts themselves did the summaries capture other secondary issues topics besides the main issue topic if so then how many did the punctuations help in making the summaries more readable for understanding the content of the transcripts have the punctuations been generally restored correctly how did our summaries compare with manually generated summaries if available note a manually generated summary would consist of a fixed number same as that for the automated of ordered sentences extracted manually from the period only customer and agent transcripts generated at step of the proposed procedure that the user would deem as most important from those transcripts use case specific would the user be able to send the agent summaries as short text messages to the customers to prevent them from making repeat calls would the summaries indicate the possibility of churning for callers which are classified as churners what percentage of customer summaries included negative snippets for churn classification how did the summaries generated by the proposed step approach compare with summaries generated by several other open source off the shelf summarizers from transcripts which were originally ill punctuated with periods and where transcripts for external summarizers did nt go through an accurate period restoration step i e step of the proposed procedure we have used the feedbacks from each use case to improve our method and the results we are happy to report that different business units are now using our summaries for very different business purposes on an ongoing basis these c automated evaluation for automated evaluation we have looked at effectiveness and efficiency for measuring the effectiveness of our summarization and for comparing performances with another popular open source extractive summarizer we have used the metric rouge l score we have determined the efficacy of our punctuation own using punctuation restoration accuracy score metric restoration algorithm our the efficiency of a summarizer is important to real world applications we have measured the efficiency of our summarizer by recording the time taken by each of the steps of our proposed procedure method we have also compared the efficiency of our summary generation through sentence extraction algorithm step with that of the same open source extractive summarizer by recording the time taken by each to summarize each of the four different samples fig internet service disruption confirmation of technician s visit copyright verizon all rights reserved information contained herein is provided as is and subject to change without notice all trademarks used herein are property of their respective owners this work has now been submitted to the ieee for possible publication copyright may be transferred without notice after which this version may no longer be accessible fig customer complaint about phone internet disruption vii conclusion in this paper we have presented an extractive summarization technique to address some of the challenges associated in general with call transcript summarization we have combined channel separation topic modeling and sentence selection with punctuation restoration to generate more readable call transcript summaries in order to provide a better understanding of the customer complaints and the agent recommended solutions this is perhaps the first summarizer that creates and evaluates multiple different types of topic models before selecting the most optimal one for summarization we have provided a finer grained similarity analysis by using both term based similarities for significant term extraction and sentence based similarities for extractive summarization this similarity analysis leverages both and use based embeddings to exploit the semantic contents of words and sentences to determine their significance uniqueness and relevance the proposed extractive summarizer is the only one that restores full punctuation to the summaries generated from either ill punctuated or unpunctuated original call transcripts using a novel bert transformer based model we have introduced a new metric to evaluate the accuracy of the punctuation restoration in the resultant summaries finally we have established the efficacy of the proposed summarizer through extensive evaluations and performance comparisons conflict of interest statement the authors state that they are all employees of verizon and this paper addresses work performed in course of authors employment references mehdi allahyari seyedamin pouriyeh mehdi assefi saeid safaei elizabeth d trippe juan b gutierrez and krys kochut text summarization techniques a brief survey computing research repository arxiv david m blei andrew y ng and michael i jordan latent dirichlet allocation the journal of machine learning research a bookstein s t klein and t raita detecting content bearing words by serial clustering in proceedings of the acm sigir conference pages ronald brandow karl mitze and lisa f rau automatic condensation of electronic publications by sentence selection information processing and management daniel cer yinfei yang sheng yi kong nan hua nicole limtiaco rhomni constant mario john noah guajardo cespedes steve yuan chris tar yun hsuan sung brian strope ray kurzweil universal sentence encoder computing research repository arxiv st fang chen kesong han and guilin chen an approach to sentence selection based text summarization proceedings of ieee john m conroy and dianne p text summarization via hidden models proceedings of the annual international acm sigir conference on research and development in information retrieval pp scott c deerwester susan t dumais thomas k landauer george w furnas and richard a harshman indexing by latent semantic analysis jasis jacob devlin ming wei chang kenton lee kristina toutanova bert pre training of deep bi directional transformers for language understanding computing research repository arxiv fig agent s confirmation of technician s visit summarizer comparisons table i evaluation metric scores for extractive summarizers from samples of call transcripts copyright verizon all rights reserved information contained herein is provided as is and subject to change without notice all trademarks used herein are property of their respective owners this work has now been submitted to the ieee for possible publication copyright may be transferred without notice after which this version may no longer be accessible harold p edmundson new methods in automatic extracting journal of the acm jacm book advances in artificial intelligence lecture notes in computer science springer berlin heidelberg vol gunes erkan and dragomir r radev lexrank graph based lexical centrality as salience in text summarization j artif intell res jair mohamed abdel fattah and fuji ren automatic text summarization proceedings of world academy of science engineering and technology vol feb deepali k gaikwad and c namrata mahender a review paper on text summarization international journal of advanced research in computer and communication engineering vol issue pp yihong gong and xin liu generic text summarization using relevance measure and latent semantic analysis in proceedings of the annual international acm sigir conference on research and development in information retrieval acm vishal gupta and gurpreet singh lehal a survey of text summarization extractive techniques journal of emerging technologies in web intelligence karel jezek and josef steinberger automatic text summarization vaclav snasel ed znalosti pp isbn fiit stu brarislava ustav informatiky a softveroveho inzinierstva karen sparck jones automatic summarizing the state of the art information processing management ani nenkova and kathleen mckeown a survey of text summarization techniques in mining text data springer romain paulus caiming xiong and richard socher a deep reinforced model for abstractive summarization corr dragomir r radev eduard hovy and kathleen mckeown introduction to the special issue on summarization computational linguistics lawrence h reeve hyoil han saya v nagori jonathan c yang tamara a schwimmer ari d brooks concept frequency distribution in biomedical text summarization acm conference on information and knowledge management cikm arlington va usa david e rumelhart geoffrey e hintont and ronald j williams representations by back propagating errors nature learning horacio saggion and thierry poibeau automatic text summarization past present and future in multi source multilingual information extraction and summarization springer c s saranyamol l sindhu a survey on automatic text international journal of computer science and summarization information technologies vol issue atif khan and naomie salim a review on abstractive summarization methods journal of theoretical and applied information technology vol no abigail see peter j liu christopher d manning get to the point summarization with pointer generator networks computing research repository arxiv mark steyvers and tom griffiths probabilistic topic models handbook of latent semantic analysis ladda suanmali naomie salim mohammed salem binwahlan fuzzy logic based method for improved text summarization computing research repository arxiv ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan n gomez lukasz kaiser illia polosukhin attention is all you need computing research repository arxiv sukriti verma and vagisha nidhi extractive summarization using deep learning computing research repository arxiv dingding wang shenghuo zhu tao li and yihong gong multidocument summarization using sentence based topic models in proceedings of the acl ijcnlp conference short papers zeqiu wu rik koncel kedziorski mari ostendorf and hannaneh hajishirzi extracting summary knowledge graphs from long documents computing research repository arxiv jiacheng xu and greg durrett neural extractive summarization with syntactic compression in proceedings of emnlp ijcnlp pp klaus zechner a literature survey on information extraction and text summarization computational linguistics program carnegie mellon university april ming zhong pengfei liu yiran chen danqing wang xipeng qiu xuanjing huang extractive summarization as text matching in proceedings of annual meeting of acl pp bert extractive summarizer khosrow kaikhah automatic text summarization with neural networks in proceedings of second international conference on intelligent systems ieee texas usa j kupiec j pedersen and f chen a trainable document summarizer in proceedings of the acmsigir conference pages pirmin lemberger deep for automatic summarization computing research repository arxiv chin yew lin rouge a package for automatic evaluation of summaries in text summarization branches out proceedings of the workshop learning models hui lin and vincent ng abstractive summarization a survey of the state of the art in the proceedings of the aaai conference on artificial intelligence yang liu fine tune bert for extractive summarization computing research repository arxiv yang liu mirella lapata text summarization with pre trained encoders in proceedings of emnlp ijcnlp pp hans peter luhn the automatic creation of literature abstracts ibm journal of research and development inderjeet mani automatic summarization natural language processing john benjamins publishing company derek miller leveraging bert for text summarization on lectures computing research repository arxiv e mittendorf and p schauble document and passage retrieval based on hidden markov models in proceedings of the acm sigir conference pages ramesh nallapati feifei zhai and bowen zhou summarunner a recurrent neural network based sequence model for extractive summarization of documents in proceedings of the aaai conference on artificial intelligence san francisco california usa pages ramesh nallapati bowen zhou ccero nogueira dos santos c aglar gulcehre and bing xiang abstractive text summarization using sequence to sequence rnns and beyond in proceedings of the signll conference on computational natural language learning berlin germany pages shashi narayan shay b cohen mirella lapata ranking sentences for extractive summarization with reinforcement learning in proceedings of naacl hlt pp joel larocca neto alex a freitas and celso a a kaestner automatic text summarization using a machine learning approach copyright verizon all rights reserved information contained herein is provided as is and subject to change without notice all trademarks used herein are property of their respective owners this work has now been submitted to the ieee for possible publication copyright may be transferred without notice after which this version may no longer be accessible
