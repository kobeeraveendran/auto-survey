r l c s c v v x r reader aware multi document summarization sparse coding piji li lidong bing wai lam hang li yi liao department systems engineering engineering management chinese university hong kong hong kong machine learning department carnegie mellon university pittsburgh pa usa noah s ark lab huawei technologies hong kong wlam cuhk edu hk cmu edu hangli com abstract propose new mds paradigm called aware multi document summarization ra mds specically set reader comments associated news reports collected erated summaries reports event salient according reports reader comments tackle mds problem propose sparse coding based method able calculate salience text units jointly considering news reports reader comments reader aware teristic framework improve linguistic quality entity rewriting rewriting eration jointly assessed marization requirements unied tion model support generation sive summaries optimization explore ner syntactic unit noun verb phrase work generate data set conducting ra mds extensive experiments data set classical data sets demonstrate tiveness proposed approach introduction typical multi document summarization mds ting input set documents reports topic event reports event normally cover aspects continuous follow reports bring information lenging generate short salient summary event mds drawn attention method proposed example wan et al posed extraction based approach employs fold ranking method calculate salience tence filatova hatzivassiloglou modeled mds task instance maximum coverage set lem gillick favre developed exact solution work described paper substantially supported grants research development grant huawei nologies co ltd research grant council hong kong special administrative region china project codes model similar filatova hatzivassiloglou based weighted sum concepts approximated bigrams li et al proposed guided sentence pression framework generate compressive summaries training conditional random eld crf based notated corpus li et al considered linguistic ity framework ng et al exploited timelines enhance mds works liu et al kageback et al denil et al cao et al utilized deep learning techniques tackle summarization tasks user generated content available natural extension setting incorporate content event directly indirectly improve generated summaries greater user satisfaction paper investigate new setting direction ically set reader comments associated news ports collected generated summaries reports event salient according reports reader comments paradigm extension reader aware multi document marization ra mds real example taken data set collected illustrate importance ra mds hot event malaysia airlines jet disappeared outbreak event lots reports posted ferent news media existing summarization systems create summaries general information e flight carrying passengers crew members ished early saturday departing kuala lumpur jing fact extract information solely report content analyzing reader ments nd readers interested cic aspects military radar indicated plane turned ight route losing contact passengers appear stolen pean passports board ra mds setting jointly consider news comments generating summary summary content cover important aspects event aspects attract reader interests reected reader comments previous work investigated incorporate comments mds problem challenge duct salience calculation jointly considering focus news reports reader interests revealed comments model sensitive ability diverse aspects reader comments lenge reader comments noisy grammatically informatively previous works explore effect comments social contexts single document marization blog summarization et al yang et al problem setting mds challenging considered comments event multiple reports spanning time riod resulting diverse noisy comments tackle challenges propose coding based method able calculate salience text units jointly considering news reports reader comments intuitively nature summarization lect small number semantic units reconstruct inal semantic space topic ra mds setting semantic space incorporates news reader comments selected semantic units sparse hold semantic diversity property issue nd sparse diverse semantic units ciently supervised training data sparse coding suitable method learning sets complete bases represent data efciently demonstrated useful computer vision mairal et al sparse coding jointly consider news comments select semantic units simple elegant way adding comments reconstruction error item original loss function currently works employing sparse coding summarization task dsdr et al represents sentence non negative linear combination summary sentences method consider sparsity mds sparse liu et al proposed level sparse representation model ing coverage sparsity diversity results signicant improvement paper propose efcient direct sparse model tackle lems achieve encouraging results different data sets reader aware characteristic framework improve linguistic quality entity rewriting maries contain phrases understandable context sentences compiled different uments contain little repeated mation referent human summary writer uses form mention e president barack obama entity time uses short form mention e obama places analogously lar entity framework requires form tion entity appear time mary appearances use cise form early works perform rewriting greedy selection individual sentence nenkova works perform summary rewriting processing step siddharthan et al contrast works rewriting consideration framework jointly assessed summarization ments unied optimization model brings advantages assessment rewriting tion jointly considered generation news sentences news reconstruction comment sentences nps vps uo summary generation compression comments partial reconstruction figure ra mds framework sive summary global view generate better rewriting results second use length limit effect rewriting operation summary length simultaneously considered constraints model support generation compressive maries optimization explore ner syntactic unit noun verb phrase precisely rst decompose sentences noun verb phrases salience phrase calculated jointly considering importance reports comments work generate data set conducting ra mds extensive experiments data set benchmark data sets conducted examine cacy framework description proposed framework overview tackle ra mds problem propose vised compressive summarization framework overview framework depicted fig sparse coding based method proposed reconstruct semantic space topic revealed news sentences e xi s comment sentences e zi s news sentences expressiveness score ai designed news sentence dashed boxes comment sentences indicate cial treatment applied comments avoid noise reconstruction details introduced section compression carried deleting unimportant constituents e phrases input sentence rst decompose sentence noun phrases nps verb phrases vps salience phrase depends ria expressiveness score inherited tence concept score phrase extraction phrases calculation phrase salience troduced section framework carries mention rewriting entities improve linguistic quality summary specically rewrite mentions types named entities person location organization discuss details mention detection mention cluster merging short form form mention nding section preparation steps troduce summarization model section model simultaneously performs sentence compression mention rewriting unied optimization method variety summarization requirements considered mulating constraints reader aware sentence expressiveness intuitively nature summarization select semantic units reconstruct original semantic space topic expressiveness score sentence news dened contribution constructing semantic space topic news content reader comments expressiveness conveys attention sentence attracts news ers readers propose sparse coding model compute expressiveness scores typical sparse coding aim nd set basis vectors reconstruct m target input vectors xi linear combination minimize following loss function m min kxi aijj s sparsity cost function penalizes ai far zero summarization task topic contains set news reports set reader comments ming stop word removal build dictionary topic unigrams bigrams news sentence news comments represented weighted term frequency vector let x xm z denote vectors sentences news comments respectively rd zi rd term frequency vectors d terms dictionary m sentences news n sentences ments topic semantic units sentences assume sentence xi cient variable ai named expressiveness score represent contribution sentence reconstruction based spirit sparse coding directly regard news sentence xi candidate basis vector xi s employed reconstruct semantic space topic including x z propose preliminary error formulation expressed eq aim minimizing m m m n m kxi kzi coefcient aj s expressiveness scores target vectors share coefcient vector harness characteristics summarization lem setting effectively rene preliminary ror formulation given eq directions mentioned original sentence vector space constructed subset e number summary sentences sparse sparsity straint coefcient vector norm eq weight scaling constant determine relative importance consider negative linear reconstruction framework add non negative constraints coefcients previous work ng et al mentioned prior knowledge benet sentence expressiveness detection performance e sentence position add variable weight news sentence reconstruction error employ position information generate cp cp p paragraph id document starting c positive constant smaller sides useful information comments usually introduce lots noise data tackle problem rst step eliminate terms appear comments step add parameter control comment sentence struction error fact semantic units erated summaries news intuitively sentence introduce information similar news employ mean cosine similarity comment sentence zi news sentences x weight variable considerations global loss function follows ikxi ajxj k j min m m x n x m x m x ikzi ajxjk s t aj j m optimization problem sparse coding ready classical algorithms mairal et al paper utilize coordinate descent method shown gorithm iterative updating rule eq objective function j non increasing convergence iteration guaranteed sparse coding model introduces advantages sparse coding class unsupervised methods manual annotations training data needed second optimization procedure modular leading easily plug different loss functions model incorporates mantic diversity naturally mentioned et al helps subsequent unied timization component generates compressive maries particular reduces number variables cause sparsity constraint generate sparse ness scores e sentences score phrase extraction salience calculation employ stanford parser klein manning tain constituency tree input sentence extract nps vps tree follows nps vps direct children s node tracted vps nps path nodes vps nps recursively extracted regarded ing parent node s recursive operation second step carried levels phrases lower levels able convey complete fact algorithm coordinate descent algorithm sentence pressiveness detection input news sentences x rdm comments sentences z rdn news reconstruction weight comments reconstruction weight penalty parameter ping criterion t root s vp vp np dt jj nn vp cc vp armed man vbd pp vbd np advp vp cc vp walked np sent dt nns rb vbn prt vpn np dt nnp nn boys outside tied rp shot dt nns amish school girls vbg np s vp killing np pp cd np prp partial derivatives reconstruction error items figure constituency tree sentence output salience vector rm initialize t t t j t reconstructing x j m m n m n j ak x txk x txk select coordinate maximum partial derivative k arg max m update donoho johnstone coordinate j ak soft thresholding k j ak s ai j t jat t t end return tree fig example corresponding tence decomposed phrases armed man walked amish school sent boys outside tied shot girls killing walked amish school sent boys outside tied shot girls killing salience phrase depends criteria rst criterion expressiveness score inherited corresponding sentence output sparse ing model second criterion concept score conveys overall importance individual concepts phrase let tf t frequency term t igram bigram topic salience si recursive operation extracted phrases overlaps later avoid overlapping phrase extraction consider recursive operation vp parallel sub vps highest vp fig sub vps following modal link auxiliary verbs extracted individual vps addition extract clauses functioning subjects sentences nps clause note mention clauses noun phrase labels tree sbar s phrase pi dened tf t ai tf t si ptpi opic ai expressiveness sentence containing pi resolution preparation entity mentions rewriting rst conduct co reference resolution ument stanford co reference age lee et al adopt resolution rules able achieve high quality address need summarization particular sieve package employed set clusters obtained cluster contains mentions corresponding entity document clusters different documents topic merged matching named entities types entities considered person location organization let m denote mention cluster entity form mention determined arg max tf mm xtm tf calculated m simply select longest verbose short form mention ms determined ms arg max mm xtm tf m contains mentions shortest pronouns unied optimization framework objective function optimization formulation ned isi x j ij si sj rij selection indicator phrase pi si salience scores pi ij rij co occurrence indicator similarity pair phrases pi pj respectively similarity calculated jaccard index based method specically objective maximizes salience score selected phrases indicated rst term nalizes selection similar phrase pairs constraints govern selected phrases able form compressive sentences constraints entity rewriting given note rewriting consideration conducted different candidates purpose assessment effects summarization optimization framework consequently actual permanent rewriting operations conducted optimization process actual ing operations carried selected phrases optimization component post processing stage compressive sentence generation let k note selection indicator sentence xk phrase xk selected k k generating compressed summary sentence required np lease vp sentence selected expressed pi xk pi n p k k xi xi pi xk pi v p k entity rewriting let pm phrases contain entity corresponding cluster m pi pm indicators f indicates entity pi rewritten form s indicates entity pi rewritten short form adopt rewriting strategy design following constraints dened f s pi pm f j xpj pm s pi pm f note phrase contains mentions entity safely rewrite appearances short form mention need decide rewriting strategy rst appearance phrases path constituency tree selected time pk pj k j example walked amish school sent boys outside tied shot girls killing walked amish school selected phrase co occurrence constraints control co occurrence relation phrases ij ij j j ij rst constraints state summary includes units pi pj include vidually constraint inverse rst short sentence avoidance select vps sentences shorter threshold short sentence normally convey complete key fact pronoun avoidance observed woodsend lapata normally human summary writers exclude nps pronouns selected previously pronouns length constraint overall length selected nps vps larger limit l note length calculation considers effect rewriting operations rewriting indicators objective function constraints linear optimization solved existing integer linear programming ilp solvers simplex rithm dantzig thapa implementation use package called lp postprocessing timestamp summary sentence dened tamp corresponding document sentences dered based pseudo timestamps sentences document ordered according original der finally conduct appropriate entity rewriting indicated optimization output experiments experimental setting data set data set contains topics topic contains related news reports reader ments topic employ summary writers nalist background write model summaries writing summaries account interest readers digesting reader comments event model maries written topic separate development tuning set containing topics topic model summary duc order sparse coding based work work traditional mds task employ benchmark data sets duc duc uation duc duc contain topics respectively topic news documents model summaries length model summary limited words evaluation metric use rouge score ation metric lin f measures rouge reported parameter settings set c p position weight function sparse coding model set stopping criteria t learning rate sparsity item penalty set results data set compare system summarization baselines random baseline selects sentences randomly topic lead baseline wasson ranks news cally extracts leading sentences mead radev et al generates summaries cluster troids produced topic detection tracking system shown table system reports best results rouge metrics reasons follows sparse coding model directly assigns coefcient values sourceforge berouge com summarization com system random lead mead rouge table results data set expressiveness scores news sentences tained minimizing global semantic space tion error able precisely represent importance sentences model jointly consider news tent reader comments taking account aware information sparse coding model weight reconstruction error prior knowledge e paragraph position improve summarization performance signicantly unied optimization framework ther lter unimportant nps vps generate pressed summaries conduct entity rewriting unied optimization framework order improve guistic quality results duc order illustrate performance framework traditional mds task compare state art systems standard data set duc framework mds task reader comments ignoring components comments random lead methods compare system unsupervised sparse coding based methods dsdr et al mds sparse liu et al mds mds sparse div data set evaluation metrics standard directly retrieve results papers results given tables system signicantly form comparison methods reasons mentioned section rouge system random lead dsdr non mds mds sparse div system random lead dsdr non mds mds sparse div table results duc table results duc rouge case study based news comments topic bitcoin change mt gox goes ofine generate summaries model considering comments ing comments noc respectively summaries rouge evaluation given table rouge values model considering comments better ignoring comments large gaps sentences italic bold summaries different ing comments topic nd comments talking company lost bitcoins anonymity prevents reversal transactions identied model rouge system noc mt gox went ofine today trading tokyo based site came screeching halt withdrawal ban imposed exchange earlier month deposits insured ernment sudden closure mt gox bitcoin exchange sent virtual currency month low monday currency s value fallen past hours statement bitcoin companies day night signed mr silbert committed future bitcoin security customer funds mt gox went ofine today trading tokyo based site came screeching halt company lost coins theft gone unnoticed years sudden closure mt gox bitcoin exchange sent virtual rency month low monday currency s value fallen past hours anonymity prevents reversal transactions statement coin companies monday night signed mr silbert committed future bitcoin security customer funds table generated summaries topic bitcoin change mt gox goes ofine present entity rewriting case study son dong nguyen topic flappy bird summary entity rewriting contains different tion forms dong nguyen dong nguyen rewriting dong replaced nguyen makes co reference mentions clearer expected form mention nguyen ha dong hanoi based game developer shuhei yoshida president sony computer entertainment worldwide studios australian maritime safety authority s rescue nation centre overseeing rescue mary conclusion propose new mds paradigm called reader aware document summarization ra mds tackle mds problem propose sparse coding based method jointly considering news reports reader comments propose compression based unied optimization work explores ner syntactic unit noun verb phrase generate compressive summaries conducts entity rewriting aiming better linguistic quality work generate data set ra mds task experimental results framework achieve good performance outperform state art vised systems references cao et al ziqiang cao furu wei li dong sujian li ming zhou ranking recursive neural works application multi document tion aaai dantzig thapa george mukund n thapa duction springer verlag new york inc dantzig b linear programming denil et al misha denil alban demiraj nal brenner phil blunsom nando de freitas elling visualising summarising documents arxiv preprint single convolutional neural network donoho johnstone david l donoho jain m johnstone ideal spatial adaptation wavelet shrinkage biometrika filatova hatzivassiloglou elena filatova vasileios hatzivassiloglou formal model mation selection multi sentence text extraction coling gillick favre dan gillick benoit favre scalable global model summarization workshop ilp nlp pages et al zhanying chun chen jiajun bu wang lijun zhang deng cai xiaofei document summarization based data reconstruction aaai et al meishan hu aixin sun ee peng lim comments oriented document summarization standing documents readers feedback sigir pages kageback et al mikael kageback olof mogren nina tahmasebi devdatt dubhashi extractive marization continuous vector space models pages klein manning dan klein christopher d manning accurate unlexicalized parsing acl pages lee et al heeyoung lee angel chang yves man nathanael chambers mihai surdeanu dan rafsky deterministic coreference resolution based entity centric precision ranked rules comput linguist et al chen li fei liu fuliang weng yang liu document summarization guided sentence pression emnlp pages et al chen li yang liu fei liu lin zhao fuliang weng improving multi documents tion sentence compression based expanded stituent parse trees emnlp pages lin chin yew lin rouge package automatic evaluation summaries text summarization branches proceedings workshop pages liu et al yan liu sheng hua zhong wenjie li query oriented multi document summarization pervised deep learning aaai liu et al liu hongliang yu zhi hong deng multi document summarization based level sparse representation model aaai et al julien mairal francis bach jean ponce sparse modeling image vision processing foundations trends computer graphics sion nenkova ani nenkova entity driven rewrite multi document summarization international joint conference natural language processing nlp pages et al jun ping ng praveen bysani ziheng lin min yen kan chew lim tan swing exploiting category specic information guided summarization proceedings tac et al jun ping ng yan chen min yen kan zhoujun li exploiting timelines enhance document summarization acl pages radev et al dragomir radev timothy allison sasha blair goldensohn john blitzer arda celebi stanko dimitrov elliott drabek ali hakim wai lam danyu liu al mead platform multidocument tilingual text summarization siddharthan et al advaith ani information nenkova kathleen mckeown tus distinctions referring expressions empirical study references people news summaries comput linguist siddharthan wan et al xiaojun wan jianwu yang guo xiao manifold ranking based topic focused document summarization ijcai pages wasson mark wasson leading text news summaries evaluation results implications mercial summarization applications acl pages association computational linguistics woodsend lapata kristian woodsend mirella lapata multiple aspect summarization integer linear programming emnlp conll pages yang et al zi yang keke cai jie tang li zhang zhong su juanzi li social context summarization sigir pages
