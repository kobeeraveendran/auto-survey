attend beginning study bidirectional attention extractive summarization ahmed cezary university pittsburgh pitssburgh usa microsoft research fuse lab ccp bellevue usa edu com abstract forum discussion data differ structure ties generic form textual data news forth summarization techniques turn use differences craft models benet structural nature discussion data work propose attending beginning document improve performance extractive summarization models plied forum discussion data evaluations demonstrated help bidirectional attention mechanism ing beginning document initial comment post discussion thread introduce consistent boost rouge scores introducing new state art sota rouge scores forum discussions dataset additionally explored hypothesis able generic forms textual data use tendency introducing important information early text attending rst sentences generic tual data evaluations demonstrated attending ductory sentences bidirectional attention improves performance extractive summarization models applied generic form textual data introduction recently automatic text summarization models tractive abstractive witnessed fast performance strides emergence deep learning models specially models large number recent neural abstractive rization models employ encoder decoder structure liu manning gehrmann deng rush paulus xiong socher convert input quence relatively shorter sequence recent extractive models hand employ coder convert input sequence xed feature vector followed classication nallapati zhai zhou liu lapata text summarization applied different natural language domains news academic papers emails meeting notes forum discussions models transferable main benecial craft work internship copyright association advancement articial intelligence www aaai org rights reserved ditional modications models account ences domains forum discussion data tarnpradab liu hua example different ture properties compared generic textual data news discussion threads usually start initial post comment seeking knowledge help following comments tend target initial post comment providing additional information opinions said question arises enhance existing summarization models benet properties inspired seo propose integrating rectional attention mechanism extractive summarization models help attend early pieces text initial ment main objective benet dependency initial comment following comments try distinguish important irrelevant supercial replies recent research jung showed domains humans tend duce relatively important information early beginning textual articles unlike discussion threads explore benet attending beginning generic textual setting simply integrating bidirectional attention mechanism attending rst sentences ument conducted experiments evaluate pothesis dataset generic non discussion based documents contributions work fold introduce integrating bidirectional attention mechanism extractive summarization models help attend earlier pieces text second achieved new sota forum discussion dataset proposed attending beginning mechanism ify transferability hypothesis attending beginning perform evaluations attending earlier sentence generic text benet summarization models different domains cussions related work automatic text summarization seen increasing est improved performance emergence models sutskever vinyals tention mechanisms bahdanau cho bengio true automatically generating coherent mary abstractive summarization extracting salient pieces text extractive summarization majority recent research directed news domain liu manning paulus xiong socher existence huge annotated datasets cnn dailmail gigawords newyork times unlike news domains emails discussions meeting notes students feedback opinions considered derexplored recent efforts tackle domains started emerge luo liu litman targeted student feedback marization extracting set representative phrases proposed abstractive summarization meeting notes employing textual visual tion multi model setting zong yang tackled problem opinion review summarization work targeting similar domain tarnpradab liu hua proposed hierarchical attention perform extractive summarization dataset forum discussions collected trip advisor work shares similar sign concept wang quan wang integrated bidirectional attention nism model model ferent intuition application major motive integrate bidirectional attention design attend external template summarization process propose attending early pieces input text bidirectional attention mechanism major difference intended application developed model task abstractive summarization news data intended extractive tion task dataset work employ extractive summarization datasets discussion dataset proposed tarnpradab liu hua discussion dataset extracted trip advisor forum discussions data consists threads work tarnpradab liu hua threads training idation use data distribution reported authors kept testing data size comparability reasons split verify utility proposed techniques threads training validation testing conducted additional experiments msw msw dataset generic textual dataset verify transferability pothesis generic textual domains verify benet documents structure human dency present important information earlier attending early sentences msw dataset consists collection generic documents different domains split dataset trip advisor msw train val test train val test documents total sentences table model datasets data training validation testing documents respectively table summarizes bution datasets baselines order validate hypothesis utility proposed enhancements implemented baselines following sections provide additional details baselines implemented python shelf package text rization sumy summarization hierarchically comment discussion thread passed rately sumy resultant summaries combined passed sumy nal document thread summary lsa clustering implemented simple baseline extractive rization baseline uses latent semantic analysis lsa embed sentences vector space sentences clustered means clustering algorithm use number clusters number tences input document lastly cluster ter head picked cluster head sentence closest mean point cluster summarunner summarunner auto regressive extractive tion model proposed nallapati zhai zhou siatl siatl sentence classication model developed chronopoulou baziotis potamianos model employs multi task learning integrating language ing auxiliary loss training process siatl model developed originally sentence classication model decided deal extractive summarization pure sentence classication problem use siatl model extractive summarizer unlike summarunner auto regressive model previous decisions model affect future decisions siatl performs classication independently downloaded dropbox com threaddataset dataset publicly available org project paper original model design sentence sive models perform better xiao carenini showed non auto regressive models efcient decided use siatl model baseline compare performance auto regressive non auto regressive models extent study attend beginning work hypothesize attending initial text extractive summarization help selecting salient sentences intuition discussion threads initial text holds important topical information forth renders important factor selecting salient tences summarization objective validate hypothesis calculating importance sentence respect initial text form tention inuenced seo wang quan wang interaction approach employed produce beginning aware sentence representations sentence document sentence resentation produced sentence document sentence beginning ument initial post comment case discussion rmn computed dataset similarity matrix pair document beginning sentences sij hip number sentences beginning number sentences input document concatenation operator sentence representation sentence document sentence representation sentence beginning row column ized softmax produces new matrices bidirectional attention calculated represents document beginning tention represents beginning document attention finally obtain beginning aware sentence tions sentence document underlying mechanism integrate bidirectional tention siatl level granularity attention operates summarunner operates level ment bidirectional attention mechanism calculated level sentences document sentences beginning sentences sentence resentation sentence document sentence representation sentence beginning hand siatl operates level sentence bidirectional attention calculated words input sentence words beginning document word representation word input sentence word representation word beginning available github com amagooda summarunner coattention additional proposed modications bert embedding recently released embeddings showed ability outperform simply shallow word embeddings helped pushing state art numerous tasks nlp community work integrate bert embeddings summarunner model instead initializing word embeddings randomly bert embeddings model embedding layer initialized bert embedding froze ing phase keyword extraction attention mechanisms aim weight tokens differently based importance modication duce work directed feeding model extra signal extra signal case keywords intuition feeding model keywords pushing model attention specic words way integrate keywords summarunner model extracting keywords sentence separately encode keywords hidden states ing bilstm hkwi nkwi nkwi ber extracted keywords sentence den state represent keywords hkw new sentence embedding hdkw formed directly concatenating original document aware sentence sentation keywords representation hdkw hkw experiments verify hypotheses validate utility proposed modications conducted number iments experimental designs address following hypotheses hypothesis attending beginning discussion thread help extractive summarization models select salient sentences hypothesis non auto regressive models siatl suitable thread discussion summarization compared auto regressive models summarunner hypothesis adding additional features contextual embeddings bert keywords summarization models boost performance hypothesis attend beginning transferable different forms text discussion threads lsa kmeans lsa vector spaces vector space trained wikipedia second vector space trained forum discussion dataset scikitlearn python package produce lsa vector spaces dimensions lsa baseline summarunner summarunner model following nallapati zhai zhou operate forum discussion data comments split implemented sentences stanford sentence parser sentences concatenated single document sentence concatenation operator comment number comments operate msw dataset document split sentences stanford sentence parser summarunner randomly initialized embeddings size hidden state size lstm input sentences truncated tokens shorter sentences padded model trained batch size epoch calculate rouge score development set epoch later checkpoint maximum rouge testing siatl implementation siatl released model embeddings size dimensions hidden state size shared lstm task lstm size input tences truncated tokens shorter sentences padded model trained batch size epoch similarly calculate rouge score development set later checkpoint maximum rouge score testing summarunner bidirectional att bidirectional attention mechanism integrated marunner operates level document conduct experiments forum discussion data beginning rst comment initial post thread hand experiments msw dataset beginning rst sentences document work summarunner bert embedding tialize summarunner bert word embeddings bert base uncased embeddings word represented concatenation bert layers leads word representation size tried combining different number word representation found combining layers performs better layer decided use layers reduce number model parameters summarunner keyword extraction extract keywords use rapid automatic keyword extraction rake rose identify keywords sentence document keywords extracted concatenated pair sentence corresponding concatenated keywords passed summarunner separate inputs siatl bidirectional att like summarunner siatl operates level com alexandra chron siatl googleapis com bert uncased zip individual sentences bidirectional attention mechanism integrated operates level words conduct experiments forum discussion data beginning words initial comment thread hand experiments msw dataset beginning words rst sentences document work results forum dataset table presents summarization performance results non neural extractive baselines original proposed variants summarization models marunner siatl nally highest score reported tarnpradab liu hua following tarnpradab liu hua recent work performance evaluated rouge lin summarization model baselines summarunner tarnpradab best sumy lsa kmeans discussions lsa kmeans wikipedia summarunner basic siatl self attention bidir att bert keywords kws bidir att kws bert kws bert bidir att bert bidir att kws bidir att self att bidir att siatl table rouge results italics indicates outperforms baselines boldface indicates best result models underlining indicates best result model group motivation bidirectional attention nism hypothesis table supports esis rouge scores summarunner siatl involves attending beginning tional attention mechanism rows form corresponding counterpart tional attention rows respectively ond hypothesis non auto regressive models suitable auto regressive ones discussion marization table shows non auto regressive model siatl improve rouge scores compared auto regressive model summarunner rows siatl improved scores similarly improved respectively additionally siatl introduced new sota huge improvement rouge scores compared previous work chical attention rows improved improved nally proved benets ing beginning siatl model compared self attention mechanism original model ing bidirectional attention combining attention mechanisms self bidirectional boost rouge scores rows hypothesis enriching models additional features contextual embeddings words boost performance experiments summarunner model room improvement catch siatl model table shows hypothesis valid cases shows adding contextual embeddings adding keywords helps model bining contextual embeddings keywords tends harm model adding keywords variants summarunner original bidirectional tion introduces slight improvement rouge scores rows improved respectively improved proved larly adding bert contextual embedding introduces good improvement rouge scores variants marunner rows improved respectively improved improved surprisingly adding features bert keywords tends harmful model rows analysis needed reach solid conclusion behavior results msw dataset table presents summarization performance results original proposed variants summarunner best performing variant siatl motivation conducting experiments msw dataset validate hypothesis table clearly shows hypothesis valid shows tending beginning document helps selecting salient sentences discussion threads generic textual documents similar results cussions dataset attending beginning bidirectional attention mechanism boosts rouge scores rows additionally ing bidirectional attention bert embeddings improves performance rows discussion analysis unlike promising performance discussions table siatl performed poorly msw dataset ble surprisingly able outperform baseline analyzing different criteria generated output summarunner siatl model summarunner bidir att bert bidir att bert siatl self att bidir att table rouge results msw dataset datasets observed siatl tends erate longer summaries compared summarunner likely non auto regressive nature marunner hand tends generate shorter summaries table shows average standard viation number sentences generated marunner siatl model compared human notation shows forum discussion dataset expected summary length sentences dataset siatl model generates summaries length sentences summarunner generates summaries length sentences justify superior mance siatl compared summarunner rum discussion dataset hand notice expected summary length msw dataset sentences dataset summarunner tently generates shorter summaries compared siatl lengths compared respectively clear huge difference length summary human siatl generated reason siatl performs msw dataset potential solution siatl model adding nal post processing step clustering redundancy reduction rule post processing step slightly ltering generated summary help pick number sentences close average number humans select model human summarunner siatl forum discussions avg std msw avg std table average standard deviation number sentences generated model human lected sentences conclusion future work explored improving performance neural extractive summarizers applied discussion threads ing beginning text initial comment post bidirectional attention mechanism showed attending beginning text improved rouge scores different models different variants models showed applicability recent sentence classication model siatl extractive marization introduced new sota rouge score trip advisor forum discussion dataset additionally luo liu litman improved phrase based approach annotating summarizing proc coling dent course responses international conference computational tics technical papers nallapati zhai zhou summarunner recurrent neural network based sequence model tractive summarization documents thirty aaai conference articial intelligence paulus xiong socher deep inforced model abstractive summarization tional conference learning representations rose engel cramer cowley tomatic keyword extraction individual documents text mining applications theory liu manning point summarization pointer generator networks proc annual meeting acl volume long papers volume seo kembhavi farhadi hajishirzi bidirectional attention machine comprehension proc international conference learning sentations iclr sutskever vinyals sequence sequence learning neural networks advances neural information processing systems tarnpradab liu hua tractive summarization online forum discussions archical attention networks thirtieth international flairs conference wang quan wang biset directional selective encoding template abstractive summarization arxiv preprint xiao carenini extractive summarization long documents combining global local context arxiv preprint yang shen liu zhao zhu aspect sentiment aware abstractive review summarization proc international conference computational linguistics showed attending beginning text ited datasets form discussion threads showed transferable generic forms text attend rst sentences text similar attending initial post comment discussion threads lastly showed utility attending ning constant regardless model dataset integrating bidirectional attention introduces provement rouge scores future plans include trying generic datasets news verify ity attending beginning experimenting siatl model datasets showed promising results extractive summarizer plan try extending siatl model post processing step enforce control output length plan try different values number sentences tial generic documents references bahdanau cho bengio neural chine translation jointly learning align translate arxiv preprint chronopoulou baziotis potamianos embarrassingly simple approach transfer learning pretrained language models proc ference naacl hlt volume long short papers devlin chang lee toutanova bert pre training deep bidirectional transformers guage understanding arxiv preprint gehrmann deng rush abstractive summarization proc conference emnlp jha pakdaman celikyilmaz bodev mcdonald artemis novel tation methodology indicative single document rization arxiv preprint jung kang mentch hovy earlier better sub aspect analysis corpus system biases summarization proc conference emnlp ijcnlp zhang radke ing summaries topic abstractive multi modal meeting summarization proc annual meeting acl florence italy acl zong personalized review summarization user aware sequence network proc aaai conference articial intelligence ume lin rouge package automatic evaluation summaries text summarization branches liu lapata text summarization trained encoders proc conference ical methods natural language processing international joint conference natural language cessing emnlp ijcnlp
