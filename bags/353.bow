improving zero shot abstractive summarization intermediate fine tuning data augmentation alexander r fabbri haoran li marjan ghazvininejad simeng han haoyuan li shaq joty dragomir radev yashar mehdad yale university facebook ai nanyang technological university renmin university china alexander fabbri dragomir edu edu sg edu cn aimeeli ghazvini com abstract models pretrained self supervised tives large text corpora achieve state art performance text summarization tasks models typically ne tuned hundreds thousands data points infeasible requirement applying rization new niche domains work introduce general method called itransfer ne tuning pretrained models summarization unsupervised specic manner makes use istics target dataset length abstractiveness desired summaries achieve state art zero shot tive summarization performance dailymail dataset demonstrate tiveness approach additional diverse datasets models ne tuned unsupervised manner robust noisy data achieve better shot performance training ples perform ablation studies effect components unsupervised tuning data analyze performance models shot scenarios data augmentation techniques matic human evaluation introduction automatic text summarization aims distill salient content given text compact form recent advances summarization driven availability large scale datasets cnn dailymail cnndm corpus nallapati et al new york times corpus sandhaus duction large pretrained models bart lewis et al pegasus zhang et al cases resulting summaries favored human written reference summaries fine tuning models ically requires large corpus labeled summaries creating data domain infeasible highly costly ability transfer large trained models new domains small data necessary especially models way production ments unsupervised summarization approaches clude autoencoders mirror information pression inherent summarization baziotis et al chu liu brazinskas et al large scale pretraining specic adaptation yang et al little work focused domain adaptation summarization wang et al examine main adaptation extractive summarization hua wang showed summarization els difculty generating text style target domain recently zhang et al report strong performance trained models trained shot settings brazinskas et al ne tune specic components model shot ing aim build recent work trained models improve unsupervised shot summarization encoding characteristics target summarization dataset unsupervised intermediate ne tuning data view summarization seen function sub functions called aspects determine output form jung et al dene subaspects rization position importance diversity study subaspects manifest summarization corpora model outputs example common subaspect cnndm dataset position earlier sentences tend good summary inspired view rization subaspects aim encode subaspects target dataset unlabeled data allow model ne tuned data learn t c o l c s c v v x r tics target dataset improve zero shot shot transfer model work focus subaspects extractive diversity determined extractive model forms data compression ratio source document summary case cnndm lead bias assume knowledge target dataset size input ments size desired summaries extent summary abstractive prior knowledge task dened encode knowledge wikipedia article data extracting summaries desired output length ltering examples based desired level abstraction contributions following troduce method called wikitransfer create pseudo summaries subaspects target dataset unlabeled data termediate ne tuning method improves zero shot domain transfer transfer domains achieving state art supervised abstractive summarization performance cnndm dataset generalizing domains perform extensive ablation studies factors inuencing zero shot performance demonstrate additional improvements ferring wikitransfer models shot ting analyze differences performance data augmentation techniques datasets different level extractiveness related work advances neural niques summarization large datasets work focused domain tion methods summarization zero shot settings wang et al examine domain adaptation extractive summarization hua wang examine domain adaptation opinion news summarization ing models trained domain applied domain capture relevant content differ style generating summary brazinskas et al introduce plug works small netune able layers added larger model aims reproduce characteristics target dataset seen small set labeled amples contrast aim encode teristics target dataset ness compression priori intermediate training phase better adaptation work lebanoff et al adapt single document summarization model multi document ting zhu et al use references wikipedia data downstream query based summarization similar task wikipedia paragraph generation dened liu et al approaches unsupervised rization use variational coders baziotis et al chu liu brazinskas et al zhou rush makes use pretrained language models pervised text summarization aligning erage generated summary source ument laban et al train unsupervised summarization model guiding model reinforcement learning rewards line work extractive models textrank cea tarau lexrank erkan radev recently pacsum zheng ata aim use graph centrality order extract important sentences document power pretrained models shot transfer shown zhang et al work focuses zero shot case transferability models ne tuned given datasets multiple datasets transferability single pre trained model closest work zero shot transfer yang et al makes use bias news articles pretrain unsupervised model large dataset news articles proach focuses ne tuning pretrained model specic task rization downstream dataset shows generalizability ne tuning domains bart lewis et al pretrained denoising autoencoder achieved state art mance ne tuned summarization tasks time work use bart base pretrained model future work ment intermediate ne tuning shot transfer pretrained models methods section introduce methods prove zero shot abstractive summarization let xt xn source ument n words n sentences xi represents th word represented st sn st resents t th sentence corresponding target summary y contains m words m tences yt denotes t token y standard training minimizes negative likelihood loss supervised teacher forcing williams zipser label lsup y m represents distribution vocabulary predicted model eter ignored following equations simplicity intermediate fine tuning propose method ne tuning pretrained models unsupervised wikipedia data create data intermediate ne tuning assume knowledge characteristics target dataset average length input ments average summary length general bin summaries desired stractive extractive specications necessary priori summarization problem underconstrained kryscinski et al assume want summary m tences source documents n sentences average assume know approximately oracle extractive model performs dataset dened bins extractive oracle rouge scores ranging extremely abstractive rouge oracle abstractive rouge oracle extractive rouge oracle extremely extractive rouge oracle iterate following procedure wikipedia articles available wikipedia dump remove rst m sentences wikipedia article use summary select m sentences remaining article highest individual rouge scores pseudo summary calculate rouge score m sentences joint pseudo summary amounts greedy upper bound performance extractive model example example kept rouge score falls general range extractive oracle target dataset dened previously discarded use knowledge abstractive dataset type summary style end user know ahead time lter data points wikipedia fall bin given dataset ne tuning datasets extremely abstractive examples hard nd remove high rouge sentences input desired rouge oracle score reached refer data created process wikitransfer ne tune trained model dataset specic wikitransfer data transfer target domain data augmentation round trip translation addition ne tuning wikitransfer data zero shot domain transfer test ability model transfer examples data augmentation improves results shot ne tuning conduct data augmentation reduce brute force memorization introduce regularization effect specically perform round trip translation yu et al generate paraphrases source ments summaries given dataset size n translate source target sentence wise non english language k beam hypotheses beam search output likewise backtranslation english results n n total data points data augmentation consistency data augmentation introduce ization effect naively training augmented data necessarily account noise introduced augmented examples balance learning examples overtting small number supervised samples model learn robust small changes input amples investigate effect consistency loss shot training building ideas unsupervised data augmentation uda xie et al formulation output distribution given augmented example diverge distribution given original document teacher forcing model learns resilient small tions let x paraphrase input document generated round trip translation described previous section addition vised loss y introduce loss x y m x kl kl divergence penalizes loss probability distribution output original input far distribution round trip translated input document xie et al gradient agate model distribution original input propagate round trip translated input result total loss training consistency x y y y note original formulation uda forces consistency semi supervised framework experimented setup beled examples target dataset pseudo labels teacher forcing generated model trained associated shot subset approach sensitive quality pseudo labels appendix experimental settings section describe experimental tings data usage intermediate netuning zero shot shot domain transfer datasets experiment datasets ndm xsum narayan et al reddit tifu reddit kim et al bigpatent sharma et al datasets chosen differ abstractiveness output length ing sentence xsum average sentences bigpatent cover multiple mains news cnndm xsum social media reddit patent documents bigpatent generalizability results model selection metric ments follow rst choose model best zero shot performance given domain test zero shot performance domains domain els wikitransfer subset choose best model based performance vised validation subset found ne tuning model longer result performance gains shot transfer checkpoints sen typically ne tuned epochs results ablation studies fer subset shown validation set given target dataset stated results reported rouge l run shot transfer ments ve subsets supervised data found results vary run run reported numbers zero shot average results ve runs data point sets subsets data point sets data augmentation parameters data mentation round trip translation use beam size german russian translation models fairseq provides bidirectional pretrained translation models edunov et al ng et al language pairs data points use k resulting total data points label model ne tuned settings aug aug consistency loss use augmented data model hyperparameters use fairseq codebase ott et al experiments base abstractive text summarization model bart lewis et al pretrained denoising coder builds sequence sequence transformer vaswani et al tune bart polynomial decay learning rate scheduler adam optimizer kingma ba mainly vary learning rate uler warm updates total updates previous shot summarization work zhang et al work unsupervised machine translation lample conneau use validation set early stopping based validation loss following learning rates warmup updates total parameters based examination validation curves initial experiments aug aug consistency loss experiments use value experiments data points experiments data points appendix additional training details zero shot transfer results section compare transferring bart model ne tuned wikitransfer data transferred summarization datasets terms zero shot performance ablations different choices wikitransfer tuning data cnndm xsum datasets datasets falls different extractive bin ranging extractive cnndm dataset abstractive xsum discuss settings appendix zero shot transfer comparison ne tune bart wikitransfer data datasets described tune model fully supervised datasets compare zero shot performance transferring wikitransfer best zero shot fer performance dataset current state art fully supervised results table zero shot transfer wikitransfer data outperforms transfer datasets terms mented training model data combined multiple datasets leave dataset train test zero shot setting setting proved results bigpatent transfer zero shot transfer increases lower wikitransfer model experiments follow use best performing single domain transfer model note difference performance pervised bigpatent results bart likely differences capacity training batch size compared current state art gasus large model zhang et al result bigpatent comparable pegasus base model zhang et al additionally table compare shot performance model state art unsupervised abstractive model cnndm dataset seen comparison pervised summarization literature datasets study outperform recently introduced ted model yang et al cally motivated news domain showing generalizability approach ablation studies wikitransfer data conduct ablation studies determine fect characteristics intermediate tuning data downstream zero shot mance perform ablation studies ndm xsum effect ends extractive abstractive dataset spectrum target dataset sota dataset wikitransfer cnndm xsum reddit bigpatent transfer best reddit reddit cnndm cnndm table comparison zero shot fer performance dataset specic wikitransfer vs transfer dataset best performing zero shot model shown right column s state art supervised formance dataset parentheses mance bart model trained dataset model wikitransfer ted yang et al l table comparison approach pervised pretraining yang et al showing superior performance generalizability proach versus ted model focused news domain ablation bin bin bin cnndm xsum table ablation studies effect learning rate use extractive bin data ltering choice m intermediate ne tuning rouge mance cnndm xsum validation sets effect learning rate intermediate tuning examine extent overtting unsupervised wikitransfer data occurs examining effect learning rate diate ne tuning zero shot transfer performance netune models cnndm xsum wikitransfer data respectively mum learning rate results shown table smaller learning rate intermediate ne tuning improves results ndm xsum likely simple extractive lead bias objective ily overt ne tuning opposed abstractive object xsum wikitransfer data similar trend effect dataset size datasets cnndm use learning rate intermediate ne tuning effect extractive oracle bin use choice m tested tive bin lter examples unsupervised data affected zero shot transfer ablation periment rst m sentences wikipedia article summary ing n source lter examples according extractive table extractive bin noticeable effect transfer results xsum ate effect cnndm expected model missing information xsum s distinctive output style examined choice m affected formance set m cnndm m xsum ltered examples similar way based extractive bin target dataset choice m large impact cnndm performance decrease xsum result combined effect ltering examples based extractive bin gives insight importance subaspect abstractiveness compression xsum performance effect intermediate pretraining dataset size examined effect size transfer data downstream performance experiment single subset vised data points validation data vary data training examples results shown ble general increase addition data smaller increases data points decrease xsum likely noise variation compared xsum performance data points cnndm closer best performance believe highly extractive nature cnndm objective especially easy model bart learn pretrained denoising autoencoder xsum noticeable provement examples suspect abstractive objective harder model learn small datasets add amples noticeable improvement observations agree observation effect learning rate cnndm objective easier model overt learn remaining experiments use data points worked initial experiments effect summary sentence choice rst m sentences given wikipedia article chosen introduction intuitively form coherent mary article examine effect ing rst sentences compared choosing based intermediate dataset size cnndm xsum table comparison effect dataset size unsupervised intermediate ne tuning data zero shot transfer rouge performance target dataset cnndm xsum m sents ind orig ind orig p table comparison effect summary tence choice wikitransfer zero shot transfer criteria alternative pick sentences highest self rouge rouge score sentence sentences reference summary greedy fashion equivalent ind orig settings zhang et al zhang et al use setting sentences sen heuristic consistently corresponded longest resulting maries longer mented choosing important sentences precision ind orig p ison methods shown table choice summary sentence noticeable impact performance esize coherence lost summaries especially important longer cnndm maries important sentences rst sentence likely adds diversity data nding balance coherence style interesting direction additional work christensen et al effect lead bias cnndm ne tuning examined effect selecting m sentences greedily chosen calculating extractive oracle inserting beginning vised source document versus leaving place cnndm insertion meant mirror lead bias present dataset slight impact performance vs bias lead bias wikipedia vs target domain unlabeled data wikipedia natural source unlabeled data tested creating unsupervised data unlabeled domain data improved results performed dataset creation treating source data target domain wikipedia data resulted ples cnndm examples xsum fine tuning data resulted performance vs transfer data cnndm vs wikitransfer data xsum removal rst sentences remove mation case cnndm xsum initial sentence headline moved summary rst sentence constitute good summary remainder document wikipedia data contains multi paragraph introductions removal rst sentences leave structured document coherent informative tent placed result supports emphasis learning subaspects target domain simply domain training ysis output intermediate ne tuning cnndm revealed output tive information present summary directly stated source ne tuning wikipedia experimented domain pretraining denoising autoencoder objective zero shot transfer result consistent improvements datasets shot transfer results examine improvements shot transfer carry shot setting effect data augmentation niques results experiments varying training data sizes augmentation methods datasets shown table appendix shot performance round trip translation augmentation shot settings data augmentation tency training model outperforms transferring domain vanilla bart transfer reddit despite similar zero shot formance transfer cnndm sizeable gap shot transfer gests intermediate ne tuning closely align bart model target main furthermore training augmented data round trip translation best performance transfer wikitransfer cases bart transfer cnndm aug likely autoencoder pretraining objective bart biases ing lead bias allowing perform applications cnndm improvements target dataset transfer aug cons aug cons target dataset transfer aug cons aug cons target dataset transfer aug cons aug cons target dataset transfer aug cons aug cons wikitransfer wikitransfer wikitransfer wikitransfer cnndm reddit xsum reddit reddit cnndm bigpatent cnndm bart bart bart bart table comparison transfer results training dataset size data augmentatation datasets techniques showing generalizable robust formance models transferred wikitransfer training augmented data example cases example cases wikitransfer improvement seen aug setting transferring bart domain hypothesize noise present larger augmented dataset causes occasional mance drop wikitransfer models appear robust potential noise interestingly transfer bart domain aug improves cnndm extractive dataset largest drop performance augmented data occurs xsum xsum formance drop caused high pression xsum summaries leaves room noisy output compared longer cnndm bigpatent summaries preserve main meaning original summary better despite backtranslation noise cases aug wikitransfer results best formance points away art supervised performance table transfer consistency training nd contrasting trends added consistency loss target dataset cnndm xsum relevance consistency relevance consistency aug aug supervision table summary relevance factual consistency cnndm xsum datasets varying amounts training data results asterisks differ statistically signicant way p value supervision score compared data augmentation round trip lation note sizeable improvements abstractive cases xsum reddit hypothesize consistency loss promotes better abstraction model learns ant noise change meaning text equipped better tion paraphrasing consistency loss allows better training vanilla bart general better transfer domains consistency loss loss likely provides regularization factor prevents models overtting supervised examples wikitransfer model closely tuned target domain regularization large difference aligns observation wikitransfer models robust noisy backtranslated data xsum reddit transfer reddit shows similar results models consistency loss amples better rouge l wikitransfer better reddit vanilla bart s strong performance examples suggests formation provided subset sufcient good performance diminishing gains head start wikitransfer model provides zero shot transfer leave aspects consistency training role quality round trip translation data relation transfer domain future work human quality assessment examine improved performance wikitransfer manifests qualitative tations varying training data collect human judgment annotations quality dimensions studied kryscinski et al fabbri et al tency relevance consistency dened factual alignment summary summarized source text relevance dened selection important content relevant information included summary include uency dimension initial inspection data found uency high quality include ence inclusion single sentence xsum summaries coherence factor domly select examples dataset collect model output best performing shot aug aug fully supervised models cnndm xsum annotator sees source article randomly ordered output models rates summaries relevance consistency likert best score averaged score tive english speaking annotators example examples found moderate strong annotator correlations relevance sistency respectively results shown table cnndm increase consistency training data added statistically signicant difference student s t test value supervision relevance consistency results relevance model outperform likely model output concise judged including source information zero shot output closely resembles lead bias longer judged informative xsum relevance improves ably training data varied sults consistency statistically signicant differences uctuation scores transition model ing knowledge pretraining output versus knowledge target dataset obtained ne tuning discuss appendix conclusion improved performance ne tuning pretrained models dataset specic unsupervised data zero shot transfer ments demonstrate benets backs data augmentation consistency ing techniques future work plan porate additional subaspects intermediate ne tuning data redundancy diversity explicitly token overlap plan pand experiments domains compare pretrained models transfer references christos baziotis ion androutsopoulos ioannis konstas alexandros potamianos differentiable sequence sequence sequence autoencoder unsupervised abstractive sentence proceedings compression ference association computational linguistics human language technologies volume long short papers pages minneapolis minnesota association computational linguistics north american chapter arthur brazinskas mirella lapata ivan titov shot learning abstractive document opinion summarization emnlp arthur brazinskas mirella lapata ivan titov unsupervised opinion summarization proceedings copycat review generation annual meeting association tational linguistics pages online sociation computational linguistics janara christensen mausam stephen soderland coherent oren etzioni proceedings document summarization conference north american chapter association computational linguistics man language technologies pages lanta georgia association computational guistics eric chu peter liu meansum neural model unsupervised multi document abstractive summarization international conference chine learning pages sergey edunov myle ott michael auli david grangier understanding translation scale conference association tational linguistics acl gunes erkan dragomir r radev lexrank graph based lexical centrality salience text summarization journal articial intelligence search alexander r fabbri wojciech kryscinski bryan socher summeval arxiv mccann caiming xiong richard dragomir radev evaluating summarization evaluation preprint xinyu hua lu wang pilot study main adaptation effect neural abstractive proceedings workshop marization new frontiers summarization pages copenhagen denmark association tional linguistics empirical methods natural language ing international joint conference natural language processing emnlp ijcnlp pages hong kong china association computational linguistics byeongchang kim hyunwoo kim gunhee kim abstractive summarization reddit posts multi level memory networks diederick p kingma jimmy ba adam method stochastic optimization international conference learning representations iclr wojciech kryscinski nitish shirish keskar bryan cann caiming xiong richard socher neural text summarization critical evaluation proceedings conference empirical methods natural language processing international joint conference natural guage processing emnlp ijcnlp pages hong kong china association tional linguistics philippe laban andrew hsi john canny marti hearst summary loop learning write abstractive summaries examples ceedings annual meeting ciation computational linguistics pages online association computational guistics guillaume lample alexis conneau lingual language model pretraining advances neural information processing systems neurips logan lebanoff kaiqiang song fei liu adapting neural encoder decoder framework single multi document summarization proceedings conference empirical methods natural language processing pages brussels belgium association computational linguistics mike lewis yinhan liu naman goyal jan ghazvininejad abdelrahman mohamed omer levy veselin stoyanov luke zettlemoyer bart denoising sequence sequence training natural arxiv preprint lation language generation comprehension mike lewis yinhan liu naman goyal jan ghazvininejad abdelrahman mohamed omer levy veselin stoyanov luke zettlemoyer bart denoising sequence sequence training natural language generation translation comprehension proceedings nual meeting association computational linguistics pages online association computational linguistics taehee jung dongyeop kang lucas mentch uard hovy earlier nt better aspect analysis corpus system biases marization proceedings conference peter j liu mohammad saleh etienne pot ben goodrich ryan sepassi lukasz kaiser noam shazeer generating wikipedia ing long sequences iclr qizhe xie zihang dai eduard hovy minh thang ong quoc v le unsupervised data mentation consistency training arxiv preprint ziyi yang chenguang zhu robert gmyr michael zeng xuedong huang eric darve ted pretrained unsupervised summarization model theme modeling denoising arxiv preprint adams wei yu david dohan quoc le thang luong rui zhao kai chen fast accurate reading comprehension combining self attention international conference convolution learning representations jingqing zhang yao zhao mohammad saleh ter j liu pegasus pre training tracted gap sentences abstractive summarization hao zheng mirella lapata sentence trality revisited unsupervised summarization proceedings association computational linguistics pages florence italy association tational linguistics annual meeting jiawei zhou alexander rush simple supervised summarization contextual matching proceedings annual meeting association computational linguistics pages florence italy association tational linguistics haichao zhu li dong furu wei bing qin ting liu transforming wikipedia augmented data query focused summarization arxiv preprint rada mihalcea paul tarau textrank bringing order text proceedings conference empirical methods natural guage processing pages barcelona spain association computational linguistics ramesh nallapati bowen zhou cicero dos santos c aglar bing xiang tive text summarization sequence sequence proceedings rnns signll conference computational natural guage learning pages berlin germany association computational linguistics shashi narayan shay b cohen mirella lapata nt details summary topic aware convolutional neural networks treme summarization arxiv nathan ng kyra yee alexei baevski myle ott michael auli sergey edunov facebook fair s news translation task submission proceedings fourth conference chine translation volume shared task papers day pages florence italy association computational linguistics myle ott sergey edunov alexei baevski angela fan sam gross nathan ng david grangier fairseq fast extensible michael auli proceedings toolkit sequence modeling conference north american ter association computational linguistics demonstrations pages minneapolis nesota association computational linguistics evan sandhaus new york times annotated corpus linguistic data consortium philadelphia eva sharma chen li lu wang bigpatent large scale dataset abstractive coherent summarization christian szegedy vincent vanhoucke sergey ioffe jon shlens zbigniew wojna rethinking inception architecture computer vision proceedings ieee conference computer sion pattern recognition pages ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan n gomez ukasz kaiser illia polosukhin attention advances neural information need cessing systems pages danqing wang pengfei liu ming zhong jie fu xipeng qiu xuanjing huang exploring domain shift extractive text summarization ronald j williams david zipser ing algorithm continually running fully recurrent neural networks neural computation appendix comparison previous work comparison best performing itransfer shot results zhang et al table note reddit dataset predened subset son exact pegasus numbers obtained single run opposed average best subsets large improvements shot approach pared previous numbers shot experiment xsum xsum dataset highest overlap pegasus pretraining dataset datasets explored zhang et al work states effect removing overlap affect dataset mance hope comparison promotes future benchmarking shot results sample summary outputs include example model output summaries xsum dataset table example serves demonstrate output style varies training data increased source pretraining ne tuning data affects style model hallucinations source ment state rst ms jones model output gold target gives zero aug model outputs raine jones likely inuence bart wikipedia pretraining wikipedia ticle welsh politician ruth lorraine jones appear intermediate ne tuning subset zero aug resemble wikipedia introduction sentences output compact abstractive like xsum target sentence x y format wikipedia appears aug examples model output stylistically like fully supervised output gold summary stylistic change reected change hallucination use rachel jones likely caused appearance minister rachel haves article welsh tics found aug subset model point tting strongly target domain fully supervised output use carwyn jones match gender ms jones found times ing source documents caroline jones actual person question appears times training set phenomenon points esting research directions future work properly preserve world knowledge ing improvement faithfulness source text knowing insert world knowledge semi supervised uda experiments experimented original formulation uda semi supervised setting work label summary outputted model augmented example label original document unlabeled examples let xu unsupervised source ment target dataset vised shot examples let xu paraphrase input xu generated round trip translation data augmentation experiments apply teacher forcing require label yu obtain model applying model ne tuned analogous shot subset addition supervised loss y introduce loss xu yu m xu xu practice epoch iterate supervised examples loss lsup followed iterating unsupervised examples luda sampled unlabeled data points uda periments unlabeled data points uda results initial experiments shown table nd performance uda models dependent quality pseudo labels generated chose model trained rst data subset runs erate pseudo labels model higher performance model likely performed ter uda result quality labels improves shot training uda performance improves comparable unaugmented performance additional training setting details found precision oating point gave slightly better stable results port precision oating point numbers set maximum tokens batch use dient accumulation update frequency experiments data points aug experiments mented data points cnndm examples target dataset training samples cnndm xsum reddit bigpatent wikitransfer pegasus zhang et al table comparison zero shot performance best performing wikitransfer model case cnndm bigpatent xsum reddit zero shot results reported zhang et al source document ms jones told bbc radio wales want brussels replace nathan gill ukip wales leader mr gill told ukip assembly group ukip party chairman steve crowther stop double jobbing mep mr gill said making calls malice ve got brexit think possibly best leave role unlled ms jones told good morning wales programme m surprised ve formally asked d like ms jones south wales west people role ukip wales mep mr gill vacant south wales east david rowlands lorraine jones welsh labour party member welsh assembly south wales west aug lorraine jones welsh labour member welsh assembly south wales west aug wales assembly member south wales west rachel jones says formally asked ukip mep supervision minister carwyn jones said surprised asked ukip mep gold summary ukip s welsh mep post better left unlled result brexit party caroline jones said table example wikitransfer model output dataset size ne tuning illustrating model output style hallucinated entities differ model moves wikipedia pretraining source knowledge target dataset text stated source document highlighted red target dataset transfer uda target dataset transfer uda target dataset transfer uda target dataset transfer uda wikitransfer cnndm reddit bart wikitransfer wikitransfer wikitransfer xsum reddit reddit cnndm bigpatent cnndm bart bart bart table results experiments original formulation uda xie et al examples found necessary use smaller learning rate avoid immediate overtting figure rouge l scores datasets training dataset size data tion consistency loss showing eralizable robust performance models ferred wikitransfer form validation model update models typically converge iterations aug setting begin validation ing iterations models typically verged iterations train smoothed cross entropy szegedy et al loss shot transfer found models sensitive choice hyperparameters shot settings averaging subsets reduce variation serve large differences preliminary experiments varying size validation set leave task training little dation data future work use statistics original papers determine extractive bin dataset case reddit seeing strong shot performance cnndm investigated extractive oracle reddit dataset found higher r stated original paper select rst m sentences pseudo summaries wikipedia case reddit choose orig setting result difference zero shot performance qualitative inspection output found ind orig biased wikipedia style coherence summaries issue
